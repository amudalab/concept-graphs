
Heapsort
 /firstHeading 
 bodyContent 

 tagline 
From Wikipedia, the free encyclopedia
 /tagline 
 subtitle 

 /subtitle 
 jumpto 

					Jump to:					navigation, 					search

 /jumpto 
 bodycontent 

Heapsort



A run of the heapsort algorithm sorting an array of randomly permuted values. In the first stage of the algorithm the array elements are reordered to satisfy the heap property. Before the actual sorting takes place, the heap tree structure is shown briefly for illustration.


Class
Sorting algorithm


Data structure
Array


Worst case performance



Best case performance
[1]


Average case performance



Worst case space complexity
 total,  auxiliary


Heapsort is a comparison-based sorting algorithm to create a sorted array (or list), and is part of the selection sort family. Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort.




Contents


1 Overview
2 Variations
3 Comparison with other sorts
4 Pseudocode
5 Example
6 Notes
7 References
8 External links




[edit] Overview
Heapsort is a two step algorithm.
The first step is to build a heap out of the data.
The second step begins with removing the largest element from the heap. We insert the removed element into the sorted array. For the first element, this would be position 0 of the array. Next we reconstruct the heap and remove the next largest item, and insert it into the array. After we have removed all the objects from the heap, we have a sorted array. We can vary the direction of the sorted elements by choosing a min-heap or max-heap in step one.
Heapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed at Binary heap#Heap implementation.The heap's invariant is preserved after each extraction, so the only cost is that of extraction.
[edit] Variations

The most important variation to the simple variant is an improvement by R. W. Floyd that, in practice, gives about a 25% speed improvement by using only one comparison in each siftup run, which must be followed by a siftdown for the original child. Moreover, it is more elegant to formulate. Heapsort's natural way of indexing works on indices from 1 up to the number of items. Therefore the start address of the data should be shifted such that this logic can be implemented avoiding unnecessary +/- 1 offsets in the coded algorithm. The worst-case number of comparisons during the Floyd's heap-construction phase of Heapsort is known to be equal to 2N  2s2(N)  e2(N), where s2(N) is the sum of all digits of the binary representation of N and e2(N) is the exponent of 2 in the prime factorization of N.[2]
Ternary heapsort[3] uses a ternary heap instead of a binary heap; that is, each element in the heap has three children. It is more complicated to program, but does a constant number of times fewer swap and comparison operations. This is because each step in the shift operation of a ternary heap requires three comparisons and one swap, whereas in a binary heap two comparisons and one swap are required. The ternary heap does two steps in less time than the binary heap requires for three steps, which multiplies the index by a factor of 9 instead of the factor 8 of three binary steps. Ternary heapsort is about 12% faster than the simple variant of binary heapsort.[citation needed]
The smoothsort algorithm[4][5] is a variation of heapsort developed by Edsger Dijkstra in 1981. Like heapsort, smoothsort's upper bound is O(n logn). The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n logn) regardless of the initial sorted state. Due to its complexity, smoothsort is rarely used.
Levcopoulos and Petersson[6] describe a variation of heapsort based on a Cartesian tree that does not add an element to the heap until smaller values on both sides of it have already been included in the sorted output. As they show, this modification can allow the algorithm to sort more quickly than O(nlogn) for inputs that are already nearly sorted.

[edit] Comparison with other sorts
Heapsort primarily competes with quicksort, another very efficient general purpose nearly-in-place comparison-based sort algorithm.
Quicksort is typically somewhat faster due to better cache behavior and other factors, but the worst-case running time for quicksort is O(n2), which is unacceptable for large data sets and can be deliberately triggered given enough knowledge of the implementation, creating a security risk. See quicksort for a detailed discussion of this problem and possible solutions.
Thus, because of the O(n log n) upper bound on heapsort's running time and constant upper bound on its auxiliary storage, embedded systems with real-time constraints or systems concerned with security often use heapsort.
Heapsort also competes with merge sort, which has the same time bounds. Merge sort requires (n) auxiliary space, but heapsort requires only a constant amount. Heapsort typically runs faster in practice on machines with small or slow data caches. On the other hand, merge sort has several advantages over heapsort:

Like quicksort, merge sort on arrays has considerably better data cache performance, often outperforming heapsort on modern desktop computers because merge sort frequently accesses contiguous memory locations (good locality of reference); heapsort references are spread throughout the heap.
Heapsort is not a stable sort; merge sort is stable.
Merge sort parallelizes well and can achieve close to linear speedup with a trivial implementation; heapsort is not an obvious candidate for a parallel algorithm.
Merge sort can be adapted to operate on linked lists with O(1) extra space.[7] Heapsort can be adapted to operate on doubly linked lists with only O(1) extra space overhead.[citation needed]
Merge sort is used in external sorting; heapsort is not. Locality of reference is the issue.

Introsort is an interesting alternative to heapsort that combines quicksort and heapsort to retain advantages of both: worst case speed of heapsort and average speed of quicksort.
[edit] Pseudocode
The following is the "simple" way to implement the algorithm in pseudocode. Arrays are zero-based and swap is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at a[0], while at the end of the sort, the largest element is in a[end].

 function heapSort(a, count) is
     input:  an unordered array a of length count
 
     (first place a in max-heap order)
     heapify(a, count)
 
     end := count-1 //in languages with zero-based arrays the children are 2*i+1 and 2*i+2
     while end > 0 do
         (swap the root(maximum value) of the heap with the last element of the heap)
         swap(a[end], a[0])
         (decrease the size of the heap by one so that the previous max value will
         stay in its proper placement) 
         end := end - 1
         (put the heap back in max-heap order)
         siftDown(a, 0, end)
         
 
 function heapify(a, count) is
     (start is assigned the index in a of the last parent node)
     start := (count - 2) / 2
     
     while start  0 do
         (sift down the node at index start to the proper place such that all nodes below
          the start index are in heap order)
         siftDown(a, start, count-1)
         start := start - 1
     (after sifting down the root all nodes/elements are in heap order)
 
 function siftDown(a, start, end) is
     input:  end represents the limit of how far down the heap
                   to sift.
     root := start

     while root * 2 + 1  end do          (While the root has at least one child)
         child := root * 2 + 1        (root*2 + 1 points to the left child)
         swap := root        (keeps track of child to swap with)
         (check if root is smaller than left child)
         if a[swap] < a[child]
             swap := child
         (check if right child exists, and if it's bigger than what we're currently swapping with)
         if child+1  end and a[swap] < a[child+1]
             swap := child + 1
         (check if we need to swap at all)
         if swap != root
             swap(a[root], a[swap])
             root := swap          (repeat to continue sifting down the child now)
         else
             return

The heapify function can be thought of as building a heap from the bottom up, successively shifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be conceptually simpler to grasp. This "siftUp" version can be visualized as starting with an empty heap and successively inserting elements, whereas the "siftDown" version given above treats the entire input array as a full, "broken" heap and "repairs" it starting from the last non-trivial sub-heap (that is, the last parent node).




Difference in time complexity between the "siftDown" version and the "siftUp" version.


Also, the "siftDown" version of heapify has O(n) time complexity, while the "siftUp" version given below has O(n log n) time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.[8] This may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never has an impact on asymptotic analysis.
To grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call increases with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more "deep" nodes than there are "shallow" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the "bottom" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call decreases as the depth of the node on which the call is made increases. Thus, when the "siftDown" heapify begins and is calling siftDown on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the "height" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.
The heapsort algorithm itself has O(n log n) time complexity using either version of heapify.

 function heapify(a,count) is
     (end is assigned the index of the first (left) child of the root)
     end := 1
     
     while end < count
         (sift up the node at index end to the proper place such that all nodes above
          the end index are in heap order)
         siftUp(a, 0, end)
         end := end + 1
     (after sifting up the last node all nodes are in heap order)
 
 function siftUp(a, start, end) is
     input:  start represents the limit of how far up the heap to sift.
                   end is the node to sift up.
     child := end 
     while child > start
         parent := floor((child - 1) / 2)
         if a[parent] < a[child] then (out of max-heap order)
             swap(a[parent], a[child])
             child := parent (repeat to continue sifting up the parent now)
         else
             return

[edit] Example
Let { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)




An example on heapsort.


1. Build the heap


Heap
newly added element
swap elements


nul
6



6
5



6, 5
3



6, 5, 3
1



6, 5, 3,1
8



6, 5, 3, 1, 8

5, 8


6, 8, 3, 1, 5

6, 8


8, 6, 3, 1, 5
7



8, 6, 3, 1, 5, 7

3, 7


8, 6, 7, 1, 5, 3
2



8, 6, 7, 1, 5, 3, 2
4



8, 6, 7, 1, 5, 3, 2, 4

1, 4


8, 6, 7, 4, 5, 3, 2, 1




2. Sorting.


Heap
swap elements
delete element
sorted array
details


8, 6, 7, 4, 5, 3, 2, 1
8, 1


swap 8 and 1 in order to delete 8 from heap


1, 6, 7, 4, 5, 3, 2, 8

8

delete 8 from heap and add to sorted array


1, 6, 7, 4, 5, 3, 2
1, 7

8
swap 1 and 7 as they are not in order in the heap


7, 6, 1, 4, 5, 3, 2
1, 3

8
swap 1 and 3 as they are not in order in the heap


7, 6, 3, 4, 5, 1, 2
7, 2

8
swap 7 and 2 in order to delete 7 from heap


2, 6, 3, 4, 5, 1, 7

7
8
delete 7 from heap and add to sorted array


2, 6, 3, 4, 5, 1
2, 6

7, 8
swap 2 and 6 as they are not in order in the heap


6, 2, 3, 4, 5, 1
2, 5

7, 8
swap 2 and 5 as they are not in order in the heap


6, 5, 3, 4, 2, 1
6, 1

7, 8
swap 6 and 1 in order to delete 6 from heap


1, 5, 3, 4, 2, 6

6
7, 8
delete 6 from heap and add to sorted array


1, 5, 3, 4, 2
1, 5

6, 7, 8
swap 1 and 5 as they are not in order in the heap


5, 1, 3, 4, 2
1, 4

6, 7, 8
swap 1 and 4 as they are not in order in the heap


5, 4, 3, 1, 2
5, 2

6, 7, 8
swap 5 and 2 in order to delete 5 from heap


2, 4, 3, 1, 5

5
6, 7, 8
delete 5 from heap and add to sorted array


2, 4, 3, 1
2, 4

5, 6, 7, 8
swap 2 and 4 as they are not in order in the heap


4, 2, 3, 1
4, 1

5, 6, 7, 8
swap 4 and 1 in order to delete 4 from heap


1, 2, 3, 4

4
5, 6, 7, 8
delete 4 from heap and add to sorted array


1, 2, 3
1, 3

4, 5, 6, 7, 8
swap 1 and 3 as they are not in order in the heap


3, 2, 1
3, 1

4, 5, 6, 7, 8
swap 3 and 1 in order to delete 3 from heap


1, 2, 3

3
4, 5, 6, 7, 8
delete 3 from heap and add to sorted array


1, 2
1, 2

3, 4, 5, 6, 7, 8
swap 1 and 2 as they are not in order in the heap


2, 1
2, 1

3, 4, 5, 6, 7, 8
swap 2 and 1 in order to delete 2 from heap


1, 2

2
3, 4, 5, 6, 7, 8
delete 2 from heap and add to sorted array


1

1
2, 3, 4, 5, 6, 7, 8
delete 1 from heap and add to sorted array





1, 2, 3, 4, 5, 6, 7, 8
completed


[edit] Notes


^ http://dx.doi.org/10.1006/jagm.1993.1031
^ Suchenek, Marek A. (2012), "Elementary Yet Precise Worst-Case Analysis of Floyd's Heap-Construction Program", Fundamenta Informaticae (IOS Press) 120 (1): 75-92, doi:10.3233/FI-2012-751.
^ "Data Structures Using Pascal", 1991, page 405, gives a ternary heapsort as a student exercise. "Write a sorting routine similar to the heapsort except that it uses a ternary heap."
^ http://www.cs.utexas.edu/users/EWD/ewd07xx/EWD796a.PDF
^ http://www.cs.utexas.edu/~EWD/transcriptions/EWD07xx/EWD796a.html
^ Levcopoulos, Christos; Petersson, Ola (1989), "Heapsort - Adapted for Presorted Files", WADS '89: Proceedings of the Workshop on Algorithms and Data Structures, Lecture Notes in Computer Science, 382, London, UK: Springer-Verlag, pp.499509, doi:10.1007/3-540-51542-9_41.
^ Merge sort Wikipedia page
^ "Priority Queues". http://faculty.simpson.edu/lydia.sinapova/www/cmsc250/LN250_Weiss/L10-PQueues.htm. Retrieved 24 May 2011.


[edit] References

Williams, J. W. J. (1964), "Algorithm 232 - Heapsort", Communications of the ACM 7 (6): 347348
Floyd, Robert W. (1964), "Algorithm 245 - Treesort 3", Communications of the ACM 7 (12): 701
Carlsson, Svante (1987), "Average-case results on heapsort", BIT 27 (1): 2-17
Knuth, Donald (1997), "5.2.3, Sorting by Selection", Sorting and Searching, The Art of Computer Programming, 3 (third ed.), Addison-Wesley, pp.144155, ISBN0-201-89685-0
Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapters 6 and 7 Respectively: Heapsort and Priority Queues
A PDF of Dijkstra's original paper on Smoothsort
Heaps and Heapsort Tutorial by David Carlson, St. Vincent College
Heaps of Knowledge

[edit] External links



The Wikibook Algorithm implementation has a page on the topic of: Heapsort



Animated Sorting Algorithms: Heap Sort  graphical demonstration and discussion of heap sort
Courseware on Heapsort from Univ. Oldenburg - With text, animations and interactive exercises
NIST's Dictionary of Algorithms and Data Structures: Heapsort
Heapsort implemented in 12 languages
Sorting revisited by Paul Hsieh
A color graphical Java applet that allows experimentation with initial state and shows statistics
A PowerPoint presentation demonstrating how Heap sort works that is for educators.
Open Data Structures - Section 11.1.3 - Heap-Sort









v
t
e


Sorting algorithms






Theory



Computational complexity theory
Big O notation
Total order
Lists
Stability
Comparison sort
Adaptive sort
Sorting network
Integer sorting








Exchange sorts



Bubble sort
Cocktail sort
Oddeven sort
Comb sort
Gnome sort
Quicksort
Stooge sort
Bogosort








Selection sorts



Selection sort
Heapsort
Smoothsort
Cartesian tree sort
Tournament sort
Cycle sort








Insertion sorts



Insertion sort
Shellsort
Tree sort
Library sort
Patience sorting








Merge sorts



Merge sort
Cascade merge sort
Oscillating merge sort
Polyphase merge sort
Strand sort








Distribution sorts



American flag sort
Bead sort
Bucket sort
Burstsort
Counting sort
Pigeonhole sort
Proxmap sort
Radix sort
Flashsort








Concurrent sorts



Bitonic sorter
Batcher oddeven mergesort
Pairwise sorting network








Hybrid sorts



Timsort
Introsort
Spreadsort
UnShuffle sort
JSort








Other



Topological sorting
Pancake sorting
Spaghetti sort








 
NewPP limit report
Preprocessor visited node count: 8156/1000000
Preprocessor generated node count: 45505/1500000
Post-expand include size: 58197/2048000 bytes
Template argument size: 18387/2048000 bytes
Highest expansion depth: 21/40
Expensive parser function count: 3/500

 Saved in parser cache with key enwiki:pcache:idhash:13995-0!0!0!!en!4!* and timestamp 20130115062941 
  /bodycontent 
 printfooter 

				Retrieved from "http://en.wikipedia.org/w/index.php?title=Heapsort&oldid=533009183"				
 /printfooter 
 catlinks 
Categories: Sorting algorithmsComparison sortsHeaps (data structures)Hidden categories: Use dmy dates from July 2012All articles with unsourced statementsArticles with unsourced statements from March 2007Articles with unsourced statements from June 2012Articles with example pseudocode  /catlinks 

 debughtml 
 /debughtml 

 /bodyContent 

 