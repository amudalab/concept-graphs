
Parallel processing
 /firstHeading 
 bodyContent 

 tagline 
From Wikipedia, the free encyclopedia
 /tagline 
 subtitle 

 /subtitle 
 jumpto 

					Jump to:					navigation, 					search

 /jumpto 
 bodycontent 





This article includes a list of references, but its sources remain unclear because it has insufficient inline citations. Please help to improve this article by introducing more precise citations. (January 2009) 







This article needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2009) 


Parallel processing is the ability to carry out multiple operations or tasks simultaneously. The term is used in the contexts of both human cognition, particularly in the ability of the brain to simultaneously process incoming stimuli, and in parallel computing by machines.
[edit] Parallel processing by the brain
Parallel processing is the ability of the brain to simultaneously process incoming stimuli of differing quality. This becomes most important in vision, as the brain divides what it sees into four components: color, motion, shape, and depth. These are individually analyzed and then compared to stored memories, which helps the brain identify what you are viewing. The brain then combines all of these into the field of view that you see and comprehend. Parallel processing has been linked, by some experimental psychologists, to the Stroop effect. This is a continual and seamless operation.
[edit] Parallel processing in computers
Main article: Parallel computing
The simultaneous use of more than one CPU or processor core to execute a program or multiple computational threads. Ideally, parallel processing makes programs run faster because there are more engines (CPUs or Cores) running it. In practice, it is often difficult to divide a program in such a way that separate CPUs or cores can execute different portions without interfering with each other. Most computers have just one CPU, but some models have several, and multi-core processor chips are becoming the norm. There are even computers with thousands of CPUs.
With single-CPU, single-core computers, it is possible to perform parallel processing by connecting the computers in a network. However, this type of parallel processing requires very sophisticated software called distributed processing software.
Note that parallelism differs from concurrency. Concurrency is a term used in the operating systems and databases communities which refers to the property of a system in which multiple tasks remain logically active and make progress at the same time by interleaving the execution order of the tasks and thereby creating an illusion of simultaneously executing instructions.[1][2] Parallelism, on the other hand, is a term typically used by the supercomputing community to describe executions that physically execute simultaneously with the goal of solving a problem in less time or solving a larger problem in the same time. Parallelism exploits concurrency.[1]
Parallel processing is also called parallel computing. In the quest of cheaper computing alternatives parallel processing provides a viable option. The idle time of processor cycles across network can be used effectively by sophisticated distributed computing software. The term parallel processing is used to represent a large class of techniques which are used to provide simultaneous data processing tasks for the purpose of increasing the computational speed of a computer system.
Advantages:- Faster execution time., so higher throughput. Disadvantages:- More hardware required, also more power requirements. Not good for low power and mobile devices.
[edit] References


^ a b How to sound like a Parallel Programming Expert
^ Principles of Parallel Programming by Lin C. and Snyder L.



Myers, David G. Psychology. 6th ed. New York: Worth, 2001.
Parallel Processing via MPI & OpenMP, M. Firuziaan, O. Nommensen. Linux Enterprise, 10/2002
Gaurav Wadhwa
Computer Chronicles: Parallel Processing (1986) Last Access 2010-11-20

 
NewPP limit report
Preprocessor visited node count: 896/1000000
Preprocessor generated node count: 6029/1500000
Post-expand include size: 9357/2048000 bytes
Template argument size: 4127/2048000 bytes
Highest expansion depth: 17/40
Expensive parser function count: 2/500

 Saved in parser cache with key enwiki:pcache:idhash:105075-0!*!0!!*!4!* and timestamp 20130101170648 
  /bodycontent 
 printfooter 

				Retrieved from "http://en.wikipedia.org/w/index.php?title=Parallel_processing&oldid=530777537"				
 /printfooter 
 catlinks 
Categories: Neural codingHidden categories: Articles lacking in-text citations from January 2009All articles lacking in-text citationsArticles needing additional references from January 2009All articles needing additional references  /catlinks 

 debughtml 
 /debughtml 

 /bodyContent 

 