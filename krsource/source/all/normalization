
Database normalization
 /firstHeading 
 bodyContent 

 tagline 
From Wikipedia, the free encyclopedia
 /tagline 
 subtitle 

 /subtitle 
 jumpto 

					Jump to:					navigation, 					search

 /jumpto 
 bodycontent 
Database normalization is the process of organizing the fields and tables of a relational database to minimize redundancy and dependency. Normalization usually involves dividing large tables into smaller (and less redundant) tables and defining relationships between them. The objective is to isolate data so that additions, deletions, and modifications of a field can be made in just one table and then propagated through the rest of the database via the defined relationships.
Edgar F. Codd, the inventor of the relational model, introduced the concept of normalization and what we now know as the First Normal Form (1NF) in 1970.[1] Codd went on to define the Second Normal Form (2NF) and Third Normal Form (3NF) in 1971,[2] and Codd and Raymond F. Boyce defined the Boyce-Codd Normal Form (BCNF) in 1974.[3] Informally, a relational database table is often described as "normalized" if it is in the Third Normal Form.[4] Most 3NF tables are free of insertion, update, and deletion anomalies.
A standard piece of database design guidance is that the designer should create a fully normalized design; selective denormalization can subsequently be performed for performance reasons.[5]




Contents


1 Objectives of normalization

1.1 Free the database of modification anomalies
1.2 Minimize redesign when extending the database structure
1.3 Make the data model more informative to users
1.4 Avoid bias towards any particular pattern of querying
1.5 Example


2 Background to normalization: definitions
3 Normal forms
4 Denormalization

4.1 Non-first normal form (NF or N1NF)


5 See also
6 Notes and references
7 Further reading
8 External links




[edit] Objectives of normalization
A basic objective of the first normal form defined by Codd in 1970 was to permit data to be queried and manipulated using a "universal data sub-language" grounded in first-order logic.[6] (SQL is an example of such a data sub-language, albeit one that Codd regarded as seriously flawed.)[7]
The objectives of normalization beyond 1NF (First Normal Form) were stated as follows by Codd:


1. To free the collection of relations from undesirable insertion, update and deletion dependencies;
2. To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs;
3. To make the relational model more informative to users;
4. To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by.

 E.F. Codd, "Further Normalization of the Data Base Relational Model"[8]

The sections below give details of each of these objectives.
[edit] Free the database of modification anomalies




An update anomaly. Employee 519 is shown as having different addresses on different records.






An insertion anomaly. Until the new faculty member, Dr. Newsome, is assigned to teach at least one course, his details cannot be recorded.






A deletion anomaly. All information about Dr. Giddens is lost when he temporarily ceases to be assigned to any courses.


When an attempt is made to modify (update, insert into, or delete from) a table, undesired side-effects may follow. Not all tables can suffer from these side-effects; rather, the side-effects can only arise in tables that have not been sufficiently normalized. An insufficiently normalized table might have one or more of the following characteristics:

The same information can be expressed on multiple rows; therefore updates to the table may result in logical inconsistencies. For example, each record in an "Employees' Skills" table might contain an Employee ID, Employee Address, and Skill; thus a change of address for a particular employee will potentially need to be applied to multiple records (one for each of his skills). If the update is not carried through successfullyif, that is, the employee's address is updated on some records but not othersthen the table is left in an inconsistent state. Specifically, the table provides conflicting answers to the question of what this particular employee's address is. This phenomenon is known as an update anomaly.
There are circumstances in which certain facts cannot be recorded at all. For example, each record in a "Faculty and Their Courses" table might contain a Faculty ID, Faculty Name, Faculty Hire Date, and Course Codethus we can record the details of any faculty member who teaches at least one course, but we cannot record the details of a newly hired faculty member who has not yet been assigned to teach any courses except by setting the Course Code to null. This phenomenon is known as an insertion anomaly.
Under certain circumstances, deletion of data representing certain facts necessitates deletion of data representing completely different facts. The "Faculty and Their Courses" table described in the previous example suffers from this type of anomaly, for if a faculty member temporarily ceases to be assigned to any courses, we must delete the last of the records on which that faculty member appears, effectively also deleting the faculty member. This phenomenon is known as a deletion anomaly.

[edit] Minimize redesign when extending the database structure
When a fully normalized database structure is extended to allow it to accommodate new types of data, the pre-existing aspects of the database structure can remain largely or entirely unchanged. As a result, applications interacting with the database are minimally affected.
[edit] Make the data model more informative to users
Normalized tables, and the relationship between one normalized table and another, mirror real-world concepts and their interrelationships.
[edit] Avoid bias towards any particular pattern of querying
Normalized tables are suitable for general-purpose querying. This means any queries against these tables, including future queries whose details cannot be anticipated, are supported. In contrast, tables that are not normalized lend themselves to some types of queries, but not others.
For example, consider an online bookseller whose customers maintain wishlists of books they'd like to have. For the obvious, anticipated querywhat books does this customer want?it's enough to store the customer's wishlist in the table as, say, a homogeneous string of authors and titles.
With this design, though, the database can answer only that one single query. It cannot by itself answer interesting but unanticipated queries: What is the most-wished-for book? Which customers are interested in WWII espionage? How does Lord Byron stack up against his contemporary poets? Answers to these questions must come from special adaptive tools completely separate from the database. One tool might be software written especially to handle such queries. This special adaptive software has just one single purpose: in effect to normalize the non-normalized field.
Unforeseen queries can be answered trivially, and entirely within the database framework, with a normalized table.
[edit] Example
Querying and manipulating the data within an unnormalized data structure, such as the following non-1NF representation of customers' credit card transactions, involves more complexity than is really necessary:


Customer
Transactions


Jones



Tr. ID
Date
Amount


12890
14-Oct-2003
87


12904
15-Oct-2003
50





Wilkins



Tr. ID
Date
Amount


12898
14-Oct-2003
21





Stevens



Tr. ID
Date
Amount


12907
15-Oct-2003
18


14920
20-Nov-2003
70


15003
27-Nov-2003
60






To each customer corresponds a repeating group of transactions. The automated evaluation of any query relating to customers' transactions therefore would broadly involve two stages:

Unpacking one or more customers' groups of transactions allowing the individual transactions in a group to be examined, and
Deriving a query result based on the results of the first stage

For example, in order to find out the monetary sum of all transactions that occurred in October 2003 for all customers, the system would have to know that it must first unpack the Transactions group of each customer, then sum the Amounts of all transactions thus obtained where the Date of the transaction falls in October 2003.
One of Codd's important insights was that this structural complexity could always be removed completely, leading to much greater power and flexibility in the way queries could be formulated (by users and applications) and evaluated (by the DBMS). The normalized equivalent of the structure above would look like this:


Customer
Tr. ID
Date
Amount


Jones
12890
14-Oct-2003
87


Jones
12904
15-Oct-2003
50


Wilkins
12898
14-Oct-2003
21


Stevens
12907
15-Oct-2003
18


Stevens
14920
20-Nov-2003
70


Stevens
15003
27-Nov-2003
60


Now each row represents an individual credit card transaction, and the DBMS can obtain the answer of interest, simply by finding all rows with a Date falling in October, and summing their Amounts. The data structure places all of the values on an equal footing, exposing each to the DBMS directly, so each can potentially participate directly in queries; whereas in the previous situation some values were embedded in lower-level structures that had to be handled specially. Accordingly, the normalized design lends itself to general-purpose query processing, whereas the unnormalized design does not.
[edit] Background to normalization: definitions

Functional dependency
In a given table, an attribute Y is said to have a functional dependency on a set of attributes X (written X  Y) if and only if each X value is associated with precisely one Y value. For example, in an "Employee" table that includes the attributes "Employee ID" and "Employee Date of Birth", the functional dependency {Employee ID}  {Employee Date of Birth} would hold. It follows from the previous two sentences that each {Employee ID} is associated with precisely one {Employee Date of Birth}.
Trivial functional dependency
A trivial functional dependency is a functional dependency of an attribute on a superset of itself. {Employee ID, Employee Address}  {Employee Address} is trivial, as is {Employee Address}  {Employee Address}.
Full functional dependency
An attribute is fully functionally dependent on a set of attributes X if it is:

functionally dependent on X, and
not functionally dependent on any proper subset of X. {Employee Address} has a functional dependency on {Employee ID, Skill}, but not a full functional dependency, because it is also dependent on {Skill}.Even by the removal of {Employee ID} functional dependency still holds between {Employee Address} and {Skill}.




Transitive dependency
A transitive dependency is an indirect functional dependency, one in which XZ only by virtue of XY and YZ.
Multivalued dependency
A multivalued dependency is a constraint according to which the presence of certain rows in a table implies the presence of certain other rows.
Join dependency
A table T is subject to a join dependency if T can always be recreated by joining multiple tables each having a subset of the attributes of T.
Superkey
A superkey is a combination of attributes that can be used to uniquely identify a database record. A table might have many superkeys.
Candidate key
A candidate key is a special subset of superkeys that do not have any extraneous information in them: it is a minimal superkey.
Example:

A table with the fields <Name>, <Age>, <SSN> and <Phone Extension> has many possible superkeys. Three of these are <SSN>, <Phone Extension, Name> and <SSN, Name>. Of those, only <SSN> is a candidate key as the others contain information not necessary to uniquely identify records ('SSN' here refers to Social Security Number, which is unique to each person).




Non-prime attribute
A non-prime attribute is an attribute that does not occur in any candidate key. Employee Address would be a non-prime attribute in the "Employees' Skills" table.
Prime attribute
A prime attribute, conversely, is an attribute that does occur in some candidate key.
Primary key
One candidate key in a relation may be designated the primary key. While that may be a common practice (or even a required one in some environments), it is strictly notational and has no bearing on normalization. With respect to normalization, all candidate keys have equal standing and are treated the same.

[edit] Normal forms
The normal forms (abbrev. NF) of relational database theory provide criteria for determining a table's degree of vulnerability to logical inconsistencies and anomalies. The higher the normal form applicable to a table, the less vulnerable it is. Each table has a "highest normal form" (HNF): by definition, a table always meets the requirements of its HNF and of all normal forms lower than its HNF; also by definition, a table fails to meet the requirements of any normal form higher than its HNF.
The normal forms are applicable to individual tables; to say that an entire database is in normal form n is to say that all of its tables are in normal form n.
Newcomers to database design sometimes suppose that normalization proceeds in an iterative fashion, i.e. a 1NF design is first normalized to 2NF, then to 3NF, and so on. This is not an accurate description of how normalization typically works. A sensibly designed table is likely to be in 3NF on the first attempt; furthermore, if it is 3NF, it is overwhelmingly likely to have an HNF of 5NF. Achieving the "higher" normal forms (above 3NF) does not usually require an extra expenditure of effort on the part of the designer, because 3NF tables usually need no modification to meet the requirements of these higher normal forms.
The main normal forms are summarized below.



Normal form
Defined by
In
Brief definition


1NF
First normal form
Two versions: E.F. Codd (1970), C.J. Date (2003)
1970 [1] and 2003 [9]
Table faithfully represents a relation, primarily meaning it has at least one candidate key


2NF
Second normal form
E.F. Codd
1971 [2]
No non-prime attribute in the table is functionally dependent on a proper subset of any candidate key


3NF
Third normal form
Two versions: E.F. Codd (1971), C. Zaniolo (1982)
1971 [2] and 1982 [10]
Every non-prime attribute is non-transitively dependent on every candidate key in the table. The attributes that do not contribute to the description of the primary key are removed from the table. In other words, no transitive dependency is allowed.


EKNF
Elementary Key Normal Form
C. Zaniolo
1982 [10]
Every non-trivial functional dependency in the table is either the dependency of an elementary key attribute or a dependency on a superkey


BCNF
BoyceCodd normal form
Raymond F. Boyce and E.F. Codd
1974 [11]
Every non-trivial functional dependency in the table is a dependency on a superkey


4NF
Fourth normal form
Ronald Fagin
1977 [12]
Every non-trivial multivalued dependency in the table is a dependency on a superkey


5NF
Fifth normal form
Ronald Fagin
1979 [13]
Every non-trivial join dependency in the table is implied by the superkeys of the table


DKNF
Domain/key normal form
Ronald Fagin
1981 [14]
Every constraint on the table is a logical consequence of the table's domain constraints and key constraints


6NF
Sixth normal form
C.J. Date, Hugh Darwen, and Nikos Lorentzos
2002 [15]
Table features no non-trivial join dependencies at all (with reference to generalized join operator)


[edit] Denormalization
Main article: Denormalization
Databases intended for online transaction processing (OLTP) are typically more normalized than databases intended for online analytical processing (OLAP). OLTP applications are characterized by a high volume of small transactions such as updating a sales record at a supermarket checkout counter. The expectation is that each transaction will leave the database in a consistent state. By contrast, databases intended for OLAP operations are primarily "read mostly" databases. OLAP applications tend to extract historical data that has accumulated over a long period of time. For such databases, redundant or "denormalized" data may facilitate business intelligence applications. Specifically, dimensional tables in a star schema often contain denormalized data. The denormalized or redundant data must be carefully controlled during extract, transform, load (ETL) processing, and users should not be permitted to see the data until it is in a consistent state. The normalized alternative to the star schema is the snowflake schema. In many cases, the need for denormalization has waned as computers and RDBMS software have become more powerful, but since data volumes have generally increased along with hardware and software performance, OLAP databases often still use denormalized schemas.
Denormalization is also used to improve performance on smaller computers as in computerized cash-registers and mobile devices, since these may use the data for look-up only (e.g. price lookups). Denormalization may also be used when no RDBMS exists for a platform (such as Palm), or no changes are to be made to the data and a swift response is crucial.
[edit] Non-first normal form (NF or N1NF)
In recognition that denormalization can be deliberate and useful, the non-first normal form is a definition of database designs which do not conform to first normal form, by allowing "sets and sets of sets to be attribute domains" (Schek 1982). The languages used to query and manipulate data in the model must be extended accordingly to support such values.
One way of looking at this is to consider such structured values as being specialized types of values (domains), with their own domain-specific languages. However, what is usually meant by non-1NF models is the approach in which the relational model and the languages used to query it are extended with a general mechanism for such structure; for instance, the nested relational model supports the use of relations as domain values, by adding two additional operators (nest and unnest) to the relational algebra that can create and flatten nested relations, respectively.
Consider the following table:

First Normal Form

Person
Favourite Colour


Bob
blue


Bob
red


Jane
green


Jane
yellow


Jane
red


Assume a person has several favourite colours. Obviously, favourite colours consist of a set of colours modeled by the given table. To transform a 1NF into an NF table a "nest" operator is required which extends the relational algebra of the higher normal forms. Applying the "nest" operator to the 1NF table yields the following NF table:

Non-First Normal Form

Person
Favourite Colours


Bob



Favourite Colour


blue


red





Jane



Favourite Colour


green


yellow


red





To transform this NF table back into a 1NF an "unnest" operator is required which extends the relational algebra of the higher normal forms. The unnest, in this case, would make "colours" into its own table.
Although "unnest" is the mathematical inverse to "nest", the operator "nest" is not always the mathematical inverse of "unnest". Another constraint required is for the operators to be bijective, which is covered by the Partitioned Normal Form (PNF).
[edit] See also

Aspect (computer science)
Business rule
Canonical form
Cross-cutting concern
Database testing
Optimization (computer science)
Refactoring
Universal relation assumption

[edit] Notes and references


^ a b Codd, E.F. (June 1970). "A Relational Model of Data for Large Shared Data Banks". Communications of the ACM 13 (6): 377387. doi:10.1145/362384.362685. http://www.acm.org/classics/nov95/toc.html.
^ a b c Codd, E.F. "Further Normalization of the Data Base Relational Model". (Presented at Courant Computer Science Symposia Series 6, "Data Base Systems", New York City, May 2425, 1971.) IBM Research Report RJ909 (August 31, 1971). Republished in Randall J. Rustin (ed.), Data Base Systems: Courant Computer Science Symposia Series 6. Prentice-Hall, 1972.
^ Codd, E. F. "Recent Investigations into Relational Data Base Systems". IBM Research Report RJ1385 (April 23, 1974). Republished in Proc. 1974 Congress (Stockholm, Sweden, 1974). , N.Y.: North-Holland (1974).
^ C.J. Date. An Introduction to Database Systems. Addison-Wesley (1999), p. 290
^ Chris Date, for example, writes: "I believe firmly that anything less than a fully normalized design is strongly contraindicated ... [Y]ou should "denormalize" only as a last resort. That is, you should back off from a fully normalized design only if all other strategies for improving performance have somehow failed to meet requirements." Date, C.J. Database in Depth: Relational Theory for Practitioners. O'Reilly (2005), p. 152.
^ "The adoption of a relational model of data ... permits the development of a universal data sub-language based on an applied predicate calculus. A first-order predicate calculus suffices if the collection of relations is in first normal form. Such a language would provide a yardstick of linguistic power for all other proposed data languages, and would itself be a strong candidate for embedding (with appropriate syntactic modification) in a variety of host Ianguages (programming, command- or problem-oriented)." Codd, "A Relational Model of Data for Large Shared Data Banks", p. 381
^ Codd, E.F. Chapter 23, "Serious Flaws in SQL", in The Relational Model for Database Management: Version 2. Addison-Wesley (1990), pp. 371389
^ Codd, E.F. "Further Normalization of the Data Base Relational Model", p. 34
^ Date, C. J. "What First Normal Form Really Means" in Date on Database: Writings 20002006 (Springer-Verlag, 2006), pp. 127128.
^ a b Zaniolo, Carlo. "A New Normal Form for the Design of Relational Database Schemata." ACM Transactions on Database Systems 7(3), September 1982.
^ Codd, E. F. "Recent Investigations into Relational Data Base Systems". IBM Research Report RJ1385 (April 23, 1974). Republished in Proc. 1974 Congress (Stockholm, Sweden, 1974). New York, N.Y.: North-Holland (1974).
^ Fagin, Ronald (September 1977). "Multivalued Dependencies and a New Normal Form for Relational Databases". ACM Transactions on Database Systems 2 (1): 267. doi:10.1145/320557.320571. http://www.almaden.ibm.com/cs/people/fagin/tods77.pdf.
^ Ronald Fagin. "Normal Forms and Relational Database Operators". ACM SIGMOD International Conference on Management of Data, May 31-June 1, 1979, Boston, Mass. Also IBM Research Report RJ2471, Feb. 1979.
^ Ronald Fagin (1981) A Normal Form for Relational Databases That Is Based on Domains and Keys, Communications of the ACM, vol. 6, pp. 387415
^ C.J. Date, Hugh Darwen, Nikos Lorentzos. Temporal Data and the Relational Model. Morgan Kaufmann (2002), p. 176




Paper: "Non First Normal Form Relations" by G. Jaeschke, H. -J Schek; IBM Heidelberg Scientific Center. -> Paper studying normalization and denormalization operators nest and unnest as mildly described at the end of this wiki page.


[edit] Further reading

Litt's Tips: Normalization
Date, C. J. (1999), An Introduction to Database Systems (8th ed.). Addison-Wesley Longman. ISBN 0-321-19784-4.
Kent, W. (1983) A Simple Guide to Five Normal Forms in Relational Database Theory, Communications of the ACM, vol. 26, pp.120125
H.-J. Schek, P. Pistor Data Structures for an Integrated Data Base Management and Information Retrieval System

[edit] External links

Database Normalization Basics by Mike Chapple (About.com)
Database Normalization Intro, Part 2
An Introduction to Database Normalization by Mike Hillyer.
A tutorial on the first 3 normal forms by Fred Coulson
DB Normalization Examples
Description of the database normalization basics by Microsoft
Database Normalization and Design Techniques by Barry Wise, recommended reading for the Harvard MIS.
A Simple Guide to Five Normal Forms in Relational Database Theory









v
t
e


Topics in Database normalization









First normal form
Second normal form
Third normal form
BoyceCodd normal form
Fourth normal form
Fifth normal form
Domain/key normal form
Sixth normal form











Normalization
Denormalization
















v
t
e


Database






Main



Requirements
Theory
Models
Database management system
Machine
Server
Application
Connection

datasource
DSN


Administrator
Lock
Types
Tools








Languages



Data definition
Data manipulation
Query

information retrieval










Security



Activity monitoring
Audit
Forensics
Negative database








Design



Entities and relationships (and Enhanced notation)
Normalization
Refactoring








Programming



Abstraction layer
Object-relational mapping








Management



Virtualization
Tuning

caching


Migration
Preservation
Integrity








See also



Database-centric architecture
Intelligent database
Two-phase locking
Locks with ordered sharing
Load file
Publishing
Halloween Problem
Log shipping











 Book
 Category
 WikiProject
















v
t
e


Database management systems









Database models
Database normalization
Database storage
Distributed DBMS
Federated database system
Referential integrity
Relational algebra
Relational calculus
Relational database
Relational DBMS
Relational model
Object-relational database
Transaction processing








Concepts



Database
ACID
CRUD
Null
Candidate key
Foreign key
Primary key
Superkey
Surrogate key
Armstrong's axioms
NoSQL








Objects



Relation

table
column
row


View
Transaction
Log
Trigger
Index
Stored procedure
Cursor
Partition








Components



Concurrency control
Data dictionary
JDBC
XQJ
ODBC
Query language
Query optimizer
Query plan








Functions



Administration and automation
Query optimization
Replication











Database products
Object-oriented

comparison


Relational

comparison


Document-oriented
NewSQL









 
NewPP limit report
Preprocessor visited node count: 3285/1000000
Preprocessor generated node count: 24106/1500000
Post-expand include size: 51708/2048000 bytes
Template argument size: 19277/2048000 bytes
Highest expansion depth: 14/40
Expensive parser function count: 0/500

 Saved in parser cache with key enwiki:pcache:idhash:8640-0!*!0!!en!4!* and timestamp 20121220095214 
  /bodycontent 
 printfooter 

				Retrieved from "http://en.wikipedia.org/w/index.php?title=Database_normalization&oldid=528790005"				
 /printfooter 
 catlinks 
Categories: Database normalizationDatabase management systemsData modelingDatabase constraintsRelational algebra  /catlinks 

 debughtml 
 /debughtml 

 /bodyContent 

 