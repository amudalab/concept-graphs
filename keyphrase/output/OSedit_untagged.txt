however  the cost of this compaction is time and it can be particularly severe for large hard disks that use contiguous allocation  where compacting all the space 11.4 473 may take hours and may be necessary on a weekly basis some systems require that this function be done with the file system unmounted during this normal system operation generally can not be permitted  so such compaction is avoided at all costs on production machines most modern systems that need defragmentation can perform it during normal system operations  but the performance penalty can be substantial  another problem with contiguous allocation is determining how much space is needed for a file when the file is created  the total amount of space it will need must be found and allocated how does the creator  program or person  know the size of the file to be created in some cases  this detennination may be fairly simple  copying an existing file  for example  ; in general  however  the size of an output file may be difficult to estimate  if we allocate too little space to a file  we may find that the file can not be extended especially with a best-fit allocation strategy  the space on both sides of the file may be in use hence  we can not make the file larger in place  two possibilities then exist first  the user program can be terminated  with an appropriate error message the user must then allocate more space and run the program again these repeated runs may be costly to prevent them  the user will normally overestimate the amount of space needed  resulting in considerable wasted space the other possibility is to find a larger hole  copy the contents of the file to the new space  and release the previous space this series of actions can be repeated as long as space exists  although it can be time consuming however  the user need never be informed explicitly about what is happening ; the system continues despite the problem  although more and more slowly  even if the total amount of space needed for a file is known in advance  preallocation may be inefficient a file that will grow slowly over a long period  months or years  must be allocated enough space for its final size  even though much of that space will be unused for a long time the file therefore has a large amount of internal fragmentation  to minimize these drawbacks  some operating systems use a modified contiguous-allocation scheme here  a contiguous chunk of space is allocated initially ; then  if that amount proves not to be large enough  another chunk of contiguous space  known as an is added the location of a file 's blocks is then recorded as a location and a block count  plus a link to the first block of the next extent on some systems  the owner of the file can set the extent size  but this setting results in inefficiencies if the owner is incorrect internal fragm.entation can still be a problem if the extents are too large  and external fragmentation can become a problem as extents of varying sizes are allocated and deallocated the commercial veritas file system uses extents to optimize performance it is a high-performance replacement for the standard unix ufs  11.4.2 linked allocation solves all problems of contiguous allocation with linked allocation  each file is a linked list of disk blocks ; the disk blocks may be scattered anywhere on the disk the directory contains a pointer to the first and last blocks of the file for example  a file of five blocks might start at block 9 and continue at block 16  then block 1  then block 10  and finally block 25  figure 11.6   each block contains a pointer to the next block these pointers 474 chapter 11 directory 12 16 170180190 20021 ~ _20_ ~ 23_0-4 ~ 2402sc.51  260270 280290300310 figure i 1.6 linked allocation of disk space  are not made available to the user thus  if each block is 512 bytes in size  and a disk address  the poileter  requires 4 bytes  then the user sees blocks of 508 bytes  to create a new file  we simply create a new entry ile the directory with linked allocation  each directory entry has a pointer to the first disk block of the file this pointer is initialized to nil  the end-of-list pointer value  to signify an empty file the size field is also set to 0 a write to the file causes the free-space management system to filed a free block  and this new block is written to and is linked to the end of the file to read a file  we simply read blocks by following the pointers from block to block there is no external fragmentation with linked allocation  and any free block on the free-space list can be used to satisfy a request the size of a file need not be declared when that file is created  a file can continue to grow as long as free blocks are available consequently  it is never necessary to compact disk space  linked allocation does have disadvantages  however the major problem is that it can be used effectively only for sequential-access files to filed the ith block of a file  we must start at the begirueing of that file and follow the pointers rnetil we get to the ith block each access to a pointer requires a disk read  and some require a disk seek consequently  it is inefficient to support a direct-access capability for linked-allocation files  another disadvantage is the space required for the pointers if a pointer requires 4 bytes out of a 512-byte block  then 0.78 percent of the disk is being used for pointers  rather than for information each file requires slightly more space than it would otherwise  the usual solution to this problem is to collect blocks into multiples  called and to allocate clusters rather than blocks for instance  the file system may define a cluster as four blocks and operate on the disk only in cluster units pointers then use a much smaller percentage of the file 's disk space  this method allows the logical-to-physical block mapping to remain simple 11.4 475 but improves disk throughput  because fewer disk-head seeks are required  and decreases the space needed for block allocation and free-list management  the cost of this approach is an increase in internal fragmentation  because more space is wasted when a cluster is partially full than when a block is partially full clusters can be used to improve the disk-access time for many other algorithms as welt so they are used in most file systems  yet another problem of linked allocation is reliability recall that the files are linked together by pointers scattered all over the disk  and consider what would happen if a pointer were lost or damaged a bug in the operating-system software or a disk hardware failure might result in picking up the wrong pointer this error could in turn result in linking into the free-space list or into another file one partial solution is to use doubly linked lists  and another is to store the file name and relative block number in each block ; however  these schemes require even more overhead for each file  an important variation on linked allocation is the use of a  fat !  this simple but efficient method of disk-space allocation is used by the ms-dos and os/2 operating systems a section of disk at the beginning of each volume is set aside to contain the table the table has one entry for each disk block and is indexed by block number the fat is used in much the same way as a linked list the directory entry contains the block number of the first block of the file the table entry indexed by that block number contains the block number of the next block in the file this chain continues until it reaches the last block  which has a special end-of-file value as the table entry  an unused block is indicated by a table value of 0 allocating a new block to a file is a simple matter of finding the first 0-valued table entry and replacing the previous end-of-file value with the address of the new block the 0 is then replaced with the end-of-file value an illustrative example is the fat structure shown in figure 11.7 for a file consisting of disk blocks 217  618  and 339  directory entry name start block 0 217 618 339  618 339 number of disk blocks -1 fat figure 11.7 file-allocation table  476 chapter 11 the fat allocation scheme can result in a significant number of disk head seeks  unless the fat is cached the disk head must move to the start of the volume to read the fat and find the location of the block in question  then move to the location of the block itself in the worst case  both moves occur for each of the blocks a benefit is that random-access time is improved  because the disk head can find the location of any block by reading the information in the fat  11.4.3 indexed allocation linked allocation solves the external-fragmentation and size-declaration problems of contiguous allocation however  in the absence of a fat  linked allocation can not support efficient direct access  since the pointers to the blocks are scattered with the blocks themselves all over the disk and must be retrieved in order solves this problem by bringil1.g all the pointers together into one location  the blo ; ct   each file has its own index block  which is an array of disk-block addresses  the i th entry in the index block points to the i 111 block of the file the directory contains the address of the index block  figure 11.8   to find and read the i 1jz block  we use the pointer in the i 1lz index-block entry this scheme is similar to the paging scheme described il1 section 8.4  when the file is created  all pointers in the index block are set to nil when the ith block is first written  a block is obtained from the free-space manage1 ~ and its address is put in the ith index-block entry  indexed allocation supports direct access  without suffering from external fragmentation  because any free block on the disk can satisfy a request for more space indexed allocation does suffer from wasted space  however the pointer overhead of the index block is generally greater than the pointer overhead of linked allocation consider a common case in which we have a file of only one or two blocks with linked allocation  we lose the space of only one pointer per directory file jeep 16 figure 11.8 indexed allocation of disk space  11.4 allocation methods 477 block with indexed allocation  an entire index block must be allocated  even if only one or two pointers will be non-nil  this point raises the question of how large the index block should be every file must have an index block  so we want the index block to be as small as possible if the index block is too small  however  it will not be able to hold enough pointers for a large file  and a mechanism will have to be available to deal with this issue mechanisms for this purpose include the following  c linked scheme an index block is normally one disk block thus  it can be read and written directly by itself to allow for large files  we can link together several index blocks for example  an index block might contain a small header giving the name of the file and a set of the first 100 disk-block addresses the next address  the last word in the index block  is nil  for a small file  or is a pointer to another index block  for a large file   multilevel index a variant of linked representation uses a first-level index block to point to a set of second-level index blocks  which in tum point to the file blocks to access a block  the operating system uses the first-level index to find a second-level index block and then uses that block to find the desired data block this approach could be continued to a third or fourth level  depending on the desired maximum file size with 4,096-byte blocks  we could store 1,024 four-byte pointers in an index block two levels of indexes allow 1,048,576 data blocks and a file size of up to 4gb  combined scheme another alternative  used in the ufs  is to keep the first  say  15 pointers of the index block in the file 's inode the first 12 of these pointers point to direct blocks ; that is  they contain addresses of blocks that contain data of the file thus  the data for small files  of no more than 12 blocks  do not need a separate index block if the block size is 4 kb  then up to 48 kb of data can be accessed directly the next three pointers point to indirect blocks the first points to a single indirect block  which is an index block containing not data but the addresses of blocks that do contain data the second points to a double indirect block  which contains the address of a block that contains the addresses of blocks that contain pointers to the actual data blocks the last pointer contains the address of a triple indirect block under this method  the number of blocks that can be allocated to a file exceeds the amount of space addressable by the four-byte file pointers used by many operating systems a 32-bit file pointer reaches only 232 bytes  or 4gb many unix implementations  including solaris and ibm 's aix  now support up to 64-bit file pointers pointers of this size allow files and file systems to be terabytes in size a unix inode is shown in figure 11.9  indexed-allocation schemes suffer from some of the same performance problems as does linked allocation specifically  the index blocks can be cached in memory  but the data blocks may be spread all over a volume  11.4.4 performance the allocation methods that we have discussed vary in their storage efficiency and data-block access times both are important criteria in selecting the proper method or methods for an operating system to implement  478 chapter 11 implementing file systems figure 11.9 the unix inode  before selecting an allocation method  we need to determine how the systems will be used a system with mostly sequential access should not use the same method as a system with mostly random access  for any type of access  contiguous allocation requires only one access to get a disk block since we can easily keep the initial address of the file in memory  we can calculate immediately the disk address of the ith block  or the next block  and read it directly  for linked allocation  we can also keep the address of the next block in memory and read it directly this method is fine for sequential access ; for direct access  however  an access to the ith block might require i disk reads this problem indicates why linked allocation should not be used for an application requiring direct access  as a result  some systems support direct-access files by using contiguous allocation and sequential-access files by using linked allocation for these systems  the type of access to be made must be declared when the file is created a file created for sequential access will be linked and can not be used for direct access a file created for direct access will be contiguous and can support both direct access and sequential access  but its maximum length must be declared when it is created in this case  the operating system must have appropriate data structures and algorithms to support both allocation methods  files can be converted from one type to another by the creation of a new file of the desired type  into which the contents of the old file are copied the old file may then be deleted and the new file renamed  indexed allocation is more complex if the index block is already in memory  then the access can be made directly however  keeping the index block in memory requires considerable space if this memory space is not available  then we may have to read first the index block and then the desired data block for a two-level index  two index-block reads might be necessary for an 11.5 11.5 479 extremely large file  accessing a block near the end of the file would require reading in all the index blocks before the needed data block finally could be read thus  the performance of indexed allocation depends on the index structure  on the size of the file  and on the position of the block desired  some systems combine contiguous allocation with indexed allocation by using contiguous allocation for small files  up to three or four blocks  and automatically switching to an indexed allocation if the file grows large since most files are small  and contiguous allocation is efficient for small files  average performance can be quite good  for instance  the version of the unix operating system from sun microsystems was changed in 1991 to improve performance in the file-system allocation algorithm the performance measurements indicated that the maximum disk throughput on a typical workstation  a 12-mips sparcstation1  took 50 percent of the cpu and produced a disk bandwidth of only 1.5 me per second to improve performance  sun made changes to allocate space in clusters of 56 kb whenever possible  56 kb was the maximum size of a dma transfer on sun systems at that time   this allocation reduced external fragmentation  and thus seek and latency times in addition  the disk-reading routines were optimized to read in these large clusters the inode structure was left unchanged as a result of these changes  plus the use of read-ahead and free-behind  discussed in section 11.6.2   25 percent less cpu was used  and throughput substantially improved  many other optimizations are in use given the disparity between cpu speed and disk speed  it is not unreasonable to add thousands of extra instructions to the operating system to save just a few disk-head movements  furthermore  this disparity is increasing over time  to the point where hundreds of thousands of instructions reasonably could be used to optimize head movements  since disk space is limited  we need to reuse the space from deleted files for new files  if possible  write-once optical disks only allow one write to any given sector  and thus such reuse is not physically possible  to keep track of free disk space  the system maintains a the free-space list records all free disk blocks-those not allocated to some file or directory to create a file  we search the free-space list for the required amount of space and allocate that space to the new file this space is then removed from the free-space list when a file is deleted  its disk space is added to the free-space list the free-space list  despite its name  might not be implemented as a list  as we discuss next  11.5.1 bit vector frequently  the free-space list is implemented as a or each block is represented by 1 bit if the block is free  the bit is 1 ; if the block is allocated  the bit is 0  for example  consider a disk where blocks 2  3  4  5  8  9  10  11  12  13  17  18  25  26  and 27 are free and the rest of the blocks are allocated the free-space bit map would be 480 chapter 11 001111001111110001100000011100000   the main advantage of this approach is its relative simplicity and its efficiency in finding the first free block or n consecutive free blocks on the disk indeed  many computers supply bit-manipulation instructions that can be used effectively for that purpose for example  the intel family starting with the 80386 and the motorola family starting with the 68020 have instructions that return the offset in a word of the first bit with the value 1  these processors have powered pcs and macintosh systems  respectively   one technique for finding the first free block on a system that uses a bit-vector to allocate disk space is to sequentially check each word in the bit map to see whether that value is not 0  since a 0-valued word contains only 0 bits and represents a set of allocated blocks the first non-0 word is scanned for the first 1 bit  which is the location of the first free block the calculation of the block number is  number of bits per word  x  number of 0-value words  + offset of first 1 bit  again  we see hardware features driving software functionality unfortunately  bit vectors are inefficient unless the entire vector is kept in main memory  and is written to disk occasionally for recovery needs   keeping it in main memory is possible for smaller disks but not necessarily for larger ones  a 1.3-gb disk with 512-byte blocks would need a bit map of over 332 kb to track its free blocks  although clustering the blocks in groups of four reduces this number to around 83 kb per disk a 1-tb disk with 4-kb blocks requires 32 mb to store its bit map given that disk size constantly increases  the problem with bit vectors will continue to escalate a 1-pb file system would take a 32-gb bitmap just to manage its free space  11.5.2 linked list another approach to free-space management is to link together all the free disk blocks  keeping a pointer to the first free block in a special location on the disk and caching it in memory this first block contains a pointer to the next free disk block  and so on recall our earlier example  section 11.5.1   in which blocks 2  3  4  5  8  9  10  11  12  13  17  18  25  26  and 27 were free and the rest of the blocks were allocated in this situation  we would keep a pointer to block 2 as the first free block block 2 would contain a pointer to block 3  which would point to block 4  which would point to block 5  which would point to block 8  and so on  figure 11.10   this scheme is not efficient ; to traverse the list  we must read each block  which requires substantial i/0 time fortunately  however  traversing the free list is not a frequent action usually  the operating system simply needs a free block so that it can allocate that block to a file  so the first block in the free list is used the fat method incorporates free-block accounting into the allocation data structure no separate method is needed  11.5.3 grouping a modification of the free-list approach stores the addresses of n free blocks in the first free block the first n-1 of these blocks are actually free the last block contains the addresses of another n free blocks  and so on the addresses 11.5 481 figure 11.10 linked free-space list on disk  of a large number of free blocks can now be found quickly  unlike the situation when the standard linked-list approach is used  11.5.4 counting another approach takes advantage of the fact that  generally  several contiguous blocks may be allocated or freed simultaneously  particularly when space is allocated with the contiguous-allocation algorithm or through clustering thus  rather than keeping a list of n free disk addresses  we can keep the address of the first free block and the number  n  of free contiguous blocks that follow the first block each entry in the free-space list then consists of a disk address and a count although each entry requires more space than would a simple disk address  the overall list is shorter  as long as the count is generally greater than 1 note that this method of tracking free space is similar to the extent method of allocating blocks these entries can be stored in a b-tree  rather than a linked list for efficient lookup  insertion  and deletion  11.5.5 space maps sun 's zfs file system was designed to encompass huge numbers of files  directories  and even file systems  in zfs  we can create file-system hierarchies   the resulting data structures could have been large and inefficient if they had not been designed and implemented properly on these scales  metadata i/0 can have a large performance impact conside1 ~ for example  that if the freespace list is implemented as a bit map  bit maps must be modified both when blocks are allocated and when they are freed freeing 1gb of data on a 1-tb disk could cause thousands of blocks of bit maps to be updated  because those data blocks could be scattered over the entire disk  482 chapter 11 11.6 zfs uses a combination of techniques in its free-space managem.ent algorithm to control the size of data structures and minimize the i/0 needed to manage those structures first  zfs creates to divide the space on the device into chucks of manageable size a given volume may contain hundreds of metaslabs each metaslab has an associated space map zfs uses the counting algorithm to store information about free blocks rather than write count structures to disk  it uses log-structured file system techniques to record them the space map is a log of all block activity  allocatil g and freemg   in time order  in countil g format when zfs decides to allocate or free space from a metaslab  it loads the associated space map into memory in a balanced-tree structure  for very efficient operation   indexed by offset  and replays the log into that structure the in-memory space map is then an accurate representation of the allocated and free space in the metaslab zfs also condenses the map as much as possible by combining contiguous free blocks into a sil gle entry finally  the free-space list is updated on disk as part of the transaction-oriented operations of zfs during the collection and sortmg phase  block requests can still occur  and zfs satisfies these requests from the log in essence  the log plus the balanced tree is the free list  now that we have discussed various block-allocation and directorymanagement options  we can further consider their effect on performance and efficient disk use disks tend to represent a major bottleneck in system performance  since they are the slowest main computer component in this section  we discuss a variety of techniques used to improve the efficiency and performance of secondary storage  11.6.1 efficiency the efficient use of disk space depends heavily on the disk allocation and directory algorithms in use for instance  unix inodes are preallocated on a volume even an empty disk has a percentage of its space lost to inodes  however  by preallocating the inodes and spreading them across the volume  we improve the file system 's performance this improved performance results from the unix allocation and free-space algorithms  which try to keep a file 's data blocks near that file 's inode block to reduce seek time  as another example  let 's reconsider the clustermg scheme discussed in section 11.4  which aids in file-seek and file-transfer performance at the cost of internal fragmentation to reduce this fragmentation  bsd unix varies the cluster size as a file grows large clusters are used where they can be filled  and small clusters are used for small files and the last cluster of a file this system is described in appendix a  the types of data normally kept in a file 's directory  or inode  entry also require consideration commonly  a last write date is recorded to supply information to the user and to determine whether the file needs to be backed up some systems also keep a last access date  so that a user can determine when the file was last read the result of keeping this information is that  whenever the file is read  a field in the directory structure must be written 11.6 483 to that means the block must be read into memory  a section changed  and the block written back out to disk  because operations on disks occur only in block  or cluster  chunks so any time a file is opened for reading  its directory entry must be read and written as well this requirement can be inefficient for frequently accessed files  so we must weigh its benefit against its performance cost when designing a file system generally  every data item associated with a file needs to be considered for its effect on efficiency and performance  as an example  consider how efficiency is affected by the size of the pointers used to access data most systems use either 16 or 32-bit pointers throughout the operating system these pointer sizes limit the length of a file to either 216  64 kb  or 232 bytes  4 gb   some systems implement 64-bit pointers to increase this limit to 264 bytes  which is a very large number indeed however  64-bit pointers take more space to store and in turn make the allocation and free-space-management methods  linked lists  indexes  and so on  use more disk space  one of the difficulties in choosing a pointer size  or indeed any fixed allocation size within an operating system  is planning for the effects of changing technology consider that the ibm pc xt had a 10-mb hard drive and an ms-dos file system that could support only 32 mb  each fat entry was 12 bits  pointing to an 8-kb cluster  as disk capacities increased  larger disks had to be split into 32-mb partitions  because the file system could not track blocks beyond 32mb as hard disks with capacities of over 100mb became common  the disk data structures and algorithms in ms-dos had to be modified to allow larger file systems  each fat entry was expanded to 16 bits and later to 32 bits  the initial file-system decisions were made for efficiency reasons ; however  with the advent of ms-dos version 4  millions of computer users were inconvenienced when they had to switch to the new  larger file system sun 's zfs file system uses 128-bit pointers  which theoretically should never need to be extended  the minimum mass of a device capable of storing 2128 bytes using atomic-level storage would be about 272 trillion kilograms  as another example  consider the evolution of sun 's solaris operating system originally  many data structures were of fixed length  allocated at system startup these structures included the process table and the open-file table when the process table became full  no more processes could be created  when the file table became full  no more files could be opened the system would fail to provide services to users table sizes could be increased only by recompiling the kernel and rebooting the system since the release of solaris 2  almost all kernel structures have been allocated dynamically  eliminating these artificial limits on system performance of course  the algorithms that manipulate these tables are more complicated  and the operating system is a little slower because it must dynamically allocate and deallocate table entries ; but that price is the usual one for more general functionality  11.6.2 performance even after the basic file-system algorithms have been selected  we can still improve performance in several ways as will be discussed in chapter 13  most disk controllers include local memory to form an on-board cache that is large enough to store entire tracks at a time once a seek is performed  the track is read into the disk cache starting at the sector under the disk head 484 chapter 11 1/0 using read   and write   tile system figure 11.11 1/0 without a unified buffer cache   reducing latency time   the disk controller then transfers any sector requests to the operating system once blocks make it from the disk controller into main memory  the operating system may cache the blocks there  some systems maintain a separate section of main memory for a where blocks are kept under the assumption that will be used again shortly other systems cache file data using a the page cache uses virtual memory techniques to cache file data as pages rather than as file-system-oriented blocks cachii lg file data using virtual addresses is far more efficient than caching through physical disk blocks  as accesses interface with virtual memory rather than the file system several systems-including solaris  linux  and windows nt  2000  and xp-use caching to cache both process pages and file data this is known as some versions of unix and linux provide a to illustrate the benefits of the unified buffer cache  consider the two alternatives for opening and accessing a file one approach is to use memory mapping  section 9.7  ; the second is to use the standard system calls read   and write    without a unified buffer cache  we have a situation similar to figure 11.11 here  the read   and write   system calls go through the buffer cache  the memory-mapping call  however  requires using two caches-the page cache and the buffer cache a memory mapping proceeds by reading in disk blocks from the file system and storing them in the buffer cache because the virtual memory system does not interface with the buffer cache  the contents of the file in the buffer cache must be copied into the page cache this situation is known as and requires caching file-system data twice not only does it waste memory but it also wastes significant cpu and i/o cycles due to the extra data movement within system memory in addition  inconsistencies between the two caches can result in corrupt files in contrast  when a unified buffer cache is provided  both memory mapping and the read   and write   system calls use the same page cache this has the benefit of a voiding double 11.6 485 memory-mapped 1/0 buffer cache file system figure 11.12 1/0 using a unified buffer cache  caching  and it allows the virtual memory system to manage file-system data  the unified buffer cache is shown in figure 11.12  regardless of whether we are caching disk blocks or pages  or both   lru  section 9.4.4  seems a reasonable general-purpose algorithm for block or page replacement however  the evolution of the solaris page-caching algorithms reveals the difficulty in choosil1.g an algorithm solaris allows processes and the page cache to share unused memory versions earlier than solaris 2.5.1 made no distmction between allocatmg pages to a process and allocating them to the page cache as a result  a system performing many i/0 operations used most of the available memory for caching pages because of the high rates of i/0  the page scanner  section 9.10.2  reclaimed pages from processesrather than from the page cache-when free memory ran low solaris 2.6 and solaris 7 optionally implemented priority paging  in which the page scanner gives priority to process pages over the page cache solaris 8 applied a fixed limit to process pages and the file-system page cache  preventing either from forcing the other out of memory solaris 9 and 10 again changed the algorithms to maximize memory use and mmimize thrashing  another issue that can affect the performance of i/0 is whether writes to the file system occur synchronously or asynchronously  occur in the order in which the disk subsystem receives and the writes are not buffered thus  the calling routine must wait for the data to reach the disk drive before it can proceed in an the data are stored in the cache  and control returns to the caller asynchronous writes are done the majority of the time however  metadata writes  among others  can be synchronous operating systems frequently include a flag in the open system call to allow a process to request that writes be performed synchxonously for example  databases use this feature for atomic transactions  to assure that data reach stable storage in the required order  some systems optimize their page cache by using different replacement algorithms  depending on the access type of the file a file being read or written sequentially should not have its pages replaced in lru order  because the most recently used page will be used last  or perhaps never again instead  sequential access can be optimized by techniques known as free-behind and read-ahead removes a page from the buffer as soon as the next 486 chapter 11 11.7 page is requested the previous are not likely to be used again and waste buffer space with a requested page and several subsequent pages are read and cached these pages are likely to be requested after the current page is processed retrieving these data from the disk in one transfer and caching them saves a considerable ancount of time one might think that a track cache on the controller would elincinate the need for read-ahead on a multiprogrammed system however  because of the high latency and overhead involved in making many small transfers from the track cache to main memory  performing a read-ahead remains beneficial  the page cache  the file system  and the disk drivers have some interesting interactions when data are written to a disk file  the pages are buffered in the cache  and the disk driver sorts its output queue according to disk address  these two actions allow the disk driver to minimize disk-head seeks and to write data at times optimized for disk rotation unless synchronous writes are required  a process writing to disk simply writes into the cache  and the system asynchronously writes the data to disk when convenient the user process sees very fast writes when data are read from a disk file  the block i/0 system does some read-ahead ; however  writes are much more nearly asynchronous than are reads thus  output to the disk through the file system is often faster than is input for large transfers  counter to intuition  files and directories are kept both in main memory and on disk  and care must be taken to ensure that a system failure does not result in loss of data or in data inconsistency we deal with these issues in this section as well as how a system can recover from such a failure  a system crash can cause inconsistencies among on-disk file-system data structures  such as directory structures  free-block pointers  and free fcb pointers many file systems apply changes to these structures in place a typical operation  such as creating a file  can involve many structural changes within the file system on the disk directory structures are modified  fcbs are allocated  data blocks are allocated  and the free counts for all of these blocks are decreased these changes can be interrupted by a crash  and inconsistencies among the structures can result for example  the free fcb count might indicate that an fcb had been allocated  but the directory structure might not point to the fcb compounding this problem is the caching that operating systems do to optimize i/0 performance some changes may go directly to disk  while others may be cached if the cached changes do not reach disk before a crash occurs  more corruption is possible  in addition to crashes  bugs in file-system implementation  disk controllers  and even user applications can corrupt a file system file systems have varying methods to deal with corruption  depending on the file-system data structures and algorithms we deal with these issues next  11.7.1 consistency checking whatever the cause of corruption  a file system must first detect the problems and then correct them for detection  a scan of all the metadata on each file 11.7 487 system can confirm or deny the consistency of the systenl unfortunately  this scan can take minutes or hours and should occur every time the system boots  alternatively  a file system can record its state within the file-system metadata  at the start of any metadata change  a status bit is set to indicate that the metadata is in flux if all updates to the metadata complete successfully  the file system can clear that bit it however  the status bit remains set  a consistency checker is run  the systems program such as f s ck in unix or chkdsk in windows-compares the data in the directory structure with the data blocks on disk and tries to fix any inconsistencies it finds the allocation and free-space-management algorithms dictate what types of problems the checker can find and how successful it will be in fixing them for instance  if linked allocation is used and there is a link from any block to its next block  then the entire file can be reconstructed from the data blocks  and the directory structure can be recreated in contrast the loss of a directory entry on an indexed allocation system can be disastrous  because the data blocks have no knowledge of one another for this reason  unix caches directory entries for reads ; but any write that results in space allocation  or other metadata changes  is done synchronously  before the corresponding data blocks are written of course  problems can still occur if a synchronous write is interrupted by a crash  11.7.2 log-structured file systems computer scientists often fin.d that algorithms and technologies origil1.ally used in one area are equally useful in other areas such is the case with the database log-based recovery algorithms described in section 6.9.2 these logging algorithms have been applied successfully to the of consistency '-.il'c' .ll the resulting implementations are known as  or file systems  note that with the consistency-checking approach discussed in the preceding section  we essentially allow structures to break and repair them on recovery however  there are several problems with this approach one is that the inconsistency may be irreparable the consistency check may not be able to recover the structures  resulting in loss of files and even entire directories  consistency checking can require human intervention to resolve conflicts  and that is inconvenient if no human is available the system can remain unavailable until the human tells it how to proceed consistency checking also takes system and clock time to check terabytes of data  hours of clock time may be required  the solution to this problem is to apply log-based recovery techniques to file-system metadata updates both ntfs and the veritas file system use this method  and it is included in recent versions of ufs on solaris in fact it is becoming common on many operating systems  fundamentally  all metadata changes are written each set of operations for performing a specific task is a the changes are written to this log  they are considered to be committed  and the system call can return to the user process  allowing it to continue execution meanwhile  these log entries are replayed across the actual filesystem structures as the changes are made  a pointer is updated to indicate 488 chapter 11 which actions have completed and which are still incomplete when an entire committed transaction is completed  it is removed from the log file  which is actually a circular buffer a cb  ul ; n writes to the end of its space and then continues at the beginning  overwriting older values as it goes we would not want the buffer to write over data that had not yet been saved  so that scenario is avoided the log may be in a separate section of the file system or even on a separate disk spindle it is more efficient  but more complex  to have it under separate read and write heads  thereby decreasing head contention and seek times  if the system crashes  the log file will contain zero or more transactions  any transactions it contains were not completed to the file system  even though they were committed by the operating system  so they must now be completed  the transactions can be executed from the pointer until the work is complete so that the file-system structures remain consistent the only problem occurs when a transaction was aborted -that is  was not committed before the system crashed any changes from such a transaction that were applied to the file system must be undone  again preserving the consistency of the file system  this recovery is all that is needed after a crash  elimil ating any problems with consistency checking  a side benefit of using logging on disk metadata updates is that those updates proceed much faster than when they are applied directly to the on-disk data structures the reason for this improvement is found in the performance advantage of sequential i/0 over random i/0 the costly synchronous random meta data writes are turned into much less costly synchronous sequential writes to the log-structured file system 's loggil g area those changes in turn are replayed asynchronously via random writes to the appropriate structures  the overall result is a significant gain in performance of metadata-oriented operations  such as file creation and deletion  11.7.3 other solutions another alternative to consistency checking is employed by network appliance 's wafl file system and sun 's zfs file system these systems never overwrite blocks with new data rather  a transaction writes all data and metadata changes to new blocks when the transaction is complete  the metadata structures that pointed to the old versions of these blocks are updated to point to the new blocks the file system can then remove the old pointers and the old blocks and make them available for reuse if the old pointers and blocks are kept  a is created ; the snapshot is a view of the file system before the last update took place this solution should require no consistency checking if the pointer update is done atomically wafl does have a consistency checke1 ~ however  so some failure scenarios can still cause metadata corruption  see 11.9 for details of the wafl file system  sun 's zfs takes an even more im ovative approach to disk consistency  it never overwrites blocks  just as is the case with wafl however  zfs goes further and provides check-summing of all metadata and data blocks this solution  when combined with raid  assures that data are always correct zfs therefore has no consistency checker  more details on zfs are found in section 12.7.6  11.7 489 11.7.4 backup and restore magnetic disks sometimes fail  and care must be taken to ensure that the data lost in such a failure are not lost forever to this end  system programs can be used to data from disk to another storage device  such as a floppy disk  magnetic tape  optical disk  or other hard disk recovery from the loss of an individual file  or of an entire disk  may then be a matter of the data from backup  to minimize the copying needed  we can use information from each file 's directory entry for instance  if the backup program knows when the last backup of a file was done  and the file 's last write date in the directory indicates that the file has not changed since that date  then the file does not need to be copied again a typical backup schedule may then be as follows  1 copy to a backup medium all files from the disk this is called a to another medium all files changed since day 1 this is an day 3 copy to another medium all files changed since day 2  day n copy to another medium all files changed since day n-1 then go back to day 1  the new cycle can have its backup written over the previous set or onto a new set of backup media in this manner  we can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups of course  the larger the value of n  the greater the number of media that must be read for a complete restore an added advantage of this backup cycle is that we can restore any file accidentally deleted during the cycle by retrieving the deleted file from the backup of the previous day the length of the cycle is a compromise between the amount of backup medium needed and the number of days back from which a restore can be done to decrease the number of tapes that must be read to do a restore  an option is to perform a full backup and then each day back up all files that have changed since the full backup in this way  a restore can be done via the most recent incremental backup and the full backup  with no other incremental backups needed the trade-off is that more files will be modified each day  so each successive incremental backup involves more files and more backup media  a user ncay notice that a particular file is missing or corrupted long after the damage was done for this reason  we usually plan to take a full backup from time to time that will be saved forever it is a good idea to store these permanent backups far away from the regular backups to protect against hazard  such as a fire that destroys the computer and all the backups too  and if the backup cycle reuses media  we must take care not to reuse the 490 chapter 11 11.8 media too many times-if the media wear out  it might not be possible to restore any data from the backups  network file systems are commonplace they are typically integrated with the overall directory structure and interface of the client system nfs is a good example of a widely used  well-implemented client-server network file system here  we use it as an example to explore the implementation details of network file systems  nfs is both an implementation and a specification of a software system for accessing remote files across lans  or even wans   nfs is part of onc +  which most unix vendors and some pc operating systems support the implementation described here is part of the solaris operating system  which is a modified version of unix svr4 running on sun workstations and other hardware it uses either the tcp or udp /ip protocol  depending on the interconnecting network   the specification and the implementation are intertwined in our description of nfs whenever detail is needed  we refer to the sun implementation ; whenever the description is general  it applies to the specification also  there are multiple versions of nfs  with the latest being version 4 here  we describe version 3  as that is the one most commonly deployed  11.8.1 overview nfs views a set of interconnected workstations as a set of independent machines with independent file systems the goal is to allow some degree of sharing among these file systems  on explicit request  in a transparent manner sharing is based on a client-server relationship a machine may be  and often is  both a client and a server sharing is allowed between any pair of machines to ensure machine independence  sharing of a remote file system affects only the client machine and no other machine  so that a remote directory will be accessible in a transparent manner from a particular machine-say  from ml-a client of that machine must first carry out a mount operation the semantics of the operation involve mounting a remote directory over a directory of a local file system once the mount operation is completed  the mounted directory looks like an integral subtree of the local file system  replacing the subtree descending from the local directory the local directory becomes the name of the root of the newly mounted directory specification of the remote directory as an argument for the mount operation is not done transparently ; the location  or host name  of the remote directory has to be provided however  fron l then on  users on machine ml can access files in the remote directory in a totally transparent manner  to illustrate file mounting  consider the file system depicted in figure 11.13  where the triangles represent subtrees of directories that are of interest the figure shows three independent file systems of machines named u  51  and 52 at this point  on each machine  only the local files can be accessed figure 11.14  a  shows the effects of mounting 81  /usr/shared over u  /usr/local  this figure depicts the view users on u have of their file system notice that after the mount is complete  they can access any file within the dirl directory 11.8 491 u  s1  s2  usr usr usr figure 11.13 three independent file systems  using the prefix /usr /local/ dir1 the original directory /usr /local on that machine is no longer visible  subject to access-rights accreditation  any file system  or any directory within a file system  can be mounted remotely on top of any local directory  diskless workstations can even mount their own roots from servers cascading mounts are also permitted in some nfs implementations that is  a file system can be mounted over another file system that is remotely mounted  not local a machine is affected by only those mounts that it has itself invoked mounting a remote file system does not give the client access to other file systems that were  by chance  mounted over the former file system thus  the mount mechanism does not exhibit a transitivity property  in figure 11.14  b   we illustrate cascading mounts the figure shows the result of mounting s2  /usr /dir2 over u  /usr/local/dir1  which is already remotely mounted from 51 users can access files within dir2 on u using the u  u   a   b  figure 11.14 mounting in nfs  a  mounts  b  cascading mounts  492 chapter 11 prefix /usr/local/dir1 if a shared file system is mounted over a user 's home directories on all machines in a network  the user can log into any workstation and get his honce environment this property permits one of the design goals of nfs was to operate in a heterogeneous environment of different machines  operating systems  and network architectures  the nfs specification is independent of these media and thus encourages other implementations this independence is achieved through the use of rpc primitives built on top of an external data representation  xdr  protocol used between two implementation-independent interfaces hence  if the system consists of heterogeneous machines and file systems that are properly interfaced to nfs  file systems of different types can be mounted both locally and remotely  the nfs specification distinguishes between the services provided by a mount mechanism and the actual remote-file-access services accordingly  two separate protocols are specified for these services  a mount protocol and a protocol for remote file accesses  the the protocols are specified as sets of rpcs these rpcs are the building blocks used to implement transparent remote file access  11.8.2 the mount protocol the establishes the initial logical connection between a server and a client in sun 's implementation  each machine has a server process  outside the kernel  performing the protocol functions  a mount operation includes the name of the remote directory to be mounted and the name of the server machine storing it the mount request is mapped to the corresponding rpc and is forwarded to the mount server running on the specific server machine the server maintains an that specifies local file systems that it exports for mounting  along with names of machines that are permitted to mount them  in solaris  this list is the i etc/dfs/dfstab  which can be edited only by a superuser  the specification can also include access rights  such as read only to simplify the maintenance of export lists and mount tables  a distributed naming scheme can be used to hold this information and make it available to appropriate clients  recall that any directory within an exported file system can be mounted remotely by an accredited machine a component unit is such a directory when the server receives a mount request that conforms to its export list  it returns to the client a file handle that serves as the key for further accesses to files within the mounted file system the file handle contains all the information that the server needs to distinguish an individual file it stores in unix terms  the file handle consists of a file-system identifier and an inode number to identify the exact mounted directory within the exported file system  the server also maintains a list of the client machines and the corresponding currently mounted directories this list is used mainly for administrative purposes-for instance  for notifying all clients that the server is going down  only through addition and deletion of entries in this list can the server state be affected by the mount protocol  usually  a system has a static mounting preconfiguration that is established at boot time  i etc/vfstab in solaris  ; however  this layout can be modified in 11.8 493 addition to the actual mount procedure  the mount protocol includes several other procedures  such as unmount and return export list  11.8.3 the nfs protocol the nfs protocol provides a set of rpcs for remote file operations the procedures support the following operations  searching for a file within a directory reading a set of directory entries manipulating links and directories accessing file attributes reading and writing files these procedures can be invoked only after a file handle for the remotely mounted directory has been established  the omission of open   and close   operations is intentional a prominent feature of nfs servers is that they are stateless servers do not maintain information about their clients from one access to another no parallels to unix 's open-files table or file structures exist on the server side consequently  each request has to provide a full set of arguments  including a unique file identifier and an absolute offset inside the file for the appropriate operations  the resulting design is robust ; no special measures need be taken to recover a server after a crash file operations must be idempotent for this purpose  every nfs request has a sequence number  allowing the server to determine if a request is duplicated or if any are missing  maintaining the list of clients that we mentioned seems to violate the statelessness of the server howeve1 ~ this list is not essential for the correct operation of the client or the server  and hence it does not need to be restored after a server crash consequently  it might include inconsistent data and is treated as only a hint  a further implication of the stateless-server philosophy and a result of the synchrony of an rpc is that modified data  including indirection and status blocks  must be committed to the server 's disk before results are returned to the client that is  a client can cache write blocks  but when it flushes them to the server  it assumes that they have reached the server 's disks the server must write all nfs data synchronously thus  a server crash and recovery will be invisible to a client ; all blocks that the server is managing for the client will be intact the consequent performance penalty can be large  because the advantages of caching are lost performance can be increased using storage with its own nonvolatile cache  usually battery-backed-up memory   the disk controller ackiwwledges the disk write when the write is stored in the nonvolatile cache in essence  the host sees a very fast synchronous write  these blocks remain intact even after system crash and are written from this stable storage to disk periodically  a single nfs write procedure call is guaranteed to be atomic and is not intermixed with other write calls to the same file the nfs protocol  however  does not provide concurrency-control mechanisms a write   system call may 494 chapter 11 client server figure 11.15 schematic view of the nfs architecture  be broken down into several rpc writes  because each nfs write or read call can contain up to 8 kb of data and udp packets are limited to 1,500 bytes as a result  two users writing to the same remote file may get their data intermixed  the claim is that  because lock management is inherently stateful  a service outside the nfs should provide locking  and solaris does   users are advised to coordinate access to shared files using mechanisms outside the scope of nfs  nfs is integrated into the operating system via a vfs as an illustration of the architecture  let 's trace how an operation on an already open remote file is handled  follow the example in figure 11.15   the client initiates the operation with a regular system call the operating-system layer maps this call to a vfs operation on the appropriate vnode the vfs layer identifies the file as a remote one and invokes the appropriate nfs procedure an rpc call is made to the nfs service layer at the remote server this call is reinjected to the vfs layer on the remote system  which finds that it is local and invokes the appropriate file-system operation this path is retraced to return the result  an advantage of this architecture is that the client and the server are identical ; thus  a machine may be a client  or a server  or both the actual service on each server is performed by kernel threads  11.8.4 path-name translation in nfs involves the parsing of a path name such as /usr/local/dir1/file txt into separate directory entries  or components   1  usr   2  local  and  3  dir1 path-name translation is done by breaking the path into component names and perform.ing a separate nfs lookup call for every pair of component name and directory vnode once a n10unt point is crossed  every component lookup causes a separate rpc to the server this 11.8 495 expensive path-name-traversal scheme is needed  since the layout of each client 's logical name space is unique  dictated by the mounts the client has performed it would be itluch more efficient to hand a server a path name and receive a target vnode once a mount point is encountered at any point  however  there might be another mount point for the particular client of whicb the stateless server is unaware  so that lookup is fast  a directory-name-lookup cache on the client side holds the vnodes for remote directory names this cache speeds up references to files with the same initial path name the directory cache is discarded when attributes returned from the server do not match the attributes of the cached vnode  recall that mounting a remote file system on top of another already mounted remote file system  a cascading mount  is allowed in some implementations of nfs however  a server can not act as an intermediary between a client and another server instead  a client must establish a direct client-server com1ection with the second server by directly mounting the desired directory  when a client has a cascading mount  more than one server can be involved in a path-name traversal however  each component lookup is performed between the original client and some server therefore  when a client does a lookup on a directory on which the server has mounted a file system  the client sees the underlying directory instead of the mounted directory  11.8.5 remote operations with the exception of opening and closing files  there is almost a one-to-one correspondence between the regular unix system calls for file operations and the nfs protocol rpcs thus  a remote file operation can be translated directly to the corresponding rpc conceptually  nfs adheres to the remote-service paradigm ; but in practice  buffering and caching techniques are employed for the sake of performance no direct correspondence exists between a remote operation and an rpc instead  file blocks and file attributes are fetched by the rpcs and are cached locally future remote operations use the cached data  subject to consistency constraints  there are two caches  the file-attribute  inode-infonnation  cache and the file-blocks cache when a file is opened  the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes the cached file blocks are used only if the corresponding cached attributes are up to date the attribute cache is updated whenever new attributes arrive from the server cached attributes are  by default  discarded after 60 seconds both read-ahead and delayed-write techniques are used between the server and the client clients do not free delayed-write blocks until the server confirms that the data have been written to disk delayed-write is retained even when a file is opened concurrently  in conflicting modes hence  unix semantics  section 10.5.3.1  are not preserved  tuning the system for performance makes it difficult to characterize the consistency semantics of nfs new files created on a machine may not be visible elsewhere for 30 seconds furthermore  writes to a file at one site may or may not be visible at other sites that have this file open for reading new opens of a file observe only the changes that have already been flushed to the server thus  nfs provides neither strict emulation of unix semantics nor the 496 chapter 11 11.9 session sen antics of andrew  section 10.5.3.2  .ln spite of these drawbacks  the utility and good performance of the mechanism make it the most widely used multi-vendor-distributed system in operation  disk i/o has a huge impact on system performance as a result  file-system design and implementation command quite a lot of attention from system designers some file systems are general purpose  in that they can provide reasonable performance and functionality for a wide variety of file sizes  file types  and i/0 loads others are optimized for specific tasks in an attempt to provide better performance in those areas than general-purpose file systems  the wafl file system from network appliance is an example of this sort of optimization wafl  the write-anywhere file layout  is a powerful  elegant file system optimized for random writes  wafl is used exclusively on network file servers produced by network appliance and so is meant for use as a distributed file system it can provide files to clients via the nfs  cifs  ftp  and http protocols  although it was designed just for nfs and cifs when many clients use these protocols to talk to a file server  the server may see a very large demand for random reads and an even larger demand for random writes the nfs and cifs protocols cache data from read operations  so writes are of the greatest concern to file-server creators  wafl is used on file servers that include an nvram cache for writes  the wafl designers took advantage of running on a specific architecture to optimize the file system for random i/0  with a stable-storage cache in front  ease of use is one of the guiding principles of wafl  because it is designed to be used in an appliance its creators also designed it to include a new snapshot functionality that creates multiple read-only copies of the file system at different points in time  as we shall see  the file system is similar to the berkeley fast file system  with many modifications it is block-based and uses inodes to describe files each inode contains 16 pointers to blocks  or indirect blocks  belonging to the file described by the inode each file system has a root inode all of the metadata lives in files  all inodes are in one file  the free-block map in another  and the free-inode root inode 1 free blotk map i figure 11.16 the wafl file layout 11.9 497 map in a third  as shown in figure 11.16 because these are standard files  the data blocks are not limited in location and can be placed anywhere if a file system is expanded by addition of disks  the lengths of the metadata files are automatically expanded by the file systen  thus  a wafl file system is a tree of blocks with the root inode as its base to take a snapshot  wafl creates a copy of the root inode any file or metadata updates after that go to new blocks rather than overwriting their existing blocks the new root inode points to metadata and data changed as a result of these writes meanwhile  the snapshot  the old root inode  still points to the old blocks  which have not been updated it therefore provides access to the file system just as it was at the instant the snapshot was made-and takes very little disk space to do so ! in essence  the extra disk space occupied by a snapshot consists of just the blocks that have been modified since the snapshot was taken  an important change from more standard file systems is that the free-block map has more than one bit per block it is a bitmap with a bit set for each snapshot that is using the block when all snapshots that have been using the block are deleted  the bit map for that block is all zeros  and the block is free to be reused used blocks are never overwritten  so writes are very fast  because a write can occur at the free block nearest the current head location there are many other performance optimizations in wafl as well  many snapshots can exist simultaneously  so one can be taken each hour of the day and each day of the month a user with access to these snapshots can access files as they were at any of the times the snapshots were taken  the snapshot facility is also useful for backups  testing  versioning  and so on  wafl 's snapshot facility is very efficient in that it does not even require that copy-on-write copies of each data block be taken before the block is modified  other file systems provide snapshots  but frequently with less efficiency wafl snapshots are depicted in figure 11.17  newer versions of wafl actually allow read-write snapshots  known as ,.hj ' ' ' clones are also efficient  using the same techniques as shapshots in this case  a read-only snapshot captures the state of the file system  and a clone refers back to that read-only snapshot any writes to the clone are stored in new blocks  and the clone 's pointers are updated to refer to the new blocks  the original snapshot is unmodified  still giving a view into the file system as it was before the clone was updated clones can also be promoted to replace the original file system ; this involves throwing out all of the old pointers and any associated old blocks clones are useful for testing and upgrades  as the original version is left untouched and the clone deleted when the test is done or if the upgrade fails  another feature that naturally falls from the wafl file system implementation is the duplication and synchronization of a set of data over a network to another system first  a snapshot of a wafl file system is duplicated to another system when another snapshot is taken on the source system  it is relatively easy to update the remote system just by sending over all blocks contained in the new snapshot these blocks are the ones that have changed between the times the two snapshots were taken the remote system adds these blocks to the file system and updates its pointers  and the new system then is a duplicate of the source system as of the time of the second snapshot repeating this process maintains the remote system as a nearly up-to-date copy of the first 498 chapter 11 11.10  a  before a snapshot   b  after a snapshot  before any blocks change   c  after block d has changed to o  figure 11.17 snapshots in wafl  system such replication is used for disaster recovery should the first system be destroyed  most of its data are available for use on the remote system  finally  we should note that sun 's zfs file system supports similarly efficient snapshots  clones  and replication  the file system resides permanently on secondary storage  which is designed to hold a large amount of data permanently the most common secondary-storage medium is the disk  physical disks may be segmented into partitions to control media use and to allow multiple  possibly varying  file systems on a single spindle  these file systems are mounted onto a logical file system architecture to make then available for use file systems are often implemented in a layered or modular structure the lower levels deal with the physical properties of storage devices upper levels deal with symbolic file names and logical properties of files intermediate levels map the logical file concepts into physical device properties  any file-system type can have different structures and algorithms a vfs layer allows the upper layers to deal with each file-system type uniformly even 499 remote file systems can be integrated into the system 's directory structure and acted on by standard system calls via the vfs interface  the various files can be allocated space on the disk in three ways  through contiguous  linked  or indexed allocation contiguous allocation can suffer from external fragmentation direct access is very inefficient with linked allocation indexed allocation may require substantial overhead for its index block these algorithms can be optimized in many ways contiguous space can be enlarged through extents to increase flexibility and to decrease external fragmentation indexed allocation can be done in clusters of multiple blocks to increase throughput and to reduce the number of index entries needed indexing in large clusters is similar to contiguous allocation with extents  free-space allocation methods also influence the efficiency of disk-space use  the performance of the file system  and the reliability of secondary storage  the methods used include bit vectors and linked lists optimizations include grouping  countilcg  and the fat  which places the linked list in one contiguous area  directory-management routines must consider efficiency  performance  and reliability a hash table is a commonly used method  as it is fast and efficient unfortunately  damage to the table or a system crash can result in inconsistency between the directory information and the disk 's contents  a consistency checker can be used to repair the damage operating-system backup tools allow disk data to be copied to tape  enabling the user to recover from data or even disk loss due to hardware failure  operating system bug  or user error  network file systems  such as nfs  use client-server methodology to allow users to access files and directories from remote machines as if they were on local file systems system calls on the client are translated into network protocols and retranslated into file-system operations on the server  networking and multiple-client access create challenges in the areas of data consistency and performance  due to the fundamental role that file systems play in system operation  their performance and reliability are crucial techniques such as log structures and cachirtg help improve performance  while log structures and raid improve reliability the wafl file system is an example of optimization of performance to match a specific i/o load  11.1 in what situations would using memory as a ram disk be more useful than using it as a disk cache 11.2 consider a file systenc that uses a modifed contiguous-allocation scheme with support for extents a file is a collection of extents  with each extent corresponding to a contiguous set of blocks a key issue in such systems is the degree of variability in the size of the 500 chapter 11 extents what are the advantages and disadvantages of the following schemes a all extents are of the same size  and the size is predetermined  b extents can be of any size and are allocated dynamically  c extents can be of a few fixed sizes  and these sizes are predetermined  11.3 some file systems allow disk storage to be allocated at different levels of granularity for instance  a file system could allocate 4 kb of disk space as a single 4-kb block or as eight 512-byte blocks how could we take advantage of this flexibility to improve performance what modifications would have to be made to the free-space management scheme in order to support this feature 11.4 what are the advantages of the variant of linked allocation that uses a fat to chain together the blocks of a file 11.5 consider a file currently consisting of 100 blocks assume that the filecontrol block  and the index block  in the case of indexed allocation  is already in memory calculate how many disk i/0 operations are required for contiguous  linked  and indexed  single-level  allocation strategies  if  for one block  the following conditions hold in the contiguous-allocation case  assume that there is no room to grow at the beginning but there is room to grow at the end also assume that the block information to be added is stored in memory  a the block is added at the beginning  b the block is added in the middle  c the block is added at the end  d the block is removed from the beginning  e the block is removed from the middle  f the block is removed from the end  11.6 consider a file system that uses inodes to represent files disk blocks are 8 kb in size  and a pointer to a disk block requires 4 bytes this file system has 12 direct disk blocks  as well as single  double  and triple indirect disk blocks what is the maximum size of a file that can be stored in this file system 11.7 assume that in a particular augmentation of a reinote-file-access protocol  each client maintains a name cache that caches translations from file names to corresponding file handles what issues should we take into account in implementing the name cache 11.8 consider the following backup scheme  day 1 copy to a backup medium all files from the disk  day 2 copy to another m.edium all files changed since day 1  day 3 copy to another medium all files changed since day 1  501 this differs from the schedule given in section 11.7.4 by having all subsequent backups copy all files modified since the first full backup  what are the benefits of this system over the one in section 11.7.4 what are the drawbacks are restore operations made easier or more difficult explain your answer  11.9 why must the bit map for file allocation be kept on mass storage  rather than in main memory 11.10 consider a file system on a disk that has both logical and physical block sizes of 512 bytes assume that the information about each file is already in memory for each of the three allocation strategies  contiguous  linked  and indexed   answer these questions  a how is the logical-to-physical address mapping accomplished in this system  for the indexed allocation  assume that a file is always less than 512 blocks long  b if we are currently at logical block 10  the last block accessed was block 10  and want to access logical block 4  how many physical blocks must be read from the disk 11.11 why is it advantageous to the user for an operating system to dynamically allocate its internal tables what are the penalties to the operating system for doing so 11.12 explain why logging metadata updates ensures recovery of a file system after a file-system crash  11.13 fragmentation on a storage device can be eliminated by recompaction of the information typical disk devices do not have relocation or base registers  such as those used when memory is to be compacted   so how can we relocate files give three reasons why recompacting and relocation of files are often avoided  11.14 consider a system where free space is kept in a free-space list  a suppose that the pointer to the free-space list is lost can the system reconstruct the free-space list explain your answer  b consider a file system similar to the one used by unix with indexed allocation how many disk i/0 operations might be 502 chapter 11 required to read the contents of a small local file at /a/b/c assume that none of the disk blocks is currently being cached  c suggest a scheme to ensure that the pointer is never lost as a result of memory failure  11.15 one problem with contiguous allocation is that the user must preallocate enough space for each file if the file grows to be larger than the space allocated for it  special actions must be taken one solution to this problem is to define a file structure consisting of an initial contiguous area  of a specified size   if this area is filled  the operating system automatically defines an overflow area that is linked to the initial contiguous area if the overflow area is filled  another overflow area is allocated compare this implementation of a file with the standard contiguous and linked implementations  11.16 discuss how performance optimizations for file systems might result in difficulties in maintaining the consistency of the systems in the event of com.puter crashes  the ms-dos fat system is explained in norton and wilton  1988   and the os/2 description can be found in iacobucci  1988   these operating systems use the intel 8086 cpus  intel  1985b   intel  1985a   intel  1986   and intel  1990    ibm allocation methods are described in deitel  1990   the internals of the bsd unl ' system are covered in full in mckusick et al  1996   mcvoy and kleiman  1991  discusses optimizations of these methods made in solaris the coogle file system is described in ghemawat et al  2003   fuse can be found at http  / /fuse.sourceforge.net/  disk file allocation based on the buddy system is covered in koch  1987   a file-organization scheme that guarantees retrieval in one access is described by larson and kajla  1984   log-structured file organizations for enhancing both performance and consistency are discussed in rosenblum and ousterhout  1991   seltzer et al  1993   and seltzer et al  1995   algorithms such as balanced trees  and much more  are covered by knuth  1998  and carmen et al  2001   the zfs source code for space maps can be found at http  //src.opensolaris.org/source/xref/onnv/onnvgate/ usr i src/uts/ common/ fs/ zfs/ space_map.c  disk caching is discussed by mckeon  1985  and smith  1985   caching in the experimental sprite operating system is described in nelson et al  1988   general discussions concerning mass-storage technology are offered by chi  1982  and hoagland  1985   folk and zoellick  1987  covers the gamut of file structures silvers  2000  discusses implementing the page cache in the netbsd operating system  the network file system  nfs  is discussed in sandberg et al  1985   sandberg  1987   sun  1990   and callaghan  2000   nfs version 4 is a standard described at http  / /www.ietf.org/rfc/rfc3530.txt the characteristics of 503 workloads in distributed file systems are examined in baker et al  1991   ousterhout  1991  discusses the role of distributed state in networked file systems log-structured designs for networked file systems are proposed in hartman and ousterhout  1995  and thekkath et al  1997   nfs and the unix file system  ufs  are described in vahalia  1996  and mauro and mcdougall  2007   the windows nt file system  ntfs  is explained in solomon  1998   the ext2 file system used in linux is described in bovet and cesati  2002  and the wafl file system in hitz et al  1995   zfs documentation can be found at http  / /www.opensolaris.org/ os/ community /zfs/ docs  12.1 the file system can be viewed logically as consisting of three parts in chapter 10  we examined the user and programmer interface to the file system in chapter 11  we described the internal data structures and algorithms used by the operating system to implement this interface in this chapter  we discuss the lowest level of the file system  the secondary and tertiary storage structures we first describe the physical structure of magenetic disks and magnetic tapes we then describe disk-scheduling algorithms  which schedule the order of disk i/ os to improve performance next  we discuss disk formatting and management of boot blocks  damaged blocks  and swap space we then examine secondary storage structure  covering disk reliability and stablestorage implementation we conclude with a brief description of tertiary storage devices and the problems that arise when an operating system uses tertiary storage  to describe the physical structure of secondary and tertiary storage devices and its effects on the uses of the devices  to explain the performance characteristics of mass-storage devices  to discuss operating-system services provided for mass storage  including raid and hsm  in this section  we present a general overview of the physical structure of secondary and tertiary storage devices  12.1.1 magnetic disks provide the bulk of secondary storage for modern computer systems conceptually  disks are relatively simple  figure 12.1   each disk platter has a flat circular shape  like a cd common platter diameters range 505 506 chapter 12 arm assembly rotation figure 12.1 moving-head disk mechanism  from 1.8 to 5.25 inches the two surfaces of a platter are covered with a magnetic material we store information by recording it magnetically on the platters  a read -write head flies just above each surface of every platter the heads are attached to a that moves all the heads as a unit the surface of a platter is logically divided into circular which are subdivided into the set of tracks that are at one arm position makes up a there may be thousands of concentric cylinders in a disk drive  and each track may contain hundreds of sectors the storage capacity of common disk drives is measured iil gigabytes  when the disk is in use  a drive motor spins it at high speed most drives rotate 60 to 200 times per second disk speed has two parts the is the rate at which data flow between the drive and the computer the sometimes called the consists of the time necessary to move the disk arm to the desired cylinder  called the and the time necessary for the desired sector to rotate to the disk head  called the typical disks can transfer several megabytes of data per second  and they seek times and rotational latencies of several milliseconds  because the disk head flies on an extremely thin cushion of air  measured in microns   there is a danger that the head will make contact with the disk surface although the disk platters are coated with a thin protective laye1 ~ the head will sometimes damage the magnetic surface this accident is called a a head crash normally can not be repaired ; the entire disk must be replaced  a disk can be allowing different disks to be mounted as needed  removable magnetic disks generally consist of one platter  held in a plastic case to prevent damage while not in the disk drive are inexpensive removable magnetic disks that have a soft plastic case containing a flexible platter the head of a floppy-disk drive generally sits directly on the disk 12.1 507 disk transfer rates as with many aspects of computingf published performance numbers for disks are not the same as real-world performance numbers stated transfer rates are always lower than for example the transfer rate may be the rate at which bits can be read from the magnetic media by the disk head  but that is different from the rate at which blocks are delivered to the operating system  surface  so the drive is designed to rotate more slowly than a hard-disk drive to reduce the wear on the disk surface the storage capacity of a floppy disk is typically only 1.44mb or so removable disks are available that work much like normal hard disks and have capacities measured in gigabytes  a disk drive is attached to a computer by a set of wires called an several kinds of buses are available  including buses the data transfers on a bus are carried out by special electronic processors called the is the controller at the computer end of the bus a is built into each disk drive to perform a disk i/0 operation  the computer places a command into the host controller  typically using memory-mapped i/0 portsf as described in section 9.7.3 the host controller then sends the command via messages to the disk controller  and the disk controller operates the disk-drive hardware to carry out the command disk controllers usually have a built-in cache data transfer at the disk drive happens between the cache and the disk surface  and data transfer to the host  at fast electronic speeds  occurs between the cache and the host controller  12.1.2 magnetic tapes was used as an early secondary-storage medium although it is relatively permanent and can hold large quantities of dataf its access time is slow compared with that of main memory and magnetic disk in addition  random access to magnetic tape is about a thousand times slower than random access to magnetic disk  so tapes are not very useful for secondary storage  tapes are used mainly for backup  for storage of infrequently used information  and as a medium for transferring information from one system to another  a tape is kept in a spool and is wound or rewound past a read-write head  moving to the correct spot on a tape can take minutes  but once positioned  tape drives can write data at speeds comparable to disk drives tape capacities vary greatly  depending on the particular kind of tape drive typically  they store from 20gb to 200gb some have built-in compression that can more than double the effective storage tapes and their drivers are usually categorized by width  includil1.g 4  8f and 19 millimeters and 1/4 and 1/2 inch some are named according to technology  such as lt0-2 and sdlt tape storage is further described in section 12.9  508 chapter 12 12.2 fire wire refers to an interface designed for connecting peripheral devices such as hard drives  dvd drives  and digital video cameras to a computer system fire wire was first developed by apple computer and became the ieee 1394 standard in 1995 the originalfirewire standard provided bandwidth up to 400 megabits per second recently  a new standardfirewire 2-has emerged and is identified by the ieee 1394b standard  firewire 2 provides double the data rate of the original firewire-800 megabits per second  modern disk drives are addressed as large one-dimensional arrays of where the logical block is the smallest unit of transfer the size of a logical block is usually 512 bytes  although some disks can be to have a different logical block size  such as 1,024 bytes this option is described in section 12.5.1 the one-dimensional array of logical blocks is mapped onto the sectors of the disk sequentially sector 0 is the first sector of the first track on the outermost cylinder the mapping proceeds in order through that track  then through the rest of the tracks in that cylinder  and then through the rest of the cylinders from outermost to innermost  by using this mapping  we can -at least in theory-convert a logical block number into an old-style disk address that consists of a cylinder number  a track number within that cylinder  and a sector number within that track in practice  it is difficult to perform this translation  for two reasons first  most disks have some defective sectors  but the mapping hides this by substituting spare sectors from elsewhere on the disk second  the number of sectors per track is not a constant on smne drives  let 's look more closely at the second reason on media that use the density of bits per track is uniform the farther a track is from the center of the disk  the greater its length  so the more sectors it can hold as we move from outer zones to inner zones  the number of sectors per track decreases tracks in the outermost zone typically hold 40 percent more sectors than do tracks in the innermost zone the drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head this method is used in cd-rom and dvd-rom drives alternatively  the disk rotation speed can stay constant ; in this case  the density of bits decreases from inner tracks to outer tracks to keep the data rate constant this method is used in hard disks and is known as the number of sectors per track has been increasing as disk technology improves  and the outer zone of a disk usually has several hundred sectors per track similarly  the number of cylinders per disk has been increasing ; large disks have tens of thousands of cylinders  12.3 12.3 509 computers access disk storage in two ways one way is via i/o ports  or this is common on small systems the other way is via a remote host in a distributed file system ; this is referred to as 12.3.1 host-attached storage host-attached storage is storage accessed through local i/0 ports these ports use several technologies the typical desktop pc uses an i/0 bus architecture called ide or ata this architecture supports a maximum of two drives per i/0 bus a newer  similar protocol that has simplified cabling is sata high-end workstations and servers generally use more sophisticated i/0 architectures  such as scsi and fiber charmel  fc   scsi is a bus architecture its physical medium is usually a ribbon cable with a large number of conductors  typically 50 or 68   the scsi protocol supports a maximum of 16 devices per bus generally  the devices include one controller card in the host  the and up to 15 storage devices  the to.rgr    ts   a scsi disk is a common scsi target  but the protocol provides the ability to address up to 8 in each scsi target a typical use of logical unit addressing is to commands to components of a raid array or components of a removable media library  such as a cd jukebox sendil g commands to the media-changer mechanism or to one of the drives   fc is a high-speed serial architecture that can operate over optical fiber or over a four-conductor copper cable it has two variants one is a large switched fabric having a 24-bit address space this variant is expected to dominate in the future and is the basis of  sjld '  ; s   discussed in section 12.3.3 because of the large space and the switched nature of the communication  multiple hosts and storage devices can attach to the fabric  allowing great flexibility in i/0 communication the other fc variant is an that can address 126 devices  drives and controllers   a wide variety of storage devices are suitable for use as host-attached storage among these are hard disk drives  raid arrays  and cd  dvd  and tape drives the i/0 commands that initiate data transfers to a host-attached storage device are reads and writes of logical data blocks directed to specifically identified storage units  such as bus id  scsi id  and target logical unit   12.3.2 network-attached storage a network-attached storage  nas  device is a special-purpose storage system that is accessed remotely over a data network  figure 12.2   clients access network-attached storage via a remote-procedure-call interface such as nfs for unix systems or cifs for windows machines the remote procedure calls  rpcs  are carried via tcp or udp over an ip network-usually the same local-area network  lan  that carries all data traffic to the clients the networkattached storage unit is usually implemented as a raid array with software that implements the rpc interface it is easiest to thil k of nas as simply another storage-access protocol for example  rather than using a scsi device driver and scsi protocols to access storage  a system using nas would use rpc over tcp /ip  510 chapter 12 12.4 lan/wan figure 12.2 network-attached storage  network-attached storage provides a convenient way for all the computers on a lan to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage however  it tends to be less efficient and have lower performance than some direct-attached storage options  is the latest network-attached storage protocol in essence  it uses the ip network protocol to carry the scsi protocol thus  networks-rather than scsi cables-can be used as the interconnects between hosts and their storage  as a result  hosts can treat their storage as if it were directly attached  even if the storage is distant from the host  12.3.3 storage-area network one drawback of network-attached storage systems is that the storage i/o operations consume bandwidth on the data network  thereby increasing the latency of network communication this problem can be particularly acute in large client-server installations-the communication between servers and clients competes for bandwidth with the communication among servers and storage devices  a storage-area network  san  is a private network  using storage protocols rather than networking protocols  connecting servers and storage units  as shown in figure 12.3 the power of a san lies in its flexibility multiple hosts and multiple storage arrays can attach to the same san  and storage can be dynamically allocated to hosts a san switch allows or prohibits access between the hosts and the storage as one example  if a host is running low on disk space  the san can be configured to allocate more storage to that host  sans make it possible for clusters of servers to share the same storage and for storage arrays to include multiple direct host com1.ections sans typically have more ports  and less expensive ports  than storage arrays  fc is the most common san interconnect  although the simplicity of iscsi is increasing its use an emerging alternative is a special-purpose bus architecture named infiniband  which provides hardware and software support for highspeed interconnection networks for servers and storage units  one of the responsibilities of the operating system is to use the hardware efficiently for the disk drives  meeting this responsibility entails having 12.4 511 figure 12.3 storage-area network  fast access time and large disk bandwidth the access time has two major components  also see section 12.1.1   the is the time for the disk arm to move the heads to the cylinder containing the desired sector the is the additional time for the disk to rotate the desired sector to the disk head the disk is the total number of bytes transferred  divided by the total time between the first request for service and the completion of the last transfer we can improve both the access time and the bandwidth by managing the order in which disk i/o requests are serviced  whenever a process needs i/0 to or from the disk  it issues a system call to the operating system the request specifies several pieces of information  whether this operation is input or output what the disk address for the transfer is what the memory address for the transfer is what the number of sectors to be transferred is if the desired disk drive and controller are available  the request can be serviced immediately if the drive or controller is busy  any new requests for service will be placed in the queue of pending requests for that drive  for a multiprogramming system with many processes  the disk queue may often have several pending requests thus  when one request is completed  the operating system chooses which pending request to service next how does the operating system make this choice any one of several disk-scheduling algorithms can be used  and we discuss them next  12.4.1 fcfs scheduling the simplest form of disk scheduling is  of course  the first-come  first-served  fcfs  algorithm this algorithm is intrinsically fair  but it generally does not provide the fastest service consider  for example  a disk queue with requests for i/0 to blocks on cylinders 98  183  37  122  14  124  65  67  512 chapter 12 queue = 98  183,37,122  14,124,65,67 head starts at 53 0 14 37 536567 98 122124 figure 12.4 fcfs disk scheduling  183199 in that order if the disk head is initially at cylinder 53  it will first move from 53 to 98  then to 183  37  122  14  124  65  and finally to 67  for a total head movement of 640 cylinders this schedule is diagrammed in figure 12.4  the wild swing from 122 to 14 and then back to 124 illustrates the problem with this schedule if the requests for cylinders 37 and 14 could be serviced together  before or after the requests for 122 and 124  the total head movement could be decreased substantially  and performance could be thereby improved  12.4.2 sstf scheduling it seems reasonable to service all the requests close to the current head position before moving the head far to service other this assumption is the basis for the the sstf algorithm selects the request with the least seek time from the current head position  since seek time increases with the number of cylinders traversed by the head  sstf chooses the pending request closest to the current head position  for our example request queue  the closest request to the initial head position  53  is at cylinder 65 once we are at cylinder 65  the next closest request is at cylinder 67 from there  the request at cylinder 37 is closer than the one at 98  so 37 is served next continuing  we service the request at cylinder 14  then 98  122  124  and finally 183  figure 12.5   this scheduling method results in a total head movement of only 236 cylinders-little more than one-third of the distance needed for fcfs scheduling of this request queue clearly  this algorithm gives a substantial improvement in performance  sstf scheduling is essentially a form of shortest-job-first  sjf  scheduling ; and like sjf scheduling  it may cause starvation of some requests remember that requests may arrive at any time suppose that we have two requests in the queue  for cylinders 14 and 186  and while the request from 14 is being serviced  a new request near 14 arrives this new request will be serviced next  making the request at 186 wait while this request is being serviced  another request close to 14 could arrive in theory  a continual stream of requests near one another could cause the request for cylinder 186 to wait indefinitely  12.4 queue = 98  183  37  122  14  124  65  67 head starts at 53 0 14 37 536567 98 122124 figure 12.5 sstf disk scheduling  513 183199 this scenario becomes increasingly likely as the pending-request queue grows longer  although the sstf algorithm is a substantial improvement over the fcfs algorithm  it is not optimal in the example  we can do better by moving the head from 53 to 37  even though the latter is not closest  and then to 14  before turning around to service 65  67  98  122  124  and 183 this strategy reduces the total head movement to 208 cylinders  12.4.3 scan scheduling in the toward the end  servicing requests as it reaches each cylinder  until it gets to the other end of the disk at the other end  the direction of head movement is reversed  and servicing continues the head continuously scans back and forth across the disk the scan algorithm is sometimes called the since the disk arm behaves just like an elevator in a building  first servicing all the requests going up and then reversing to service requests the other way  let 's return to our example to illustrate before applying scan to schedule the requests on cylinders 98  183,37  122  14  124  65  and 67  we need to know the direction of head movement in addition to the head 's current position  assuming that the disk arm is moving toward 0 and that the initial head position is again 53  the head will next service 37 and then 14 at cylinder 0  the arm will reverse and will move toward the other end of the disk  servicil lg the requests at 65  67  98  122  124  and 183  figure 12.6   if a request arrives in the queue just in front of the head  it will be serviced almost immediately ; a request arriving just behind the head will have to wait until the arm moves to the end of the disk  reverses direction  and comes back  assuming a uniform distribution of requests for cylinders  consider the density of requests when the head reaches one end and reverses direction at this point  relatively few requests are immediately in front of the head  since these cylinders have recently been serviced the heaviest density of requests 514 chapter 12 queue = 98  183,37,122  14,124,65,67 head starts at 53 0 14 37 536567 98 122124 figure 12.6 scan disk scheduling  183199 is at the other end of the disk these requests have also waited the longest so why not go there first that is the idea of the next algorithm  12.4.4 c-scan scheduling is a variant of scan designed to provide a more uniform wait time like scan  c-scan moves the head from one end of the disk to the other  servicing requests along the way when the head reaches the other end  however  it immediately returns to the beginning of the disk without servicing any requests on the return trip  figure 12.7   the c-scan scheduling algorithm essentially treats the cylinders as a circular list that wraps around from the final cylinder to the first one  queue = 98  183  37  122  14  124  65  67 head starts at 53 0 1 4 37 53 65 67 98 1 22 1 24 figure 12.7 c-scan disk scheduling  183199 12.4 queue = 98  183  37  122  14  124  65  67 head starts at 53 0 14 37 536567 98 122124 figure 12.8 c-look disk scheduling  12.4.5 look scheduling 515 183199 as we described themf both scan and c-scan move the disk arm across the full width of the disk in practicef neither algorithm is often implemented this way more commonlyf the arm goes only as far as the final request in each direction then  it reverses direction immediatelyf without going all the way to the end of the disk versions of scan and c-scan that follow this pattern are called and because they look for a request before continuing to move in a given direction  figure 12.8   12.4.6 selection of a disk-scheduling algorithm given so many disk-scheduling algorithmsf how do we choose the best one sstf is common and has a natural appeal because it increases performance over fcfs scan and c-scan perform better for systems that place a heavy load on the diskf because they are less likely to cause a starvation problem for any particular list of requestsf we can define an optimal order of retrievat but the computation needed to find an optimal schedule may not justify the savings over sstf or scan with any scheduling algoritlunf howeverf performance depends heavily on the number and types of requests for instance  suppose that the queue usually has just one outstanding request thenf all scheduling algorithms behave the samef because they have only one choice of where to move the disk head  they all behave like fcfs scheduling  requests for disk service can be greatly influenced by the file-allocation method a program reading a contiguously allocated file will generate several requests that are close together on the disk  resulting in limited head movement  a linked or indexed fik in contrastf may include blocks that are widely scattered on the diskf resulting in greater head movement  the location of directories and index blocks is also important since every file must be opened to be usedf and opening a file requires searching the directory structuref the directories will be accessed frequently suppose that a directory entry is on the first cylinder and a filef s data are on the final cylinder in this casef the disk head has to move the entire width of the disk if the directory 516 chapter 12 12.5 entry were on the middle cylinder  the head would have to move only one-half the width caching the directories and index blocks in main memory can also help to reduce disk-arm movement particularly for read requests  because of these complexities  the disk-scheduling algorithm should be written as a separate module of the operating system  so that it can be replaced with a different algorithm if necessary either sstf or look is a reasonable choice for the default algorithm  the scheduling algorithms described here consider only the seek distances  for modern disks  the rotational latency can be nearly as large as the average seek time it is difficult for the operating system to schedule for improved rotational latency  though  because modern disks do not disclose the physical location of logical blocks disk manufacturers have been alleviating this problem by implementing disk-scheduling algorithms in the controller hardware built into the disk drive if the operating system sends a batch of requests to the controller  the controller can queue them and then schedule them to improve both the seek time and the rotational latency  if i/o performance were the only consideration  the operating system would gladly turn over the responsibility of disk scheduling to the disk hardware  in practice  however  the operating system may have other constraints on the service order for requests for instance  demand paging may take priority over application i/0  and writes are more urgent than reads if the cache is running out of free pages also  it may be desirable to guarantee the order of a set of disk writes to make the file system robust in the face of system crashes  consider what could happen if the operating system allocated a disk page to a file and the application wrote data into that page before the operating system had a chance to flush the modified inode and free-space list back to disk to accommodate such requirements  an operating system may choose to do its own disk scheduling and to spoon-feed the requests to the disk controller  one by one  for some types of i/0  the operating system is responsible for several other aspects of disk management  too here we discuss disk initialization  booting from disk  and bad-block recovery  12.5.1 disk formatting a new magnetic disk is a blank slate  it is just a platter of a magnetic recording material before a disk can store data  it must be divided into sectors that the disk controller can read and write this process is called or low-level formatting fills the disk with a special data structure for each sector the data structure for a sector typically consists of a header  a data area  usually 512 bytes in size   and a trailer the header and trailer contain information used by the disk controller  such as a sector number and an  when the controller writes a sector of data during normal i/0  the ecc is updated with a value calculated from all the bytes in the data area when the sector is read  the ecc is recalculated and compared with the stored value if the stored and calculated numbers are different  this 12.5 517 mismatch indicates that the data area of the sector has become corrupted and that the disk sector may be bad  section 12.5.3   the ecc is an error-correcting code because it contains enough information  if only a few bits of data have been corrupted  to enable the controller to identify which bits have changed and calculate what their correct values should be it then reports a recoverable  the controller automatically does the ecc processing whenever a sector is read or written  most hard disks are low-level-formatted at the factory as a part of the manufacturing process this formatting enables the manufacturer to test the disk and to initialize the mapping from logical block numbers to defect-free sectors on the disk for many hard disks  when the disk controller is instructed to low-level-format the disk  it can also be told how many bytes of data space to leave between the header and trailer of all sectors it is usually possible to choose among a few sizes  such as 256,512  and 1,024 bytes formatting a disk with a larger sector size means that fewer sectors can fit on each track ; but it also means that fewer headers and trailers are written on each track and more space is available for user data some operating systems can handle only a sector size of 512 bytes  before it can use a disk to hold files  the operating system still needs to record its own data structures on the disk it does so in two steps the first step is to the disk into one or more groups of cylinders the operatiltg system can treat each partition as though it were a separate disk for instance  one partition can hold a copy of the operating system 's executable code  while another holds user files the second step is icgicz ; i or creation of a file system in this step  the operating system stores the iltitial file-system data structures onto the disk these data structures may include maps of free and allocated space  a fat or inodes  and an initial empty directory  to increase efficiency  most file systems group blocks together into larger chunks  frequently called disk i/0 is done via blocks  but file system ii 0 is done via clusters  effectively assuring that ii 0 has more sequential-access and fewer random-access characteristics  some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks  without any file-system data structures this array is sometimes called the raw disk  and ii 0 to this array is termed raw l/0 for example  some database systems prefer raw iio because it enables them to control the exact disk location where each database record is stored raw l/0 bypasses all the file-system services  such as the buffer cache  file locking  prefetching  space allocation  file names  and directories we can make certain applications more efficient by allowing them to implement their own special-purpose storage services on a raw partition  but most applications perform better when they use the regular file-system services  12.5.2 boot block for a computer to start running-for instance  when it is powered up or rebooted -it must have an initial program to run this initial bootstrap program tends to be simple it initializes all aspects of the system  from cpu registers to device controllers and the contents of main memory  and then starts the operating system to do its job  the bootstrap program finds the 518 chapter 12 operating-system kernel on disk  loads that kernel into memory  and jumps to an initial address to begin the operating-system execution  for most computers  the bootstrap is stored in this location is convenient  because rom needs no initialization and is at a fixed location that the processor can start executing when powered up or reset and  since rom is read only  it can not be infected by a computer virus the problem is that changing this bootstrap code requires changing the rom hardware chips  for this reason  most systems store a tiny bootstrap loader program in the boot rom whose only job is to bring in a full bootstrap program from disk the full bootstrap program can be changed easily  a new version is simply written onto the disk the full bootstrap program is stored in the boot blocks at a fixed location on the disk a disk that has a boot partition is called a or the code in the boot rom instructs the disk controller to read the boot blocks into memory  no device drivers are loaded at this point  and then starts executing that code the full bootstrap program is more sophisticated than the bootstrap loader in the boot rom ; it is able to load the entire operating system from a non-fixed location on disk and to start the operating system ruru1ing  even so  the full bootstrap code may be small  let 's consider as an example the boot process in windows 2000 the windows 2000 system places its boot code in the first sector on the hard disk  which it terms the or furthermore  windows 2000 allows a hard disk to be divided into one or more partitions ; one partition  identified as the contains the operating system and device drivers bootil1g begins in a windows 2000 system by running code that is resident in the system 's rom memory this code directs the system to read the boot code from the mbr in addition to containing boot code  the mbr contains a table listing the partitions for the hard disk and a flag indicating which partition the system is to be booted from  as illustrated in figure 12.9  once the system identifies the boot partition  it reads the first sector from that partition  which is called the and contilmes with the remainder of the boot process  which includes loading the various subsystems and system services  mbr partition 1 partition 2 partition 3 partition 4 boot code partition table boot partition figure 12.9 booting from disk in windows 2000  12.5 519 12.5.3 bad blocks because disks have moving parts and small tolerances  recall that the disk head flies just above the disk surface   they are prone to failure sometimes the failure is complete ; in this case  the disk needs to be replaced and its contents restored from backup media to the new disk more frequently  one or more sectors become defective most disks even con'le from the factory with depending on the disk and controller in use  these blocks are handled in a variety of ways  on simple disks  such as some disks with ide controllers  bad blocks are handled manually for instance  the ms-dos format command performs logical formatting and  as a part of the process  scans the disk to find bad blocks if format finds a bad block  it writes a special value into the corresponding fat entry to tell the allocation routines not to use that block if blocks go bad during normal operation  a special program  such as chkdsk  must be run manually to search for the bad blocks and to lock them away data that resided on the bad blocks usually are lost  more sophisticated disks  such as the scsi disks used in high-end pcs and most workstations and servers  are smarter about bad-block recovery the controller maintains a list of bad blocks on the disk the list is initialized during the low-level formatting at the factory and is updated over the life of the disk  low-level formatting also sets aside spare sectors not visible to the operating system the controller can be told to replace each bad sector logically with one of the spare sectors this scheme is known as or a typical bad-sector transaction might be as follows  the operating system tries to read logical block 87  the controller calculates the ecc and finds that the sector is bad it reports this finding to the operating system  the next time the system is rebooted  a special command is run to tell the scsi controller to replace the bad sector with a spare  after that  whenever the system requests logical block 87  the request is translated into the replacement sector 's address by the controller  note that such a redirection by the controller could invalidate any optimization by the operating system 's disk-scheduling algorithm ! for this reason  most disks are formatted to provide a few spare sectors in each cylinder and a spare cylinder as well when a bad block is remapped  the controller uses a spare sector from the same cylinder  if possible  as an alternative to sector some controllers can be instructed to replace a bad block by here is an example  suppose that logical block 17 becomes defective and the first available spare follows sector 202 then  sector slipping remaps all the sectors front 17 to 202  moving them all down one spot that is  sector 202 is copied into the spare  then sector 201 into 202  then 200 into 201  and so on  until sector 18 is copied into sector 19  slipping the sectors in this way frees up the space of sector 18  so sector 17 can be mapped to it  the replacement of a bad block generally is not totally automatic because the data in the bad block are usually lost soft errors may trigger a process in 520 chapter 12 12.6 which a copy of the block data is made and the block is spared or slipped  an unrecoverable howeverf results in lost data whatever file was using th.at block must be repaired  for instancef by restoration from a backup tape  f and that requires manual intervention  swapping was first presented in section 8.2f where we discussed moving entire processes between disk and main memory swapping in that setting occurs when the amount of physical memory reaches a critically low point and processes are moved from memory to swap space to free available memory  in practicef very few modern operating systems implement swapping in this fashion rathel ~ systems now combine swapping with virtual memory techniques  chapter 9  and swap pagesf not necessarily entire processes in fact some systems now use the terms swapping and paging interchangeablyf reflecting the merging of these two concepts  is another low-level task of the operating system virtual memory uses disk space as an extension of main memory  since disk access is much slower than memory accessf using swap space significantly decreases system performance the main goal for the design and implementation of swap space is to provide the best throughput for the virtual memory system in this sectionf we discuss how swap space is usedf where swap space is located on diskf and how swap space is managed  12.6.1 swap-space use swap space is used in various ways by different operating systemsf depending on the memory-management algorithms in use for instancef systems that implement swapping may use swap space to hold an entire process imagef including the code and data segments paging systems may simply store pages that have been pushed out of main memory the amount of swap space needed on a system can therefore vary from a few megabytes of disk space to gigabytesf depending on the amow1.t of physical memoryf the amount of virtual memory it is backingf and the way in which the virtual memory is used  note that it may be safer to overestimate than to underestimate the amount of swap space requiredf because if a system runs out of swap space it may be forced to abort processes or may crash entirely overestimation wastes disk space that could otherwise be used for filesf but it does no other harm some systems recommend the amount to be set aside for swap space solarisf for examplef suggests setting swap space equal to the amount by which virtual memory exceeds pageable physical memory in the past linux has suggested setting swap space to double the amount of physical memoryf although most linux systems now use considerably less swap space in factf there is currently much debate in the linux community about whether to set aside swap space at all ! some operating systems-including linux-allow the use of multiple swap spaces these swap spaces are usually put on separate disks so that the load placed on the i/0 system by paging and swapping can be spread over the systemfs i/o devices  12.6 521 12.6.2 swap-space location a swap space can reside in one of two places  it can be carved out of the normal file system  or it can be in a separate disk partition if the swap space is simply a large file within the file system  normal file-system routines can be used to create it  name it and allocate its space this approach  though easy to implement is inefficient navigating the directory structure and the diskallocation data structures takes time and  possibly  extra disk accesses external fragmentation can greatly increase swapping times by forcing multiple seeks during reading or writing of a process image we can improve performance by caching the block location information in physical memory and by using special tools to allocate physically contiguous blocks for the swap file  but the cost of traversing the file-system data structures remains  alternatively  swap space can be created in a separate partition no file system or directory structure is placed in this space rather  a separate swap-space storage manager is used to allocate and deallocate the blocks from the raw partition this manager uses algorithms optimized for speed rather than for storage efficiency  because swap space is accessed much more frequently than file systems  when it is used   internal fragmentation may increase  but this trade-off is acceptable because the life of data in the swap space generally is much shorter than that of files in the file system since swap space is reinitialized at boot time  any fragmentation is short-lived the raw-partition approach creates a fixed amount of swap space during disk partitioning adding more swap space requires either repartitioning the disk  which involves moving the other file-system partitions or destroying them and restoring them from backup  or adding another swap space elsewhere  some operating systems are flexible and can swap both in raw partitions and in file-system space linux is an example  the policy and implementation are separate  allowing the machine 's administrator to decide which type of swapping to use the trade-off is between the convenience of allocation and management in the file system and the performance of swapping in raw partitions  12.6.3 swap-space management  an example we can illustrate how swap space is used by following the evolution of swapping and paging in various unix systems the traditional unix kernel started with an implementation of swapping that copied entire processes between contiguous disk regions and memory unix later evolved to a combination of swapping and paging as pagiltg hardware became available  in solaris 1  sunos   the designers changed standard unix methods to improve efficiency and reflect technological developments when a process executes  text-segment pages containing code are brought in from the file system  accessed in main memory  and thrown away if selected for pageout it is more efficient to reread a page from the file system than to write it to swap space and then reread it from there swap space is only used as a backing store for pages of memory  which includes memory allocated for the stack  heap  and uninitialized data of a process  more changes were made in later versions of solaris the biggest change is that solaris now allocates swap space only when a page is forced out of physical memory  rather than when the virtual memory page is first created  522 chapter 12 12.7 swap partition or swap file swap map 1---------swap area--------1 page i slot -1 l ~  ~ ---_l __ _l _ ~ figure 12.10 the data structures for swapping on linux systems  this scheme gives better performance on modern computers  which have more physical memory than older systems and tend to page less  linux is similar to solaris in that swap space is only used for anonymous memory or for regions of memory shared by several processes linux allows one or more swap areas to be established a swap area may be in either a swap file on a regular file system or a raw-swap-space partition each swap area consists of a series of 4-kb which are used to hold swapped pages  associated with each swap area is a .u1.2.p-an array of integer counters  each corresponding to a page slot in the swap area if the value of a counter is 0  the corresponding page slot is available values greater than 0 indicate that the page slot is occupied by a swapped page the value of the counter ij.l.dicates the number of mappings to the swapped page ; for example  a value of 3 indicates that the swapped page is mapped to three different processes  which can occur if the swapped page is storing a region of memory shared by three processes   the data structures for swapping on linux systems are shown in figure 12.10  disk drives have continued to get smaller and cheaper  so it is now economically feasible to attach many disks to a computer system having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written  if the disks are operated in parallel furthermore  this setup offers the potential for improving the reliability of data storage  because redundant information can be stored on multiple disks thus  failure of one disk does not lead to loss of data a of disk-organization techniques  collectively called disks  raids   are commonly used to address the performance and reliability issues  in the past  raids composed of small  cheap disks were viewed as a cost-effective alternative to large  expensive disks ; today  raids are used for their higher reliability and higher data-transfer rate  rather than for economic reasons hence  the i in raid  which once stood for inexpensive/ ' now stands for ij.l.dependent  12.7.1 improvement of reliability via redundancy let us first consider the reliability of raids the chance that some disk out of a set of n disks will fail is much higher than the chance that a specific single 12.7 523 structuring raid raid storage can be structured in a variety of ways for example  a system can have disks directly attached to its buses in this case  the operating system or system software can implement raid flmctionality alternatively  an intelligent host controller can control multiple attached disks and can implement raid on those disks in hardware finally  a  or can be used a raid array is a standalone unit with its own controller  cache  usually   and disks it is attached to the host via one or more standard ata scsi or fc controllers this common setup allows any operating system and software without raid functionality to have raid-protected disks it is even used on systems that do have raid software layers because of its simplicity and flexibility  disk will fail suppose that the of a single disk is 100,000 hours then the mean time to failure of some disk in an array of 100 disks will be 100,000/100 = 1,000 hours  or 41.66 days  which is not long at all ! if we store only one copy of the data  then each disk failure will result in loss of a significant amount of data -and such a high rate of data loss is unacceptable  the solution to the problem of reliability is to introduce  we store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information thus  even if a disk fails  data are not lost  the simplest  but most expensive  approach to introducing redundancy is to duplicate every disk this technique is called with mirroring  a logical disk consists of two physical disks  and every write is carried out on both disks the result is called a mirrored volume if one of the disks in the volume fails  the data can be read from the other data will be lost only if the second disk fails before the first failed disk is replaced  the mean time to failure of a mirrored volume-where failure is the loss of data depends on two factors one is the mean time to failure of the individual disks the other is the which is the time it takes  on average  to replace a failed disk and to restore the data on it suppose that the failures of the two disks are that is  the failure of one disk is not connected to the failure of the other then  if the mean time to failure of a single disk is 100,000 hours and the mean time to repair is 10 hours  the of a mirrored disk system is 100  0002 /  2 10  = 500 106 hours  or 57,000 years ! you should be aware that the assumption of independence of disk failures is not valid power failures and natural disasters  such as earthquakes  fires  and floods  may result in damage to both disks at the same time also  manufacturing defects in a batch of disks can cause correlated failures as disks age  the probability of failure grows  increasing the chance that a second disk will fail while the first is being repaired in spite of all these considerations  however  n1.irrored-disk systems offer much higher reliability than do singledisk systems  power failures are a particular source of concern  since they occur far more frequently than do natural disasters even with mirroring of disks  if writes are 524 chapter 12 in progress to the same block in both disks  and power fails before both blocks are fully written  the two blocks can be in an inconsistent state one solution to this is to write one copy first then the next another is to add a cache to the raid array this write-back cache is protected from data loss during power failures  so the write can be considered complete at that point  assuming the nvram has some kind of error protection and correction  such as ecc or mirroring  12.7.2 improvement in performance via parallelism now let 's consider how parallel access to multiple disks improves performance  with disk mirroring  the rate at which read requests can be handled is doubled  since read requests can be sent to either disk  as long as both disks in a pair are functionat as is almost always the case   the transfer rate of each read is the same as in a single-disk system  but the number of reads per unit time has doubled  with multiple disks  we can improve the transfer rate as well  or instead  by striping data across the disks in its simplest form  consists of the bits of each byte across multiple disks ; such striping is called for example  if we have an array of eight disks  we write bit i of each byte to disk i the array of eight disks can be treated as a single disk with sectors that are eight times the normal size and  more important that have eight times the access rate in such an organization  every disk participates in every access  read or write  ; so the number of accesses that can be processed per second is about the same as on a single disk  but each access can read eight times as many data in the same time as on a single disk  bit-level striping can be generalized to include a number of disks that either is a multiple of 8 or divides 8 for example  if we use an array of four disks  bits i and 4 + i of each go to disk i further  striping need not occur at the bit level in for instance  blocks of a file are striped across multiple disks ; with n disks  block i of a file goes to disk  i mod n  + 1  other levels of striping  such as bytes of a sector or sectors of a block  also are possible block-level striping is the most common  parallelism in a disk system  as achieved through striping  has two main goals  increase the throughput of multiple small accesses  that is  page accesses  by load balancing  reduce the response time of large accesses  12.7.3 raid levels mirroring provides high reliability  but it is expensive striping provides high data-transfer rates  but it does not improve reliability numerous schemes to provide redundancy at lower cost by using disk striping combined with parity bits  which we describe next  l1.ave been proposed these schemes have different cost-performance trade-offs and are classified according to levels called we describe the various levels here ; figure 12.11 shows them pictorially  in the figure  p indicates error-correcting bits  and c 12.7 raid structure 525  a  raid 0  non-redundant striping   b  raid 1  mirrored disks   c  raid 2  memory-style error-correcting codes   d  raid 3  bit-interleaved parity   e  raid 4  block-interleaved parity   f  raid 5  block-interleaved distributed parity   g  raid 6  p + q redundancy  figure 12.11 raid levels  indicates a second copy of the data   in all cases depicted in the figure  four disks ' worth of data are stored  and the extra disks are used to store redundant information for failure recovery  raid level 0 raid level 0 refers to disk arrays with striping at the level of blocks but without any redundancy  such as mirroring or parity bits   as shown in figure 12.1l  a   raid ievell raid level1 refers to disk mirroring figure 12.1l  b  shows a mirrored organization  ' raid level2 raid level2 is also known as memory-style error-correctingcode  ecc  organization memory systems have long detected certain errors by using parity bits each byte in a memory system may have a parity bit associated with it that records whether the number of bits in the byte set to 1 is even  parity = 0  or odd  parity = 1   if one of the bits in the 526 chapter 12 byte is damaged  either a 1 becomes a 0  or a 0 becomes an the parity of the byte changes and thus does not match the stored parity similarly  if the stored parity bit is damaged  it does not match the computed parity thus  all single-bit errors are detected by the menwry system  error-correcting schemes store two or more extra bits and can reconstruct the data if a single bit is damaged the idea of ecc can be used directly in disk arrays via striping of bytes across disks for example  the first bit of each byte can be stored in disk 1  the second bit in disk 2  and so on until the eighth bit is stored in disk 8 ; the error-correction bits are stored in further disks this scheme is shown pictorially in figure 12.1l  c   where the disks labeled p store the error-correction bits if one of the disks fails  the remaining bits of the byte and the associated error-correction bits can be read from other disks and used to reconstruct the damaged data note that raid level 2 requires only three disks ' overhead for four disks of data  unlike raid level 1  which requires four disks ' overhead  raid level 3 raid level 3  or improves on level 2 by taking into account the fact that  unlike memory systems  disk controllers can detect whether a sector has been read correctly  so a single parity bit can be used for error correction as well as for detection the idea is as follows  if one of the sectors is damaged  we know exactly which sector it is  and we can figure out whether any bit in the sector is a 1 or a 0 by computing the parity of the corresponding bits from sectors in the other disks if the parity of the remaining bits is equal to the stored parity  the missing bit is 0 ; otherwise  it is 1 raid level3 is as good as level 2 but is less expensive in the number of extra disks required  it has only a one-disk overhead   so level 2 is not used in practice this scheme is shown pictorially in figure 12.1l  d   raid level 3 has two advantages over level 1 first  the storage overhead is reduced because only one parity disk is needed for several regular disks  whereas one mirror disk is needed for every disk in level1 second  since reads and writes of a byte are spread out over multiple disks with n-way striping of data  the transfer rate for reading or writing a single block is n times as fast as with raid level 1 on the negative side  raid level3 supports fewer l/os per second  since every disk has to participate in every i/0 request  a further performance problem with raid 3-and with all paritybased raid levels-is the expense of computing and writing the parity  this overhead results in significantly slower writes than with non-parity raid arrays to moderate this performance penalty  many raid storage arrays include a hardware controller with dedicated parity hardware this controller offloads the parity computation from the cpu to the array the array has an nvram cache as well  to store the blocks while the parity is computed and to buffer the writes from the controller to the spindles this combination can make parity raid almost as fast as non-parity in fact  a caching array doing parity raid can outperform a non-caching non-parity raid  raid level 4 raid level4  or uses block-level striping  as in raid 0  and in addition keeps a parity block on a separate disk for corresponding blocks from n other disks this scheme is 12.7 527 diagramed in figure 12.1l  e   if one of the disks fails  the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk  a block read accesses only one disk  allowing other requests to be processed by the other disks thus  the data-transfer rate for each access is slowe1 ~ but multiple read accesses can proceed in parallel  leading to a higher overall i/0 rate the transfer rates for large reads are high  since all the disks can be read in parallel ; large writes also have high transfer rates  since the data and parity can be written in parallel  small independent writes can not be performed in parallel an operatingsystem write of data smaller than a block requires that the block be read  modified with the new data  and written back the parity block has to be updated as well this is known as the syti  .e  thus  a single write requires four disk accesses  two to read the two old blocks and two to write the two new blocks  wafl  chapter 11  uses raid level4 because this raid level allows disks to be added to a raid set seamlessly if the added disks are initialized with blocks containing all zeros  then the parity value does not change  and the raid set is still correct  raid levels raid levels  or  differs from level 4 by spreading data and parity among all n + 1 disks  rather than storing data in n disks and parity in one disk for each block  one of the disks stores the parity  and the others store data for example  with an array of five disks  the parity for the nth block is stored in disk  n mod 5  + 1 ; the nth blocks of the other four disks store actual data for that block this setup is shown in figure 12.11  f   where the ps are distributed across all the disks a parity block can not store parity for blocks in the same disk  because a disk failure would result in loss of data as well as of parity  and hence the loss would not be recoverable by spreading the parity across all the disks in the set  raid 5 avoids potential overuse of a single parity disk  which can occur with raid 4 raid 5 is the most common parity raid system  raid level 6 raid level 6  also called the is much like raid level 5 but stores extra redundant information to guard against disk failures instead of parity  error-correcting codes such as the are used in the scheme shown in figure 12.11  g   2 bits of redundant data are stored for every 4 bits of datacompared with 1 parity bit in level 5-and the system can tolerate two disk failures  raid levels 0 + 1 and 1 + 0 raid level 0 + 1 refers to a combination of raid levels 0 and 1 raid 0 provides the performance  while raid 1 provides the reliability generally  this level provides better performance than raid 5 it is common in enviromnents where both performance and reliability are important unfortunately  like raid 1  it doubles the number of disks needed for storage  so it is also relatively expensive in raid 0 + 1  a set of disks are striped  and then the stripe is mirrored to another  equivalent stripe  528 chapter 12 stripe a  raid 0 + 1 with a single disk failure  ua mirror b  raid 1 + 0 with a single disk failure  figure 12.12 raid 0 + 1 and 1 + 0  another raid option that is becoming available commercially is raid level 1 + 0  in which disks are mirrored in pairs and then the resulti.j.l.g mirrored pairs are striped this scheme has some theoretical advantages over raid 0 + 1 for example  if a single disk fails in raid 0 + 1  an entire stripe is inaccessible  leaving only the other stripe available with a failure in raid 1 + 0  a single disk is unavailable  but the disk that mirrors it is still available  as are all the rest of the disks  figure 12.12   numerous variations have been proposed to the basic raid schemes described here as a result  some confusion may exist about the exact definitions of the different raid levels  the implementation of raid is another area of variation consider the following layers at which raid can be implemented  volume-management software can implement raid within the kernel or at the system software layer in this case  the storage hardware can provide a minimum of features and still be part of a full raid solution parity raid is fairly slow when implemented in software  so typically raid 0  1  or 0 + 1 is used  raid can be implemented in the host bus-adapter  hba  hardware only the disks directly connected to the hba can be part of a given raid set  this solution is low in cost but not very flexible  12.7 529 raid can be implemented in the hardware of the storage array the storage array can create raid sets of various levels and can even slice these sets into smaller volumes  which are then presented to the operating system  the operating system need only implement the file system on each of the volumes arrays can have multiple connections available or can be part of a san  allowing multiple hosts to take advantage of the array 's features  raid can be implemented in the san interconnect layer by disk virtualization devices in this case  a device sits between the hosts and the storage  it accepts commands from the servers and manages access to the storage  it could provide mirroring  for example  by writing each block to two separate storage devices  other features  such as and replication  can be implemented at each of these levels as well involves the automatic duplication of writes between separate sites for redundancy and disaster recovery replication can be synchronous or asynchronous in synchronous replication  each block must be written locally and remotely before the write is considered complete  whereas in asynchronous replication  the writes are grouped together and written periodically asynchronous replication can result in data loss if the primary site fails  but it is faster and has no distance limitations  the implementation of these features differs depending on the layer at which raid is implemented for example  if raid is implemented in software  then each host may need to carry out and manage its own replication if replication is implemented in the storage array or in the san intercom1ect  however  then whatever the host operating system or its features  the host 's data can be replicated  one other aspect of most raid implementations is a hot spare disk or disks  a is not used for data but is configured to be used as a replacement in case disk failure for instance  a hot spare can be used to rebuild a mirrored pair should one of the disks in the pair fail in this way  the raid level can be reestablished automatically  without waiting for the failed disk to be replaced  allocating more than one hot spare allows more than one failure to be repaired without human intervention  12.7.4 selecting a raid level given the many choices they have  how do system designers choose a raid level one consideration is rebuild performance if a disk fails  the time needed to rebuild its data can be significant this may be an important factor if a continuous supply of data is required  as it is in high-performance or interactive database systems furthermore  rebuild performance influences the mean time to failure  rebuild performance varies with the raid level used rebuilding is easiest or raid level1  since data can be copied from another disk ; for the other levels  we need to access all the other disks in the array to rebuild data in a failed disk  rebuild times can be hours for raid 5 rebuilds of large disk sets  raid level 0 is used in high-performance applications where data loss is not critical raid level1 is popular for applications that require high reliability with fast recovery raid 0 + 1 and 1 + 0 are used where both performance and reliability are important-for example  for small databases due to raid 1 's 530 chapter 12 the inserv storage array im1ovation  in an effort to provide better  faster  and less expensive solutions  frequently blurs the lines that separated previous technologies consider the inserv storage array from 3par unlike most other storage arrays  inserv does not require that a set of disks be configured at a specific raid level  rather  each disk is broken into 256-mb chunklets ram is then applied at the chunklet level a disk can thus participate in multiple and various raid levels as its chunklets are used for multiple volumes  inserv also provides snapshots similar to those created by the wafl file system the format of inserv snapshots can be read-write as well as readonly  allowing multiple hosts to mount copies of a given file system without needing their own copies of the entire file system any changes a host makes in its own copy are copy-on-write and so are not reflected in the other copies  a further innovation is  some file systems do not expand or shrink on these systems  the original size is the only size  and any change requires copying data an administrator can configure inserv to provide a host with a large amount of logical storage that initially occupies only a small amount of physical storage as the host starts using the storage  unused disks are allocated to the host  up to the original logical level the host thus can believe that it has a large fixed storage space  create its file systems there  and so on disks can be added or removed from the file system by inserv without the file systems noticing the change this feature can reduce the number of drives needed by hosts  or at least delay the purchase of disks until they are really needed  high space overhead  raid levels is often preferred for storing large volumes of data level6 is not supported currently by many raid implementations  but it should offer better reliability than levels  raid system designers and administrators of storage have to make several other decisions as well for example  how many disks should be in a given raid set how many bits should be protected by each parity bit if more disks are in an array  data-transfer rates are higher  but the system is more expensive  if more bits are protected by a parity bit  the space overhead due to parity bits is lower  but the chance that a second disk will fail before the first failed disk is repaired is greater  and that will result in data loss  12.7.5 extensions the concepts of raid have been generalized to other storage devices  including arrays of tapes  and even to the broadcast of data over wireless systems when applied to arrays of tapes  raid structures are able to recover data even if one of the tapes in an array is damaged when applied to broadcast of data  a block of data is split into short units and is broadcast along with a parity unit ; if one of the units is not received for any reason  it can be reconstructed from the other units comrnonly  tape-drive robots containing multiple tape drives will stripe data across all the drives to increase throughput and decrease backup time  12.7 531 12.7.6 problems with raid unfortunately  raid does not always assure that data are available for the operating system and its users a pointer to a file could be wrong  for example  or pointers within the file structure could be wrong incomplete writes  if not properly recovered  could result in corrupt data some other process could accidentally write over a file system 's structures  too raid protects against physical media errors  but not other hardware and software errors as large as is the landscape of software and hardware bugs  that is how numerous are the potential perils for data on a system  the solaris zfs file system takes an innovative approach to solving these problems through the use of  a technique which is used to verify the integrity of data zfs maintains internal checksums of all blocks  including data and metadata these checksums are not kept with the block that is being checksummed rathel ~ they are stored with the pointer to that block  see figure 12.13  consider an inode with pointers to its data within the inode is the checksum of each block of data if there is a problem with the data  the checksum will be incorrect and the file system will know about it if the data are mirrored  and there is a block with a correct checksum and one with an incorrect checksum  zfs will automatically update the bad block with the good one similarly  the directory entry that points to the inode has a checksum for the inode any problem in the inode is detected when the directory is accessed  this checksumming takes places throughout all zfs structures  providing a much higher level of consistency  error detection  and error correction than is found in raid disk sets or standard file systems the extra overhead that is created by the checksum calculation and extra block read-modify-write cycles is not noticeable because the overall performance of zfs is very fast  another issue with most raid implementations is lack of flexibility  consider a storage array with twenty disks divided into four sets of five disks  each set of five disks is a raid level 5 set as a result  there are four separate data 1 figure 12.13 zfs checksums all metadata and data  532 chapter 12 volumes  each holding a file system but what if one file system is too large to fit on a five-disk raid level 5 set and what if another file system needs very little space if such factors are known ahead of time  then the disks and volumes can be properly allocated very frequently  however  disk use and requirements change over time  even if the storage array allowed the entire set of twenty disks to be created as one large raid set other issues could arise several volumes of various sizes could be built on the set but some volume managers do not allow us to change a volume 's size in that case  we would be left with the same issue described above-mismatched file-system sizes some volume n lanagers allow size changes  but some file systems do not allow for file-system growth or shrinkage the volumes could change sizes  but the file systems would need to be recreated to take advantage of those changes  zfs combines file-system management and volume management into a unit providing greater functionality than the traditional separation of those functions allows disks  or partitions of disks  are gathered together via raid sets into of storage a pool can hold one or more zfs file systems the entire pool 's free space is available to all file systems within that pool zfs uses the memory model of malloc and free to allocate and release storage for each file system as blocks are used and freed within the file system as a result there are no artificial limits on storage use and no need to relocate file systems between volumes or resize volumes zfs provides quotas to limit the size of a file system and reservations to assure that a file system can grow by a specified amount  but those variables may be changed by the file system owner at any time figure 12.14  a  depicts traditional volumes and file systems  and figure 12.14  b  shows the zfs model  i fs i ~  a  traditional volumes and file systems   b  zfs and pooled storage  figure 12.14  a  traditional volumes and file systems  b  a zfs pool and file systems  12.8 12.8 533 in chapter 6  we introduced the write-ahead log  which requires the availability of stable storage by definition  information residing in stable storage is never lost to implement such storage  we need to replicate the required information on multiple storage devices  usually disks  with independent failure modes  we also need to coordinate the writing of updates in a way that guarantees that a failure during an update will not leave all the copies in a damaged state and that  when we are recovering from a failure  we can force all copies to a consistent and correct value  even if another failure occurs during the recovery  in this section  we discuss how to meet these needs  a disk write results in one of three outcomes  successful completion the data were written correctly on disk  partial failure a failure occurred in the midst of transfer  so only some of the sectors were written with the new data  and the sector being written during the failure may have been corrupted  total failure the failure occurred before the disk write started  so the previous data values on the disk remain intact  whenever a failure occurs during writing of a block  the system needs to detect it and invoke a recovery procedure to restore the block to a consistent state to do that  the system must maintain two physical blocks for each logical block an output operation is executed as follows  write the information onto the first physical block  when the first write completes successfully  write the same injormation onto the second physical block  declare the operation complete only after the second write completes successfully  during recovery from a failure  each pair of physical blocks is examined  if both are the same and no detectable error exists  then no further action is necessary if one block contains a detectable error  then we replace its contents with the value of the other block if neither block contains a detectable error  but the blocks differ in content  then we replace the content of the first block with that of the second this recovery procedure ensures that a write to stable storage either succeeds completely or results in no change  we can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage although having a large number of copies further reduces the probability of a failure  it is usually reasonable to simulate stable storage with only two copies the data in stable storage are guaranteed to be safe unless a failure destroys all the copies  because waiting for disk writes to complete  synchronous i/o  is time consuming  many storage arrays add nvram as a cache since the memory is nonvolatile  it usually has battery power to back up the unit 's power   it can be trusted to store the data en route to the disks it is thus considered part of 534 chapter 12 12.9 the stable storage writes to it are much faster than to disk  so performance is greatly improved  would you buy a dvd or cd player that had one disk sealed inside of course not you expect to use a dvd or cd player with many relatively inexpensive disks on a computer as well  using many inexpensive cartridges with one drive lowers the overall cost low cost is the defining characteristic of tertiary storage  which we discuss in this section  12.9.1 tertiary-storage devices because cost is so important  in practice  tertiary storage is built with the most common examples are floppy disks  tapes  and read-only  write-once  and rewritable cds and dvds many any other kinds of tertiarystorage devices are available as well  including removable devices that store data in flash memory and interact with the computer system via a usb interface  12.9.1.1 removable disks removable disks are one kind of tertiary storage floppy disks are an example of removable magnetic disks they are made from a thin  flexible disk coated with magnetic material and enclosed in a protective plastic case although common floppy disks can hold only about 1 mb  similar technology is used for removable magnetic disks that hold more than 1 gb removable magnetic disks can be nearly as fast as hard disks  although the recording stuface is at greater risk of from scratches  a is another kind of removable disk it records data on a rigid platter coated with magnetic material  but the recording technology is quite different from that for a magnetic disk the magneto-optic head flies much farther from the disk surface than a magnetic disk head does  and the magnetic material is covered with a thick protective layer of plastic or glass  this arrangement makes the disk much more resistant to head crashes  the magneto-optic disk drive has a coil that produces a magnetic field ; at room temperature  the field is too large and too weak to magnetize a bit on the disk to write a bit  the disk head flashes a laser beam at the disk surface the laser is aimed at a tiny spot where a bit is to be written the laser heats this spot  which makes the spot susceptible to the magnetic field now the large  weak magnetic field can record a tiny bit  the magneto-optic head is too far from the disk surface to read the data by detecting the tiny magnetic fields in the way that the head of a hard disk does  instead  the drive reads a bit using a property of laser light called the when a laser beam is bounced off of a magnetic spot  the polarization of the laser beam is rotated clockwise or counterclockwise  dependin ~ g on the orientation of the magnetic field this rotation is what the head detects to read a bit  another category of removable disk is the optical disks do not use magnetism at all instead  they use special materials that can be altered by laser light to have relatively dark or bright spots one exarnple of optical-disk 12.9 535 technology is the which is coated with a material that can freeze into either a crystalline or an amorphous state the crystalline state is more transparent  and hence a laser beam is brighter when it passes through the ltlaterial and bounces off the reflective layer the phase-change drive uses laser light at three different powers  low power to read data  medium power to erase the disk by melting and refreezing the recording medium into the crystalline state  and high power to melt the medium into the amorphous state to write to the disk the most common examples of this technology are the re-recordable cd-rw and dvd-rw  the kinds of disks just described can be used over and over they are called in contrast  can be written only once an old way to make a worm disk is to manufacture a thin aluminum film sandwiched between two glass or plastic platters to write a bit  the drive uses a laser light to burn a small hole through the aluminum this burning can not be reversed although it is possible to destroy the information on a worm disk by burning holes everywhere  it is virtually impossible to alter data on the disk  because holes can only be added  and the ecc code associated with each sector is likely to detect such additions worm disks are considered durable and reliable because the metal layer is safely encapsulated between the protective glass or plastic platters and magnetic fields can not damage the recording a newer write-once technology records on an organic polymer dye instead of an aluminum layer ; the dye absorbs laser light to form marks this technology is used in the recordable cd-r and dvd-r  read-oniv such as cd-rom and dvd-rom  come from the factory with the data prerecorded they use technology similar to that of worm disks  although the bits are pressed  not burned   and they are very durable  most removable disks are slower than their nonremovable counterparts  the writing process is slower  as are rotation and sometimes seek time  12.9.1.2 tapes magnetic tape is another type of removable medium as a general rule  a tape holds more data than an optical or magnetic disk cartridge tape drives and disk drives have similar transfer rates but random access to tape is much slower than a disk seek  because it requires a fast-forward or rewind operation that takes tens of seconds or even minutes  although a typical tape drive is more expensive than a typical disk drive  the price of a tape cartridge is lower than the price of the equivalent capacity of magnetic disks so tape is an economical medium for purposes that do not require fast random access tapes are commonly used to hold backup copies of disk data they are also used in large supercomputer centers to hold the enornwus volumes of data used in scientific research and by large commercial enterprises  large tape installations typically use robotic tape changers that move tapes between tape drives and storage slots in a tape library these mechanisms give the computer automated access to many tape cartridges  a robotic tape library can lower the overall cost of data storage a diskresident file that will not be needed for a while can be to tape  where the cost per gigabyte is lower ; if the file is needed in the future  the computer can it back into disk storage for active use a robotic tape library is 536 chapter 12 sometimes called storage  since it is between the high performance of on-line magnetic disks and the low cost of off-line tapes sitting on shelves in a storage room  12.9.1.3 future technology in the future  other storage technologies may become important sometimes old technologies are used in new ways  as economics change or the technologies evolve for example  solid-state disks  or are growing in importance and becoming more common simply described  an ssd is a disk that is used like a hard drive depending on the memory technology used  it can be volatile or nonvolatile the memory technology also affects performance nonvolatile ssds have the same characteristics as traditional hard disks but can be more reliable because they have no moving parts and faster because they have no seek time or latency in addition  they use less energy however  they are more expensive per megabyte than traditional hard disks  have lower capacity than the larger hard disks  and may have shorter life-spans than hard disks ; so their uses are limited in one example  ssds are being used in storage arrays to hold metadata which requires high-performance such as the journal of a journaling file system ssds are also being added to notebook computers to make them smaller  faster  and more energy efficient  another promising storage technology  bologt ; ;  phk uses laser light to record holographic photographs on special media we can think of a hologram as a three-dimensional array of pixels each pixel represents one bit  0 for black or 1 for white and all the pixels in a hologram are transferred in one flash of laser light  so the data transfer rate is extremely high with continued development  holographic storage may become commercially viable  another technology under active research is based on  iv ! e \ 1s   the idea is to apply the fabrication technologies that produce electronic chips to the manufacture of small datastorage machines one proposal calls for the fabrication of an array of 10,000 tiny disk heads  with a square centimeter of magnetic storage material suspended above the array when the storage material is moved lengthwise over the heads  each head accesses its own linear track of data on the material the storage material can be shifted sideways slightly to enable all the heads to access their next track although it remains to be seen whether this technology can be successful  it may provide a nonvolatile data-storage technology that is faster than magnetic disk and cheaper than semiconductor dram  whether the storage medium is a removable magnetic disk  a dvd  or a magnetic tape  the operating system needs to provide several capabilities to use removable media for data storage these capabilities are discussed in section 12.9.2  12.9.2 operating-system support two major jobs of an operating system are to manage physical devices and to present a virtual machine abstraction to applications in this chapter  we have seen that  for hard disks  the operating system provides two abstractions  one is the raw device  which is just an array of data blocks the other is a file system for a file system on a magnetic disk  the operating system queues and 12.9 537 schedules the interleaved requests from several applications now  we shall see how the operating system does its job when the storage media are removable  12.9.2.1 application interface most operating systems can handle removable disks almost exactly as they do fixed disks when a blank cartridge is inserted into the drive  or mounted   the cartridge must be formatted  and then an empty file system is generated on the disk this file system is used just like a file system on a hard disk tapes are often handled differently the operating system usually presents a tape as a raw storage medium an application does not open a file on the tape ; it opens the whole tape drive as a raw device usually  the tape drive is then reserved for the exclusive use of that application until the application exits or closes the tape device this exclusivity makes sense  because random access on a tape can take tens of seconds  or even a few minutes  so interleaving random accesses to tapes from more than one application would be likely to cause thrashing  when the tape drive is presented as a raw device  the operating system does not provide file-system services the application must decide how to use the array of blocks for instance  a program that backs up a hard disk to tape might store a list of file names and sizes at the beginning of the tape and then copy the data of the files to the tape in that order  it is easy to see the problems that can arise from this way of using tape  since every application makes up its own rules for how to organize a tape  a tape full of data can generally be used only by the program that created it  for instance  even if we know that a backup tape contains a list of file names and file sizes followed by the file data  we will still find it difficult to use the tape how exactly are the file names stored are the file sizes in binary or ascii form are the files written one per block  or are they all concatenated in one tremendously long string of bytes we do not even know the block size on the tape  because this variable is generally one that can be chosen separately for each block written  for a disk drive  the basic operations are read    write    and seek    tape drives have a different set of basic operations instead of seek    a tape drive uses the locate   operation the tape locate   operation is more precise than the disk seek   operation  because it positions the tape to a specific logical block  rather than an entire track locating to block 0 is the same as rewinding the tape  for most kinds of tape drives  it is possible to locate to any block that has been written on a tape in a partly filled tape  however  it is not possible to locate into the empty space beyond the written area  because most tape drives do not manage their physical space in the same way disk drives do for a disk drive  the sectors have a fixed size  and the formatting process must be used to place empty sectors in their final positions before any data can be written most tape drives have a variable block size  and the size of each block is detern ined on the fly  when that block is written if an area of defective tape is encountered during writing  the bad area is skipped and the block is written again this operation explains why it is not possible to locate into the empty space beyond the written area -the positions and numbers of the logical blocks have not yet been detennined  538 chapter 12 most tape drives have a read_position   operation that returns the logical block number where the tape head is currently located many tape drives also support a space   operation for relative motion so  for example  the operation space  -2  would locate backward over two logical blocks  for most kinds of tape drives  writing a block has the side effect of logically erasing everything beyond the position of the write in practice  this side effect means that most tape drives are append-only devices  because updating a block in the middle of the tape also effectively erases everything beyond that block the tape drive implements this appending by placing an end-of-tape  eot  mark after a block that is written the drive refuses to locate past the eot mark  but it is possible to locate to the eot and then start writing doing so overwrites the old eot mark and places a new one at the end of the new blocks just written  in principle  a file system can be implemented on a tape but many of the file-system data structures and algorithms would be different from those used for disks  because of the append-only property of tape  12.9.2.2 file naming another question that the operating system needs to handle is how to name files on removable media for a fixed disk  naming is not difficult on a pc  the file name consists of a drive letter followed by a path name in unix  the file name does not contain a drive letter  but the molmt table enables the operating system to discover on what drive the file is located if the disk is removable  however  knowing what drive contained the cartridge at some time in the past does not mean knowing how to find the file if every removable cartridge in the world had a different serial number  the name of a file on a removable device could be prefixed with the serial number  but to ensure that no two serial numbers are the same would require each one to be about 12 digits in length who could remember the names of her files if she had to memorize a 12-digit serial number for each one the problem becomes even more difficult when we want to write data on a removable cartridge on one computer and then use the cartridge in another computer if both machines are of the same type and have the same kind of removable drive  the only difficulty is knowing the contents and data layout on the cartridge but if the machines or drives are different  many additional problems can arise even if the drives are compatible  different computers may store bytes in different orders and may use different encodings for binary numbers and even for letters  such as ascii on pcs versus ebcdic on mainframes   today 's operating systems generally leave the name-space problem unsolved for removable media and depend on applications and users to figure out how to access and interpret the data fortunately  a few kinds of removable media are so well standardized that all computers use them the same way one example is the cd music cds use a universal format that is understood by any cd drive data cds are available in only a few different formats  so it is usual for a cd drive and the operating-system device driver to be programmed to handle all the comn1on formats dvd fonnats are also well standardized  12.9 539 12.9.2.3 hierarchical storage management a ju enables the computer to change the removable cartridge in a tape or disk drive without human assistance two major uses of this technology are for backups and hierarchical storage systems the use of a jukebox for backups is simple  when one cartridge becomes full  the computer instructs the jukebox to switch to the next cartridge some jukeboxes hold tens of drives and thousands of cartridges  with robotic arms managing the movement of tapes to the drives  a hierarchical storage system extends the storage hierarchy beyond primary memory and secondary storage  that is  magnetic disk  to incorporate tertiary storage tertiary storage is usually implemented as a jukebox of tapes or removable disks this level of the storage hierarchy is larger  cheaper  and slower  although the virtual memory system can be extended in a straightforward manner to tertiary storage  this extension is rarely carried out in practice the reason is that a retrieval from a jukebox can take tens of seconds or even minutes  and such a long delay is intolerable for demand paging and for other forms of virtual memory use  the usual way to incorporate tertiary storage is to extend the file system  small and frequently used files remain on magnetic disk  while large and old files that are not actively used are archived to the jukebox in some file-archiving systems  the directory entry for the file continues to exist  but the contents of the file no longer occupy space in secondary storage if an application tries to open the file  the open   system call is suspended until the file contents can be staged in from tertiary storage when the contents are again available from magnetic disk  the open   operation returns control to the application  which proceeds to use the disk-resident copy of the data  today  is usually found in installations that have large volumes of data that are used seldom  sporadically  current work in hsm includes extending it to provide full here  data move from disk to tape and back to disk  as needed  but are deleted on a schedule or according to policy for example  some sites save e-mail for seven years but want to be sure that at the end of seven years it is destroyed at that point  the data might be on disk  hsm tape  and backup tape ilm centralizes knowledge of where the data are so that policies can be applied across all these locations  12.9.3 performance issues as with any component of the operating system  the three most important aspects of tertiary-storage performance are speed  reliability  and cost  12.9.3.1 speed the speed of tertiary storage has two aspects  bandwidth and latency we measure the bandwidth in bytes per second the ~ 'l ' is the average data rate during a transfer-that is  the number of bytes divided by the transfer time the calculates the average over the entire l/0 time  including the time for seek   or locate   and any 540 chapter 12 cartridge-switching time in a jukebox in essence  the sustained bandwidth is the rate at which the data stream actually flows  and the effective bandwidth is the overall data rate provided by the drive the bandwidth of a drive is generally understood to mean the sustained bandwidth  for removable disks  tlce bandwidth ranges from a few megabytes per second for the slowest to over 40 mb per second for the fastest tapes have a similar range of bandwidths  from a few megabytes per second to over 30mb per second  the second aspect of speed is the  by this performance measure  disks are much faster than tapes disk storage is essentially twodimensional all the bits are out in the open a disk access simply moves the ann to the selected cylinder and waits for the rotational latency  which may take less than 5 milliseconds by contrast  tape storage is three-dimensional  at any time  a small portion of the tape is accessible to the head  whereas most of the bits are buried below hundreds or thousands of layers of tape wound on the reel a random access on tape requires winding the tape reels until the selected block reaches the tape head  which can take tens or hundreds of seconds so we can generally say that random access within a tape cartridge is more than a thousand times slower than random access on disk  if a jukebox is involved  the access latency can be significantly higher for a removable disk to be changed  the drive must stop spinning  then the robotic arm must switch the disk cartridges  and then the drive must spin up the new cartridge this operation takes several seconds-about a hundred times longer than the random-access time within one disk so switching disks in a jukebox incurs a relatively high performance penalty  for tapes  the robotic-ann time is about the same as for disks but for tapes to be switched  the old tape generally must rewind before it can be ejected  and that operation can take as long as 4 minutes and  after a new tape is loaded into the drive  many seconds can be required for the drive to calibrate itself to the tape and to prepare for i/0 although a slow tape jukebox can have a tape-switch time of 1 or 2 minutes  this time is not enormously greater than the random-access time within one tape  to generalize  we can say that random access in a disk jukebox has a latency of tens of seconds  whereas random access in a tape jukebox has a latency of hundreds of seconds ; switching tapes is expensive  but switching disks is not we must be careful not to overgeneralize  though some expensive tape jukeboxes can rewind  eject  load a new tape  and fast-forward to a random item of data all in less than 30 seconds  if we pay attention to only the performance of the drives in a jukebox  the bandwidth and latency seem reasonable but if we focus our attention on the cartridges instead  we find a terrible bottleneck consider first the bandwidth the bandwidth-to-storage-capacity ratio of a robotic library is much less favorable than that of a fixed disk to read all the data stored on a large hard disk could take about an hour to read all the data stored in a large tape library could take years the situation with respect to access latency is nearly as bad to illustrate  if 100 requests are queued for a disk drive  the average waiting time will be about a second if 100 requests are queued for a tape library  the average waiting time could be over an hour the low cost of tertiary storage results from having many cheap cartridges share a few expensive drives but a removable library is best devoted to the storage of 12.9 541 infrequently used data  because the library can satisfy only a relatively small number of i/0 requests per hour  12.9.3.2 reliability although we often think good pe1jormance means high speed  another important aspect of performance is reliability if we try to read some data and are unable to do so because of a drive or media failure  for all practical purposes the access time is infinitely long and the bandwidth is infinitely small so it is important to understand the reliability of removable media  removable n1.ag  netic disks are somewhat less reliable than are fixed hard disks  because they are more likely to be exposed to harmful environmental conditions such as dust  large changes in temperature and humidity  and mechanical forces such as shock and bending optical disks are considered very reliable  because the layer that stores the bits is protected by a transparent plastic or glass layer the reliability of magnetic tape varies widely  depending on the kind of drive some inexpensive drives wear out tapes after a few dozen uses ; other drives are gentle enough to allow millions of reuses by comparison with a magnetic-disk head  the head in a magnetic-tape drive is a weak spot  a disk head flies above the media  but a tape head is in close contact with the tape the scrubbing action of the tape can wear out the head after a few thousands or tens of thousands of hours  in summary  we can say that a fixed-disk drive is likely to be more reliable than a removable-disk or tape drive  and an optical disk is likely to be more reliable than a magnetic disk or tape but a fixed magnetic disk has one weakness a head crash in a hard disk generally destroys the data  whereas the failure of a tape drive or optical-disk drive often leaves the data cartridge unharmed  12.9.3.3 cost storage cost is another important factor here is a concrete example of how removable media may lower the overall storage cost suppose that a hard disk that holds x gb has a price of $ 200 ; of this amom1.t  $ 190 is for the housing  motor  and controller  and $ 10 is for the magnetic platters the storage cost for this disk is $ 200/ x per gigabyte now  suppose that we can manufacture the platters in a removable cartridge for one drive and 10 cartridges  the total price is $ 190 + $ 100  and the capacity is lox gb  so the storage cost is $ 291 x per gigabyte even if it is a little more expensive to make a removable cartridge  the cost per gigabyte of removable storage may well be lower than the cost per gigabyte of a hard disk  because the expense of one drive is averaged with the low price of many removable cartridges  figures 12.15  12.16  and 12.17 show cost trends per megabyte for dram memory  magnetic hard disks  and tape drives the prices in the graphs are the lowest prices found in advertisements in various computer magazines and on the world wide web at the end of each year these prices reflect the smallcomputer marketplace of the readership of these magazines  where prices are low by comparison with the mainframe and minicomputer markets in the case of tape  the price is for a drive with one tape the overall cost of tape storage becomes much lower as more tapes are purchased for use with the drive  542 chapter 12 co   ; ; ; &  5 160 80 40 20 10 1.2 0.8 0.4 64 kb 32 128mb 512mb 2gb 0 02 --'-c-19 = '  ,8.,.-2 -1 98-cc4  -19 = '  c8 = -6 -1 ~ 9 '     88---,19 = '  c9 = -o -1c  '91 '     92  1 c  '  99-4 -c  c19l96---,19c  '  9 = -8 -2.,-,o '     oo  2c  '  oo = -2 -2      0l.04     2c  '  oo = -6   '2oos year figure 12 15 price per megabyte of dram  from 1981 to 2008  because the price of a tape is a small fraction of the price of the drive however  in a huge tape library containing thousands of cartridges  the storage cost is dominated by the cost of the tape cartridges as of 2004  the cost per gb of tape cartridges was around $ .40  as figure 12.15 shows  the cost of dram fluctuates widely in the period from 1981 to 2004  we can see three price crashes  around 1981  1989  and 1996  as excess production caused a glut in the marketplace we can also see two periods  around 1987 and 1993  where shortages in the marketplace caused sigrtificant price increases in the case of hard disks  figure 12.16   the price decline has been steadier tape-drive prices also fell steadily up to 1997  figure 12.17   since 1997  the price per gigabyte of inexpensive tape drives has ceased its dramatic fall  although the price of mid-range tape technology  such as dat /dds  has continued to fall and is now approaching that of the co   ; ; ; ' 100 50 20 5 2 0.5 0.2 0.05 0.02 0.004 0.001 0.0005 0.0002 10 20 1982 1984 1986 120 1.2 2 1988 1990 1992 19 gb gb gb 1994 1996 1998 2000 2002 2004 2006 2008 year figure 12.16 price per megabyte of magnetic hard disk  from 1981 to 2008  12.10 oj ~ 12.10 40 20 8 60 120 1.2 0.5 0.1 72gb 0.025 320gb 0.01 320gb 0.0051 ~ 9c-c84-1  l98  -6 -19  '  -88c---cc19 ~ 90c  c19 ~ 92  --c-c19l94-c-c19l96,---.,-c19'cc98-2,-jooc-c0--c2,-j.00,-,-2  ~   .,j = ~ 2008 year figure 12.17 price per megabyte of a tape drive  from 1984 to 2008  543 in.expensive drives tape-drive prices are not shown for years prior to 1984  because  as mentioned  the magazines used in tracking prices are targeted to the small-computer marketplace  and tape drives were not widely used with small computers prior to 1984  we can see from these graphs that the cost of storage has fallen dramatically  by comparing the graphs  we can also see that the price of disk storage has plummeted relative to the price of dram and tape  the price per megabyte of magnetic disk storage improved by more than four orders of magnitude from 1981 to 2004  whereas the corresponding improvement for main memory was only three orders of magnitude main memory today is more expensive than disk storage by a factor of 100  the price per megabyte dropped much more rapidly for disk drives than for tape drives as well in fact  the price per megabyte of a magnetic disk drive is approaching that of a tape cartridge without the tape drive consequently  small and medium-sized tape libraries have a higher storage cost than disk systems with equivalent capacity  the dramatic fall in disk prices has largely rendered tertiary storage obsolete we no longer have any tertiary storage technology that is orders of magnitude less expensive than magnetic disk it appears that the revival of tertiary storage must await a revolutionary technology breakthrough  meanwhile  tape storage will find its use mostly limited to purposes such as backups of disk drives and archival storage in enormous tape libraries that greatly exceed the practical storage capacity of large disk farms  disk drives are the major secondary-storage i/0 devices on most computers  most secondary storage devices are either magnetic disks or n1.agnetic tapes  modern disk drives are structured as large one-dimensional arrays of logical disk blocks generally  these logical blocks are 512 bytes in size disks may be attached to a computer system in one of two ways   1  through the local i/0 ports on the host computer or  2  through a network cmmection  544 chapter 12 requests for disk i/0 are generated by the file system and by the virtual memory system each request specifies the address on the disk to be referenced  in the form of a logical block number disk-schedliling algorithms can improve the effective bandwidth  the average response time  and the variance in response time algorithms such as sstf  scan  c-scan  look  and c-look are designed to make such improvements through strategies for disk-queue ordering  performance can be harmed by external fragmentation some systems have utilities that scan the file system to identify fragmented files ; they then move blocks around to decrease the fragmentation defragmenting a badly fragmented file system can significantly improve performance  but the systenc may have reduced performance while the defragmentation is in progress  sophisticated file systems  such as the unix fast file system  incorporate many strategies to control fragmentation during space allocation so that disk reorganization is not needed  the operating system manages the disk blocks first  a disk must be lowlevel formatted to create the sectors on the raw hardware-new disks usually come preformatted then  the disk is partitioned  file systems are created  and boot blocks are allocated to store the system 's bootstrap program finally  when a block is corrupted  the system must have a way to lock out that block or to replace it logically with a spare  because an efficient swap space is a key to good performance  systems usually bypass the file system and use raw disk access for paging i/0 some systems dedicate a raw disk partition to swap space  and others use a file within the file system instead still other systems allow the user or system administrator to make the decision by providing both options  because of the amount of storage required on large systems  disks are frequently made redundant via raid algorithms these algorithms allow more than one disk to be used for a given operation and allow continued operation and even automatic recovery in the face of a disk failure raid algorithms are organized into different levels ; each level provides some combination of reliability and high transfer rates  the write-ahead log scheme requires the availability of stable storage  to implement such storage  we need to replicate the needed information on multiple nonvolatile storage devices  usually disks  with independent failure modes we also need to update the information in a controlled manner to ensure that we can recover the stable data after any failure during data transfer or recovery  tertiary storage is built from disk and tape drives that use removable media many different technologies are available  including magnetic tape  removable magnetic and magneto-optic disks  and optical disks  for removable disks  the operating system generally provides the full services of a file-system interface  including space management and requestqueue scheduling for many operating systems  the name of a file on a removable cartridge is a combination of a drive name and a file name within that drive this convention is simpler but potentially more confusing than is using a name that identifies a specific cartridge  for tapes  the operating system generally provides only a raw interface  many operating systems have no built-in support for jukeboxes jukebox 545 support can be provided by a device driver or by a privileged application designed for backups or for hsm  three important aspects of performance are bandwidth  latency  and reliability many bandwidths are available for both disks and tapes  but the random-access latency for a tape is generally much greater than that for a disk  switching cartridges in a jukebox is also relatively slow because a jukebox has a low ratio of drives to cartridges  reading a large fraction of the data in a jukebox can take a long time optical media  which protect the sensitive layer with a transparent coating  are generally more robust than magnetic media  which are more likely to expose the magnetic material to physical damage  lastly  the cost of storage has decreased greatly in the past two decades  most notably for disk storage  12.1 what would be the effects on cost and performance if tape storage had the same areal density as disk storage  areal density is the number of gigabits per square inch  12.2 it is sometimes said that tape is a sequential-access medium  whereas a magnetic disk is a random-access medium in fact the suitability of a storage device for random access depends on the transfer size  the term streaming transfer rate denotes the rate for a data transfer that is underway  excluding the effect of access latency by contrast  the effective transfer rate is the ratio of total bytes per total seconds  including overhead time such as access latency  suppose that  in a computer  the level-2 cache has an access latency of 8 nanoseconds and a streaming transfer rate of 800 megabytes per second  the main memory has an access latency of 60 nanoseconds and a streaming transfer rate of 80 megabytes per second  the magnetic disk has an access latency of 15 milliseconds and a streaming transfer rate of 5 megabytes per second  and a tape drive has an access latency of 60 seconds and a streaming transfer rate of 2 megabytes per seconds  a random access causes the effective transfer rate of a device to decrease  because no data are transferred during the access time  for the disk described  what is the effective transfer rate if an average access is followed by a streaming transfer of  1  512 bytes   2  8 kilobytes   3  1 megabyte  and  4  16 megabytes b the utilization of a device is the ratio of effective transfer rate to streaming transfer rate calculate the utilization of the disk drive for each of the four transfer sizes given in part a  c suppose that a utilization of 25 percent  or higher  is considered acceptable using the performance figures given  compute the smallest transfer size for disk that gives acceptable utilization  546 chapter 12 d complete the following sentence  a disk is a random-access device for transfers larger than ______ bytes and is a sequentialaccess device for s1naller transfers  e compute the minimum transfer sizes that give acceptable utilization for cache  memory  and tape  f when is a tape a random-access device  and when is it a sequential-access device 12.3 the reliability of a hard-disk drive is typically described in terms of a quantity called mean time between failures  mtbf   although this quantity is called a time  the mtbf actually is measured in drive-hours per failure  a if a system contains 1,000 disk drives  each of which has a 750,000 hour mtbf  which of the following best describes how often a drive failure will occur in that disk farm  once per thousand years  once per century  once per decade  once per year  once per month  once per week  once per day  once per hour  once per minute  or once per second b mortality statistics indicate that  on the average  a u.s resident has about 1 chance in 1,000 of dying between the ages of 20 and 21  deduce the mtbf hours for 20-year-olds convert this figure from hours to years what does this mtbf tell you about the expected lifetime of a 20-year-old c the manufacturer guarantees a 1-million-hour mtbf for a certain model of disk drive what can you conclude about the number of years for which one of these drives is under warranty 12.4 discuss how an operating system could maintain a free-space list for a tape-resident file system assume that the tape technology is append-only and that it uses eot marks and locate  space  and read position commands as described in section 12.9.2.1  12.5 imagine that a holographic storage drive has been invented the drive costs $ 10,000 and has an average access time of 40 milliseconds it uses a $ 100 cartridge the size of a cd this cartridge holds 40,000 images  and each image is a square black-and-white picture with a resolution of 6  000 x 6  000 pixels  each pixel stores 1 bit   the drive can read or write one picture in 1 millisecond answer the following questions  a what would be some good uses for this device b how would this device affect the l/0 performance of a computing system c what kinds of storage devices  if any  would become obsolete as a result of the invention of this device 547 12.6 the term fast wide scsi-ii denotes a scsi bus that operates at a data rate of 20 megabytes per second when it moves a packet of bytes between the host and a device suppose that a fast wide scsi-ii disk drive spins at 7,200 rpm  has a sector size of 512 bytes  and holds 160 sectors per track a estimate the sustained transfer rate of this drive in megabytes per second  b suppose that the drive has 7,000 cylinders  20 tracks per cylinde1 ~ a head-switch time  from one platter to another  of 0.5 millisecond  and an adjacent-cylinder seek time of 2 milliseconds use this additional information to give an accurate estimate of the sustained transfer rate for a huge transfer  c suppose that the average seek time for the drive is 8 milliseconds  estimate the i/0 operations per second and the effective transfer rate for a random-access workload that reads individual sectors that are scattered across the disk d calculate the random-access i/0 operations per second and transfer rate for i/0 sizes of 4 kilobytes  8 kilobytes  and 64 kilobytes  e if multiple requests are in the queue  a scheduling algorithm such as scan should be able to reduce the average seek distance suppose that a random-access workload is reading 8-kilobyte pages  the average queue length is 10  and the scheduling algorithm reduces the average seek time to 3 milliseconds now calculate the i/0 operations per second and the effective transfer rate of the drive  12.7 compare the performance of write operations achieved by a raid level 5 organization with that achieved by a raid level1 organization  12.8 suppose that a disk drive has 5,000 cylinders  numbered 0 to 4999 the drive is currently serving a request at cylinder 143  and the previous request was at cylinder 125 the queue of pending requests  in fifo order  is  86,1470,913,1774,948,1509,1022,1750,130 starting from the current head position  what is the total distance  in cylinders  that the disk arm moves to satisfy all the pending requests for each of the following disk-scheduling algorithms a fcfs b sstf 548 chapter 12 c scan d look e c-scan f c-look 12.9 elementary physics states that when an object is subjected to a constant acceleration a  the relationship between distance d and time t is given by d = ~ at2  suppose that  during a seek  the disk in exercise 12.8 accelerates the disk arm at a constant rate for the first half of the seek  then decelerates the disk arm at the same rate for the second half of the seek assume that the disk can perform a seek to an adjacent cylinder in 1 n lillisecond and a full-stroke seek over all 5,000 cylinders in 18 milliseconds  a the distance of a seek is the number of cylinders that the head moves explain why the seek time is proportional to the square root of the seek distance  b write an equation for the seek time as a function of the seek distance this equation should be of the form t = x + y ~  where t is the time in milliseconds and l is the seek distance in cylinders  c calculate the total seek time for each of the schedules in exercise 12.8 determine which schedule is the fastest  has the smallest total seek time   d the percentage speedup is the time saved divided by the original time what is the percentage speedup of the fastest schedule over fcfs 12.10 the accelerating seek described in exercise 12.9 is typical of hard-disk drives by contrast  floppy disks  and many hard disks manufactured before the mid-1980s  typically seek at a fixed rate suppose that the disk in exercise 12.9 has a constant-rate seek rather than a constantacceleration seek  so the seek time is of the form t = x + yl  where t is the time in milliseconds and l is the seek distance suppose that the time to seek to an adjacent cylinder is 1 millisecond  as before  and the time to seek to each additional cylinder is 0.5 milliseconds  a write an equation for this seek time as a function of the seek distance  b using this seek-time function  calculate the total seek time or each of the schedules in exercise 12.8 is your answer the same as the one or exercise 12.9  c  549 c what is the percentage speedup of the fastest scb.edule over fcfs in this case 12.11 suppose that the disk in exercise 12.9 rotates at 7,200 rpm  a what is the average rotational latency of this disk drive b what seek distance can be covered in the tim.e that you found or part a 12.12 suppose that a one-sided 5.25-inch optical-disk cartridge has an areal density of 1 gigabit per square inch further suppose that a magnetic tape has an areal density of 20 megabits per square inch and is 1/2 inch wide and 1,800 feet long calculate an estimate of the storage capacities of these two kinds of storage media suppose that an optical tape exists that has the same physical size as the magnetic tape but the same storage density as the optical disk what volume of data could the optical tape hold what would be a marketable price for the optical tape if the magnetic tape cost $ 25 12.13 write a program that simulates the disk-scheduling algorithms discussed in section 12.4  12.14 why is rotational latency usually not considered in disk scheduling how would you modify sstf  scan  and c-scan to include latency optimization 12.15 remapping bad blocks by sector sparing or sector slipping can influence perfonnance suppose that the drive in exercise 12.6 has a total of 100 bad sectors at random locations and that each bad sector is mapped to a spare that is located on a different track within the same cylinder estimate the number of i/0 operations per second and the effective transfer rate for a random-access workload consisting of 8 kilobyte reads  assuming a queue length of 1  that is  the choice of scheduling algorithm is not a factor   what is the effect of a bad sector on performance 12.16 discuss the relative advantages and disadvantages of sector sparing and sector slipping  12.17 compare the performance of c-scan and scan scheduling  assuming a uniform distribution of requests consider the average response time  the time between the arrival of a request and the completion of that request 's service   the variation in response time  and the effective 550 chapter 12 bandwidth how does performance depend on the relative sizes of seek time and rotational latency 12.18 none of the disk-scheduling disciplines  except fcfs  is truly fair  starvation may occur   a explain why this assertion is true  b describe a way to modify algorithms such as scan to ensure fairness  c explain why fairness is an important goal in a time-sharing system  d give three or more examples of circumstances in which it is important that the operating system be unfair in serving i/o requests  12.19 consider a raid level 5 organization comprising five disks  with the parity for sets of four blocks on four disks stored on the fifth disk how many blocks are accessed in order to perform the following a a write of one block of data b a write of seven continuous blocks of data 12.20 the operating system generally treats removable disks as shared file systems but assigns a tape drive to only one application at a time give three reasons that could explain this difference in treatment of disks and tapes describe the additional features that an operating system would need to support shared file-system access to a tape jukebox would the applications sharing the tape jukebox need any special properties  or could they use the files as though the files were disk-resident explain your answer  12.21 how would use of a ram disk affect your selection of a disk-scheduling algorithm what factors would you need to consider do the same considerations apply to hard-disk scheduling  given that the file system stores recently used blocks in a buffer cache in main memory 12.22 you can use simple estimates to compare the cost and performance of a terabyte storage system made entirely from disks with one that incorporates tertiary storage suppose that each magnetic disk holds 10gb  costs $ 1,000  transfers 5mb per second  and has an average access latency of 15 milliseconds also suppose that a tape library costs $ 10 per gigabyte  transfers 10 mb per second  and has an average access latency of 20 seconds compute the total cost  the maximum total data rate  and the average waiting time for a pure disk system if you make 551 any assumptions about the workload  describe and justify them now  suppose that 5 percent of the data are frequently used  so they must reside on disk  but the other 95 percent are archived in the tape library  further suppose that the disk system handles 95 percent of the requests and the library handles the other 5 percent what are the total cost  the maximum total data rate  and the average waiting time for this hierarchical storage system 12.23 assume that you have a mixed configuration comprising disks organized as raid levell and raid levels disks assume that the system has flexibility in deciding which disk organization to use for storing a particular file which files should be stored in the raid level 1 disks and which in the raid levels disks in order to optimize performance 12.24 what are the tradeoffs involved in rereading code pages from the file system versus using swap space to store them 12.25 requests are not usually uniformly distributed for example  we can expect a cylinder containing the file-system fat or inodes to be accessed more frequently than a cylinder containing only files suppose you know that 50 percent of the requests are for a small  fixed number of cylinders  a would any of the scheduling algorithms discussed in this chapter be particularly good for this case explain your answer  b propose a disk-scheduling algorithm that gives even better performance by taking advantage of this hot spot on the disk  c file systems typically fil1.d data blocks via an indirection table  such as a fat in dos or inodes in unix describe one or more ways to take advantage of this indirection to improve disk performance  12.26 discuss the reasons why the operating system might require accurate information on how blocks are stored on a disk how could the operating system improve file system performance with this knowledge 12.27 in a disk jukebox  what would be the effect of having more open files than the number of drives in the jukebox 12.28 compare the throughput achieved by a raid levels organization with that achieved by a raid levell organization for the following  a read operations on single blocks b read operations on multiple contiguous blocks 552 chapter 12 12.29 could a raid level 1 organization achieve better performance for read requests than a raid level 0 organization  with nonredundant striping of data  if so  how discussions of redundant arrays of independent disks  raids  are presented by patterson et al  1988  and in the detailed survey of chen et al  1994   disk-system architectures for high-performance computing are discussed by katz et al  1989   enhancements to raid systems are discussed in wilkes et al  1996  and yu et al  2000   teorey and pinkerton  1972  present an early comparative analysis of disk-scheduling algorithms they use simulations that model a disk for which seek time is linear in the number of cylinders crossed  for this disk look is a good choice for queue lengths below 140  and c-look is good for queue lengths above 100 king  1990  describes ways to improve the seek time by moving the disk ann when the disk is otherwise idle seltzer et al   1990  and jacobson and wilkes  1991  describe disk-scheduling algorithms that consider rotational latency in addition to seek time scheduling optimizations that exploit disk idle times are discussed in lumb et al  2000   worthington et al  1994  discuss disk performance and show the negligible performance impact of defect management the placement of hot data to improve seek times has been considered by ruemmler and wilkes  1991  and akyurek and salen'l  1993   ruemmler and wilkes  1994  describe an accurate performance model for a modern disk drive worthington et al  1995  tell how to determine low-level disk properties such as the zone structure  and this work is further advanced by schindler and gregory  1999   disk power management issues are discussed in douglis et al  1994l douglis et al  1995l greenawalt  1994l and golding et al  1995   the i/0 size and randomness of the workload has a considerable influence on disk performance ousterhout et al  1985  and ruemmler and wilkes  1993  report numerous interesting workload characteristics  including that most files are smalt most newly created files are deleted soon thereafter  most files that are opened for reading are read sequentially in their entirety  and most seeks are short mckusick et al  1984  describe the berkeley fast file system  ffs   which uses many sophisticated techniques to obtain good performance for a wide variety of workloads mcvoy and kleiman  1991  discuss further improvements to the basic ffs quinlan  1991  describes how to implement a file system on worm storage with a magnetic disk cache ; richards  1990  discusses a file-system approach to tertiary storage maher et al  1994  give an overview of the integration of distributed file systems and tertiary storage  the concept of a storage hierarchy has been studied for more than thirty years for instance  a 1970 paper by mattson et al  1970  describes a mathematical approach to predicting the performance of a storage hierarchy  alt  1993  describes the accommodation of removable storage in a commercial operating system  and miller and katz  1993  describe the characteristics of tertiary-storage access in a supercomputing environment benjamin  1990  gives an overview of the massive storage requirements for the eosdis project at nasa management and use of network-attached disks and programmable 553 disks are discussed in gibson et al  1997b t gibson et al  1997at riedel et al   1998t and lee and thekkath  1996   holographic storage technology is the subject of an article by psaltis and mok  1995  ; a collection of papers on this topic dating from 1963 has been assembled by sincerbox  1994   asthana and finkelstein  1995  describe several emerging storage technologies  including holographic storage  optical tape  and electron trapping toigo  2000  gives an in-depth description of modern disk technology and several potential future storage technologies  13.1 r the two main jobs of a computer are i/0 and processing in many cases  the main job is i/0  and the processing is merely incidental for instance  when we browse a web page or edit a file  our immediate interest is to read or enter some information  not to compute an answer  the role of the operating system in computer i/0 is to manage and control i/0 operations and i/0 devices although related topics appear in other chapters  here we bring together the pieces to paint a complete picture of i/0 first  we describe the basics of i/o hardware  because the nature of the hardware interface places constraints on the internal facilities of the operating system next  we discuss the i/0 services provided by the operating system and the embodiment of these services in the application i/0 interface then  we explain how the operating system bridges the gap between the hardware interface and the application interface we also discuss the unix system v streams mechanism  which enables an application to assemble pipelines of driver code dynamically finally  we discuss the performance aspects of i/o and the principles of operating-system design that improve i/0 performance  to explore the structure of an operating system 's 1/0 subsystem  to discuss the principles and complexities of 110 hardware  to explain the performance aspects of 110 hardware and software  the control of devices connected to the computer is a major concern of operating-system designers because i/o devices vary so widely in their function and speed  consider a mouse  a hard disk  and a cd-rom jukebox   varied methods are needed to control them these methods form the i/0 subsystem of the kernet which separates the rest of the kernel from the complexities of managing i/0 devices  555 556 chapter 13 13.2 i/o-device technology exhibits two conflicting trends on the one hand  we see increasing standardization of software and hardware interfaces this trend helps 11s to incorporate improved device generations into existing computers and operating systems on the other hand  we see an increasingly broad variety of 1/0 devices some new devices are so unlike previous devices that it is a challenge to incorporate them into our computers and operating systems this challenge is met by a combination of hardware and software techniques the basic i/0 hardware elements  such as ports  buses  and device controllers  accommodate a wide variety of i/0 devices to encapsulate the details and oddities of different devices  the kernel of an operating system is structured to use device-driver modules the present a uniform deviceaccess interface to the i/0 subsystem  much as system calls provide a standard interface between the application and the operating system  computers operate a great many kinds of devices most fit into the general categories of storage devices  disks  tapes   transmission devices  network cards  modems   and human-interface devices  screen  keyboard  mouse   other devices are more specialized  s11ch as those involved in the steering of a military fighter jet or a space shuttle in these aircraft  a human gives input to the flight computer via a joystick and foot pedals  and the computer sends output commands that cause motors to move rudders  flaps  and thrusters  despite the incredible variety of i/0 devices  though  we need only a few concepts to understand how the devices are attached and how the software can control the hardware  a device communicates with a computer system by sending signals over a cable or even through the air the device communicates with the machine via a connection point  or example  a serial port if devices use a common set of wires  the connection is called a bus a is a set of wires and a rigidly defined protocol that specifies a set of messages that can be sent on the wires  in terms of the electronics  the messages are conveyed by patterns of electrical voltages applied to the wires with defined timings when device a has a cable that plugs into device b  and device b has a cable that plugs into device c  and device c plugs into a port on the computer  this arrangement is called a a daisy chain usually operates as a bus  buses are used widely in computer architecture and vary in their signaling methods  speed  throughput  and connection methods a typical pc bus structure appears in figure 13.1 this figure shows a  the common pc system bus  that connects the processor-memory subsystem to the fast devices and an that connects relatively slow devices  such as the keyboard and serial and usb ports in the upper-right portion of the figure  four disks are c01mected together on a scsi bus plugged into a scsi controller  other common buses used to interconnect main parts of a computer include with up to 4.3 gb ;  pcie   with throughput up with throughput up to 20 gb  is a collection of electronics that can operate a port  a bus  or a device a serial-port controller is a simple device controller it is a single chip  or portion of a chip  in the computer that controls the signals on the 13.2 557 figure 13.1 a typical pc bus structure  wires of a serial port by contrast  a scsi bus controller is not simple because the scsi protocol is complex  the scsi bus controller is often implemented as a separate circuit board  or a that plugs into the computer it typically contains a processor  microcode  and some private memory to enable it to process the scsi protocol messages some devices have their own built-in controllers if you look at a disk drive  you will see a circuit board attached to one side this board is the disk controller it implements the disk side of the protocol for some kind of com1ection-scsi or ata  for instance it has microcode and a processor to do many tasks  such as bad-sector mapping  prefetching  buffering  and caching  how can the processor give commands and data to a controller to accomplish an i/0 transfer the short answer is that the controller has one or more registers for data and control signals the processor communicates with the controller by reading and writing bit patterns in these registers one way in which this communication can occur is through the use of special i/0 instructions that specify the transfer of a byte or word to an i/0 port address the i/0 instruction triggers bus lines to select the proper device and to move bits into or out of a device register alternatively  the device controller can support in this case  the device-control registers are mapped into the address space of the processor the cpu executes i/0 requests using the standard data-transfer instructions to read and write the device-control registers  some systems use both techniques for instance  pcs use i/0 instructions to control some devices and memory-mapped i/0 to control others figure 13.2 shows the usual i/o port addresses for pcs the graphics controller has i/o ports for basic control operations  but the controller has a large memory558 chapter 13 000-00f dma controller 020-021 interrupt controller 040-043 timer 200-20f game controller 2f8-2ff serial port  secondary  320-32f hard-disk controller 378-37f parallel port 3d0-3df graphics controller 3f0-3f7 diskette-drive controller 3f8-3ff serial port  primary  figure 13.2 device 1/0 port locations on pcs  partial   mapped region to hold screen contents the process sends output to the screen by writing data into the memory-mapped region the controller generates the screen image based on the contents of this memory this technique is simple to use moreover  writing millions of bytes to the graphics memory is faster than issuing millions of i/0 instructions but the ease of writing to a memory-mapped i/0 controller is offset by a disadvantage because a common type of software fault is a write through an incorrect pointer to an unintended region of memory  a memory-mapped device register is vulnerable to accidental modification of course  protected memory helps to reduce this risk  an i/0 port typically consists of four registers  called the  1  status   2  control   3  data-in  and  4  data-out registers  the the is read by the host to get input  is written by the host to send output  the contains bits that can be read by the host these bits indicate states  such as whether the current command has completed  whether a byte is available to be read from the data-in register  and whether a device error has occurred  the can be written by the host to start a command or to change the nlode of a device for instance  a certain bit in the control register of a serial port chooses between full-duplex and half-duplex communication  another bit enables parity checking  a third bit sets the word length to 7 or 8 bits  and other bits select one of the speeds supported by the serial port  the data registers are typically 1 to 4 bytes in size some controllers have fifo chips that can hold several bytes of input or output data to expand the capacity of the controller beyond the size of the data register a fifo chip can hold a small burst of data until the device or host is able to receive those data  13.2 559 13.2.1 polling the complete protocol for interaction between the host and a controller can be intricate  but the basic handshaking notion is simple we explain handshaking with an example assume that 2 bits are used to coordinate the producer-consumer relationship between the controller and the host the controller indicates its state through the busy bit in the status register  recall that to set a bit means to write a 1 into the bit and to clear a bit means to write a 0 into it  the controller sets the busy bit when it is busy working and clears the busy bit when it is ready to accept the next comm.and the host signals its wishes via the command-ready bit in the command register the host sets the command-ready bit when a command is available for the controller to execute  for this example  the host writes output through a port  coordinating with the controller by handshaking as follows  the host repeatedly reads the busy bit until that bit becomes clear  the host sets the write bit in the command register and writes a byte into the data-out register  the host sets the command-ready bit  when the controller notices that the command-ready bit is set  it sets the busy bit  the controller reads the command register and sees the write command  it reads the data-out register to get the byte and does the i/o to the device  the controller clears the command-ready bit  clears the error bit in the status register to indicate that the device i/o succeeded  and clears the busy bit to indicate that it is finished  this loop is repeated for each byte  in step 1  the host is or it is in a loop  reading the status register over and over until the busy bit becomes clear if the controller and device are fast  this method is a reasonable one but if the wait may be long  the host should probably switch to another task how  then  does the host know when the controller has become idle for some devices  the host must service the device quickly  or data will be lost for instance  when data are streaming in on a serial port or from a keyboard  the small buffer on the controller will overflow and data will be lost if the host waits too long before returning to read the bytes  in many computer architectures  three cpu-instruction cycles are sufficient to poll a device  read a device register  logical-and to extract a status bit  and branch if not zero clearly  the basic polling operation is efficient but polling becomes inefficient when it is attempted repeatedly yet rarely finds a device to be ready for service  while other useful cpu processing remains undone in such instances  it may be more efficient to arrange for the hardware controller to notify the cpu when the device becomes ready for service  rather than to require the cpu to poll repeatedly for an i/0 completion the hardware mechanism that enables a device to notify the cpu is called an 560 chapter 13 7 cpu device driver initiates 1/0 cpu executing checks for interrupts between instructions cpu resumes processing of interrupted task 1/0 controller 4 figure 13.3 interrupt-driven 1/0 cycle  13.2.2 interrupts the basic interrupt mechanism works as follows the cpu hardware has a wire called the that the cpu senses after executing every instruction when the cpu detects that a controller has asserted a signal on the line  the cpu performs a state save and jumps to the at a fixed address in memory the interrupt handler determines the cause of the interrupt  performs the necessary processing  performs a state restore  and executes a return from interrupt instruction to return the cpu to the execution state prior to the interrupt we say that the device controller raises an interrupt by asserting a signal on the interrupt request line  the cpu catches the interrupt and dispatches it to the interrupt handler  and the handler clears the interrupt by servicing the device figure 13.3 summarizes the interrupt-driven i/0 cycle  this basic interrupt mechanism enables the cpu to respond to an asynchronous event  as when a device controller becomes ready for service in a modern operating system  however  we need nlore sophisticated interrupthandling features  we need the ability to defer interrupt handling during critical processing  13.2 561 we need an efficient way to dispatch to the proper interrupt handler for a device without first polling all the devices to see which one raised the interrupt  we need multilevel interrupts  so that the operating system can distinguish between high and low-priority interrupts and can respond with the appropriate degree of urgency  in modern computer hardware  these three features are provided by the cpu and by the most cpus have two interrupt request lines one is the ' ' ' ' which is reserved for events such as unrecoverable memory errors  the second interrupt line is it can be turned off by the cpu before the execution of critical instruction sequences that must not be interrupted  the maskable interrupt is used by device controllers to request service  the interrupt mechanism accepts an number that selects a specific interrupt-handling routine from a small set in most architectures  this address is an offset in a table called the  this vector contains the memory addresses of specialized interrupt handlers the purpose of a vectored interrupt mechanism is to reduce the need for a single interrupt handler to search all possible sources of interrupts to determine which one needs service in practice  however  computers have more devices  and  hence  interrupt handlers  than they have address elements in the interrupt vector  a common way to solve this problem is to use the technique of interrupt chaining  in which each element in the interrupt vector points to the head of a list of interrupt handlers when an il1.terrupt is raised  the handlers on the corresponding list are called one by one  until one is found that can service the request this structure is a compromise between the overhead of a huge interrupt table and the inefficiency of dispatching to a single interrupt handler  figure 13.4 illustrates the design of theinterruptvector for the intel pentium processor the events from 0 to 31  which are nonmaskable  are used to signal various error conditions the events from 32 to 255  which are maskable  are used for purposes such as device-generated interrupts  the interrupt mechanism also implements a system of this mechanism enables the cpu to defer the handling of low-priority interrupts without maskii1.g off all interrupts and makes it possible for a high-priority interrupt to preempt the execution of a low-priority interrupt  a modern operating system interacts with the interrupt mechanism in several ways at boot time  the operating system probes the hardware buses to determine what devices are present and installs the corresponding interrupt handlers into the interrupt vector during i/0  the various device controllers raise interrupts when they are ready for service these interrupts signify that output has cornpleted  or that input data are available  or that a failure has been detected the interrupt mechanism is also used to handle a wide variety of such as dividing by zero  accessing a protected or nonexistent memory address  or attempting to execute a privileged instruction from user mode the events that trigger interrupts have a common property  they are occurrences that induce the cpu to execute an urgent self-contained routine  an operating system has other good uses for an efficient hardware and software mechanism that saves a small amount of processor state and then 562 chapter 13 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 i 18 19-31 32-255 breakpoint into-detected overflow bound range exception invalid opcode device not available double fault coprocessor segment overrun  reserved  invalid task state segment segment not present stack fault general protection page fault  intel reserved  do not use  floating-point error alignment check machine check  intel reserved  do not use  maskable interrupts figure i3.4 intel pentium processor event-vector table  calls a privileged routine in the kernel for example  many operating systems use the interrupt mechanism for virtual memory paging a page fault is an exception that raises an interrupt the interrupt suspends the current process and jumps to the page-fault handler in the kernel this handler saves the state of the process  moves the process to the wait queue  performs page-cache management  schedules an i/0 operation to fetch the page  schedules another process to resume execution  and then returns from the interrupt  another example is found in the implementation of system calls usually  a program uses library calls to issue system calls the library routines check the arguments given by the application  build a data structure to convey the arguments to the kernel  and then execute a special instruction called a or  this instruction has an operand that identifies the desired kernel service when a process executes the trap instruction  the interrupt hardware saves the state of the user code  switches to supervisor mode  and dispatches to the kernel routine that implements the requested service the trap is given a relatively low interrupt priority compared with those assigned to device interrupts-executilcg a system call on behalf of an application is less urgent than servicing a device controller before its fifo queue overflows and loses data  interrupts can also be used to manage the flow of control within the kernel  for example  consider the processing required to complete a disk read one step is to copy data from kernel space to the user buffer this copying is time consuming but not urgent-it should not block other high-priority interrupt 13.2 563 handling another step is to start the next pending l/0 for that disk drive this step has higher priority if the disks are to be used efficiently  we need to start the next i/o as soon as the previous one completes consequently  a pair of interrupt handlers implen ents the kernel code that completes a disk read the high-priority handler records the l/0 status  clears the device interrupt  starts the next pending i/0  and raises a low-priority interrupt to complete the work  later  when the cpu is not occupied with high-priority work  the low-priority interrupt will be dispatched the corresponding handler completes the userlevel i/0 by copying data from kernel buffers to the application space and then calling the scheduler to place the application on the ready queue  a threaded kernel architecture is well suited to implement multiple interrupt priorities and to enforce the precedence of interrupt handling over background processing in kernel and application routines we illustrate this point with the solaris kernel in solaris  interrupt handlers are executed as kernel threads a range of high priorities is reserved for these threads  these priorities give interrupt handlers precedence over application code and kernel housekeeping and implement the priority relationships among interrupt handlers the priorities cause the solaris thread scheduler to preempt lowpriority interrupt handlers in favor of higher-priority ones  and the threaded implementation enables multiprocessor hardware to run several interrupt handlers concurrently we describe the interrupt architecture of windows xp and unix in chapter 22 and appendix a  respectively  in summa  r  y  interrupts are used throughout modern operating systems to handle asynchronous events and to trap to supervisor-mode routines in the kernel to enable the most urgent work to be done first  modern computers use a system of interrupt priorities device controllers  hardware faults  and system calls all raise interrupts to trigger kernel routines because interrupts are used so heavily for time-sensitive processing  efficient interrupt handling is required for good system performance  13.2.3 direct memory access for a device that does large transfers  such as a disk drive  it seems wasteful to use an expensive general-purpose processor to watch status bits and to feed data into a controller register one byte at a time-a process termed many computers avoid burdening the main cpu with pio by offloading some of this work to a special-purpose processor called a to initiate a dma transfer  the host writes a dma command block into memory this block contains a pointer to the source of a transfer  a pointer to the destination of the transfer  and a count of the number of bytes to be transferred the cpu writes the address of this command block to the dma controller  then goes on with other work the dma controller proceeds to operate the memory bus directly  placing addresses on the bus to perform transfers without the help of the main cpu a simple dma controller is a standard component in pcs  and for the pc usually contain their own high-speed dma hardware  handshaking between the dma controller and the device controller is performed via a pair of wires called dma-request and dma-acknowledge  the device controller places a signal on the dma-request wire when a word of data is available for transfer this signal causes the dma controller to seize 564 chapter 13 the memory bus  place the desired address on the memory-address wires  and place a signal on the dl \ iia -acknowledge wire when the device controller receives the dma-acknowledge signat it transfers the word of data to memory and removes the dma-request signal  when the entire transfer is finished  the dma controller interrupts the cpu  this process is depicted in figure 13.5 when the dma controller seizes the memory bus  the cpu is momentarily prevented from accessing main memory although it can still access data items in its primary and secondary caches  although this can slow down the cpu computation  offloading the data-transfer work to a dma controller generally improves the total system performance some computer architectures use physical memory addresses for dma  but others perform mercwry using virtual addresses that undergo translation to physical addresses dvma can perform a transfer between two memory-mapped devices without the intervention of the cpu or the use of main memory  on protected-mode kernels  the operating system generally prevents processes from issuing device commands directly this discipline protects data from access-control violations and also protects the system from erroneous use of device controllers that could cause a system crash instead  the operating system exports functions that a sufficiently privileged process can use to access low-level operations on the underlying hardware on kernels without memory protection  processes can access device controllers directly this direct access can be used to achieve high performance/ since it can avoid kernel communication  context switches  and layers of kernelsoftware unfortunately  it interferes with system security and stability the trend in general-purpose operating systems is to protect memory and devices so that the system can try to guard against erroneous or malicious applications  5 dma controller transfers bytes to buffer x  increasing memory address and decreasing c until c = 0 1 device driver is told to transfer disk data to buffer at address x 2 device driver tells l  ' '  ~ i ' ' ~ --' ' disk controller to transfer c bytes from disk to buffer at address x 6 when c = 0  dma interrupts cpu to signal transfer completion 1  2.'.c ~ li.ip  2i  .2-j rc-c ~ .,.,---j' = ,._ ~ 3 disk controller initiates dma transfer c'g ! ,or  tt_ront ; r ' i 4 disk controller sends each byte to dma controller figure 13.5 steps in a dma transfer  13.3 13.3 565 13.2.4 1/0 hardware summary although the hardware aspects of i/0 are complex when considered at the level of detail of electronics-hardware design  the concepts that we have just described are sufficient to enable us to understand many i/0 features of operating systen s let 's review the main concepts  a bus a controller an i/0 port and its registers the handshaking relationship between the host and a device controller the execution of this handshaking in a polling loop or via interrupts the offloading of this work to a dma controller for large transfers we gave a basic example of the handshaking that takes place between a device controller and the host earlier in this section in reality  the wide variety of available devices poses a problem for operating-system implementers each kind of device has its own set of capabilities  control-bit definitions  and protocols for interacting with the host-and they are all different how can the operating system be designed so that we can attach new devices to the computer without rewriting the operating system and when the devices vary so widely  how can the operating system give a convenient  uniform i/0 interface to applications we address those questions next  in this section  we discuss structuring techniques and interfaces for the operating system that enable i/0 devices to be treated in a standard  uniform way we explain  for instance  how an application can open a file on a disk without knowing what kind of disk it is and how new disks and other devices can be added to a cmnputer without disruption of the operating system  like other complex software-engineering problems  the approach here involves abstraction  encapsulation  and software layering specifically  we can abstract away the detailed differences in i/0 devices by identifying a few general kinds each kind is accessed through a standardized set of functions-an the differences are encapsulated in kernel modules called device drivers that internally are custom-tailored to specific devices but that export one of the standard interfaces figure 13.6 illustrates how the i/o-related portions of the kernel are structured in software layers  the purpose of the device-driver layer is to hide the differences among device controllers from the i/o subsystem of the kernel  much as the i/0 system calls encapsulate the behavior of devices in a few generic classes that hide hardware differences from applications making the i/0 subsystem independent of the hardware simplifies the job of the operating-system developer it also benefits the hardware manufacturers they either design new devices to be compatible with an existing host controller interface  such as scsi-2   or they write device drivers to interface the new hardware to popular 566 chapter 13 figure 13.6 a kernel i/o structure  operating systems thus  we can attach new peripherals to a computer without waiting for the operating-system vendor to develop support code  unfortm1ately for device-hardware manufacturers  each type of operating system has its own standards for the device-driver interface a given device may ship with multiple device drivers-for instance  drivers for ms-dos  windows 95/98  windows nt/2000  and solaris devices vary on many dimensions  as illustrated in figure 13.7  character-stream or block a character-stream device transfers bytes one by one  whereas a block device transfers a block of bytes as a unit  sequential or random access a sequential device transfers data in a fixed order determined by the device  whereas the user of a random-access device can instruct the device to seek to any of the available data storage locations  synchronous or asynchronous a synchronous device performs data transfers with predictable response times an asynchronous device exhibits irregular or unpredictable response times  sharable or dedicated a sharable device can be used concurrently by several processes or threads ; a dedicated device can not  speed of operation device speeds range from a few bytes per second to a few gigabytes per second  read -write  read only  or write only some devices perform both input and output  but others support only one data transfer direction  access method transfer schedule i/o direction 13.3 synchronous asynchronous dedicated sharable latency seek time transfer rate delay between operations read only write only read-write tape keyboard tape keyboard cd-rom figure i 3 7 characteristics of 1/0 devices  567 for the purpose of application access  many of these differences are hidden by the operating system  and the devices are grouped into a few conventional types the resulting styles of device access have been found to be useful and broadly applicable although the exact system calls may differ across operating systems  the device categories are fairly standard the major access conventions include block i/0  character-stream i/0  memory-mapped file access  and network sockets operating systems also provide special system calls to access a few additional devices  such as a time-of-day clock and a timer  some operating systems provide a set of system calls for graphical display  video  and audio devices  most operating systems also have an  or that transparently passes arbitrary conunands from an application to a device driver in unix  this system call is ioctl    for i/0 control   the ioctl   system call enables an application to access any functionality that can be implernented by any device driver  without the need to invent a new system call the ioctl   system call has three arguments the first is a file descriptor that connects the application to the driver by referring to a hardware device managed by that driver the second is an integer that selects one of the commands implemented in the driver the third is a pointer to an arbitrary data structure in memory that enables the application and driver to communicate any necessary control information or data  13.3.1 block and character devices the captures all the aspects necessary for accessing disk drives and other block-oriented devices the device is expected to understand commands such as read   and write   ; if it is a random-access device  it is also expected to have a seek   command to specify which block to transfer next  568 chapter 13 applications normally access such a device through a file-system interface  we can see that read    write    and seek 0 capture the essen.tial behaviors of block-storage devices  so that applications are insulated from the low-level differences among those devices  the operating system itself  as well as special applications such as databasemanagement systems  may prefer to access a block device as a simple linear array of blocks this mode of access is sometimes called if the application performs its own buffering  then using a file systen1 would cause extra  unneeded buffering likewise  if an application provides its own locking of file blocks or regions  then any operating-system locking services would be redundant at the least and contradictory at the worst to avoid these conflicts  raw-device access passes control of the device directly to the application  letting the operating system step out of the way unfortunately  no operating-system services are then performed on this device a compromise that is becoming common is for the operating system to allow a mode of operation on a file that disables buffering and locking in the unix world  this is called memory-mapped file access can be layered on top of block-device drivers  rather than offering read and write operations  a memory-mapped interface provides access to disk storage via an array of bytes in main memory the system call that maps a file into memory returns the virtual memory address that contains a copy of the file the actual data transfers are performed only when needed to satisfy access to the memory image because the transfers are handled by the same mechanism as that used for demand-paged virtual memory access  memory-mapped i/o is efficient memory mapping is also convenient for programmers-access to a memory-mapped file is as simple as reading from and writing to memory operating systems that offer virtual memory commonly use the mapping interface for kernel services for instance  to execute a program  the operating system maps the executable into memory and then transfers control to the entry address of the executable the mapping interface is also commonly used for kernel access to swap space on disk  a keyboard is an example of a device that is accessed through a the basic system calls in this interface enable an application to get   or put   one character on top of this interface  libraries can be built that offer line-at-a-time access  with buffering and editing services  for example  when a user types a backspace  the preceding character is removed from the input stream   this style of access is convenient for input devices such as keyboards  mice  and modems that produce data for input spontaneously -that is  at times that cam1.ot necessarily be predicted by the application this access style is also good for output devices such as printers and audio boards  which naturally fit the concept of a linear stream of bytes  13.3.2 network devices because the performance and addressing characteristics of network i/0 differ significantly from those of disk i/0  most operating systems provide a network i/o interface that is different from the read   -write   -seek   interface used for disks one interface available in many operating systerns  including unix and windows nt  is the network interface  think of a wall socket for electricity  any electrical appliance can be plugged in by analogy  the system calls in the socket interface enable an application 13.3 569 to create a socket  to connect a local socket to a remote address  which plugs this application into a socket created by another application   to listen for any remote application to plug into the local socket  and to send and receive packets over the connection to support the implementation of servers  the socket interface also provides a function called select   that manages a set of sockets a call to select   returns information about which sockets have a packet waiting to be received and which sockets have room to accept a packet to be sent the use of select   eliminates the polling and busy waiting that would otherwise be necessary for network i/0 these functions encapsulate the essential behaviors of networks  greatly facilitating the creation of distributed applications that can use any underlying network hardware and protocol stack many other approaches to interprocess communication and network communication have been implemented for instance  windows nt provides one interface to the network interface card and a second interface to the network protocols  appendix c.6   in unix  which has a long history as a proving ground for network technology  we find half-duplex pipes  full-duplex fifos  full-duplex streams  message queues  and sockets information on unix networking is given in appendix a.9  13.3.3 clocks and timers most computers have hardware clocks and timers that provide three basic functions  give the current time  give the elapsed time  set a timer to trigger operation x at time t  these functions are used heavily by the operating system  as well as by timesensitive applications unfortunately  the system calls that implement these functions are not standardized across operating systems  the hardware to measure elapsed time and to trigger operations is called a  it can be set to wait a certain amount of time generate an interrupt  and it can be set to do this once or to repeat the process to generate periodic interrupts the scheduler uses this mechanism to generate an interrupt that will preempt a process at the end of its time slice  the disk i/o subsystem uses it to invoke the periodic flushing of dirty cache buffers to disk  and the network subsystem uses it to cancel operations that are proceeding too slowly because of network congestion or failures the operating system may also provide an interface for user processes to use timers the operating system can support more timer requests than the number of timer hardware chan11els by simulating virtual clocks to do so  the kernel  or the timer device driver  maintains a list of interrupts wanted by its own routines and by user requests  sorted in earliest-time-first order it sets the timer for the earliest tince when the timer interrupts  the kernel signals the requester and reloads the timer with the next earliest time  on many computers  the interrupt rate generated by the hardware clock is between 18 and 60 ticks per second this resolution is coarse  since a modern computer can execute hundreds of millions of instructions per second the 570 chapter 13 precision of triggers is limited by the coarse resolution of the timer  together with the overhead of maintaining virtual clocks furthermore  if the timer ticks are used to maintain the system time-of-day clock  the system clock can drift in most computers  the hardware clock is constructed from a highfrequency counter in some computers  the value of this counter can be read from a device register  in which case the counter can be considered a highresolution clock although this clock does not generate interrupts  it offers accurate measurements of time intervals  13.3.4 blocking and nonblocking 1/0 another aspect of the system-call interface relates to the choice between blocking i/0 and nonblocking i/0 when an application issues a system call  the execution of the application is suspended the application is moved from the operating system 's run queue to a wait queue after the system call completes  the application is moved back to the run queue  where it is eligible to resume execution when it resumes execution  it will receive the values returned by the system call the physical actions performed by i/0 devices are generally asynchronous-they take a varying or unpredictable amount of time nevertheless  most operating systems use blocking system calls for the application interface  because blocking application code is easier to understand than nonblocking application code  some user-level processes need i/0 one example is a user interface that receives keyboard and mouse input while processing and displaying data on the screen another example is a video application that reads frames from a file on disk while simultaneously decompressing and displaying the output on the display  one way an application writer can overlap execution with i/0 is to write a multithreaded application some threads can perform blocking system calls  while others continue executing the solaris developers used this technique to implement a user-level library for asynchronous i/0  freeing the application writer from that task some operating systems provide nonblocking i/0 system calls a nonblocking call does not halt the execution of the application for an extended time h1.stead  it returns quickly  with a return value that indicates how many bytes were transferred  an alternative to a nonblocking system call is an asynchronous system call an asynchronous call returns immediately  without waiting for the i/0 to complete the application continues to execute its code the completion of the i/0 at some future time is communicated to the application  either through the setting of some variable in the address space of the application or through the triggering of a signal or software interrupt or a call-back routine that is executed outside the linear control flow of the application the difference between nonblocking and asynchronous system calls is that a nonblocking read   returns immediately with whatever data are available-the full number of bytes requested  fewer  or none at all an asynchronous read   call requests a transfer that will be performed in its entirety but will complete at some future time these two i/0 methods are shown in figure 13.8  a good example of nonblocking behavior is the select   system call for network sockets this system call takes an argument that specifies a maximum waiting time by setting it to 0  an application can poll for network activity 13.4 13.4 571 kernel user user kernel  a   b  figure 13.8 two 1/0 methods   a  synchronous and  b  asynchronous  without blocking but using select   introduces extra overhead  because the select   call only checks whether i/0 is possible for a data transfer  select   must be followed by some kind of read   or write   command  a variation on this approach  fotmd in mach  is a blocking multiple-read call  it specifies desired reads for several devices in one system call and returns as soon as any one of them completes  kernels provide many services related to i/0 several services-scheduling  buffering  caching  spooling  device reservation  and error handlil1.g-are provided by the kernel 's i/0 subsystem and build on the hardware and devicedriver infrastructure the i/o subsystem is also responsible for protectil1.g itself from errant processes and malicious users  13.4.1 1/0 scheduling to schedule a set of i/o requests means to determine a good order in which to execute them the order in which applications issue system calls rarely is the best choice scheduling can improve overall system performance  can share device access fairly among processes  and can reduce the average waiting time for i/0 to complete here is a simple example to illustrate suppose that a disk arm is near the begilming of a disk and that three applications issue blocking read calls to that disk application 1 requests a block near the end of the disk  application 2 requests one near the beginning  and application 3 requests one in the middle of the disk the operating system can reduce the distance that the disk ann travels by serving the applications in the order 2  3  1 rearrangil1.g the order of service in this way is the essence of i/0 scheduling  operating-system developers implement scheduling by maintaining a wait queue of requests for each device when an application issues a blocking i/0 system call  the request is placed on the queue for that device the i/0 scheduler rearranges the order of the queue to improve the overall system efficiency and the average response time experienced by applications the operating 572 chapter 13 figure 13.9 device-status table  system may also try to be fair  so that no one application receives especially poor service  or it may give priority service for delay-sensitive requests for instance  requests from the virtual memory subsystem may take priority over application requests several scheduling algorithms for disk i/0 are detailed in section 12.4  when a kernel supports asynchronous i/0  it must be able to keep track of many i/0 requests at the same time for this purpose  the operating system might attach the wait queue to a  able the kernel manages this table  which contains an entry for each i/0 device  as shown in figure 13.9  each table entry indicates the device 's type  address  and state  not functioning  idle  or busy   if the device is busy with a request  the type of request and other parameters will be stored in the table entry for that device  one way in which the i/0 subsystem improves the efficiency of the computer is by scheduling i/0 operations another way is by using storage space in main memory or on disk via teclul.iques called buffering  caching  and spooling  13.4.2 buffering a is a memory area that stores data being transferred between two devices or between a device and an application buffering is done for three reasons one reason is to cope with a speed mismatch between the producer and consumer of a data stream suppose  for example  that a file is being received via modem for storage on the hard disk the modem is about a thousand times slower than the hard disk so a buffer is created in main mernory to accumulate the bytes received from the modem when an entire buffer of data has arrived  the buffer can be written to disk in a single operation since the disk write is not instantaneous and the modem still needs a place to store additional incoming data  two buffers are used after the modem fills the first buffer  the disk write is requested the modem then starts to fill the second buffer while the first buffer is written to disk by the time the modem has filled 13.4 573 the second buffer  the disk write from the first one should have completed  so the modem can switch back to the first buffer while the disk writes the second one this decouples the producer of data from the consun1.er  thus relaxing timing requirements between them the need for this decoupling is illustrated in figure 13.10  which lists the enormous differences in device speeds for typical computer hardware  a second use of buffering is to provide adaptations for devices that have different data-transfer sizes such disparities are especially common in computer networking  where buffers are used widely for fragmentation and reassembly of messages at the sending side  a large message is fragmented into small network packets the packets are sent over the network  and the receiving side places them in a reassembly buffer to form an image of the source data  a third use of buffering is to support copy semantics for application i/0  an example will clarify the meaning of copy semantics suppose that an application has a buffer of data that it wishes to write to disk it calls the write   systemcalt providing a pointer to the buffer and an integer specifying the number of bytes to write after the system call returns  what happens if the application changes the contents of the buffer with the version of the data written to disk is guaranteed to be version at the time of the application system calt independent of any subsequent changes in the application 's buffer a simple way in which the operating system can guarantee copy semantics is for the write   system call to copy the application i system bus hype  ~ ransport  32,pair  ~ ~ ~ iii ~ ~ ~ ~ ~ ~ ~ ~ i pci ~ xpress 2.0  32  i lnfi ! l.i band  qdr ; .1 2x  0.00001 0.001 0.1 10 1000 100000 1 efigure 13.10 sun enterprise 6000 device-transfer rates  logarithmic   574 chapter 13 data into a kernel buffer before returning control to the application the disk write is performed from the kernel buffer  so that subsequent changes to the application buffer have no effect copying of data between kernel buffers and application data space is common in operating systems  despite the overhead that this operation introduces  because of the clean semantics the same effect can be obtained more efficiently by clever use of virtual memory mapping and copy-on-write page protection  13.4.3 caching a is a region of fast memory that holds copies of data access to the cached copy is more efficient than access to the original for instance  the instructions of the currently running process are stored on disk  cached ilc physical memory  and copied again ill the cpu 's secondary and primary caches the difference between a buffer and a cache is that a buffer may hold the only existing copy of a data item  whereas a cache  by definition  holds a copy on faster storage of an item that resides elsewhere  caching and buffering are distinct functions  but sometinces a region of memory can be used for both purposes for illstance  to preserve copy semantics and to enable efficient scheduling of disk i/0  the operating system uses buffers in maill memory to hold disk data these buffers are also used as a cache  to improve the i/o efficiency for files that are shared by applications or that are being written and reread rapidly when the kernel receives a file i/0 request  the kernel first accesses the buffer cache to see whether that region of the file is already available in main memory if it is  a physical disk i/o can be avoided or deferred also  disk writes are accumulated ill the buffer cache for several seconds  so that large transfers are gathered to allow efficient write schedules this strategy of delayilcg writes to improve i/o efficiency is discussed  in the context of remote file access  ill section 17.3  13.4.4 spooling and device reservation a is a buffer that holds output for a device  such as a printer  that can not accept ilcterleaved data streams although a prillter can serve only one job at a time  several applications may wish to print their output concurrently  without having their output mixed together the operating system solves this problem by intercepting all output to the printer each application 's output is spooled to a separate disk file when an application finishes printing  the spooling system queues the correspondilcg spool file for output to the printer  the spooling system copies the queued spool files to the printer one at a time in some operating systems  spooling is managed by a system daemon process in others  it is handled by an in-kernel thread in either case  the operating system provides a control interface that enables users and system administrators to display the queue  remove unwanted jobs before those jobs print  suspend printing while the printer is serviced  and so on  some devices  such as tape drives and printers  can not usefully multiplex the i/0 requests of multiple concurrent applications spooling is one way operating systems can coordinate concurrent output another way to deal with concurrent device access is to provide explicit facilities for coordination some operating systems  including vms  provide support for exclusive device access by enabling a process to allocate an idle device and to deallocate that device 13.4 575 when it is no longer needed other operating systems enforce a limit of one open file handle to such a device many operating systems provide functions that enable processes to coordinate exclusive access among then'lselves for instance  windows nt provides system calls to wait until a device object becomes available it also has a parameter to the open   system call that declares the types of access to be permitted to other concurrent threads on these systems  it is up to the applications to avoid deadlock  13.4.5 error handling an operating system that uses protected memory can guard against many kinds of hardware and application errors  so that a complete system failure is not the usual result of each minor mechanical glitch devices and i/0 transfers can fail in many ways  either for transient reasons  as when a network becomes overloaded  or for permanent reasons  as when a disk controller becomes defective operating systems can often compensate effectively for transient failures for instance  a disk read   failure results in a read   retry  and a network send   error results in a res end    if the protocol so specifies  unfortunately  if an important component experiences a permanent failure  the operating system is unlikely to recover  as a general rule  an i/0 system call will return one bit of information about the status of the call  signifying either success or failure in the unix operating system  an additional integer variable named errno is used to return an error code-one of about a hundred values-indicating the general nature of the failure  for example  argument out of range  bad pointer  or file not open   by contrast  some hardware can provide highly detailed error information  although many current operating systems are not designed to convey this information to the application for instance  a failure of a scsi device is reported by the scsi protocol in three levels of detail  a key that identifies the general nature of the failure  such as a hardware error or an illegal request ; an that states the category of failure  such as a bad command parameter or a self-test failure ; and an 'x ' l '.l that gives even more detail  such as which command parameter was in error or which hardware subsystem failed its self-test further  many scsi devices maintain internal pages of error-log information that can be requested by the host-but seldom are  13.4.6 1/0 protection errors are closely related to the issue of protection a user process may accidentally or purposely attempt to disrupt the normal operation of a systern by attempting to issue illegal i/0 instructions we can use various mechanisms to ensure that such disruptions cam'lot take place in the system  to prevent users from performing illegal i/0  we define all i/0 instructions to be privileged instructions thus  users can not issue i/o instructions directly ; they must do it through the operating system to do i/0  a user program executes a system call to request that the operating system perform i/0 on its behalf  figure 13.11   the operating system  executing in monitor mode  checks that the request is valid and  if it is  does the i/0 requested the operating system then returns to the user  576 chapter 13 cd trap to monitor kernel perform 1/0 return to user user program figure 13.1 1 use of a system call to perform 1/0  in addition  any memory-mapped and i/o port memory locations must be protected from user access by the memory-protection system note that a kernel can not simply deny all user access most graphics games and video editing and playback software need direct access to memory-mapped graphics controller memory to speed the performance of the graphics  for example the kernel might in this case provide a locking mechanism to allow a section of graphics memory  representing a window on screen  to be allocated to one process at a time  13.4.7 kernel data structures the kernel needs to keep state information about the use of i/0 components  it does so through a variety of in-kernel data structures  such as the open-file table structure from section 11.1 the kernel uses many similar structures to track network connections  character-device communications  and other i/0 activities  unix provides file-system access to a variety of entities  such as user files  raw devices  and the address spaces of processes although each of these entities supports a read   operation  the semantics differ for instance  to read a user file  the kernel needs to probe the buffer cache before deciding whether to perform a disk i/0 to read a raw disk  the kernel needs to ensure that the request size is a multiple of the disk sector size and is aligned on a sector boundary to read a process image  it is merely necessary to copy data from memory unix encapsulates these differences within a uniform structure by using an object-oriented teclucique the open-file record  shown in 13.4 577 system-wide open-file table 1 ;  ;       ;  ' ; t ; . ' file-system record 1  ~  s  1 ~ if ~    inode pointer +   ;       ' pointer to read and write functions i      i  ; .l  pointer to select function ; ;  ;    ;  ~ ti ~ ~ ~ ~ ~ ple pointer to ioctl function file descriptor .,_   ;  \ ' '  ' ' pointer to close function n   .r              r ; i ~ ~ ~  ~ ~ ~ kl  f user-process memory networking  socket  record i  ' ~ 1 ~ t ~ 6 ~ ! v'.'  pointer to network info + f   ;    pointer to read and write.functions     ~   pointer to select function pointer to ioctl function pointer to close f..un ction  kernel memory figure 13 12 unix 1/0 kernel structure  figure 13.12  contains a dispatch table that holds pointers to the appropriate routines  depending on the type of file  some operating systems use object-oriented methods even more extensively  for instance  windows nt uses a message-passing implementation for i/0 an i/0 request is converted into a message that is sent through the kernel to the ii 0 manager and then to the device driver  each of which may change the message contents for output  the message contains the data to be written for input  the message contains a buffer to receive the data the message-passing approach can add overhead  by comparison with procedural techniques that use shared data structures  but it simplifies the structure and design of the i/0 system and adds flexibility  13.4.8 kernel i/o subsystem summary in summary  the i/0 subsystem coordinates an extensive collection of services that are available to applications and to other parts of the kernel the i/0 subsystenc supervises these procedures  management of the name space for files and devices access control to files and devices operation control  for example  a modem can not seek    file-system space allocation device allocation 578 chapter 13 13.5 buffering  caching  and spooling i/0 scheduling device-status monitoring  error handling  and failure recovery device-driver configuration and initialization the upper levels of the i/o subsystem access devices via the uniform interface provided by the device drivers  earlier  we described the handshaking between a device driver and a device controller  but we did not explain how the operating system connects an application request to a set of network wires or to a specific disk sector  consider  for example  reading a file from disk the application refers to the data by a file name within a disk  the file system maps from the file name through the file-system directories to obtain the space allocation of the file for instance  in ms-dos  the name maps to a number that indicates an entry in the file-access table  and that table entry tells which disk blocks are allocated to the file in unix  the name maps to an inode number  and the corresponding inode contains the space-allocation information but how is the connection made from the file name to the disk controller  the hardware port address or the memory-mapped controller registers  one method is that used by ms-dos  a relatively simple operating system  the first part of an ms-dos file name  preceding the colon  is a string that identifies a specific hardware device for example  c  is the first part of every file name on the primary hard disk the fact that c  represents the primary hard disk is built into the operating system ; c  is mapped to a specific port address through a device table because of the colon separator  the device name space is separate from the file-system name space this separation makes it easy for the operating system to associate extra functionality with each device for instance  it is easy to invoke spooling on any files written to the printer  if  instead  the device name space is incorporated in the regular file-system name space  as it is in unix  the normal file-system name services are provided automatically if the file system provides ownership and access control to all file names  then devices have owners and access control since files are stored on devices  such an interface provides access to the i/o system at two levels  names can be used to access the devices themselves or to access the files stored on the devices  unix represents device names in the regular file-system name space unlike an ms-dos file name  which has a colon separator  a unix path name has no clear separation of the device portion in fact  no part of the path name is the name of a device unix has a that associates prefixes of path names with specific device names to resolve a path name  unix looks up the name in the mount table to find the longest ncatchilcg prefix ; the corresponding entry in the mount table gives the device name this device name also has the form of a name in the file-system name space when unix looks up this name in the file-system directory structures  it finds not an inode number but a major  13.5 579 minor device number the m.ajor device number identifies a device driver that should be called to handle l/0 to this device the minor device number is passed to the device driver to index into a device table the corresponding device-table entry gives the port address or the memory-mapped address of the device controller  modern operating systems obtain significant flexibility from the multiple stages of lookup tables in the path between a request and a physical device controller the mechanisms that pass requests between applications and drivers are general thus  we can introduce new devices and drivers into a computer without recompiling the kernel in fact  some operating systems have the ability to load device drivers on demand at boot time  the system first probes the hardware buses to determine what devices are present ; it then loads in the necessary drivers  either immediately or when first required by an i/0 request  we next describe the typical life cycle of a blocking read request  as depicted in figure 13.13 the figure suggests that an i/0 operation requires a great many steps that together consume a tremendous number of cpu cycles  a process issues a blocking read   system call to a file descriptor of a file that has been opened previously  the system-call code in the kernel checks the parameters for correctness  in the case of input  if the data are already available irl the buffer cache  the data are returned to the process  and the i/o request is completed  otherwise  a physical i/0 must be performed the process is removed from the run queue and is placed on the wait queue for the device  and the i/0 request is scheduled eventually  the i/0 subsystem sends the request to the device driver depending on the operating system  the request is sent via a subroutine call or an in-kernel message  the device driver allocates kernel buffer space to receive the data and schedules the i/0 eventually  the driver sends commands to the device controller by writing into the device-control registers  the device controller operates the device hardware to perform the data transfer  the driver may poll for status and data  or it may have set up a dma transfer into kernel memory we assume that the transfer is managed by a dma controller  which generates an interrupt when the transfer completes  the correct interrupt handler receives the interrupt via the interruptvector table  stores any necessary data  signals the device driver  and returns from the interrupt  the device driver receives the signal  determines which i/0 request has completed  determines the request 's status  and signals the kernel i/0 subsystem that the request has been completed  the kernel transfers data or return codes to the address space of the requesting process and moves the process from the wait queue back to the ready queue  580 chapter 13 13.6 system call device-controller commands user process kernel 1/0 subsystem kernel 1/0 subsystem device driver interrupt handler device controller return from system call interrupt ~ -------tim_e ~  ~   figure 13.13 the life cycle of an 1/0 request  moving the process to the ready queue unblocks the process when the scheduler assigns the process to the cpu  the process resumes execution at the completion of the system call  unix system v has an interesting mechanism  called that enables an application to assemble pipelines of driver code dynamically a stream is a full-duplex connection between a device driver and a user-level process it consists of a that interfaces with the user process  a  id that controls the device  and zero or more between the stream user process 13.6 i streams modules _j figure 13.14 the streams structure  581 head and the driver end each of these components contains a pair of queues -a read queue and a write queue message passing is used to transfer data between queues the streams structure is shown in figure 13.14  modules provide the functionality of streams processing ; they are pushed onto a stream by use of the ioctl   system call for examplef a process can open a serial-port device via a stream and can push on a module to handle input editing because messages are exchanged between queues in adjacent modules  a queue in one module may overflow an adjacent queue to prevent this from occurring  a queue may support without flow control  a queue accepts all messages and immediately sends them on to the queue in the adjacent module without buffering them a queue supporting flow control buffers messages and does not accept messages without sufficient buffer space ; this process involves exchanges of control messages between queues in adjacent modules  a user process writes data to a device using either the write   orputmsg   system call the write   system call writes raw data to the stream  whereas putmsg   allows the user process to specify a message regardless of the system call used by the user process  the stream head copies the data into a message and delivers it to the queue for the next module in line this copying of messages continues until the message is copied to the driver end and hence the device similarly  the user process reads data from the stream head using either the read   or getmsg   system call if read   is used  the stream head gets a message from its adjacent queue and returns ordinary data  an unstructured byte stream  to the process if getmsg   is used  a message is returned to the process  582 chapter 13 13.7 streams i/0 is asynchronous  or nonblocking  except when the user process communicates with the stream ~ head when writing to the stream  the user process will block  assuming the next queue uses flow controt until there is room to copy the message likewise  the user process will block when reading from the stream ~ until data are available  as mentioned  the driver end-like the stream head and modules-has a read and write queue however  the driver end must respond to interrupts  such as one triggered when a frame is ready to be read from a network unlike the stream head  which may block if it is unable to copy a message to the next queue in line  the driver end must handle all incoming data drivers must support flow control as well however  if a device 's buffer is fult the device typically resorts to dropping incoming messages consider a network card whose input buffer is full the network card must simply drop further messages until there is ample buffer space to store incoming messages  the benefit of using streams is that it provides a framework for a modular and incremental approach to writing device drivers and network protocols modules may be used by different streams and hence by different devices for example  a networking module may be used by both an ethernet network card and a 802.11 wireless network card furthermore  rather than treating character-device i/o as an unstructured byte stream  streams allows support for message boundaries and control information when communicating between modules most unix variants support streams  and it is the preferred method for writing protocols and device drivers for example  system v unix and solaris implement the socket mechanism using streams  i/ 0 is a major factor in system performance it places heavy demands on the cpu to execute device-driver code and to schedule processes fairly and efficiently as they block and unblock the resulting context switches stress the cpu and its hardware caches i/o also exposes any inefficiencies in the interrupt-handling mechanisms in the kernel in addition  i/o loads down the memory bus during data copies between controllers and physical memory and again durilcg copies between kernel buffers and application data space coping gracefully with all these demands is one of the major concerns of a computer architect  although modern computers can handle many thousands of interrupts per second  interrupt handling is a relatively expensive task each interrupt causes the system to perform a state change  to execute the interrupt handler  and then to restore state programmed i/0 can be more efficient than internjpt-driven i/0  if the number of cycles spent in busy waiting is not excessive an i/0 completion typically unblocks a process  leading to the full overhead of a context switch  network traffic can also cause a high context-switch rate consider  for instance  a remote login from one machine to another each character typed on the local machine must be transported to the remote machine on the local machine  the character is typed ; a keyboard interrupt is generated ; and the character is passed through the interrupt handler to the device driver  to the kernet and then to the user process the user process issues a network i/o system call to send the character to the remote machine the character then 13.7 583 flows into the local kernel  through the network layers that construct a network packet  and into the network device driver the network device driver transfers the packet to the network controller  which sends the character and generates an interrupt the interrupt is passed back up through the kernel to cause the network l/0 system call to complete  now  the remote system 's network hardware receives the packet  and an interrupt is generated the character is unpacked from the network protocols and is given to the appropriate network daemon the network daemon identifies which remote login session is involved and passes the packet to the appropriate subdaemon for that session throughout this flow  there are context switches and state switches  figure 13.15   usually  the receiver echoes the character back to the sender ; that approach doubles the work  to eliminate the context switches involved in moving each character between daemons and the kernel  the solaris developers reimplemented the daemon using in-kernel threads sun estimates that this improvement sending system receiving system figure 13.15 lntercomputer communications  584 chapter 13 increased the maximum number of network logins from a few hundred to a few thousand on a large server  other systems use separate for terminal i/0 to reduce the interrupt burden on the main cpu for instance  a can multiplex the traffic from hundreds of remote terminals into one port on a large computer an is a dedicated  special-purpose cpu found in mainframes and in other high-end systems the job o a channel is to offload i/0 work from the main cpu the idea is that the cham1.els keep the data flowing smoothly  while the main cpu remains free to process the data like the device controllers and dma controllers found in smaller computers  a channel can process more general and sophisticated programs  so channels can be tuned for particular workloads  we can employ several principles to improve the efficiency of i/0  reduce the number of context switches  reduce the number of times that data must be copied in memory while passing between device and application  reduce the frequency of interrupts by using large transfers  smart controllers  and polling  if busy waiting can be minimized   increase concurrency by using dma-knowledgeable controllers or channels to offload simple data copying from the cpu  move processing primitives into hardware  to allow their operation in device controllers to be concurrent with cpu and bus operation  balance cpu  memory subsystem  bus  and i/o performance  because an overload in any one area will cause idleness in others  i/0 devices vary greatly in complexity for instance  a mouse is simple the mouse movements and button clicks are converted into numeric values that are passed from hardware  through the mouse device driver  to the application by contrast  the functionality provided by the windows nt disk device driver is complex it not only manages individual disks but also implements raid arrays  section 12.7   to do so  it converts an application 's read or write request into a coordinated set of disk i/0 operations moreover  it implements sophisticated error-handling and data-recovery algorithms and takes many steps to optimize disk performance  where should the i/0 functionality be implemented -in the device hardware  in the device driver  or in application software sometimes we observe the progression depicted in figure 13.16  initially  we implement experimental i/0 algorithms at the application level  because application code is f1exible and application bugs are unlikely to cause system crashes furthermore  by developing code at the application level  we avoid the need to reboot or reload device drivers after every change to the code an application-level implementation can be inefficient  however  because of the overhead o context switches and because the application can not take advantage of internal kernel data structures and 13.8 13.8 585 device code  hardware  figure 13.16 device functionality progression  kernel functionality  such as efficient in-kernel messaging  threading  and locking   when an application-level algorithm has demonstrated its worth  we may reimplement it in the kernel this can improve performance  but the development effort is more challenging  because an operating-system kernel is a large  complex software system moreover  an in-kernel implementation must be thoroughly debugged to avoid data corruption and system crashes  the highest performance may be obtained through a specialized implementation in hardware  either in the device or in the controller the disadvantages of a hardware implementation include the difficulty and expense of making further improvements or of fixing bugs  the increased development time  months rather than days   and the decreased flexibility  for instance  a hardware raid controller may not provide any means for the kernel to influence the order or location of individual block reads and writes  even if the kernel has special information about the workload that would enable it to improve the i/0 performance  the basic hardware elements involved in i/0 are buses  device controllers  and the devices themselves the work of moving data between devices and main memory is perform.ed by the cpu as programmed i/0 or is offloaded to a dma controller the kernel module that controls a device is a device driver the system-call interface provided to applications is designed to handle several basic categories of hardware  including block devices  character devices  memory-mapped files  network sockets  and programmed interval timers the system calls usually block the processes that issue them  but nonblocking and 586 chapter 13 asynchronous calls are used by the kernel itself and by applications that must not sleep while waiting for an i/0 operation to complete  the kernel 's i/o subsystem provides num.erous services among these are i/0 scheduling  buffering  caching  spooling  device reservation  and error handling another service  name translation  makes the connections between hardware devices and the symbolic file names used by applications it involves several levels of mapping that translate from character-string names  to specific device drivers and device addresses  and then to physical addresses of ii 0 ports or bus controllers this mapping may occur within the file-system name space  as it does in unix  or in a separate device name space  as it does in ms-dos  streams is an implementation and methodology that provides a framework for a modular and incremental approach to writing device drivers and network protocols through streams  drivers can be stacked  with data passing through them sequentially and bidirectionally for processing  i/o system calls are costly in terms of cpu consumption because of the many layers of software between a physical device and an application these layers imply overhead from several sources  context switching to cross the kernel 's protection boundary  signal and interrupt handling to service the i/0 devices  and the load on the cpu and memory system to copy data between kernel buffers and application space  13.1 write  in pseudocode  an implementation of virtual clocks  including the queueing and management of timer requests for the kernel and applications assume that the hardware provides three timer channels  13.2 what are the advantages and disadvantages of supporting memorymapped i/0 to device control registers 13.3 typically  at the completion of a device i/0  a single interrupt is raised and appropriately handled by the host processor in certain settings  however  the code that is to be executed at the completion of the i/0 can be broken into two separate pieces the first piece executes immediately after the i/0 completes and schedules a second interrupt for the remaining piece of code to be executed at a later time what is the purpose of using this strategy in the design of interrupt handlers 13.4 why might a system use interrupt-driven i/0 to manage a single serial port and polling i/0 to manage a front-end processor  such as a termii1.al concentrator 13.5 what are the various kinds of performance overhead associated with servicing an interrupt 13.6 unix coordinates the activities of the kernel i/0 components by manipulating shared in-kernel data structures  whereas windows nt uses object-oriented message passing between kernel i/o components  discuss three pros and three cons of each approach  587 13.7 in most multiprogrammed systems  user programs access memory through virtual addresses  while the operating system uses raw physical addresses to access men10ry what are the implications of this design for the initiation of i/0 operations by the user program and their execution by the operating system 13.8 polling for an i/0 completion can waste a large number of cpu cycles if the processor iterates a busy-waiting loop many times before the i/0 completes but if the i/0 device is ready for service  polling can be much more efficient than is catching and dispatching an interrupt describe a hybrid strategy that combines polling  sleeping  and interrupts for i/0 device service for each of these three strategies  pure polling  pure interrupts  hybrid   describe a computing environment in which that strategy is more efficient than is either of the others  13.9 consider the following i/0 scenarios on a single-user pc  a a mouse used with a graphical user interface b a tape drive on a multitasking operating system  with no device preallocation available  c a disk drive containing user files d a graphics card with direct bus connection  accessible through memory-mapped i/0 for each of these scenarios  would you design the operating system to use buffering  spooling  caching  or a combin_ation would you use polled i/o or interrupt-driven i/0 give reasons for your choices  13.10 the example of handshaking in section 13.2 used 2 bits  a busy bit and a command-ready bit is it possible to implement this handshaking with only 1 bit if it is  describe the protocol if it is not  explain why 1 bit is insufficient  13.11 discuss the advantages and disadvantages of guaranteeing reliable transfer of data between modules in the streams abstraction  13.12 some dma controllers support direct virtual memory access  where the targets of i/0 operations are specified as virtual addresses and a translation from virtual to physical address is performed during the dma how does this design complicate the design of the dma controller what are the advantages of providing such functionality 13.13 why is it important to scale up system-bus and device speeds as cpu speed increases 13.14 when multiple interrupts from different devices appear at about the same time  a priority scheme could be used to determine the order in which the interrupts would be serviced discuss what issues need to be considered in assigning priorities to different interrupts  588 chapter 13 13.15 describe three circumstances under which blocking ii 0 should be used  describe three circumstances under which nonblocking i/0 should be used why not just implement nonblocking i/0 and have processes busy-wait until their devices are ready vahalia  1996  provides a good overview of i/o and networking in unix  leffler et al  1989  detail the i/o structures and methods employed in bsd unix milenkovic  1987  discusses the complexity of i/0 methods and implementation the use and programming of the various interprocesscommunication and network protocols in unix are explored in stevens  1992   brain  1996  documents the windows nt application interface the i/o implementation in the sample minix operating system is described in tanenbaum and woodhull  1997   custer  1994  includes detailed information on the nt message-passing implementation of i/0  for details of hardware-level ii 0 handling and memory-mapping functionality  processor reference manuals  motorola  1993  and intel  1993   are among the best sources hennessy and patterson  2002  describe multiprocessor systems and cache-consistency issues tanenbaum  1990  describes hardware i/0 design at a low level  and sargent and shoemaker  1995  provide a programmer 's guide to low-level pc hardware and software the ibm pc device i/o address map is given in ibm  1983   the march 1994 issue of ieee computer is devoted to i/0 hardware and software raga  1993  provides a good discussion of streams  part six protection mechanisms control access to a system by limiting the types of file access permitted to users in addition  protection must ensure that only processes that have gained proper authorization from the operating system can operate on memory segments  the cpu  and other resources  protection is provided by a mechanism that controls the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means for specifying the controls to be imposed  together with a means of enforcing them  security ensures the authentication of system users to protect the integrity of the information stored in the system  both data and code   as well as the physical resources of the computer system the security system prevents unauthorized access  malicious destruction 01 alteration of data  and accidental introduction of inconsistency  14.1 chapter the processes in an operating system must be protected from one another 's activities to provide such protection  we can use various mechanisms to ensure that only processes that have gained proper authorization from the operating system can operate on the files  memory segments  cpu  and other resources of a system  protection refers to a mechanism for controlling the access of programs  processes  or users to the resources defined by a computer system this mechanism must provide a means for specifying the controls to be imposed  together with a means of enforcement we distinguish between protection and security  which is a measure of confidence that the integrity of a system and its data will be preserved in this chapter  we focus on protection security assurance is a much broader topic  and we address it in chapter 15  to discuss the goals and principles of protection in a modern computer system  to explain how protection domains  combined with an access matrix  are used to specify the resources a process may access  to examine capability and language-based protection systems  as computer systems have become more sophisticated and pervasive in their applications  the need to protect their integrity has also grown protection was originally conceived as an adjunct to multiprogramming operating systems  so that untrustworthy users might safely share a common logical name space  such as a directory of files  or share a common physical name space  such as memory modern protection concepts have evolved to increase the reliability of any complex system that makes use of shared resources  we need to provide protection for several reasons the most obvious is the need to prevent the mischievous  intentional violation of an access restriction 591 592 chapter 14 14.2 by a user of more general importance  however  is the need to ensure that each program component active in a system uses system resources only in ways consistent with stated policies this requirement is an absolute one for a reliable system  protection can improve reliability by detecting latent errors at the interfaces between component subsystems early detection of interface errors can often prevent contamination of a healthy subsystem by a malfunctioning subsystem  also  an unprotected resource can not defend against use  or misuse  by an unauthorized or incompetent user a protection-oriented system provides means to distinguish between authorized and unauthorized usage  the role of protection in a computer system is to provide a mechanism for the enforcement of the policies governing resource use these policies can be established in a variety of ways some are fixed in the design of the system  while others are formulated by the management of a system still others are defined by the individual users to protect their own files and programs a protection system must have the flexibility to enforce a variety of policies  policies for resource use may vary by application  and they may change over time for these reasons  protection is no longer the concern solely of the designer of an operating system the application programmer needs to use protection mechanisms as well  to guard resources created and supported by an application subsystem against misuse in this chapter  we describe the protection mechanisms the operating system should provide  but application designers can use them as well in designing their own protection software  note that mechanisms are distinct from policies mechanisms determine how something will be done ; policies decide what will be done the separation of policy and mechanism is important for flexibility policies are likely to change from place to place or time to time in the worst case  every change in policy would require a change in the underlying mechanism using general mechanisms enables us to avoid such a situation  frequently  a guiding principle can be used throughout a project  such as the design of an operating system following this principle simplifies design decisions and keeps the system consistent and easy to understand a key  time-tested guiding principle for protection is the it dictates that programs  users  and even systems be given just enough privileges to perform their tasks  consider the analogy of a security guard with a passkey if this key allows the guard into just the public areas that she guards  then misuse of the key will result in minimal damage if  however  the passkey allows access to all areas  then damage from its being lost  stolen  misused  copied  or otherwise compromised will be much greater  an operating system following the principle of least privilege implements its features  programs  system calls  and data structures so that failure or compromise of a component does the minimum damage and allows the n1inimum damage to be done the overflow of a buffer in a system daemon might cause the daemon process to fail  for example  but should not allow the execution of code from the daemon process 's stack that would enable a remote 14.3 14.3 593 user to gain maximum privileges and access to the entire system  as happens too often today   such an operating system also provides system calls and services that allow applications to be written with fine-grained access controls it provides mechanisms to enable privileges when they are needed and to disable them when they are not needed also beneficial is the creation of audit trails for all privileged function access the audit trail allows the prograrnmer  systems administrator  or law-enforcement officer to trace all protection and security activities on the system  managing users with the principle of least privilege entails creating a separate account for each user  with just the privileges that the user needs an operator who needs to mount tapes and back up files on the system has access to just those commands and files needed to accomplish the job some systems implement role-based access control  rbac  to provide this functionality  computers implemented in a computing facility under the principle of least privilege can be limited to running specific services  accessing specific remote hosts via specific services  and doing so during specific times typically  these restrictions are implemented through enabling or disabling each service and through using access control lists  as described in sections 10.6.2 and 14.6  the principle of least privilege can help produce a more secure computing environment unfortunately  it frequently does not for example  windows 2000 has a complex protection scheme at its core and yet has many security holes by comparison  solaris is considered relatively secure  even though it is a variant of unix  which historically was designed with little protection in mind one reason for the difference may be that windows 2000 has more lines of code and more services than solaris and thus has more to secure and protect another reason could be that the protection scheme in windows 2000 is irtcomplete or protects the wrong aspects of the operating system  leaving other areas vulnerable  a computer system is a collection of processes and objects by objects  we mean both  such as the cpu  memory segments  printers  disks  and tape drives  and  such as files  programs  and semaphores   each object has a unique name that differentiates it from all other objects in the system  and each can be accessed only through well-defined and meaningful operations objects are essentially abstract data types  the operations that are possible may depend on the object for example  on a cpu  we can only execute memory segments can be read and written  whereas a cd-rom or dvd-rom can only be read tape drives can be read  written  and rewound data files can be created  opened  read  written  closed  and deleted ; program files can be read  written  executed  and deleted  a process should be allowed to access only those resources for which it has authorization furthermore  at any time  a process should be able to access only those reso1jrces that it currently reqllires to complete its task this second requirement  conunonly referred to as the need-to-know principle  is useful in limiting the amount of damage a faulty process can cause in the system for example  when process p invokes procedure a    the procedure should be 594 chapter14 allowed to access only its own variables and the formal parameters passed to it ; it should not be able to access all the variables of process p similarly  consider the case in which process p invokes a compiler to compile a particular file the compiler should not be able to access files arbitrarily but should have access only to a well-defined subset of files  such as the source file  listing file  and so on  related to the file to be compiled conversely  the compiler may have private files used for accounting or optimization purposes that process p should not be able to access the need-to-know principle is similar to the principle of least privilege discussed in section 14.2 in that the goals of protection are to minimize the risks of possible security violations  14.3.1 domain structure to facilitate the scheme just described  a process operates within a which specifies the resources that the process may access each domain defines a set of objects and the types of operations that may be invoked on each object the ability to execute an operation on an object is an a domain is a collection of access rights  each of which is an ordered pair object-name  rights-set  for example  if domain d has the access right file f   read  write   then a process executing in domain d can both read and write file f ; it can not  however  perform any other operation on that object  domains do not need to be disjoint ; they may share access rights for example  in figure 14.1  we have three domains  d1  d2  and d3  the access right 0 4   print  is shared by d2 and d3  implying that a process executing in either of these two domains can print object 0 4  note that a process must be executing in domain d1 to read and write object 0 1  while only processes in domain d3 may execute object 0 1  the association between a process and a domain may be either if the set of resources available to the process is fixed throughout the process 's lifetime  or as might be expected  establishing dynamic protection domains is more complicated than establishing static protection domains  if the association between processes and domains is fixed  and we want to adhere to the need-to-know principle  then a mechanism must be available to change the content of a domain the reason stems from the fact that a process may execute in two different phases and may  for example  need read access in one phase and write access in another if a domain is static  we must define the domain to include both read and write access however  this arrangement provides more rights than are needed in each of the two phases  since we have read access in the phase where we need only write access  and vice versa  thus  the need-to-know principle is violated we must allow the contents of 0 3   read  write  0 1  read  write  0 2   execute  0 2   write  01   execute  0 3   read  figure 14.1 system with three protection domains  14.3 595 a domain to be modified so that the domain always reflects the n1inimum necessary access rights  if the association is dynamic  a mechanism is available to allow enabling the process to switch from one domain to another we may also want to allow the content of a domain to be changed if we can not change the content of a domain  we can provide the same effect by creating a new domain with the changed content and switching to that new domain when we want to change the domain content  a domain can be realized in a variety of ways  each user may be a domain in this case  the set of objects that can be accessed depends on the identity of the user domain switching occurs when the user is changed -generally when one user logs out and another user logs in  each process may be a domain in this case  the set of objects that can be accessed depends on the identity of the process domain switching occurs when one process sends a message to another process and then waits for a response  each procedure may be a domain in this case  the set of objects that can be accessed corresponds to the local variables defined within the procedure  domain switching occurs when a procedure call is made  we discuss domain switching in greater detail in section 14.4  consider the standard dual-mode  monitor-user mode  model of operating-system execution when a process executes in monitor mode  it can execute privileged instructions and thus gain complete control of the computer system in contrast  when a process executes in user mode  it can invoke only nonprivileged instructions consequently  it can execute only within its predefined memory space these two modes protect the operating system  executing in monitor domain  from the user processes  executing in user domain   in a multiprogrammed operating system  two protection domains are insufficient  since users also want to be protected from one another therefore  a more elaborate scheme is needed we illustrate such a scheme by examining two influential operating systems-unix and multics -to see how they implement these concepts  14.3.2 an example  unix in the unix operating system  a domain is associated with the user switching the domain corresponds to changing the user identification temporarily  this change is accomplished tbough the file system as follows an owner identification and a domain bit  known as the setuid bit  are associated with each file when the setuid bit is on  and a user executes that file  the user id is set to that of the owner of the file ; when the bit is off  however  the user id does not change for example  when a user a  that is  a user with userid = a  starts executing a file owned by b  whose associated domain bit is off  the userid of the process is set to a when the setuid bit is on  the userid is set to that of the owner of the file  b when the process exits  this temporary userid change ends  596 chapter 14 other methods are used to change domains in operating systems in which user ids are used for domain definition  because almost all systems need to provide such a mechanism this mechanism is used when an otherwise privileged facility needs to be made available to the general user population  for instance  it might be desirable to allow users to access a network without letting them write their own networking programs in such a case  on a unix system  the setuid bit on a networking program would be set  causing the user ld to change when the program was run the user ld would change to that of a user with network access privilege  such as root  the most powerful user id   one problem with this method is that if a user manages to create a file with user id root and with its setuid bit on  that user can become root and do anything and everything on the system the setuid mechanism is discussed further in appendix a  an alternative to this method used in other operating systems is to place privileged programs in a special directory the operating system would be designed to change the user ld of any program run from this directory  either to the equivalent of root or to the user ld of the owner of the directory this eliminates one security problem with setuid programs in which crackers create and hide such programs for later use  using obscure file or directory names   this method is less flexible than that used in unix  however  even more restrictive  and thus more protective  are systems that simply do not allow a change of user id in these instances  special techniques must be used to allow users access to privileged facilities for instance  a may be started at boot time and run as a special user id users then run a separate program  which sends requests to this process whenever they need to use the facility this method is used by the tops-20 operating system  in any of these systems  great care must be taken in writing privileged programs any oversight can result in a total lack of protection on the system  generally  these programs are the first to be attacked by people trying to break into a system ; unfortunately  the attackers are frequently successful  for example  security has been breached on many unix systems because of the setuid feature we discuss security in chapter 15  14.3.3 an example  mul tics in the multics system  the protection domains are organized hierarchically into a ring structure each ring corresponds to a single domain  figure 14.2   the rings are numbered from 0 to 7 let d ; and dj be any two domain rings  if j i  then d ; is a subset of dj that is  a process executing in domain dj has more privileges than does a process executing in domain d ;  a process executing in domain do has the most privileges if only two rings exist  this scheme is equivalent to the monitor-user n1ode of execution  where monitor mode corresponds to do and user mode corresponds to d1  multics has a segmented address space ; each segment is a file  and each segment is associated with one of the rings a segm.ent description includes an entry that identifies the ring number in addition  it includes three access bits to control reading  writing  and execution the association between segments and rings is a policy decision with which we are not concerned here  a current-ring-number counter is associated with each process  identifying the ring in which the process is executing currently when a process is executing 14.3 597 figure 14.2 multics ring structure  in ring i  it cmmot access a segment associated with ring j  j i   it can access a segment associated with ring k  k    i   the type of access  however  is restricted according to the access bits associated with that segment  domain switching in multics occurs when a process crosses from one ring to another by calling a procedure in a different ring obviously  this switch must be done in a controlled mmmer ; otherwise  a process could start executing in ring 0  and no protection would be provided to allow controlled domain switching  we modify the ring field of the segment descriptor to include the following  access bracket a pair of integers  bl and b2  such that bl   =  b2  limit an integer b3 such that b3 b2  list of gates identifies the entry points  or may be called  at which the segments if a process executing in ring i calls a procedure  or segncent  with access bracket  bl,b2   then the call is allowed if bl   =  i   =  b2  and the current ring number of the process remains i otherwise  a trap to the operating system occurs  and the situation is handled as follows  if i bl  then the call is allowed to occur  because we have a transfer to a ring  or domain  with fewer privileges however  if parameters are passed that refer to segments in a lower ring  that is  segments not accessible to the called procedure   then these segments must be copied into an area that can be accessed by the called procedure  if i b2  then the call is allowed to occur only if b3 is greater than or equal to i and the call has been directed to one of the designated entry points in the list of gates this scheme allows processes with limited access rights to call procedures in lower rings that have more access rights  but only in a carefully controlled mmmer  598 chapter 14 14.4 the main disadvantage of the ring  or hierarchical  structure is that it does not allow us to enforce the need-to-know principle in particular  if an object must be accessible in domain 0 j but not accessible in domain oi  then we must have j i but this requirement means that every segment accessible in oi is also accessible in 0 1  the multics protection system is generally more complex and less efficient than are those used in current operating systems if protection interferes with the ease of use of the system or significantly decreases system performance  then its use must be weighed carefully against the purpose of the system for instance  we would want to have a complex protection system on a computer used by a university to process students ' grades and also used by students for classwork a similar protection system would not be suited to a computer being used for number crunching  in which performance is of utmost importance we would prefer to separate the mechanism from the protection policy  allowing the same system to have complex or simple protection depending on the needs of its users to separate mechanism from policy  we require a more general model of protection  our model of protection can be viewed abstractly as a matrix  called an the rows of the access matrix represent domains  and the columns represent objects each entry in the matrix consists of a set of access rights  because the column defines objects explicitly  we can omit the object name from the access right the entry access  i,j  defines the set of operations that a process executing in domain oi can invoke on object oj  to illustrate these concepts  we consider the access matrix shown in figure 14.3 there are four domains and four objects-three files  f1  f2  f3  and one laser printer a process executing in domain 0 1 can read files f1 and f3  a process executing in domain 0 4 has the same privileges as one executing in domain 0 1 ; but in addition  it can also write onto files f1 and f3  note that the laser printer can be accessed only by a process executing in domain 0 2  the access-matrix scheme provides us with the mechanism for specifying a variety of policies the mechanism consists of implementing the access 01 read read 02 print 03 read execute 04 read read write write figure 14.3 access matrix  14.4 599 matrix and ensuring that the semantic properties we have outlined hold  more specifically  we must ensure that a process executing in domain n can access only those objects specified in row  and then only as allowed by the access-matrix entries  the access matrix can implement policy decisions concerning protection  the policy decisions involve which rights should be included in the  i,j  th entry we must also decide the domain in which each process executes this last policy is usually decided by the operating system  the users normally decide the contents of the access-matrix entries when a user creates a new object oi  the column oi is added to the access matrix with the appropriate initialization entries  as dictated by the creator the user may decide to enter some rights in some entries in cohum1 j and other rights in other entries  as needed  the access matrix provides an appropriate mechanism for defining and implementing strict control for both the static and dynamic association between processes and domains when we switch a process from one domain to another  we are executing an operation  switch  on an object  the domain   we can control domain switching by including domains among the objects of the access matrix similarly  when we change the content of the access matrix  we are performing an operation on an object  the access matrix again  we can control these changes by including the access matrix itself as an object  actually  since each entry in the access matrix may be modified individually  we must consider each entry in the access matrix as an object to be protected  now  we need to consider only the operations possible on these new objects  domains and the access matrix  and decide how we want processes to be able to execute these operations  processes should be able to switch from one domain to another switching from domain d ; to domain di is allowed if and only if the access right switch e access  i,j   thus  in figure 14.4  a process executing in domain d2 can switch to domain d3 or to domain d4  a process in domain d4 can switch to d1  and one in domain d1 can switch to d2 allowing controlled change in the contents of the access-matrix entries requires three additional operations  copy  owner  and control we examine these operations next  01 read read switch 02 print switch switch 03 read execute 04 read read switch write write figure 14.4 access matrix of figure 14.3 with domains as objects  600 chapter 14  a  execute read execute execute read  b  figure 14.5 access matrix with copy rights  the ability to copy an access right from one domain  or row  of the access matrix to another is denoted by an asterisk   appended to the access right  the copy right allows the access right to be copied only within the colurrm   that is  for the object  for which the right is defined for example  in figure 14.5  a   a process executing in domain d2 can copy the read operation into any entry associated with file f2  hence  the access matrix of figure 14.5  a  can be modified to the access matrix shown in figure 14.5  b   this scheme has two variants  a right is copied from access  i  j  to access  jc  j  ; it is then removed from access  i  j   this action is a transfer of a right  rather than a copy  propagation of the copy right may be limited that is  when the right r is copied from access  i,j  to access  lc,j   only the right r  not r  is created a process executing in domain d ~ r can not further copy the right r  a system may select only one of these three copy rights  or it may provide all three by identifying them as separate rights  copy  transfer  and limited copy  we also need a mechanism to allow addition of new rights and removal of some rights the owner right controls these operations if access  i  j  includes the owner right  then a process executing in domain di can add and remove any right in any entry in column j for example  in figure 14.6  a   domain d1 is the owner of f1 and thus can add and delete any valid right in column f1  similarly  domain d2 is the owner of f2 and f3 and thus can add and remove any valid right within these two columns thus  the access matrix of figure 14.6  a  can be modified to the access matrix shown in figure 14.6  b   14.4 601 01 owner write execute read read 02 owner owner write 03 execute  a  01 owner write execute owner read 02 read owner write write 03 write write  b  figure 14.6 access matrix with owner rights  the copy and owner rights allow a process to change the entries in a column  a mechanism is also needed to change the entries in a row the control right is applicable only to domain objects if access  i  j  includes the control right  then a process executing in domain di can remove any access right from row j for example  suppose that  in figure 14.4  we include the control right in access  d2  d4   then  a process executil1.g in domain d2 could modify domai11 d4  as shown in figure 14.7  read read switch print switch switch control read execute write write switch figure 14.7 modified access matrix of figure 14.4  602 chapter 14 14.5 the copy and owner rights provide us with a mechanism to limit the propagation of access rights however  they do not give us the appropriate tools for preventing the propagation  or disclosure  of information the problem of guaranteeing that no information initially held in an object can migrate outside of its execution environment is called the  this problem is in general unsolvable  see the bibliographical notes at the end of the chapter   these operations on the domains and the access matrix are not in themselves important  but they illustrate the ability of the access-matrix model to allow the implementation and control of dynamic protection requirements  new objects and new domains can be created dynamically and included in the access-matrix model however  we have shown only that the basic mechanism exists ; system designers and users must make the policy decisions concerning which domains are to have access to which objects in which ways  how can the access matrix be implemented effectively in general  the matrix will be sparse ; that is  most of the entries will be empty although datastructure techniques are available for representing sparse matrices  they are not particularly useful for this application  because of the way in which the protection facility is used here  we first describe several methods of implementing the access matrix and then compare the methods  14.5.1 global table the simplest implementation of the access matrix is a global table consisting of a set of ordered triples domain  object  rights-set  whenever an operation m is executed on an object oj within domain d ;  the global table is searched for a triple d ;  0 1  r ~ c  with me r ~ c if this triple is found  the operation is allowed to continue ; otherwise  an exception  or error  condition is raised  this implementation suffers from several drawbacks the table is usually large and thus can not be kept in main memory  so additional i/0 is needed  virtual memory techniques are often used for managing this table in addition  it is difficult to take advantage of special groupings of objects or domains  for example  if everyone can read a particular object  this object must have a separate entry in every domain  14.5.2 access lists for objects each column in the access matrix can be implemented as an access list for one object  as described in section 10.6.2 obviously  the empty entries can be discarded the resulting list for each object consists of ordered pairs domain  rights-set  which define all domains with a nonempty set of access rights for that object  this approach can be extended easily to define a list plus a default set of access rights when an operation m on an object oi is attempted in domain d ;  we search the access list for object 0 i  looking for an entry d ;  r1c with me rjc if the entry is found  we allow the operation ; if it is not  we check the default set if m is in the default set  we allow the access otherwise  access is 14.5 603 denied  and an exception condition occurs for efficiency  we may check the default set first and then search the access list  14.5.3 capability lists for domains rather than associating the columns of the access matrix with the objects as access lists  we can associate each row with its domain a ltst for a domain is a list of objects together with the operations allowed on tbose objects an object is often represented by its physical name or address  called a to execute operation m on object 0 1  the process executes the operation m  specifying the capability  or pointer  for object 0 j as a parameter  simple of the capability means that access is allowed  the capability list is associated with a domain  but it is never directly accessible to a process executing in that domain rather  the capability list is itself a protected object  maintained by the operating system and accessed by the user only indirectly capability-based protection relies on the fact that the capabilities are never allowed to migrate into any address space directly accessible by a user process  where they could be modified   if all capabilities are secure  the object they protect is also secure against unauthorized access  capabilities were originally proposed as a kind of secure pointer  to meet the need for resource protection that was foreseen as multiprogrammed computer systems came of age the idea of an inherently protected pointer provides a fom1dation for protection that can be extended up to the applications level  to provide inherent protection  we must distinguish capabilities from other kinds of objects  and they must be interpreted by an abstract machine on which higher-level programs run capabilities are usually distinguished from other data in one of two ways  each object has a to denote whether it is a capability or accessible data the tags themselves must not be directly accessible by an application program hardware or firmware support may be used to enforce this restriction although only one bit is necessary to distinguish between capabilities and other objects  more bits are often used this extension allows all objects to be tagged with their types by the hardware thus  the hardware can distinguish integers  floating-point numbers  pointers  booleans  characters  instructions  capabilities  and uninitialized values by their tags  alternatively  the address space associated with a program can be split into two parts one part is accessible to the program and contains the program 's normal data and instructions the other part  containing the capability list  is accessible only by the operating system a segmented memory space  section 8.6  is useful to support this approach  several capability-based protection systems have been developed ; we describe them briefly in section 14.8 the mach operating system also uses a version of capability-based protection ; it is described in appendix b  604 chapter 14 14.5.4 a lock-key mechanism the t ' is a compromise between access lists and capability lists each object has a list of unique bit patterns  called similarly  each domain has a list of unique bit patterns  called a process executing in a domain can access an object only if that domain has a key that matches one of the locks of the object  as with capability lists  the list of keys for a domain must be managed by the operating system on behalf of the domain users are not allowed to examine or modify the list of keys  or locks  directly  14.5.5 comparison as you might expect choosing a technique for implementing an access matrix involves various trade-offs using a global table is simple ; however  the table can be quite large and often can not take advantage of special groupings of objects or domains access lists correspond directly to the needs of users  when a user creates an object he can specify which domains can access the object as well as what operations are allowed however  because access-rights information for a particular domain is not localized  determining the set of access rights for each domain is difficult in addition  every access to the object must be checked  requiring a search of the access list in a large system with long access lists  this search can be time consuming  capability lists do not correspond directly to the needs of users ; they are usefut however  for localizing information for a given process the process attempting access must present a capability for that access then  the protection system needs only to verify that the capability is valid revocation of capabilities  however  may be inefficient  section 14.7   the lock-key mechanism  as mentioned  is a compromise between access lists and capability lists the mechanism can be both effective and flexible  depending on the length of the keys the keys can be passed freely from domain to domain in addition  access privileges can be effectively revoked by the simple technique of changing some of the locks associated with the object  section 14.7   most systems use a combination of access lists and capabilities when a process first tries to access an object  the access list is searched if access is denied  an exception condition occurs otherwise  a capability is created and attached to the process additional references use the capability to demonstrate swiftly that access is allowed after the last access  the capability is destroyed  this strategy is used in the multics system and in the cal system  as an example of how such a strategy works  consider a file system in which each file has an associated access list when a process opens a file  the directory structure is searched to find the file  access permission is checked  and buffers are allocated all this information is recorded in a new entry in a file table associated with the process the operation returns an index into this table for the newly opened file all operations on the file are made by specification of the index into the file table the entry in the file table then points to the file and its buffers when the file is closed  the file-table entry is deleted since the file table is maintained by the operating system  the user carmot accidentally corrupt it thus  the user can access only those files that have been opened  14.6 14.6 605 since access is checked when the file is opened  protection is ensured this strategy is used in the unix system  the right to access must still be checked or1 each access  and the file-table entry has a capability only for the allowed operations if a file is opened for reading  then a capability for read access is placed in the file-table entry if an attempt is made to write onto the file  the system identifies this protection violation by com.paring the requested operation with the capability in the file-table entry  in section 10.6.2  we described how access controls can be used on files within a file system each file and directory are assigned an owner  a group  or possibly a list of users  and for each of those entities  access-control information is assigned a similar function can be added to other aspects of a computer system a good example of this is found in solaris 10  solaris 10 advances the protection available in the sun microsystems operating system by explicitly adding the principle of least privilege via this facility revolves around privileges  a privilege is the right to execute a system call or to use an option within that system call  such as opening a file with write access   privileges can be assigned to processes,limiting them to exactly the access they need to perform their work privileges and programs can also be assigned to users are assigned roles or can take roles based on passwords to the roles in this way a user can take a role that enables a privilege  allowing the user to run a program to accomplish a specific task  as depicted in figure 14.8 this implementation of privileges decreases the security risk associated with superusers and setuid programs  user1 executes with role 1 privileges ~ figure 14.8 role-based access control in solaris 10  606 chapter 14 14.7 notice that this facility is similar to the access matrix described in section 14.4 this relationship is further explored in the exercises at the end of the chapter  in a dynamic protection system  we may sometimes need to revoke access rights to objects shared by different users various questions about revocation may arise  immediate versus delayed does revocation occur immediately  or is it delayed if revocation is delayed  can we find out when it will take place selective versus general when an access right to an object is revoked  does it affect all the users who have an access right to that object  or can we specify a select group of users whose access rights should be revoked partial versus total can a subset of the rights associated with an object be revoked  or must we revoke all access rights for this object temporary versus permanent can access be revoked permanently  that is  the revoked access right will never again be available   or can access be revoked and later be obtained again with an access-list scheme  revocation is easy the access list is searched for any access rights to be revoked  and they are deleted from the list revocation is immediate and can be general or selective  total or partial  and permanent or temporary  capabilities  howeve1 ~ present a much more difficult revocation problem  as mentioned earlier since the capabilities are distributed throughout the system  we must find them before we can revoke them schemes that implement revocation for capabilities include the following  reacquisition periodically  capabilities are deleted from each domain if a process wants to use a capability  it may find that that capability has been deleted the process may then try to reacquire the capability if access has been revoked  the process will not be able to reacquire the capability  back-pointers a list of pointers is maintained with each object  pointing to all capabilities associated with that object when revocation is required  we can follow these pointers  changing the capabilities as necessary this scheme was adopted in the multics system it is quite general  but its implementation is costly  indirection the capabilities point indirectly  not directly  to the objects  each capability points to a unique entry in a global table  which in turn points to the object we implement revocation by searching the global table for the desired entry and deleting it then  when an access is attempted  the capability is found to point to an illegal table entry table entries can be reused for other capabilities without difficulty  since both the capability and the table entry contain the unique name of the object the object for a 14.8 14.8 607 capability and its table entry must match this scheme was adopted in the cal system it does not allow selective revocation  keys a key is a unique bit pattern that can be associated with a capability  this key is defined when the capability is created  and it can be neither modified nor inspected by the process that owns the capability a is associated with each object ; it can be defined or replaced with the set-key operation when a capability is created  the current value of the master key is associated with the capability when the capability is exercised  its key is compared with the master key if the keys match  the operation is allowed to continue ; otherwise  an exception condition is raised revocation replaces the master key with a new value via the set-key operation  invalidating all previous capabilities for this object  this scheme does not allow selective revocation  since only one master key is associated with each object if we associate a list of keys with each object  then selective revocation can be implemented finally  we can group all keys into one global table of keys a capability is valid only if its key matches some key in the global table we implement revocation by removing the matching key from the table with this scheme  a key can be associated with several objects  and several keys can be associated with each object  providing maximum flexibility  in key-based schemes  the operations of defining keys  inserting them into lists  and deleting them from lists should not be available to all users  in particular  it would be reasonable to allow only the owner of an object to set the keys for that object this choice  however  is a policy decision that the protection system can implement but should not define  in this section  we survey two capability-based protection systems these systems differ in their complexity and in the types of policies that can be implemented on them neither system is widely used  but both provide interesting proving grounds for protection theories  14.8.1 an example  hydra hydra is a capability-based protection system that provides considerable flexibility the system implements a fixed set of possible access rights  including such basic forms of access as the right to read  write  or execute a memory segment in addition  a user  of the protection system  can declare other rights  the interpretation of user-defined rights is performed solely by the user 's program  but the system provides access protection for the use of these rights  as well as for the use of system-defined rights these facilities constitute a significant development in protection technology  operations on objects are defined procedurally the procedures that implement such operations are themselves a form of object  and they are accessed indirectly by capabilities the names of user-defined procedures must be identified to the protection system if it is to deal with objects of the userdefined type when the definition of an object is made krtown to hydra  the names of operations on the type become auxiliary rights 608 chapter 14 can be described in a capability for an instance of the type for a process to perform an operation on a typed object  the capability it holds for that object must contain the name of the operation being invoked among its auxiliary rights this restriction enables discrin lination of access rights to be made on an instance-by-instance and process-by-process basis  hydra also provides   ' ! fnrwk ;  j1o  l this scheme allows a procedure to be certified as to act on a formal parameter of a specified type on behalf of any process that holds a right to execute the procedure the rights held by a trustworthy procedure are independent oc and may exceed  the rights held by the calling process however  such a procedure must not be regarded as universally trustworthy  the procedure is not allowed to act on other types  for instance   and the trustworthiness must not be extended to any other procedures or program segments that might be executed by a process  amplification allows implementation procedures access to the representation variables of an abstract data type if a process holds a capability to a typed object a  for instance  this capability may include an auxiliary right to invoke some operation p but does not include any of the so-called kernel rights  such as read  write  or execute  on the segment that represents a such a capability gives a process a means of indirect access  through the operation p  to the representation of a  but only for specific purposes  when a process invokes the operation p on an object a  howeve1 ~ the capability for access to a may be amplified as control passes to the code body of p this amplification may be necessary to allow p the right to access the storage segment representing a so as to implement the operation that p defines on the abstract data type the code body of p may be allowed to read or to write to the segment of a directly  even though the calling process cmmot  on return from p the capability for a is restored to its originat unamplified state this case is a typical one in which the rights held by a process for access to a protected segment must change dynamically  depending on the task to be performed the dynamic adjustment of rights is performed to guarantee consistency of a programmer-defined abstraction amplification of rights can be stated explicitly in the declaration of an abstract type to the hydra operating system  when a user passes an object as an argument to a procedure  we may need to ensure that the procedure can not modify the object we can implement this restriction readily by passing an access right that does not have the modification  write  right howeve1 ~ if amplification may occur  the right to modify may be reinstated thus  the user-protection requirement can be circumvented  in generat of course  a user may trust that a procedure performs its task correctly this assumption is not always correct however  because of hardware or software errors hydra solves this problem by restricting amplifications  the procedure-call mechanism of hydra was designed as a direct solution to the problem of mutually suspicious subsystems this problem is defined as follows suppose that a program is provided that can be invoked as a service by a number of different users  for example  a sort routine  a compile1 ~ a game   when users invoke this service program  they take the risk that the program will malfunction and will either damage the given data or retain some access right to the data to be used  without authority  later similarly  the service program may have som.e private files  for accounting purposes  14.8 609 for example  that should not be accessed directly by the calling user program  hydra provides mechanisms for directly dealing with this problem  a hydra subsystem is built on top of its protection kernel and may require protection of its own components a subsystem interacts with the kernel through calls on a set of kernel-defined primitives that define access rights to resources defined by the subsystenl the subsystem designer can define policies for use of these resources by user processes  but the policies are enforceable by use of the standard access protection afforded by the capability system  programmers can make direct use of the protection system after acquainting themselves with its features in the appropriate reference rnanual hydra provides a large library of system-defined procedures that can be called by user programs programmers can explicitly incorporate calls on these system procedures into their program code or can use a program translator that has been interfaced to hydra  14.8.2 an example  cambridge cap system a different approach to capability-based protection has been taken in the design of the cambridge cap system cap 's capability system is simpler and superficially less powerful than that of hydra however  closer examination shows that it  too  can be used to provide secure protection of user-defined objects cap has two kinds of capabilities the ordinary kind is called a  it can be used to provide access to objects  but the only rights provided are the standard read  write  and execute of the individual storage segments associated with the object data capabilities are interpreted by microcode in the cap machine  the second kind of capability is the so-called which is protected  but not interpreted  by the cap microcode it is interpreted by a protected  that is  privileged  procedure  which may be written by an application programmer as part of a subsystem a particular kind of rights amplification is associated with a protected procedure when executing the code body of such a procedure  a process temporarily acquires the right to read or write the contents of a software capability itself this specific kind of rights amplification corresponds to an implementation of the seal and unseal primitives on capabilities of course  this privilege is still subject to type verification to ensure that only software capabilities for a specified abstract type are passed to any such procedure universal trust is not placed in any code other than the cap machine 's microcode  see bibliographical notes for references  the interpretation of a software capability is left completely to the subsystem  through the protected procedures it contains this scheme allows a variety of protection policies to be implemented although programmers can define their own protected procedures  any of which might be incorrect   the security of the overall system can not be compromised the basic protection system will not allow an unverified  user-defined  protected procedure access to any storage segments  or capabilities  that do not belong to the protection environment in which it resides the most serious consequence of an insecure protected procedure is a protection breakdown of the subsystem for which that procedure has responsibility  610 chapter 14 14.9 the designers of the cap system have noted that the use of software capabilities allowed them to realize considerable economies in formulating and implementing protection policies commensurate with the requirements of abstract resources however  subsystem designers who want to make use of this facility can not simply study a reference manual  as is the case with hydra  instead  they must learn the principles and techniques of protection  since the system provides them with no library of procedures  to the degree that protection is provided in existing computer systems  it is usually achieved through an operating-system kernel  which acts as a security agent to inspect and validate each attempt to access a protected resource since comprehensive access validation may be a source of considerable overhead  either we must give it hardware support to reduce the cost of each validation or we must allow the system designer to compromise the goals of protection  satisfying all these goals is difficult if the flexibility to implement protection policies is restricted by the support mechanisms provided or if protection environments are made larger than necessary to secure greater operational efficiency  as operating systems have become more complex  and particularly as they have attempted to provide higher-level user interfaces  the goals of protection have become much more refined the designers of protection systems have drawn heavily on ideas that originated in programming languages and especially on the concepts of abstract data types and objects protection systems are now concerned not only with the identity of a resource to which access is attempted but also with the functional nature of that access in the newest protection systems  concern for the function to be invoked extends beyond a set of system-defined functions  such as standard file-access methods  to include functions that may be user-defined as well  policies for resource use may also vary  depending on the application  and they may be subject to change over time for these reasons  protection can no longer be considered a matter of concern only to the designer of an operating system it should also be available as a tool for use by the application designe1 ~ so that resources of an applications subsystem can be guarded against tampering or the influence of an error  14.9.1 compiler-based enforcement at this point  programming languages enter the picture specifying the desired control of access to a shared resource in a system is making a declarative statement about the resource this kind of statement can be integrated into a language by an extension of its typing facility when protection is declared along with data typing  the designer of each subsystem can specify its requirements for protection  as well as its need for use of other resources in a system such a specification should be given directly as a program is composed  and in the language in which the program itself is stated this approach has several significant advantages  14.9 611 protection needs are simply declared  rather than programmed as a sequence of calls on procedures of an operating system  protection requirements can be stated independently of the facilities provided by a particular operating system  the means for enforcement need not be provided by the designer of a subsystem  a declarative notation is natural because access privileges are closely related to the linguistic concept of data type  a variety of techniques can be provided by a programming-language implementation to enforce protection  but any of these must depend on some degree of support from an underlying machine and its operating system for example  suppose a language is used to generate code to run on the cambridge cap system on this system  every storage reference made on the underlying hardware occurs indirectly through a capability this restriction prevents any process from accessing a resource outside of its protection environment at any time however  a program may impose arbitrary restrictions on how a resource can be used during execution of a particular code segment  we can implement such restrictions most readily by usin.g the software capabilities provided by cap a language implementation might provide standard protected procedures to interpret software capabilities that would realize the protection policies that could be specified in the language this scheme puts policy specification at the disposal of the programmers  while freeing them from implementing its enforcement  even if a system does not provide a protection kernel as powerful as those of hydra or cap  mechanisms are still available for implementing protection specifications given in a programming language the principal distinction is that the security of this protection will not be as great as that supported by a protection kernel  because the mechanism must rely on more assumptions about the operational state of the system a compiler can separate references for which it can certify that no protection violation could occur from those for which a violation might be possible  and it can treat them differently the security provided by this form of protection rests on the assumption that the code generated by the compiler will not be modified prior to or during its execution  what  then  are the relative merits of enforcement based solely on a kernel  as opposed to enforcement provided largely by a compiler security enforcement by a kernel provides a greater degree of security of the protection system itself than does the generation of protectionchecking code by a compiler in a compiler-supported scheme  security rests on correctness of the translator  on some underlying mechanism of storage management that protects the segments from which compiled code is executed  and  ultimately  on the security of files from which a program is loaded some of these considerations also apply to a softwaresupported protection kernel  but to a lesser degree  since the kernel may reside in fixed physical storage segments and may be loaded only from a designated file with a tagged-capability system  in which all address 612 chapter 14 computation is performed either by hardware or by a fixed microprogram  even greater security is possible hardware-supported protection is also relatively immune to protection violations that might occur as a result of either hardware or system software malfunction  flexibility there are limits to the flexibility of a protection kernel in implementing a user-defined policy  although it may supply adequate facilities for the system to provide enforcement of its own policies  with a programming language  protection policy can be declared and enforcem.ent provided as needed by an implementation if a language does not provide sufficient flexibility  it can be extended or replaced with less disturbance of a system in service than would be caused by the modification of an operating-system kernel efficiency the greatest efficiency is obtained when enforcement of protection is supported directly by hardware  or microcode   insofar as software support is required  language-based enforcement has the advantage that static access enforcement can be verified off-line at compile time also  since an intelligent compiler can tailor the enforcement mechanism to meet the specified need  the fixed overhead of kernel calls can often be avoided  in summary  the specification of protection in a programming language allows the high-level description of policies for the allocation and use of resources a language implementation can provide software for protection enforcement when automatic hardware-supported checking is unavailable in addition  it can interpret protection specifications to generate calls on whatever protection system is provided by the hardware and the operating system  one way of making protection available to the application program is through the use of a software capability that could be used as an object of computation inherent in this concept is the idea that certain program components might have the privilege of creating or examining these software capabilities a capability-creating program would be able to execute a primitive operation that would seal a data structure  rendering the latter 's contents inaccessible to any program components that did not hold either the seal or the unseal privilege such components might copy the data structure or pass its address to other program components  but they could not gain access to its contents the reason for introducing such software capabilities is to bring a protection mechanism into the programming language the only problem with the concept as proposed is that the use of the seal and unseal operations takes a procedural approach to specifying protection a nonprocedural or declarative notation seems a preferable way to make protection available to the application programmer  what is needed is a safe  dynamic access-control mechanism for distributing capabilities to system resources among user processes to contribute to the overall reliability of a system  the access-control mechanism should be safe to use to be useful in practice  it should also be reasonably efficient this requirement has led to the development of a number of language constructs that allow the programmer to declare various restrictions on the use of a specific managed resource  see the bibliographical notes for appropriate references  these constructs provide mechanisms for three functions  14.9 613 distributing capabilities safely and efficiently among customer processes  in particular  mechanisms ensure that a user process will use the managed resource only if it was granted a capability to that resource  specifying the type of operations that a particular process may invoke on an allocated resource  for example  a reader of a file should be allowed only to read the file  whereas a writer should be able both to read and to write   it should not be necessary to grant the same set of rights to every user process  and it should be impossible for a process to enlarge its set of access rights  except with the authorization of the access-control mechanism  specifying the order in which a particular process may invoke the various operations of a resource  for example  a file must be opened before it can be read   it should be possible to give two processes different restrictions on the order in which they can invoke the operations of the allocated resource  the incorporation of protection concepts into programming languages  as a practical tool for system design  is in its infancy protection will likely become a matter of greater concern to the designers of new systems with distributed architectures and increasingly stringent requirements on data security then the importance of suitable language notations in which to express protection requirements will be recognized more widely  14.9.2 protection in java because java was designed to run in a distributed environment  the java virtual machine-or jvm-has many built-in protection mechanisms java programs are composed of each of which is a collection of data fields and functions  called that operate on those fields the jvm loads a class in response to a request to create instances  or objects  of that class one of the most novel and useful features ofj ava is its support for dynamically loading untrusted classes over a network and for executing mutually distrusting classes within the same jvm  because of these capabilities of java  protection is a paramount concern  classes running in the same jvm may be from different sources and may not be equally trusted as a result  enforcing protection at the granularity of the jvm process is insufficient intuitively  whether a request to open a file should be allowed will generally depend on which class has requested the open the operating system lacks this knowledge  thus  such protection decisions are handled within the jvm when the jvm loads a class  it assigns the class to a protection domain that gives the permissions of that class the protection domain to which the class is assigned depends on the url from which the class was loaded and any digital signatures on the class file  digital signatures are covered in section 15.4.1.3  a configurable policy file determines the permissions granted to the domain  and its classes   for example  classes loaded from a trusted server might be placed in a protection domain that allows them to access files in the user 's home directory  whereas classes loaded from an untrusted server might have no file access permissions at all  614 chapter 14 it can be complicated for the jvm to determine what class is responsible for a request to access a protected resource accesses are often performed indirectly  through system libraries or other classes for example  consider a class that is not allowed to open network connections it could call a system library to request the load of the contents of a url the jvm must decide whether or not to open a network connection for this request but which class should be used to determine if the connection should be allowed  the application or the system library the philosophy adopted in java is to require the library class to explicitly permit a network corucection more generally  in order to access a protected resource  some method in the calling sequence that resulted in the request must explicitly assert the privilege to access the resource by doing so  this method takes responsibility for the request ; presumably  it will also perform whatever checks are necessary to ensure the safety of the request of course  not every method is allowed to assert a privilege ; a method can assert a privilege only if its class is in a protection domain that is itself allowed to exercise the privilege  this implementation approach is called every thread in the jvm has an associated stack of its ongoing invocations when a caller may not be trusted  a method executes an access request within a dopri vileged block to perform the access to a protected resource directly or indirectly dopri vileged   is a static method in the accesscontroller class that is passed a class with a run   method to invoke when the dopri vileged block is entered  the stack frame for this method is annotated to indicate this fact then  the contents of the block are executed when an access to a protected resource is subsequently requested  either by this method or a method it calls  a call to checkpermissions   is used to invoke stack inspection to determine if the request should be allowed the inspection examines stack frames on the calling thread 's stack  starting from the most recently added frame and working toward the oldest if a stack frame is first found that has the dopri vileged   annotation  then checkpermissions   returns immediately and silently  allowing the access if a stack frame is first found for which access is disallowed based on the protection domain of the method 's class  then checkpermissions   throws an accesscontrolexception if the stack inspection exhausts the stack without finding either type of frame  then whether access is allowed depends on the implementation  for example  some implementations of the jvm may allow access  while other implementations may disallow it   stack inspection is illustrated in figure 14.9 here  the gui   method of a class in the untrusted applet protection domain performs two operations  first a get   and then an open    the former is an invocation of the get   method of a class in the url loader protection domain  which is permitted to open   sessions to sites in the lucent com domain  in particular a proxy server proxy .lucent com for retrieving urls for this reason  the untrusted applet 's get   invocation will succeed  the checkpermissions   call in the networking library encounters the stack frame of the get   method  which performed its open   in a dopri vileged block however  the untrusted applet 's open   invocation will result in an exception  because the checkpermissions   call finds no dopri vileged annotation before encountering the stack frame of the gui   method  14.10 protection domain  socket permission  class  none gui  get  uri  ; open  addr  ; 14.10 .lucent.com  80  connect get  url u   doprivileged  open  'proxy.lucent.com  80 '  ;  request u from proxy figure 14.9 stack inspection  615 any open  addr a   checkpermission  a  connect  ; connect  a  ; of course  for stack inspection to work  a program must be unable to modify the annotations on its own stack frame or to do other manipulations of stack inspection this is one of the most important differences between java and many other languages  including c + +   a java program can not directly access memory ; it can manipulate only an object for which it has a reference references can not be forged  and the manipulations are made only through well-defined interfaces compliance is enforced through a sophisticated collection of load-time and run-time checks as a result  an object can not manipulate its run-time stack  because it camlot get a reference to the stack or other components of the protection system  more generally  java 's load-time and run-time checks enforce of java classes type safety ensures that classes can not treat integers as pointers  write past the end of an array  or otherwise access memory in arbitrary ways  rather  a program can access an object only via the methods defined on that object by its class this is the f01mdation of java protection  since it enables a class to effectively  and protect its data and methods from other classes loaded in the same jvm for example  a variable can be defined as private so that only the class that contains it can access it or protected so that it can be accessed only by the class that contains it  subclasses of that class  or classes in the same package type safety ensures that these restrictions can be enforced  computer systems contain many objects  and they need to be protected from misuse objects may be hardware  such as memory  cpu time  and i/0 devices  or software  such as files  programs  and semaphores   an access right is permission to perform an operation on an object a domain is a set of access rights processes execute in domains and may use any of the access rights in the domain to access and manipulate objects during its lifetime  a process may be either bound to a protection domain or allowed to switch from one domain to another  616 chapter 14 the access matrix is a general model of protection that provides a mechanisnc for protection without imposing a particular protection policy on the system or its users the separation of policy and mechanism is an important design property  the access matrix is sparse it is normally implemented either as access lists associated with each object or as capability lists associated with each domain  we can include dynamic protection in the access-matrix model by considering domains and the access matrix itself as objects revocation of access rights in a dynamic protection model is typically easier to implement with an access-list scheme than with a capability list  real systems are much more limited than the general model and tend to provide protection only for files unix is representative  providing read  write  and execution protection separately for the owner  group  and general public for each file multics uses a ring structure in addition to file access hydra  the cambridge cap system  and mach are capability systems that extend protection to user-defined software objects solaris 10 implements the principle of least privilege via role-based access controt a form of the access matrix  language-based protection provides finer-grained arbitration of requests and privileges than the operating system is able to provide for example  a single java jvm can run several threads  each in a different protection class it enforces the resource requests through sophisticated stack inspection and via the type safety of the language  14.1 consider a computer system in which computer games can be played by students only between 10 p.m and 6 a.m  by faculty members between 5 p.m and 8 a.m  and by the computer center staff at all times suggest a scheme for implementing this policy efficiently  14.2 the rc 4000 system  among others  has defined a tree of processes  called a process tree  such that all the descendants of a process can be given resources  objects  and access rights by their ancestors only thus  a descendant can never have the ability to do anything that its ancestors can not do the root of the tree is the operating system  which has the ability to do anything assume the set of access rights is represented by an access matrix  a a  x,y  defines the access rights of process x to object y if xis a descendant of z  what is the relationship between a  x,y  and a  z,y  for an arbitrary object y 14.3 how are the access-matrix facility and the role-based access-control facility similar how do they differ 14.4 discuss the need for rights amplification in hydra how does this practice compare with the cross-ring calls in a ring-protection scheme 617 14.5 explain why a capability-based system such as hydra provides greater flexibility than the ring-protection scheme in enforcing protection policies  14.6 consider the ring-protection scheme in multics if we were to implement the system calls of a typical operating system and store them in a segment associated with ring 0  what should be the values stored in the ring field of the segment descriptor what happens during a system call when a process executing in a higher-numbered ring invokes a procedure in ring 0 14.7 discuss the strengths and weaknesses of implementing an access matrix using capabilities that are associated with domains  14.8 discuss the strengths and weaknesses of implementing an access matrix using access lists that are associated with objects  14.9 the access-control matrix can be used to determine whether a process can switch from  say  domain a to domain b and enjoy the access privileges of domain b is this approach equivalent to including the access privileges of domain b in those of domain a 14.10 how can systems that implement the principle of least privilege still have protection failures that lead to security violations 14.11 how does the principle of least privilege aid in the creation of protection systems 14.12 what protection problems may arise if a shared stack is used for parameter passing 14.13 if all the access rights to an object are deleted  the object can no longer be accessed at this point the object should also be deleted  and the space it occupies should be returned to the system suggest an efficient implementation of this scheme  14.14 discuss which of the following systems allow module designers to enforce the need-to-know principle  a the multics ring-protection scheme b hydra 's capabilities c jvm 's stack-inspection scheme 618 chapter 14 14.15 consider a computing environment where a unique number is associated with each process and each object in the system suppose that we allow a process with number n to access an object with number m only if n m what type of protection structure do we have 14.16 what is the need-to-know principle why is it important for a protection system to adhere to this principle 14.17 what hardware features does a computer system need for efficient capability manipulation can these features be used for memory protection 14.18 describe how the java protection model would be compromised if a java program were allowed to directly alter the annotations of its stack frame  14.19 a burroughs b7000/b6000 mcp file can be tagged as sensitive data  when such a file is deleted  its storage area is overwritten by some random bits for what purpose would such a scheme be useful the access-matrix model of protection between domains and objects was developed by lampson  1969  and lampson  1971   popek  1974  and saltzer and schroeder  1975  provided excellent surveys on the subject of protection  harrison et al  1976  used a formal version of this model to enable them to prove properties of a protection system mathematically  the concept of a capability evolved from iliffe 's and jodeit 's codewords  which were implemented in the rice university computer  iliffe and jodeit  1962    the term capability was introduced by dennis and horn  1966   the hydra system was described by wulf et al  1981   the cap system was described by needham and walker  1977   organick  1972  discussed the multics ring-protection system  revocation was discussed by redell and fabry  1974   cohen and jefferson  1975   and ekanadham and bernstein  1979   the principle of separation of policy and mechanism was advocated by the designer of hydra  levin et al   1975    the confinement problem was first discussed by lampson  1973  and was further examined by lipner  1975   the use of higher-level languages for specifying access control was suggested first by morris  1973   who proposed the use of the seal and unseal operations discussed in section 14.9 kieburtz and silberschatz  1978   kieburtz and silberschatz  1983   and mcgraw and andrews  1979  proposed various language constructs for dealing with general dynamic-resource-management schemes jones and liskov  1978  considered how a static access-control scheme can be incorporated in a programming language that supports abstract data types the use of minimal operating-system support to enforce protection was advocated by the exokernel project  ganger et al  2002   kaashoek et al  1997    15.1 protection  as we discussed in chapter 14  is strictly an internal problem  how do we provide controlled access to programs and data stored in a computer system on the other hand  requires not only an adequate protection system but also consideration of the external environment within which the system operates a protection system is ineffective if user authentication is compromised or a program is run by an unauthorized user  computer resources must be guarded against unauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency  these resources include information stored in the system  both data and code   as well as the cpu  memory  disks  tapes  and networking that are the computer  in this chapter  we start by examining ways in which resources may be accidentally or purposely misused we then explore a key security enabler -cryptography finally  we look at mechanisms to guard against or detect attacks  to discuss security threats and attacks  to explain the fundamentals of encryption  authentication  and hashing  to examine the uses of cryptography in computing  to describe various countermeasures to security attacks  in many applications  ensuring the security of the computer system is worth considerable effort large commercial systems containing payroll or other financial data are inviting targets to thieves systems that contain data pertaining to corporate operations may be of interest to unscrupulous competitors  furthermore  loss of such data  whether by accident or fraud  can seriously impair the ability of the corporation to function  in chapter 14  we discussed mechanisms that the operating system can provide  with appropriate aid from the hardware  that allow users to protect 621 622 chapter 15 their resources  including programs and data these mechanisms work well only as long as the users conform to the intended use of and access to these resources we say that a system is if its resources are used and accessed as intended under all circumstances unfortunately total security can not be achieved nonetheless  we must have mechanisms to make security breaches a rare occurrence  rather than the norm  security violations  or misuse  of the system can be categorized as intentional  malicious  or accidental it is easier to protect against accidental misuse than against malicious misuse for the most part protection mechanisms are the core of protection from accidents the following list includes several forms of accidental and malicious security violations we should note that in our discussion of security  we use the terms intruder and cracker for those attempting to breach security in addition  a is the potential for a security violation  such as the discovery of a vulnerability  whereas an is the attempt to break security  breach of confidentiality this type of violation involves 1mauthorized reading of data  or theft of information   typically  a breach of confidentiality is the goal of an intruder capturing secret data from a system or a data stream  such as credit-card information or identity information for identity theft  can result directly in money for the intruder  breach of integrity this violation involves unauthorized modification of data such attacks can  for example  result in passing of liability to an innocent party or modification of the source code of an important commercial application  breach of availability this violation involves unauthorized destruction of data some crackers would rather wreak havoc and gain status or bragging rights than gain financially web-site defacement is a common example of this type of security breach  theft of service this violation involves unauthorized use of resources  for example  an intruder  or intrusion program  may install a daemon on a system that acts as a file server  denial of service this violation involves preventing legitimate use of the system or attacks are sometimes accidental the original internet worm turned into a dos attack when a bug failed to delay its rapid spread we discuss dos attacks further in section 15.3.3  attackers use several standard methods in their attempts to breach security the most common is in which one participant in a communication pretends to be someone  another host or another person   by masquerading  attackers breach the correctness of identification ; they can then gain access that they would not normally be allowed or escalate their privileges-obtain privileges to which they would not normally be entitled another common attack is to replay a captured exchange of data a consists of the malicious or fraudulent repeat of a valid data transmission sometimes the replay comprises the entire attackfor example  in a repeat of a to transfer money but frequently it is done along with again to escalate privileges consider 15.1 623 normal attacker masquerading attacker man-in-the-middle attacker figure 15.1 standard security attacks  the damage that could be done if a request for authentication had a legitimate user 's information with an unauthorized user 's yet another kind of attack is the in which an attacker sits in the data flow of a communication  masquerading as the sender to the receiver  and vice versa in a network communication  a man-in-the-middle attack may be preceded by a in which an active communication session is intercepted several attack methods are depicted in figure 15.1  as we have already suggested  absolute protection of the system from malicious abuse is not possible  but the cost to the perpetrator can be made sufficiently high to deter most intruders in some cases  such as a denial-ofservice attack  it is preferable to prevent the attack but sufficient to detect the attack so that cmmtermeasures can be taken  to protect a system  we must take security measures at four levels  physical the site or sites containing the computer systems must be physically secured against armed or surreptitious entry by intruders  both the machine rooms and the terminals or workstations that have access to the machines must be secured  624 chapter 15 must be done carefully to assure that only access to the system even authorized users/ to let others use their access  in exchange they may also be tricked into allowing one type of social-engineering attack here  a legitimate-looking e-mail or web page misleads a user into entering confidential information another teclucique is human authorization appropriate users have however  may be for a bribe  for access via is a general term for attempting to gather information in order to gain unauthorized access to the computer  by looking through trash  finding phone books  or finding notes containing passwords  for example   these security problems are management and personnel issues  not problems pertaining to operating systems  operating system the system must protect itself from accidental or purposeful security breaches a runaway process could constitute an accidental denial-of-service attack a query to a service could reveal passwords  a stack overflow could the launching of an unauthorized process list of possible breaches is almost endless  network much computer data in modern systems travels over private leased lines  shared lines like the internet  wireless connections  or dial-up lines intercepting these data could be just as harmful as breaking into a computer ; and interruption communications could constitute a remote denial-of-service attack  diminishing users ' use of and trust in the system  at the first two levels must be inaintained if operating-system is to be ensured a weakness at a high security  physical or allows circumvention of strict low-level  operating-system  security measures the old adage that a chal.'l is as weak as its weakest link is true of system security all these aspects must be addressed for to be maintained  to allow the more is as intruders countermeasures are created and deployed  this causes intruders to become more in their attacks for incidents include the use of to section tools needed to block the in the remainder of this 15 15.2 625 ways  ranging from passwords for authentication through guarding against viruses to detecting intrusions we start with an exploration of security threats  processes  along with the kernel  are the only means of accomplishing work on a computer therefore  writing a program that creates a breach of security  or causing a normal process to change its behavior and create a breach  is a common goal of crackers in fact even most nonprogram security events have as their goal causing a program threat for example  while it is useful to log in to a without authorization  it is quite a lot more useful to leave behind a daemon that provides information or allows easy access even if the original exploit is blocked in this section  we describe common methods which programs cause security breaches note that there is considerable variation in the naming conventions of security holes and that we use the most common or descriptive terms  15.2.1 horse systems have mechanisms for allowing programs written by users to be executed by other users if these programs are executed in a domain that provides the access rights of the executing user  the other users may misuse these rights a text-editor program  for example  may include code to search the file to be edited for certairl keywords if any are found  the entire file may be to a special area accessible to the creator of the text editor  a code segment that misuses its environment is called a long search paths  as are common on unix systems  exacerbate the trojanhorse the search path lists the set of directories to search when an program name is given the is searched for a file of that name  and the file is executed all the directories in such a search path must be secure  or a horse could be slipped into the user 's and executed consider the use of the  character in a search path the  to include the current directory in the search if a user has  in her search has set her current to a friend 's directory  and enters the name of a normal system commanct be executed from the friend 's instead the program would run the user 's to do anything that the user is allowed to the user 's instance  horse is a that emulates a what 626 chapter 15 such as the control-alt-delete conlbination used by all modern windows operating systems  another variation on the trojan horse is spyware sometimes accompanies a program that the user has chosen to install most frequently  it comes along with freeware or shareware programs  but sometimes it is included with commercial software the goal of spyware is to download ads to display on the user 's system  create when certain sites are visited  or capture information from the user 's system and return it to a central site this latter is an example of a general category of attacks known as in which surreptitious communication occurs for example  the installation of an innocuous-seeming program on a windows system could result in the loading of a spyware daemon the spyware could contact a central site  be given a message and a list of recipient addresses  and deliver the spam message to those users from the windows machine this process continues until the user discovers the spyware frequently  the spyware is not discovered  in 2004  it was estimated that 80 percent of spam was being delivered by this method this theft of service is not even considered a crime in most countries ! spyware is a micro example of a macro problem  violation of the principle of least privilege under most circumstances  a user of an operating system does not need to install network daemons such daemons are installed via two mistakes first  a user may run with more privileges than necessary  for example  as the administrator   allowing programs that she runs to have more access to the system than is necessary this is a case of human error-a common security weakness second  an operating system may allow by default more privileges than a normal user needs this is a case of poor operating-system design decisions an operating system  and  indeed  software in general  should allow fine-grained control of access and security  but it must also be easy to manage and understand inconvenient or inadequate security measures are bound to be circumvented  causing an overall weakening of the security they were designed to implement  15.2.2 trap door the designer of a program or system might leave a hole in the software that only he is capable of using this type of security breach  or was shown in the movie war games for instance  the code inight check a specific user id or password  and it might circumvent normal security procedures programmers have been arrested for embezzling from banks by including rounding errors in their code and having the occasional half-cent credited to their accounts  this account credititrg can add up to a large amount of money  considering the number of transactions that a large bank executes  a clever trap door could be included in a compiler the compiler could generate standard object code as well as a trap door  regardless of the source code being compiled this activity is particularly nefarious  since a search of the source code of the program will not reveal any problems only the source code of the compiler would contain the information  trap doors pose a difficult problem because  to detect them  we have to analyze all the source code for all components of a system given that software systems may consist of millions of lines of code  this analysis is not done frequently  and frequently it is not done at all ! 15.2 627 15.2.3 logic bomb consider a program that initiates a security incident only under certain circltmstances it would be hard to detect because under normal operations  there would be no security hole however  when a predefined set of parameters were met  the security hole would be created this scenario is known as a a programmer  for example  might write code to detect whether was still employed ; if that check failed  a daemon could be spawned to allow remote access  or code could be launched to cause damage to the site  15.2.4 stack and buffer overflow the stack or buffer-overflow attack is the most common way for an attacker outside the system  on a network or dial-up connection  to gain unauthorized access to the target system an authorized user of the system may also use this exploit for privilege escalation  essentially  the attack exploits a bug in a program the bug can be a simple case of poor programming  in which the programmer neglected to code bounds checking on an input field in this case  the attacker sends more data than the program was expecting by using trial and error  or by examining the source code of the attacked program if it is available  the attacker determines the vulnerability and writes a program to do the following  overflow an input field  command-line argument  or input buffer-for example  on a network daemon-wl.til it writes into the stack  overwrite the current return address on the stack with the address of the exploit code loaded in step 3  write a simple set of code for the next space in the stack that includes the commands that the attacker wishes to execute-for instance  spawn a shell  the result of this attack program 's execution will be a root shell or other privileged command execution  for instance  if a web-page form expects a user name to be entered into a field  the attacker could send the user name  plus extra characters to overflow the buffer and reach the stack  plus a new return address to load onto the stack  plus the code the attacker wants to run when the buffer-reading subroutine returns from execution  the return address is the exploit code  and the code is run  let 's look at a buffer-overflow exploit in more detail consider the simple c program in fig1-1re 15.2 this program creates a character array size buffer_size and copies the contents of the parameter provided on the command line-argv  1   as long as the size of this parameter is less than buffer_size  we need one byte to store the null terminator   this program works properly but consider what happens if the parameter provided on the command line is longer than buffer_size in this scenario  the strcpy   function will begin copying from argv  1  until it encounters a null terminator  \ 0  or until the program crashes thus  this program suffers from a potential problem in which copied data overflow the buffer array  628 chapter 15 # include stdio.h # define buffer_size 256 int main  int argc  char argv      char buffer  buffer_size  ; if  argc 2  return -1 ; else  strcpy  buffer,argv  1   ; return 0 ;  figure 15.2 c program with buffer-overflow condition  note that a careful programmer could have performed bounds checking on the size of argv  1  by using the strncpy   function rather than strcpy    replacing the line strcpy  buffer  argv  1   ; with strncpy  buffer  argv  1   sizeof  buffer  -1  ; .unfortunately  good bounds checking is the exception rather than the norm  furthermore  lack of bounds checking is not the only possible cause of the behavior of the program in figure 15.2 the program could instead have been carefully designed to compromise the integrity of the system we now consider the possible security vulnerabilities of a buffer overflow  when a function is invoked in a typical computer architecture  the variables defined locally to the function  sometimes known as automatic variables   the parameters passed to the function  and the address to which control returns once the function exits are stored in a stack frame the layout for a typical stack frame is shown in figure 15.3 examining the stack frame from top to bottom  we first see the parameters passed to the function  followed by any automatic variables declared in the function we next see the frame pointer  which is the address of the beginning of the stack frame finally  we have the return bottom ~ frame pointer grows top figure 15.3 the layout for a typical stack frame  15.2 629 address  which specifies where to return control once the function exits the frame pointer must be saved on the stack  as the value of the stack pointer can vary during the function call ; the saved frame pointer allows relative access to parameters and automatic variables  given this standard memory layout  a cracker could execute a bufferoverflow attack i  ter goal is to replace the return address in the stack frame so that it now points to the code segment containing the attacking program  the programmer first writes a short code segment such as the following  # include stdio.h int main  int argc  char argv     execvp  ' ' \ bin \ sh' '  ' ' \ bin \ sh' '  null  ; return 0 ; using the execvp   system call  this code segment creates a shell process  if the program being attacked runs with system-wide permissions  this newly created shell will gain complete access to the system of course  the code segment could do anything allowed by the privileges of the attacked process  this code segment is then compiled so that the assembly language instructions can be modified the primary modification is to remove unnecessary features in the code  thereby reducing the code size so that it can fit into a stack frame  this assembled code fragment is now a binary sequence that will be at the heart of the attack  refer again to the program shown in figure 15.2 let 's assume that when the main   function is called in that program  the stack frame appears as shown in figure 15.4  a   using a debugger  the programmer then finds the copied  a   b  figure 15.4 hypothetical stack frame for figure 15.2   a  before and  b  after  630 chapter 15 address of buffer  0  in the stack that address is the location of the code the attacker wants executed the binary sequence is appended with the necessary amount of no-op instructions  for no-operation  to fill the stack frame up to the location of the return address ; and the location of buffer  0   the new return address  is added the attack is complete when the attacker gives this constructed binary sequence as input to the process the process then copies the binary sequence from argv  1  to position buffer  0  in the stack frame  now  when control returns from main    instead of returning to the location specified by the old value of the return address  we return to the modified shell code  which runs with the access rights of the attacked process ! figure 15.4  b  contains the modified shell code  there are many ways to exploit potential buffer-overflow problems in this example  we considered the possibility that the program being attackedthe code shown in figure 15.2-ran with system-wide permissions however  the code segment that runs once the value of the return address has been modified might perform any type of malicious act  such as deleting files  opening network ports for further exploitation  and so on  this example buffer-overflow attack reveals that considerable knowledge and programming skill are needed to recognize exploitable code and then to exploit it unfortunately  it does not take great programmers to launch security attacks rather  one cracker can determine the bug and then write an exploit anyone with rudimentary computer skills and access to the exploita so-called then try to launch the attack at target systems  the buffer-overflow attack is especially pernicious because it can be run between systems and can travel over allowed communication channels such attacks can occur within protocols that are expected to be used to communicate with the target machine  and they can therefore be hard to detect and prevent  they can even bypass the security added by firewalls  section 15.7   one solution to this problem is for the cpu to have a feature that disallows execution of code in a stack section of memory recent versions of sun 's sparc chip include this setting  and recent versions of solaris enable it the return address of the overflowed routine can still be modified ; but when the return address is within the stack and the code there attempts to execute  an exception is generated  and the program is halted with an error  recent versions of amd and intel x86 chips include the nx feature to prevent this type of attack the use of the feature is supported in several x86 operating systems  including linux and windows xp sp2 the hardware implementation involves the use of a new bit in the page tables of the cpus this bit marks the associated page as nonexecutable  so that instructions can not be read from it and executed as this feature becomes prevalent  buffer-overflow attacks should greatly diminish  15.2.5 viruses another form of program threat is a a virus is a fragment of code embedded in a legitimate program viruses are self-replicating and are designed to infect other programs they can wreak havoc in a system by modifying or destroying files and causing system crashes and program malfunctions as with most penetration attacks  viruses are very specific to architectures  operating systems  and applications viruses are a particular problem for users of 15.2 631 pcs unix and other multiuser operating systems generally are not susceptible to viruses because the executable programs are protected from writing by the operating system even if a virus does infect such a progran  1  its powers usually are limited because other aspects of the system are protected  viruses are usually borne via e-mail  with spam the most comrnon vector  they can also spread when users download viral programs internet file-sharing services or exchange infected disks  another common form of virus transmission uses microsoft office files  such as microsoft word documents these documents can contain macros visual basic programs  that programs in the office suite powerpoint  and excel  will execute automatically because these programs run under the user 's own account  the macros can run largely unconstrained  for example  deleting user files at will   commonly  the virus will also e-mail itself to others in the user 's contact list here is a code sample that shows the simplicity of writing a visual basic macro that a virus could use to format the hard drive of a windows computer as soon as the file containing the macro was opened  sub autoopen   dim ofs set ofs = createobject  ' 'scripting.filesystemobject' '  vs = shell  ' 'c  command.com /k format c  ' ',vbhide  end sub how do viruses work once a virus reaches a target machine  a program known as a inserts the virus into the system the virus dropper is usually a trojan horse  executed for other reasons but installing the virus as its core activity once installed  the virus may do any one of a number of things there are literally thousands of viruses  but they fall into several main categories note that many viruses belong to more than one category  file a standard file virus infects a system by appending itself to a file  it changes the start of the program so that execution jumps to its code  after it executes  it returns control to the program so that its execution is not noticed file viruses are sometimes known as parasitic viruses  as they leave no full files behind and leave the host program still functional  boot a boot virus infects the boot sector of the system  executing every time the system is booted and before the operating system is loaded it watches for other boatable media  that is  floppy disks  and infects them  these viruses are also known as memory viruses  because they do not appear in the file system figure 15.5 shows how a boot virus works  macro most viruses are written in a low-levellanguage  such as assembly or c macro viruses are written in a high-level language  such as visual basic these viruses are triggered when a program capable of executing the macro is run for example  a macro virus could be contained in a spreadsheet file  source code a source code virus looks for source code and modifies it to include the virus and to help spread the virus  632 chapter 15 figure i 5.5 a boot-sector computer virus  polymorphic a polymorphic virus changes each time it is installed to avoid detection by antivirus software the changes do not affect the virus 's functionality but rather change the virus 's signature a a pattern that cili'i be used to identify a virus  typically a series make up the virus code  encrypted an encrypted virus includes decryption code along with the encrypted virus  again to avoid detection the virus first decrypts and then executes  stealth this tricky virus attempts to avoid detection by modifying parts of the system that could be used to detect it for example  it could modify the read system call so that if the file it has modified is read  the original form of the code is returned rather than the infected code  tunneling this virus attempts to bypass detection by an anti virus scanner by installing itself in the interrupt-handler chain similar viruses install themselves in device drivers  15.3 15.3 633 multipartite a virus of this type is able to infect nmltiple parts of a system  including boot sectors  memory  and files this makes it difficult to detect and contain  armored an armored virus is coded to ncake it hard for antivirus researchers to unravel and understand it can also be compressed to avoid detection and disinfection in addition  virus droppers and other full files that are part of a virus infestation are frequently hidden via file attributes or unviewable file names  this vast variety of viruses is likely to continue to grow in fact  in 2004 a new and widespread virus was detected it exploited three separate bugs for its operation this virus started by infecting hundreds of windows servers  including many trusted sites  running microsoft internet information server  iis   any vulnerable microsoft explorer web browser visiting those sites received a browser virus with any download the browser virus installed several back-door programs  including a which records all things entered on the keyboard  including and credit-card numbers   it also installed a daemon to allow unlimited remote access by an intruder and another that allowed an intruder to route spam through the infected desktop computer  generally  viruses are the most disruptive security attacks ; and because they are effective  they will continue to be written and to spread the active debates within the computing community is whether a jth'-yhcn ; _ in which many systems run the same hardware  operating system  and/ or application software  is increasing the threat of and damage caused by security intrusions this monoculture supposedly consists of microsoft products  and part of the debate concerns whether such a monoculture even exists today  program threats typically use a breakdown in the protection mechanisms of a system to attack programs in contrast  system and network threats involve the abuse of services and network comcections system and network threats create a situation in which operating-system resources and user files are inisused  sometimes a system and network attack is used to launch a program attack  and vice versa  the more an operating system is-the more services it has enabled and the more functions it allows-the more likely it is that a is available to exploit increasingly  operating systems strive to be for example  solaris 10 moved from a model in which many services  ftp  telnet  and others  were enabled by default when the system was installed to a model in which almost all services are disabled at installation time and must specifically be enabled system administrators such changes reduce the system 's set of ways in which an attacker can to break into the system  in the remainder of this section  we discuss some examples of system and network threats  including worms  port scamcing  and denial-of-service attacks it is important to note that masquerading and replay attacks are also 634 chapter 15 commonly launched over netvvorks between systems in fact  these attacks are more effective and harder to counter when multiple systems are involved  for example  within a computer  the operating system usually can determine the sender and receiver of a message even if the sender changes to the id of someone else  there may be a record of that id change when multiple systems are involved  especially systems controlled by attackers  then such tracing is much more difficult  in general  we can say that sharing secrets  to prove identity and as keys to encryption  is required for authentication and encryption  and sharing secrets is easier in environments  such as a single operating system  in which secure sharing methods exist these methods include shared memory and interprocess comnmnications creating secure communication and authentication is discussed in sections 15.4 and 15.5  15.3.1 worms a is a process that uses the mechanism to ravage system performance the worm spawns copies of itself  using up system resources and perhaps locking out all other processes on computer networks  worms are particularly potent  since they may reproduce themselves among systems and thus shut down an entire network such an event occurred in 1988 to unix systems on the internet  causing the loss of system and system-administrator time worth millions of dollars  at the close of the workday on november 2  1988  robert tappan morris  jr  a first-year cornell graduate student  unleashed a worm program on one or more hosts corm.ected to the internet targeting sun microsystems ' sun 3 workstations and vax computers running variants of version 4 bsd unix  the worm quickly spread over great distances ; within a few hours of its release  it had consumed system resources to the point of bringing down the infected machines  although robert morris designed the self-replicating program for rapid reproduction and distribution  some of the features of the unix networking environment provided the means to propagate the worm throughout the system  it is likely that morris chose for in.itial infection an internet host left open for and accessible to outside users from there  the worm program exploited flaws in the unix operating system 's security routines and took advantage of unix utilities that simplify resource sharing in local-area networks to gain unauthorized access to thousands of other connected sites morris 's methods of attack are outlined next  the worm was made up of two programs  a  also called a or program and the main program ll.c  the grappling hook consisted of 99 lines of c code compiled and run on each machine it accessed once established on the computer system under attack  the grappling hook connected to the machine where it originated and uploaded a copy of the main worm onto the hooked system  figure 15.6   the main program proceeded to search for other machines to which the newly infected system could connect easily in these actions  morris exploited the unix networking utility rsh for easy remote task execution by setting up special files that list host-login name pairs  users can omit entering a password each time they access a remote account on the paired list the worm searched these special files for site names 15.3 635 rsh attack finger attack sendmail attack worm sent target system infected system figure 15.6 the morris internet worm  that would allow remote execution without a password where remote shells were established  the worm program was uploaded and began executing anew  the attack via remote access was one of three infection methods built into the worm the other two methods involved operating-system bugs in the unix finger and sendmail programs  the finger utility functions as an electronic telephone directory ; the command finger user-name hostname returns a person 's real and login names along with other information that the user may have provided  such as office and home address and telephone number  research plan  or clever quotation finger runs as a background process  or daemon  at each bsd site and responds to queries throughout the internet the worm executed a buffer-overflow attack on finger the program queried finger with a 536-byte string crafted to exceed the buffer allocated for input and to overwrite the stack frame instead of returning to the main routine where it resided before morris 's calt the finger daemon was routed to a procedure within the invading 536-byte string now residing on the stack the new procedure executed /bin/ sh  which  if successful  gave the worm a remote shell on the machine under attack  the bug exploited in sendmail also involved using a daemon process for malicious entry sendmail sends  receives  and routes electronic mail  debugging code in the utility permits testers to verify and display the state of the ncail system the debugging option was useful to system administrators and was often left on morris included in his attack arsenal a call to debug that -instead of specifying a user address  as would be normal in testing-issued a set of cornmands that mailed and executed a copy of the grappling-hook program  once in place  the main worm systematically attempted to discover user passwords it began by trying simple cases of no password or passwords constructed of account-user-name combinations  then used comparisons with an internal dictionary of 432 favorite password choices  and then went to the 636 chapter 15 final stage of trying each word in the standard unix on-line dictionary as a possible password this elaborate and efficient three-stage password-cracking algorithm enabled the worm to gain access to other user accounts on the infected system the wontt then searched for rsh data files in these newly broken accounts and used them as described previously to gain access to user accounts on remote systems  with each new access  the worm program searched for already active copies of itself if it found one  the new copy exited  except in every seventh instance had the worm exited on all duplicate sightings  it might have remained undetected allowing every seventh duplicate to proceed  possibly to confound efforts to stop its spread baiting with fake worms  created a wholesale infestation of sun and vax systems on the internet  the very features of the unix network environment that assisted il l the worm 's propagation also helped to stop its advance ease of electronic communication  mechanisms to copy source and binary files to remote machines  and access to both source code and human expertise allowed cooperative efforts to develop solutions quickly by the evening of the next day  november 3  methods of halting the invading program were circulated to system administrators via the internet within days  specific software patches for the exploited security flaws were available  why did morris unleash the worm the action has been characterized as both a harmless prank gone awry and a serious criminal offense based on the complexity of the attack  it is unlikely that the worm 's release or the scope of its spread was unintentional the worm program took elaborate steps to cover its tracks and to repel efforts to stop its spread yet the program contained no code aimed at damaging or destroying the systems on which it ran the author clearly had the expertise to include such commands ; in fact  data structures were present in the bootstrap code that could have been used to transfer trojan-horse or virus programs the behavior of the program may lead to interesting observations  but it does not provide a sound basis for inferring motive what is not open to speculation  however  is the legal outcome  a federal court convicted morris and handed down a sentence of three years ' probation  400 hours of community service ; and a $ 10,000 fine morris 's legal costs probably exceeded $ 100,000  security experts continue to evaluate methods to decrease or eliminate worms a more recent event ; though  shows that worms are still a fact of life on the internet it also shows that as the internet grows  the damage that even harmless worms can do also grows and can be significant this example occurred during august 2003 the fifth version of the sobig worm  more properly known as w32.sobig.f @ mm  was released by persons at this time unknown it was the fastest-spreading worm released to date  at its peak mfecting hundreds of thousands of computers and one in seventeen e-mail messages on the internet it clogged e-mail inboxes  slowed networks  and took a huge number of hours to clean up  sobig.f was launched by being uploaded to a pornography newsgroup via an account created with a stolen credit card it was disguised as a photo the virus targeted microsoft windows systems and used its own smtp engine to e-mail itself to all the addresses found on an infected system it used a variety of subject lines to help avoid detection  including thank you ! your details,' ' and re  approved it also used a random address on the host as the from  15.3 637 address  making it difficult to determine from the message which machine was the infected source sobig.f included an attachment for the target e-mail reader to click on  again with a variety of names if this payload was executed  it stored a program called winppr32.exe in the default windows directory  along with a text file it also modified the windows registry  the code included in the attachment was also programmed to periodically attempt to connect to one of twenty servers and download and execute a program from them fortunately  the servers were disabled before the code could be downloaded the content of the program from these servers has not yet been determined if the code was malevolent  untold damage to a vast number of machines could have resulted  15.3.2 port scanning port scanning is not an attack but rather a means for a cracker to detect a system 's vulnerabilities to attack port scanning typically is automated  involving a tool that attempts to create a tcp lip connection to a specific port or a range of ports for example  suppose there is a known vulnerability  or bug  in sendmail a cracker could launch a port scanner to try to connect  say  to port 25 of a particular system or to a range of systems if the connection was successful  the cracker  or tool  could attempt to communicate with the answering service to determine if the service was indeed sendmail and  if so  if it was the version with the bug  now imagine a tool in which each bug of every service of every operath g system was encoded the tool could attempt to connect to every port of one or nwre systems for every service that answered  it could try to use each known bug frequently  the bugs are buffer overflows  allowing the creation of a privileged command shell on the system from there  of course  the cracker could install trojan horses  back-door programs  and so on  there is no such tool  but there are tools that perform subsets of that functionality for example  nmap  from http  / /www.insecure.org/mrtap/  is a very versatile open-source utility for network exploration and security auditing when pointed at a target  it will determine what services are n.1n..tling  including application names and versions it can identify the host operating system it can also provide information about defenses  such as what firewalls are defending the target it does not exploit any known bugs  nessus  from http  / /www.nessus.org/  performs a similar function  but it has a database of bugs and their exploits it can scan a range of systems  determine the services running on those systems  and attempt to attack all appropriate bugs it generates reports about the results it does not perform the final step of exploiting the found bugs  but a knowledgeable cracker or a script kiddie could  because port scans are detectable  section 15.6.3   they frequently are launched from such systems are previously compromised  independent systems that are serving their owners while being used for nefarious purposes  including denial-of-service attacks and spam relay zombies make crackers particularly difficult to prosecute because determining the source of the attack and the person that launched it is challenging this is one of many reasons for securing inconsequential systems  not just systems containing valuable information or services  638 chapter 15 15.3.3 denial of service as mentioned earlier  denial-of-service attacks are aimed not at gaming information or stealing resources but rather at disrupting legitimate use of a system or facility most such attacks involve systems that the attacker has not penetrated indeed  launching an attack that prevents legitimate use is frequently easier than breaking into a machine or facility  denial-of-service attacks are generally network based they fall into two categories attacks in the first category use so many facility resources that  in essence  no useful work can be done for example  a web-site click could download a java applet that proceeds to use all available cpu time or to pop up windows infinitely the second category involves disrupting the network of the facility there have been several successful denial-of-service attacks of this kind against major web sites these attacks result from abuse of some of the fundamental functionality of tcp lip for instance  if the attacker sends the part of the protocol that says i want to start a tcp connection  but never follows with the standard the connection is now complete  the result can be partially started tcp sessions if enough of these sessions are launched  they can eat up all the network resources of the system  disabling any further legitimate tcp connections such attacks  which can last hours or days  have caused partial or full failure of attempts to use the target facility the attacks are usually stopped at the network level until the operating systems can be updated to reduce their vulnerability  generally  it is impossible to prevent denial-of-service attacks the attacks use the same mechanisms as normal even more difficult to prevent and resolve are these attacks are launched from multiple sites at once  toward a common target  typically by zombies ddos attacks have become more comncon and are sometimes associated with blackmail attempts a site comes under attack  and the attackers offer to halt the attack in exchange for money  sometimes a site does not even know it is under attack it can be difficult to determine whether a system slowdown is an attack or just a surge in system use consider that a successful advertising campaign that greatly increases traffic to a site could be considered a ddos  there are other interesting aspects of dos attacks for example  if an authentication algorithm locks an account for a period of time after several incorrect attempts to access the account  then an attacker could cause all authentication to be blocked by purposely making incorrect attempts to access all accounts similarly  a firewall that automatically blocks certain kinds of traffic could be induced to block that traffic when it should not these examples suggest that programmers and systems managers need to fully understand the algorithms and technologies they are deploying finally  computer science classes are notorious sources of accidental system dos attacks consider the first programming exercises in which students learn to create subprocesses or threads a common bug involves spawning subprocesses infinitely the system 's free memory and cpu resources do n't stand a chance  there are many defenses against computer attacks  running the gamut from methodology to technology the broadest tool available to system designers 15.4 639 and users is cryptography in this section  we discuss the details of cryptography and its use in computer security  in an isolated computer  the operating system can reliably determine the sender and recipient of ali interprocess communication  since it controls all communication channels in the computer in a network of computers  the situation is quite different a networked computer receives bits from the wire with no immediate and reliable way of determining what machine or application sent those bits similarly  the computer sends bits onto the network with no of knowing who might eventually receive them  commonly  network addresses are used to infer the potential senders and receivers of network messages network packets arrive with a source address  such as an ip address and when a computer sends a message  it names the intended receiver by specifying a destination address however  for applications where security matters  we are asking for trouble if we assume that the source or destination address of a packet reliably determines who sent or received that packet a rogue computer can send a message with a falsified source address  and numerous computers other than the  one specified by the destination address can  and typically do  receive a packet for example  all of the routers on the way to the destination will receive the packet  too how  then  is an operating system to decide whether to grant a request when it can not trust the named source of the request and how is it supposed to provide protection for a request or data when it can not determine who will receive the response or message contents it sends over the network it is generally considered infeasible to build a network of any scale in which the source and destination addresses of packets can be trusted in this sense therefore  the only alternative is somehow to eliminate the need to trust the network this is the job of cryptography abstractly  ~ ' ' ~ .,-ro.rnron ~ used to constrain the potential senders and/ or receivers of a message  cryptography is based on secrets called that are selectively distributed to computers in a network and used to process messages cryptography enables a recipient of a message to verify that the message was created by some computer possessing a certain key-the key is the source of the message similarly  a sender can encode its message so that only a computer with a certain key can decode the message  so that the key becomes the destination unlike network addresses  however  keys are designed so that it is not computationally feasible to derive them from the messages they were used to generate or from any other public information thus  they provide a much more trustworthy means of constraining senders and receivers of messages note that cryptography is a field of study unto itself  with large and small complexities and subtleties  here  we explore the most important aspects of the parts of cryptography that pertain to operating systems  15.4.1 encryption because it solves a wide variety of communication security problems  is used frequently in many aspects of modern computing encryption a means for constraining the possible receivers of a message an encryption algorithm enables the sender of a message to ensure that only a computer possessing a certain key can read the message encryption of messages is an ancient practice  of course  and there have been many encryption algorithms  640 chapter 15 dating back to ancient times in this section  we describe important modern encryption principles and algorithms  figure 15.7 shows an example of two users communicating securely over an insecure channel we refer to this figure throughout the section note that the key exchange can take place directly between the two parties or via a trusted third party  that is  a certificate authority   as discussed in section 15.4.1.4  an encryption algorithm consists of the following components  a set k of keys  a set m of messages  a set c of ciphertexts  a function e  k  +  m  + c   thatis  for each k e k  e  k  is a function for generating ciphertexts from messages both e and e  lc  for any k should be efficiently computable functions  a function d  i  +  c  + m   thatis  for eachlc e i  d  k  is a function for generating messages from ciphertexts both d and d  lc  for any k should be efficiently computable functions  an encryption algorithm must provide this essential property  given a ciphertext c e c a computer can compute m such that e  lc   m  = c only if it possesses write ----1 rnessage rn 1 i ii 12  0 ~  z ~ -x ~  read 1 message ml figure i5.7 a secure communication over an insecure medium  15.4 641 d  lc   thus  a computer holding d  lc  can decrypt ciphertexts to the plaintexts used to produce them  but a computer not holding d  lc  can not decrypt ciphertexts  since ciphertexts are generally exposed  for example  sent on a network   it is important that it be infeasible to derive d  lc  from the ciphertexts  there are two main types of encryption algorithms  symmetric and asymmetric we discuss both types in the following sections  15.4.1.1 symmetric encryption in a the same key is used to encrypt and to decrypt that is  e can be from d  lc   and vice versa therefore  the secrecy of e  lc  must be protected to the same extent as that of d  lc   for the past several decades  the most commonly used symmetric encryption algorithm in the united states for civilian applications has been the adopted by the national institute of stantechxwlogy  nist   des works by taking a 64-bit value and a 56-bit key and performing a series of transformations these transformations are based on substitution and permutation operations  as is generally the case for symmetric encryption transformations some of the transformations are in that their algorithms are hidden in fact  these so-called s-boxes are classified by the united states government messages longer than 64 bits are broken into 64-bit chunks because des works on a chunk of bits at a time  is known as a cipher if the same key is used for encrypting an extended anwunt of data  it becomes vulnerable to attack  consider  for example  that the same source block would result in the same ciphertext if the same key and encryption algorithm were used therefore  the chunks are not just encrypted but also exclusive-or'ed  xored  with the ciphertext block before encryption this is known as des is now considered insecure for many applications because its keys can be exhaustively searched with moderate computing resources rather than giving up on des  though  nist created a modification called in which the des algorithm is repeated three times  two encryptions and one decryption  on the same plaintext usli'lg two or three keys-for example  c = e  k3   d  lc2   e  k1   m     when three keys are used  the effective key length is 168 bits triple des is in widespread use today  in 2001  nist a new encryption algorithm  called the to replace des aes is another symmetric block cipher it can use key lengths of 128  192  and 256 bits and works on 128-bit blocks it works by performing 10 to 14 rounds of transformations on a matrix formed from a block generally  the algorithm is compact and efficient  several other symmetric block encryption algorithms in use today bear mentioning the algorithm is fast compact  and easy to implement it can use a variable key length of up to 256 bits and works on 128-bit blocks  can vary in key length  number of transformations  and block size because it uses only basic computational operations  it can run on a wide variety of crus  is perhaps the most common stream cipher a is designed to encrypt and decrypt a stream of bytes or bits rather than a block  this is useful when the length of a communication would make a block cipher too slow the key is input into a pseudo-random-bit generator  which is an 642 chapter 15 algorithm that attempts to produce random bits the output of the generator when fed a key is a keystream a is an infinite set of keys that can be used for the input plaintext stream rc4 is used in encrypting steams of data  such as in wep  the wireless lan protocol lt is also used in communications between web browsers and web servers  as we discuss below unfortunately  rc4 as used in wep  ieee standard 802.11  has been found to be breakable in a reasonable amount of con1.puter time in fact rc4 itself has vulnerabilities  15.4.1.2 asymmetric encryption in an there are different encryption and decryption keys here  we one such algorithm  known as rsa after the names of its inventors  rivest  shamir  and adleman   the rsa cipher is a block-cipher public-key algorithm and is the most widely used asymmetrical algorithm asymmetrical algorithms based on elliptical curves are gaining ground  however  because the key length of such an algorithm can be shorter for the same amount of cryptographic strength  it is computationally infeasible to derive d  kd  n  from e  lee  n   and so e  ke  n  need not be kept secret and can be widely disseminated ; thus  e  lee  n   or just ice  is the and d  kd  n   or just led  is the n is the write 'i messlge 69/ 0 isl l m ~ encryption_... 695 mod 91 key k5.91 ~  + ' gl ~ gl  j c uc m m  f  .r     ~ 0 ' 1      n read ~ figure 15.8 encryption and decryption using rsa asymmetric cryptography  15.4 643 product of two large  randomly chosen prime numbers p and q  for example  p andq are512bitseach   theencryptionalgorithmis e  kc  n   rn  = mk  mod n  where icc satisfies leekd mod  p -1   q -1  = 1 the decryption algorithm is then d  kd  n   c  = ckd mod n an example using small values is shown in figure 15.8 in this example  we make p = 7 and q = 13 we then calculate n = 7 13 = 91 and  p-1   q -1  = 72  we next select kc relatively prime to 72 and 72  yielding 5 finally  we calculate kd such that kekrt mod 72 = 1  yielding 29 we now have our keys  the public key  lee  n = 5  91  and the private key  led  n = 29  91 encrypting the message 69 with the public key results in the message 62  which is then decoded by the receiver via the private key  the use of asymmetric encryption begins with the publication of the public key of the destination for bidirectional communication  the source also must publish its public key publication can be as simple as handing over an electronic copy of the key  or it can be more complex the private key  or secret key  must be jealously guarded  as anyone holding that key can decrypt any message created by the matching public key  we should note that the seemingly small difference in key use between asymmetric and symmetric cryptography is quite large in practice asymmetric cryptography is based on mathematical functions rather than transformations  inaking it much more computationally expensive to execute it is much faster for a computer to encode and decode ciphertext by using the usual symmetric algorithms than by using asymmetric algorithms why  then  use an asymmetric algorithm in truth  these algorithms are not used for generalpurpose encryption of large amounts of data however  they are used not only for encryption of small amounts of data but also for authentication  confidentiality  and key distribution  as we show in the following sections  15.4.1.3 authentication we have seen that encryption offers a way of constraining the set of possible receivers of a message constraining the set of potential senders of a message is called authentication is thus complementary to encryption in fact  sometimes their functions overlap consider that an encrypted message can also prove the identity of the sender for example  if d  kd  n   e  ke n   m   produces a valid message  then we know that the creator of the message must hold ke authentication is also useful for proving that a message has not been modified in this section  we discuss authentication as a constraint on possible receivers of a message note that this sort of authentication is similar to but distinct from user authentication  which we discuss in section 15.5  an authentication algorithm consists of the following components  a set k of keys  a set m of messages  a set a of authenticators  a functions  k    m + a   that is  for each k e k  s  k  is a function for generating authenticators from messages both sand s  k  for any k should be efficiently computable functions  644 chapter 15 a function v     +  m x a +  true  false    that is  for each lc e k  v  lc  is a function for verifying authenticators on messages both v and v  lc  for any lc should be efficiently computable functions  the critical property that an authentication algorithm must possess is this  for a message m  a computer can generate an authenticator a e a such that v  lc   m  a  = true only if it possesses s  lc   thus  a computer holding s  lc  can generate authenticators on messages so that any computer possessing v  lc  can verify them however  a computer not holding s  lc  can not generate authenticators on messages that can be verified using v  lc   since authenticators are generally exposed  for example  sent on a network with the messages themselves   it must not be feasible to derive s  lc  from the authenticators  just as there are two types of encryption algorithms  there are two main varieties of authentication algorithms the first in understanding these algorithms is to explore hash functions a h  m  creates a small  fixed-sized block of data  known as a or from a message m hash functions work by taking a message in n-bit blocks and processing the blocks to produce an n-bit hash h must be collision resistant on m-that is  it must be infeasible to find an 1111 # m such that h  m  = h  n/   now  if h  m  = h  m1   we know that m m1 -that is  we know that the message has not been modified common message-digest functions include which produces a 128-bit hash  and which outputs a 160-bit hash  message digests are useful for detecting changed messages but are not useful as authenticators for example  h  m  can be sent along with a message ; but if his known  then someone could modify m and recompute h  m   and the message modification would not be detected therefore  an authentication algorithm takes the message digest and encrypts it  the first main type of authentication algorithm uses symmetric encryption  in a a cryptographic checksum is generated from message using a secret key knowledge of v  lc  and knowledge of s  lc  are equivalent  one can be derived from the other  so lc must be kept secret a simple example of a mac defines s  lc   m  = f  k  h  m    where f is a function that is one-way on its first argument  that is  k cam10t be derived from f  k  h  m     because of the collision resistance in the hash function  we are reasonably assured that no other message could create the same mac a suitable verification algorithm is then v  lc   m  a  =  j  lc  m  = a   note that k is needed to compute both s  lc  and v  lc   so anyone able to compute one can compute the other  the second main type of authentication algorithm is a and the authenticators thus produced are called in a digital-signature algorithm  it is computationally to derive s  ks  from v  lcv  ; in particular  vis a one-way function thus  kv is the public key and lc5 is the private key  consider as an example the rsa digital-signature algorithm it is similar to the rsa encryption algorithm  but the key use is reversed the digital signature of a message is derived by computing s  lcs   m  = h  m  s mod n  the key /c5 again is a pair  d  n   where n is the product of two large  randomly chosen prime numbers p and q the verification algorithm is then v  kv   m  a  =  ak mod n = h  m    where kv satisfies lc   c5 mod  p  1   q  1  = 1  15.4 645 lf encryption can prove the identity of the sender of a m ~ essage  then why do we need separate authentication algorithms there are three primary reasons  authentication algorithms generally require fewer computations  with the notable exception of h.sa digital signatures   over large amounts of plaintext  this efficiency can make a huge difference in resource use and the time needed to authenticate a message  the authenticator of a message is almost always shorter than the message and its ciphertext this improves space use and transmission time efficiency  sometimes  we want authentication but not confidentiality for example  a company could provide a software patch and could sign that patch to prove that it came from the company and that it has n't been modified  authentication is a component of many aspects of security for example  it is the core of which supplies proof that an entity performed an action a typical example of nonrepudiation involves the filling out of electronic forms as an alternative to the signing of paper contracts nonrepudiation assures that a person filling out an electronic form can not deny that he did so  15.4.1.4 key distribution certainly  a good part of the battle between cryptographers  those inventing ciphers  and cryptanalysts  those trying to break them  involves keys with symmetric algorithms  both parties need the key  and no one else should have it the delivery of the symmetric key is a huge challenge sometimes it is performed cut-or-band -say  via a paper document or a conversation  these methods do not scale well  however also consider the key-management challenge suppose a user wanted to communicate with n other users privately  that user would need n keys and  for more security  would need to change those keys frequently  these are the very reasons for efforts to create asymmetric key algorithms  not only can the keys be exchanged in public  but a given user needs only one private key  no matter how many other people she wants to communicate with there is still the matter of managing a public key for each party to be communicated with  but since public keys need not be secured  simple storage can be used for that unfortunately  even the distribution of public keys requires some care  consider the man-in-the-middle attack shown in figure 15.9 here  the person who wants to receive an encrypted message sends out his public key  but an attacker also sends her bad public key  which matches her private key   the person who wants to send the encrypted message knows no better and so uses the bad key to encrypt the message the attacker then happily decrypts it  the problem is one of authentication-what we need is proof of who  or what  owns a public one to solve that problem involves the use of digital certificates a is a public key digitally signed by a trusted party the trusted party receives proof of identification from some entity 646 chapter 15 encryption __  key kbad attacker decryption key kd __  co ' -.u ' c ; aq cd 0 _ decryption ...  key kbad .,......_read  + message m i figure 15.9 a man-in-the-middle attack on asymmetric cryptography  and certifies that the public key we can trust the certifier these have their public keys i.j.l.cluded within web browsers  and other consumers of certificates  before they are distributed the certificate authorities can then vouch for other authorities  digitally signing the public keys of these other authorities   and so on  creating a web of trust the certificates can be distributed in a standard x.509 digital certificate format that can be parsed by computer this scheme is used for secure web communication  as we discuss in section 15.4.3  15.4.2 implementation of cryptography network protocols are typically organized in each layer acting as a client of the one below it that is  when one protocol generates a message to send to its protocol peer on another machine  it hands its message to the protocol below it in the network-protocol stack for delivery to its peer on that machine  for example  in an ip network  tcp  a transport-layer protocol  acts as a client of ip  a network-layer protocol   tcp packets are passed down to ip for delivery to the tcp peer at the other end of the tcp connection ip encapsulates the tcp 15.4 647 packet in an ip packet  which it similarly passes down to the data-link layer to be transmitted across the network to its ip peer on the destination computer this ip peer then delivers the tcp up to the tcp peer on that machine all in all  the which has been almost universally adopted as a model for data networking  defines seven such protocol layers  you will read more about the iso model of networking in chapter 16 ; figure 16.6 shows a diagram of the model  cryptography can be inserted at almost any layer in the iso model ssl  section 15.4.3   for example  provides security at the transport layer networklayer security generally has been standardized on which defines ip packet formats that allow the insertion of authenticators and the encryption of packet contents it uses symmetric encryption and uses the protocol for key ipsec is becoming widely used as the basis for in which all traffic between two ipsec endpoints is encrypted to make a private network out of one that may otherwise be public numerous protocols also have been developed for use by applications  but then the applications themselves must be coded to implement security  where is cryptographic protection best placed in a protocol stack in general  there is no definitive answer on the one hand  more protocols benefit from protections placed lower in the stack for example  since ip packets encapsulate tcp packets  encryption of ip packets  using ipsec  for example  also hides the contents of the encapsulated tcp packets similarly  authenticators on ip packets detect the modification of contaii1.ed tcp header information  on the other hand  protection at lower layers in the protocol stack may give insufficient protection to higher-layer protocols for example  an application server that runs over ipsec might be able to authenticate the client computers from which requests are received however  to authenticate a user at a client computer  the server may need to use an application-level protocol-for example  the user may be required to type a password also consider the problem of e-mail e-mail delivered via the industry standard smtp protocol is stored and forwarded  frequently multiple times  before it is delivered each of these transmissions could go over a secure or an insecure network for e-mail to be secure  the e-mail message needs to be encrypted so that its security is independent of the transports that carry it  15.4.3 an example  ssl ssl 3.0 is a cryptographic protocol that enables two computers to corrumjj1icate securely-that is  so that each can limit the sender and receiver of to the other it is perhaps the most commonly used cryptographic on the internet today  since it is the standard protocol by which web communicate securely with web servers for completeness  we should note that ssl was designed by netscape and that it evolved into the industry standard tls protocol in this discussion  we use ssl to mean both ssl and tls  ssl is a complex protocol with many options here  we present only a single variation of it  and even then in a very simplified and abstract form  so as to maintain focus on its use of cryptographic primitives what we are about to see is a complex dance in which asymmetric cryptography is used so that a client and a server can establish a secure  -cey that can be used for symmetric encryption of the session between the two-all of this while 648 chapter 15 avoiding man-in-the-middle and replay attacks for added cryptographic strength  the session keys are forgotten once a session is completed another communication between the two will generation of new session keys  the ssl protocol is initiated by a c to communicate securely with a prior to the protocol 's use  the server s is assumed to have obtained a certificate  denoted cert  from certification authority ca this certificate is a structure containing the following  various attributes attrs of the server  such as its unique distinguished name and its common  dns  name the identity of a public encryption algorithm e   for the server the public key kc of this server a validity interval interval durirtg which the certificate should be considered valid a digital signature a on the above information made by theca-that is  a = s  kca    attrs  e  ke   interval   in addition  prior to the protocol 's use  the client is presumed to have obtained the public verification algorithm v  kca  for ca in the case of the web  the user 's browser is shipped from its vendor containing the verification algorithms and public keys of certain certification authorities the user can add or delete these for certification authorities as she chooses  when c connects to s 1 it sends a 28-byte random value nc to the server  which responds with a random value n5 of its own  plus its certificate cert5 the client verifies that v  kca    attrs  e  lee   interval   a  = true and that the current time is in the validity interval interval if both of these tests are satisfied  the server has proved its identity then the client generates a random 46-byte and sends cpms = e  ks   pms  to the server the server recovers pms = d  kd   cpms   now both the client and the server are in possession of nc  n5  and pms  and each can cmnpute a shared 48-byte l ' '' ' c f  nc  715  pms   where f is a one-way and collision-resistant function  server and client can compute ms  since only they know pms  dependence of ms on nc and n5 ensures that ms is a fresh value-that is  a session key that has not been used in a previous communication at this point  the client and the server both compute the keys the ms  a symmetric encryption key k ~  ypt for encrypting messages from to the server client a symmetric encryption to the client lc ~ rypt for encrypting messages from the server a mac generation jc ~ ac generating authenticators on from the client to the server a mac generation k ~ ~ ac for generating authenticators on from the server to the to send a message m to the server  the client sends 15.5 15.5 649 upon receiving c  the server recovers  m a  = d  jc ~ pt   c  and accepts m if v  lc ~ ac   m  a  = true similarly  to send a message m to the client  the server sends and the client recovers and accepts m if v  k ~ ac   m  a  = true  this protocol enables the server to limit the recipients of its messages to the client that generated pms and to limit the senders of the messages it accepts to that same client similarly  the client can limit the recipients of the messages it sends and the senders of the messages it accepts to the party that knows s  kd   that is  the party that can decrypt cpms   in many applications  such as web transactions  the client needs to verify the identity of the party that knows s  lcd   this is one purpose of the certificate cert5 ; in particular  the attrs field contains information that the client can use to determine the identityfor example  the domain name-of the server with which it is communicating  for applications in which the server also needs information about the client  ssl supports an option by which a client can send a certificate to the server  in addition to its use on the internet  ssl is being used for a wide variety of tasks for example  ipsec vpns now have a competitor in ssl vpns ipsec is good for point-to-point encryption of traffic-say  between two company offices ssl vpns are more flexible but not as efficient  so they might be used between an individual employee working remotely and the corporate office  our earlier discussion of authentication involves messages and sessions but what about users if a system can not authenticate a user  then authenticating that a message can'le from that user is thus  a major security problem for operating systems is the protection system depends on the ability to identify the programs and processes currently executing  which in turn depends on the ability to identify each user the system users identify themselves how do we determine a user 's is authentic generally  user authentication is based on one or more things  the user 's possession of something  a or card   the user 's of something user identifier and password   an attribute retina or signature   15.5.1 passwords the most comm.on when the user a user is the use of user id or account name  she 650 chapter 15 is asked for a password i the user-supplied password matches the password stored in the system  the system assumes that the account is being accessed by the owner of that account  passwords are often used to protect objects in the computer system  in the absence of more complete protection schemes they can be considered a special case of either keys or capabilities for instance  a password may be associated with each resource  such as a file   whenever a request is made to use the resource  the password nmst be given if the password is correct  access is granted different passwords may be associated with different access rights  for example  different passwords may be used for reading files  appending files  and updating files  in practice  most systems require only one password for a user to gain full rights although more passwords theoretically would be more secure  such systems tend not to be implemented due to the classic trade-off between security and convenience if security makes something inconvenient then the security is frequently bypassed or otherwise circumvented  15.5.2 password vulnerabilities passwords are extremely common because they are easy to understand and use  unfortunately  passwords can often be guessed  accidentally exposed  sniffed  or illegally transferred from an authorized user to an unauthorized one  as we show next  there are two common ways to guess a password one way is for the intruder  either human or program  to know the user or to have information about the user all too frequently  people use obvious information  such as the names of their cats or spouses  as their passwords the other way is to use brute force  trying enumeration-or all possible combinations of valid password characters  letters  numbers  and punctuation on some systems  -until the password is found short passwords are especially vulnerable to this method  for example  a four-character password provides only 10,000 variations on average  guessing 5,000 times would produce a correct hit a program that could try a password every millisecond would take only about 5 seconds to guess a four-character password enumeration is less successful where systems allow longer passwords that include both uppercase and lowercase letters  along with numbers and all punctuation characters of course  users must take advantage of the large password space and must not  for example  use only lowercase letters  in addition to being guessed  passwords can be exposed as a result of visual or electronic monitoring an intruder can look over the shoulder of a user when the user is logging iil and can learn the password easily by watching the keyboard alternatively  anyone with access to the network on which a computer resides can seamlessly add a network monitor  allowing him to watch all data being transferred on the network including user ids and passwords encrypting the data stream containing the password solves this problem even such a system could have passwords stolen  however for example  if a file is used to contain the passwords  it could be copied for off-system analysis or consider a trojan-horse program installed on the system that captures every keystroke before sending it on to the application  15.5 651 exposure is a particularly severe problem if the password is written down where it can be read or lost as we shall see  some systems force users to select hard-to-remember or long passwords  which may cause a user to record the password or to reuse it as a result  such systems provide much less security than systems that allow users to select easy passwords ! the final type of password compromise  illegal transfer  is the result of human nature most computer installations have a rule that forbids users to share accounts this rule is sometimes implemented for accounting reasons but is often aimed at improving security for instance  suppose one user id is shared by several users  and a security breach occurs from that user id it is impossible to know who was using the id at the time the break occurred or even whether the user was an authorized one with one user per user id  any user can be questioned directly about use of the account ; in addition  the user might notice something different about the account and detect the break-in sometimes  users break account-sharing rules to help friends or to circumvent accounting  and this behavior can result in a system 's being accessed by unauthorized users -possibly harmful ones  passwords can be either generated by the system or selected by a user  system-generated passwords may be difficult to remember  and thus users may write them down as mentioned  however  user-selected passwords are often easy to guess  the user 's name or favorite car  for example   some systems will check a proposed password for ease of guessing or cracking before accepting it  at some sites  administrators occasionally check user passwords and notify a user if his password is easy to guess some systems also age passwords  forcing users to change their passwords at regular intervals  every three months  for instance   this method is not foolproof either  because users can easily toggle between two passwords the solution  as implemented on some systems  is to record a password history for each user for instance  the system could record the last n passwords and not allow their reuse  several variants on these simple password schemes can be used for example  the password can be changed more frequently in the extren  1e  the password is changed from session to session a new password is selected  either by the system or by the user  at the end of each session  and that password must be used for the next session in such a case  even if a password is misused  it can be used only once when the legitimate user tries to use a now-invalid password at the next session  he discovers the security violation steps can then be taken to repair the breached security  15.5.3 encrypted passwords one problem with all these approaches is the difficulty of keeping the password secret within the computer how can the system store a password securely yet allow its use for authentication when the user presents her password the unix system uses encryption to avoid the necessity of keeping its password list secret each user has a password the system contains a function that is extremely difficult-the designers hope impossible-to invert but is simple to compute that is  given a value x  it is easy to compute the function value f  x   given a function value j  x   however  it is impossible to compute x this function is used to encode all passwords only encoded passwords are stored  when a user presents a password  it is encoded and compared against the 652 chapter 15 stored encoded password even if the stored encoded password is seen  it cam1ot be decoded  so the password can not be determined thus  the password file does not need to be kept secret the functionf  x  is typically an encryption algorithm that has been designed and tested rigorously  the flaw in this method is that the system no longer has control over the passwords although the passwords are encrypted  anyone with a copy of the password file can run fast encryption routines against it-encrypting each word in a dictionary  for instance  and comparing the results against the passwords if the user has selected a password that is also a word in the dictionary  the password is cracked on sufficiently fast computers  or even on clusters of slow computers  such a comparison may take only a few hours  furthermore  because unix systems use a well-known encryption algorithm  a cracker might keep a cache of passwords that have been cracked previously  for these reasons  new versions of unix store the encrypted password entries in a file readable only by the the programs that compare a presented password to the stored password run setuid to root ; so they can read this file  but other users can not they also include a salt  or recorded random number  in the encryption algorithm the salt is added to the password to ensure that if two plaintext passwords are the same  they result in different ciphertexts  another weakness in the unix password methods is that many unix systems treat only the first eight characters as significant it is therefore extremely important for users to take advantage of the available password space to avoid the dictionary encryption method  some systems disallow the use of dictionary words as passwords a good technique is to generate your password by using the first letter of each word of an easily remembered phrase using both upper and lower characters with a number or punctuation mark thrown in for good measure for example  the phrase my mother 's name is katherine might yield the password mmn.isk !  the password is hard to crack but easy for the user to remember  15.5.4 one-time passwords to avoid the problems of password sniffing and shoulder surfing  a system could use a set of paired when a session begins  the system randomly selects and presents one part of a password pair ; the user must supply the other part in this system  the user is challenged and must with the correct answer to that challenge  this approach can be generalized to the use of an algorithm as a password  the algorithm might be an integer function  for example the system selects a random integer and presents it to the user the user applies a function and replies with the correct result the system also applies the function if the two results match  access is allowed  such algorithmic passwords are not susceptible to reuse ; that is  a user can type in a password  and no entity intercepting that password will be able to reuse it in this scheme  the system and the user share a secret the secret is never transmitted over a medium that allows exposure rather  the secret is used as input to the function  along with a shared seed a is a random number or alphanumeric sequence the seed is the authentication challenge from the computer the secret and the seed are used as input to the function f  secret  seed   the result of this function is transmitted as the password to the 15.5 653 computer becallse the computer also knows the secret and the seed  it can perform the same computation if the results match  the user is authenticated  the next time the user needs to be authenticated  another seed is generated  and the same ensue this time  the password is different  in this system  the password is different in each instance anyone capturing the password from one session and trying to reuse it in another session will fail one-time passwords are among the only ways to prevent improper authentication clue to password exposure  one-time password systems are implemented in various ways commercial implementations  such as securid  use hardware calculators most of these calculators are shaped like a credit card  a key-chain dangle  or a usb device ; they include a display and may or may not also have a keypad some use the current time as the random seed others the user to enter the shared secret  also known as a or on the keypad the display then shows the one-time password the use of both a one-time password generator and a pin is one form of n ! jn  ' ' ' ' two different types of components are needed in this case two-factor authentication offers far better authentication protection than single-factor authentication  another variation on one-time passwords uses a or which is a list of single-use passwords each password on the list is used once and then is crossed out or erased the commonly used s/key system uses either a software calculator or a code book based on these calculations as a source of one-time passwords of course  the user must protect his code book  15.5.5 biometrics yet another variation on the use of passwords for authentication involves the use of biometric measures palm or hand-readers are commonly used to secure physical access-for example  access to a data center these readers match stored parameters against what is being read from hand-reader pads  the parameters can include a temperature map  as well as finger length  finger width  and line patterns these devices are currently too large and expensive to be used for normal computer authentication  fingerprint readers have become accurate and cost-effective and should become more common in the future these devices read finger ridge patterns and convert them into a sequence of numbers over time  they can store a set of sequences to adjust for the location of the finger on the reading pad and other factors software can then scan a finger on the pad and compare its features with these stored sequences to determine if they match of course  multiple users can have profiles stored  and the scanner can differentiate among them  a very accurate two-factor authentication scheme can result from requiring a password as well as a user name and fingerprint scan if this information is encrypted in transit  the system can be very resistant to spoofing or replay attack  is better still consider how strong authentication can be with a usb device that must be plugged into the system  a pin  and a fingerprint scan except for the user 's having to place her finger on a pad and plug the usb into the system  this authentication method is no less convenient 654 chapter 15 15.6 that using normal passwords recall  though  that strong authentication by itself is not sufficient to guarantee the id of the user an authenticated session can still be hijacked if it is not encrypted  just as there are myriad threats to system and network security  there are many security solutions the solutions run the gamut from improved user education  through technology  to bug-free software most security professionals subscribe to the theory of which states that more layers of defense are better than fewer layers of course  this theory applies to any kind of security consider the security of a house without a door lock  with a door lock  and with a lock and an alarm in this section  we look at the major methods  tools  and techniques that can be used to improve resistance to threats  15.6.1 security policy toward improving the security of any aspect of computing is to have a  policies vary widely but generally include a statement of what is being secured for example  a policy might state that all outsideaccessible applications must have a code review before being deployed  or that users should not share their passwords  or that all connection points between a company and the outside must have port scans nm every six months without a policy in place  it is impossible for users and administrators to know what is permissible  what is required  and what is not allowed the policy is a road map to security  and if a site is trying to move from less secure to more secure  it needs a map to know how to get there  once the security policy is in place  the people it affects should know it well it should be their guide the policy should also be a that is reviewed and updated periodically to ensure that it is still pertinent and still followed  15.6.2 vulnerability assessment how can we determine whether a security policy has been correctly implemented the best way is to execute a vulnerability assessment such assessments can cover broad ground  from social engineering through risk assessment to port scans rlsl for example  endeavors to value the assets of the entity in question  a program  a management team  a system  or a facility  and determine the odds that a security incident will affect the entity and decrease its value when the odds of suffering a loss and the amount of the potential loss are known  a value can be placed on trying to secure the entity  the core activity of most vulnerability assessments is a '., ~ '''--in which the entity is scanned for known vulnerabilities because this book is concerned with operating systems and the software that runs on them  we concentrate on those aspects of vulnerability assessment  vulnerability scans typically are done at times when computer use is relatively low  to minimize their impact when appropriate  they are done on 15.6 655 test systems rather than production systems  because they can induce unhappy behavior from the target systems or network devices  a scan within an individual system can check a variety of aspects of the system  short or easy-to-guess passwords unauthorized privileged programs  such as setuid programs unauthorized programs in system directories unexpectedly long-running processes improper directory protections on user and system directories improper protections on system data files  such as the password file  device drivers  or the operating-system kernel itself dangerous entries in the program search path  for example  the trojan horse discussed in section 15.2.1  changes to system programs detected with checksum values unexpected or hidden network daemons any problems found by a security scan can be either fixed automatically or reported to the managers of the system  networked computers are much more susceptible to security attacks than are standalone systems rather than attacks from a known set of access points  such as directly connected terminals  we face attacks from an unknown and large set of access points-a potentially severe security problem to a lesser extent  systems connected to telephone lines via modems are also more exposed  in fact  the u.s government considers a system to be only as secure as its most far-reaching connection for instance  a top-secret system may be accessed only from within a building also considered top-secret the system loses its topsecret rating if any form of communication call occur outside that environment  some government facilities take extreme security precautions the connectors that plug a terminal into the secure computer are locked in a safe in the office when the terminal is not in use a person must have proper id to gain access to the building and her office  must know a physical lock combination  and must know authentication information for the computer itself to gain access to the computer-an example of multifactor authentication  unfortunately for systems administrators and computer-security professionals  it is frequently impossible to lock a machine in a room and disallow all remote access for instance  the internet network currently connects millions of computers it is becoming a mission-critical  indispensable resource for many companies and individuals if you consider the internet a club  then  as in any club with millions of members  there are many good members and some bad members the bad members have many tools they can use to attempt to gain access to the interconnected computers  just as morris did with his worm  vulnerability scans can be applied to networks to address some of the problems with network security the scans search a network for ports that respond to a request if services are enabled that should not be  access to them can be blocked  or they can be disabled the scans then determine the details of 656 chapter 15 the application listening on that port and try to determine if it has any known vulnerabilities testing those vulnerabilities can determine if the system is ncisconfigured or lacks needed patches  finally though  consider the use of port scanners in the hands of a cracker rather than someone trying to improve security these tools could help crackers find vulnerabilities to attack  fortunately  it is possible to detect port scans through anomaly detection  as we discuss next  it is a general challenge to security that the same tools can be used for good and for harm in fact  some people advocate stating that no tools should be written to test security  because such tools can be used to find  and exploit  security holes others believe that this approach to security is not a valid one  pointing out  for example  that crackers could write their own tools it seems reasonable that security through obscurity be considered one of the layers of security only so long as it is not the only layer for example  a company could publish its entire network configuration ; but keeping that information secret makes it harder for intruders to know what to attack or to determine what might be detected even here  though  a company assuming that such information will remain a secret has a false sense of security  15.6.3 intrusion detection and facilities is intimately linked to intrusion detection  as its name suggests  strives to detect attempted or successful intrusions into computer systems and to initiate appropriate responses to the intrusions intrusion detection encompasses a wide array of techniques that vary on a number of axes  including the following  the time at which detection occurs detection can occur in real time  while the intrusion is occurring  or after the fact  the types of inputs examined to detect intrusive activity these may include user-shell commands  process system calls  and network packet headers or contents some forms of intrusion might be detected only by correlating information from several such sources  the range of response capabilities simple forms of response include alerting an administrator to the potential intrusion or somehow halting the potentially intrusive activity-for example  killing a process engaged in such activity in a sophisticated fonn of response ; a system might transparently divert an intruder 's activity to a false resource exposed to the attacker the resource appears real to the attacker and enables the system to monitor and gain information about the attack these degrees of freedom in the design space for detecting intrusions have a wide range of solutions  known as and ids systems raise an alarm when an intrusion is detected  while idp systems act as routers  passing traffic unless an intrusion is detected  at which point that traffic is blocked   but just what constitutes an intrusion defining a suitable specification of intrusion turns out to be quite difficult  and thus automatic idss and idps today settle for one of two less ambitious approaches in the first  called system input or network traffic is examined for 15.6 657 specific behavior patterns  or known to indicate attacks a simple example of signature-based detection is scanning network packets for the string /etc/passwd/ targeted for a unix systenl another example is virus-detection software  which scans binaries or network packets for lmown viruses  the second approach  typically called attempts through various techniques to detect anomalous behavior within computer systen s of course  not all anomalous system activity indicates an intrusion  but the presumption is that intrusions often induce anomalous behavior an example of anomaly detection is monitoring system calls of a daemon process to detect whether the system-call behavior deviates from normal patterns  possibly indicating that a buffer overflow has been exploited in the daemon to corrupt its behavior another example is monitoring shell commands to detect anomalous commands for a given user or detecting an anomalous login time for a user  either of which may indicate that an attacker has succeeded in gaining access to that user 's account  signature-based detection and anomaly detection can be viewed as two sides of the same coin  signature-based detection attempts to characterize dangerous behaviors and to detect when one of these behaviors occurs  whereas anomaly detection attempts to characterize normal  or non dangerous  behaviors and to detect when something other than these behaviors occurs  these different approaches yield idss and idps with very different properties  however in particular  anomaly detection can find previously unknown methods of intrusion  so-called signature-based detection  in contrast  will identify only known attacks that can be codified in a recognizable pattern thus  new attacks that were not contemplated when the signatures were generated will evade signature-based detection this problem is well known to vendors of virus-detection software  who must release new signatures with great frequency as new viruses are detected manually  anomaly detection is not necessarily superior to signature-based detection  however indeed  a significant challenge for systems that attempt anomaly detection is to benchmark normal system behavior accurately if the system has already been penetrated when it is benchmarked  then the intrusive activity may be included in the normal benchmark even if the system is benchinarked cleanly  without influence from intrusive behaviorf the benchmark must give a fairly complete picture of normal behavior otherwise  the number of  false alarms  orf worse   missed intrusions  will be excessive  to illustrate the impact of even a marginally high rate of false alarms  consider an installation consisting of a hundred unix workstations from which security-relevant events are recorded for purposes of intrusion detection a small installation such as this could easily generate a million audit records per day only one or two might be worthy of an administrator 's investigation if we suppose  optimistically  that each actual attack is reflected in ten audit recordsf we can roughly compute the rate of occurrence of audit records reflecting truly intrusive activity as follows  2 intrusions  10  recor s mtrus10n 0.00002  658 chapter 15 interpreting this as a probability of occurrence of intrusive records/ ' we denote it as p  i  ; that is  event i is the occurrence of a record reflecting truly intrusive behavior since p  i  = 0.00002  we also know that p  ~ i  = 1-p  i  = 0.99998 now we let a denote the raising of an alarm by an ids an accurate ids should maximize both p  i la  and p  ~ i i ~ a  -that is  the probabilities that an alarm indicates an intrusion and that no alarm indicates no intrusion focusil g on p  i i a  for the moment  we can compute it using p  iia  p  i  p  aii  p  i  p  aii  + p  ~ i  p  ai ~ i  0.00002 p  aii  0.00002 p  aii  + 0.99998 p  ai ~ i  now consider the impact ofthe false-alarm rate p  ai ~ i  on p  iia   even with a very good true-alarm rate of p  ail  = 0.8  a seemingly good falsealarm rate of p  ai ~ i  = 0.0001 yields p  iia  ~ 0.14 that is  fewer than one ill every seven alarms indicates a real intrusion ! in systems where a security administrator ilwestigates each alarm  a high rate of false alarms-called a christmas tree effect -is exceedingly wasteful and will quickly teach the admilcistrator to ignore alarms  this example illustrates a general principle for idss and idps  for usability  they must offer an extremely low false-alarm rate achieving a sufficiently low false-alarm rate is an especially serious challenge for anomaly-detection systems  as mentioned  because of the difficulties of adequately benchmarking normal system behavior however  research contil ues to improve anomalydetection techniques intrusion detection software is evolving to implement signatures  anomaly algorithms  and other algorithms and to combine the results to arrive at a more accurate anomaly-detection rate  15.6.4 virus protection as we have seen  viruses can and do wreak havoc on systems protection from viruses thus is an important security concern antivirus programs are often used to provide this protection some of these programs are effective against only particular known viruses they work by searching all the programs on a system for the specific pattern of instructions known to make up the virus  when they find a known pattern  they remove the instructions  the program antivirus programs may have catalogs of thousands of viruses for which they search  both viruses and antivirus software continue to become more sophisticated  some viruses modify themselves as they infect other software to avoid the basic pattern-match approach of antivirus programs antivirus programs ill turn now look for families of patterns rather than a single pattern to identify a virus  in fact  some antivirus programs implement a variety of detection algorithms  they can decompress compressed viruses before checking for a signature  some also look for process anomalies a process opening an executable file for writing is suspicious  for example  unless it is a compiler another popular teducique is to run a program in a which is a controlled or emulated 15.6 659 the tripwire file system an example of an anomaly-detection tool is the checking tool for unix  developed at purdue university tripwire operates on the premise that many intrusions result in modification of system directories and files for example  an attacker might modify the system programs  perhaps inserting copies with trojan horses  or might insert new programs into directories commonly found in user-shell search paths or an intruder might remove system log files to cover his tracks tripwire is a tool to monitor file systems for added  deleted  or changed files and to alert system administrators to these modifications  the operation of tripwire is controlled by a configurationfile tw.config that enumerates the directories and files to be monitored for changes  deletions  or additions each entry in this configuration file includes a selection mask to specify the file attributes  inode attributes  that will be monitored for changes for example  the selection mask might specify that a file 's permissions be monitored but its access time be ignored in addition  the selection mask can instruct thatthe file be monitored for changes monitoring the hash of a file for changes is as good as monitoring the file itselt but storing hashes of files requires far less room than copying the files themselves  when run initially  tripwire takes as input the tw.config file and computes a sign.ature for each file or directory consisting of its monitored attributes  inode attributes and hash values   these signatures are stored in a database when run subsequently  tripwire inputs both tw.config and the previously stored database  recomputes the signature for each file or directory named in tw.conf ig  and compares this signature with the signature  if any  in the previously compl.j-ted database events reported to an administrator include any monitored file or directory whose signature differs from that in the database  a changed file   any file or directory in a monitored directory for which a signature does not exist in the database  an added file   and any signature in the database for which the corresponding file or directory no longer exists  a deleted file   although effective for a wide class of attacks  tripwire does have limitations  perhaps the most obvious is the need to protect the tripwire program and its associated files  especially the database file  from unauthorized modification  for this reason  tripwire and its associated files should be stored on some tamper-proof medium  such as a write-protected disk or a secure server where logins can be tightly controlled unforhm.ately  this makes it less convenient to update the database after authorized updates to monitored directories and files a second limitation is that some security-relevant files-for example  system log files-are supposed to change over time  and tripwire does not provide a way to distinguish between an authorized and an unauthorized change so  for example  an attack that modifies  without deleting  a system log that would normally change anyway would escape tripwire 's detection capabilities the best tripwire can do in this case is to detectcertain obvious inconsistencies  for example  a shrinking log file   free and commercial versions of tripwire are available from http  / /tripwire.org and.http  / /tripwire.com  660 chapter 15 section of the system the antivirus software analyzes the behavior of the code in the sandbox before letting it run unmonitored some antivirus programs also put up a complete shield rather than just scanning files within a file system  they search boot sectors  menlory  inbound and outbound e-mail  files as they are downloaded  files on removable devices or media  and so on  the best protection against computer viruses is prevention  or the practice of purchasing unopened software from vendors and avoiding free or pirated copies from public sources or disk exchange offer the safest route to preventing infection however  even new copies of legitimate software applications are not immune to virus infection  in a few cases  disgruntled employees of a software company have infected the master copies of software programs to do economic harm to the company for macro viruses  one defense is to exchange microsoft word documents in an alternative file format called unlike the native word format rtf does not include the capability to attach macros  another defense is to avoid opening any e-mail attachments from unknown users unfortunately  history has shown that e-mail vulnerabilities appear as fast as they are fixed for example  in 2000  the love bug virus became very widespread by traveling in e-mail messages that pretended to be love notes sent by friends of the receivers once a receiver opened the attached visual basic script  the virus propagated by sending itself to the first addresses in the receiver 's e-mail contact list fortunately  except for clogging e-mail systems and users ' inboxes  it was relatively harmless it did  however  effectively negate the defensive strategy of opening attachments only from people known to the receiver a more effective defense method is to avoid opening any e-mail attachment that contains executable code some companies now enforce this as policy by removing all incoming attachments to e-mail messages  another safeguard  although it does not prevent infection  does permit early detection a user must begin by completely reformatting the hard disk  especially the boot sector  which is often targeted for viral attack only secure software is uploaded  and a signature of each program is taken via a secure message-digest computation the resulting filename and associated messagedigest list must then be kept free from unauthorized access periodically  or each time a program is run  the operating system recomputes the signature and compares it with the signature on the original list ; any differences serve as a warning of possible infection this technique can be combined with others for example  a high-overhead antivirus scan  such as a sandbox  can be used ; and if a program passes the test  a signature can be created for it if the signatures match the next time the program is run  it does not need to be virus-scanned again  15.6.5 auditing  accounting  and logging auditing  accounting  and logging can decrease system performance  but they are useful in several areas  including security logging can be general or specific all system-call executions can be logged for analysis of program behavior  or misbehavior   more typically  suspicious events are logged  authentication failures and authorization failures can tell us quite a lot about break-in attempts  15 15.7 661 accounting is another potential tool in a security administrator 's kit it can be used to find performance changes  which in tum can reveal security problems one of the early unix computer break-ins was detected by cliff stoll when he was exam5ning accounting logs and spotted an anomaly  we turn next to the question of how a trusted computer can be connected safely to an untrustworthy network one solution is the use of a firewall to separate trusted and unh usted systems a is a computer  appliance  or router that sits between the trusted and the untrusted a network firewall limits network access between the two and monitors and logs all connections it can also limit coru1.ections based on source or destination address  source or destination port  or direction of the connection for instance  web servers use http to communicate with web browsers a firewall therefore may allow only http to pass from all hosts outside the firewall to the web server within the firewall the morris internet worm used the finger protocol to break into computers  so finger would not be allowed to pass  for example  in fact  a network firewall can separate a network into multiple domains  a common implementation has the internet as the untrusted domain ; a semitrusted and semisecure network  called the as another domain ; and a company 's computers as a third domain  figure 15.10   coru1.ections are allowed from the internet to the dmz computers and from the company computers to the internet but are not allowed from the internet or dmz computers to the company computers optionally  controlled commurucations may be allowed between the dmz and one company computer or more for instance  a web server on the dmz may need to query a database server on the corporate network with a firewall  however  access is contained  and any dmz systems that are broken into still are unable to access the company computers  internet internet access from company 's computers r---------i company computers access between dmz and company 's computers figure 15.10 domain separation via firewall  662 chapter 15 15.8 of course  a firewall itself must be secure and attack-proof ; otherwise  its ability to secure connections can be compromised furthermore  firewalls do not prevent attacks that or travel within protocols or com1ections that the firewall allows a buffer-overflow attack to a web server will not be stopped by the firewall  for example  because the http connection is allowed ; it is the contents of the http connection that house the attack likewise  denialof service attacks can affect firewalls as much as any other machines another vulnerability of firewalls is in which an unauthorized host pretends to be an authorized host by meeting some authorization criterion for example  if a firewall rule allows a connection from a host and identifies that host by its ip address  then another host could send packets using that same address and be allowed through the firewall  in addition to the most common network firewalls  there are other  newer kinds of firewalls  each with its pros and cons a is a software layer either included with the operating system or added as an application rather than limiting communication between security domains  it limits communication to  and possibly from  a given host a user could add a personal firewall to her pc so that a trojan horse would be denied access to the network to which the pc is connected  for example an prex-y understands the protocols that applications speak across the network  for example  smtp is used for mail transfer an application proxy accepts a com1ection just as an smtp server would and then initiates a connection to the original destination smtp server it can monitor the traffic as it forwards the message  watching for and disabling illegal commands  attempts to exploit bugs  and so on some firewalls are designed for one specific protocol an for example  has the specific purpose of analyzing xml traffic and blocking disallowed or malformed xml sit between applications and the kernel  monitoring system-call execution for example  in solaris 10  the least privilege feature implements a list of more than fifty system calls that processes may or may not be allowed to make a process that does not need to spawn other processes can have that ability taken away  for instance  the u.s department of defense trusted computer system evaluation criteria specify four security classifications in systems  a  b  c  and d this specification is widely used to determine the security of a facility and to model security solutions  so we explore it here the lowest-level classification is division d  or minimal protection division d includes only one class and is used for systems that have failed to meet the requirements of any of the other security classes  for instance  ms-dos and windows 3.1 are in division d  division c  the next level of security  provides discretionary protection and accountability of users and their actions through the use of audit capabilities  division c has two levels  c1 and c2 a c1-class system incorporates some form of controls that allow users to protect private information and to keep other users from accidentally reading or destroying their data a c1 environment is one in which cooperating users access data at the same levels of sensitivity most versions of unix are c1 class  15.8 663 the total of all protection systems within a computer system  hardware  software  firmware  that correctly enforce a security policy is known as a the tcb of a cl system controls access between users and files by allowing the user to specify and control sharing of objects by named individuals or defined groups in addition  the tcb requires that the users identify themselves before they start any activities that the tcb is expected to mediate this identification is accomplished via a protected mechanism or password ; the tcb protects the authentication data so that they are inaccessible to unauthorized users  a c2-class system adds an individual-level access control to the requirements of a cl system for example  access rights of a file can be specified to the level of a single individual in addition  the system adrninistrator can selectively audit the actions of any one or more users based on individual identity the tcb also protects itself from modification of its code or data structures in addition  no information produced by a prior user is available to another user who accesses a storage object that has been released back to the system some speciat secure versions of unix have been certified at the c2 level  division-b mandatory-protection systems have all the properties of a classc2 system ; in addition  they attach a sensitivity label to each object the bl-class tcb maintains the security label of each object in the system ; the label is used for decisions pertaining to mandatory access control for example  a user at the confidential level could not access a file at the more sensitive secret level the tcb also denotes the sensitivity level at the top and bottom of each page of any human-readable output in addition to the normal user-namepassword authentication information  the tcb also maintains the clearance and authorizations of individual users and will support at least two levels of security these levels are hierarchicat so that a user may access any objects that carry sensitivity labels equal to or lower than his security clearance for example  a secret-level user could access a file at the confidential level in the absence of other access controls processes are also isolated through the use of distinct address spaces  a b2-class system extends the sensitivity labels to each system resource  such as storage objects physical devices are assigned minimum and maximum security levels that the system uses to enforce constraints imposed by the physical environments in which the devices are located in addition  a b2 system supports covert channels and the auditing of events that could lead to the exploitation of a covert channel  a b3-class system allows the creation of access-control lists that denote users or groups not granted access to a given named object the tcb also contains a mechanism to monitor events that may indicate a violation of security policy the mechanism notifies the security administrator and  if necessary  terminates the event in the least disruptive manner  the highest-level classification is division a architecturally  a class-al system is functionally equivalent to a b3 system  but it uses formal design specifications and verification techniques  granting a high degree of assurance that the tcb has been implemented correctly a system beyond class al might be designed and developed in a trusted facility by trusted personnel  the use of a tcb merely ensures that the system can enforce aspects of a security policy ; the tcb does not specify what the policy should be typically  664 chapter 15 15.9 a given computing environment develops a security policy for and has the plan by a security agency  such as the national computer security center certain computing environments may require other certification  such as that supplied by tempest  which guards against electronic eavesdropping for example  a tempest-certified system has terminals that are shielded to prevent electromagnetic fields from escaping this shielding ensures that equipment outside the room or building where the terminal is housed camwt detect what information is being displayed by the terminal  microsoft windows xp is a general-purpose operating system designed to support a variety of security features and methods in this section  we examine features that windows xp uses to perform security functions for more information and background on wilcdows xp  see chapter 22  the windows xp security model is based on the notion of windows xp allows the creation of any number of user accounts  which can be grouped in any manner access to system objects can then be permitted or denied as desired users are identified to the system by a unique security id  when a user logs on  windows xp creates a that includes the security id for the user  security ids for any groups of which the user is a member  and a list of any special privileges that the user has examples of special privileges include backing up files and directories  shutting down the compute1 ~ logging on interactively  and changing the system clock every process that windows xp runs on behalf of a user will receive a copy of the access token the system uses the security ids in the access token to permit or deny access to system objects whenever the use1 ~ or a process on behalf of the user  attempts to access the object authentication of a user account is typically accomplished via a user name and password  although the modular design of windows xp allows the development of custom authentication packages for example  a retinal  or eye  scanner might be used to verify that the user is who she says she is  windows xp uses the idea of a subject to ensure that programs run by a user do not get greater access to the system than the user is authorized to have  a is used to track and manage permissions for each program that a user runs ; it is composed of the user 's access token and the program acting on behalf of the user since windows xp operates with a client-server model  two classes of subjects are used to control access  simple subjects and server subjects an example of a is the typical application program that a user executes after she logs on simple subject is assigned a based on the security access token of the user a is a process implemented as a protected server that uses the security context of the client when acting on the client 's behalf  as mentioned in section 15.7  auditing is a useful security technique  windows xp has bl1ilt-in auditing that allows many common security threats to be monitored examples include failure auditing for login and logoff events to detect random password break-ins  success auditing for login and logoff events to detect login activity at strange hours  success and failure write-access auditing for executable files to track a virus outbreak  and success and failure auditing for file access to detect access to sensitive files  15.10 15.10 665 security attributes of an object in windows xp are described by a the security descriptor contains the security id of the owner  who can change the access permissions   a group security id used the posix subsystem  a discretionary access-control list that identifies users or groups are allowed  and which are not allowed  access  and a system access-control list that controls which auditing messages the system will generate for example  the security descriptor of the file foo.bar might have owner avi and this discretionary access-control list  a vi -all access group cs-read-write access user cliff-no access in addition  it might have a system access-control list of audit writes by everyone  an access-control list is composed of access-control entries that contain the security id of the individual and an access mask that defines all possible actions on the object  with a value of accessallowed or accessdenied for each action files in windows xp may have the following access types  readdata  writedata,appenddata  execute,readextendedattribute  writeextendedattribute  readattributes  and wri teattributes we can see how this allows a fine degree of control over access to objects  windows xp classifies objects as either container objects or noncontainer objects such as directories  can logically contain other objects by default  an object is created within a container object  the new object inherits permissions from the parent object similarly  if the user copies a file from one directory to a new directory  the file will inherit the permissions of the destination directory inherit no other permissions  furthermore  if a permission is changed on a directory  the new permissions do not automatically apply to existing files and subdirectories ; the user may explicitly apply them if she so desires  the system administrator can prohibit printilig to a printer on the system for all or part of a day and can use the windows xp performance monitor to help her spot approaching problems in general  windows xp does a good job of providing features to help ensure a secure computing environment many of these features are not enabled by default  however  which may be one reason for the myriad security breaches on windows xp systems another reason is the vast number of services windows xp starts at system boot tiine and the number of applications that typically are installed on a windows xp system  for a real multiuser environment  the system administrator should formulate a security plan and implement it  using the features that windows xp provides and other security tools  protection is an internal problem security  in contrast  must consider both the computer system and the environment-people  buildings  businesses  valuable objects  and threats-within which the system is used  666 chapter 15 the data stored in the computer system must be protected from unauthorized access  malicious destruction or alteration  and accidental introduction of inconsistency it is easier to protect against accidental loss of data consistency than to protect against malicious access to the data absolute protection of the information stored in a computer system from malicious abuse is not possible ; but the cost to the perpetrator can be made sufficiently high to deter most  if not all  attempts to access that information without proper authority  several types of attacks can be launched against programs and agaitlst individual computers or the masses stack and buffer-overflow techniques allow successful attackers to change their level of system access viruses and worms are self-perpetuating  sometimes infecting thousands of computers  denial-of-service attacks prevent legitimate use of target systems  encryption limits the domain of receivers of data  while authentication limits the domain of senders encryption is used to provide confidentiality of data being stored or transferred symmetric encryption requires a shared key  while asymn'letric encryption provides a public key and a private key  authentication  when combined with hashing  can prove that data have not been changed  user authentication methods are used to identify legitimate users of a system in addition to standard user-name and password protection  several authentication methods are used one-time passwords  for example  change from session to session to avoid replay attacks two-factor authentication requires two forms of authentication  such as a hardware calculator with an activation pin multifactor authentication uses three or more forms these methods greatly decrease the chance of authentication forgery  methods of preventing or detecting security incidents include intrusiondetection systems  antivirus software  auditing and logging of system events  monitoring of system software changes  system-call monitoring  and firewalls  15.1 argue for or against the judicial sentence handed down against robert morris  jr  for his creation and execution of the internet worm discussed in section 15.3.1  15.2 discuss a means by which managers of systems connected to the internet could design their systems to limit or eliminate the damage done by worms what are the drawbacks of making the change that you suggest 15.3 what commonly used computer programs are prone to man-in-themiddle attacks discuss solutions for preventing this form of attack  15.4 the unix program cops scans a given system for possible security holes and alerts the user to possible problems what are two potential hazards of using such a system for security how can these problems be limited or eliminated 667 15.5 make a list of six security concerns for a bank 's computer system for each item on your list  state whether this concern relates to physicat human  or operating-system ~ security  15.6 an experimental addition to unix allows a user to connect a program to a file the watchdog is invoked whenever a program requests access to the file the watchdog then either grants or denies access to the file discuss two pros and two cons of using watchdogs for security  15.7 discuss how the asymmetric encryption algorithm can be used to achieve the following goals  a authentication  the receiver knows that only the sender could have generated the message  b secrecy  only the receiver can decrypt the message  c authentication and secrecy  only the receiver can decrypt the message  and the receiver knows that only the sender could have generated the message  15.8 why does n't d  lce  n   e  /cd  n   m   provide authentication of the sender to what uses can such an encryption be put 15.9 consider a system that generates 10 million audit records per day also assume that there are on average 10 attacks per day on this system and that each such attack is reflected in 20 records if the intrusion-detection system has a true-alarm rate of 0.6 and a false-alarm rate of 0.0005  what percentage of alarms generated by the system correspond to real intrusions 15.10 what is the purpose of using a salt along with the user-provided password where should the salt be stored  and how should it be used general discussions concerning security are given by hsiao et al  1979l landwehr  1981   deru  1ing  1982   pfleeger and pfleeger  2003   tanenbaum 2003  and russell and gangemi  1991   also of general interest is the text by lobel  1986   computer networking is discussed in kurose and ross  2005   issues concernin ~ g the design and verification of secure systems are discussed by rushby  1981  and by silverman  1983   a security kernel for a multiprocessor microcomputer is described by schell  1983   a distributed secure system is described by rushby and randell  1983   morris and thompson  1979  discuss password security morshedian  1986  presents methods to fight password pirates password authentication 668 chapter 15 with insecure communications is considered by lamport  1981   the issue of password cracking is examined by seely  1989   cmnputer break-ins are discussed by lehmann  1987  and by reid  1987   issues related to trusting computer programs are discussed in thompson  1984   discussions concerning unix security are offered by grampp and morris  1984 l wood and kochan  1985   farrow  1986b   farrow  1986a   filipski and hanko  1986   hecht et al  1988   kramer  1988   and garfinkel et al  2003   bershad and pinkerton  1988  present the watchdog extension to bsd unix the cops security-scanning package for unix was written by farmer at purdue university it is available to users on the internet via the ftp program from host ftp.uu.net in directory /pub i security i cops  spafford  1989  presents a detailed technical discussion of the internet worm the spafford article appears with three others in a special section on the morris internet worm in communications of the acm  volume 32  number 6  june 1989   security problems associated with the tcp /ip protocol suite are described in bellovin  1989   the mechanisms commonly used to prevent such attacks are discussed in cheswick et al  2003   another approach to protecting networks from insider attacks is to secure topology or route discovery kent et al  2000   hu et al  2002   zapata and asokan  2002   and hu and perrig  2004  present solutions for secure routing savage et al  2000  examine the distributed denialof service attack and propose ip trace-back solutions to address the problem  perlman  1988  proposes an approach to diagnose faults when the network contains malicious routers  information about viruses and worms can be found at http  / /www.viruslist.com  as well as in ludwig  1998  and ludwig  2002   other web sites containing up-to-date security information include http  / /www.trusecure.com and httpd  / /www.eeye.com a paper on the dangers of a computer monoculture can be found at http  / /www.ccianet.org/papers/cyberinsecurity.pdf  diffie and hellman  1976  and diffie and hellman  1979  were the first researchers to propose the use of the public-key encryption scheme the algorithm presented in section 15.4.1 is based on the public-key encryption scheme ; it was developed by rivest et al  1978   lempel  1979   simmons  1979   denning and demting  1979   gifford  1982   denning  1982   ahituv et al   1987   schneier  1996   and stallings  2003  explore the use of cryptography in computer systems discussions concerning protection of digital signatures are offered by akl  1983   davies  1983   denning  1983   and denning  1984   the u.s government is  of course  concerned about security the department of defense trusted computer system evaluation criteria  dod  1985    known also as the orange book  describes a set of security levels and the features that an operating system must have to qualify for each security rating reading it is a good starting point for understanding security concerns the microsoft windows nt workstation resource kit  microsoft  1996   describes the security inodel of nt and how to use that model  the rsa algorithm is presented in rivest et al  1978   information about nist 's aes activities can be found at http  / /www.nist.gov/aes/ ; information about other cryptographic standards for the united states can also be found at that site more complete coverage of ssl 3.0 can be found at 669 http  / /home.netscape.com/eng/ssl3/ in 1999  ssl 3.0 was modified slightly and presented in an ietf request for comments  rfc  under the name tls  the example in section 15.6.3 illustrating the impact of false-alarm rate on the effectiveness of idss is based on axelsson  1999   the description of tripwire in section 15.6.5 is based on kim and spafford  1993   research into system-call-based anomaly detection is described in forrest et al  1996   part seven a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory  and the processors communicate with one another through communication lines such as local-area or wide-area networks the processors in a distributed system vary in size and function such systems may include small handheld or real-time devices  personal computers  workstations  and large mainframe computer systems  a distributed file system is a file-service system whose users  servers  and storage devices are dispersed among the sites of a distributed system accordingly  service activity has to be carried out across the network ; instead of a single centralized data repository  there are multiple independent storage devices  the benefits of a distributed system include giving users access to the resources maintained by the system and thereby speeding up computation and improving data availability and reliability because a system is distributed  however  it must provide mechanisms for process synchronization and communication  for dealing with the deadlock problem  and for handling failures that are not encountered in a centralized system  16.1 a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory the processors communicate with one another through various communication networks  such as high-speed buses or telephone lines in this chapter  we discuss the general structure of distributed systems and the networks that interconnect them we contrast the main differences in operating-system design between these systems and centralized systems in chapter 17  we go on to discuss distributed file systems then  i11 chapter 18  we describe the methods necessary for distributed operating systems to coordinate their actions  to provide a high-level overview of distributed systems and the networks that interconnect them  to discuss the general structure of distributed operating systems  a is a collection of loosely coupled processors interconnected by a communication network from the point of view of a specific processor in a distributed system  the rest of the processors and their respective resources are remote  whereas its own resources are local  the processors in a distributed system may vary in size and function  they may include small microprocessors  workstations  minicomputers  and large general-purpose cornputer systems these processors are referred to by a number of names  such as sites  nodes  computers  machines  and hosts  depending on the context in which they are mentioned we mainly use site to indicate the location of a machine and host to refer to a specific system at a site generally  one host at one site  the server  has a resource that another host at another site  the client  or user   would like to use a general structure of a distributed system is shown in figure 16.1  673 674 chapter 16 site a site c network communication site b figure 16.1 a distributed system  d d d d l resources l there are four major reasons for building distributed systems  resource sharing  computation speedup  reliability  and communication in this section  we briefly discuss each of them  16.1.1 resource sharing if a number of different sites  with different capabilities  are connected to one another  then a user at one site may be able to use the resources available at another for example  a user at site a may be using a laser printer located at site b meanwhile  a user at b may access a file that resides at a in general  in a distributed system provides mechanisms for sharing files at remote sites  processing information in a distributed database  printing files at remote sites  using remote specialized hardware devices  such as a high-speed array processor   and performing other operations  16.1.2 computation speedup if a particular computation can be partitioned into subcomputations that can run concurrently  then a distributed system allows us to distribute the subcomputations among the various sites ; the subcomputations can be run concurrently and thus provide in addition  if a particular site is currently overloaded with jobs  some of them can be moved to other  lightly loaded sites this movement of jobs is called automated load sharing  in which the distributed operating system automatically moves jobs  is not yet comnlon in commercial systems  16.1.3 reliability if one site fails in a distributed system  the remammg sites can continue operating  giving the system better reliability if the system is composed of multiple large autonomous installations  that is  general-purpose computers   the failure of one of them should not affect the rest if  however  the system 16.2 16.2 675 is composed of sncall machines  each of which is responsible for some crucial system function  such as tenninal character i/0 or the file system   then a single failure may halt the operation of the whole system in general  with enough redundancy  in both hardware and data   the system can continue operation  even if some of its sites have failed  the failure of a site must be detected by the system  and appropriate action may be needed to recover from the failure the system must no longer use the services of that site in addition  if the function of the failed site can be taken over by another site  the system must ensure that the transfer of function occurs correctly finally  when the failed site recovers or is repaired  mechanisms must be available to integrate it back into the system smoothly as we shall see in chapters 17 and 18  these actions present difficult problems that have many possible solutions  16.1.4 communication when several sites are connected to one another by a communication network  users at the various sites have the opportunity to exchange information at a low level  are passed between systems  much as messages are passed between processes in the single-computer message system discussed in section 3.4 given message passing  all the higher-level flmctionality found in standalone systems can be expanded to encompass the distributed system  such functions include file transfer  login  mail  and remote procedure calls  rpcs   the advantage of a distributed system is that these functions can be carried out over great distances two people at geographically distant sites can collaborate on a project  for example by transferring the files of the project  logging in to each other 's remote systems to run programs  and exchanging mail to coordinate the work  users minimize the limitations inherent in longdistance work we wrote this book by collaborating in such a manner  the advantages of distributed systems have resulted in an industry-wide trend toward dovmslzing many companies are replacing their mainframes with networks of workstations or personal computers companies get a bigger bang for the buck  that is  better functionality for the cost   more flexibility in locating resources and expanding facilities  better user interfaces  and easier maintenance  in this section  we describe the two general categories of network-oriented operating systems  network operating systems and distributed operating systems network operating systems are simpler to implement but generally more difficult for users to access and utilize than are distributed operating systems  which provide more features  16.2.1 network operating systems a operating provides an environment in which users  who are aware of the multiplicity of machines  can access remote resources by either 676 chapter 16 logging in to the appropriate remote machine or transferring data from the remote machine to their own machines  16.2.1.1 remote login an important function of a network operating system is to allow users to log in remotely the internet provides the telnet facility for this p1.npose to illustrate this facility  lets suppose that a user at westminster college wishes to compute on cs.yale.edu  a computer that is located at yale university to do so  the user must have a valid account on that machine to log in remotely  the user issues the command telnet cs.yale.edu this command results in the formation of a socket connection between the local machine at westminster college and the cs.yale.edu computer after this connection has been established  the networking software creates a transparent  bidirectional link so that all characters entered by the user are sent to a process on cs.yale.edu and all the output from that process is sent back to the user the process on the remote machine asks the user for a login name and a password  once the correct information has been received  the process acts as a proxy for the use1 ~ who can compute on the remote machine just as any local user can  16.2.1.2 remote file transfer another major function of a network operating system is to provide a mechanism for remote file transfer from one machine to another in such an enviromnent  each computer maintains its own local file system if a user at one site  say  cs.uvm.edu  wants to access a file located on another computer  say  cs.yale.edu   then the file must be copied explicitly from the computer at yale to the computer at the university of vermont  the internet provides a mechanism for such a transfer with the file transfer protocol  ftp  program suppose that a user on cs.uvm.edu wants to copy a java program server java that resides on cs.yale.edu the user must first invoke the ftp program by executing ftp cs.yale.edu the program then asks the user for a login name and a password once the correct information has been received  the user must connect to the subdirectory where the file server java resides and then copy the file by executing get server java in this scheme  the file location is not transparent to the user ; users must know exactly where each file is moreover  there is no real file sharing  because a user can only copy a file from one site to another thus  several copies of the same file may exist  resulting in a waste of space tn addition  if these copies are modified  the vario-us copies will be inconsistent  16.2 677 notice that  in our example  the user at the university of vermont must have login permission on cs.yale.edu ptp also provides a way to allow a user who does not have an account on the yale computer to copy files remotely this remote copying is accomplished through the anonymous ft'p method  which works as follows the file to be copied  that is  server java  must be placed in a special subdirectory  say  jtp  with the protection set to allow the public to read the file a user who wishes to copy the file uses the ftp command as before when the user is asked for the login nan'le  the user supplies the name anonymous and an arbitrary password  once anonymous login is accomplished  care must be taken by the system to ensure that this partially authorized user does not access inappropriate files generally  the user is allowed to access only those files that are in the directory tree of user anonymous any files placed here are accessible to any anonymous users  subject to the usual file-protection scheme used on that machine anonymous users  however  cam'lot access files outside of this directory tree  implementation of the ftp mechanism is similar to telnet implementation  a daemon on the remote site watches for requests to coru'lect to the system 's ptp port login authentication is accomplished  and the user is allowed to execute commands remotely unlike the telnet daemon  which executes any command for the user  the ptp daemon responds only to a predefined set of file-related commands these include the following  get-transfer a file from the remote machine to the local machine  put-transfer from the local machine to the remote machine  ls or dir-list files in the current directory on the remote machine  cd -change the current directory on the remote machine  there are also various commands to change transfer modes  for binary or ascii files  and to determine connection status  an important point about telnet and ptp is that they require the user to change paradigms ptp requires the user to know a command set entirely different from the normal operating-system commands telnet requires a smaller shift  the user must know appropriate commands on the remote system  for instance  a user on a windows machine who teh'lets to a unix machine must switch to unix commands for the duration of the telnet session facilities are more convenient for users if they do not require the use of a different set of commands distributed operating systems are designed to address this problem  16.2.2 distributed operating systems in a distributed operating system  users access remote resources in the same way they access local resources data and process migration from one site to another is under the control of the distributed operating system  16.2.2.1 data migration suppose a user on site a wants to access data  such as a file  that reside at site b the system can transfer the data by one of two basic methods one approach 678 chapter 16 to is to transfer the entire file to site a from that point on  all access to the file is local when the user no longer needs access to the file  a copy of the file  if it has been modified  is sent back to site b even if only a modest change has been made to a large file  all the data must be transferred  this mechanism can be thought of as an automated ftp system this approach was used in the andrew file system  as we discuss in chapter 17  but it was found to be too inefficient  the other approach is to transfer to site a only those portions of the file that are actually necessary for the immediate task if another portion is required later  another transfer will take place when the user no longer wants to access the file  any part of it that has been modified must be sent back to site b  note the similarity to demand paging  the sun microsystems network file system  nfs  protocol uses this method  chapter 17   as do newer versions of andrew  the microsoft smb protocol  running on top of either tcp /ip or the microsoft netbeui protocol  also allows file sharing over a network smb is described in appendix c.6.1  clearly  if only a small part of a large file is being accessed  the latter approach is preferable if significant portions of the file are being accessed  however  it is more efficient to copy the entire file in both methods  data migration includes more than the mere transfer of data from one site to another  the system must also perform various data translations if the two sites involved are not directly compatible  for instance  if they use different character-code representations or represent integers with a different number or order of bits   16.2.2.2 computation migration in some circumstances  we may want to transfer the computation  rather than the data  across the system ; this approach is called for example  consider a job that needs to access various large files that reside at different sites  to obtain a summary of those files it would be more efficient to access the files at the sites where they reside and return the desired results to the site that il itiated the computation generally  if the time to transfer the data is longer than the time to execute the remote cmmnand  the remote command should be used  such a computation can be carried out in different ways suppose that process p wants to access a file at site a access to the file is carried out at site a and could be il itiated by an rpc an rpc uses a  udp on the internet  to execute a routine on a remote system  section 3.6.2   process p invokes a predefilced procedure at site a the procedure executes appropriately and then returns the results to p  alternatively process p can send a message to site a the operatil g system at site a then creates a new process q whose function is to carry out the designated task when process q completes its execution  it sends the needed result back to p via the message system in this scheme  process p may execute concurrently with process q ; in fact  it may have several processes running concurrently on several sites  either method could be used to access several files residing at various sites  one rpc might result in the ilwocation of another rpc or even in the transfer of messages to another site similarly  process q could  duril g the course of its 16.3 16.3 679 execution  send a message to another site  which in turn would create another process this process might either send a message back to q or repeat the cycle  16.2.2.3 process migration a logical extension of computation migration is na  ' ' ~ c process is submitted for execution  it is not always executed at it is initiated the entire process  or parts of it  may be executed at different sites this scheme may be used for several reasons  load balancing the processes  or subprocesses  may be distributed across the network to even the workload  computation speedup if a single process can be divided into a number of subprocesses that can run concurrently on different sites  then the total process turnaround time can be reduced  hardware preference the process may have characteristics that make it more suitable for execution on some specialized processor  such as matrix inversion on an array processor  rather than on a microprocessor  software preference the process may require software that is available at only a particular site  and either the software can not be moved  or it is less expensive to move the process  data access just as in computation migration  if the data being used in the computation are numerous  it may be more efficient to have a process run remotely than to transfer all the data  we use two complementary techniques to move processes in a computer network in the first  the system can attempt to hide the fact that the process has migrated from the client this scheme has the advantage that the user does not need to code her program explicitly to accomplish the migration this method is usually employed for achieving load balancing and computation speedup among homogeneous systems  as they do not need user input to help them execute programs remotely  the other approach is to allow  or require  the user to specify explicitly how the process should migrate this method is usually employed when the process must be moved to satisfy a hardware or software preference  you have probably realized that the web has many aspects of a distributedcomputing environment certainly it provides data migration  between a web server and a web client   it also provides computation migration for instance  a web client could trigger a database operation on a web server finally  with java  it provides a form of process migration  java applets are sent from the server to the client  where they are executed a network operating system provides most of these features  but a distributed operating system makes them seamless and easily accessible the result is a powerful and easy-to-use facility-one of the reasons for the huge growth of the world wide web  there are basically two types of networks  and the main difference between the two is the way in 680 chapter 16 which they are geographically distributed local-area networks are composed of processors distributed over small areas  such as a single building or a number of adjacent buildings   whereas wide-area networks are composed of a number of autonomous processors distributed over a large area  such as the united states   these differences imply major variations in the speed and reliability of the communications networks  and they are reflected in the distributed operating-system design  16.3.1 local-area networks local-area networks emerged in the early 1970s as a substitute for large mainframe computer systems for many enterprises  it is more economical to have a number of small computers  each with its own self-contained applications  than to have a single large system because each small computer is likely to need a full complement of peripheral devices  such as disks and printers   and because some form of data sharing is likely to occur in a single enterprise  it was a natural step to connect these small systems into a network  lans  as mentioned  are usually designed to cover a small geographical area  such as a single building or a few adjacent buildings  and are generally used in an office environment all the sites in such systems are close to one another  so the communication links tend to have a higher speed and lower error rate than do their cou.rjerparts in wide-area networks high-quality  expensive  cables are needed to attain this higher speed and reliability it is also possible to use the cable exclusively for data network traffic over longer distances  the cost of using high-quality cable is enormous  and the exclusive use of the cable tends to be prohibitively expensive  the most conunon links in a local-area network are twisted-pair and fiberoptic cabling the most common configurations are multiaccess bus  ring  and star networks communication speeds range from 1 megabit per second  for networks such as appletalk  infrared  and the new bluetooth local radio network  to 1 gigabit per second for ethernet ten megabits per second is the speed of requires a higher-quality cable but runs at 100 m ~ egabits per second and is common also growing is the use of optical-fiber-based fddi networking the fddi network is token-based and runs at over 100 megabits per second  a typical lan may consist of a number of different computers  from mainframes to laptops or pdas   various shared peripheral devices  such as laser printers and magnetic-tape drives   and one or more gateways  specialized processors  that provide access to other networks  figure 16.2   an ethernet scheme is commonly used to construct lans an ethernet network has no central controller  because it is a multiaccess bus  so new hosts can be added easily to the network the ethernet protocol is defined by the ieee 802.3 standard  there has been significant growth in using the wireless spectrum for designing local-area networks wireless  or wifi  networks allow constructing a network using only a wireless router for transmitting signals between hosts  each host has a wireless adapter networking card which allows it to join and use the wireless network however  where ethernet systems often run at 100 megabits per second  wifi networks typically run at slower speeds there are 16.3 681 workstation workstation workstation printer laptop file server figure 16.2 local-area network  several ieee standards for wireless networks  802.11g can theoretically run at 54 megabits per second  although ilc practice data rates are often less than half that amount the recent 802.11n standard provides theoretically much higher data rates than 802.11g  although in actual practice 802.11n networks have typical data rates of around 75 megabits per second data rates of wireless networks are heavily influenced by the distance between the wireless router and the host as well as interference in the wireless spectrum wireless networks often have a physical advantage over wired ethernet networks as no cabling needs to be run to connect communicatilcg hosts as a result  wireless networks are popular in homes as well as public areas such as libraries and internet cafes  16.3.2 wide-area networks wide-area networks emerged in the late 1960s  mainly as an academic research project to provide efficient communication among sites  allowing hardware and software to be shared conveniently and economically by a wide community of users the first wan to be designed and developed was the arpanet begun in 1968  the arpanet has grown from a four-site experimental network to a worldwide network of networks  the internet  comprising millions of computer systems  because the sites in a wan are physically distributed over a large geographical area  the communication links are  by default  relatively slow and unreliable  typical links are telephone lines  leased  dedicated data  lines  microwave links  and satellite channels these communication links are controlled by special  figure 16.3   which are responsible for defilcing the interface through which the sites communicate over the network  as well as for transferring information among the various sites  682 chapter 16 communication subsystem h h netwot k host communication processor figure 16.3 communication processors in a wide-area network  for example  the internet wan enables hosts at geographically separated sites to communicate with one another the host computers typically differ from one another in type  speed  word length  operatil1.g system  and so on hosts are generally on lans  which are  in turn  connected to the internet via regional networks the regional networks  such as nsfnet il1  the northeast united states  are interlinked with  section 16.5.2  to form the worldwide network connections between networks frequently use a telephone-system service called t1  which provides a transfer rate of 1.544 megabits per second over a leased line for sites requiring faster internet access  tls are collected into multiple-t1 units that work in parallel to provide more throughput for instance  a t3 is composed of 28 t1 connections and has a transfer rate of 45 megabits per second the routers control the path each message takes through the net this routing may be either dynamic  to increase commmlication efficiency  or static  to reduce security risks or to allow communication charges to be computed  other wans use standard telephone lines as their primary means of communication  are devices that accept digital data from the computer side and convert it to the analog signals that the telephone system uses a modem at the destination site converts the analog signal back to digital form  and the destination receives the data the unix news network  uucp  allows systems to communicate with each other at predetermined times  via modems  to exchange messages the messages are then routed to other nearby systems and in this way either are propagated to all hosts on the network  public messages  or are transferred to specific destinations  private messages   wans are generally slower than lans ; their transmission rates range from 1,200 bits 16.4 16.4 683 per second to over 1 megabit per second uucp has been superseded by ppp  the point-to-point protocol ppp functions over modem coru1ections  allowing home computers to be fully connected to the internet  the sites in a distributed system can be connected physically in a variety of ways each configuration has advantages and disadvantages we can compare the configurations by using the following criteria  installation cost the cost of physically linking the sites in the system communication cost the cost in time and money to send a message from site a to site b availability the extent to which data can be accessed despite the failure of some links or sites the various topologies are depicted in figure 16.4 as graphs whose nodes correspond to sites an edge from node a to node b corresponds to a direct communication link between the two sites in a fully connected network  each site is directly connected to every other site however  the number of links grows as the square of the number of sites  resulting in a huge installation cost  therefore  fully connected networks are impractical in any large system  in a pc  ntially direct links exist between some-but not all-pairs of sites hence  the installation cost of such a configuration is lower than that of the fully connected network however  if two sites a and b are not directly connected  messages from one to the other must be through a sequence of communication links this requirement results in a higher communication cost  if a communication link fails  messages that would have been transmitted across the link must be rerouted in some cases  another route through the network may be found  so that the messages are able to reach their destination  in other cases  a failure may mean that no connection exists between some pair  or pairs  of sites when a system is split into two  or more  unconnected subsystems  it is partitioned under this definition  a subsystem  or partition  may consist of a single node  the various partially connected network types include tree-structured networks  ring networks  and star networks  as shown in figure 16.4 these types have different failure characteristics and installation and communication costs installation and communication costs are relatively low for a treestructured network however  the failure of a single link in such a network can result in the network 's becoming partitioned in a ring network  at least two links must fail for partition to occur thus  the ring network has a higher degree of availability than does a tree-structured network however  the communication cost is high  since a message may have to cross a large number of links in a star network  the failure of a single link results in a network partition  but one of the partitions has only a single site such a partition can be treated as a single-site failure the star network also has a low communication cost  since each site is at most two links away from every other site howeve1 ~ 684 chapter 16 16.5 fully connected network partially connected network b d f tree-structured network star network f ring network figure 16.4 network topology  if the central site fails  all the sites in the system become disconnected from one another  now that we have discussed the physical aspects of networking  we turn to the internal workings the designer of a communication network must address five basic issues  naming and name resolution how do two processes locate each other to communicate routing strategies how are messages sent through the network packet strategies are packets sent individually or as a sequence connection strategies how do two processes send a sequence of messages 16.5 685 contention how do we resolve conflicting demands for the network 's lise  given that it is a shared resource in the following sections  we elaborate on each of these issues  16.5.1 naming and name resolution the first component of network communication is the naming o the systems in the network for a process at site a to exchange information with a process at site b  each must be able to specify the other within a computer system  each process has a process identifier  and messages may be addressed with the process identifier beca use networked systems share no memory  however  a host within the system initially has no knowledge about the processes on other hosts  to solve this problem  processes on remote systems are generally identified by the pair host name  identifier  where host name is a name unique within the network and identifier may be a process identifier or other unique number within that host a host name is usually an alphanumeric identifier  rather than a number  to make it easier for users to specify for instance  site a might have hosts named homer  marge  bart  and lisa bart is certainly easier to remember than is 12814831100  names are convenient for humans to use  but computers prefer numbers for speed and simplicity for this reason  there must be a mechanism to !  '  the host name into a that describes the destination system to the networking hardware this mechanism is similar to the name-to-address binding that occurs during program compilation  linking  loading  and execution  chapter 8   in the case of host names  two possibilities exist first  every host may have a data file containing the names and addresses of all the other hosts reachable on the network  similar to binding at compile time   the problem with this model is that adding or removing a host from the network requires updati.n.g the data files on all the hosts the alternative is to distribute the information among systems on the network the network must then use a protocol to distribute and retrieve the information this scheme is like execution-time binding the first method was the one originally used on the internet ; as the internet rnarr-,,or it became untenable  so the second method  the domain-name ' ~ ' ' is now in use  dns specifies the naming structure of the hosts  as well as name-to-address resolution hosts on the internet are logically addressed with multipart names known as ip addresses the parts of an ip address progress frorn the most specific to the most general part  with periods separating the fields for instance  bob.cs.brown.edu refers to host bob in the depattment of science at brown university within the top-level domain edu  domains include com for commercial sites and for organizations  as well as connected to the for systems the resolves in reverse order each a a process on a a name and returns the address of the name server as the final the name server for the host in host-id is returned for a made communicate with bob.cs.brown.edu would result in 686 chapter 16 the kernel of system a issues a request to the name server for the edu domain  asking for the address of the name server for brown.edu the name server for the edu domain must be at a known address  so that it can be queried  the edu nance server returns the address of the host on which the brown.edu name server resides  the kernel on system a then queries the name server at this address and asks about cs.brown.edu  an address is returned ; and a request to that address for bob.cs.brown.edu now  finally  returns an host-id for that host  for example  128.148.31.100   this protocol may seem inefficient  but local caches are usually kept by each name server to speed the process for example  the edu name server would have brown.edu in its cache and would inform system a that it could resolve two portions of the name  returning a pointer to the cs.brown.edu name server  of course  the contents of these caches must be refreshed over time in case the name server is moved or its address changes in fact  this service is so important that many optimizations have occurred in the protocol  as well as many safeguards consider what would happen if the primary edu name server crashed it is possible that no edu hosts would be able to have their addresses resolved  making them all lmreachable ! the solution is to use secondary  back-up name servers that duplicate the contents of the primary servers  before the domain-name service was introduced  all hosts on the internet needed to have copies of a file that contained the names and addresses of each host on the network all changes to this file had to be registered at one site  host sri-nic   and periodically all hosts had to copy the updated file from sri-nic to be able to contact new systems or find hosts whose addresses had changed  under the domain-name service  each name-server site is responsible for updating the host information for that domain for instance  any host changes at brown university are the responsibility of the name server for brown.edu and need not be reported anywhere else dns lookups will automatically retrieve the updated information because they will contact brown.edu directly  within domains  there can be autonomous subdomains to further distribute the responsibility for host-name and host-id changes  java provides the necessary api to design a program that maps ip names to ip addresses the program shown in figure 16.5 is passed an ip name  such as bob.cs.brown.edu  on the command line and either outputs the ip address of the host or returns a message indicating that the host name could not be resolved  an inetaddress is a java class representing an ip name or address the static method getbyname   belonging to the inetaddress class is passed a string representation of an ip name  and it returns the corresponding inetaddress  the program then invokes the gethostaddress   method  which internally uses dns to look up the ip address of the designated host  generally  the operating system is responsible for accepting from its processes a message destined for host name  identifier and for transferring that message to the appropriate host the kernel on the destination host is then responsible for transferring the message to the process named by the identifier  this exchange is by no means trivial ; it is described in section 16.5.4  16.5 i usage  java dnslookup ip name i.e java dnslookup www.wiley.com i public class dnslookup   public static void main  string   args   inetaddress hostaddress ; try   hostaddress = inetaddress.getbyname  args  o   ; system.out.println  hostaddress.gethostaddress    ; catch  unknownhostexception uhe     system err println  unknown host  + args  0   ; figure 16.5 java program illustrating a dns lookup  16.5.2 routing strategies 687 when a process at site a wants to communicate with a process at site b  how is the message sent if there is only one physical path from a to b  such as in a star or tree-structured network   the message must be sent through that path however  if there are multiple physical paths from a to b  then several routing options exist each site has a indicating the alternative paths that can be used to send a message to other sites the table may include information about the speed and cost of the various communication paths  and it may be updated as necessary  either manually or via programs that exchange routing information the three most common routing schemes are td ~ .-i  ja  and fixed routing a path from a to b is specified in advance and does not change unless a hardware failure disables it usually  the shortest path is chosen  so that communication costs are minimized  virtual routing a path from a to b is fixed for the duration of one different sessions involving messages from a to b may use different paths  a session could be as short as a file transfer or as long as a remote-login period  dynamic routing the path used to send a message from site a to site b is chosen only when the message is sent because the decision is made dynamically  separate messages may be assigned different paths site a will make a decision to send the message to site c ; c  in turn  will decide to send it to sited  and so on eventually  a site will deliver the message to b usually  a site sends a message to another site on whatever link is the least used at that particular time  there are tradeoffs among these three schem.es fixed routing can not adapt to link failures or load changes in other words  if a path has been established 688 chapter 16 between a and b  the messages must be sent along this path  even if the path is down or is used more heavily than another possible path we can partially remedy this problem by using virtual routing and can avoid it completely by using dynamic routing fixed routing and virtual routing ensure that ncessages from a to b will be delivered in the order in which they were sent in dynamic routing  messages may arrive out of order we can remedy this problem by appending a sequence number to each message  dynamic routing is the most complicated to set up and run ; however  it is the best way to manage routing in complicated environments unix provides both fixed routing for use on hosts within simple networks and dynamic routing for complicated network environments it is also possible to mix the two within a site  the hosts may just need to know how to reach the system that connects the local network to other networks  such as company-wide networks or the internet   such a node is known as a each individual host has a static route to the gateway  although the gateway itself uses dynamic routing to reach any host on the rest of the network  a router is the entity within the computer network responsible for routing messages a router can be a host computer with routing software or a special-purpose device either way  a router must have at least two network cmmections  or else it would have nowhere to route messages a router decides whether any given message needs to be passed from the network on which it is received to any other network connected to the router it makes this determination by examining the destination internet address of the message  the router checks its tables to determine the location of the destination host  or at least of the network to which it will send the message toward the destination host in the case of static routing  this table is changed only by manual update  a new file is loaded onto the router   with dynamic routing  a is used between routers to inform them of network changes and to allow them to update their routing tables automatically gateways and routers typically are dedicated hardware devices that run code out of firmware  16.5.3 packet strategies messages generally vary in length to simplify the system design  we commonly implement communication with fixed-length messages called or a communication incplemented in one packet can be sent to its destination in a a connectionless message can be in which case the sender has no guarantee that  and can not tell whether  the packet reached its destination alternatively  the packet can be usually  in this case  a packet is returned from the destination indicating that the packet arrived  of course  the return packet could be lost along the way  if a message is too long to fit within one packet  or if the packets need to how back and forth between the two communicators  a connection is established to allow the reliable exchange of multiple packets  16.5.4 connection strategies c ~ uuc'' ' ~ u are able to reach their destinations  processes can institute to exchange information pairs of processes that want to communicate over the network can be connected in a number of ways  16.5 689 the three most common schemes are and circuit switching if two processes want to con1municate  a permanent physical link is established between them tl1is link is allocated for the duration of the communication session  and no other process can use that link during this period  even if the two processes are not actively communicating for a while   this scheme is similar to that used in the telephone system once a communication line has been opened between two parties  that is  party a calls party b   no one else can use this circuit until the communication is terminated explicitly  for example  when the parties hang up   message switching if two processes want to communicate  a temporary link is established for the duration of one message transfer physical links are allocated dynamically among correspondents as needed and are allocated for only short periods each message is a block of data with system information-such as the source  the destination  and errorcorrection codes  ecc  -that allows the communication network to deliver the message to the destination correctly this scheme is similar to the post-office mailing system each letter is a message that contains both the destination address and source  return  address many messages  from different users  can be shipped over the same link  packet switching one logical message may have to be divided into a number of packets each packet may be sent to its destination separately  and each therefore must include a source and a destination address with its data furthermore  the various packets may take different paths through the network the packets must be reassembled into messages as they arrive note that it is not harmful for data to be broken into packets  possibly routed separately  and reassembled at the destination breaking up an audio signal  say  a telephone communication   in contrast  could cause great confusion if it was not done carefully  there are obvious tradeoffs among these schemes circuit switching requires substantial set-up time and may waste network bandwidth  but it incurs less overhead for shipping each message conversely  message and packet switching require less set-up time but incur more overhead per message also  in packet switching  each message must be divided into packets and later reassembled packet switching is the method most commonly used on data networks because it makes the best use of network bandwidth  16.5.5 contention depending on the network topology  a link may cmmect more than two sites in the computer network  and several of these sites may want to transmit information over a link simultaneously this situation occurs mainly in a ring or multiaccess bus network in this case  the transmitted information may become scrambled if it does  it must be discarded ; and the sites must be notified about the problem so that they can retransmit the information if no special provisions are made  this situation may be repeated  resulting in degraded performance  690 chapter 16 16.6 several techniques have been developed to avoid repeated collisions  including collision detection and token passing  csma/cd before transmitting a message over a link  a site must listen to determine whether another message is currently being transmitted over that link ; this technique is called -uvith  if the link is free  the site can start transmitting otherwise  it must wait  and continue to listen  until the link is free if two or more sites begin transmitting at exactly the same time  each thinking that no other site is using the link   then they will register a and will stop transmitting each site will try again after some random time interval  the main problem with this approach is that  when the system is very busy  many collisions may occur  and thus performance may be degraded  nevertheless  csma/cd has been used successfully in the ethernet system  the most common local area network system one strategy for limiting the number of collisions is to limit the number of hosts per ethernet network  adding more hosts to a congested network could result in poor network throughput as systems get faster  they are able to send more packets per time segment as a result  the number of systems per ethernet network generally is decreasing so that networking performance is kept reasonable  token passing a unique message type  known as a continuously circulates in the system  usually a ring structure   a site that wants to transmit information must wait until the token arrives it then removes the token from the ring and begins to transmit its messages when the site completes its round of message passing  it retransmits the token this action  in turn  allows another site to receive and remove the token and to start its message transmission if the token gets lost  the system must detect the loss and generate a new token it usually does that by declaring an to choose a unique site where a new token will be generated  later  in section 18.6  we present one election algorithm a token-passing scheme has been adopted by the ibm and hp i apollo systems the benefit of a token-passing network is that performance is constant adding new sites to a network may lengthen the waiting time for a token  but it will not cause a large performance decrease  as may happen on ethernet on lightly loaded networks  however  ethernet is more efficient  because systems can send messages at any time  when we are designing a communication network  we must deal with the inherent complexity of coordinating asynchronous operations communicating in a potentially slow and error-prone environment in addition  the systems on the network must agree on a protocol or a set of protocols for determining host names  locating hosts on the network  establishing connections  and so on we can simplify the design problem  and related implementation  by partitioning the problem into multiple layers each layer on one system communicates with the equivalent layer on other systems typically  each layer has its own protocols  and communication takes place between peer layers 16.6 691 network environment iso environment real systems environment figure 16.6 two computers communicating via the iso network model  using a specific protocol the protocols may be implemented in hardware or software for instance  figure 16.6 shows the logical communications between two computers  with the three lowest-level layers implemented in hardware  following the international standards organization  iso   we refer to the layers as follows  physical layer the physical layer is responsible for handling both the mechanical and the electrical details of the physical transmission of a bit stream at the physical layer  the communicating systems must agree on the electrical representation of a binary 0 and 1  so that when data are sent as a stream of electrical signals  the receiver is able to interpret the data properly as binary data this layer is implemented in the hardware of the networking device  data-link layer the data-link layer is responsible for handlingfi'ames  or fixed-length parts of packets  including any error detection and recovery that occurs in the physical layer  network layer the network layer is responsible for providing connecti01cs and for routing packets in the communication network  including handling the addresses of outgoing packets  decoding the addresses of incoming packets  and maintaining routing information for proper response to changing load levels routers work at this layer  transport layer the transport layer is responsible for low-level access to the network and for transfer of messages between clients  including partitioning messages into packets  maintaining packet order  controlling flow  and generating physical addresses  session layer the session layer is responsible for implementing sessions  or process-to-process communication protocols typically  these protocols are the actual communications for remote logins and for file and mail transfers  692 chapter 16 presentation layer the presentation layer is responsible for resolving the differences in formats among the various sites in the network  including character conversions and half duplex-full duplex modes  character echoing   application layer the application layer is responsible for interacting directly with users this layer deals with file transfe1 ~ remote-login protocols  and electronic mail  as well as with schemas for distributed databases  figure 16.7 summarizes the set of cooperating protocols-showing the physical flow of data as mentioned  logically each layer of a protocol stack communicates with the equivalent layer on other systems but physically  a message starts at or above the application layer and end-user application process distributed information transfer-syntax negotiation data-representation transformations dialog and synchronization control for application entities network-independent message-interchange service j end-to ~ end message transfer  connection management  error control  fragmentation  flow control  network routing  addressing  call set-up and clearing application layer presentation layer session layer transport layer network layer data-link control  framing  data transparency  error control  link layer mechanical and electrical networkcinterface connections physical connection to network termination equipment physical layer 16.7 the iso protocol stack  16.6 data-link -layer header network-layer header transport-layer header f-------1 session-layer header f-------1 presentation layer f-------1 application layer message l_ _ _____j data-link -layer trailer figure 16.8 an iso network message  693 is passed through each lower level in turn each layer may modify the message and il1.clude message-header data for the equivalent layer on the receiving side ultimately  the message reaches the data-network layer and is transferred as one or more packets  figure 16.8   the data-lil1.k layer of the target system receives these data  and the message is moved up through the protocol stack ; it is analyzed  modified  and stripped of headers as it progresses it fu1.ally reaches the application layer for use by the receiving process  the iso model formalizes some of the earlier work done in network protocols but was developed in the late 1970s and is currently not in widespread use perhaps the most widely adopted protocol stack is the tcp /ip model  which has been adopted by virtually all internet sites the tcp /ip protocol stack has fewer layers than does the iso model theoretically  because it combilles several functions ill each layer  it is more difficult to implement but more efficient than iso networking the relationship between the iso and tcp /ip models is shown in figure 16.9 the tcp /ip application layer identifies several protocols ill widespread use ill the internet  illcluding http  ftp  telnet  dns  and smtp the transport layer identifies the unreliable  connectionless user datagram protocol  udp  and the reliable  connection-oriented transmission control protocol  tcp   the internet protocol  ip  is responsible for routing ip datagrams through the internet the tcp /ip model does not formally identify a link or physical laye1 ~ allowing tcp /ip traffic to run across any physical network in section 16.9  we consider the tcp /ip model running over an ethernet network  security should be a concern in the design and implementation of any modern communication protocol both strong authentication and encryption are needed for secure communication strong authentication ensures that the sender and receiver of a communication are who or what they are supposed to be encryption protects the contents of the communication from eavesdropping weak authentication and clear-text communication are still very common  however  for a variety of reasons when most of the 694 chapter 16 16.7 iso presentation session physical tcp/ip http  dns  telnet smtp  ftp not defined not defined tcp-udp not defined not defined figure 16.9 the iso and tcp/ip protocol stacks  common protocols were designed  security was frequently less important than performance  simplicity  and efficiency  strong authentication requires a multistep handshake protocol or authentication devices  adding complexity to a protocol modern cpus can efficiently perform encryption  and systems frequently offload encryption to separate cryptography processors  so system performance is not compromised longdistance communication can be made secure by authenticating the endpoints and encrypting the stream of packets in a virtual private network  as discussed in 15.4.2 lan communication remains unencrypted at most sites  but protocols such as nfs version 4  which includes strong native authentication and encryption  should help improve even lan security  a distributed system may suffer from various types of hardware failure the failure of a link  the failure of a site  and the loss of a message are the most common types to ensure that the system is robust  we must detect any of these failures  reconfigure the system so that computation can continue  and recover when a site or a link is repaired  16.7.1 failure detection in an environment with no shared memory  we are generally unable to differentiate among link failure  site failure  and message loss we can usually detect only that one of these failures has occurred once a failure has been 16.7 695 detected  appropriate action must be taken what action is appropriate depends on the particular application  to detect link and site failure  we use a procedure suppose that sites a and b have a direct physical link between them  at fixed intervals  the sites send each other an j-am-up m.essage if site a does not receive this message within a predetermined time period  it can assume that site b has failed  that the link between a and b has failed  or that the message from b has been lost at this point  site a has two choices it can wait for another time period to receive an j-am-up message from b  or it can send an are-you-up message to b  if time goes by and site a still has not received an j-am-up message  or if site a has sent an are-you-up message and has not received a reply  the procedure can be repeated again  the only conclusion that site a can draw safely is that some type of failure has occurred  site a can try to differentiate between link failure and site failure by sending an are-you-up message to b by another route  if one exists   if and when b receives this message  it immediately replies positively this positive reply tells a that b is up and that the failure is in the direct link between them since we do not know in advance how long it will take the message to travel from a to b and back  we must use a at the time a sends the are-you-up message  it specifies a time interval during which it is willing to wait for the reply from b if a receives the reply message within that time interval  then it can safely conclude that b is up if not  however  that is  if a time-out occurs   then a may conclude only that one or more of the following sih1ations has occurred  site b is down  the direct link  if one exists  from a to b is down  the alternative path from a to b is down  the message has been lost  site a can not  however  determine which of these events has occurred  16.7.2 reconfiguration suppose that site a has discovered  through the mechanism described in the previous section  that a failure has occurred it must then initiate a procedure that will allow the system to reconfigure and to continue its normal mode of operation  if a direct link from a to b has failed  this information must be broadcast to every site in the system  so that the various routing tables can be updated accordingly  if the system believes that a site has failed  because that site can be reached no longer   then all sites in the system must be so notified  so that they will no longer attempt to use the services of the failed site the failure of a site that serves as a central coordinator for some activity  such as deadlock detection  requires the election of a new coordinator similarly  if the failed 696 chapter 16 site is part of a logical ring  then a new logical ring must be constructed  note that  if the site has not failed  that is  if it is up but camwt be reached   then we may have the undesirable situation in which two sites serve as the coordinator when the network is partitioned  the two coordinators  each for its own partition  may initiate conflicting actions for example  if the coordinators are responsible for implementing mutual exclusion  we may have a situation in which two processes are executing simultaneously in their critical sections  16.7.3 recovery from failure when a failed link or site is repaired  it must be integrated into the system gracefully and smoothly  suppose that a link between a and b has failed wlcen it is repaired  both a and b must be notified we can accomplish this notification by continuously repeating the handshaking procedure described in section 16.7.1  suppose that site b has failed wlcen it recovers  it must notify all other sites that it is up again site b then may have to receive information from the other sites to update its local tables ; for example  it may need routing-table information  a list of sites that are down  or mcdelivered messages and mail if the site has not failed but simply could not be reached  then this information is still required  16.7.4 fault tolerance a distributed system must tolerate a certain level of failure and continue to function normally when faced with various types of failures making a facility fault tolerant starts at the protocol level  as described above  but continues through all aspects of the system we use the term fault tolerance in a broad sense communication faults  machine failures  of type fail-stop where the machine stops before performing an erroneous operation that is visible to other processors   storage-device crashes  and decays of storage media should all be tolerated to some extent a should continue to function  perhaps in a degraded form  when faced with such failures the degradation can be in performance  in functionality  or in both it should be proportional  however  to the failures that caused it a system that grinds to a halt when only one of its components fails is certainly not fault tolerant  unfortunately  fault tolerance can be difficult and expensive to implement  at the network layer  multiple redundant communication paths and network devices such as switches and routers are needed to avoid a cmnmunication failure a storage failure can cause loss of the operating system  applications  or data storage units can include redundant hardware components that automatically take over from each other in case of failure in addition  raid systems can ensure continued access to the data even in the event of one or more disk failures  section 12.7   a system failure without redundancy can cause an application or an entire facility to stop operation the inost simple system failure involves a system running only stateless applications these applications can be restarted without 16.8 16.8 697 compromising the operation ; so as long as the applications can run on more than one computer  node   operation can continue such a facility is commonly known as a because it is computation-centric  in contrast  systems involve running applications that access and modify shared data as a result  data-centric computing facilities are more difficult to make fault tolerant they failure-monitoring software and special infrastructure for instance  such as veritas cluster and sun cluster include two or more computers and a set of shared disks any given application can be stored on the computers or on the shared disk  but the data must be stored on the shared disk the running application 's node has exclusive access to the application 's data on disk the application is monitored by the cluster software  and if it fails it is automatically restarted  if it camwt be restarted  or if the entire computer fails  the node 's exclusive access to the application 's data is terminated and is granted to another node in the cluster the application is restarted on that new node the application loses whatever state information was in the failed system 's memory but can continue based on whatever state it last wrote to the shared disk from a user 's point of view  a service was interrupted and then restarted  possibly with some data missing  specific applications may improve on this functionality by implementing lock management along with clustering with lock management  section 18.4.1   the application can run on multiple nodes and can use the same data on shared disks concurrently clustered databases frequently implement this functionality if anode fails  transactions can continue on other nodes  and users notice no interruption of service  as long as the client is able to automatically locate the other nodes in the cluster any noncommitted transactions on the failed node are lost  but again  client applications can be designed to retry noncommitted transactions if they detect a failure of their database node  making the multiplicity of processors and storage devices to the users has been a key challenge to many designers ideally  a distributed system should look to its users like a conventional  centralized system the user interface of a transparent distributed system should not distinguish between local and remote resources that is  users should be able to access remote resources as though these resources were local  and the distributed system should be responsible for locating the resources and for arranging for the appropriate interaction  another aspect of transparency is user mobility it would be convenient to allow users to log into any machine in the system rather than forcing them to use a specific machine a transparent distributed system facilitates user mobility by bringiicg over the user 's environment  for example  home directory  to wherever he logs in both the andrew file system from cmu and project athena from mit provide this functionality on a large scale ; nfs can provide it on a smaller scale  still another issue is l ;  -the capability of a system to adapt to increased service load systems have bounded resources and can become completely saturated under increased load for example  with respect to a file 698 chapter 16 system  saturation occurs either when a server 's cpu runs at a high utilization rate or when disks are almost full scalability is a relative property  but it can be measured accurately a scalable system reacts more gracefully to increased load than does a nonscalable one first  its performance degrades more moderately ; and second  its resources reach a saturated state later even perfect design can not accommodate an ever-growing load adding new resources might solve the problem  but it might generate additional indirect load on other resources  for example  adding machines to a distributed system can clog the network and increase service loads   even worse  expanding the system can call for expensive design modifications a scalable system should have the potential to grow without these problems in a distributed system  the ability to scale up gracefully is of special importance  since expanding the network by adding new machines or interconnecting two networks is commonplace in short  a scalable design should withstand high service load  accommodate growth of the user community  and enable simple integration of added resources  scalability is related to fault tolerance  discussed earlier a heavily loaded component can become paralyzed and behave like a faulty component also  shifting the load from a faulty component to that component 's backup can saturate the latter generally  having spare resources is essential for ensuring reliability as well as for handling peak loads gracefully an inherent advantage of a distributed system is a potential for fault tolerance and scalability because of the multiplicity of resources however  inappropriate design can obscure this potential fault-tolerance and scalability considerations call for a design demonstrating distribution of control and data  very large-scale distributed systems  to a great extent  are still only theoretical no magic guidelines ensure the scalability of a system it is easier to point out why current designs are not scalable we next discuss several designs that pose problems and propose possible solutions  all in the context of scalability  one principle for designing very large-scale systems is that the service demand from any component of the system should be bounded by a constant that is independent of the number of nodes in the system any service mechanism whose load demand is proportional to the size of the system is destined to become clogged once the system grows beyond a certain size  adding more resources will not alleviate such a problem the capacity of this mechanism simply limits the growth of the system  another principle concerns centralization central control schemes and central resources should not be used to build scalable  and fault-tolerant  systems examples of centralized entities are central authentication servers  central naming servers  and central file servers centralization is a form of functional asyrrunetry among machines constituting the system the ideal alternative is a functionally symmetric configuration ; that is  all the component machines have an equal role in the operation of the system  and hence each machine has some degree of autonomy practically  it is virtually impossible to comply with such a principle for instance  incorporating diskless machines violates functional symmetry  since the workstations depend on a central disk however  autonomy and symmetry are important goals to which we should aspire  deciding on the process structure of the server is a major problem in the design of any service servers are supposed to operate efficiently in peak 16.9 16.9 699 periods  when hundreds of active clients need to be served simultaneously a single-process server is certainly not a good choice  since whenever a request necessitates disk i/0  the whole service will be blocked assigning a process for each client is a better choice ; however  the expense of frequent context switches between the processes must be considered a related problem occurs because all the server processes need to share information  one of the best solutions for the server architecture is the use of lightweight processes  or threads  which we discuss in chapter 4 we can think of a group of lightweight processes as multiple threads of control associated with some shared resources usually  a lightweight process is not bound to a particular client instead  it serves single requests of different clients scheduling of threads can be preemptive or nonpreemptive if threads are allowed to run to completion  nonpreemptive   then their shared data do not need to be protected explicitly otherwise  some explicit locking mechanism must be used  clearly  some form of lightweight-process scheme is essential if servers are to be scalable  we now return to the name-resolution issue raised in section 16.5.1 and examine its operation with respect to the tcf /if protocol stack on the internet  we consider the processing needed to transfer a packet between hosts on different ethernet networks  in a tcf /if network  every host has a name and an associated if address  or host-id   both of these strings must be unique ; and so that the name space can be managed  they are segmented the name is hierarchical  as explained in section 16.5.1   describing the host name and then the organization with which the host is associated the host-id is split into a network number and a host number the proportion of the split varies  depending on the size of the network once the internet adrninistrators assign a network number  the site with that number is free to assign host-ids  the sending system checks its routing tables to locate a router to send the frame on its way the routers use the network part of the host-id to transfer the packet from its source network to the destination network the destination system then receives the packet the packet may be a complete message  or it may just be a component of a message  with more packets needed before the message can be reassembled and passed to the tcf /udf layer for transmission to the destination process  now we know how a packet moves from its source network to its destination within a network  how does a packet move from sender  host or router  to receiver ethernet device has a unique byte number  called the assigned to it for addressing two devices on a lan communicate with each other only with this number if a system needs to send data to another system  the networking software generates an containing the if address of the destination system this packet is to all other systems on that ethernet network  a broadcast uses a special network address  usually  the maximum address  to signal that all hosts should receive and process the packet the 700 chapter 16 broadcast is not re-sent by gateways  so only systems on the local network receive it only the system whose ip address matches the ip address of the arp request responds and sends back its mac address to the system that initiated the query for efficiency  the host caches the ip-mac address pair in an internal table the cache entries are so that an entry is eventually removed from the cache if an access to that system is not required within a given time in this way  hosts that are removed from a network are eventually forgotten for added performance  arp entries for heavily used hosts may be hardwired in the arp cache  once an ethernet device has announced its host-id and address  communication can begin a process may specify the name of a host with which to communicate networking software takes that name and determines the ip address of the target  using a dns lookup the message is passed from the application laye1 ~ through the software layers  and to the hardware layer at the hardware layer  the packet  or packets  has the ethernet address at its start ; a trailer indicates the end of the packet and contains a for detection of packet damage  figure 16.10   the packet is placed on the network by the ethernet device the data section of the packet may contain some or all of the data of the original message  but it may also contain some of the upper-level headers that compose the message in other words  all parts of the original message must be sent from source to destination  and all headers above the 802.3layer  data-link layer  are included as data in the ethernet packets  if the destination is on the same local network as the source  the system can look in its arp cache  find the ethernet address of the host  and place the packet on the wire the destination ethernet device then sees its address in the packet and reads in the packet passing it up the protocol stack  if the destination system is on a network different from that of the source  the source system finds an appropriate router on its network and sends the packet there routers then pass the packet along the wan 1-mtil it reaches its bytes 7 2 or 6 2 or 6 2 0-1500 0-46 4 pt.e ~ ~ n1bh  l ~ s.tartfc1ft  r = ccll et  1 each byte pattern 1010101 o data pattern 10101011 ethernet address or broadcast ethernet address length in bytes message data message must be 63 bytes long for error detection figure i 6.10 an ethernet packet  16.10 701 destination network the router that connects the destination network checks its arp cache  finds the ethernet number of the destination  and sends the packet to that host through all of these transfers  the data-link-layer header may change as the ethernet address of the next router in the chain is used  but the other headers of the packet remain the same until the packet is received and processed by the protocol stack and finally passed to the receiving process by the kernel  a distributed system is a collection of processors that do not share memory or a clock instead  each processor has its own local memory  and the processors communicate with one another through various communication lines  such as high-speed buses and telephone lines the processors in a distributed system vary in size and function they may include small microprocessors  workstations  minicomputers  and large general-purpose computer systems  the processors in the system are connected through a communication network  which can be configured in a number of ways the network may be fully or partially connected it may be a tree  a star  a ring  or a multiaccess bus the communication-network design must include routing and com1ection strategies  and it must solve the problems of contention and security  a distributed system provides the user with access to the resources the system provides access to a shared resource can be provided by data migration  computation migration  or process migration  protocol stacks  as specified by network layering models  massage the message  adding information to it to ensure that it reaches its destination a naming system  such as dns  must be used to translate from a host name to a network address  and another protocol  such as arp  may be needed to translate the network number to a network device address  an ethernet address  for instance   if systems are located on separate networks  routers are needed to pass packets from source network to destination network  a distributed system may suffer from various types of hardware failure  for a distributed system to be fault tolerant  it must detect hardware failures and reconfigure the system when the failure is repaired  the system must be reconfigured again  16.1 what are the advantages of using dedicated hardware devices for routers and gateways what are the disadvantages of using these devices compared with using general-purpose computers 16.2 why would it be a bad idea for gateways to pass broadcast packets between networks what would be the advantages of doing so 16.3 consider a network layer that senses collisions and retransmits immediately on detection of a collision what problems could arise with this strategy how could they be rectified 702 chapter 16 16.4 even though the iso model of networking specifies seven layers of functionality  most computer systems use fewer layers to implement a network why do they use fewer layers what problems could the use of fewer layers cause 16.5 the lower layers of the iso network model provide datagram service  with no delivery guarantees for messages a transport-layer protocol such as tcp is used to provide reliability discuss the advantages and disadvantages of supporting reliable message delivery at the lowest possible layer  16.6 what are the advantages and the disadvantages of making the computer network transparent to the user 16.7 under what circumstances is a token-passing network more effective than an ethernet network 16.8 process migration within a heterogeneous network is usually impossible  given the differences in architectures and operating systems  describe a method for process migration across different architectures running  a the same operating system b different operating systems 16.9 contrast the various network topologies in terms of the following attributes  a reliability b available bandwidth for concurrent communications c installation cost d load balance in routing responsibilities 16.10 how does using a dynamic routing strategy affect application behavior for what type of applications is it beneficial to use virtual routing instead of dynamic routing 16.11 the original http protocol used tcp /ip as the underlying network protocol for each page  graphic  or applet  a separate tcp session was constructed  used  and torn down because of the overhead of building and destroying tcp lip connections  performance problems resulted from this implementation method would using udp rather than tcp be a good alternative what other changes could you make to improve http performance 16.12 what are the advantages and disadvantages of using circuit switching for what kinds of applications is circuit switching a viable strategy 16.13 in what ways is using a name server better than using static host tables what problems or complications are associated with name servers what methods could you use to decrease the amount of traffic name servers generate to satisfy translation requests 703 16.14 of what use is an address-resolution protocol why is it better to use such a protocol than to make each host read each packet to determine that packet 's destination does a token-passing network need such a protocol explain your answer  16.15 what is the difference between computation migration and process migration which is easier to implement  and why 16.16 run the program shown in figure 16.5 and determine the ip addresses of the following host names  www.wiley.com www.cs.yale.edu www.apple.com www.westminstercollege.edu www.ietf.org 16.17 to build a robust distributed system  you must know what kinds of failures can occur  a list three possible types of failure in a distributed system  b specify which of the entries in your list also are applicable to a centralized system  16.18 explain why doubling the speed of the systems on an ethernet segment may result in decreased network performance what changes could help solve this problem 16.19 name servers are organized in a hierarchical manner what is the purpose of using a hierarchical organization 16.20 consider a distributed system with two sites  a and b consider whether site a can distinguish among the following  a b goes down  b the link between a and b goes down  c b is extremely overloaded  and its response time is 100 times longer than normal  what implications does your answer have for recovery in distributed systems tanenbaum  2003   stallings  2000a   and kurose and ross  2005  provide general overviews of computer networks williams  2001  covers computer networking from a computer-architecture viewpoint  the internet and its protocols are described in comer  1999  and comer  2000   coverage of tcp /ip can be found in stevens  1994  and stevens  1995   704 chapter 16 unix network programming is described thoroughly in stevens  1997  and stevens  1998   discussions concerning distributed operating-system structures have been offered by coulouris et al  2001  and tanenbaum and van steen  2002   load balancing and load sharing are discussed by i-iarchol-balter and downey  1997  and vee and i-isu  2000   i-iarish and owens  1999  describes load-balancing dns servers process migration is discussed by jul et al  1988   douglis and ousterhout  1991   han and ghosh  1998   and milojicic et al   2000   issues relating to a distributed virtual machine for distributed systems are examined in sirer et al  1999   17.1 in the previous chapter  we discussed network construction and the low-level protocols needed to transfer between systems now we examine one use of this infrastructure a is a distributed implementation of the classical time-sharing of a file system  where multiple users share files and storage resources  chapter 11   the purpose of a dfs is to support the same kind of sharing when the files are physically dispersed among the sites of a distributed system  in this chapter  we describe how a dfs can be designed and implemented  first  we discuss common concepts on which dfss are based then  we illustrate our concepts by examining one influential dfs-the andrew file system  afs   to explain the naming mechanism that provides location transparency and independence  to describe the various methods for accessing distributed files  to contrast stateful and stateless distributed file servers  to show how replication of files on different machines in a distributed file system is a useful redundancy for improving availability  to introduce the andrew file system  afs  as an example of a distributed file system  as we noted in the preceding chapter  a distributed system is a collection of loosely coupled computers interconnected by a communication network  these computers can share physically dispersed files by using a distributed file system  dfs   in this chapter  we use the term dfs to mean distributed file systems in general  not the commercial transarc dfs product ; we refer to the latter as transarc dfs also  nfs refers to nfs version 3  unless otherwise noted  705 706 chapter 17 to explain the structure of a dfs  we need to define the terms service  server  and client a is a software entity running on one or more machines and providing a particular type of function to clients a is the service software running on a single machine a is a process that can invoke a service using a set of operations that form its sometimes a lower-level interface is defined for the actual cross-machine interaction ; it is the using this terminology  we say that a file system provides file services to clients a client interface for a file service is formed by a set of primitive file operations  such as create a file  delete a file  read from a file  and write to a file  the primary hardware concponent that a file server controls is a set of local secondary-storage devices  usually  magnetic disks  on which files are stored and from which they are retrieved according to the clients ' requests  a dfs is a file system whose clients  servers  and storage devices are dispersed among the machines of a distributed system accordingly  service activity has to be carried out across the network instead of a single centralized data repository  the system frequently has multiple and independent storage devices as you will see  the concrete configuration and implementation of a dfs may vary from system to system in some configurations  servers run on dedicated machines ; in others  a machine can be both a server and a client a dfs can be implemented as part of a distributed operating system or  alternatively  by a software layer whose task is to manage the communication between conventional operating systems and file systems the distinctive features of a dfs are the multiplicity and autonomy of clients and servers in the system  ideally  a dfs should appear to its clients to be a conventional  centralized file system the multiplicity and dispersion of its servers and storage devices should be made invisible that is  the client interface of a dfs should not distinguish between local and remote files it is up to the dfs to locate the files and to arrange for the transport of the data a dfs facilitates user mobility by bringing a user 's environment  that is  home directory  to wherever the user logs in  the most important performance measure of a dfs is the amount of time needed to satisfy service requests in conventional systems  this time consists of disk-access time and a small amount of cpu-processing time in a dfs  however  a remote access has the additional overhead attributed to the distributed structure this overhead includes the time to deliver the request to a server  as well as the time to get the response across the network back to the client for each direction  in addition to the transfer of the information  there is the cpu overhead of running the communication protocol software the performance of a dfs can be viewed as another dimension of the dfs 's transparency that is  the performance of an ideal dfs would be comparable to that of a conventional file system  the fact that a dfs manages a set of dispersed storage devices is the dfs ' s key distinguishing feature the overall storage space managed by a dfs is composed of different and remotely located smaller storage spaces usually  these constituent storage spaces correspond to sets of files a cmnpm1.c  nt is the smallest set of files that can be stored on a single machine  independently from other units all files belonging to the same component unit must reside in the same location  17.2 17.2 707 is a mapping between logical and physical objects for instance  users deal with logical data objects represented by file nances  whereas the system manipulates physical blocks of data stored on disk tracks usually  a user refers to a file by a textual name the latter is mapped to a lower-level numerical identifier that in turn is mapped to disk blocks this multilevel mapping provides users with an abstraction of a file that hides the details of how and where on the disk the file is stored  in a transparent dfs  a new dimension is added to the abstraction  that of hiding where in the network the file is located in a conventional file system  the range of the naming mapping is an address within a disk in a dfs  this range is expanded to include the specific machine on whose disk the file is stored  going one step further with the concept of treating files as abstractions leads to the possibility of given a file name  the mapping returns a set of the locations of this file 's replicas in this abstraction  both the existence of multiple copies and their locations are hidden  17.2.1 naming structures we need to differentiate two related notions regarding name mappings in a dfs   the name of a file does not reveal any hint of the file 's physical storage location  'j ' ~ ' '''  ' ' ' ' the name of a file does not need to be changed when the file 's physical storage location changes  both definitions relate to the level of naming discussed previously  since files have different names at different levels  that is  user-level textual names and system-level numerical identifiers   a location-independent naming scheme is a dynamic mapping  since it can map the same file name to different locations at two different times therefore  location independence is a stronger property than is location transparency  in practice  most of the current dfss provide a static  location-transparent mapping for user-level names these systems  however  do not support that is  changing the location of a file automatically is impossible  hence  the notion of location independence is irrelevant for these systems  files are associated permanently with a specific set of disk blocks files and disks can be moved between machines manually  but file migration implies an automatic  operating-system-initiated action only afs and a few experimental file systems support location independence and file mobility afs supports file mobility mainly for administrative purposes a protocol provides migration of afs component units to satisfy high-level user requests  without changing either the user-level names or the low-level names of the corresponding files  a few aspects can further differentiate location independence and static location transparency  divorce of data from location  as exhibited by location independence  provides a better abstraction for files a file name should denote the file 's 708 chapter 17 most significant attributes  which are its contents ratber than its location  location-independent files can be viewed as logical data containers that are not attached to a specific storage location if only static location transparency is supported  the file name still denotes a specific  although hidden  set of physical disk blocks  static location transparency provides users with a convenient way to share data users can share remote files by simply naming the files in a locationtransparent manner  as though the files were local nevertheless  sharing the storage space is cumbersome  because logical names are still statically attached to physical storage devices location independence promotes sharing the storage space itself  as well as the data objects when files can be mobilized  the overall  system-wide storage space looks like a single virtual resource a possible benefit of such a view is the ability to balance the utilization of disks across the system  location independence separates the naming hierarchy from the storagedevices hierarchy and from the intercomputer structure by contrast  if static location transparency is used  although names are transparent   we can easily expose the correspondence between component units and machines the machines are configured in a pattern similar to the naming structure this configuration may restrict the architecture of the system um1.ecessarily and conflict with other considerations a server in charge of a root directory is an example of a structure that is dictated by the naming hierarchy and contradicts decentralization guidelines  once the separation of name and location has been completed  clients can access files residing on remote server systems in fact  these clients may be and rely on servers to provide all files  including the operatingsystem kernel special protocols are needed for the boot sequence  however  consider the problem of getting the kernel to a diskless workstation the diskless workstation has no kernel  so it cam1.ot use the dfs code to retrieve the kernel instead  a special boot protocol  stored in read-only memory  rom  on the client  is invoked it enables networking and retrieves only one special file  the kernel or boot code  from a fixed location once the kernel is copied over the network and loaded  its dfs makes all the other operating-system files available the advantages of diskless clients are many  including lower cost  because the client machines require no disks  and greater convenience  when an operating-system upgrade occurs  only the server needs to be modified   the disadvantages are the added complexity of the boot protocols and the performance loss resulting from the use of a network rather than a local disk  the current trend is for clients to use both local disks and remote file servers  operating systems and networking software are stored locally ; file systems containing user data-and possibly applications-are stored on remote file systems some client systems may store commonly used applications  such as word processors and web browsers  on the local file system as well other  less commonly used applications may be from the remote file server to the client on demand the main reason for providing clients with local file systems rather than pure diskless systems is that disk drives are rapidly increasing in capacity and decreasing in cost  with new generations appearing every year or so the same can not be said for networks  which evolve every few years  17.2 709 overall  systems are growing more quickly than are networks  so extra work is needed to limit network access to improve system throughput  17.2.2 naming schemes there are three main approaches to naming schemes in a dfs in the simplest approach  a file is identified by some combination of its host name and local name  which guarantees a unique system-wide name in ibis  for instance  a file is identified uniquely by the name host  local-name  where local-name is a unix-like path this naming scheme is neither location transparent nor location independent nevertheless  the same file operations can be used for both local and remote files the dfs is structured as a collection of isolated component units  each of which is an entire conventional file system in this first approach  component 1-mits remain isolated  although means are provided to refer to a remote file we do not consider this scheme any further in this text  the second approach was popularized by sun 's network file system  nfs  nfs is the file-system component of onc +  a networking package supported by many unix vendors nfs provides a means to attach remote directories to local directories  thus giving the appearance of a coherent directory tree  early nfs versions allowed only previously mmmted remote directories to be accessed transparently with the advent of the feature  mounts are done on demand  based on a table of mount points and file-structure names components are integrated to support transparent sharing  although this integration is limited and is not uniform  because each machine may attach different remote directories to its tree the resulting structure is versatile  we can achieve total integration of the component file systems by using the third approach here  a single global name structure spans all the files in the system ideally  the composed file-system structure is the same as the structure of a conventional file system in practice  however  the many special files  for example  unix device files and machine-specific binary directories  make this goal difficult to attain  to evaluate naming structures  we look at their the most complex and most difficult-to-maintain structure is the nfs structure  because any rem.ote directory can be attached anywhere onto the local directory tree  the resulting hierarchy can be highly m1structured if a server becomes unavailable  some arbitrary set of directories on different machines becomes unavailable in addition  a separate accreditation mechanism controls which machine is allowed to attach which directory to its tree thus  a user might be able to access a remote directory tree on one client but be denied access on another client  17.2.3 implementation techniques implementation of transparent naming requires a provision for the mapping of a file naine to the associated location to keep this mapping manageable  we must aggregate sets of files into component units and provide the mapping on a component-unit basis rather than on a single-file basis this aggregation serves administrative purposes as well unix-like systems use the hierarchical directory tree to provide name-to-location mapping and to aggregate files recursively into directories  710 chapter 17 17.3 to enhance the availability of the crucial mapping information  we can use replication  local caching  or both as we noted  location independence means that the mapping changes over time ; hence  replicating the mapping makes a simple yet consistent update of this information impossible a teclllcique to overcome this obstacle is to introduce low-level me textual file names are mapped to lower-level file identifiers that indicate to which component unit the file belongs these identifiers are still location independent they can be replicated and cached freely without being invalidated by migration of component units the inevitable price is the need for a second level of mapping  which maps component units to locations and needs a simple yet consistent update mechanism implementing unix-like directory trees using these low-levet location-independent identifiers makes the whole hierarchy invariant under component-unit migration the only aspect that does change is the component-unit location mapping  a common way to implement low-level identifiers is to use structured names these names are bit strings that usually have two parts the first part identifies the component unit to which the file belongs ; the second part identifies the particular file within the unit variants with more parts are possible the invariant of structured names  however  is that individual parts of the name are unique at all times only within the context of the rest of the parts we can obtain uniqueness at all times by taking care not to reuse a name that is still in use  by adding sufficiently more bits  this method is used in afs   or by using a timestamp as one part of the name  as done in apollo domain   another way to view this process is that we are taking a location-transparent system  such as ibis  and adding another level of abstraction to produce a location-independent naming scheme  aggregating files into component units and using lower-level locationindependent file identifiers are techniques exemplified in afs  consider a user who requests access to a remote file the server storing the file has been located by the nanling scheme  and now the actual data transfer must take place  one way to achieve this transfer is through a whereby requests for accesses are delivered to the server  the server machine performs the accesses  and their results are forwarded back to the user one of the most common ways of implementing remote service is the remote procedure call  rpc  paradigm  which we discussed in chapter 3 a direct analogy exists between disk-access methods in conventional file systems and the remote-service method in a dfs  using the remote-service method is analogous to performing a disk access for each access request  to ensure reasonable performance of a remote-service mechanism  we can use a form of caching in conventional file systems  the rationale for caching is to reduce disk i/0  thereby increasing performance   whereas in dfss  the goal is to reduce both network traffic and disk i/0 in the following discussion  we describe the implementation of caching in a dfs and contrast it with the basic remote-service paradigm  17.3 711 17.3.1 basic caching scheme the concept of caching is simple if the data needed to satisfy the access request are not already cached  then a copy of those data is brought from the server to the client system accesses are performed on the cached copy the idea is to retain recently accessed disk blocks in the cache  so that repeated accesses to the same information can be handled locally  without additional network traffic  a replacement policy  for example  the least-recently-used algorithm  keeps the cache size bounded no direct correspondence exists between accesses and traffic to the server files are still identified with one master copy residing at the server machine  but copies  or parts  of the file are scattered in different caches  when a cached copy is modified  the changes need to be reflected on the master copy to preserve the relevant consistency semantics the problem of keeping the cached copies consistent with the master file is the which we discuss in section 17.3.4 dfs caching could just as easily be called  it acts sincilarly to demand-paged virtual memory  except that the backing store usually is not a local disk but rather a remote server nfs allows the swap space to be mounted remotely  so it actually can implement virtual memory over a network  notwithstanding the resulting performance penalty  the granularity of the cached data in a dfs can vary from blocks of a file to an entire file usually  more data are cached than are needed to satisfy a single access  so that many accesses can be served by the cached data this procedure is much like diskread-ahead  section 11.6.2   afs caches files in large chunks  64 kb   the other systems discussed in this chapter support caching of individual blocks driven by client demand increasing the caching unit increases the hit ratio  but it also increases the miss penalty  because each miss requires more data to be transferred it increases the potential for consistency problems as well selecting the unit of caching involves considering parameters such as the network transfer unit and the rpc protocol service unit  if an rpc protocol is used   the network transfer unit  for ethernet  a packet  is about 1.5 kb  so larger units of cached data need to be disassembled for delivery and reassembled on reception  block size and total cache size are obviously of importance for blockcaching schemes in unix-like systems  common block sizes are 4 kb and 8 kb for large caches  over 1mb   large block sizes  over 8 kb  are beneficial for smaller caches  large block sizes are less beneficial because they result in fewer blocks in the cache and a lower hit ratio  17.3.2 cache location where should the cached data be stored-on disk or in main memory disk caches have one clear advantage over main-memory caches  they are reliable  modifications to cached data are lost in a crash if the cache is kept in volatile memory moreove1 ~ if the cached data are kept on disk  they are still there during recovery  and there is no need to fetch them again main-memory caches have several advantages of their own  however  main-memory caches permit workstations to be diskless  data can be accessed more quickly from a cache in main memory than from one on a disk  712 chapter 17 technology is moving toward larger and less expensive memory the resulting performance speedup is predicted to outweigh the advantages of disk caches  the server caches  used to speed up disk i/0  will be in main memory regardless of where user caches are located ; if we use main-memory caches on the user machine  too  we can build a single caching nl.echanism for use by both servers and users  many remote-access implementations can be thought of as hybrids of caching and remote service in nfs  for instance  the implementation is based on remote service but is augmented with client and server-side memory caching for performance similarly sprite 's implementation is based on caching ; but under certain circumstances  a remote-service method is adopted thus  to evaluate the two methods  we must evaluate the degree to which either method is emphasized  the nfs protocol and most implementations do not provide disk caching  recent solaris implementations ofnfs  solaris 2.6 and beyond  include a clientside disk-caching option  the  file system once the nfs client reads blocks of a file from the serve1 ~ it caches them in memory as well as on disk  if the memory copy is flushed  or even if the system reboots  the disk cache is referenced if a needed block is neither in memory nor in the cachefs disk cache  an rpc is sent to the server to retrieve the block  and the block is written into the disk cache as well as stored in the memory cache for client use  17.3.3 cache-update policy the policy used to write modified data blocks back to the server 's master copy has a critical effect on the performance and reliability the simplest policy is to write data to disk as soon as they are placed in any cache  the advantage of a is reliability  little information is lost when a client system crashes however  this policy requires each write access to wait until the information is sent to the server  so it causes poor write performance caching with write-through is equivalent to using remote service for write accesses and exploiting caching for read accesses  an alternative is the also known as where we delay updates to the master copy modifications are written to the cache and then are written through to the server at a later time this policy has two advantages over write-through first  because writes are made to the cache  write accesses complete much more quickly second  data may be overwritten before they are written back  in which case only the last update needs to be written at all unfortunately  delayed-write schemes introduce reliability problems  since unwritten data are lost whenever a user machine crashes  variations of the delayed-write policy differ in when modified data blocks are flushed to the server one alternative is to flush a block when it is about to be ejected from the client 's cache this option can result in good performance  but some blocks can reside in the client 's cache a long time before they are written back to the server a compromise between this alternative and the write-through policy is to scan the cache at regular intervals and to flush blocks that have been modified since the most recent scan  just as unix scans 17.3 713 nfs server network workstation figure 17.1 cachefs and its use of caching  its local cache sprite uses this policy with a 30-second interval nfs uses the policy for file data  but once a write is issued to the server durilcg a cache flush  the write must reach the server 's disk before it is considered complete  nfs treats meta data  directory data and file-attribute data  differently any metadata changes are issued synchronously to the server thus  file-structure loss and directory-structure corruption are avoided when a client or the server crashes  for nfs with cachefs  writes are also written to the local disk cache area when they are written to the server  to keep all copies consistent thus  nfs with cachefs improves performance over standard nfs on a read request with a cachefs cache hit but decreases performance for read or write requests with a cache miss as with all caches  it is vital to have a high cache hit rate to gain performance figure 17.1 shows how cachefs uses write-through and write-back caching  yet another variation on delayed write is to write data back to the server when the file is closed this is used in afs in the case of files that are open for short periods or are modified rarely  this policy does not significantly reduce network traffic in addition  the write-on-close policy requires the closing process to delay while the file is written through  which reduces the performance advantages of delayed writes for files that are open for long periods and are modified frequently  however  the performance advantages of this policy over delayed write with more frequent flushing are apparent  17.3.4 consistency a client machine is faced with the problem of deciding whether a locally cached copy of the data is consistent with the master copy  and hence can be used   if 714 chapter 17 the client machine determines that its cached data are out of date  accesses can no longer be served by those cached data an up-to-date copy of the data needs to be cached there are two approaches to verifying the validity of cached data  client-initiated approach the client initiates a validity check  in which it contacts the server and checks whether the local data are consistent with the master copy the frequency of the validity checking is the crux of this approach and determines the resulting consistency semantics it can range from a check before every access to a check only on first access to a file  on file open  basically   every access coupled with a validity check is delayed  compared with an access served immediately by the cache  alternatively  checks can be initiated at fixed time intervals depending on its frequency  the validity check can load both the network and the server  server-initiated approach the server records  for each client  the files  or parts of files  that it caches when the server detects a potential inconsistency  it must react a potential for inconsistency occurs when two different clients in conflicting modes cache a file if unix semantics  section 10.5.3  is implemented  we can resolve the potential inconsistency by having the server play an active role the server must be notified whenever a file is opened  and the intended mode  read or write  must be indicated for every open the server can then act when it detects that a file has been opened simultaneously in conflicting modes by disabling caching for that particular file actually  disabling caching results in switching to a remote-service mode of operation  17.3.5 a comparison of caching and remote service essentially  the choice between caching and remote service trades off potentially increased performance with decreased simplicity we evaluate this tradeoff by listing the advantages and disadvantages of the two methods  when caching is used  the local cache can handle a substantial number of the remote accesses efficiently capitalizing on locality in file-access patterns makes caching even more attractive thus  most of the remote accesses will be served as fast as will local ones moreover  servers are contacted only occasionally  rather than for each access consequently  server load and network traffic are reduced  and the potential for scalability is enhanced by contrast  when the remote-service method is used  every remote access is handled across the network the penalty in network traffic  server load  and performance is obvious  total network overhead is lower for transmitting big chunks of data  as is done in caching  than for transmitting series of responses to specific requests  as in the remote-service method   furthermore  disk-access routines on the server may be better optimized if it is known that requests will always be for large  contiguous segments of data rather than for random disk blocks  the cache-consistency problem is the major drawback of caching when access patterns exhibit infrequent writes  caching is superior however  17.4 17.4 715 when writes are frequent  the mechanisms employed to overcome the consistency problem incur substantial overhead in terms of performance  network traffic  and server load  so that caching will confer a benefit  execution should be carried out on machines that have either local disks or large main memories remote access on diskless  small-memory-capacity machines should be done through the remote-service method  in caching  since data are transferred en masse between the server and the client  rather than in response to the specific needs of a file operation  the lower-level intermachine interface is different from the upper-level user interface the remote-service paradigm  in contrast  is just an extension of the local file-system interface across the network thus  the intermachine interface mirrors the user interface  there are two approaches for storing server-side information when a client accesses remote files  either the server tracks each file being accessed by each client  or it simply provides blocks as they are requested by the client without knowledge of how those blocks are used in the former case  the service provided is stateful ; in the latter case  it is stateless  the typical scenario involving a is as follows  a client must perform an open   operation on a file before accessing that file the server fetches information about the file from its disk  stores it in its memory  and gives the client a connection identifier that is unique to the client and the open file   in unix terms  the server fetches the inode and gives the client a file descriptor  which serves as an index to an in-core table of inodes  this identifier is used for subsequent accesses ru1.til the session ends a stateful service is characterized as a connection between the client and the server during a session either on closing the file or through a garbage-collection mechanism  the server must reclaim the main-memory space used by clients that are no longer active the key point regarding fault tolerance in a stateful service approach is that the server keeps main-memory information about its clients afs is a stateful file service  a avoids state information by making each request self-contained that is  each request identifies the file and the position in the file  for read and write accesses  in full the server does not need to keep a table of open files in main memory  although it usually does so for efficiency reasons moreover  there is no need to establish and terminate a com1.ection through open   and close   operations they are totally redundant since each file operation stands on its own and is not considered part of a session a client process would open a file  and that open would not result in the sending of a remote message reads and writes would take place as remote messages  or cache lookups   the final close by the client would again result in only a local operation nfs is a stateless file service  the advantage of a stateful over a stateless service is increased performance  file information is cached in main memory and can be accessed easily via the connection identifier  thereby saving disk accesses in addition  a stateful 716 chapter 17 5 server knows whether a file is open for sequential access and can therefore read ahead the next blocks stateless servers cmmot do so  since they have no knowledge of the purpose of the client 's requests  the distinction between stateful and stateless service becomes more evident when we consider the effects of a crash that occurs during a service activity a stateful server loses all its volatile state in a crash ensuring the graceful recovery of such a server involves restoring this state  usually by a recovery protocol based on a dialog with clients less graceful recovery requires that the operations that were underway when the crash occurred be aborted  a different problem is caused by client failures the server needs to become aware of such failures so that it can reclaim space allocated to record the state of crashed client processes this phenomenon is sometimes referred to as a stateless computer server avoids these problems  silcce a newly reincarnated server can respond to a self-contained request without any difficulty  therefore  the effects of server failures and recovery are almost unnoticeable  there is no difference between a slow server and a recovering server from a client 's point of view the client keeps retransmitting its request if it receives no response  the penalty for using the robust stateless service is longer request messages and slower processing of requests  since there is no in-core i,_'lformation to speed the processing in addition  stateless service imposes additional constraints on the design of the dfs first  since each request identifies the target file  a uniform  system-wide  low-level naming scheme should be used translating remote to local names for each request would cause even slower processing of the requests second  since clients retransmit requests for file operations  these operations must be idempotent ; that is  each operation must have the same effect and return the same output if executed several times consecutively  self-contained read and write accesses are idempotent  as long as they use an absolute byte count to indicate the position withilc the file they access and do not rely on an incremental offset  as is done in unix read   and write   system calls   however  we must be careful when implementing destructive operations  such as deleting a file  to make them idempotent  too  in some environments  a stateful service is a necessity if the server employs the server-initiated method for cache validation  it camcot provide stateless service  since it maintains a record of which files are cached by which clients  the way unix uses file descriptors and implicit offsets is inherently stateful  servers must mailctain tables to map the file descriptors to inodes and must store the current offset within a file this requirement is why nfs  which employs a stateless service  does not use file descriptors and does include an explicit offset in every access  replication of files on different machines in a distributed file system is a useful redundancy for improving availability multimachine replication can benefit performance too  selecting a nearby replica to serve an access request results in shorter service time  the basic requirement of a replication scheme is that different replicas of the same file reside on failure-independent machines that is  the availability 17.5 717 nfsv4 ourcov,era.geofnfs thus .far has 0p1y considered version 3  orv3  nf  the mostrecentnps standard is version 4  v 4   and it differs fundanrentaljy from pr ~ vious versimi.s jhe most significant ch9nge is that the protocol is now stqteful,meaping tha  tthesetv ~ er maintains the.state ofthe client session from the  time the r ~ j1lote file is 0pt   ned untij itis closed th.t1s  thenfs protocol now provides open   ar ; td c1o $ e   operations ; previous v  ersions of nfs  v \ thich are stateless  .proyide .np such operations furthen   ore  preytous \ cersions specify s ~ parate protocols or j  lounting remote fil ~ systews and for lockii1g remote files  v4 provides ali of these features under l phlgle prqtocol in patticular ; the 1nrnmt protocol was elimin ; 1.ted  allowing 1 \  fs to work with network fitewalls  the nj.ount protocol was a notorimis security hole in nps implem,entations ~         .additionally  v4 has enhan edthe ability of.dients jo cache file data local  y  this feature ; i ~ prov ~ s tne performapc ~ of the disttil  mt  3d file system  a.s plients are able to reso ~ ve more file a9cesses from the loc ~ l c ~ che rgther thanh ; lvingto gpjhroughthe s ~ !  ver '   4 ali   ws diel   tsjo req  c ! estfile jocks from s ~ rvers as we 'll .if the  senr ~ r grqrct ~  the request the client maintains the loci ,tmtil it is released or its lea.st = expires  clier ~ ts ar,e als   permitted to r ~ new ex ~ stil'tg least = s  traditiora11y1 unix ~ ba ~ ~ d  systems .provide advisory jile locking  whereas windows operatirg systen1 ~ use mandat   rylockil1g to allov \ t l ' \  ps towm  wellwithnon-unixsyste1fts  v4t1qw.p.rovides mandatory locking as vvell the new lockinga11d caching mec ~ anisms are based on the concept   f d ~ legahon  whe ~ eby the server delegates responsibilities for a file 's lockand contents to .the client thatrequested tnel   ckcrhat delegated client maintains in cache the .current version of .the file  and  other clients can  ask that deleg21ted client for lock access  a1i.d filt = confentsuntifthe del  egated client reli11quishesth ~ lock andde ~ egation               finally  whereas.preyiousversionb   f npsarebasedon the udj network ptqtocol  \  4 is based on .tcp,whioh allows itto betteraclj \ lstto varying traffic loads on thel  etwork  peleg2lting these responsibilities to cliel ! cts reduces the foado11the s.eryet and.i ~ proves.cache.coherency  of one replica is not affected by the availability of the rest of the replicas  this obvious requirement implies that replication management is inherently a location-opaque activity provisions for placing a replica on a particular machine must be available  it is desirable to hide the details of replication from users mapping a replicated file name to a particular replica is the task of the naming scheme  the existence of replicas should be invisible to higher levels at lower levels  however  the replicas must be distinguished from one another by different lower-level names another transparency requirement is providing replication control at higher levels replication control includes determination of the degree of replication and of the placement of replicas under certain circumstances  we may want to expose these details to users locus  for instance  provides users and system administrators with mechanisms to control the replication scheme  718 chapter 17 17.6 the main problem ~ associated with replicas is updating from a user 's point of view  replicas of a file denote the same logical entity  and thus an update to any replica must be reflected on all other replicas more precisely  the relevant consistency sen1antics must be preserved when accesses to replicas are viewed as virtual accesses to the replicas ' logical files if consistency is not of primary incportance  it can be sacrificed for availability and performance in this fundamental tradeoff in the area of fault tolerance  the choice is between preserving consistency at all costs  thereby creating a potential for indefinite blocking  and sacrificing consistency under some  we hope  rare  circumstances for the sake of guaranteed progress locus  for example  employs replication extensively and sacrifices consistency in the case of network partition for the sake of availability of files for read and write accesses  ibis uses a variation of the primary-copy approach the domain of the name mapping is a pair primary-replica-identifier  local-replica-identifier  if no local replica exists  a special value is used thus  the mapping is relative to a machine if the local replica is the primary one  the pair contains two identical identifiers ibis supports demand replication  an automatic replication-control policy similar to whole-file caching under demand replication  reading of a nonlocal replica causes it to be cached locally  thereby generating a new nonprimary replica updates are performed only on the primary copy and cause all other replicas to be invalidated through the sending of appropriate messages atomic and serialized invalidation of all nonprimary replicas is not guaranteed hence  a stale replica may be considered valid to satisfy remote write accesses  we migrate the primary copy to the requesting machine  andrew is a distributed computing environment designed and implemented at carnegie mellon university the andrew file system  afs  constitutes the underlying information-sharing mechanism among clients of the environment  the transarc corporation took over development of afs and then was purchased by ibm ibm has since produced several commercial implementations of afs afs was subsequently chosen as the dfs for an industry coalition ; the result was part of the distributed computing environment  dce  from the osf organization  in 2000  ibm 's transarc lab announced that afs would be an open-source product  termed openafs  available under the ibm public license  and transarc dfs was canceled as a commercial product openafs is available under most commercial versions of unix as well as linux and microsoft windows systems  many unix vendors  as well as microsoft  support the dce system and its dfs  which is based on afs  and work is ongoing to make dce a cross-platform  universally accepted dfs as afs and transarc dfs are very similar ~ we describe afs throughout this section  unless transarc dfs is named specifically  afs seeks to solve many of the problems of the simpler dfss  such as nfs  and is arguably the most feature-rich nonexperimental dfs it features a uniform name space  location-independent file sharing  client-side caching with cache consistency  and secure authentication via kerberos it also includes server-side caching in the form of replicas  with high availability through automatic switchover to a replica if the source server is unavailable one of the 17.6 719 most formidable attributes of afs is scalability  the andrew system is targeted to span over 5,000 workstations between afs and transarc dfs  there are hundreds of implementations worldwide  17.6.1 overview afs distinguishes between client machines  sometimes referred to as workstations  and dedicated server machines servers and clients originally ran only 4.2 bsd unix  but afs has been ported to many operating systems the clients and servers are interconnected by a network of lans or wans  clients are presented with a partitioned space of file names  a and a dedicated servers  collectively called vice after the name of the software they run  present the shared name space to the clhe local name space is the root file system of a workstation  from which the shared name space descends workstations run the virtue protocol to communicate with vice  and each is required to have a local disk where it stores its local name space servers collectively are responsible for the storage and management of the shared name space the local name space is small  is distinct for each workstation  and contains system programs essential for autonomous operation and better performance also local are temporary files and files that the workstation owner  for privacy reasons  explicitly wants to store locally  viewed at a finer granularity  clients and servers are structured in clusters interconnected by a wan each cluster consists of a collection of workstations on a lan and a representative of vice called a and each cluster is com1.ected to the wan by a router the decomposition into clusters is done primarily to address the problem of scale for optimal performance  workstations should use the server on their own cluster most of the time  thereby making cross-cluster file references relatively infrequent  the file-system architecture is also based on considerations of scale the basic heuristic is to offload work from the servers to the clients  in light of experience indicating that server cpu speed is the system 's bottleneck following this heuristic  the key mechanism for remote file operations is to cache files in large chunks  64 kb   this feature reduces file-open latency and allows reads and writes to be directed to the cached copy without frequently involving the servers  briefly  here are a few additional issues in the design of afs  client mobility clients are able to access any file in the shared name space from any workstation a client may notice some initial performance degradation due to the caching of files when accessil g files a workstation other than the usual one  security the vice interface is considered the boundary of trustworthiness  because no client programs are executed on vice machines authentication and secure-transmission functions are provided as part of a connectionbased communication package based on the rpc paradigm after mutual authentication  a vice server and a client communicate via encrypted messages encryption is performed by hardware devices or  more slowly  720 chapter 17 in software information about clients and groups is stored in a protection database replicated at each server  protection afs provides for protecting directories and the regular unlxbits for file protection the access list ncay contain information about those users allowed to access a directory  as well as information about those users not allowed to access it thus  it is simple to specify that everyone except  say  jim can access a directory afs supports the access types read  write  lookup  insert  administer  lock  and delete  heterogeneity defining a clear interface to vice is a key for integration of diverse workstation hardware and operating systems so that heterogeneity is facilitated  some files in the local /bin directory are symbolic links pointing to machine-specific executable files residing in vice  17.6.2 the shared name space afs 's shared name space is made up of component units called the volumes are unusually small component units typically  they are associated with the files of a single client few volumes reside within a single disk partition  and they may grow  up to a quota  and shrink in size conceptually  volumes are glued together by a mechanism similar to the unix m01mt mechanism however  the granularity difference is significant  since in unix only an entire disk partition  containing a file system  can be mounted volumes are a key administrative unit and play a vital role in identifying and locating an individual file  a vice file or directory is identified by a low-level identifier called a fid  each afs directory entry maps a path-name component to a fid a fid is 96 bits long and has three equal-length components  a volume number  a vnode number  and a uniquifier the vnode number is used as an index into an array containing the inodes of files in a single volume the allows reuse of vnode numbers  thereby keeping certain data structures compact fids are location transparent ; therefore  file movements from server to server do not invalidate cached directory contents  location information is kept on a volume basis in a replicated on each server a client can identify the location of every volume in the system by querying this database the aggregation of files into volumes makes it possible to keep the location database at a manageable size  to balance the available disk space and utilization of servers  volumes need to be migrated among disk partitions and servers when a volume is shipped to its new location  its original server is left with temporary forwarding information  so that the location database need not be updated synchronously  while the volume is being transferred  the original server can still handle updates  which are shipped later to the new server at some point  the volume is briefly disabled so that the recent modifications can be processed ; then  the new volume becomes available again at the new site the volume-movement operation is atomic ; if either server crashes  the operation is aborted  read-only replication at the granularity of an entire volume is supported for system-executable files and for seldom-updated files in the upper levels of the vice name space the volume-location database specifies the server 17.6 721 contammg the only read-write copy of a volume and a list of read-only replication sites  17.6.3 file operations and consistency semantics the fundamental architectural principle in afs is the caching of entire files from servers accordingly  a client workstation interacts with vice servers only during opening and closing of files  and even this interaction is not always necessary reading and writing files do not cause remote interaction  in contrast to the remote-service n lethod   this key distinction has far-reaching ramifications for performance  as well as for semantics of file operations  the operating system on each workstation intercepts file-system calls and forwards them to a client-level process on that workstation this process  called venus  caches files from vice when they are opened and stores modified copies of files back on the servers from which they came when they are closed venus may contact vice only when a file is opened or closed ; reading and writing of individual bytes of a file are performed directly on the cached copy and bypass venus as a result  writes at some sites are not visible immediately at other sites  caching is further exploited for future opens of the cached file venus assumes that cached entries  files or directories  are valid unless notified otherwise therefore  venus does not need to contact vice on a file open to validate the cached copy the mechanism to support this policy  called callback  dramatically reduces the number of cache-validation requests received by servers it works as follows when a client caches a file or a directory  the server updates its state information to record this caching we say that the client has a callback on that file the server notifies the client before allowing another client to modify the file in such a case  we say that the server removes the callback on the file for the former client a client can use a cached file for open purposes only when the file has a callback if a client closes a file after modifying it  all other clients caching this file lose their callbacks therefore  when these clients open the file later  they have to get the new version from the server  readin.g and writing bytes of a file are done directly by the kernel without venus 's intervention on the cached copy venus regains control when the file is closed if the file has been modified locally  it updates the file on the appropriate server thus  the only occasions on which venus contacts vice servers are on opens of files that either are not in the cache or have had their callback revoked and on closes of locally modified files  basically  afs implements session semantics the only exceptions are file operations other than the primitive read and write  such as protection changes at the directory level   which are visible everywhere on the network immediately after the operation completes  in spite of the callback mechanism  a small amount of cached validation traffic is still present  usually to replace callbacks lost because of machine or network failures when a workstation is rebooted  venus considers all cached files and directories suspect  and it generates a cache-validation request for the first use of each such entry  the callback mechanism forces each server to maintain callback information and each client to maintain validity information if the amount of callback 