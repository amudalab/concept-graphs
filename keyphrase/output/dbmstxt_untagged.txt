1.2 purpose of database systems 3 about you may be retrieved from a database to select which advertisements you should see furthermore  data about your web accesses may be stored in a database thus  although user interfaces hide details of access to a database  and most people are not even aware they are dealing with a database  accessing databases forms an essential part of almost everyone ? s life today the importance of database systems can be judged in another way ? today  database system vendors like oracle are among the largest software companies in the world  and database systems form an important part of the product line of microsoft and ibm 1.2 purpose of database systems database systems arose in response to early methods of computerized management of commercial data as an example of such methods  typical of the 1960s  consider part of a university organization that  among other data  keeps information about all instructors  students  departments  and course offerings one way to keep the information on a computer is to store it in operating system files to allow users to manipulate the information  the system has a number of application programs that manipulate the files  including programs to  ? add new students  instructors  and courses ? register students for courses and generate class rosters ? assign grades to students  compute grade point averages  gpa   and generate transcripts system programmers wrote these application programs to meet the needs of the university new application programs are added to the system as the need arises for example  suppose that a university decides to create a new major  say  computer science  .as a result  the university creates a newdepartment and creates new permanent files  or adds information to existing files  to record information about all the instructors in the department  students in that major  course offerings  degree requirements  etc the university may have to write new application programs to deal with rules specific to the new major new application programs may also have to be written to handle new rules in the university thus  as time goes by  the system acquires more files and more application programs this typical file-processing system is supported by a conventional operating system the system stores permanent records in various files  and it needs different application programs to extract records from  and add records to  the appropriate files before database management systems  dbmss  were introduced  organizations usually stored information in such systems keeping organizational information in a file-processing system has a number of major disadvantages  thesumit67.blogspot.com 4 chapter 1 introduction ? data redundancy and inconsistency since different programmers create the files and application programs over a long period  the various files are likely to have different structures and the programs may bewritten in several programming languages moreover  the same information may be duplicated in several places  files   for example  if a student has a double major  say  music and mathematics  the address and telephone number of that student may appear in a file that consists of student records of students in the music department and in a file that consists of student records of students in the mathematics department this redundancy leads to higher storage and access cost in addition  it may lead to data inconsistency ; that is  the various copies of the same datamayno longer agree for example  a changed student address may be reflected in the music department records but not elsewhere in the system ? difficulty in accessing data suppose that one of the university clerks needs to find out the names of all students who live within a particular postal-code area the clerk asks the data-processing department to generate such a list because the designers of the original system did not anticipate this request  there is no application program on hand to meet it there is  however  an application program to generate the list of all students the university clerk has now two choices  either obtain the list of all students and extract the needed information manually or ask a programmer to write the necessary application program both alternatives are obviously unsatisfactory suppose that such a program is written  and that  several days later  the same clerk needs to trim that list to include only those students who have taken at least 60 credit hours as expected  a program to generate such a list does not exist again  the clerk has the preceding two options  neither of which is satisfactory the point here is that conventional file-processing environments do not allow needed data to be retrieved in a convenient and efficientmanner more responsive data-retrieval systems are required for general use ? data isolation because data are scattered in various files  and files may be in different formats  writing new application programs to retrieve the appropriate data is difficult ? integrity problems the data values stored in the database must satisfy certain types of consistency constraints suppose the university maintains an account for each department  and records the balance amount in each account suppose also that the university requires that the account balance of a department may never fall below zero developers enforce these constraints in the system by adding appropriate code in the various application programs however  when new constraints are added  it is difficult to change the programs to enforce them the problem is compoundedwhen constraints involve several data items from different files ? atomicity problems a computer system  like any other device  is subject to failure in many applications  it is crucial that  if a failure occurs  the data thesumit67.blogspot.com 1.2 purpose of database systems 5 be restored to the consistent state that existed prior to the failure consider a program to transfer $ 500 from the account balance of department a to the account balance of department b if a system failure occurs during the execution of the program  it is possible that the $ 500 was removed from the balance of department a butwas not credited to the balance of department b  resulting in an inconsistent database state clearly  it is essential to database consistency that either both the credit and debit occur  or that neither occur that is  the funds transfer must be atomic ? it must happen in its entirety or not at all it is difficult to ensure atomicity in a conventional file-processing system ? concurrent-access anomalies for the sake of overall performance of the system and faster response  many systems allow multiple users to update the data simultaneously indeed  today  the largest internet retailers may have millions of accesses per day to their data by shoppers in such an environment  interaction of concurrent updates is possible and may result in inconsistent data consider department a  with an account balance of $ 10,000 if two department clerks debit the account balance  by say $ 500 and $ 100  respectively  of department a at almost exactly the same time  the result of the concurrent executions may leave the budget in an incorrect  or inconsistent  state suppose that the programs executing on behalf of each withdrawal read the old balance  reduce that value by the amount beingwithdrawn  and write the result back if the two programs run concurrently  they may both read the value $ 10,000  and write back $ 9500 and $ 9900  respectively depending on which one writes the value last  the account balance of department a may contain either $ 9500 or $ 9900  rather than the correct value of $ 9400 to guard against this possibility  the system must maintain some form of supervision but supervision is difficult to provide because data may be accessed by many different application programs that have not been coordinated previously as another example  suppose a registration program maintains a count of students registered for a course  in order to enforce limits on the number of students registered.when a student registers  the program reads the current count for the courses  verifies that the count is not already at the limit  adds one to the count  and stores the count back in the database suppose two students register concurrently  with the count at  say  39 the two program executions may both read the value 39  and both would then write back 40  leading to an incorrect increase of only 1  even though two students successfully registered for the course and the count should be 41 furthermore  suppose the course registration limit was 40 ; in the above case both students would be able to register  leading to a violation of the limit of 40 students ? security problems not every user of the database system should be able to access all the data for example  in a university  payroll personnel need to see only that part of the database that has financial information they do not need access to information about academic records but  since application programs are added to the file-processing system in an ad hoc manner  enforcing such security constraints is difficult thesumit67.blogspot.com 6 chapter 1 introduction these difficulties  among others  prompted the development of database systems in what follows  we shall see the concepts and algorithms that enable database systems to solve the problems with file-processing systems in most of this book  we use a university organization as a running example of a typical data-processing application 1.3 view of data a database system is a collection of interrelated data and a set of programs that allow users to access and modify these data a major purpose of a database system is to provide users with an abstract view of the data that is  the system hides certain details of how the data are stored and maintained 1.3.1 data abstraction for the system to be usable  it must retrieve data efficiently the need for efficiency has led designers to use complex data structures to represent data in the database since many database-system users are not computer trained  developers hide the complexity from users through several levels of abstraction  to simplify users ? interactions with the system  ? physical level the lowest level of abstraction describes how the data are actually stored the physical level describes complex low-level data structures in detail ? logical level the next-higher level of abstraction describes what data are stored in the database  and what relationships exist among those data the logical level thus describes the entire database in terms of a small number of relatively simple structures although implementation of the simple structures at the logical level may involve complex physical-level structures  the user of the logical level does not need to be aware of this complexity this is referred to as physical data independence database administrators  who must decide what information to keep in the database  use the logical level of abstraction ? view level the highest level of abstraction describes only part of the entire database even though the logical level uses simpler structures  complexity remains because of the variety of information stored in a large database many users of the database system do not need all this information ; instead  they need to access only a part of the database the view level of abstraction exists to simplify their interaction with the system the system may provide many views for the same database figure 1.1 shows the relationship among the three levels of abstraction an analogy to the concept of data types in programming languages may clarify the distinction among levels of abstraction many high-level programming thesumit67.blogspot.com 1.3 view of data 7 view 1 view 2 logical level physical level ? view n view level figure 1.1 the three levels of data abstraction languages support the notion of a structured type for example  we may describe a record as follows  1 type instructor = record id  char  5  ; name  char  20  ; dept name  char  20  ; salary  numeric  8,2  ; end ; this code defines a new record type called instructor with four fields each field has a name and a type associated with it a university organization may have several such record types  including ? department  with fields dept name  building  and budget ? course  with fields course id  title  dept name  and credits ? student  with fields id  name  dept name  and tot cred at the physical level  an instructor  department  or student record can be described as a block of consecutive storage locations the compiler hides this level of detail from programmers similarly  the database system hides many of the lowest-level storage details from database programmers database administrators  on the other hand  may be aware of certain details of the physical organization of the data 1the actual type declaration depends on the language being used c and c + + use struct declarations java does not have such a declaration  but a simple class can be defined to the same effect thesumit67.blogspot.com 8 chapter 1 introduction at the logical level  each such record is described by a type definition  as in the previous code segment  and the interrelationship of these record types is defined as well programmers using a programming language work at this level of abstraction similarly  database administrators usually work at this level of abstraction finally  at the view level  computer users see a set of application programs that hide details of the data types at the view level  several views of the database are defined  and a database user sees some or all of these views in addition to hiding details of the logical level of the database  the views also provide a security mechanism to prevent users from accessing certain parts of the database for example  clerks in the university registrar office can see only that part of the database that has information about students ; they can not access information about salaries of instructors 1.3.2 instances and schemas databases change over time as information is inserted and deleted the collection of information stored in the database at a particular moment is called an instance of the database the overall design of the database is called the database schema schemas are changed infrequently  if at all the concept of database schemas and instances can be understood by analogy to a program written in a programming language.adatabase schema corresponds to the variable declarations  along with associated type definitions  in a program each variable has a particular value at a given instant the values of the variables in a program at a point in time correspond to an instance of a database schema database systems have several schemas  partitioned according to the levels of abstraction the physical schema describes the database design at the physical level  while the logical schema describes the database design at the logical level a database may also have several schemas at the view level  sometimes called subschemas  that describe different views of the database of these  the logical schema is by far the most important  in terms of its effect on application programs  since programmers construct applications by using the logical schema the physical schema is hidden beneath the logical schema  and can usually be changed easily without affecting application programs application programs are said to exhibit physical data independence if they do not depend on the physical schema  and thus need not be rewritten if the physical schema changes we study languages for describing schemas after introducing the notion of data models in the next section 1.3.3 data models underlying the structure of a database is the data model  a collection of conceptual tools for describing data  data relationships  data semantics  and consistency constraints a data model provides a way to describe the design of a database at the physical  logical  and view levels thesumit67.blogspot.com 1.4 database languages 9 there are a number of different data models that we shall cover in the text the data models can be classified into four different categories  ? relational model the relational model uses a collection of tables to represent both data and the relationships among those data each table has multiple columns  and each column has a unique name tables are also known as relations the relational model is an example of a record-based model record-based models are so named because the database is structured in fixed-format records of several types each table contains records of a particular type each record type defines a fixed number of fields  or attributes the columns of the table correspond to the attributes of the record type the relational data model is the most widely used data model  and a vast majority of current database systems are based on the relational model chapters 2 through 8 cover the relational model in detail ? entity-relationship model the entity-relationship  e-r  data model uses a collection of basic objects  called entities  andrelationships among these objects an entity is a ? thing ? or ? object ? in the real world that is distinguishable from other objects the entity-relationship model is widely used in database design  and chapter 7 explores it in detail ? object-based data model.object-oriented programming  especially in java  c + +  or c #  has become the dominant software-development methodology this led to the development of an object-oriented data model that can be seen as extending the e-r model with notions of encapsulation  methods  functions   and object identity the object-relational data model combines features of the object-oriented data model and relational data model chapter 22 examines the object-relational data model ? semistructured data model the semistructured data model permits the specification of data where individual data items of the same type may have different sets of attributes this is in contrast to the data models mentioned earlier  where every data item of a particular type must have the same set of attributes the extensible markup language  xml  is widely used to represent semistructured data chapter 23 covers it historically  the network data model and the hierarchical data model preceded the relational data model these modelswere tied closely to the underlying implementation  and complicated the task of modeling data as a result they are used little now  except in old database code that is still in service in some places they are outlined online in appendices d and e for interested readers 1.4 database languages a database system provides a data-definition language to specify the database schema and a data-manipulation language to express database queries and upthesumit67 blogspot.com 10 chapter 1 introduction dates in practice  the data-definition and data-manipulation languages are not two separate languages ; instead they simply form parts of a single database language  such as the widely used sql language 1.4.1 data-manipulation language a data-manipulation language  dml  is a language that enables users to access or manipulate data as organized by the appropriate data model the types of access are  ? retrieval of information stored in the database ? insertion of new information into the database ? deletion of information from the database ? modification of information stored in the database there are basically two types  ? procedural dmls require a user to specify what data are needed and how to get those data ? declarative dmls  also referred to as nonprocedural dmls  require a user to specify what data are needed without specifying how to get those data declarative dmls are usually easier to learn and use than are procedural dmls however  since a user does not have to specify how to get the data  the database system has to figure out an efficient means of accessing data a query is a statement requesting the retrieval of information the portion of a dml that involves information retrieval is called a query language although technically incorrect  it is common practice to use the terms query language and data-manipulation language synonymously there are a number of database query languages in use  either commercially or experimentally.we study the most widely used query language  sql  inchapters 3  4  and 5 we also study some other query languages in chapter 6 the levels of abstraction that we discussed in section 1.3 apply not only to defining or structuring data  but also to manipulating data at the physical level  we must define algorithms that allow efficient access to data at higher levels of abstraction  we emphasize ease of use the goal is to allow humans to interact efficiently with the system the query processor component of the database system  which we study in chapters 12 and 13  translates dml queries into sequences of actions at the physical level of the database system 1.4.2 data-definition language we specify a database schema by a set of definitions expressed by a special language called a data-definition language  ddl   the ddl is also used to specify additional properties of the data thesumit67.blogspot.com 1.4 database languages 11 we specify the storage structure and access methods used by the database system by a set of statements in a special type of ddl called a data storage and definition language these statements define the implementation details of the database schemas  which are usually hidden from the users the data values stored in the database must satisfy certain consistency constraints for example  suppose the university requires that the account balance of a department must never be negative the ddl provides facilities to specify such constraints the database system checks these constraints every time the database is updated in general  a constraint can be an arbitrary predicate pertaining to the database.however  arbitrary predicates may be costly to test thus  database systems implement integrity constraints that can be testedwith minimal overhead  ? domain constraints a domain of possible values must be associated with every attribute  for example  integer types  character types  date/time types   declaring an attribute to be of a particular domain acts as a constraint on the values that it can take domain constraints are the most elementary form of integrity constraint they are tested easily by the system whenever a new data item is entered into the database ? referential integrity there are cases where we wish to ensure that a value that appears in one relation for a given set of attributes also appears in a certain set of attributes in another relation  referential integrity   for example  the department listed for each course must be one that actually exists more precisely  the dept name value in a course record must appear in the dept name attribute of some record of the department relation database modifications can cause violations of referential integrity.when a referential-integrity constraint is violated  the normal procedure is to reject the action that caused the violation ? assertions an assertion is any condition that the database must always satisfy domain constraints and referential-integrity constraints are special forms of assertions however  there are many constraints that we can not express by using only these special forms for example  ? every department must have at least five courses offered every semester ? must be expressed as an assertion when an assertion is created  the system tests it for validity if the assertion is valid  then any future modification to the database is allowed only if it does not cause that assertion to be violated ? authorization we may want to differentiate among the users as far as the type of access they are permitted on various data values in the database these differentiations are expressed in terms of authorization  the most common being  read authorization  which allows reading  but not modification  of data ; insert authorization  which allows insertion of new data  but not modification of existing data ; update authorization  which allows modification  but not deletion  of data ; and delete authorization  which allows deletion of data we may assign the user all  none  or a combination of these types of authorization thesumit67.blogspot.com 12 chapter 1 introduction the ddl  just like any other programming language  gets as input some instructions  statements  and generates some output the output of the ddl is placed in the data dictionary,which contains metadata ? that is  data about data the data dictionary is considered to be a special type of table that can only be accessed and updated by the database system itself  not a regular user   the database system consults the data dictionary before reading or modifying actual data 1.5 relational databases a relational database is based on the relational model and uses a collection of tables to represent both data and the relationships among those data it also includes a dml and ddl in chapter 2 we present a gentle introduction to the fundamentals of the relational model most commercial relational database systems employ the sql language  which we cover in great detail in chapters 3  4  and 5 in chapter 6 we discuss other influential languages 1.5.1 tables each table has multiple columns and each column has a unique name figure 1.2 presents a sample relational database comprising two tables  one shows details of university instructors and the other shows details of the various university departments the first table  the instructor table  shows  for example  that an instructor named einstein with id 22222 is a member of the physics department and has an annual salary of $ 95,000 the second table  department  shows  for example  that the biology department is located in the watson building and has a budget of $ 90,000 of course  a real-world university would have many more departments and instructors we use small tables in the text to illustrate concepts a larger example for the same schema is available online the relational model is an example of a record-based model record-based models are so named because the database is structured in fixed-format records of several types each table contains records of a particular type each record type defines a fixed number of fields  or attributes the columns of the table correspond to the attributes of the record type it is not hard to see how tables may be stored in files for instance  a special character  such as a comma  may be used to delimit the different attributes of a record  and another special character  such as a new-line character  may be used to delimit records the relational model hides such low-level implementation details from database developers and users we also note that it is possible to create schemas in the relational model that have problems such as unnecessarily duplicated information for example  suppose we store the department budget as an attribute of the instructor record then  whenever the value of a particular budget  say that one for the physics department  changes  that change must to be reflected in the records of all instructors thesumit67.blogspot.com 1.5 relational databases 13 id name dept name salary 22222 einstein physics 95000 12121 wu finance 90000 32343 el said history 60000 45565 katz comp sci 75000 98345 kim elec eng 80000 76766 crick biology 72000 10101 srinivasan comp sci 65000 58583 califieri history 62000 83821 brandt comp sci 92000 15151 mozart music 40000 33456 gold physics 87000 76543 singh finance 80000  a  the instructor table dept name building budget comp sci taylor 100000 biology watson 90000 elec eng taylor 85000 music packard 80000 finance painter 120000 history painter 50000 physics watson 70000  b  the department table figure 1.2 a sample relational database associated with the physics department in chapter 8  we shall study how to distinguish good schema designs from bad schema designs 1.5.2 data-manipulation language the sql query language is nonprocedural a query takes as input several tables  possibly only one  and always returns a single table here is an example of an sql query that finds the names of all instructors in the history department  select instructor.name from instructor where instructor.dept name = ? history ? ; the query specifies that those rows fromthe table instructor where the dept name is historymust be retrieved  and the name attribute of these rows must be displayed more specifically  the result of executing this query is a table with a single column thesumit67.blogspot.com 14 chapter 1 introduction labeled name  and a set of rows  each of which contains the name of an instructor whose dept name  is history if the query is run on the table in figure 1.2  the result will consist of two rows  one with the name el said and the other with the name califieri queries may involve information from more than one table for instance  the following query finds the instructor id and department name of all instructors associated with a department with budget of greater than $ 95,000 select instructor.id  department.dept name from instructor  department where instructor.dept name = department.dept name and department.budget > 95000 ; if the above query were run on the tables in figure 1.2  the system would find that there are two departments with budget of greater than $ 95,000 ? computer science and finance ; there are five instructors in these departments thus  the result will consist of a table with two columns  id  dept name  and five rows   12121  finance    45565  computer science    10101  computer science    83821  computer science   and  76543  finance   1.5.3 data-definition language sql provides a rich ddl that allows one to define tables  integrity constraints  assertions  etc for instance  the following sql ddl statement defines the department table  create table department  dept name char  20   building char  15   budget numeric  12,2   ; execution of the above ddl statement creates the department table with three columns  dept name  building  and budget  each of which has a specific data type associated with it.we discuss data types in more detail in chapter 3 in addition  the ddl statement updates the data dictionary  which contains metadata  see section 1.4.2   the schema of a table is an example of metadata 1.5.4 database access from application programs sql is not as powerful as a universal turing machine ; that is  there are some computations that are possible using a general-purpose programming language but are not possible using sql sql also does not support actions such as input from users  output to displays  or communication over the network such computations and actions must be written in a host language  such as c  c + +  or java  with embedded sql queries that access the data in the database application programs are programs that are used to interact with the database in this fashion thesumit67.blogspot.com 1.6 database design 15 examples in a university system are programs that allow students to register for courses  generate class rosters  calculate student gpa  generate payroll checks  etc to access the database  dml statements need to be executed from the host language there are two ways to do this  ? by providing an application program interface  set of procedures  that can be used to send dml and ddl statements to the database and retrieve the results the open database connectivity  odbc  standard for use with the c language is a commonly used application program interface standard the java database connectivity  jdbc  standard provides corresponding features to the java language ? by extending the host language syntax to embed dml calls within the host language program usually  a special character prefaces dml calls  and a preprocessor  called the dml precompiler  converts the dml statements to normal procedure calls in the host language 1.6 database design database systems are designed to manage large bodies of information these large bodies of information do not exist in isolation they are part of the operation of some enterprise whose end product may be information from the database or may be some device or service for which the database plays only a supporting role database design mainly involves the design of the database schema the design of a complete database application environment that meets the needs of the enterprise being modeled requires attention to a broader set of issues in this text  we focus initially on the writing of database queries and the design of database schemas chapter 9 discusses the overall process of application design 1.6.1 design process a high-level data model provides the database designerwith a conceptual framework in which to specify the data requirements of the database users  and how the database will be structured to fulfill these requirements the initial phase of database design  then  is to characterize fully the data needs of the prospective database users the database designer needs to interact extensively with domain experts and users to carry out this task the outcome of this phase is a specification of user requirements next  the designer chooses a data model  and by applying the concepts of the chosen datamodel  translates these requirements into a conceptual schema of the database the schema developed at this conceptual-design phase provides a detailed overview of the enterprise the designer reviews the schema to confirm that all data requirements are indeed satisfied and are not in conflict with one another the designer can also examine the design to remove any redundant thesumit67.blogspot.com 16 chapter 1 introduction features the focus at this point is on describing the data and their relationships  rather than on specifying physical storage details in terms of the relational model  the conceptual-design process involves decisions on what attributes we want to capture in the database and how to group these attributes to form the various tables the ? what ? part is basically a business decision  and we shall not discuss it further in this text the ? how ? part is mainly a computer-science problem there are principally two ways to tackle the problem the first one is to use the entity-relationship model  section 1.6.3  ; the other is to employ a set of algorithms  collectively known as normalization  that takes as input the set of all attributes and generates a set of tables  section 1.6.4   a fully developed conceptual schema indicates the functional requirements of the enterprise in a specification of functional requirements  users describe the kinds of operations  or transactions  that will be performed on the data example operations include modifying or updating data  searching for and retrieving specific data  and deleting data at this stage of conceptual design  the designer can review the schema to ensure it meets functional requirements the process of moving from an abstract data model to the implementation of the database proceeds in two final design phases in the logical-design phase  the designer maps the high-level conceptual schema onto the implementation data model of the database system that will be used the designer uses the resulting system-specific database schema in the subsequent physical-design phase  in which the physical features of the database are specified these features include the form of file organization and the internal storage structures ; they are discussed in chapter 10 1.6.2 database design for a university organization to illustrate the design process  let us examine how a database for a university could be designed the initial specification of user requirements may be based on interviews with the database users  and on the designer ? s own analysis of the organization the description that arises from this design phase serves as the basis for specifying the conceptual structure of the database here are the major characteristics of the university ? the university is organized into departments each department is identified by a unique name  dept name   is located in a particular building  and has a budget ? each department has a list of courses it offers each course has associated with it a course id  title  dept name  and credits  and may also have have associated prerequisites ? instructors are identified by their unique id each instructor has name  associated department  dept name   and salary ? students are identified by their unique id each student has a name  an associated major department  dept name   and tot cred  total credit hours the student earned thus far   thesumit67.blogspot.com 1.6 database design 17 ? the university maintains a list of classrooms  specifying the name of the building  room number  and room capacity ? the university maintains a list of all classes  sections  taught each section is identified by a course id  sec id  year  and semester  and has associated with it a semester  year  building  room number  and time slot id  the time slot when the class meets   ? the department has a list of teaching assignments specifying  for each instructor  the sections the instructor is teaching ? the university has a list of all student course registrations  specifying  for each student  the courses and the associated sections that the student has taken  registered for   a real university database would be much more complex than the preceding design.howeverwe use this simplified model to help you understand conceptual ideas without getting lost in details of a complex design 1.6.3 the entity-relationship model the entity-relationship  e-r  data model uses a collection of basic objects  called entities  and relationships among these objects an entity is a ? thing ? or ? object ? in the real world that is distinguishable from other objects for example  each person is an entity  and bank accounts can be considered as entities entities are described in a database by a set of attributes for example  the attributes dept name  building  and budget may describe one particular department in a university  and they form attributes of the department entity set similarly  attributes id  name  and salary may describe an instructor entity.2 the extra attribute id is used to identify an instructor uniquely  since it may be possible to have two instructors with the same name and the same salary   a unique instructor identifier must be assigned to each instructor in the united states  many organizations use the social-security number of a person  a unique number the u.s government assigns to every person in the united states  as a unique identifier arelationship is an association among several entities for example  a member relationship associates an instructor with her department the set of all entities of the same type and the set of all relationships of the same type are termed an entity set and relationship set  respectively the overall logical structure  schema  of a database can be expressed graphically by an entity-relationship  e-r  diagram there are several ways in which to draw these diagrams one of the most popular is to use the unified modeling language  uml   in the notation we use  which is based on uml  an e-r diagram is represented as follows  2the astute reader will notice that we dropped the attribute dept name from the set of attributes describing the instructor entity set ; this is not an error in chapter 7 we shall provide a detailed explanation of why this is the case thesumit67.blogspot.com 18 chapter 1 introduction instructor id name salary department dept_name building budget member figure 1.3 a sample e-r diagram ? entity sets are represented by a rectangular box with the entity set name in the header and the attributes listed below it ? relationship sets are represented by a diamond connecting a pair of related entity sets the name of the relationship is placed inside the diamond as an illustration  consider part of a university database consisting of instructors and the departments with which they are associated figure 1.3 shows the corresponding e-r diagram the e-r diagram indicates that there are two entity sets  instructor and department  with attributes as outlined earlier the diagram also shows a relationship member between instructor and department in addition to entities and relationships  the e-r model represents certain constraints to which the contents of a database must conform one important constraint is mapping cardinalities  which express the number of entities to which another entity can be associated via a relationship set for example  if each instructor must be associated with only a single department  the e-r model can express that constraint the entity-relationship model iswidely used in database design  and chapter 7 explores it in detail 1.6.4 normalization another method for designing a relational database is to use a process commonly known as normalization the goal is to generate a set of relation schemas that allows us to store information without unnecessary redundancy  yet also allows us to retrieve information easily the approach is to design schemas that are in an appropriate normal form to determine whether a relation schema is in one of the desirable normal forms  we need additional information about the real-world enterprise that we are modeling with the database the most common approach is to use functional dependencies  which we cover in section 8.4 to understand the need for normalization  let us look at what can go wrong in a bad database design among the undesirable properties that a bad design may have are  ? repetition of information ? inability to represent certain information thesumit67.blogspot.com 1.6 database design 19 id name salary dept name building budget 22222 einstein 95000 physics watson 70000 12121 wu 90000 finance painter 120000 32343 el said 60000 history painter 50000 45565 katz 75000 comp sci taylor 100000 98345 kim 80000 elec eng taylor 85000 76766 crick 72000 biology watson 90000 10101 srinivasan 65000 comp sci taylor 100000 58583 califieri 62000 history painter 50000 83821 brandt 92000 comp sci taylor 100000 15151 mozart 40000 music packard 80000 33456 gold 87000 physics watson 70000 76543 singh 80000 finance painter 120000 figure 1.4 the faculty table we shall discuss these problems with the help of a modified database design for our university example suppose that instead of having the two separate tables instructor and department  we have a single table  faculty  that combines the information from the two tables  as shown in figure 1.4   notice that there are two rows in faculty that contain repeated information about the history department  specifically  that department ? s building and budget the repetition of information in our alternative design is undesirable repeating information wastes space furthermore  it complicates updating the database suppose that we wish to change the budget amount of the history department from $ 50,000 to $ 46,800 this change must be reflected in the two rows ; contrast this with the original design  where this requires an update to only a single row thus  updates are more costly under the alternative design than under the original design when we perform the update in the alternative database  we must ensure that every tuple pertaining to the history department is updated  or else our database will show two different budget values for the history department now  let us shift our attention to the issue of ? inability to represent certain information ? suppose we are creating a new department in the university in the alternative design above,we can not represent directly the information concerning a department  dept name  building  budget  unless that department has at least one instructor at the university this is because rows in the faculty table require values for id  name  and salary this means that we can not record information about the newly created department until the first instructor is hired for the new department one solution to this problem is to introduce null values the null value indicates that the value does not exist  or is not known   an unknown value may be either missing  the value does exist  but we do not have that information  or not known  we do not know whether or not the value actually exists   as we thesumit67.blogspot.com 20 chapter 1 introduction shall see later  null values are difficult to handle  and it is preferable not to resort to them if we are not willing to deal with null values  then we can create a particular itemof department information only when the department has at least one instructor associated with the department furthermore  we would have to delete this information when the last instructor in the department departs clearly  this situation is undesirable  since  under our original database design  the department information would be available regardless of whether or not there is an instructor associated with the department  and without resorting to null values an extensive theory of normalization has been developed that helps formally define what database designs are undesirable  and how to obtain desirable designs chapter 8 covers relational-database design  including normalization 1.7 data storage and querying a database system is partitioned into modules that deal with each of the responsibilities of the overall system the functional components of a database system can be broadly divided into the storage manager and the query processor components the storage manager is important because databases typically require a large amount of storage space corporate databases range in size from hundreds of gigabytes to  for the largest databases  terabytes of data a gigabyte is approximately 1000 megabytes  actually 1024   1 billion bytes   and a terabyte is 1 million megabytes  1 trillion bytes   since the main memory of computers can not store this much information  the information is stored on disks data are moved between disk storage and main memory as needed since the movement of data to and from disk is slow relative to the speed of the central processing unit  it is imperative that the database system structure the data so as to minimize the need to move data between disk and main memory the query processor is important because it helps the database system to simplify and facilitate access to data the query processor allows database users to obtain good performance while being able to work at the view level and not be burdened with understanding the physical-level details of the implementation of the system it is the job of the database system to translate updates and queries written in a nonprocedural language  at the logical level  into an efficient sequence of operations at the physical level 1.7.1 storage manager the storage manager is the component of a database system that provides the interface between the low-level data stored in the database and the application programs and queries submitted to the system the storage manager is responsible for the interaction with the file manager the raw data are stored on the disk using the file system provided by the operating system the storage manager translates the various dml statements into low-level file-system commands thesumit67.blogspot.com 1.7 data storage and querying 21 thus  the storage manager is responsible for storing  retrieving  and updating data in the database the storage manager components include  ? authorization and integrity manager  which tests for the satisfaction of integrity constraints and checks the authority of users to access data ? transaction manager  which ensures that the database remains in a consistent  correct  state despite system failures  and that concurrent transaction executions proceed without conflicting ? file manager  which manages the allocation of space on disk storage and the data structures used to represent information stored on disk ? buffer manager,which is responsible for fetching data from disk storage into main memory  and deciding what data to cache in main memory the buffer manager is a critical part of the database system  since it enables the database to handle data sizes that are much larger than the size of main memory the storage manager implements several data structures as part of the physical system implementation  ? data files  which store the database itself ? data dictionary  which stores metadata about the structure of the database  in particular the schema of the database ? indices  which can provide fast access to data items like the index in this textbook  a database index provides pointers to those data items that hold a particular value for example  we could use an index to find the instructor record with a particular id  or all instructor records with a particular name hashing is an alternative to indexing that is faster in some but not all cases we discuss storage media  file structures  and buffer management in chapter 10 methods of accessing data efficiently via indexing or hashing are discussed in chapter 11 1.7.2 the query processor the query processor components include  ? ddl interpreter,which interprets ddl statements and records the definitions in the data dictionary ? dml compiler,which translates dml statements in a query language into an evaluation plan consisting of low-level instructions that the query evaluation engine understands thesumit67.blogspot.com 22 chapter 1 introduction a query can usually be translated into any of a number of alternative evaluation plans that all give the same result the dmlcompiler also performs query optimization ; that is  it picks the lowest cost evaluation plan from among the alternatives ? query evaluation engine  which executes low-level instructions generated by the dml compiler query evaluation is covered in chapter 12,while themethods bywhich the query optimizer chooses from among the possible evaluation strategies are discussed in chapter 13 1.8 transaction management often  several operations on the database form a single logical unit of work an example is a funds transfer  as in section 1.2  in which one department account  say a  is debited and another department account  say b  is credited clearly  it is essential that either both the credit and debit occur  or that neither occur that is  the funds transfer must happen in its entirety or not at all this all-or-none requirement is called atomicity in addition  it is essential that the execution of the funds transfer preserve the consistency of the database that is  the value of the sum of the balances of aand b must be preserved this correctness requirement is called consistency finally  after the successful execution of a funds transfer  the new values of the balances of accounts a and b must persist  despite the possibility of system failure this persistence requirement is called durability a transaction is a collection of operations that performs a single logical function in a database application each transaction is a unit of both atomicity and consistency thus  we require that transactions do not violate any databaseconsistency constraints that is  if the database was consistent when a transaction started  the database must be consistent when the transaction successfully terminates however  during the execution of a transaction  it may be necessary temporarily to allow inconsistency  since either the debit of a or the credit of b must be done before the other this temporary inconsistency  although necessary  may lead to difficulty if a failure occurs it is the programmer ? s responsibility to define properly the various transactions  so that each preserves the consistency of the database for example  the transaction to transfer funds from the account of department a to the account of department b could be defined to be composed of two separate programs  one that debits account a  and another that credits account b the execution of these two programs one after the otherwill indeed preserve consistency.however  each program by itself does not transform the database from a consistent state to a new consistent state thus  those programs are not transactions ensuring the atomicity and durability properties is the responsibility of the database system itself ? specifically  of the recovery manager in the absence of failures  all transactions complete successfully  and atomicity is achieved easily thesumit67.blogspot.com 1.9 database architecture 23 however  because of various types of failure  a transaction may not always complete its execution successfully if we are to ensure the atomicity property  a failed transaction must have no effect on the state of the database thus  the database must be restored to the state in which it was before the transaction in question started executing the database system must therefore perform failure recovery  that is  detect system failures and restore the database to the state that existed prior to the occurrence of the failure finally  when several transactions update the database concurrently  the consistency of data may no longer be preserved  even though each individual transaction is correct it is the responsibility of the concurrency-control manager to control the interaction among the concurrent transactions  to ensure the consistency of the database the transaction manager consists of the concurrency-control manager and the recovery manager the basic concepts of transaction processing are covered in chapter 14 the management of concurrent transactions is covered in chapter 15 chapter 16 covers failure recovery in detail the concept of a transaction has been applied broadly in database systems and applications while the initial use of transactions was in financial applications  the concept is now used in real-time applications in telecommunication  as well as in the management of long-duration activities such as product design or administrative workflows these broader applications of the transaction concept are discussed in chapter 26 1.9 database architecture we are now in a position to provide a single picture  figure 1.5  of the various components of a database system and the connections among them the architecture of a database system is greatly influenced by the underlying computer system on which the database system runs database systems can be centralized  or client-server  where one server machine executes work on behalf of multiple client machines database systems can also be designed to exploit parallel computer architectures distributed databases span multiple geographically separated machines in chapter 17 we cover the general structure of modern computer systems chapter 18 describes how various actions of a database  in particular query processing  can be implemented to exploit parallel processing chapter 19 presents a number of issues that arise in a distributed database  and describes how to deal with each issue the issues include how to store data  how to ensure atomicity of transactions that execute at multiple sites  how to perform concurrency control  and how to provide high availability in the presence of failures distributed query processing and directory systems are also described in this chapter most users of a database system today are not present at the site of the database system  but connect to it through a network we can therefore differentiate between client machines  on which remote database users work  and server machines  on which the database system runs thesumit67.blogspot.com 24 chapter 1 introduction database applications are usually partitioned into two or three parts  as in figure 1.6 in a two-tier architecture  the application resides at the client machine  where it invokes database system functionality at the server machine through naive users  tellers  agents  web users  query processor storage manager disk storage indices data statistical data data dictionary application programmers application interfaces application program object code compiler and linker buffer manager file manager authorization and integrity manager transaction manager dml compiler and organizer query evaluation engine dml queries ddl interpreter application programs query tools administration tools sophisticated users  analysts  database administrators use write use use figure 1.5 system structure thesumit67.blogspot.com 1.10 data mining and information retrieval 25 user application database system network  a  two-tier architecture client server user application client database system network application server  b  three-tier architecture figure 1.6 two-tier and three-tier architectures query language statements application program interface standards like odbc and jdbc are used for interaction between the client and the server in contrast  in a three-tier architecture  the client machine acts as merely a front end and does not contain any direct database calls instead  the client end communicates with an application server  usually through a forms interface the application server in turn communicates with a database system to access data the business logic of the application  which says what actions to carry out under what conditions  is embedded in the application server  instead of being distributed across multiple clients three-tier applications are more appropriate for large applications  and for applications that run on the worldwideweb 1.10 data mining and information retrieval the term data mining refers loosely to the process of semiautomatically analyzing large databases to find useful patterns like knowledge discovery in artificial intelligence  also called machine learning  or statistical analysis  data mining attempts to discover rules and patterns from data however  data mining differs from machine learning and statistics in that it deals with large volumes of data  stored primarily on disk that is  data mining deals with ? knowledge discovery in databases ? some types of knowledge discovered from a database can be represented by a set of rules the following is an example of a rule  stated informally  ? young womenwith annual incomes greater than $ 50,000 are the most likely people to buy small sports cars ? of course such rules are not universally true  but rather have thesumit67.blogspot.com 26 chapter 1 introduction degrees of ? support ? and ? confidence ? other types of knowledge are represented by equations relating different variables to each other  or by other mechanisms for predicting outcomes when the values of some variables are known there are a variety of possible types of patterns that may be useful  and different techniques are used to find different types of patterns in chapter 20 we study a few examples of patterns and see how they may be automatically derived from a database usually there is a manual component to data mining  consisting of preprocessing data to a form acceptable to the algorithms  and postprocessing of discovered patterns to find novel ones that could be useful there may also be more than one type of pattern that can be discovered from a given database  and manual interaction may be needed to pick useful types of patterns for this reason  data mining is really a semiautomatic process in real life however  in our description we concentrate on the automatic aspect of mining businesses have begun to exploit the burgeoning data online to make better decisions about their activities  such as what items to stock and how best to target customers to increase sales many of their queries are rather complicated  however  and certain types of information can not be extracted even by using sql several techniques and tools are available to help with decision support several tools for data analysis allow analysts to view data in different ways other analysis tools precompute summaries of very large amounts of data  in order to give fast responses to queries the sql standard contains additional constructs to support data analysis large companies have diverse sources of data that they need to use for making business decisions to execute queries efficiently on such diverse data  companies have built data warehouses data warehouses gather data from multiple sources under a unified schema  at a single site thus  they provide the user a single uniform interface to data textual data  too  has grown explosively textual data is unstructured  unlike the rigidly structured data in relational databases querying of unstructured textual data is referred to as information retrieval information retrieval systems have much in common with database systems ? in particular  the storage and retrieval of data on secondary storage however  the emphasis in the field of information systems is different from that in database systems  concentrating on issues such as querying based on keywords ; the relevance of documents to the query ; and the analysis  classification  and indexing of documents in chapters 20 and 21  we cover decision support  including online analytical processing  data mining  data warehousing  and information retrieval 1.11 specialty databases several application areas for database systems are limited by the restrictions of the relational data model as a result  researchers have developed several datamodels to deal with these application domains  including object-based data models and semistructured data models thesumit67.blogspot.com 1.12 database users and administrators 27 1.11.1 object-based data models object-oriented programming has become the dominant software-development methodology this led to the development of an object-oriented data model that can be seen as extending the e-r model with notions of encapsulation  methods  functions   and object identity inheritance  object identity  and encapsulation  information hiding   with methods to provide an interface to objects  are among the key concepts of object-oriented programming that have found applications in data modeling the object-oriented data model also supports a rich type system  including structured and collection types in the 1980s  several database systems based on the object-oriented data model were developed the major database vendors presently support the object-relational data model  a data model that combines features of the object-oriented datamodel and relational data model it extends the traditional relational model with a variety of features such as structured and collection types  as well as object orientation chapter 22 examines the object-relational data model 1.11.2 semistructured data models semistructured data models permit the specification of data where individual data items of the same type may have different sets of attributes this is in contrast with the data models mentioned earlier  where every data item of a particular type must have the same set of attributes the xml language was initially designed as a way of adding markup information to text documents  but has become important because of its applications in data exchange xml provides a way to represent data that have nested structure  and furthermore allows a great deal of flexibility in structuring of data  which is important for certain kinds of nontraditional data chapter 23 describes the xml language  different ways of expressing queries on data represented in xml  and transforming xml data from one form to another 1.12 database users and administrators a primary goal of a database system is to retrieve information from and store new information into the database people who work with a database can be categorized as database users or database administrators 1.12.1 database users and user interfaces there are four different types of database-system users  differentiated by the way they expect to interact with the system different types of user interfaces have been designed for the different types of users ? na ? ive users are unsophisticated users who interact with the system by invoking one of the application programs that have been written previously for example  a clerk in the university who needs to add a new instructor to thesumit67.blogspot.com 28 chapter 1 introduction department a invokes a program called new hire this program asks the clerk for the name of the new instructor  her new id  the name of the department  that is  a   and the salary the typical user interface for na ? ive users is a forms interface  where the user can fill in appropriate fields of the form na ? ive users may also simply read reports generated from the database as another example  consider a student  who during class registration period  wishes to register for a class by using a web interface such a user connects to a web application program that runs at a web server the application first verifies the identity of the user  and allows her to access a form where she enters the desired information the form information is sent back to the web application at the server  which then determines if there is room in the class  by retrieving information from the database  and if so adds the student information to the class roster in the database ? application programmers are computer professionalswho write application programs application programmers can choose frommany tools to develop user interfaces rapid application development  rad  tools are tools that enable an application programmer to construct forms and reportswith minimal programming effort ? sophisticated users interact with the system without writing programs instead  they form their requests either using a database query language or by using tools such as data analysis software analysts who submit queries to explore data in the database fall in this category ? specialized users are sophisticated users who write specialized database applications that do not fit into the traditional data-processing framework among these applications are computer-aided design systems  knowledgebase and expert systems  systems that store data with complex data types  for example  graphics data and audio data   and environment-modeling systems chapter 22 covers several of these applications 1.12.2 database administrator one of the main reasons for using dbmss is tohave central control of both thedata and the programs that access those data a person who has such central control over the system is called a database administrator  dba   the functions of a dba include  ? schema definition the dba creates the original database schema by executing a set of data definition statements in the ddl ? storage structure and access-method definition ? schema and physical-organization modification thedbacarries out changes to the schema and physical organization to reflect the changing needs of the organization  or to alter the physical organization to improve performance thesumit67.blogspot.com 1.13 history of database systems 29 ? granting of authorization for data access by granting different types of authorization  the database administrator can regulate which parts of the database various users can access the authorization information is kept in a special system structure that the database system consults whenever someone attempts to access the data in the system ? routine maintenance examples of the database administrator ? s routine maintenance activities are  ? periodically backing up the database  either onto tapes or onto remote servers  to prevent loss of data in case of disasters such as flooding ? ensuring that enough free disk space is available for normal operations  and upgrading disk space as required ? monitoring jobs running on the database and ensuring that performance is not degraded by very expensive tasks submitted by some users 1.13 history of database systems information processing drives the growth of computers  as it has from the earliest days of commercial computers in fact  automation of data processing tasks predates computers punched cards  invented by herman hollerith  were used at the very beginning of the twentieth century to record u.s census data  and mechanical systemswere used to process the cards and tabulate results punched cards were later widely used as a means of entering data into computers techniques for data storage and processing have evolved over the years  ? 1950s and early 1960s  magnetic tapes were developed for data storage data processing tasks such as payroll were automated  with data stored on tapes processing of data consisted of reading data from one or more tapes and writing data to a new tape data could also be input from punched card decks  and output to printers for example  salary raises were processed by entering the raises on punched cards and reading the punched card deck in synchronization with a tape containing themaster salary details the records had to be in the same sorted order the salary raises would be added to the salary read from the master tape  and written to a new tape ; the new tape would become the new master tape tapes  and card decks  could be read only sequentially  and data sizeswere much larger than main memory ; thus  data processing programs were forced to process data in a particular order  by reading and merging data fromtapes and card decks ? late 1960s and 1970s  widespread use of hard disks in the late 1960s changed the scenario for data processing greatly  since hard disks allowed direct access to data the position of data on disk was immaterial  since any location on disk could be accessed in just tens of milliseconds data were thus freed from thesumit67.blogspot.com 30 chapter 1 introduction the tyranny of sequentiality.with disks  network and hierarchical databases could be created that allowed data structures such as lists and trees to be stored on disk programmers could construct and manipulate these data structures a landmark paper by codd  1970  defined the relational model and nonprocedural ways of querying data in the relational model  and relational databaseswere born the simplicity of the relational model and the possibility of hiding implementation details completely from the programmer were enticing indeed codd later won the prestigious association of computing machinery turing award for his work ? 1980s  although academically interesting  the relational model was not used in practice initially  because of its perceived performance disadvantages ; relational databases could notmatch the performance of existing network and hierarchical databases that changed with system r  a groundbreaking project at ibm research that developed techniques for the construction of an efficient relational database system excellent overviews of system r are provided by astrahan et al  1976  and chamberlin et al  1981   the fully functional system r prototype led to ibm ? s first relational database product  sql/ds at the same time  the ingres system was being developed at the university of california at berkeley it led to a commercial product of the same name initial commercial relational database systems  such as ibm db2  oracle  ingres  and dec rdb  played a major role in advancing techniques for efficient processing of declarative queries by the early 1980s  relational databases had become competitivewith network and hierarchical database systems even in the area of performance relational databases were so easy to use that they eventually replaced network and hierarchical databases ; programmers using such databases were forced to deal with many low-level implementation details  and had to code their queries in a procedural fashion most importantly  they had to keep efficiency in mind when designing their programs  which involved a lot of effort in contrast  in a relational database  almost all these low-level tasks are carried out automatically by the database  leaving the programmer free to work at a logical level since attaining dominance in the 1980s  the relational model has reigned supreme among data models the 1980s also saw much research on parallel and distributed databases  as well as initial work on object-oriented databases ? early 1990s  the sql language was designed primarily for decision support applications  which are query-intensive  yet the mainstay of databases in the 1980s was transaction-processing applications  which are update-intensive decision support and querying re-emerged as a major application area for databases tools for analyzing large amounts of data saw large growths in usage many database vendors introduced parallel database products in this period database vendors also began to add object-relational support to their databases thesumit67.blogspot.com 1.14 summary 31 ? 1990s  the major event of the 1990s was the explosive growth of the world wideweb databaseswere deployedmuchmore extensively than ever before database systems now had to support very high transaction-processing rates  as well as very high reliability and 24 ? 7 availability  availability 24 hours a day  7 days a week  meaning no downtime for scheduled maintenance activities   database systems also had to supportweb interfaces to data ? 2000s  the first half of the 2000s saw the emerging of xml and the associated query language xquery as a new database technology although xml is widely used for data exchange  as well as for storing certain complex data types  relational databases still form the core of a vast majority of large-scale database applications in this time periodwe have also witnessed the growth in ? autonomic-computing/auto-admin ? techniques for minimizing system administration effort this period also saw a significant growth in use of open-source database systems  particularly postgresql and mysql the latter part of the decade has seen growth in specialized databases for data analysis  in particular column-stores  which in effect store each column of a table as a separate array  and highly parallel database systems designed for analysis of very large data sets several novel distributed data-storage systems have been built to handle the data management requirements of very large web sites such as amazon  facebook  google  microsoft and yahoo !  and some of these are now offered as web services that can be used by application developers there has also been substantialwork onmanagement and analysis of streaming data  such as stock-market ticker data or computer network monitoring data data-mining techniques are now widely deployed ; example applications include web-based product-recommendation systems and automatic placement of relevant advertisements on web pages 1.14 summary ? a database-management system  dbms  consists of a collection of interrelated data and a collection of programs to access that data the data describe one particular enterprise ? the primary goal of a dbms is to provide an environment that is both convenient and efficient for people to use in retrieving and storing information ? database systems are ubiquitous today  and most people interact  either directly or indirectly  with databases many times every day ? database systems are designed to store large bodies of information the management of data involves both the definition of structures for the storage of information and the provision of mechanisms for the manipulation of information in addition  the database system must provide for the safety of the information stored  in the face of system crashes or attempts at unauthorized access if data are to be shared among several users  the system must avoid possible anomalous results thesumit67.blogspot.com 32 chapter 1 introduction ? a major purpose of a database system is to provide users with an abstract view of the data that is  the system hides certain details of how the data are stored and maintained ? underlying the structure of a database is the data model  a collection of conceptual tools for describing data  data relationships  data semantics  and data constraints ? the relational datamodel is themostwidely deployedmodel for storing data in databases other data models are the object-oriented model  the objectrelational model  and semistructured data models ? a data-manipulation language  dml  is a language that enables users to access or manipulate data nonprocedural dmls  which require a user to specify only what data are needed  without specifying exactly how to get those data  are widely used today ? a data-definition language  ddl  is a language for specifying the database schema and as well as other properties of the data ? database design mainly involves the design of the database schema the entity-relationship  e-r  datamodel is awidely used datamodel for database design it provides a convenient graphical representation to view data  relationships  and constraints ? a database system has several subsystems ? the storage manager subsystem provides the interface between the lowlevel data stored in the database and the application programs and queries submitted to the system ? the query processor subsystem compiles and executes ddl and dml statements ? transaction management ensures that the database remains in a consistent  correct  state despite system failures the transaction manager ensures that concurrent transaction executions proceed without conflicting ? the architecture of a database system is greatly influenced by the underlying computer system on which the database system runs database systems can be centralized  or client-server  where one server machine executes work on behalf of multiple client machines database systems can also be designed to exploit parallel computer architectures distributed databases span multiple geographically separated machines ? database applications are typically brokenup into a front-end part that runs at client machines and a part that runs at the back end in two-tier architectures  the front end directly communicates with a database running at the back end in three-tier architectures  the back end part is itself broken up into an application server and a database server thesumit67.blogspot.com practice exercises 33 ? knowledge-discovery techniques attempt to discover automatically statistical rules and patterns fromdata the field of data mining combines knowledgediscovery techniques invented by artificial intelligence researchers and statistical analysts  with efficient implementation techniques that enable them to be used on extremely large databases ? there are four different types of database-system users  differentiated by the way they expect to interact with the system different types of user interfaces have been designed for the different types of users review terms ? database-management system  dbms  ? database-system applications ? file-processing systems ? data inconsistency ? consistency constraints ? data abstraction ? instance ? schema ? physical schema ? logical schema ? physical data independence ? data models ? entity-relationship model ? relational data model ? object-based data model ? semistructured data model ? database languages ? data-definition language ? data-manipulation language ? query language ? metadata ? application program ? normalization ? data dictionary ? storage manager ? query processor ? transactions ? atomicity ? failure recovery ? concurrency control ? two and three-tier database architectures ? data mining ? database administrator  dba  practice exercises 1.1 this chapter has described several major advantages of a database system what are two disadvantages ? 1.2 list five ways in which the type declaration system of a language such as java or c + + differs from the data definition language used in a database thesumit67.blogspot.com 34 chapter 1 introduction 1.3 list six major steps that you would take in setting up a database for a particular enterprise 1.4 list at least 3 different types of information that a university would maintain  beyond those listed in section 1.6.2 1.5 suppose you want to build a video site similar to youtube consider each of the points listed in section 1.2  as disadvantages of keeping data in a file-processing system discuss the relevance of each of these points to the storage of actual video data  and to metadata about the video  such as title  the user who uploaded it  tags  and which users viewed it 1.6 keyword queries used in web search are quite different from database queries list key differences between the two  in terms of the way the queries are specified  and in terms of what is the result of a query exercises 1.7 list four applications you have used that most likely employed a database system to store persistent data 1.8 list four significant differences between a file-processing system and a dbms 1.9 explain the concept of physical data independence  and its importance in database systems 1.10 list five responsibilities of a database-management system for each responsibility  explain the problems that would arise if the responsibility were not discharged 1.11 list at least two reasons why database systems support data manipulation using a declarative query language such as sql  instead of just providing a a library of c or c + + functions to carry out data manipulation 1.12 explain what problems are caused by the design of the table in figure 1.4 1.13 what are five main functions of a database administrator ? 1.14 explain the difference between two-tier and three-tier architectures which is better suited for web applications ? why ? 1.15 describe at least 3 tables that might be used to store information in a social-networking system such as facebook tools there are a large number of commercial database systems in use today the major ones include  ibm db2  www.ibm.com/software/data/db2   oracle  www.oracle.com   microsoft sql server  www.microsoft.com/sql   sybase  www.sybase.com   and ibm informix  www.ibm.com/software/data/informix   some of these systems are available thesumit67.blogspot.com bibliographical notes 35 free for personal or noncommercial use  or for development  but are not free for actual deployment there are also a number of free/public domain database systems ; widely used ones include mysql  www.mysql.com  and postgresql  www.postgresql.org   a more complete list of links to vendor web sites and other information is available from the home page of this book  at www.db-book.com bibliographical notes we list below general-purpose books  research paper collections  and web sites on databases subsequent chapters provide references to material on each topic outlined in this chapter codd  1970  is the landmark paper that introduced the relational model textbooks covering database systems include abiteboul et al  1995   o ? neil and o ? neil  2000   ramakrishnan and gehrke  2002   date  2003   kifer et al  2005   elmasri and navathe  2006   and garcia-molina et al  2008   textbook coverage of transaction processing is provided by bernstein and newcomer  1997  and gray and reuter  1993   a book containing a collection of research papers on database management is offered by hellerstein and stonebraker  2005   a review of accomplishments in database management and an assessment of future research challenges appears in silberschatz et al  1990   silberschatz et al  1996   bernstein et al  1998   abiteboul et al  2003   and agrawal et al  2009   the home page of the acm special interestgroup on management of data  www.acm.org/sigmod  provides a wealth of information about database research database vendor web sites  see the tools section above  provide details about their respective products thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com part 1 relational databases a data model is a collection of conceptual tools for describing data  data relationships  data semantics  and consistency constraints in this part  we focus on the relational model the relational model,which is covered in chapter 2  uses a collection of tables to represent both data and the relationships among those data its conceptual simplicity has led to its widespread adoption ; today a vast majority of database products are based on the relational model the relational model describes data at the logical and view levels  abstracting away low-level details of data storage the entity-relationshipmodel  discussed later in chapter 7  in part 2   is a higher-level data model which is widely used for database design to make data from a relational database available to users  we have to address several issues the most important issue is how users specify requests for retrieving and updating data ; several query languages have been developed for this task a second  but still important  issue is data integrity and protection ; databases need to protect data from damage by user actions  whether unintentional or intentional chapters 3  4 and 5 cover the sql language  which is the most widely used query language today chapters 3 and 4 provide introductory and intermediate level descriptions of sql chapter 4 also covers integrity constraints which are enforced by the database  and authorization mechanisms  which control what access and update actions can be carried out by a user chapter 5 covers more advanced topics  including access to sql from programming languages  and the use of sql for data analysis chapter 6 covers three formal query languages  the relational algebra  the tuple relational calculus and the domain relational calculus  which are declarative query languages based on mathematical logic these formal languages form the basis for sql  and for two other user-friendly languages  qbe and datalog  which are described in appendix b  available online at db-book.com   37 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter2 introduction to the relational model the relational model is today the primary data model for commercial dataprocessing applications it attained its primary position because of its simplicity  which eases the job of the programmer  compared to earlier data models such as the network model or the hierarchical model in this chapter  we first study the fundamentals of the relational model a substantial theory exists for relational databases.we study the part of this theory dealing with queries in chapter 6 in chapters 7 through 8  we shall examine aspects of database theory that help in the design of relational database schemas  while in chapters 12 and 13we discuss aspects of the theory dealingwith efficient processing of queries 2.1 structure of relational databases a relational database consists of a collection of tables  each of which is assigned a unique name for example  consider the instructor table of figure 2.1  which stores information about instructors the table has four column headers  id  name  dept name  and salary each row of this table records information about an instructor  consisting of the instructor ? s id  name  dept name  and salary similarly  the course table of figure 2.2 stores information about courses  consisting of a course id  title  dept name  and credits  for each course note that each instructor is identified by the value of the column id  while each course is identified by the value of the column course id figure 2.3 shows a third table  prereq,which stores the prerequisite courses for each course the table has two columns  course id and prereq id each rowconsists of a pair of course identifiers such that the second course is a prerequisite for the first course thus  a row in the prereq table indicates that two courses are related in the sense that one course is a prerequisite for the other as another example  we consider the table instructor  a row in the table can be thought of as representing 39 thesumit67.blogspot.com 40 chapter 2 introduction to the relational model id name dept name salary 10101 srinivasan comp sci 65000 12121 wu finance 90000 15151 mozart music 40000 22222 einstein physics 95000 32343 el said history 60000 33456 gold physics 87000 45565 katz comp sci 75000 58583 califieri history 62000 76543 singh finance 80000 76766 crick biology 72000 83821 brandt comp sci 92000 98345 kim elec eng 80000 figure 2.1 the instructor relation the relationship between a specified id and the corresponding values for name  dept name  and salary values in general  a row in a table represents a relationship among a set of values since a table is a collection of such relationships  there is a close correspondence between the concept of table and the mathematical concept of relation  fromwhich the relational data model takes its name in mathematical terminology  a tuple is simply a sequence  or list  of values a relationship between n values is represented mathematically by an n-tuple of values  i.e  a tuple with n values  which corresponds to a row in a table course id title dept name credits bio-101 intro to biology biology 4 bio-301 genetics biology 4 bio-399 computational biology biology 3 cs-101 intro to computer science comp sci 4 cs-190 game design comp sci 4 cs-315 robotics comp sci 3 cs-319 image processing comp sci 3 cs-347 database system concepts comp sci 3 ee-181 intro to digital systems elec eng 3 fin-201 investment banking finance 3 his-351 world history history 3 mu-199 music video production music 3 phy-101 physical principles physics 4 figure 2.2 the course relation thesumit67.blogspot.com 2.1 structure of relational databases 41 course id prereq id bio-301 bio-101 bio-399 bio-101 cs-190 cs-101 cs-315 cs-101 cs-319 cs-101 cs-347 cs-101 ee-181 phy-101 figure 2.3 the prereq relation thus  in the relational model the term relation is used to refer to a table  while the term tuple is used to refer to a row similarly  the term attribute refers to a column of a table examining figure 2.1,we can see that the relation instructor has four attributes  id  name  dept name  and salary we use the term relation instance to refer to a specific instance of a relation  i.e  containing a specific set of rows the instance of instructor shown in figure 2.1 has 12 tuples  corresponding to 12 instructors in this chapter,weshall be using a number ofdifferent relations to illustrate the various concepts underlying the relational data model these relations represent part of a university they do not include all the data an actual university database would contain  in order to simplify our presentation.we shall discuss criteria for the appropriateness of relational structures in great detail in chapters 7 and 8 the order in which tuples appear in a relation is irrelevant  since a relation is a set of tuples thus  whether the tuples of a relation are listed in sorted order  as in figure 2.1  or are unsorted  as in figure 2.4  does not matter ; the relations in id name dept name salary 22222 einstein physics 95000 12121 wu finance 90000 32343 el said history 60000 45565 katz comp sci 75000 98345 kim elec eng 80000 76766 crick biology 72000 10101 srinivasan comp sci 65000 58583 califieri history 62000 83821 brandt comp sci 92000 15151 mozart music 40000 33456 gold physics 87000 76543 singh finance 80000 figure 2.4 unsorted display of the instructor relation thesumit67.blogspot.com 42 chapter 2 introduction to the relational model the two figures are the same  since both contain the same set of tuples for ease of exposition  we will mostly show the relations sorted by their first attribute for each attribute of a relation  there is a set of permitted values  called the domain of that attribute thus  the domain of the salary attribute of the instructor relation is the set of all possible salary values  while the domain of the name attribute is the set of all possible instructor names we require that  for all relations r  the domains of all attributes of r be atomic a domain is atomic if elements of the domain are considered to be indivisible units for example  suppose the table instructor had an attribute phone number  which can store a set of phone numbers corresponding to the instructor then the domain of phone number would not be atomic  since an element of the domain is a set of phone numbers  and it has subparts  namely the individual phone numbers in the set the important issue is not what the domain itself is  but rather how we use domain elements in our database suppose now that the phone number attribute stores a single phone number even then  if we split the value from the phone number attribute into a country code  an area code and a local number  we would be treating it as a nonatomic value if we treat each phone number as a single indivisible unit  then the attribute phone number would have an atomic domain in this chapter  as well as in chapters 3 through 6  we assume that all attributes have atomic domains in chapter 22  we shall discuss extensions to the relational data model to permit nonatomic domains the null value is a special value that signifies that the value is unknown or does not exist for example  suppose as before that we include the attribute phone number in the instructor relation it may be that an instructor does not have a phone number at all  or that the telephone number is unlisted we would then have to use the null value to signify that the value is unknown or does not exist we shall see later that null values cause a number of difficulties when we access or update the database  and thus should be eliminated if at all possible.we shall assume null values are absent initially  and in section 3.6 we describe the effect of nulls on different operations 2.2 database schema when we talk about a database  we must differentiate between the database schema  which is the logical design of the database  and the database instance  which is a snapshot of the data in the database at a given instant in time the concept of a relation corresponds to the programming-language notion of a variable  while the concept of a relation schema corresponds to the programming-language notion of type definition in general  a relation schema consists of a list of attributes and their corresponding domains we shall not be concerned about the precise definition of the domain of each attribute until we discuss the sql language in chapter 3 the concept of a relation instance corresponds to the programming-language notion of a value of a variable the value of a given variable may changewith time ; thesumit67.blogspot.com 2.2 database schema 43 dept name building budget biology watson 90000 comp sci taylor 100000 elec eng taylor 85000 finance painter 120000 history painter 50000 music packard 80000 physics watson 70000 figure 2.5 the department relation similarly the contents of a relation instance may change with time as the relation is updated in contrast  the schema of a relation does not generally change although it is important to know the difference between a relation schema and a relation instance  we often use the same name  such as instructor  to refer to both the schema and the instance where required  we explicitly refer to the schema or to the instance  for example ? the instructor schema  ? or ? an instance of the instructor relation ? however  where it is clear whether we mean the schema or the instance  we simply use the relation name consider the department relation of figure 2.5 the schema for that relation is department  dept name  building  budget  note that the attribute dept name appears in both the instructor schema and the department schema this duplication is not a coincidence rather  using common attributes in relation schemas is one way of relating tuples of distinct relations for example  suppose we wish to find the information about all the instructors who work in the watson building we look first at the department relation to find the dept name of all the departments housed in watson then  for each such department  we look in the instructor relation to find the information about the instructor associated with the corresponding dept name let us continue with our university database example each course in a university may be offered multiple times  across different semesters  or even within a semester.we need a relation to describe each individual offering  or section  of the class the schema is section  course id  sec id  semester  year  building  room number  time slot id  figure 2.6 shows a sample instance of the section relation we need a relation to describe the association between instructors and the class sections that they teach the relation schema to describe this association is teaches  id  course id  sec id  semester  year  thesumit67.blogspot.com 44 chapter 2 introduction to the relational model course id sec id semester year building room number time slot id bio-101 1 summer 2009 painter 514 b bio-301 1 summer 2010 painter 514 a cs-101 1 fall 2009 packard 101 h cs-101 1 spring 2010 packard 101 f cs-190 1 spring 2009 taylor 3128 e cs-190 2 spring 2009 taylor 3128 a cs-315 1 spring 2010 watson 120 d cs-319 1 spring 2010 watson 100 b cs-319 2 spring 2010 taylor 3128 c cs-347 1 fall 2009 taylor 3128 a ee-181 1 spring 2009 taylor 3128 c fin-201 1 spring 2010 packard 101 b his-351 1 spring 2010 painter 514 c mu-199 1 spring 2010 packard 101 d phy-101 1 fall 2009 watson 100 a figure 2.6 the section relation figure 2.7 shows a sample instance of the teaches relation as you can imagine  there are many more relations maintained in a real university database in addition to those relations we have listed already  instructor  department  course  section  prereq  and teaches,we use the following relations in this text  id course id sec id semester year 10101 cs-101 1 fall 2009 10101 cs-315 1 spring 2010 10101 cs-347 1 fall 2009 12121 fin-201 1 spring 2010 15151 mu-199 1 spring 2010 22222 phy-101 1 fall 2009 32343 his-351 1 spring 2010 45565 cs-101 1 spring 2010 45565 cs-319 1 spring 2010 76766 bio-101 1 summer 2009 76766 bio-301 1 summer 2010 83821 cs-190 1 spring 2009 83821 cs-190 2 spring 2009 83821 cs-319 2 spring 2010 98345 ee-181 1 spring 2009 figure 2.7 the teaches relation thesumit67.blogspot.com 2.3 keys 45 ? student  id  name  dept name  tot cred  ? advisor  s id  i id  ? takes  id  course id  sec id  semester  year  grade  ? classroom  building  room number  capacity  ? time slot  time slot id  day  start time  end time  2.3 keys we must have a way to specify how tuples within a given relation are distinguished this is expressed in terms of their attributes that is  the values of the attribute values of a tuple must be such that they can uniquely identify the tuple in other words  no two tuples in a relation are allowed to have exactly the same value for all attributes a superkey is a set of one or more attributes that  taken collectively  allow us to identify uniquely a tuple in the relation for example  the id attribute of the relation instructor is sufficient to distinguish one instructor tuple from another thus  id is a superkey the name attribute of instructor  on the other hand  is not a superkey  because several instructors might have the same name formally  let r denote the set of attributes in the schema of relation r if we say that a subset k of r is a superkey for r  we are restricting consideration to instances of relations r in which no two distinct tuples have the same values on all attributes in k that is  if t1 and t2 are in r and t1  = t2  then t1.k  = t2.k a superkey may contain extraneous attributes for example  the combination of id and name is a superkey for the relation instructor if k is a superkey  then so is any superset of k we are often interested in superkeys for which no proper subset is a superkey such minimal superkeys are called candidate keys it is possible that several distinct sets of attributes could serve as a candidate key suppose that a combination of name and dept name is sufficient to distinguish among members of the instructor relation then  both  id  and  name  dept name  are candidate keys although the attributes id and name together can distinguish instructor tuples  their combination   id  name   does not form a candidate key  since the attribute id alone is a candidate key we shall use the term primary key to denote a candidate key that is chosen by the database designer as the principal means of identifying tuples within a relation a key  whether primary  candidate  or super  is a property of the entire relation  rather than of the individual tuples any two individual tuples in the relation are prohibited from having the same value on the key attributes at the same time the designation of a key represents a constraint in the real-world enterprise being modeled primary keys must be chosen with care as we noted  the name of a person is obviously not sufficient  because there may be many people with the same name in the united states  the social-security number attribute of a person would be a candidate key since non-u.s residents usually do not have social-security thesumit67.blogspot.com 46 chapter 2 introduction to the relational model numbers  international enterprises must generate their own unique identifiers an alternative is to use some unique combination of other attributes as a key the primary key should be chosen such that its attribute values are never  or very rarely  changed for instance  the address field of a person should not be part of the primary key  since it is likely to change social-security numbers  on the other hand  are guaranteed never to change unique identifiers generated by enterprises generally do not change  except if two enterprises merge ; in such a case the same identifier may have been issued by both enterprises  and a reallocation of identifiers may be required to make sure they are unique it is customary to list the primary key attributes of a relation schema before the other attributes ; for example  the dept name attribute of department is listed first  since it is the primary key primary key attributes are also underlined a relation  say r1  may include among its attributes the primary key of another relation  say r2 this attribute is called a foreign key from r1  referencing r2 the relation r1 is also called the referencing relation of the foreign key dependency  and r2 is called the referenced relation of the foreign key for example  the attribute dept name in instructor is a foreign key frominstructor  referencing department  since dept name is the primary key of department in any database instance  given any tuple  say ta  from the instructor relation  there must be some tuple  say tb  in the department relation such that the value of the dept name attribute of ta is the same as the value of the primary key  dept name  of tb  now consider the section and teaches relations it would be reasonable to require that if a section exists for a course  it must be taught by at least one instructor ; however  it could possibly be taught by more than one instructor to enforce this constraint  we would require that if a particular  course id  sec id  semester  year  combination appears in section  then the same combination must appear in teaches however  this set of values does not form a primary key for teaches  since more than one instructor may teach one such section as a result  we can not declare a foreign key constraint from section to teaches  although we can define a foreign key constraint in the other direction  from teaches to section   the constraint from section to teaches is an example of a referential integrity constraint ; a referential integrity constraint requires that the values appearing in specified attributes of any tuple in the referencing relation also appear in specified attributes of at least one tuple in the referenced relation 2.4 schema diagrams a database schema  along with primary key and foreign key dependencies  can be depicted by schema diagrams figure 2.8 shows the schema diagram for our university organization each relation appears as a box  with the relation name at the top in blue  and the attributes listed inside the box primary key attributes are shown underlined foreign key dependencies appear as arrows from the foreign key attributes of the referencing relation to the primary key of the referenced relation thesumit67.blogspot.com 2.5 relational query languages 47 id course_id sec_id semester year grade id name dept_name tot_cred building room_no capacity s_id i_id id course_id sec_id semester year takes section classroom teaches prereq course_id prereq_id course_id title dept_name credits course student dept_name building budget department instructor id name dept_name salary advisor time_slot time_slot_id day start_time end_time course_id sec_id semester year building room_no time_slot_id figure 2.8 schema diagram for the university database referential integrity constraints other than foreign key constraints are not shown explicitly in schema diagrams we will study a different diagrammatic representation called the entity-relationship diagram later  in chapter 7 entityrelationship diagrams let us represent several kinds of constraints  including general referential integrity constraints many database systems provide design tools with a graphical user interface for creating schema diagrams we shall discuss diagrammatic representation of schemas at length in chapter 7 the enterprise that we use in the examples in later chapters is a university figure 2.9 gives the relational schema that we use in our examples  with primarykey attributes underlined as we shall see in chapter 3  this corresponds to the approach to defining relations in the sql data-definition language 2.5 relational query languages a query language is a language in which a user requests information from the database these languages are usually on a level higher than that of a standard programming language query languages can be categorized as either procedural or nonprocedural in a procedural language  the user instructs the system to perform a sequence of operations on the database to compute the desired result in a nonprocedural language  the user describes the desired information without giving a specific procedure for obtaining that information thesumit67.blogspot.com 48 chapter 2 introduction to the relational model classroom  building  room number  capacity  department  dept name  building  budget  course  course id  title  dept name  credits  instructor  id  name  dept name  salary  section  course id  sec id  semester  year  building  room number  time slot id  teaches  id  course id  sec id  semester  year  student  id  name  dept name  tot cred  takes  id  course id  sec id  semester  year  grade  advisor  s id  i id  time slot  time slot id  day  start time  end time  prereq  course id  prereq id  figure 2.9 schema of the university database query languages used in practice include elements of both the procedural and the nonprocedural approaches we study the very widely used query language sql in chapters 3 through 5 there are a number of ? pure ? query languages  the relational algebra is procedural  whereas the tuple relational calculus and domain relational calculus are nonprocedural these query languages are terse and formal  lacking the ? syntactic sugar ? of commercial languages  but they illustrate the fundamental techniques for extracting data from the database in chapter 6  we examine in detail the relational algebra and the two versions of the relational calculus  the tuple relational calculus and domain relational calculus the relational algebra consists of a set of operations that take one or two relations as input and produce a new relation as their result the relational calculus uses predicate logic to define the result desired without giving any specific algebraic procedure for obtaining that result 2.6 relational operations all procedural relational query languages provide a set of operations that can be applied to either a single relation or a pair of relations these operations have the nice and desired property that their result is always a single relation this property allows one to combine several of these operations in a modular way specifically  since the result of a relational query is itself a relation  relational operations can be applied to the results of queries as well as to the given set of relations the specific relational operations are expressed differently depending on the language  but fit the general framework we describe in this section in chapter 3  we show the specific way the operations are expressed in sql the most frequent operation is the selection of specific tuples from a single relation  say instructor  that satisfies some particular predicate  say salary > $ 85,000   the result is a new relation that is a subset of the original relation  inthesumit67 blogspot.com 2.6 relational operations 49 id name dept name salary 12121 wu finance 90000 22222 einstein physics 95000 33456 gold physics 87000 83821 brandt comp sci 92000 figure 2.10 result of query selecting instructor tuples with salary greater than $ 85000 structor   for example  if we select tuples fromthe instructor relation of figure 2.1  satisfying the predicate ? salary is greater than $ 85000 ? ,we get the result shown in figure 2.10 another frequent operation is to select certain attributes  columns  from a relation the result is a new relation having only those selected attributes for example  suppose we want a list of instructor ids and salaries without listing the name and dept name values from the instructor relation of figure 2.1  then the result  shown in figure 2.11  has the two attributes id and salary each tuple in the result is derived from a tuple of the instructor relation but with only selected attributes shown the join operation allows the combining of two relations by merging pairs of tuples  one from each relation  into a single tuple there are a number of different ways to join relations  as we shall see in chapter 3   figure 2.12 shows an example of joining the tuples from the instructor and department tables with the new tuples showing the information about each instructor and the department in which she is working this result was formed by combining each tuple in the instructor relation with the tuple in the department relation for the instructor ? s department in the form of join shown in figure 2.12  which is called a natural join  a tuple from the instructor relation matches a tuple in the department relation if the values id salary 10101 65000 12121 90000 15151 40000 22222 95000 32343 60000 33456 87000 45565 75000 58583 62000 76543 80000 76766 72000 83821 92000 98345 80000 figure 2.11 result of query selecting attributes id and salary from the instructor relation thesumit67.blogspot.com 50 chapter 2 introduction to the relational model id name salary dept name building budget 10101 srinivasan 65000 comp sci taylor 100000 12121 wu 90000 finance painter 120000 15151 mozart 40000 music packard 80000 22222 einstein 95000 physics watson 70000 32343 el said 60000 history painter 50000 33456 gold 87000 physics watson 70000 45565 katz 75000 comp sci taylor 100000 58583 califieri 62000 history painter 50000 76543 singh 80000 finance painter 120000 76766 crick 72000 biology watson 90000 83821 brandt 92000 comp sci taylor 100000 98345 kim 80000 elec eng taylor 85000 figure 2.12 result of natural join of the instructor and department relations of their dept name attributes are the same all such matching pairs of tuples are present in the join result in general  the natural join operation on two relations matches tupleswhose values are the same on all attribute names that are common to both relations the cartesian product operation combines tuples fromtworelations  but unlike the join operation  its result contains all pairs of tuples from the two relations  regardless of whether their attribute values match because relations are sets,we can perform normal set operations on relations the union operation performs a set union of two ? similarly structured ? tables  say a table of all graduate students and a table of all undergraduate students   for example  one can obtain the set of all students in a department other set operations  such as intersection and set difference can be performed as well as we noted earlier  we can perform operations on the results of queries for example  if we want to find the id and salary for those instructorswho have salary greater than $ 85,000  we would perform the first two operations in our example above first we select those tuples from the instructor relation where the salary value is greater than $ 85,000 and then  from that result  select the two attributes id and salary  resulting in the relation shown in figure 2.13 consisting of the id id salary 12121 90000 22222 95000 33456 87000 83821 92000 figure 2.13 result of selecting attributes id and salary of instructors with salary greater than $ 85,000 thesumit67.blogspot.com 2.6 relational operations 51 relational algebra the relational algebra defines a set of operations on relations  paralleling the usual algebraic operations such as addition  subtraction or multiplication  which operate on numbers just as algebraic operations on numbers take one or more numbers as input and return a number as output  the relational algebra operations typically take one or two relations as input and return a relation as output relational algebra is covered in detail in chapter 6  but we outline a few of the operations below symbol  name  example of use   salary > = 85000  instructor   selection  return rows of the input relation that satisfy the predicate   id,salary  instructor   projection  output specified attributes from all rows of the input relation remove duplicate tuples from the output  instructor  department  natural join  output pairs of rows from the two input relations that have the same value on all attributes that have the same name ? instructor ? department  cartesian product  output all pairs of rows from the two input relations  regardless of whether or not they have the same values on common attributes  ?  name  instructor  ?  name  student   union  output the union of tuples fromthe two input relations and salary in this example  we could have performed the operations in either order  but that is not the case for all situations  as we shall see sometimes  the result of a query contains duplicate tuples for example  ifwe select the dept name attribute from the instructor relation  there are several cases of duplication  including ? comp sci ?  which shows up three times certain relational languages adhere strictly to the mathematical definition of a set and remove duplicates others  in consideration of the relatively large amount of processing required to remove duplicates from large result relations  retain duplicates in these latter cases  the relations are not truly relations in the pure mathematical sense of the term of course  data in a database must be changed over time a relation can be updated by inserting new tuples  deleting existing tuples  or modifying tuples by thesumit67.blogspot.com 52 chapter 2 introduction to the relational model changing the values of certain attributes entire relations can be deleted and new ones created we shall discuss relational queries and updates using the sql language in chapters 3 through 5 2.7 summary ? the relational data model is based on a collection of tables the user of the database system may query these tables  insert new tuples  delete tuples  and update  modify  tuples there are several languages for expressing these operations ? the schema of a relation refers to its logical design  while an instance of the relation refers to its contents at a point in time the schema of a database and an instance of a database are similarly defined the schema of a relation includes its attributes  and optionally the types of the attributes and constraints on the relation such as primary and foreign key constraints ? a superkey of a relation is a set of one or more attributes whose values are guaranteed to identify tuples in the relation uniquely a candidate key is a minimal superkey  that is  a set of attributes that forms a superkey  but none of whose subsets is a superkey one of the candidate keys of a relation is chosen as its primary key ? a foreign key is a set of attributes in a referencing relation  such that for each tuple in the referencing relation  the values of the foreign key attributes are guaranteed to occur as the primary key value of a tuple in the referenced relation ? a schema diagram is a pictorial depiction of the schema of a database that shows the relations in the database  their attributes  and primary keys and foreign keys ? the relational query languages define a set of operations that operate on tables  and output tables as their results these operations can be combined to get expressions that express desired queries ? the relational algebra provides a set of operations that take one or more relations as input and return a relation as an output practical query languages such as sql are based on the relational algebra  but add a number of useful syntactic features review terms ? table ? relation ? tuple ? attribute ? domain ? atomic domain thesumit67.blogspot.com practice exercises 53 ? null value ? database schema ? database instance ? relation schema ? relation instance ? keys ? superkey ? candidate key ? primary key ? foreign key ? referencing relation ? referenced relation ? referential integrity constraint ? schema diagram ? query language ? procedural language ? nonprocedural language ? operations on relations ? selection of tuples ? selection of attributes ? natural join ? cartesian product ? set operations ? relational algebra practice exercises 2.1 consider the relational database of figure 2.14 what are the appropriate primary keys ? 2.2 consider the foreign key constraint fromthe dept name attribute of instructor to the department relation give examples of inserts and deletes to these relations  which can cause a violation of the foreign key constraint 2.3 consider the time slot relation given that a particular time slot can meet more than once in a week  explain why day and start time are part of the primary key of this relation  while end time is not 2.4 in the instance of instructor shown in figure 2.1  no two instructors have the same name from this  can we conclude that name can be used as a superkey  or primary key  of instructor ? 2.5 what is the result of first performing the cross product of student and advisor  and then performing a selection operation on the result with the predicate s id = id ?  using the symbolic notation of relational algebra  this query can be written as  s id = i d  student ? advisor    employee  person name  street  city  works  person name  company name  salary  company  company name  city  figure 2.14 relational database for exercises 2.1  2.7  and 2.12 thesumit67.blogspot.com 54 chapter 2 introduction to the relational model branch  branch name  branch city  assets  customer  customer name  customer street  customer city  loan  loan number  branch name  amount  borrower  customer name  loan number  account  account number  branch name  balance  depositor  customer name  account number  figure 2.15 banking database for exercises 2.8  2.9  and 2.13 2.6 consider the following expressions  which use the result of a relational algebra operation as the input to another operation for each expression  explain in words what the expression does a  year = 2009  takes   student b  year = 2009  takes  student  c  id,name,course id  student  takes  2.7 consider the relational database of figure 2.14 give an expression in the relational algebra to express each of the following queries  a find the names of all employees who live in city ? miami ?  b find the names of all employees whose salary is greater than $ 100,000 c find the names of all employees who live in ? miami ? and whose salary is greater than $ 100,000 2.8 consider the bank database of figure 2.15 give an expression in the relational algebra for each of the following queries a find the names of all branches located in ? chicago ?  b find the names of all borrowers who have a loan in branch ? downtown ?  exercises 2.9 consider the bank database of figure 2.15 a what are the appropriate primary keys ? b given your choice of primary keys  identify appropriate foreign keys 2.10 consider the advisor relation shown in figure 2.8  with s id as the primary key of advisor suppose a student can have more than one advisor then  would s id still be a primary key of the advisor relation ? if not  what should the primary key of advisor be ? 2.11 describe the differences in meaning between the terms relation and relation schema thesumit67.blogspot.com bibliographical notes 55 2.12 consider the relational database of figure 2.14 give an expression in the relational algebra to express each of the following queries  a find the names of all employees who work for ? first bank corporation ?  b find the names and cities of residence of all employeeswho work for ? first bank corporation ?  c find the names  street address  and cities of residence of all employees who work for ? first bank corporation ? and earn more than $ 10,000 2.13 consider the bank database of figure 2.15 give an expression in the relational algebra for each of the following queries  a find all loan numbers with a loan value greater than $ 10,000 b find the names of all depositors who have an account with a value greater than $ 6,000 c find the names of all depositors who have an account with a value greater than $ 6,000 at the ? uptown ? branch 2.14 list two reasons why null values might be introduced into the database 2.15 discuss the relative merits of procedural and nonprocedural languages bibliographical notes e f codd of the ibm san jose research laboratory proposed the relational model in the late 1960s  codd  1970    this work led to the prestigious acm turing award to codd in 1981  codd  1982    aftercodd publishedhis original paper  several research projectswere formed with the goal of constructing practical relational database systems  including system r at the ibm san jose research laboratory  ingres at the university of california at berkeley  and query-by-example at the ibm t j watson research center many relational database products are now commercially available these include ibm ? s db2 and informix  oracle  sybase  and microsoft sql server open source relational database systems include mysql and postgresql microsoft access is a single-user database product that is part of the microsoft office suite atzeni and antonellis  1993   maier  1983   and abiteboul et al  1995  are texts devoted exclusively to the theory of the relational data model thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter3 introduction to sql there are a number of database query languages in use  either commercially or experimentally in this chapter  as well as in chapters 4 and 5  we study the most widely used query language  sql although we refer to the sql language as a ? query language  ? it can domuch more than just query a database it can define the structure of the data  modify data in the database  and specify security constraints it is not our intention to provide a complete users ? guide for sql rather,we present sql ? s fundamental constructs and concepts individual implementations of sql may differ in details  or may support only a subset of the full language 3.1 overview of the sql query language ibm developed the original version of sql  originally called sequel  as part of the system r project in the early 1970s the sequel language has evolved since then  and its name has changed to sql  structured query language   many products now support the sql language sql has clearly established itself as the standard relational database language in 1986  the american national standards institute  ansi  and the international organization for standardization  iso  published an sql standard  called sql-86 ansipublishedanextendedstandard for sql  sql-89  in 1989 the next version of the standardwas sql-92 standard  followed by sql  1999  sql  2003  sql  2006  and most recently sql  2008 the bibliographic notes provide references to these standards the sql language has several parts  ? data-definition language  ddl   the sql ddl provides commands for defining relation schemas  deleting relations  and modifying relation schemas ? data-manipulation language  dml   the sql dml provides the ability to query information from the database and to insert tuples into  delete tuples from  and modify tuples in the database 57 thesumit67.blogspot.com 58 chapter 3 introduction to sql ? integrity the sql ddl includes commands for specifying integrity constraints that the data stored in the database must satisfy updates that violate integrity constraints are disallowed ? view definition the sql ddl includes commands for defining views ? transaction control sql includes commands for specifying the beginning and ending of transactions ? embedded sql and dynamic sql embedded and dynamic sql define how sql statements can be embedded within general-purpose programming languages  such as c  c + +  and java ? authorization the sql ddl includes commands for specifying access rights to relations and views in this chapter  we present a survey of basic dml and the ddl features of sql features described here have been part of the sql standard since sql-92 in chapter 4,we provide amore detailed coverage of the sql query language  including  a  various join expressions ;  b  views ;  c  transactions ;  d  integrity constraints ;  e  type system ; and  f  authorization in chapter 5,we cover more advanced features of the sql language  including  a  mechanisms to allow accessing sql from a programming language ;  b  sql functions and procedures ;  c  triggers ;  d  recursive queries ;  e  advanced aggregation features ; and  f  several features designed for data analysis  which were introduced in sql  1999  and subsequent versions of sql later  in chapter 22  we outline object-oriented extensions to sql  which were introduced in sql  1999 although most sql implementations support the standard features we describe here  you should be aware that there are differences between implementations most implementations support some nonstandard features  while omitting support for some of the more advanced features in case you find that some language features described here do not work on the database system that you use  consult the user manuals for your database system to find exactly what features it supports 3.2 sql data definition the set of relations in a database must be specified to the system by means of a data-definition language  ddl   the sql ddl allows specification of not only a set of relations  but also information about each relation  including  ? the schema for each relation ? the types of values associated with each attribute ? the integrity constraints ? the set of indices to be maintained for each relation thesumit67.blogspot.com 3.2 sql data definition 59 ? the security and authorization information for each relation ? the physical storage structure of each relation on disk we discuss here basic schema definition and basic types ; we defer discussion of the other sql ddl features to chapters 4 and 5 3.2.1 basic types the sql standard supports a variety of built-in types  including  ? char  n   a fixed-length character string with user-specified length n the full form  character  can be used instead ? varchar  n   a variable-length character string with user-specified maximum length n the full form  character varying  is equivalent ? int  an integer  a finite subset of the integers that ismachine dependent   the full form  integer  is equivalent ? smallint  a small integer  a machine-dependent subset of the integer type   ? numeric  p  d   afixed-point numberwith user-specified precision the number consists of p digits  plus a sign   and d of the p digits are to the right of the decimal point thus  numeric  3,1  allows 44.5 to be stored exactly  but neither 444.5 or 0.32 can be stored exactly in a field of this type ? real  double precision  floating-point and double-precision floating-point numbers with machine-dependent precision ? float  n   a floating-point number  with precision of at least n digits additional types are covered in section 4.5 each type may include a special value called the null value a null value indicates an absent value that may exist but be unknown or that may not exist at all in certain cases  we may wish to prohibit null values from being entered  as we shall see shortly the char data type stores fixed length strings consider  for example  an attribute a of type char  10   if we store a string ? avi ? in this attribute  7 spaces are appended to the string to make it 10 characters long in contrast  if attribute b were of type varchar  10   and we store ? avi ? in attribute b  no spaces would be added when comparing two values of type char  if they are of different lengths extra spaces are automatically added to the shorter one to make them the same size  before comparison when comparing a char type with a varchar type  one may expect extra spaces to be added to the varchar type to make the lengths equal  before comparison ; however  this may or may not be done  depending on the database system as a result  even if the same value ? avi ? is stored in the attributes a and b above  a comparison a = b may return false we recommend you always use the varchar type instead of the char type to avoid these problems thesumit67.blogspot.com 60 chapter 3 introduction to sql sql also provides the nvarchar type to store multilingual data using the unicode representation however  many databases allow unicode  in the utf-8 representation  to be stored even in varchar types 3.2.2 basic schema definition we define an sql relation by using the create table command the following command creates a relation department in the database create table department  dept name varchar  20   building varchar  15   budget numeric  12,2   primary key  dept name   ; the relation created above has three attributes  dept name  which is a character string of maximum length 20  building  which is a character string of maximum length 15  and budget  which is a number with 12 digits in total  2 of which are after the decimal point the create table command also specifies that the dept name attribute is the primary key of the department relation the general form of the create table command is  create table r  a1 d1  a2 d2      an dn   integrity-constraint1        integrity-constraintk   ; where r is the name of the relation  each ai is the name of an attribute in the schema of relation r  and di is the domain of attribute ai ; that is  di specifies the type of attribute ai along with optional constraints that restrict the set of allowed values for ai  the semicolon shown at the end of the create table statements  as well as at the end of other sql statements later in this chapter  is optional in many sql implementations sql supports a number of different integrity constraints in this section  we discuss only a few of them  ? primary key  aj1  aj2      ajm   the primary-key specification says that attributes aj1  aj2      ajm form the primary key for the relation the primarykey attributes are required to be nonnull and unique ; that is  no tuple can have a null value for a primary-key attribute  and no two tuples in the relation can be equal on all the primary-key attributes although the primary-key thesumit67.blogspot.com 3.2 sql data definition 61 specification is optional  it is generally a good idea to specify a primary key for each relation ? foreign key  ak1  ak2      akn  references s  theforeign key specification says that the values of attributes  ak1  ak2      akn  for any tuple in the relation must correspond to values of the primary key attributes of some tuple in relation s figure 3.1presents a partial sqlddl definition of the university databasewe use in the text the definition of the course table has a declaration ? foreign key  dept name  references department ?  this foreign-key declaration specifies that for each course tuple  the department name specified in the tuple must exist in the primary key attribute  dept name  of the department relation without this constraint  it is possible for a course to specify a nonexistent department name figure 3.1 also shows foreign key constraints on tables section  instructor and teaches ? not null  the not null constraint on an attribute specifies that the null value is not allowed for that attribute ; in other words  the constraint excludes the null value from the domain of that attribute for example  in figure 3.1  the not null constraint on the name attribute of the instructor relation ensures that the name of an instructor can not be null more details on the foreign-key constraint  aswell as on other integrity constraints that the create table command may include  are provided later  in section 4.4 sql prevents any update to the database that violates an integrity constraint for example  if a newly inserted or modified tuple in a relation has null values for any primary-key attribute  or if the tuple has the same value on the primary-key attributes as does another tuple in the relation  sql flags an error and prevents the update similarly  an insertion of a course tuple with a dept name value that does not appear in the department relation would violate the foreign-key constraint on course  and sql prevents such an insertion from taking place a newly created relation is empty initially we can use the insert command to load data into the relation for example  if we wish to insert the fact that there is an instructor named smith in the biology department with instructor id 10211 and a salary of $ 66,000  we write  insert into instructor values  10211  ? smith ?  ? biology ?  66000  ; the values are specified in the order in which the corresponding attributes are listed in the relation schema the insert command has a number of useful features  and is covered in more detail later  in section 3.9.2 wecan use the deletecommandto delete tuples froma relation the command delete from student ; thesumit67.blogspot.com 62 chapter 3 introduction to sql create table department  dept name varchar  20   building varchar  15   budget numeric  12,2   primary key  dept name   ; create table course  course id varchar  7   title varchar  50   dept name varchar  20   credits numeric  2,0   primary key  course id   foreign key  dept name  references department  ; create table instructor  id varchar  5   name varchar  20  not null  dept name varchar  20   salary numeric  8,2   primary key  id   foreign key  dept name  references department  ; create table section  course id varchar  8   sec id varchar  8   semester varchar  6   year numeric  4,0   building varchar  15   room number varchar  7   time slot id varchar  4   primary key  course id  sec id  semester  year   foreign key  course id  references course  ; create table teaches  id varchar  5   course id varchar  8   sec id varchar  8   semester varchar  6   year numeric  4,0   primary key  id  course id  sec id  semester  year   foreign key  course id  sec id  semester  year  references section  foreign key  id  references instructor  ; figure 3.1 sql data definition for part of the university database thesumit67.blogspot.com 3.3 basic structure of sql queries 63 would delete all tuples from the student relation other forms of the delete command allow specific tuples to be deleted ; the delete command is covered in more detail later  in section 3.9.1 to remove a relation from an sql database  we use the drop table command the drop table command deletes all information about the dropped relation from the database the command drop table r ; is a more drastic action than delete from r ; the latter retains relation r  but deletes all tuples in r the former deletes not only all tuples of r  but also the schema for r after r is dropped  no tuples can be inserted into r unless it is re-created with the create table command we use the alter table command to add attributes to an existing relation all tuples in the relation are assigned null as the value for the new attribute the form of the alter table command is alter table r add ad ; where r is the name of an existing relation  a is the name of the attribute to be added  and d is the type of the added attribute we can drop attributes from a relation by the command alter table r drop a ; where r is the name of an existing relation  and a is the name of an attribute of the relation many database systems do not support dropping of attributes  although they will allow an entire table to be dropped 3.3 basic structure of sql queries the basic structure of an sql query consists of three clauses  select  from  and where the query takes as its input the relations listed in the from clause  operates on them as specified in the where and select clauses  and then produces a relation as the result we introduce the sql syntax through examples  and describe the general structure of sql queries later 3.3.1 queries on a single relation let us consider a simple query using our university example  ? find the names of all instructors ? instructor names are found in the instructor relation  so we thesumit67.blogspot.com 64 chapter 3 introduction to sql name srinivasan wu mozart einstein el said gold katz califieri singh crick brandt kim figure 3.2 result of ? select name from instructor ?  put that relation in the from clause the instructor ? s name appears in the name attribute  so we put that in the select clause select name from instructor ; the result is a relation consisting of a single attribute with the heading name if the instructor relation is as shown in figure 2.1  then the relation that results from the preceding query is shown in figure 3.2 now consider another query  ? find the department names of all instructors  ? which can be written as  select dept name from instructor ; since more than one instructor can belong to a department  a department name could appear more than once in the instructor relation the result of the above query is a relation containing the department names  shown in figure 3.3 in the formal  mathematical definition of the relational model  a relation is a set thus  duplicate tuples would never appear in relations in practice  duplicate elimination is time-consuming therefore  sql allows duplicates in relations as well as in the results of sql expressions thus  the preceding sql query lists each department name once for every tuple in which it appears in the instructor relation in those cases where we want to force the elimination of duplicates  we insert the keyword distinct after select we can rewrite the preceding query as  select distinct dept name from instructor ; thesumit67.blogspot.com 3.3 basic structure of sql queries 65 dept name comp sci finance music physics history physics comp sci history finance biology comp sci elec eng figure 3.3 result of ? select dept name from instructor ?  ifwe want duplicates removed the result of the above querywould contain each department name at most once sql allows us to use the keyword all to specify explicitly that duplicates are not removed  select all dept name from instructor ; since duplicate retention is the default  we shall not use all in our examples to ensure the elimination of duplicates in the results of our example queries  we shall use distinct whenever it is necessary the select clause may also contain arithmetic expressions involving the operators +    *  and / operating on constants or attributes of tuples for example  the query  select id  name  dept name  salary * 1.1 from instructor ; returns a relation that is the same as the instructor relation  except that the attribute salary is multiplied by 1.1 this shows what would result if we gave a 10 % raise to each instructor ; note  however  that it does not result in any change to the instructor relation sql also provides special data types  such as various forms of the date type  and allows several arithmetic functions to operate on these types.we discuss this further in section 4.5.1 the where clause allows us to select only those rows in the result relation of the from clause that satisfy a specified predicate consider the query ? find the names of all instructors in the computer science department who have salary greater than $ 70,000 ? this query can be written in sql as  thesumit67.blogspot.com 66 chapter 3 introduction to sql name katz brandt figure 3.4 result of ? find the names of all instructors in the computer science department who have salary greater than $ 70,000 ? select name from instructor where dept name = ? comp sci ? and salary > 70000 ; if the instructor relation is as shown in figure 2.1  then the relation that results from the preceding query is shown in figure 3.4 sql allows the use of the logical connectives and  or  and not in the where clause the operands of the logical connectives can be expressions involving the comparison operators <  < =  >  > =  =  and < >  sql allows us to use the comparison operators to compare strings and arithmetic expressions  as well as special types  such as date types we shall explore other features of where clause predicates later in this chapter 3.3.2 queries on multiple relations so far our example queries were on a single relation queries often need to access information from multiple relations.we now study how to write such queries an an example  suppose we want to answer the query ? retrieve the names of all instructors  along with their department names and department building name ? looking at the schema of the relation instructor  we realize that we can get the department name from the attribute dept name  but the department building name is present in the attribute building of the relation department to answer the query  each tuple in the instructor relation must be matched with the tuple in the department relation whose dept name value matches the dept name value of the instructor tuple in sql  to answer the above query,we list the relations that need to be accessed in the from clause  and specify the matching condition in the where clause the above query can be written in sql as select name  instructor.dept name  building from instructor  department where instructor.dept name = department.dept name ; if the instructor and department relations are as shown in figures 2.1 and 2.5 respectively  then the result of this query is shown in figure 3.5 note that the attribute dept name occurs in both the relations instructor and department  and the relation name is used as a prefix  in instructor.dept name  and thesumit67.blogspot.com 3.3 basic structure of sql queries 67 name dept name building srinivasan comp sci taylor wu finance painter mozart music packard einstein physics watson el said history painter gold physics watson katz comp sci taylor califieri history painter singh finance painter crick biology watson brandt comp sci taylor kim elec eng taylor figure 3.5 the result of ? retrieve the names of all instructors  along with their department names and department building name ? department.dept name  to make clear to which attributewe are referring in contrast  the attributes name and building appear in only one of the relations  and therefore do not need to be prefixed by the relation name this naming convention requires that the relations that are present in the from clause have distinct names this requirement causes problems in some cases  such as when information from two different tuples in the same relation needs to be combined in section 3.4.1  we see how to avoid these problems by using the rename operation we now consider the general case of sql queries involving multiple relations as we have seen earlier  an sql query can contain three types of clauses  the select clause  the from clause  and the where clause the role of each clause is as follows  ? the select clause is used to list the attributes desired in the result of a query ? the from clause is a list of the relations to be accessed in the evaluation of the query ? the where clause is a predicate involving attributes of the relation in the from clause a typical sql query has the form select a1  a2      an from r1  r2      rm where p ; each ai represents an attribute  and each ri a relation p is a predicate if the where clause is omitted  the predicate p is true thesumit67.blogspot.com 68 chapter 3 introduction to sql although the clauses must be written in the order select  from  where  the easiestway to understand the operations specified by the query is to consider the clauses in operational order  first from  then where  and then select.1 the from clause by itself defines a cartesian product of the relations listed in the clause it is defined formally in terms of set theory  but is perhaps best understood as an iterative process that generates tuples for the result relation of the from clause for each tuple t1 in relation r1 for each tuple t2 in relation r2    for each tuple tm in relation rm concatenate t1  t2      tm into a single tuple t add t into the result relation the result relation has all attributes from all the relations in the from clause since the same attribute name may appear in both ri and r j  as we saw earlier  we prefix the the name of the relation from which the attribute originally came  before the attribute name for example  the relation schema for the cartesian product of relations instructor and teaches is   instructor.id  instructor.name  instructor.dept name  instructor.salary teaches.id  teaches.course id  teaches.sec id  teaches.semester  teaches.year  with this schema  we can distinguish instructor.id from teaches.id for those attributes that appear in only one of the two schemas  we shall usually drop the relation-name prefix this simplification does not lead to any ambiguity we can then write the relation schema as   instructor.id  name  dept name  salary teaches.id  course id  sec id  semester  year  to illustrate  consider the instructor relation in figure 2.1 and the teaches relation in figure 2.7 their cartesian product is shown in figure 3.6  which includes only a portion of the tuples that make up the cartesian product result.2 the cartesian product by itself combines tuples from instructor and teaches that are unrelated to each other each tuple in instructor is combined with every tuple in teaches  even those that refer to a different instructor the result can be an extremely large relation  and it rarely makes sense to create such a cartesian product 1in practice  sql may convert the expression into an equivalent form that can be processed more efficiently however  we shall defer concerns about efficiency to chapters 12 and 13 2note that we renamed instructor.id as inst.id to reduce the width of the table in figure 3.6 thesumit67.blogspot.com 3.3 basic structure of sql queries 69 inst.id name dept name salary teaches.id course id sec id semester year 10101 srinivasan physics 95000 10101 cs-101 1 fall 2009 10101 srinivasan physics 95000 10101 cs-315 1 spring 2010 10101 srinivasan physics 95000 10101 cs-347 1 fall 2009 10101 srinivasan physics 95000 10101 fin-201 1 spring 2010 10101 srinivasan physics 95000 15151 mu-199 1 spring 2010 10101 srinivasan physics 95000 22222 phy-101 1 fall 2009                   12121 wu physics 95000 10101 cs-101 1 fall 2009 12121 wu physics 95000 10101 cs-315 1 spring 2010 12121 wu physics 95000 10101 cs-347 1 fall 2009 12121 wu physics 95000 10101 fin-201 1 spring 2010 12121 wu physics 95000 15151 mu-199 1 spring 2010 12121 wu physics 95000 22222 phy-101 1 fall 2009                   15151 mozart physics 95000 10101 cs-101 1 fall 2009 15151 mozart physics 95000 10101 cs-315 1 spring 2010 15151 mozart physics 95000 10101 cs-347 1 fall 2009 15151 mozart physics 95000 10101 fin-201 1 spring 2010 15151 mozart physics 95000 15151 mu-199 1 spring 2010 15151 mozart physics 95000 22222 phy-101 1 fall 2009                   22222 einstein physics 95000 10101 cs-101 1 fall 2009 22222 einstein physics 95000 10101 cs-315 1 spring 2010 22222 einstein physics 95000 10101 cs-347 1 fall 2009 22222 einstein physics 95000 10101 fin-201 1 spring 2010 22222 einstein physics 95000 15151 mu-199 1 spring 2010 22222 einstein physics 95000 22222 phy-101 1 fall 2009                   figure 3.6 the cartesian product of the instructor relation with the teaches relation instead  the predicate in the where clause is used to restrict the combinations created by the cartesian product to those that are meaningful for the desired answer we would expect a query involving instructor and teaches to combine a particular tuple t in instructor with only those tuples in teaches that refer to the same instructor to which t refers that is  we wish only to match teaches tuples with instructor tuples that have the same id value the following sql query ensures this condition  and outputs the instructor name and course identifiers from such matching tuples thesumit67.blogspot.com 70 chapter 3 introduction to sql select name  course id from instructor  teaches where instructor.id = teaches.id ; note that the above query outputs only instructorswho have taught some course instructors who have not taught any course are not output ; if we wish to output such tuples,we could use an operation called the outer join  which is described in section 4.1.2 if the instructor relation is as shown in figure 2.1 and the teaches relation is as shown in figure 2.7  then the relation that results from the preceding query is shown in figure 3.7 observe that instructors gold  califieri  and singh  who have not taught any course  do not appear in the above result if we only wished to find instructor names and course identifiers for instructors in the computer science department  we could add an extra predicate to the where clause  as shown below select name  course id from instructor  teaches where instructor.id = teaches.id and instructor.dept name = ? comp sci ? ; note that since the dept name attribute occurs only in the instructor relation  we could have used just dept name  instead of instructor.dept name in the above query in general  the meaning of an sql query can be understood as follows  name course id srinivasan cs-101 srinivasan cs-315 srinivasan cs-347 wu fin-201 mozart mu-199 einstein phy-101 el said his-351 katz cs-101 katz cs-319 crick bio-101 crick bio-301 brandt cs-190 brandt cs-190 brandt cs-319 kim ee-181 figure 3.7 result of ? for all instructors in the university who have taught some course  find their names and the course id of all courses they taught ? thesumit67.blogspot.com 3.3 basic structure of sql queries 71 1 generate a cartesian product of the relations listed in the from clause 2 apply the predicates specified in the where clause on the result of step 1 3 for each tuple in the result of step 2  output the attributes  or results of expressions  specified in the select clause the above sequence of steps helps make clear what the result of an sql query should be  not how it should be executed a real implementation of sql would not execute the query in this fashion ; it would instead optimize evaluation by generating  as far as possible  only elements of the cartesian product that satisfy the where clause predicates we study such implementation techniques later  in chapters 12 and 13 when writing queries  you should be careful to include appropriate where clause conditions if you omit the where clause condition in the preceding sql query  it would output the cartesian product  which could be a huge relation for the example instructor relation in figure 2.1 and the example teaches relation in figure 2.7  their cartesian product has 12 * 13 = 156 tuples ? more than we can show in the text ! to make matters worse  suppose we have a more realistic number of instructors thanwe show in our sample relations in the figures  say 200 instructors let ? s assume each instructor teaches 3 courses  so we have 600 tuples in the teaches relation then the above iterative process generates 200 * 600 = 120,000 tuples in the result 3.3.3 the natural join in our example query that combined information from the instructor and teaches table  the matching condition required instructor.id to be equal to teaches.id these are the only attributes in the two relations that have the same name in fact this is a common case ; that is  the matching condition in the from clause most often requires all attributes with matching names to be equated to make the life of an sql programmer easier for this common case  sql supports an operation called the natural join  which we describe below in fact sql supports several other ways in which information from two or more relations can be joined together we have already seen how a cartesian product along with a where clause predicate can be used to join information from multiple relations otherways of joining information frommultiple relations are discussed in section 4.1 the natural join operation operates on two relations and produces a relation as the result unlike the cartesian product of two relations  which concatenates each tuple of the first relationwith every tuple of the second  natural join considers only those pairs of tuples with the same value on those attributes that appear in the schemas of both relations so  going back to the example of the relations instructor and teaches  computing instructor natural join teaches considers only those pairs of tuples where both the tuple from instructor and the tuple from teaches have the same value on the common attribute  id thesumit67.blogspot.com 72 chapter 3 introduction to sql id name dept name salary course id sec id semester year 10101 srinivasan comp sci 65000 cs-101 1 fall 2009 10101 srinivasan comp sci 65000 cs-315 1 spring 2010 10101 srinivasan comp sci 65000 cs-347 1 fall 2009 12121 wu finance 90000 fin-201 1 spring 2010 15151 mozart music 40000 mu-199 1 spring 2010 22222 einstein physics 95000 phy-101 1 fall 2009 32343 el said history 60000 his-351 1 spring 2010 45565 katz comp sci 75000 cs-101 1 spring 2010 45565 katz comp sci 75000 cs-319 1 spring 2010 76766 crick biology 72000 bio-101 1 summer 2009 76766 crick biology 72000 bio-301 1 summer 2010 83821 brandt comp sci 92000 cs-190 1 spring 2009 83821 brandt comp sci 92000 cs-190 2 spring 2009 83821 brandt comp sci 92000 cs-319 2 spring 2010 98345 kim elec eng 80000 ee-181 1 spring 2009 figure 3.8 the natural join of the instructor relation with the teaches relation the result relation  shown in figure 3.8  has only 13 tuples  the ones that give information about an instructor and a course that that instructor actually teaches notice that we do not repeat those attributes that appear in the schemas of both relations ; rather they appear only once notice also the order in which the attributes are listed  first the attributes common to the schemas of both relations  second those attributes unique to the schema of the first relation  and finally  those attributes unique to the schema of the second relation consider the query ? for all instructors in the university who have taught some course  find their names and the course id of all courses they taught ?  which we wrote earlier as  select name  course id from instructor  teaches where instructor.id = teaches.id ; this query can be written more concisely using the natural-join operation in sql as  select name  course id from instructor natural join teaches ; both of the above queries generate the same result as we saw earlier  the result of the natural join operation is a relation conceptually  expression ? instructor natural join teaches ? in the from clause is replaced thesumit67.blogspot.com 3.3 basic structure of sql queries 73 by the relation obtained by evaluating the natural join.3 the where and select clauses are then evaluated on this relation  as we saw earlier in section 3.3.2 a from clause in an sql query can have multiple relations combined using natural join  as shown here  select a1  a2      an from r1 natural join r2 natural join    natural join rm where p ; more generally  a from clause can be of the form from e1  e2      en where each ei can be a single relation or an expression involving natural joins for example  suppose we wish to answer the query ? list the names of instructors along with the the titles of courses that they teach ? the query can be written in sql as follows  select name  title from instructor natural join teaches  course where teaches.course id = course.course id ; the natural join of instructor and teaches is first computed  as we saw earlier  and a cartesian product of this result with course is computed  from which the where clause extracts only those tuples where the course identifier from the join result matches the course identifier from the course relation note that teaches.course id in the where clause refers to the course id field of the natural join result  since this field in turn came from the teaches relation in contrast the following sql query does not compute the same result  select name  title from instructor natural join teaches natural join course ; to seewhy  note that the natural join of instructor and teaches contains the attributes  id  name  dept name  salary  course id  sec id   while the course relation contains the attributes  course id  title  dept name  credits   as a result  the natural join of these two would require that the dept name attribute values from the two inputs be the same  in addition to requiring that the course id values be the same this query would then omit all  instructor name  course title  pairs where the instructor teaches a course in a department other than the instructor ? s own department the previous query  on the other hand  correctly outputs such pairs 3as a consequence  it is not possible to use attribute names containing the original relation names  for instance instructor nameor teaches.course id  to refer to attributes in the natural join result ; we can  however  use attribute names such as name and course id  without the relation names thesumit67.blogspot.com 74 chapter 3 introduction to sql to provide the benefit of natural join while avoiding the danger of equating attributes erroneously  sql provides a form of the natural join construct that allows you to specify exactly which columns should be equated this feature is illustrated by the following query  select name  title from  instructor natural join teaches  join course using  course id  ; the operation join    using requires a list of attribute names to be specified both inputs must have attributes with the specified names consider the operation r1 join r2 using  a1  a2   the operation is similar to r1 natural join r2  except that a pair of tuples t1 fromr1 and t2 fromr2 match if t1.a1 = t2.a1 and t1.a2 = t2.a2 ; even if r1 and r2 both have an attribute named a3  it is not required that t1.a3 = t2.a3 thus  in the preceding sql query  the join construct permits teaches.dept name and course.dept name to differ  and the sql query gives the correct answer 3.4 additional basic operations there are number of additional basic operations that are supported in sql 3.4.1 the rename operation consider again the query that we used earlier  select name  course id from instructor  teaches where instructor.id = teaches.id ; the result of this query is a relation with the following attributes  name  course id the names of the attributes in the result are derived from the names of the attributes in the relations in the from clause we can not  however  always derive names in this way  for several reasons  first  two relations in the from clause may have attributes with the same name  in which case an attribute name is duplicated in the result second  if we used an arithmetic expression in the select clause  the resultant attribute does not have a name third  even if an attribute name can be derived from the base relations as in the preceding example  we may want to change the attribute name in the result hence  sql provides a way of renaming the attributes of a result relation it uses the as clause  taking the form  old-name as new-name thesumit67.blogspot.com 3.4 additional basic operations 75 the as clause can appear in both the select and from clauses.4 for example  ifwewant the attribute name name to be replaced with the name instructor name  we can rewrite the preceding query as  select name as instructor name  course id from instructor  teaches where instructor.id = teaches.id ; the as clause is particularly useful in renaming relations one reason to rename a relation is to replace a long relation name with a shortened version that is more convenient to use elsewhere in the query to illustrate  we rewrite the query ? for all instructors in the university who have taught some course  find their names and the course id of all courses they taught ? select t.name  s.course id from instructor as t  teaches as s where t.id = s.id ; another reason to rename a relation is a case where we wish to compare tuples in the same relation we then need to take the cartesian product of a relation with itself and  without renaming  it becomes impossible to distinguish one tuple from the other suppose that we want to write the query ? find the names of all instructors whose salary is greater than at least one instructor in the biology department ? we can write the sql expression  select distinct t.name from instructor as t  instructor as s where t.salary > s.salary and s.dept name = ? biology ? ; observe that we could not use the notation instructor.salary  since it would not be clear which reference to instructor is intended in the above query  t and s can be thought of as copies of the relation instructor  but more precisely  they are declared as aliases  that is as alternative names  for the relation instructor an identifier  such as t and s  that is used to rename a relation is referred to as a correlation name in the sql standard  but is also commonly referred to as a table alias  or a correlation variable  or a tuple variable note that a betterway to phrase the previous query in englishwould be ? find the names of all instructors who earn more than the lowest paid instructor in the biology department ? our original wording fits more closely with the sql that we wrote  but the latter wording is more intuitive  and can in fact be expressed directly in sql as we shall see in section 3.8.2 4early versions of sql did not include the keyword as as a result  some implementations of sql  notably oracle  do not permit the keyword as in the from clause inoracle  ? old-name as new-name ? is written instead as ? old-name new-name ? in the from clause the keyword as is permitted for renaming attributes in the select clause  but it is optional and may be omitted in oracle thesumit67.blogspot.com 76 chapter 3 introduction to sql 3.4.2 string operations sql specifies strings by enclosing them in single quotes  for example  ? computer ?  asingle quote character that is part of a string can be specified by using two single quote characters ; for example  the string ? it ? s right ? can be specified by ? it ? s right ?  the sql standard specifies that the equality operation on strings is case sensitive ; as a result the expression ? ? comp sci ? = ? comp sci ? ? evaluates to false however  some database systems  such as mysql and sql server  do not distinguish uppercase from lowercase when matching strings ; as a result ? ? comp sci ? = ? comp sci ? ? would evaluate to true on these databases this default behavior can  however  be changed  either at the database level or at the level of specific attributes sql also permits a variety of functions on character strings  such as concatenating  using ?  ?   extracting substrings  finding the length of strings  converting strings to uppercase  using the function upper  s  where s is a string  and lowercase  using the function lower  s    removing spaces at the end of the string  using trim  s   and so on there are variations on the exact set of string functions supported by different database systems see your database system ? s manual for more details on exactly what string functions it supports pattern matching can be performed on strings  using the operator like we describe patterns by using two special characters  ? percent  %   the % character matches any substring ? underscore    the character matches any character patterns are case sensitive ; that is  uppercase characters do not match lowercase characters  or vice versa to illustrate pattern matching,we consider the following examples  ? ? intro % ? matches any string beginning with ? intro ?  ? ? % comp % ? matches any string containing ? comp ? as a substring  for example  ? intro to computer science ?  and ? computational biology ?  ? ? ? matches any string of exactly three characters ? ? % ? matches any string of at least three characters sql expresses patterns by using the like comparison operator consider the query ? find the names of all departments whose building name includes the substring ? watson ?  ? this query can be written as  select dept name from department where building like ? % watson % ? ; thesumit67.blogspot.com 3.4 additional basic operations 77 for patterns to include the special pattern characters  that is  % and   sql allows the specification of an escape character the escape character is used immediately before a special pattern character to indicate that the special pattern character is to be treated like a normal character we define the escape character for a like comparison using the escape keyword to illustrate  consider the following patterns  which use a backslash  \  as the escape character  ? like ? ab \ % cd % ? escape ? \ ? matches all strings beginning with ? ab % cd ?  ? like ? ab \ \ cd % ? escape ? \ ? matches all strings beginning with ? ab \ cd ?  sql allows us to search for mismatches instead of matches by using the not like comparison operator some databases provide variants of the like operation which do not distinguish lower and upper case sql  1999 also offers a similar to operation  which provides more powerful pattern matching than the like operation ; the syntax for specifying patterns is similar to that used in unix regular expressions 3.4.3 attribute specification in select clause the asterisk symbol ? * ? can be usedin the select clause to denote ? all attributes ? thus  the use of instructor * in the select clause of the query  select instructor * from instructor  teaches where instructor.id = teaches.id ; indicates that all attributes of instructor are to be selected a select clause of the form select * indicates that all attributes of the result relation of the from clause are selected 3.4.4 ordering the display of tuples sql offers the user some control over the order in which tuples in a relation are displayed the order by clause causes the tuples in the result of a query to appear in sorted order to list in alphabetic order all instructors in the physics department  we write  select name from instructor where dept name = ? physics ? order by name ; by default  the order by clause lists items in ascending order to specify the sort order  we may specify desc for descending order or asc for ascending order furthermore  ordering can be performed on multiple attributes suppose that we wish to list the entire instructor relation in descending order of salary if several thesumit67.blogspot.com 78 chapter 3 introduction to sql instructors have the same salary  we order them in ascending order by name.we express this query in sql as follows  select * from instructor order by salary desc  name asc ; 3.4.5 where clause predicates sql includes a between comparison operator to simplify where clauses that specify that a value be less than or equal to some value and greater than or equal to some other value if we wish to find the names of instructors with salary amounts between $ 90,000 and $ 100,000  we can use the between comparison to write  select name from instructor where salary between 90000 and 100000 ; instead of  select name from instructor where salary < = 100000 and salary > = 90000 ; similarly  we can use the not between comparison operator we can extend the preceding query that finds instructor names along with course identifiers  which we saw earlier  and consider a more complicated case in which we require also that the instructors be from the biology department  ? find the instructor names and the courses they taught for all instructors in the biology department who have taught some course ? to write this query  we can modify either of the sql queries we saw earlier  by adding an extra condition in the where clause we show below the modified form of the sql query that does not use natural join select name  course id from instructor  teaches where instructor.id = teaches.id and dept name = ? biology ? ; sql permits us to use the notation  v1  v2      vn  to denote a tuple of arity n containing values v1  v2      vn the comparison operators can be used on tuples  and the ordering is defined lexicographically for example   a1  a2  < =  b1  b2  thesumit67.blogspot.com 3.5 set operations 79 course id cs-101 cs-347 phy-101 figure 3.9 the c1 relation  listing courses taught in fall 2009 is true if a1 < = b1 and a2 < = b2 ; similarly  the two tuples are equal if all their attributes are equal thus  the preceding sql query can be rewritten as follows  5 select name  course id from instructor  teaches where  instructor.id  dept name  =  teaches.id  ? biology ?  ; 3.5 set operations the sql operations union  intersect  and except operate on relations and correspond to the mathematical set-theory operations ?  n  and  we shall now construct queries involving the union  intersect  and except operations over two sets ? the set of all courses taught in the fall 2009 semester  select course id from section where semester = ? fall ? and year = 2009 ; ? the set of all courses taught in the spring 2010 semester  select course id from section where semester = ? spring ? and year = 2010 ; in our discussion that follows,we shall refer to the relations obtained as the result of the preceding queries as c1 and c2  respectively  and show the results when these queries are run on the section relation of figure 2.6 in figures 3.9 and 3.10 observe that c2 contains two tuples corresponding to course id cs-319  since two sections of the course have been offered in spring 2010 5although it is part of the sql-92 standard  some sql implementations may not support this syntax thesumit67.blogspot.com 80 chapter 3 introduction to sql course id cs-101 cs-315 cs-319 cs-319 fin-201 his-351 mu-199 figure 3.10 the c2 relation  listing courses taught in spring 2010 3.5.1 the union operation to find the set of all courses taught either in fall 2009 or in spring 2010  or both  we write  6  select course id from section where semester = ? fall ? and year = 2009  union  select course id from section where semester = ? spring ? and year = 2010  ; the union operation automatically eliminates duplicates  unlike the select clause thus  using the section relation of figure 2.6  where two sections of cs-319 are offered in spring 2010  and a section of cs-101 is offered in the fall 2009 as well as in the fall 2010 semester  cs-101 and cs-319 appear only once in the result  shown in figure 3.11 if we want to retain all duplicates  we must write union all in place of union   select course id from section where semester = ? fall ? and year = 2009  union all  select course id from section where semester = ? spring ? and year = 2010  ; the number of duplicate tuples in the result is equal to the total number of duplicates that appear in both c1 and c2 so  in the above query  each of cs-319 and cs-101 would be listed twice as a further example  if it were the case that 4 sections of ece-101 were taught in the fall 2009 semester and 2 sections of ece-101 6the parentheses we include around each select-from-where statement are optional  but useful for ease of reading thesumit67.blogspot.com 3.5 set operations 81 course id cs-101 cs-315 cs-319 cs-347 fin-201 his-351 mu-199 phy-101 figure 3.11 the result relation for c1 union c2 were taught in the fall 2010 semester  then there would be 6 tuples with ece-101 in the result 3.5.2 the intersect operation to find the set of all courses taught in the fall 2009 as well as in spring 2010 we write   select course id from section where semester = ? fall ? and year = 2009  intersect  select course id from section where semester = ? spring ? and year = 2010  ; the result relation  shown in figure 3.12  contains only one tuple with cs-101 the intersect operation automatically eliminates duplicates for example  if it were the case that 4 sections of ece-101 were taught in the fall 2009 semester and 2 sections of ece-101 were taught in the spring 2010 semester  then there would be only 1 tuple with ece-101 in the result if we want to retain all duplicates  we must write intersect all in place of intersect  course id cs-101 figure 3.12 the result relation for c1 intersect c2 thesumit67.blogspot.com 82 chapter 3 introduction to sql  select course id from section where semester = ? fall ? and year = 2009  intersect all  select course id from section where semester = ? spring ? and year = 2010  ; the number of duplicate tuples that appear in the result is equal to the minimum number of duplicates in both c1 and c2 for example  if 4 sections of ece-101 were taught in the fall 2009 semester and 2 sections of ece-101 were taught in the spring 2010 semester  then there would be 2 tuples with ece-101 in the result 3.5.3 the except operation to find all courses taught in the fall 2009 semester but not in the spring 2010 semester  we write   select course id from section where semester = ? fall ? and year = 2009  except  select course id from section where semester = ? spring ? and year = 2010  ; the result of this query is shown in figure 3.13 note that this is exactly relation c1 of figure 3.9 except that the tuple for cs-101 does not appear the except operation7 outputs all tuples from its first input that do not occur in the second input ; that is  it performs set difference the operation automatically eliminates duplicates in the inputs before performing set difference for example  if 4 sections of ece-101 were taught in the fall 2009 semester and 2 sections of ece-101 were taught in the spring 2010 semester  the result of the except operation would not have any copy of ece-101 if we want to retain duplicates  we must write except all in place of except   select course id from section where semester = ? fall ? and year = 2009  except all  select course id from section where semester = ? spring ? and year = 2010  ; 7some sql implementations  notably oracle  use the keyword minus in place of except thesumit67.blogspot.com 3.6 null values 83 course id cs-347 phy-101 figure 3.13 the result relation for c1 except c2 the number of duplicate copies of a tuple in the result is equal to the number of duplicate copies in c1 minus the number of duplicate copies in c2  provided that the difference is positive thus  if 4 sections of ece-101 were taught in the fall 2009 semester and 2 sections of ece-101 were taught in spring 2010  then there are 2 tuples with ece-101 in the result if  however  there were two or fewer sections of ece-101 in the the fall 2009 semester  and two sections of ece-101 in the spring 2010 semester  there is no tuple with ece-101 in the result 3.6 null values null values present special problems in relational operations  including arithmetic operations  comparison operations  and set operations the result of an arithmetic expression  involving  for example +    *  or /  is null if any of the input values is null for example  if a query has an expression r.a + 5  and r.ais null for a particular tuple  then the expression result must also be null for that tuple comparisons involving nulls are more of a problem for example  consider the comparison ? 1 < null ?  it would be wrong to say this is true since we do not know what the null value represents but it would likewise bewrong to claim this expression is false ; if we did  ? not  1 < null  ? would evaluate to true  which does not make sense sql therefore treats as unknown the result of any comparison involving a null value  other than predicates is null and is not null  which are described later in this section   this creates a third logical value in addition to true and false since the predicate in a where clause can involve boolean operations such as and  or  and not on the results of comparisons  the definitions of the boolean operations are extended to deal with the value unknown ? and  the result of true and unknown is unknown  false and unknown is false  while unknown and unknown is unknown ? or  the result of true or unknown is true  false or unknown is unknown  while unknown or unknown is unknown ? not  the result of not unknown is unknown you can verify that if r.a is null  then ? 1 < r.a ? as well as ? not  1 < r.a  ? evaluate to unknown thesumit67.blogspot.com 84 chapter 3 introduction to sql if the where clause predicate evaluates to either false or unknown for a tuple  that tuple is not added to the result sql uses the special keyword null in a predicate to test for a null value thus  to find all instructors who appear in the instructor relation with null values for salary  we write  select name from instructor where salary is null ; the predicate is not null succeeds if the value on which it is applied is not null some implementations of sql also allow us to testwhether the result of a comparison is unknown  rather than true or false  by using the clauses is unknown and is not unknown when a query uses the select distinct clause  duplicate tuples must be eliminated for this purpose  when comparing values of corresponding attributes from two tuples  the values are treated as identical if either both are non-null and equal in value  or both are null thus two copies of a tuple  such as   ? a ? ,null    ? a ? ,null    are treated as being identical  even if some of the attributes have a null value using the distinct clause then retains only one copy of such identical tuples note that the treatment of null above is different from the way nulls are treated in predicates  where a comparison ? null = null ? would return unknown  rather than true the above approach of treating tuples as identical if they have the same values for all attributes  even if some of the values are null  is also used for the set operations union  intersection and except 3.7 aggregate functions aggregate functions are functions that take a collection  a set or multiset  of values as input and return a single value sql offers five built-in aggregate functions  ? average  avg ? minimum  min ? maximum  max ? total  sum ? count  count the input to sum and avg must be a collection of numbers  but the other operators can operate on collections of nonnumeric data types  such as strings  as well thesumit67.blogspot.com 3.7 aggregate functions 85 3.7.1 basic aggregation consider the query ? find the average salary of instructors in the computer science department ? we write this query as follows  select avg  salary  from instructor where dept name = ? comp sci ? ; the result of this query is a relation with a single attribute  containing a single tuple with a numerical value corresponding to the average salary of instructors in the computer science department the database system may give an arbitrary name to the result relation attribute that is generated by aggregation ; however  we can give a meaningful name to the attribute by using the as clause as follows  select avg  salary  as avg salary from instructor where dept name = ? comp sci ? ; in the instructor relation of figure 2.1  the salaries in the computer science department are $ 75,000  $ 65,000  and $ 92,000 the average balance is $ 232,000/3 = $ 77,333.33 retaining duplicates is important in computing an average suppose the computer science department adds a fourth instructor whose salary happens to be $ 75,000 if duplicates were eliminated  we would obtain the wrong answer  $ 232,000/4 = $ 58.000  rather than the correct answer of $ 76,750 there are cases where we must eliminate duplicates before computing an aggregate function if we do want to eliminate duplicates  we use the keyword distinct in the aggregate expression an example arises in the query ? find the total number of instructors who teach a course in the spring 2010 semester ? in this case  an instructor counts only once  regardless of the number of course sections that the instructor teaches the required information is contained in the relation teaches  and we write this query as follows  select count  distinct id  from teaches where semester = ? spring ? and year = 2010 ; because of the keyword distinct preceding id  even if an instructor teaches more than one course  she is counted only once in the result we use the aggregate function count frequently to count the number of tuples in a relation the notation for this function in sql is count  *   thus  to find the number of tuples in the course relation  we write select count  *  from course ; thesumit67.blogspot.com 86 chapter 3 introduction to sql id name dept name salary 76766 crick biology 72000 45565 katz comp sci 75000 10101 srinivasan comp sci 65000 83821 brandt comp sci 92000 98345 kim elec eng 80000 12121 wu finance 90000 76543 singh finance 80000 32343 el said history 60000 58583 califieri history 62000 15151 mozart music 40000 33456 gold physics 87000 22222 einstein physics 95000 figure 3.14 tuples of the instructor relation  grouped by the dept name attribute sql does not allow the use of distinct with count  *   it is legal to use distinct with max and min  even though the result does not change we can use the keyword all in place of distinct to specify duplicate retention  but  since all is the default  there is no need to do so 3.7.2 aggregation with grouping there are circumstances where we would like to apply the aggregate function not only to a single set of tuples  but also to a group of sets of tuples ; we specify this wish in sql using the group by clause the attribute or attributes given in the group by clause are used to form groups tuples with the same value on all attributes in the group by clause are placed in one group as an illustration  consider the query ? find the average salary in each department ? we write this query as follows  select dept name  avg  salary  as avg salary from instructor group by dept name ; figure 3.14 shows the tuples in the instructor relation grouped by the dept name attribute  which is the first step in computing the query result the specified aggregate is computed for each group  and the result of the query is shown in figure 3.15 in contrast  consider the query ? find the average salary of all instructors ? we write this query as follows  select avg  salary  from instructor ; thesumit67.blogspot.com 3.7 aggregate functions 87 dept name avg salary biology 72000 comp sci 77333 elec eng 80000 finance 85000 history 61000 music 40000 physics 91000 figure 3.15 the result relation for the query ? find the average salary in each department ?  in this case the group by clause has been omitted  so the entire relation is treated as a single group as another example of aggregation on groups of tuples  consider the query ? find the number of instructors in each department who teach a course in the spring 2010 semester ? information about which instructors teach which course sections in which semester is available in the teaches relation however  this information has to be joined with information from the instructor relation to get the department name of each instructor thus  we write this query as follows  select dept name  count  distinct id  as instr count from instructor natural join teaches where semester = ? spring ? and year = 2010 group by dept name ; the result is shown in figure 3.16 when an sql query uses grouping  it is important to ensure that the only attributes that appear in the select statement without being aggregated are those that are present in the group by clause in other words  any attribute that is not present in the group by clause must appear only inside an aggregate function if it appears in the select clause  otherwise the query is treated as erroneous for example  the following query is erroneous since id does not appear in the group by clause  and yet it appears in the select clause without being aggregated  dept name instr count comp sci 3 finance 1 history 1 music 1 figure 3.16 the result relation for the query ? find the number of instructors in each department who teach a course in the spring 2010 semester ? thesumit67.blogspot.com 88 chapter 3 introduction to sql / * erroneous query * / select dept name  id  avg  salary  from instructor group by dept name ; each instructor in a particular group  defined by dept name  can have a different id  and since only one tuple is output for each group  there is no unique way of choosing which id value to output as a result  such cases are disallowed by sql 3.7.3 the having clause at times  it is useful to state a condition that applies to groups rather than to tuples for example  we might be interested in only those departments where the average salary of the instructors is more than $ 42,000 this condition does not apply to a single tuple ; rather  it applies to each group constructed by the group by clause to express such a query  we use the having clause of sql sql applies predicates in the having clause after groups have been formed  so aggregate functions may be used.we express this query in sql as follows  select dept name  avg  salary  as avg salary from instructor group by dept name having avg  salary  > 42000 ; the result is shown in figure 3.17 as was the case for the select clause  any attribute that is present in the having clause without being aggregated must appear in the group by clause  otherwise the query is treated as erroneous the meaning of a query containing aggregation  group by  or having clauses is defined by the following sequence of operations  1 as was the case for queries without aggregation  the from clause is first evaluated to get a relation dept name avg  avg salary  physics 91000 elec eng 80000 finance 85000 comp sci 77333 biology 72000 history 61000 figure 3.17 the result relation for the query ? find the average salary of instructors in those departments where the average salary is more than $ 42,000 ? thesumit67.blogspot.com 3.7 aggregate functions 89 2 if a where clause is present  the predicate in the where clause is applied on the result relation of the from clause 3 tuples satisfying the where predicate are then placed into groups by the group by clause if it is present if the group by clause is absent  the entire set of tuples satisfying the where predicate is treated as being in one group 4 the having clause  if it is present  is applied to each group ; the groups that do not satisfy the having clause predicate are removed 5 the select clause uses the remaining groups to generate tuples of the result of the query  applying the aggregate functions to get a single result tuple for each group to illustrate the use of both a having clause and a where clause in the same query  we consider the query ? for each course section offered in 2009  find the average total credits  tot cred  of all students enrolled in the section  if the section had at least 2 students ? select course id  semester  year  sec id  avg  tot cred  from takes natural join student where year = 2009 group by course id  semester  year  sec id having count  id  > = 2 ; note that all the required information for the preceding query is available from the relations takes and student  and that although the query pertains to sections  a join with section is not needed 3.7.4 aggregation with null and boolean values null values  when they exist  complicate the processing of aggregate operators for example  assume that some tuples in the instructor relation have a null value for salary consider the following query to total all salary amounts  select sum  salary  from instructor ; the values to be summed in the preceding query include null values  since some tuples have a null value for salary rather than say that the overall sum is itself null  the sql standard says that the sum operator should ignore null values in its input in general  aggregate functions treat nulls according to the following rule  all aggregate functions except count  *  ignore null values in their input collection as a result of null values being ignored  the collection of values may be empty the count of an empty collection is defined to be 0  and all other aggregate operations thesumit67.blogspot.com 90 chapter 3 introduction to sql return a value of null when applied on an empty collection the effect of null values on some of the more complicated sql constructs can be subtle a boolean data type that can take values true  false  and unknown  was introduced in sql  1999 the aggregate functions some and every  which mean exactly what you would intuitively expect  can be applied on a collection of boolean values 3.8 nested subqueries sql provides a mechanism for nesting subqueries a subquery is a select-fromwhere expression that is nested within another query a common use of subqueries is to perform tests for set membership  make set comparisons  and determine set cardinality  by nesting subqueries in the where clause we study such uses of nested subqueries in the where clause in sections 3.8.1 through 3.8.4 in section 3.8.5  we study nesting of subqueries in the from clause in section 3.8.7  we see how a class of subqueries called scalar subqueries can appear wherever an expression returning a value can occur 3.8.1 set membership sql allows testing tuples for membership in a relation the in connective tests for set membership  where the set is a collection of values produced by a select clause the not in connective tests for the absence of set membership as an illustration  reconsider the query ? find all the courses taught in the both the fall 2009 and spring 2010 semesters ? earlier  we wrote such a query by intersecting two sets  the set of courses taught in fall 2009 and the set of courses taught in spring 2010.we can take the alternative approach of finding all courses that were taught in fall 2009 and that are also members of the set of courses taught in spring 2010 clearly  this formulation generates the same results as the previous one did  but it leads us towrite our query using the in connective of sql we begin by finding all courses taught in spring 2010  and we write the subquery  select course id from section where semester = ? spring ? and year = 2010  we then need to find those courses that were taught in the fall 2009 and that appear in the set of courses obtained in the subquery we do so by nesting the subquery in the where clause of an outer query the resulting query is select distinct course id from section where semester = ? fall ? and year = 2009 and course id in  select course id from section where semester = ? spring ? and year = 2010  ; thesumit67.blogspot.com 3.8 nested subqueries 91 this example shows that it is possible towrite the same query severalways in sql this flexibility is beneficial  since it allows a user to think about the query in the way that seems most natural we shall see that there is a substantial amount of redundancy in sql we use the not in construct in a way similar to the in construct for example  to find all the courses taught in the fall 2009 semester but not in the spring 2010 semester  we can write  select distinct course id from section where semester = ? fall ? and year = 2009 and course id not in  select course id from section where semester = ? spring ? and year = 2010  ; the in and not in operators can also be used on enumerated sets the following query selects the names of instructors whose names are neither ? mozart ? nor ? einstein ?  select distinct name from instructor where name not in  ? mozart ?  ? einstein ?  ; in the preceding examples  we testedmembership in a one-attribute relation it is also possible to test for membership in an arbitrary relation in sql for example,we canwrite the query ? find the total number of  distinct  students who have taken course sections taught by the instructor with id 110011 ? as follows  select count  distinct id  from takes where  course id  sec id  semester  year  in  select course id  sec id  semester  year from teaches where teaches.id = 10101  ; 3.8.2 set comparison as an example of the ability of a nested subquery to compare sets  consider the query ? find the names of all instructors whose salary is greater than at least one instructor in the biology department ? in section 3.4.1  we wrote this query as follows  select distinct t.name from instructor as t  instructor as s where t.salary > s.salary and s.dept name = ? biology ? ; thesumit67.blogspot.com 92 chapter 3 introduction to sql sql does  however  offer an alternative style for writing the preceding query the phrase ? greater than at least one ? is represented in sql by > some this construct allows us to rewrite the query in a form that resembles closely our formulation of the query in english select name from instructor where salary > some  select salary from instructor where dept name = ? biology ?  ; the subquery   select salary from instructor where dept name = ? biology ?  generates the set of all salary values of all instructors in the biology department the > some comparison in the where clause of the outer select is true if the salary value of the tuple is greater than at least one member of the set of all salary values for instructors in biology sql also allows < some  < = some  > = some  = some  and < > some comparisons as an exercise  verify that = some is identical to in  whereas < > some is not the same as not in.8 now we modify our query slightly let us find the names of all instructors that have a salary value greater than that of each instructor in the biology department the construct > all corresponds to the phrase ? greater than all ? using this construct  we write the query as follows  select name from instructor where salary > all  select salary from instructor where dept name = ? biology ?  ; as it does for some  sql also allows < all  < = all  > = all  = all  and < > all comparisons as an exercise  verify that < > all is identical to not in  whereas = all is not the same as in as another example of set comparisons  consider the query ? find the departments that have the highest average salary ? we begin by writing a query to find all average salaries  and then nest it as a subquery of a larger query that finds 8the keyword any is synonymous to some in sql early versions of sql allowed only any later versions added the alternative some to avoid the linguistic ambiguity of the word any in english thesumit67.blogspot.com 3.8 nested subqueries 93 those departments for which the average salary is greater than or equal to all average salaries  select dept name from instructor group by dept name having avg  salary  > = all  select avg  salary  from instructor group by dept name  ; 3.8.3 test for empty relations sql includes a feature for testing whether a subquery has any tuples in its result the exists construct returns the value true if the argument subquery is nonempty using the exists construct  we canwrite the query ? find all courses taught in both the fall 2009 semester and in the spring 2010 semester ? in still another way  select course id from section as s where semester = ? fall ? and year = 2009 and exists  select * from section as t where semester = ? spring ? and year = 2010 and s.course id = t.course id  ; the above query also illustrates a feature of sql where a correlation name from an outer query  s in the above query   can be used in a subquery in the where clause a subquery that uses a correlation name from an outer query is called a correlated subquery in queries that contain subqueries  a scoping rule applies for correlation names in a subquery  according to the rule  it is legal to use only correlation names defined in the subquery itself or in any query that contains the subquery if a correlation name is defined both locally in a subquery and globally in a containing query  the local definition applies this rule is analogous to the usual scoping rules used for variables in programming languages we can test for the nonexistence of tuples in a subquery by using the not exists construct.we can use the not exists construct to simulate the set containment  that is  superset  operation  we canwrite ? relation acontains relation b ? as ? not exists  b except a   ?  although it is not part of the current sql standards  the contains operator was present in some early relational systems  to illustrate the not exists operator  consider the query ? find all studentswho have taken all courses offered in the biology department ? using the except construct  we can write the query as follows  thesumit67.blogspot.com 94 chapter 3 introduction to sql select distinct s.id  s.name from student as s where not exists   select course id from course where dept name = ? biology ?  except  select t.course id from takes as t where s.id = t.id   ; here  the subquery   select course id from course where dept name = ? biology ?  finds the set of all courses offered in the biology department the subquery   select t.course id from takes as t where s.id = t.id  finds all the courses that student s.id has taken thus  the outer select takes each student and testswhether the set of all courses that the student has taken contains the set of all courses offered in the biology department 3.8.4 test for the absence of duplicate tuples sql includes a boolean function for testing whether a subquery has duplicate tuples in its result the unique construct9 returns the value true if the argument subquery contains no duplicate tuples using the unique construct  we can write the query ? find all courses that were offered at most once in 2009 ? as follows  select t.course id from course as t where unique  select r.course id from section as r where t.course id = r.course id and r.year = 2009  ; note that if a course is not offered in 2009  the subquery would return an empty result  and the unique predicate would evaluate to true on the empty set an equivalent version of the above query not using the unique construct is  9this construct is not yet widely implemented thesumit67.blogspot.com 3.8 nested subqueries 95 select t.course id from course as t where 1 < =  select count  r.course id  from section as r where t.course id = r.course id and r.year = 2009  ; we can test for the existence of duplicate tuples in a subquery by using the not unique construct to illustrate this construct  consider the query ? find all courses that were offered at least twice in 2009 ? as follows  select t.course id from course as t where not unique  select r.course id from section as r where t.course id = r.course id and r.year = 2009  ; formally  the unique test on a relation is defined to fail if and only if the relation contains two tuples t1 and t2 such that t1 = t2 since the test t1 = t2 fails if any of the fields of t1 or t2 are null  it is possible for unique to be true even if there are multiple copies of a tuple  as long as at least one of the attributes of the tuple is null 3.8.5 subqueries in the from clause sql allows a subquery expression to be used in the from clause the key concept applied here is that any select-from-where expression returns a relation as a result and  therefore  can be inserted into another select-from-where anywhere that a relation can appear consider the query ? find the average instructors ? salaries of those departments where the average salary is greater than $ 42,000 ? we wrote this query in section 3.7 by using the having clause we can now rewrite this query  without using the having clause  by using a subquery in the from clause  as follows  select dept name  avg salary from  select dept name  avg  salary  as avg salary from instructor group by dept name  where avg salary > 42000 ; the subquery generates a relation consisting of the names of all departments and their corresponding average instructors ? salaries the attributes of the subquery result can be used in the outer query  as can be seen in the above example thesumit67.blogspot.com 96 chapter 3 introduction to sql note that we do not need to use the having clause  since the subquery in the from clause computes the average salary  and the predicate that was in the having clause earlier is now in the where clause of the outer query we can give the subquery result relation a name  and rename the attributes  using the as clause  as illustrated below select dept name  avg salary from  select dept name  avg  salary  from instructor group by dept name  as dept avg  dept name  avg salary  where avg salary > 42000 ; the subquery result relation is named dept avg  with the attributes dept name and avg salary nested subqueries in the from clause are supported by most but not all sql implementations however  some sql implementations  notably oracle  do not support renaming of the result relation in the from clause as another example  suppose we wish to find the maximum across all departments of the total salary at each department the having clause does not help us in this task  but we can write this query easily by using a subquery in the from clause  as follows  select max  tot salary  from  select dept name  sum  salary  from instructor group by dept name  as dept total  dept name  tot salary  ; we note that nested subqueries in the from clause can not use correlation variables from other relations in the from clause however  sql  2003 allows a subquery in the from clause that is prefixed by the lateral keyword to access attributes of preceding tables or subqueries in the from clause for example  if we wish to print the names of each instructor  along with their salary and the average salary in their department  we could write the query as follows  select name  salary  avg salary from instructor i1  lateral  select avg  salary  as avg salary from instructor i2 where i2.dept name = i1.dept name  ; without the lateral clause  the subquery can not access the correlation variable i1 from the outer query currently  only a few sql implementations  such as ibm db2  support the lateral clause thesumit67.blogspot.com 3.8 nested subqueries 97 3.8.6 the with clause the with clause provides away of defining a temporary relation whose definition is available only to the query in which the with clause occurs consider the following query  which finds those departments with the maximum budget with max budget  value  as  select max  budget  from department  select budget from department  max budget where department.budget = max budget.value ; the with clause defines the temporary relation max budget  which is used in the immediately following query the with clause  introduced in sql  1999  is supported by many  but not all  database systems we could have written the above query by using a nested subquery in either the from clause or the where clause however  using nested subqueries would have made the query harder to read and understand the with clause makes the query logic clearer ; it also permits a view definition to be used in multiple places within a query for example  suppose we want to find all departments where the total salary is greater than the average of the total salary at all departments.we can write the query using the with clause as follows with dept total  dept name  value  as  select dept name  sum  salary  from instructor group by dept name   dept total avg  value  as  select avg  value  from dept total  select dept name from dept total  dept total avg where dept total.value > = dept total avg.value ; we can  of course  create an equivalent query without the with clause  but it would be more complicated and harder to understand you can write the equivalent query as an exercise 3.8.7 scalar subqueries sql allows subqueries to occur wherever an expression returning a value is permitted  provided the subquery returns only one tuple containing a single attribute ; such subqueries are called scalar subqueries for example  a subquery thesumit67.blogspot.com 98 chapter 3 introduction to sql can be used in the select clause as illustrated in the following example that lists all departments along with the number of instructors in each department  select dept name   select count  *  from instructor where department.dept name = instructor.dept name  as num instructors from department ; the subquery in the above example is guaranteed to return only a single value since it has a count  *  aggregate without a group by the example also illustrates the usage of correlation variables  that is  attributes of relations in the from clause of the outer query  such as department.dept name in the above example scalar subqueries can occur in select  where  and having clauses scalar subqueries may also be definedwithout aggregates it is not always possible to figure out at compile time if a subquery can return more than one tuple in its result ; if the result has more than one tuple when the subquery is executed  a run-time error occurs note that technically the type of a scalar subquery result is still a relation  even if it contains a single tuple however  when a scalar subquery is used in an expression where a value is expected  sql implicitly extracts the value from the single attribute of the single tuple in the relation  and returns that value 3.9 modification of the database we have restricted our attention until now to the extraction of information from the database.now  we show how to add  remove  or change informationwith sql 3.9.1 deletion adelete request is expressed inmuch the sameway as a query.we can delete only whole tuples ; we can not delete values on only particular attributes sql expresses a deletion by delete from r where p ; where p represents a predicate and r represents a relation the delete statement first finds all tuples t in r for which p  t  is true  and then deletes them from r the where clause can be omitted  in which case all tuples in r are deleted note that a delete command operates on only one relation ifwewant to delete tuples from several relations,we must use one delete command for each relation the predicate in the where clause may be as complex as a select command ? s where clause at the other extreme  the where clause may be empty the request thesumit67.blogspot.com 3.9 modification of the database 99 delete from instructor ; deletes all tuples from the instructor relation the instructor relation itself still exists  but it is empty here are examples of sql delete requests  ? delete all tuples in the instructor relation pertaining to instructors in the finance department delete from instructor where dept name = ? finance ? ; ? delete all instructors with a salary between $ 13,000 and $ 15,000 delete from instructor where salary between 13000 and 15000 ; ? delete all tuples in the instructor relation for those instructors associatedwith a department located in the watson building delete from instructor where dept name in  select dept name from department where building = ? watson ?  ; this delete request first finds all departments located in watson  and then deletes all instructor tuples pertaining to those departments note that  although we may delete tuples from only one relation at a time  we may reference any number of relations in a select-from-where nested in the where clause of a delete the delete request can contain a nested select that references the relation fromwhich tuples are to be deleted for example  suppose that we want to delete the records of all instructors with salary below the average at the university.we could write  delete from instructor where salary <  select avg  salary  from instructor  ; the delete statement first tests each tuple in the relation instructor to check whether the salary is less than the average salary of instructors in the university then  all tuples that fail the test ? that is  represent an instructor with a lower-than-average salary ? are deleted performing all the tests before performing any deletion is important ? if some tuples are deleted before other tuples thesumit67.blogspot.com 100 chapter 3 introduction to sql have been tested  the average salarymay change  and the final result of the delete would depend on the order in which the tuples were processed ! 3.9.2 insertion to insert data into a relation  we either specify a tuple to be inserted or write a querywhose result is a set of tuples to be inserted.obviously  the attribute values for inserted tuples must be members of the corresponding attribute ? s domain similarly  tuples inserted must have the correct number of attributes the simplest insert statement is a request to insert one tuple suppose that we wish to insert the fact that there is a course cs-437 in the computer science department with title ? database systems ?  and 4 credit hours.we write  insert into course values  ? cs-437 ?  ? database systems ?  ? comp sci ?  4  ; in this example  the values are specified in the order in which the corresponding attributes are listed in the relation schema for the benefit of users who may not remember the order of the attributes  sql allows the attributes to be specified as part of the insert statement for example  the following sql insert statements are identical in function to the preceding one  insert into course  course id  title  dept name  credits  values  ? cs-437 ?  ? database systems ?  ? comp sci ?  4  ; insert into course  title  course id  credits  dept name  values  ? database systems ?  ? cs-437 ?  4  ? comp sci ?  ; more generally  we might want to insert tuples on the basis of the result of a query suppose that we want to make each student in the music department who has earned more than 144 credit hours  an instructor in the music department  with a salary of $ 18,000 we write  insert into instructor select id  name  dept name  18000 from student where dept name = ? music ? and tot cred > 144 ; instead of specifying a tuple as we did earlier in this section  we use a select to specify a set of tuples sql evaluates the select statement first  giving a set of tuples that is then inserted into the instructor relation each tuple has an id  a name  a dept name  music   and an salary of $ 18,000 it is important that we evaluate the select statement fully before we carry out any insertions if we carry out some insertions even as the select statement is being evaluated  a request such as  thesumit67.blogspot.com 3.9 modification of the database 101 insert into student select * from student ; might insert an infinite number of tuples  if the primary key constraint on student were absent without the primary key constraint  the request would insert the first tuple in student again  creating a second copy of the tuple since this second copy is part of student now  the select statement may find it  and a third copy would be inserted into student the select statement may then find this third copy and insert a fourth copy  and so on  forever evaluating the select statement completely before performing insertions avoids such problems thus  the above insert statement would simply duplicate every tuple in the student relation  if the relation did not have a primary key constraint our discussion of the insert statement considered only examples in which a value is given for every attribute in inserted tuples it is possible for inserted tuples to be given values on only some attributes of the schema the remaining attributes are assigned a null value denoted by null consider the request  insert into student values  ? 3003 ?  ? green ?  ? finance ?  null  ; the tuple inserted by this request specified that a student with id ? 3003 ? is in the finance department  but the tot cred value for this student is not known consider the query  select student from student where tot cred > 45 ; since the tot cred value of student ? 3003 ? is not known  we can not determine whether it is greater than 45 most relational database products have special ? bulk loader ? utilities to insert a large set of tuples into a relation these utilities allow data to be read from formatted text files  and can execute much faster than an equivalent sequence of insert statements 3.9.3 updates in certain situations  we may wish to change a value in a tuple without changing all values in the tuple for this purpose  the update statement can be used as we could for insert and delete  we can choose the tuples to be updated by using a query suppose that annual salary increases are being made  and salaries of all instructors are to be increased by 5 percent.we write  thesumit67.blogspot.com 102 chapter 3 introduction to sql update instructor set salary = salary * 1.05 ; the preceding update statement is applied once to each of the tuples in instructor relation if a salary increase is to be paid only to instructors with salary of less than $ 70,000  we can write  update instructor set salary = salary * 1.05 where salary < 70000 ; in general  the where clause of the update statement may contain any construct legal in the where clause of the select statement  including nested selects   as with insert and delete  a nested select within an update statementmay reference the relation that is being updated as before  sql first tests all tuples in the relation to see whether they should be updated  and carries out the updates afterward for example,we can write the request ? give a 5 percent salary raise to instructors whose salary is less than average ? as follows  update instructor set salary = salary * 1.05 where salary <  select avg  salary  from instructor  ; let us now suppose that all instructors with salary over $ 100,000 receive a 3 percent raise  whereas all others receive a 5 percent raise we could write two update statements  update instructor set salary = salary * 1.03 where salary > 100000 ; update instructor set salary = salary * 1.05 where salary < = 100000 ; note that the order of the two update statements is important if we changed the order of the two statements  an instructor with a salary just under $ 100,000 would receive an over 8 percent raise sql provides a case construct that we can use to perform both the updates with a single update statement  avoiding the problem with the order of updates thesumit67.blogspot.com 3.9 modification of the database 103 update instructor set salary = case when salary < = 100000 then salary * 1.05 else salary * 1.03 end the general form of the case statement is as follows case when pred1 then result1 when pred2 then result2    when predn then resultn else result0 end the operation returns resulti  where i is the first of pred1  pred2      predn that is satisfied ; if none of the predicates is satisfied  the operation returns result0 case statements can be used in any place where a value is expected scalar subqueries are also useful in sql update statements,where they can be used in the set clause consider an update where we set the tot cred attribute of each student tuple to the sum of the credits of courses successfully completed by the student.we assume that a course is successfully completed if the student has a grade that is not ? f ? or null to specify this update  we need to use a subquery in the set clause  as shown below  update student s set tot cred =  select sum  credits  from takes natural join course where s.id = takes.id and takes.grade < > ? f ? and takes.grade is not null  ; observe that the subquery uses a correlation variable s fromthe update statement in case a student has not successfully completed any course  the above update statement would set the tot cred attribute value to null to set the value to 0 instead  we could use another update statement to replace null values by 0 ; a better alternative is to replace the clause ? select sum  credits  ? in the preceding subquery by the following select clause using a case expression  select case when sum  credits  is not null then sum  credits  else 0 end thesumit67.blogspot.com 104 chapter 3 introduction to sql 3.10 summary ? sql is the most influential commercially marketed relational query language the sql language has several parts  ? data-definition language  ddl   which provides commands for defining relation schemas  deleting relations  and modifying relation schemas ? data-manipulation language  dml   which includes a query language and commands to insert tuples into  delete tuples from  and modify tuples in the database ? the sql data-definition language is used to create relations with specified schemas in addition to specifying the names and types of relation attributes  sql also allows the specification of integrity constraints such as primary-key constraints and foreign-key constraints ? sql includes a variety of language constructs for queries on the database these include the select  from  andwhere clauses  and support for the natural join operation ? sql also provides mechanisms to rename both attributes and relations  and to order query results by sorting on specified attributes ? sql supports basic set operations on relations including union  intersect  and except  which correspond to the mathematical set-theory operations ?  n  and  ? sql handles queries on relations containing null values by adding the truth value ? unknown ? to the usual truth values of true and false ? sql supports aggregation  including the ability to divide a relation into groups  applying aggregation separately on each group sql also supports set operations on groups ? sql supports nested subqueries in the where  and from clauses of an outer query it also supports scalar subqueries  wherever an expression returning a value is permitted ? sql provides constructs for updating  inserting  and deleting information review terms ? data-definition language ? data-manipulation language ? database schema ? database instance ? relation schema ? relation instance ? primary key ? foreign key ? referencing relation ? referenced relation thesumit67.blogspot.com practice exercises 105 ? null value ? query language ? sql query structure ? select clause ? from clause ? where clause ? natural join operation ? as clause ? order by clause ? correlation name  correlation variable  tuple variable  ? set operations ? union ? intersect ? except ? null values ? truth value ? unknown ? ? aggregate functions ? avg  min  max  sum  count ? group by ? having ? nested subqueries ? set comparisons ?  <  < =  >  > =   some  all  ? exists ? unique ? lateral clause ? with clause ? scalar subquery ? database modification ? deletion ? insertion ? updating practice exercises 3.1 write the following queries in sql  using the university schema  we suggest you actually run these queries on a database  using the sample data that we provide on the web site of the book  db-book.com instructions for setting up a database  and loading sample data  are provided on the above web site  a find the titles of courses in the comp sci department that have 3 credits b find the ids of all students who were taught by an instructor named einstein ; make sure there are no duplicates in the result c find the highest salary of any instructor d find all instructors earning the highest salary  there may be more than one with the same salary   e find the enrollment of each section that was offered in autumn 2009 f find the maximum enrollment  across all sections  in autumn 2009 g find the sections that had themaximum enrollment in autumn 2009 thesumit67.blogspot.com 106 chapter 3 introduction to sql person  driver id  name  address  car  license  model  year  accident  report number  date  location  owns  driver id  license  participated  report number  license  driver id  damage amount  figure 3.18 insurance database for exercises 3.4 and 3.14 3.2 suppose you are given a relation grade points  grade  points   which provides a conversion from letter grades in the takes relation to numeric scores ; for example an ? a ? grade could be specified to correspond to 4 points  an ? a ? to 3.7 points  a ? b + ? to 3.3 points  a ? b ? to 3 points  and so on the grade points earned by a student for a course offering  section  is defined as the number of credits for the course multiplied by the numeric points for the grade that the student received given the above relation  and our university schema  write each of the following queries in sql you can assume for simplicity that no takes tuple has the null value for grade a find the total grade-points earned by the student with id 12345  across all courses taken by the student b find the grade-point average  gpa  for the above student  that is  the total grade-points divided by the total credits for the associated courses c find the id and the grade-point average of every student 3.3 write the following inserts  deletes or updates in sql  using the university schema a increase the salary of each instructor in the comp sci department by 10 %  b delete all courses that have never been offered  that is  do not occur in the section relation   c insert every student whose tot cred attribute is greater than 100 as an instructor in the same department  with a salary of $ 10,000 3.4 consider the insurance database of figure 3.18  where the primary keys are underlined construct the following sql queries for this relational database a find the total number of people who owned cars that were involved in accidents in 2009 b add a new accident to the database ; assume any values for required attributes c delete the mazda belonging to ? john smith ?  thesumit67.blogspot.com practice exercises 107 branch  branch name  branch city  assets  customer  customer name  customer street  customer city  loan  loan number  branch name  amount  borrower  customer name  loan number  account  account number  branch name  balance  depositor  customer name  account number  figure 3.19 banking database for exercises 3.8 and 3.15 3.5 suppose that we have a relation marks  id  score  and we wish to assign grades to students based on the score as follows  grade f if score < 40  grade c if 40 = score < 60  grade b if 60 = score < 80  and grade a if 80 = score write sql queries to do the following  a display the grade for each student  based on the marks relation b find the number of students with each grade 3.6 the sql like operator is case sensitive  but the lower   function on strings can be used to perform case insensitive matching to show how  write a query that finds departments whose names contain the string ? sci ? as a substring  regardless of the case 3.7 consider the sql query select distinct p.a1 from p  r1  r2 where p.a1 = r1.a1 or p.a1 = r2.a1 under what conditions does the preceding query select values of p.a1 that are either in r1 or in r2 ? examine carefully the cases where one of r1 or r2 may be empty 3.8 consider the bank database of figure 3.19  where the primary keys are underlined construct the following sql queries for this relational database a find all customers of the bank who have an account but not a loan b find the names of all customers who live on the same street and in the same city as ? smith ?  c find the names of all branches with customers who have an account in the bank and who live in ? harrison ?  3.9 consider the employee database of figure 3.20  where the primary keys are underlined give an expression in sql for each of the following queries a find the names and cities of residence of all employeeswho work for ? first bank corporation ?  thesumit67.blogspot.com 108 chapter 3 introduction to sql employee  employee name  street  city  works  employee name  company name  salary  company  company name  city  manages  employee name  manager name  figure 3.20 employee database for exercises 3.9  3.10  3.16  3.17  and 3.20 b find the names  street addresses  and cities of residence of all employees who work for ? first bank corporation ? and earn more than $ 10,000 c find all employees in the database who do not work for ? first bank corporation ?  d findall employees in thedatabasewho earnmore than each employee of ? small bank corporation ?  e assume that the companies may be located in several cities find all companies located in every city in which ? small bank corporation ? is located f find the company that has the most employees g find those companies whose employees earn a higher salary  on average  than the average salary at ? first bank corporation ?  3.10 consider the relational database of figure 3.20 give an expression in sql for each of the following queries a modify the database so that ? jones ? now lives in ? newtown ?  b give all managers of ? first bank corporation ? a 10 percent raise unless the salary becomes greater than $ 100,000 ; in such cases  give only a 3 percent raise exercises 3.11 write the following queries in sql  using the university schema a find the names of all studentswho have taken at least one comp sci course ; make sure there are no duplicate names in the result b find the ids and names of all students who have not taken any course offering before spring 2009 c for each department  find the maximum salary of instructors in that department you may assume that every department has at least one instructor d find the lowest  across all departments  of the per-department maximum salary computed by the preceding query thesumit67.blogspot.com exercises 109 3.12 write the following queries in sql  using the university schema a create a new course ? cs-001 ?  titled ? weekly seminar ? ,with 0 credits b create a section of this course in autumn 2009  with sec id of 1 c enroll every student in the comp sci department in the above section d delete enrollments in the above section where the student ? s name is chavez e delete the course cs-001 what will happen if you run this delete statement without first deleting offerings  sections  of this course f delete all takes tuples corresponding to any section of any course with the word ? database ? as a part of the title ; ignore case when matching the word with the title 3.13 write sql ddl corresponding to the schema in figure 3.18 make any reasonable assumptions about data types  and be sure to declare primary and foreign keys 3.14 consider the insurance database of figure 3.18  where the primary keys are underlined construct the following sql queries for this relational database a find the number of accidents in which the cars belonging to ? john smith ? were involved b update the damage amount for the car with the license number ? aabb2000 ? in the accident with report number ? ar2197 ? to $ 3000 3.15 consider the bank database of figure 3.19  where the primary keys are underlined construct the following sql queries for this relational database a find all customers who have an account at all the branches located in ? brooklyn ?  b find out the total sum of all loan amounts in the bank c find the names of all branches that have assets greater than those of at least one branch located in ? brooklyn ?  3.16 consider the employee database of figure 3.20  where the primary keys are underlined give an expression in sql for each of the following queries a find the names of all employees who work for ? first bank corporation ?  b find all employees in the database who live in the same cities as the companies for which they work c find all employees in the database who live in the same cities and on the same streets as do theirmanagers thesumit67.blogspot.com 110 chapter 3 introduction to sql d find all employees who earn more than the average salary of all employees of their company e find the company that has the smallest payroll 3.17 consider the relational database of figure 3.20 give an expression in sql for each of the following queries a give all employees of ? first bank corporation ? a 10 percent raise b give all managers of ? first bank corporation ? a 10 percent raise c delete all tuples in the works relation for employees of ? small bank corporation ?  3.18 list two reasons why null values might be introduced into the database 3.19 show that  in sql  < > all is identical to not in 3.20 give an sql schema definition for the employee database of figure 3.20 choose an appropriate domain for each attribute and an appropriate primary key for each relation schema 3.21 consider the library database of figure 3.21.write the following queries in sql a print the names ofmembers who have borrowed any book published by ? mcgraw-hill ?  b print the names ofmembers who have borrowed all books published by ? mcgraw-hill ?  c for each publisher  print the names of members who have borrowed more than five books of that publisher d print the average number of books borrowed per member take into account that if an member does not borrow any books  then that member does not appear in the borrowed relation at all 3.22 rewrite the where clause where unique  select title from course  without using the unique construct member  memb no  name  age  book  isbn  title  authors  publisher  borrowed  memb no  isbn  date  figure 3.21 library database for exercise 3.21 thesumit67.blogspot.com tools 111 3.23 consider the query  select course id  semester  year  sec id  avg  tot cred  from takes natural join student where year = 2009 group by course id  semester  year  sec id having count  id  > = 2 ; explain why joining section as well in the from clause would not change the result 3.24 consider the query  with dept total  dept name  value  as  select dept name  sum  salary  from instructor group by dept name   dept total avg  value  as  select avg  value  from dept total  select dept name from dept total  dept total avg where dept total.value > = dept total avg.value ; rewrite this query without using the with construct tools a number of relational database systems are available commercially  including ibm db2  ibm informix  oracle  sybase  and microsoft sql server in addition several database systems can be downloaded and used free of charge  including postgresql  mysql  free except for certain kinds of commercial use   and oracle express edition most database systems provide a command line interface for submitting sql commands in addition  most databases also provide graphical user interfaces  guis   which simplify the task of browsing the database  creating and submitting queries  and administering thedatabase.commercial ides for sqlthat work across multiple database platforms  include embarcadero ? s rad studio and aqua data studio for postgresql  thepgadmintool provides gui functionality  while formysql  phpmyadmin provides gui functionality the netbeans ide provides a gui front end thatworks with a number of different databases  butwith limited functionality  while the eclipse ide supports similar functionality through several different plugins such as the data tools platform  dtp  and jbuilder sql schema definitions and sample data for the university schema are provided on the web site for this book  db-book.com the web site also contains thesumit67.blogspot.com 112 chapter 3 introduction to sql instructions on how to set up and access some popular database systems the sql constructs discussed in this chapter are part of the sql standard  but certain features are not supported by some databases the web site lists these incompatibilities  which you will need to take into account when executing queries on those databases bibliographical notes the original version of sql  called sequel 2  is described by chamberlin et al  1976   sequel 2 was derived from the language square  boyce et al  1975  and chamberlin and boyce  1974    the american national standard sql-86 is described in ansi  1986   the ibm systems application architecture definition of sql is defined by ibm  1987   the official standards for sql-89 and sql-92 are available as ansi  1989  and ansi  1992   respectively textbook descriptions of the sql-92 language includedate and darwen  1997   melton and simon  1993   and cannan and otten  1993   date and darwen  1997  and date  1993a  include a critique of sql-92 from a programming-languages perspective textbooks on sql  1999 include melton and simon  2001  and melton  2002   eisenberg and melton  1999  provide an overview of sql  1999 donahoo and speegle  2005  covers sql from a developers ? perspective eisenberg et al  2004  provides an overview of sql  2003 the sql  1999  sql  2003  sql  2006 and sql  2008 standards are published as a collection of iso/iec standards documents  which are described in more detail in section 24.4 the standards documents are densely packed with information and hard to read  and of use primarily for database system implementers the standards documents are available from the web site http  //webstore.ansi.org  but only for purchase many database products support sql features beyond those specified in the standard  and may not support some features of the standard more information on these featuresmay be found in the sql usermanuals of the respective products the processing of sql queries  including algorithms and performance issues  is discussed in chapters 12 and 13 bibliographic references on these matters appear in those chapters thesumit67.blogspot.com chapter4 intermediate sql in this chapter  we continue our study of sql we consider more complex forms of sql queries  view definition  transactions  integrity constraints  more details regarding sql data definition  and authorization 4.1 join expressions in section 3.3.3  we introduced the natural join operation sql provides other forms of the join operation  including the ability to specify an explicit join predicate  and the ability to include in the result tuples that are excluded by natural join.we shall discuss these forms of join in this section the examples in this section involve the two relations student and takes  shown in figures 4.1 and 4.2  respectively observe that the attribute grade has a value null for the student with id 98988  for the course bio-301  section 1  taken in summer 2010 the null value indicates that the grade has not been awarded yet id name dept name tot cred 00128 zhang comp sci 102 12345 shankar comp sci 32 19991 brandt history 80 23121 chavez finance 110 44553 peltier physics 56 45678 levy physics 46 54321 williams comp sci 54 55739 sanchez music 38 70557 snow physics 0 76543 brown comp sci 58 76653 aoi elec eng 60 98765 bourikas elec eng 98 98988 tanaka biology 120 figure 4.1 the student relation 113 thesumit67.blogspot.com 114 chapter 4 intermediate sql id course id sec id semester year grade 00128 cs-101 1 fall 2009 a 00128 cs-347 1 fall 2009 a 12345 cs-101 1 fall 2009 c 12345 cs-190 2 spring 2009 a 12345 cs-315 1 spring 2010 a 12345 cs-347 1 fall 2009 a 19991 his-351 1 spring 2010 b 23121 fin-201 1 spring 2010 c + 44553 phy-101 1 fall 2009 b 45678 cs-101 1 fall 2009 f 45678 cs-101 1 spring 2010 b + 45678 cs-319 1 spring 2010 b 54321 cs-101 1 fall 2009 a 54321 cs-190 2 spring 2009 b + 55739 mu-199 1 spring 2010 a 76543 cs-101 1 fall 2009 a 76543 cs-319 2 spring 2010 a 76653 ee-181 1 spring 2009 c 98765 cs-101 1 fall 2009 c 98765 cs-315 1 spring 2010 b 98988 bio-101 1 summer 2009 a 98988 bio-301 1 summer 2010 null figure 4.2 the takes relation 4.1.1 join conditions in section 3.3.3  we saw how to express natural joins  and we saw the join    using clause  which is a form of natural join that only requires values to match on specified attributes sql supports another form of join  in which an arbitrary join condition can be specified the on condition allows a general predicate over the relations being joined this predicate is written like a where clause predicate except for the use of the keyword on rather than where like the using condition  the on condition appears at the end of the join expression consider the following query  which has a join expression containing the on condition select * from student join takes on student.id = takes.id ; the on condition above specifies that a tuple from student matches a tuple from takes if their id values are equal the join expression in this case is almost the same as the join expression student natural join takes  since the natural join operation thesumit67.blogspot.com 4.1 join expressions 115 also requires that for a student tuple and a takes tuple tomatch the one difference is that the result has the id attribute listed twice  in the join result  once for student and once for takes  even though their id values must be the same in fact  the above query is equivalent to the following query  in other words  they generate exactly the same results   select * from student  takes where student.id = takes.id ; aswe have seen earlier  the relation nameisused todisambiguate the attribute name id  and thus the two occurrences can be referred to as student.id and takes.id respectively a version of this query that displays the id value only once is as follows  select student.id as id  name  dept name  tot cred  course id  sec id  semester  year  grade from student join takes on student.id = takes.id ; the result of the above query is shown in figure 4.3 the on condition can express any sql predicate  and thus a join expressions using the on condition can express a richer class of join conditions than natural join however  as illustrated by our preceding example  a query using a join expression with an on condition can be replaced by an equivalent expression without the on condition  with the predicate in the on clause moved to the where clause thus  it may appear that the on condition is a redundant feature of sql however  there are two good reasons for introducing the on condition first  we shall see shortly that for a kind of join called an outer join  on conditions do behave in a manner different from where conditions second  an sql query is often more readable by humans if the join condition is specified in the on clause and the rest of the conditions appear in the where clause 4.1.2 outer joins suppose we wish to display a list of all students  displaying their id  and name  dept name  and tot cred  along with the courses that they have taken the following sql query may appear to retrieve the required information  select * from student natural join takes ; unfortunately  the above query does not work quite as intended suppose that there is some student who takes no courses then the tuple in the student relation for that particular student would not satisfy the condition of a natural join with any tuple in the takes relation  and that student ? s data would not appear in the result.wewould thus not see any information about studentswho have not taken thesumit67.blogspot.com 116 chapter 4 intermediate sql id name dept name tot cred course id sec id semester year grade 00128 zhang comp sci 102 cs-101 1 fall 2009 a 00128 zhang comp sci 102 cs-347 1 fall 2009 a 12345 shankar comp sci 32 cs-101 1 fall 2009 c 12345 shankar comp sci 32 cs-190 2 spring 2009 a 12345 shankar comp sci 32 cs-315 1 spring 2010 a 12345 shankar comp sci 32 cs-347 1 fall 2009 a 19991 brandt history 80 his-351 1 spring 2010 b 23121 chavez finance 110 fin-201 1 spring 2010 c + 44553 peltier physics 56 phy-101 1 fall 2009 b 45678 levy physics 46 cs-101 1 fall 2009 f 45678 levy physics 46 cs-101 1 spring 2010 b + 45678 levy physics 46 cs-319 1 spring 2010 b 54321 williams comp sci 54 cs-101 1 fall 2009 a 54321 williams comp sci 54 cs-190 2 spring 2009 b + 55739 sanchez music 38 mu-199 1 spring 2010 a 76543 brown comp sci 58 cs-101 1 fall 2009 a 76543 brown comp sci 58 cs-319 2 spring 2010 a 76653 aoi elec eng 60 ee-181 1 spring 2009 c 98765 bourikas elec eng 98 cs-101 1 fall 2009 c 98765 bourikas elec eng 98 cs-315 1 spring 2010 b 98988 tanaka biology 120 bio-101 1 summer 2009 a 98988 tanaka biology 120 bio-301 1 summer 2010 null figure 4.3 the result of student join takes on student.id = takes.id with second occurrence of id omitted any courses for example  in the student and takes relations of figures 4.1 and 4.2  note that student snow  with id 70557  has not taken any courses snow appears in student  but snow ? s id number does not appear in the id column of takes thus  snow does not appear in the result of the natural join more generally  some tuples in either or both of the relations being joined may be ? lost ? in this way the outer join operation works in a manner similar to the join operations we have already studied  but preserve those tuples that would be lost in a join  by creating tuples in the result containing null values for example  to ensure that the student named snow fromour earlier example appears in the result  a tuple could be added to the join result with all attributes from the student relation set to the corresponding values for the student snow  and all the remaining attributes which come fromthe takes relation  namely course id  sec id  semester  and year  set to null thus the tuple for the student snow is preserved in the result of the outer join there are in fact three forms of outer join  ? the left outer join preserves tuples only in the relation named before  to the left of  the left outer join operation thesumit67.blogspot.com 4.1 join expressions 117 ? the right outer join preserves tuples only in the relation named after  to the right of  the right outer join operation ? the full outer join preserves tuples in both relations in contrast  the join operationswestudiedearlier thatdo not preserve nonmatched tuples are called inner join operations  to distinguish them from the outer-join operations we now explain exactly how each form of outer join operates.we can compute the left outer-join operation as follows first  compute the result of the inner join as before then  for every tuple t in the left-hand-side relation that does not match any tuple in the right-hand-side relation in the inner join  add a tuple r to the result of the join constructed as follows  ? the attributes of tuple r that are derived from the left-hand-side relation are filled in with the values from tuple t ? the remaining attributes of r are filled with null values figure 4.4 shows the result of  select * from student natural left outer join takes ; that result includes student snow  id 70557   unlike the result of an inner join  but the tuple for snow includes nulls for the attributes that appear only in the schema of the takes relation as another example of the use of the outer-join operation  we can write the query ? find all students who have not taken a course ? as  select id from student natural left outer join takes where course id is null ; the right outer join is symmetric to the left outer join tuples from the righthand side relation that do not match any tuple in the left-hand-side relation are padded with nulls and are added to the result of the right outer join thus  if we rewrite our above query using a right outer join and swapping the order in which we list the relations as follows  select * from takes natural right outer join student ; we get the same result except for the order in which the attributes appear in the result  see figure 4.5   the full outer join is a combination of the left and right outer-join types after the operation computes the result of the inner join  it extends with nulls those tuples fromthe left-hand-side relation that did not matchwith any from the thesumit67.blogspot.com 118 chapter 4 intermediate sql id name dept name tot cred course id sec id semester year grade 00128 zhang comp sci 102 cs-101 1 fall 2009 a 00128 zhang comp sci 102 cs-347 1 fall 2009 a 12345 shankar comp sci 32 cs-101 1 fall 2009 c 12345 shankar comp sci 32 cs-190 2 spring 2009 a 12345 shankar comp sci 32 cs-315 1 spring 2010 a 12345 shankar comp sci 32 cs-347 1 fall 2009 a 19991 brandt history 80 his-351 1 spring 2010 b 23121 chavez finance 110 fin-201 1 spring 2010 c + 44553 peltier physics 56 phy-101 1 fall 2009 b 45678 levy physics 46 cs-101 1 fall 2009 f 45678 levy physics 46 cs-101 1 spring 2010 b + 45678 levy physics 46 cs-319 1 spring 2010 b 54321 williams comp sci 54 cs-101 1 fall 2009 a 54321 williams comp sci 54 cs-190 2 spring 2009 b + 55739 sanchez music 38 mu-199 1 spring 2010 a 70557 snow physics 0 null null null null null 76543 brown comp sci 58 cs-101 1 fall 2009 a 76543 brown comp sci 58 cs-319 2 spring 2010 a 76653 aoi elec eng 60 ee-181 1 spring 2009 c 98765 bourikas elec eng 98 cs-101 1 fall 2009 c 98765 bourikas elec eng 98 cs-315 1 spring 2010 b 98988 tanaka biology 120 bio-101 1 summer 2009 a 98988 tanaka biology 120 bio-301 1 summer 2010 null figure 4.4 result of student natural left outer join takes right-hand side relation  and adds them to the result similarly  it extends with nulls those tuples from the right-hand-side relation that did not match with any tuples from the left-hand-side relation and adds them to the result as an example of the use of full outer join  consider the following query  ? display a list of all students in the comp sci department  along with the course sections  if any  that they have taken in spring 2009 ; all course sections fromspring 2009 must be displayed  even if no student from the comp sci department has taken the course section ? this query can be written as  select * from  select * from student where dept name = ? comp sci ?  natural full outer join  select * from takes where semester = ? spring ? and year = 2009  ; thesumit67.blogspot.com 4.1 join expressions 119 id course id sec id semester year grade name dept name tot cred 00128 cs-101 1 fall 2009 a zhang comp sci 102 00128 cs-347 1 fall 2009 a zhang comp sci 102 12345 cs-101 1 fall 2009 c shankar comp sci 32 12345 cs-190 2 spring 2009 a shankar comp sci 32 12345 cs-315 1 spring 2010 a shankar comp sci 32 12345 cs-347 1 fall 2009 a shankar comp sci 32 19991 his-351 1 spring 2010 b brandt history 80 23121 fin-201 1 spring 2010 c + chavez finance 110 44553 phy-101 1 fall 2009 b peltier physics 56 45678 cs-101 1 fall 2009 f levy physics 46 45678 cs-101 1 spring 2010 b + levy physics 46 45678 cs-319 1 spring 2010 b levy physics 46 54321 cs-101 1 fall 2009 a williams comp sci 54 54321 cs-190 2 spring 2009 b + williams comp sci 54 55739 mu-199 1 spring 2010 a sanchez music 38 70557 null null null null null snow physics 0 76543 cs-101 1 fall 2009 a brown comp sci 58 76543 cs-319 2 spring 2010 a brown comp sci 58 76653 ee-181 1 spring 2009 c aoi elec eng 60 98765 cs-101 1 fall 2009 c bourikas elec eng 98 98765 cs-315 1 spring 2010 b bourikas elec eng 98 98988 bio-101 1 summer 2009 a tanaka biology 120 98988 bio-301 1 summer 2010 null tanaka biology 120 figure 4.5 the result of takes natural right outer join student the on clause can be used with outer joins the following query is identical to the first query we saw using ? student natural left outer join takes  ? except that the attribute id appears twice in the result select * from student left outer join takes on student.id = takes.id ; aswenoted earlier  on and where behave differently for outer join the reason for this is that outer join adds null-padded tuples only for those tuples that do not contribute to the result of the corresponding inner join the on condition is part of the outer join specification  but a where clause is not in our example  the case of the student tuple for student ? snow ? with id 70557  illustrates this distinction suppose we modify the preceding query by moving the on clause predicate to the where clause  and instead using an on condition of true select * from student left outer join takes on true where student.id = takes.id ; thesumit67.blogspot.com 120 chapter 4 intermediate sql join types inner join le  outer join right outer join full outer join join conditions natural on < predicate > using  a1  a2      an  figure 4.6 join types and join conditions the earlier query  using the left outer join with the on condition  includes a tuple  70557  snow  physics  0  null  null  null  null  null  null   because there is no tuple in takes with id = 70557 in the latter query  however  every tuple satisfies the join condition true  so no null-padded tuples are generated by the outer join the outer join actually generates the cartesian product of the two relations since there is no tuple in takes with id = 70557  every time a tuple appears in the outer join with name = ? snow ?  the values for student.id and takes.id must be different  and such tuples would be eliminated by the where clause predicate thus student snow never appears in the result of the latter query 4.1.3 join types and conditions to distinguish normal joins from outer joins  normal joins are called inner joins in sql a join clause can thus specify inner join instead of outer join to specify that a normal join is to be used the keyword inner is  however  optional the default join type  when the join clause is used without the outer prefix is the inner join thus  select * from student join takes using  id  ; is equivalent to  select * from student inner join takes using  id  ; similarly  natural join is equivalent to natural inner join figure 4.6 shows a full list of the various types of join that we have discussed as can be seen from the figure  any form of join  inner  left outer  right outer  or full outer  can be combined with any join condition  natural  using  or on   4.2 views in our examples up to this point  we have operated at the logical-model level that is  we have assumed that the relations in the collection we are given are the actual relations stored in the database thesumit67.blogspot.com 4.2 views 121 it is not desirable for all users to see the entire logical model security considerations may require that certain data be hidden from users consider a clerk who needs to know an instructor ? s id  name and department name  but does not have authorization to see the instructor ? s salary amount this person should see a relation described in sql  by  select id  name  dept name from instructor ; aside from security concerns  we may wish to create a personalized collection of relations that is better matched to a certain user ? s intuition than is the logical model we may want to have a list of all course sections offered by the physics department in the fall 2009 semester,with the building and room number of each section the relation that we would create for obtaining such a list is  select course.course id  sec id  building  room number from course  section where course.course id = section.course id and course.dept name = ? physics ? and section.semester = ? fall ? and section.year = ? 2009 ? ; it is possible to compute and store the results of the above queries and then make the stored relations available to users however  if we did so  and the underlying data in the relations instructor  course  or section changes  the stored query results would then no longer match the result of reexecuting the query on the relations in general  it is a bad idea to compute and store query results such as those in the above examples  although there are some exceptions  which we study later   instead  sql allows a ? virtual relation ? to be defined by a query  and the relation conceptually contains the result of the query the virtual relation is not precomputed and stored  but instead is computed by executing the query whenever the virtual relation is used any such relation that is not part of the logical model  but is made visible to a user as a virtual relation  is called a view it is possible to support a large number of views on top of any given set of actual relations 4.2.1 view definition we define a view in sql by using the create view command to define a view  we must give the view a name and must state the query that computes the view the form of the create view command is  create view v as < query expression > ; thesumit67.blogspot.com 122 chapter 4 intermediate sql where < query expression > is any legal query expression the view name is represented by v consider again the clerkwho needs to access all data in the instructor relation  except salary the clerk should not be authorized to access the instructor relation  we see later  in section 4.6  how authorizations can be specified   instead  a view relation faculty can bemade available to the clerk,with the viewdefined as follows  create view faculty as select id  name  dept name from instructor ; as explained earlier  the view relation conceptually contains the tuples in the query result  but is not precomputed and stored instead  the database system stores the query expression associatedwith the view relation whenever the view relation is accessed  its tuples are created by computing the query result thus  the view relation is created whenever needed  on demand to create a view that lists all course sections offered by the physics department in the fall 2009 semester with the building and room number of each section  we write  create view physics fall 2009 as select course.course id  sec id  building  room number from course  section where course.course id = section.course id and course.dept name = ? physics ? and section.semester = ? fall ? and section.year = ? 2009 ? ; 4.2.2 using views in sql queries once we have defined a view  we can use the view name to refer to the virtual relation that the view generates using the view physics fall 2009  we can find all physics courses offered in the fall 2009 semester in the watson building by writing  select course id from physics fall 2009 where building = ? watson ? ; view names may appear in a query any place where a relation name may appear  the attribute names of a view can be specified explicitly as follows  create view departments total salary  dept name  total salary  as select dept name  sum  salary  from instructor group by dept name ; thesumit67.blogspot.com 4.2 views 123 the preceding view gives for each department the sum of the salaries of all the instructors at that department since the expression sum  salary  does not have a name  the attribute name is specified explicitly in the view definition intuitively  at any given time  the set of tuples in the view relation is the result of evaluation of the query expression that defines the view thus  if a view relation is computed and stored  it may become out of date if the relations used to define it are modified to avoid this  views are usually implemented as follows when we define a view  the database system stores the definition of the view itself  rather than the result of evaluation of the query expression that defines the view wherever a view relation appears in a query  it is replaced by the stored query expression thus  whenever we evaluate the query  the view relation is recomputed one view may be used in the expression defining another view for example  we can define a view physics fall 2009 watson that lists the course id and room number of all physics courses offered in the fall 2009 semester in the watson building as follows  create view physics fall 2009 watson as select course id  room number from physics fall 2009 where building = ? watson ? ; where physics fall 2009 watson is itself a view relation this is equivalent to  create view physics fall 2009 watson as  select course id  room number from  select course.course id  building  room number from course  section where course.course id = section.course id and course.dept name = ? physics ? and section.semester = ? fall ? and section.year = ? 2009 ?  where building = ? watson ? ; 4.2.3 materialized views certain database systems allow view relations to be stored  but they make sure that  if the actual relations used in the view definition change  the view is kept up-to-date such views are called materialized views for example  consider the view departments total salary if the above view is materialized  its resultswould be stored in the database however  if an instructor tuple is added to or deleted from the instructor relation  the result of the query defining the view would change  and as a result the materialized view ? s contents must be updated similarly  if an instructor ? s salary is updated  the tuple in departments total salary corresponding to that instructor ? s department must be updated thesumit67.blogspot.com 124 chapter 4 intermediate sql the process of keeping the materialized view up-to-date is called materialized view maintenance  or often  just view maintenance  and is covered in section 13.5 view maintenance can be done immediately when any of the relations on which the view is defined is updated some database systems  however  perform view maintenance lazily  when the view is accessed some systems update materialized views only periodically ; in this case  the contents of the materialized view may be stale  that is  not up-to-date  when it is used  and should not be used if the application needs up-to-date data and some database systems permit the database administrator to control which of the above methods is used for each materialized view applications that use a view frequently may benefit if the view is materialized applications that demand fast response to certain queries that compute aggregates over large relations can also benefit greatly by creating materialized views corresponding to the queries in this case  the aggregated result is likely to be much smaller than the large relations on which the view is defined ; as a result the materialized view can be used to answer the query very quickly  avoiding reading the large underlying relations of course  the benefits to queries from the materialization of a view must be weighed against the storage costs and the added overhead for updates sql does not define a standard way of specifying that a view is materialized  but many database systems provide their own sql extensions for this task some database systems always keep materialized views up-to-date when the underlying relations change  while others permit them to become out of date  and periodically recompute them 4.2.4 update of a view although views are a useful tool for queries  they present serious problems if we express updates  insertions  or deletions with them the difficulty is that a modification to the database expressed in terms of a view must be translated to a modification to the actual relations in the logical model of the database suppose the view faculty  which we saw earlier  is made available to a clerk since we allow a view name to appear wherever a relation name is allowed  the clerk can write  insert into faculty values  ? 30765 ?  ? green ?  ? music ?  ; this insertion must be represented by an insertion into the relation instructor  since instructor is the actual relation fromwhich the database system constructs the view faculty however  to insert a tuple into instructor  we must have some value for salary there are two reasonable approaches to dealing with this insertion  ? reject the insertion  and return an error message to the user ? insert a tuple  ? 30765 ?  ? green ?  ? music ?  null  into the instructor relation thesumit67.blogspot.com 4.2 views 125 another problem with modification of the database through views occurs with a view such as  create view instructor info as select id  name  building from instructor  department where instructor.dept name = department.dept name ; this view lists the id  name  and building-name of each instructor in the university consider the following insertion through this view  insert into instructor info values  ? 69987 ?  ? white ?  ? taylor ?  ; suppose there is no instructorwith id 69987  and no department in the taylor building then the only possible method of inserting tuples into the instructor and department relations is to insert  ? 69987 ?  ? white ?  null  null  into instructor and  null  ? taylor ?  null  into department then  we obtain the relations shown in figure 4.7 however  this update does not have the desired effect  since the view relation instructor info still does not include the tuple  ? 69987 ?  ? white ?  ? taylor ?   thus  there is no way to update the relations instructor and department by using nulls to get the desired update on instructor info because of problems such as these  modifications are generally not permitted on view relations  except in limited cases different database systems specify different conditions under which they permit updates on view relations ; see the database system manuals for details the general problem of database modification through views has been the subject of substantial research  and the bibliographic notes provide pointers to some of this research in general  an sql view is said to be updatable  that is  inserts  updates or deletes can be applied on the view  if the following conditions are all satisfied by the query defining the view  ? the from clause has only one database relation ? the select clause contains only attribute names of the relation  and does not have any expressions  aggregates  or distinct specification ? any attribute not listed in the select clause can be set to null ; that is  it does not have a not null constraint and is not part of a primary key ? the query does not have a group by or having clause under these constraints  the update  insert  and delete operations would be allowed on the following view  create view history instructors as select * from instructor where dept name = ? history ? ; thesumit67.blogspot.com 126 chapter 4 intermediate sql id name dept name salary 10101 srinivasan comp sci 65000 12121 wu finance 90000 15151 mozart music 40000 22222 einstein physics 95000 32343 el said history 60000 33456 gold physics 87000 45565 katz comp sci 75000 58583 califieri history 62000 76543 singh finance 80000 76766 crick biology 72000 83821 brandt comp sci 92000 98345 kim elec eng 80000 69987 white null null instructor dept name building budget biology watson 90000 comp sci taylor 100000 electrical eng taylor 85000 finance painter 120000 history painter 50000 music packard 80000 physics watson 70000 null painter null department figure 4.7 relations instructor and department after insertion of tuples even with the conditions on updatability  the following problem still remains suppose that a user tries to insert the tuple  ? 25566 ?  ? brown ?  ? biology ?  100000  into the history instructors view this tuple can be inserted into the instructor relation  but it would not appear in the history instructors view since it does not satisfy the selection imposed by the view by default  sql would allow the above update to proceed however  views can be defined with a with check option clause at the end of the view definition ; then  if a tuple inserted into the view does not satisfy the view ? s where clause condition  the insertion is rejected by the database system updates are similarly rejected if the new value does not satisfy the where clause conditions sql  1999 has a more complex set of rules about when inserts  updates  and deletes can be executed on a view  that allows updates through a larger class of views ; however  the rules are too complex to be discussed here thesumit67.blogspot.com 4.3 transactions 127 4.3 transactions a transaction consists of a sequence of query and/or update statements the sql standard specifies that a transaction begins implicitly when an sql statement is executed one of the following sql statements must end the transaction  ? commit work commits the current transaction ; that is  it makes the updates performed by the transaction become permanent in the database after the transaction is committed  a new transaction is automatically started ? rollback work causes the current transaction to be rolled back ; that is  it undoes all the updates performed by the sql statements in the transaction thus  the database state is restored to what it was before the first statement of the transaction was executed the keyword work is optional in both the statements transaction rollback is useful if some error condition is detected during execution of a transaction commit is similar  in a sense  to saving changes to a document that is being edited  while rollback is similar to quitting the edit session without saving changes once a transaction has executed commit work  its effects can no longer be undone by rollback work the database system guarantees that in the event of some failure  such as an error in one of the sql statements  a power outage  or a system crash  a transaction ? s effects will be rolled back if it has not yet executed commit work in the case of power outage or other system crash  the rollback occurs when the system restarts for instance  consider a banking application,wherewe needto transfermoney from one bank account to another in the same bank to do so  we need to update two account balances  subtracting the amount transferred from one  and adding it to the other if the system crashes after subtracting the amount from the first account  but before adding it to the second account  the bank balances would be inconsistent a similar problem would occur  if the second account is credited before subtracting the amount from the first account  and the system crashes just after crediting the amount as another example  consider our running example of a university application we assume that the attribute tot cred of each tuple in the student relation is kept up-to-date by modifying it whenever the student successfully completes a course to do so  whenever the takes relation is updated to record successful completion of a course by a student  by assigning an appropriate grade  the corresponding student tuple must also be updated if the application performing these two updates crashes after one update is performed  but before the second one is performed  the data in the database would be inconsistent by either committing the actions of a transaction after all its steps are completed  or rolling back all its actions in case the transaction could not complete all its actions successfully  the database provides an abstraction of a transaction as being atomic  that is  indivisible either all the effects of the transaction are reflected in the database  or none are  after rollback   thesumit67.blogspot.com 128 chapter 4 intermediate sql applying the notion of transactions to the above applications  the update statements should be executed as a single transaction.an errorwhile a transaction executes one of its statements would result in undoing of the effects of the earlier statements of the transaction  so that the database is not left in a partially updated state if a program terminates without executing either of these commands  the updates are either committed or rolled back the standard does not specifywhich of the two happens  and the choice is implementation dependent in many sql implementations  by default each sql statement is taken to be a transaction on its own  and gets committed as soon as it is executed automatic commit of individual sql statements must be turned off if a transaction consisting of multiple sql statements needs to be executed how to turn off automatic commit depends on the specific sql implementation  although there is a standard way of doing this using application program interfaces such as jdbc or odbc  which we study later  in sections 5.1.1 and 5.1.2  respectively a better alternative  which is part of the sql  1999 standard  but supported by only some sql implementations currently   is to allow multiple sql statements to be enclosed between the keywords begin atomic    end all the statements between the keywords then form a single transaction we study further properties of transactions in chapter 14 ; issues in implementing transactions in a single database are addressed in chapters 15 and 16  while chapter 19 addresses issues in implementing transactions across multiple databases  to deal with problems such as transfer of money across accounts in different banks  which have different databases 4.4 integrity constraints integrity constraints ensure that changes made to the database by authorized users do not result in a loss of data consistency thus  integrity constraints guard against accidental damage to the database examples of integrity constraints are  ? an instructor name can not be null ? no two instructors can have the same instructor id ? every department name in the course relation must have a matching department name in the department relation ? the budget of a department must be greater than $ 0.00 in general  an integrity constraint can be an arbitrary predicate pertaining to the database however  arbitrary predicates may be costly to test thus  most database systems allow one to specify integrity constraints that can be tested with minimal overhead we have already seen some forms of integrity constraints in section 3.2.2.we study some more forms of integrity constraints in this section in chapter 8  we thesumit67.blogspot.com 4.4 integrity constraints 129 study another form of integrity constraint  called functional dependencies  that is used primarily in the process of schema design integrity constraints are usually identified as part of the database schema design process  and declared as part of the create table command used to create relations however  integrity constraints can also be added to an existing relation by using the command alter table table-name add constraint  where constraint can be any constraint on the relation when such a command is executed  the system first ensures that the relation satisfies the specified constraint if it does  the constraint is added to the relation ; if not  the command is rejected 4.4.1 constraints on a single relation we described in section 3.2 how to define tables using the create table command the create table command may also include integrity-constraint statements in addition to the primary-key constraint  there are a number of other ones that can be included in the create table command the allowed integrity constraints include ? not null ? unique ? check  < predicate >  we cover each of these types of constraints in the following sections 4.4.2 not null constraint as we discussed in chapter 3  the null value is a member of all domains  and as a result is a legal value for every attribute in sql by default for certain attributes  however  null values may be inappropriate consider a tuple in the student relation where name is null such a tuple gives student information for an unknown student ; thus  it does not contain useful information similarly  we would not want the department budget to be null in cases such as this  we wish to forbid null values  and we can do so by restricting the domain of the attributes name and budget to exclude null values  by declaring it as follows  name varchar  20  not null budget numeric  12,2  not null the not null specification prohibits the insertion of a null value for the attribute any database modification that would cause a null to be inserted in an attribute declared to be not null generates an error diagnostic there are many situations where we want to avoid null values in particular  sql prohibits null values in the primary key of a relation schema thus  in our university example  in the department relation  if the attribute dept name is declared thesumit67.blogspot.com 130 chapter 4 intermediate sql as the primary key for department  it can not take a null value as a result it would not need to be declared explicitly to be not null 4.4.3 unique constraint sql also supports an integrity constraint  unique  aj1  aj2      ajm  the unique specification says that attributes aj1  aj2      ajm form a candidate key ; that is  no two tuples in the relation can be equal on all the listed attributes however  candidate key attributes are permitted to be null unless they have explicitly been declared to be not null recall that a null value does not equal any other value  the treatment of nulls here is the same as that of the unique construct defined in section 3.8.4  4.4.4 the check clause when applied to a relation declaration  the clause check  p  specifies a predicate p that must be satisfied by every tuple in a relation a common use of the check clause is to ensure that attribute values satisfy specified conditions  in effect creating a powerful type system for instance  a clause check  budget > 0  in the create table command for relation department would ensure that the value of budget is nonnegative as another example  consider the following  create table section  course id varchar  8   sec id varchar  8   semester varchar  6   year numeric  4,0   building varchar  15   room number varchar  7   time slot id varchar  4   primary key  course id  sec id  semester  year   check  semester in  ? fall ?  ? winter ?  ? spring ?  ? summer ?    ; here  we use the check clause to simulate an enumerated type  by specifying that semester must be one of ? fall ?  ? winter ?  ? spring ?  or ? summer ?  thus  the check clause permits attribute domains to be restricted in powerful ways that most programming-language type systems do not permit the predicate in the check clause can  according to the sql standard  be an arbitrary predicate that can include a subquery however  currently none of the widely used database products allows the predicate to contain a subquery thesumit67.blogspot.com 4.4 integrity constraints 131 4.4.5 referential integrity often  we wish to ensure that a value that appears in one relation for a given set of attributes also appears for a certain set of attributes in another relation this condition is called referential integrity foreign keys can be specified as part of the sql create table statement by using the foreign key clause  aswe saw earlier in section 3.2.2.we illustrate foreign-key declarations by using the sql ddl definition of part of our university database  shown in figure 4.8 the definition of the course table has a declaration ? foreign key  dept name  references department ?  this foreign-key declaration specifies that for each course tuple  the department name specified in the tuple must exist in the department relation.without this constraint  it is possible for a course to specify a nonexistent department name more generally  let r1 and r2 be relations whose set of attributes are r1 and r2  respectively  with primary keys k1 and k2 we say that a subset  of r2 is a foreign key referencing k1 in relation r1 if it is required that  for every tuple t2 in r2  theremust be a tuple t1 in r1 such that t1.k1 = t2   requirements of this form are called referential-integrity constraints  or subset dependencies the latter term arises because the preceding referentialintegrity constraint can be stated as a requirement that the set of values on  in r2 must be a subset of the values on k1 in r1 note that  for a referential-integrity constraint to make sense   and k1 must be compatible sets of attributes ; that is  either  must be equal to k1  or they must contain the same number of attributes  and the types of corresponding attributes must be compatible  we assume here that  and k1 are ordered   unlike foreign-key constraints  in general a referential integrity constraint does not require k1 to be a primary key of r1 ; as a result,more than one tuple in r1 can have the same value for attributes k1 by default  in sql a foreign key references the primary-key attributes of the referenced table sql also supports a version of the references clause where a list of attributes of the referenced relation can be specified explicitly the specified list of attributes must  however  be declared as a candidate key of the referenced relation  using either a primary key constraint  or a unique constraint a more general form of a referential-integrity constraint  where the referenced columns need not be a candidate key  can not be directly specified in sql the sql standard specifies other constructs that can be used to implement such constraints ; they are described in section 4.4.7 we can use the following short form as part of an attribute definition to declare that the attribute forms a foreign key  dept name varchar  20  references department when a referential-integrity constraint is violated  the normal procedure is to reject the action that caused the violation  that is  the transaction performing the update action is rolled back   however  a foreign key clause can specify that if a delete or update action on the referenced relation violates the constraint  then  thesumit67.blogspot.com 132 chapter 4 intermediate sql create table classroom  building varchar  15   room number varchar  7   capacity numeric  4,0   primary key  building  room number   create table department  dept name varchar  20   building varchar  15   budget numeric  12,2  check  budget > 0   primary key  dept name   create table course  course id varchar  8   title varchar  50   dept name varchar  20   credits numeric  2,0  check  credits > 0   primary key  course id   foreign key  dept name  references department  create table instructor  id varchar  5   name varchar  20   not null dept name varchar  20   salary numeric  8,2   check  salary > 29000   primary key  id   foreign key  dept name  references department  create table section  course id varchar  8   sec id varchar  8   semester varchar  6   check  semester in  ? fall ?  ? winter ?  ? spring ?  ? summer ?   year numeric  4,0   check  year > 1759 and year < 2100  building varchar  15   room number varchar  7   time slot id varchar  4   primary key  course id  sec id  semester  year   foreign key  course id  references course  foreign key  building  room number  references classroom  figure 4.8 sql data definition for part of the university database instead of rejecting the action  the system must take steps to change the tuple in the referencing relation to restore the constraint consider this definition of an integrity constraint on the relation course  thesumit67.blogspot.com 4.4 integrity constraints 133 create table course     foreign key  dept name  references department on delete cascade on update cascade      ; because of the clause on delete cascade associated with the foreign-key declaration  if a delete of a tuple in department results in this referential-integrity constraint being violated  the system does not reject the delete instead  the delete ? cascades ? to the course relation  deleting the tuple that refers to the department that was deleted similarly  the system does not reject an update to a field referenced by the constraint if it violates the constraint ; instead  the system updates the field dept name in the referencing tuples in course to the new value as well sql also allows the foreign key clause to specify actions other than cascade  if the constraint is violated  the referencing field  here  dept name  can be set to null  by using set null in place of cascade   or to the default value for the domain  by using set default   if there is a chain of foreign-key dependencies across multiple relations  a deletion or update at one end of the chain can propagate across the entire chain an interesting case where the foreign key constraint on a relation references the same relation appears in practice exercises 4.9 if a cascading update or delete causes a constraint violation that can not be handled by a further cascading operation  the system aborts the transaction as a result  all the changes caused by the transaction and its cascading actions are undone null values complicate the semantics of referential-integrity constraints in sql attributes of foreign keys are allowed to be null  provided that they have not otherwise been declared to be not null if all the columns of a foreign key are nonnull in a given tuple  the usual definition of foreign-key constraints is used for that tuple if any of the foreign-key columns is null  the tuple is defined automatically to satisfy the constraint this definition may not always be the right choice  so sql also provides constructs that allow you to change the behavior with null values ; we do not discuss the constructs here 4.4.6 integrity constraint violation during a transaction transactions may consist of several steps  and integrity constraints may be violated temporarily after one step  but a later step may remove the violation for instance  suppose we have a relation person with primary key name  and an attribute spouse  and suppose that spouse is a foreign key on person that is  the constraint says that the spouse attribute must contain a name that is present in the person table suppose we wish to note the fact that john and mary are married to each other by inserting two tuples  one for john and one formary  in the above relation  with the spouse attributes set tomary and john  respectively the insertion of the first tuple would violate the foreign-key constraint  regardless of which of thesumit67.blogspot.com 134 chapter 4 intermediate sql the two tuples is inserted first after the second tuple is inserted the foreign-key constraint would hold again to handle such situations  the sql standard allows a clause initially deferred to be added to a constraint specification ; the constraint would then be checked at the end of a transaction  and not at intermediate steps a constraint can alternatively be specified as deferrable  which means it is checked immediately by default  but can be deferred when desired for constraints declared as deferrable  executing a statement set constraints constraint-list deferred as part of a transaction causes the checking of the specified constraints to be deferred to the end of that transaction however  you should beaware that the default behavior is to check constraints immediately  and many database implementations do not support deferred constraint checking we can work around the problem in the above example in another way  if the spouse attribute can be set to null  we set the spouse attributes to null when inserting the tuples for john and mary  and we update them later however  this technique requires more programming effort  and does not work if the attributes can not be set to null 4.4.7 complex check conditions and assertions the sql standard supports additional constructs for specifying integrity constraints that are described in this section however  you should be aware that these constructs are not currently supported by most database systems as defined by the sql standard  the predicate in the check clause can be an arbitrary predicate  which can include a subquery if a database implementation supports subqueries in the check clause  we could specify the following referential-integrity constraint on the relation section  check  time slot id in  select time slot id from time slot   the check condition verifies that the time slot id in each tuple in the section relation is actually the identifier of a time slot in the time slot relation thus  the condition has to be checked not only when a tuple is inserted or modified in section  but also when the relation time slot changes  in this case  when a tuple is deleted or modified in relation time slot   another natural constraint on our university schema would be to require that every section has at least one instructor teaching the section in an attempt to enforce this  we may try to declare that the attributes  course id  sec id  semester  year  of the section relation form a foreign key referencing the corresponding attributes of the teaches relation unfortunately  these attributes do not form a candidate key of the relation teaches a check constraint similar to that for the time slot attribute can be used to enforce this constraint  if check constraints with subqueries were supported by a database system complex check conditions can be useful when we want to ensure integrity of data  but may be costly to test for example  the predicate in the check clause thesumit67.blogspot.com 4.4 integrity constraints 135 create assertion credits earned constraint check  not exists  select id from student where tot cred < >  select sum  credits  from takes natural join course where student.id = takes.id and grade is not null and grade < > ? f ?  figure 4.9 an assertion example would not only have to be evaluated when a modification is made to the section relation  but may have to be checked if a modification is made to the time slot relation because that relation is referenced in the subquery an assertion is a predicate expressing a condition that we wish the database always to satisfy domain constraints and referential-integrity constraints are special forms of assertions we have paid substantial attention to these forms of assertions because they are easily tested and apply to a wide range of database applications however  there are many constraints that we can not express by using only these special forms two examples of such constraints are  ? for each tuple in the student relation  the value of the attribute tot cred must equal the sum of credits of courses that the student has completed successfully ? an instructor can not teach in two different classrooms in a semester in the same time slot.1 an assertion in sql takes the form  create assertion < assertion-name > check < predicate > ; in figure 4.9  we show how the first example of constraints can be written in sql since sql does not provide a ? for all x  p  x  ? construct  where p is a predicate   we are forced to implement the constraint by an equivalent construct  ? not exists x such that not p  x  ?  that can be expressed in sql we leave the specification of the second constraint as an exercise when an assertion is created  the system tests it for validity if the assertion is valid  then any future modification to the database is allowed only if it does not cause that assertion to be violated this testing may introduce a significant amount of overhead if complex assertions have been made hence  assertions should be used with great care the high overhead of testing and maintaining assertions has led some system developers to omit support for general assertions  or to provide specialized forms of assertion that are easier to test 1we assume that lectures are not displayed remotely in a second classroom ! an alternative constraint that specifies that ? an instructor can not teach two courses in a given semester in the same time slot ? may not hold since courses are sometimes cross-listed ; that is  the same course is given two identifiers and titles thesumit67.blogspot.com 136 chapter 4 intermediate sql currently  none of the widely used database systems supports either subqueries in the check clause predicate  or the create assertion construct however  equivalent functionality can be implemented using triggers  which are described in section 5.3  if they are supported by the database system section 5.3 also describes how the referential integrity constraint on time slot id can be implemented using triggers 4.5 sql data types and schemas in chapter 3  we covered a number of built-in data types supported in sql  such as integer types  real types  and character types there are additional built-in data types supported by sql  which we describe below.we also describe how to create basic user-defined types in sql 4.5.1 date and time types in sql in addition to the basic data types we introduced in section 3.2  the sql standard supports several data types relating to dates and times  ? date  a calendar date containing a  four-digit  year  month  and day of the month ? time  the time of day  in hours  minutes  and seconds a variant  time  p   can be used to specify the number of fractional digits for seconds  the default being 0   it is also possible to store time-zone information along with the time by specifying time with timezone ? timestamp  a combination of date and time a variant  timestamp  p   can be used to specify the number of fractional digits for seconds  the default here being 6   time-zone information is also stored if with timezone is specified date and time values can be specified like this  date ? 2001-04-25 ? time ? 09  30  00 ? timestamp ? 2001-04-25 10  29  01.45 ? dates must be specified in the format year followed by month followed by day  as shown the seconds field of time or timestamp can have a fractional part  as in the timestamp above we can use an expression of the form cast e as t to convert a character string  or string valued expression  e to the type t  where t is one of date  time  or timestamp the string must be in the appropriate format as illustrated at the beginning of this paragraph when required  time-zone information is inferred from the system settings to extract individual fields of a date or time value d  we can use extract  field from d   where field can be one of year,month  day  hour  minute  or second timezone information can be extracted using timezone hour and timezone minute thesumit67.blogspot.com 4.5 sql data types and schemas 137 sql defines several functions to get the current date and time for example  current date returns the current date  current time returns the current time  with time zone   and localtime returns the current local time  without time zone   timestamps  date plus time  are returned by current timestamp  with time zone  and localtimestamp  local date and time without time zone   sql allows comparison operations on all the types listed here  and it allows both arithmetic and comparison operations on the various numeric types sql also provides a data type called interval  and it allows computations based on dates and times and on intervals for example  if x and y are of type date  then x  y is an interval whose value is the number of days from date x to date y similarly  adding or subtracting an interval to a date or time gives back a date or time  respectively 4.5.2 default values sql allows a default value to be specified for an attribute as illustrated by the following create table statement  create table student  id varchar  5   name varchar  20  not null  dept name varchar  20   tot cred numeric  3,0  default 0  primary key  id   ; the default value of the tot cred attribute is declared to be 0 as a result  when a tuple is inserted into the student relation  if no value is provided for the tot cred attribute  its value is set to 0 the following insert statement illustrates how an insertion can omit the value for the tot cred attribute insert into student  id  name  dept name  values  ? 12789 ?  ? newman ?  ? comp sci ?  ; 4.5.3 index creation many queries reference only a small proportion of the records in a file for example  a query like ? find all instructors in the physics department ? or ? find the tot cred value of the student with id 22201 ? references only a fraction of the student records it is inefficient for the system to read every record and to check id field for the id ? 32556  ? or the building field for the value ? physics ?  an index on an attribute of a relation is a data structure that allows the database system to find those tuples in the relation that have a specified value for that attribute efficiently  without scanning through all the tuples of the relation for example  if we create in index on attribute id of relation student  the database system can find the record with any specified id value  such as 22201  or 44553  directly  without reading all the tuples of the student relation an index can also thesumit67.blogspot.com 138 chapter 4 intermediate sql be created on a list of attributes  for example on attributes name  and dept name of student we study later  in chapter 11  how indices are actually implemented  including a particularly widely used kind of index called a b + -tree index although the sql language does not formally define any syntax for creating indices,manydatabases support index creation using the syntax illustrated below create index studentid index on student  id  ; the above statement creates an index named studentid index on the attribute id of the relation student when a user submits an sql query that can benefit from using an index  the sql query processor automatically uses the index for example  given an sql query that selects the student tuple with id 22201  the sql query processor would use the index studentid index defined above to find the required tuple without reading the whole relation 4.5.4 large-object types many current-generation database applications need to store attributes that can be large  of the order of many kilobytes   such as a photograph  or very large  of the order of many megabytes or even gigabytes   such as a high-resolution medical image or video clip sql therefore provides large-object data types for character data  clob  and binary data  blob   the letters ? lob ? in these data types stand for ? large object ? for example  we may declare attributes book review clob  10kb  image blob  10mb  movie blob  2gb  for result tuples containing large objects  multiple megabytes to gigabytes   it is inefficient or impractical to retrieve an entire large object into memory instead  an application would usually use an sql query to retrieve a ? locator ? for a large object and then use the locator to manipulate the object fromthe host language in which the application itself is written for instance  the jdbc application program interface  described in section 5.1.1  permits a locator to be fetched instead of the entire large object ; the locator can then be used to fetch the large object in small pieces  rather than all at once  much like reading data from an operating system file using a read function call 4.5.5 user-defined types sql supports two forms of user-defined data types the first form  which we cover here  is called distinct types the other form  called structured data types  allows the creation of complex data types with nested record structures  arrays  thesumit67.blogspot.com 4.5 sql data types and schemas 139 and multisets.we do not cover structured data types in this chapter  but describe them later  in chapter 22 it is possible for several attributes to have the same data type for example  the name attributes for student name and instructor name might have the same domain  the set of all person names.however  the domains of budget and dept name certainly ought to be distinct it is perhaps less clear whether name and dept name should have the same domain at the implementation level  both instructor names and department names are character strings however  we would normally not consider the query ? find all instructorswhohave the same name as a department ? to be a meaningful query thus  if we view the database at the conceptual  rather than the physical  level  name and dept name should have distinct domains more importantly  at a practical level  assigning an instructor ? s name to a department name is probably a programming error ; similarly  comparing a monetary value expressed in dollars directly with a monetary value expressed in pounds is also almost surely a programming error a good type system should be able to detect such assignments or comparisons to support such checks  sql provides the notion of distinct types the create type clause can be used to define new types for example  the statements  create type dollars as numeric  12,2  final ; create type pounds as numeric  12,2  final ; define the user-defined types dollars and pounds to be decimal numbers with a total of 12 digits  two of which are placed after the decimal point  the keyword final isn ? t really meaningful in this context but is required by the sql  1999 standard for reasonswewon ? t get into here ; some implementations allow the final keyword to be omitted  the newly created types can then be used  for example  as types of attributes of relations for example  we can declare the department table as  create table department  dept name varchar  20   building varchar  15   budget dollars  ; an attempt to assign a value of type dollars to a variable of type pounds results in a compile-time error  although both are of the same numeric type such an assignment is likely to be due to a programmer error  where the programmer forgot about the differences in currency declaring different types for different currencies helps catch such errors as a result of strong type checking  the expression  department.budget + 20  would not be accepted since the attribute and the integer constant 20 have different types values of one type can be cast  that is  converted  to another domain  as illustrated below  cast  department.budget to numeric  12,2   thesumit67.blogspot.com 140 chapter 4 intermediate sql we could do addition on the numeric type  but to save the result back to an attribute of type dollars we would have to use another cast expression to convert the type back to dollars sql provides drop type and alter type clauses to drop or modify types that have been created earlier even before user-defined types were added to sql  in sql  1999   sql had a similar but subtly different notion of domain  introduced in sql-92   which can add integrity constraints to an underlying type for example  we could define a domain ddollars as follows create domain ddollars as numeric  12,2  not null ; the domain ddollars can be used as an attribute type  just as we used the type dollars however  there are two significant differences between types and domains  1 domains can have constraints  such as not null  specified on them  and can have default values defined for variables of the domain type  whereas userdefined types can not have constraints or default values specified on them user-defined types are designed to be used not just for specifying attribute types  but also in procedural extensions to sql where itmay not be possible to enforce constraints 2 domains are not strongly typed as a result  values of one domain type can be assigned to values of another domain type as long as the underlying types are compatible when applied to a domain  the check clause permits the schema designer to specify a predicate that must be satisfied by any attribute declared to be from this domain for instance  a check clause can ensure that an instructor ? s salary domain allows only values greater than a specified value  create domain yearlysalary numeric  8,2  constraint salary value test check  value > = 29000.00  ; the domain yearlysalary has a constraint that ensures that the yearlysalary is greater than or equal to $ 29,000.00 the clause constraint salary value test is optional  and is used to give the name salary value test to the constraint the name is used by the system to indicate the constraint that an update violated as another example  a domain can be restricted to contain only a specified set of values by using the in clause  create domain degree level varchar  10  constraint degree level test check  value in  ? bachelors ?  ? masters ?  or ? doctorate ?   ; thesumit67.blogspot.com 4.5 sql data types and schemas 141 support for types and domains in database implementations although the create type and create domain constructs described in this section are part of the sql standard  the forms of these constructs described here are not fully supported by most database implementations postgresql supports the create domain construct  but its create type construct has a different syntax and interpretation ibm db2 supports a version of the create type that uses the syntax create distinct type  but does not support create domain microsoft sql server implements a version of create type construct that supports domain constraints  similar to the sql create domain construct oracle does not support either construct as described here however  sql also defines a more complex object-oriented type system  which we study later in chapter 22 oracle  ibm db2  postgresql  and sql server all support objectoriented type systems using different forms of the create type construct 4.5.6 create table extensions applications often require creation of tables that have the same schema as an existing table sql provides a create table like extension to support this task  create table temp instructor like instructor ; the above statement creates a new table temp instructor that has the same schema as instructor when writing a complex query  it is often useful to store the result of a query as a new table ; the table is usually temporary two statements are required  one to create the table  with appropriate columns  and the second to insert the query result into the table sql  2003 provides a simpler technique to create a table containing the results of a query for example the following statement creates a table t1 containing the results of a query create table t1 as  select * from instructor where dept name = ? music ?  with data ; by default  the names and data types of the columns are inferred from the query result names can be explicitly given to the columns by listing the column names after the relation name as defined by the sql  2003 standard  if the with data clause is omitted  the table is created but not populated with data however many implementations populate the table with data by default even if the with data clause is omitted thesumit67.blogspot.com 142 chapter 4 intermediate sql note that several implementations support the functionality of create table    like and create table    as using different syntax ; see the respective system manuals for further details the above create table    as statement closely resembles the create view statement and both are defined by using queries the main difference is that the contents of the table are set when the table is created  whereas the contents of a view always reflect the current query result 4.5.7 schemas  catalogs  and environments to understand the motivation for schemas and catalogs  consider how files are named in a file system early file systems were flat ; that is  all files were stored in a single directory current file systems  of course  have a directory  or  synonymously  folder  structure  with files stored within subdirectories to name a file uniquely  we must specify the full path name of the file  for example  /users/avi/db-book/chapter3.tex like early file systems  early database systems also had a single name space for all relations users had to coordinate to make sure they did not try to use the same name for different relations contemporary database systems provide a three-level hierarchy for naming relations the top level of the hierarchy consists of catalogs  each of which can contain schemas sql objects such as relations and views are contained within a schema  some database implementations use the term ? database ? in place of the term catalog  in order to perform any actions on a database  a user  or a program  must first connect to the database the user must provide the user name and usually  a password for verifying the identity of the user each user has a default catalog and schema  and the combination is unique to the user when a user connects to a database system  the default catalog and schema are set up for the connection ; this corresponds to the current directory being set to the user ? s home directory when the user logs into an operating system to identify a relation uniquely  a three-part name may be used  for example  catalog5.univ schema.course we may omit the catalog component  in which case the catalog part of the name is considered to be the default catalog for the connection thus if catalog5 is the default catalog  we can use univ schema.course to identify the same relation uniquely if a user wishes to access a relation that exists in a different schema than the default schema for that user  the name of the schemamust be specified however  if a relation is in the default schema for a particular user  then even the schema name may be omitted thus we can use just course if the default catalog is catalog5 and the default schema is univ schema with multiple catalogs and schemas available  different applications and different users can work independently without worrying about name clashes moreover  multiple versions of an application ? one a production version  other test versions ? can run on the same database system thesumit67.blogspot.com 4.6 authorization 143 the default catalog and schema are part of an sql environment that is set up for each connection the environment additionally contains the user identifier  also referred to as the authorization identifier   all the usual sql statements  including the ddl and dml statements  operate in the context of a schema we can create and drop schemas bymeans of create schema and drop schema statements in most database systems  schemas are also created automatically when user accounts are created  with the schema name set to the user account name the schema is created in either a default catalog  or a catalog specified in creating the user account the newly created schema becomes the default schema for the user account creation and dropping of catalogs is implementation dependent and not part of the sql standard 4.6 authorization we may assign a user several forms of authorizations on parts of the database authorizations on data include  ? authorization to read data ? authorization to insert new data ? authorization to update data ? authorization to delete data each of these types of authorizations is called a privilege we may authorize the user all  none  or a combination of these types of privileges on specified parts of a database  such as a relation or a view when a user submits a query or an update  the sql implementation first checks if the query or update is authorized  based on the authorizations that the user has been granted if the query or update is not authorized  it is rejected in addition to authorizations on data  users may also be granted authorizations on the database schema  allowing them  for example  to create  modify  or drop relations a user who has some form of authorization may be allowed to pass on  grant  this authorization to other users  or to withdraw  revoke  an authorization that was granted earlier in this section  we see how each of these authorizations can be specified in sql the ultimate form of authority is that given to the database administrator the database administrator may authorize new users  restructure the database  and so on this form of authorization is analogous to that of a superuser  administrator  or operator for an operating system 4.6.1 granting and revoking of privileges the sql standard includes the privileges select  insert  update  and delete the privilege all privileges can be used as a short form for all the allowable privithesumit67 blogspot.com 144 chapter 4 intermediate sql leges a user who creates a new relation is given all privileges on that relation automatically the sql data-definition language includes commands to grant and revoke privileges the grant statement is used to confer authorization the basic form of this statement is  grant < privilege list > on < relation name or view name > to < user/role list > ; the privilege list allows the granting of several privileges in one command the notion of roles is covered later  in section 4.6.2 the select authorization on a relation is required to read tuples in the relation the following grant statement grants database users amit and satoshi select authorization on the department relation  grant select on department to amit  satoshi ; this allows those users to run queries on the department relation the update authorization on a relation allows a user to update any tuple in the relation the update authorization may be given either on all attributes of the relation or on only some if update authorization is included in a grant statement  the list of attributes on which update authorization is to be granted optionally appears in parentheses immediately after the update keyword if the list of attributes is omitted  the update privilege will be granted on all attributes of the relation this grant statement gives users amit and satoshi update authorization on the budget attribute of the department relation  grant update  budget  on department to amit  satoshi ; the insert authorization on a relation allows a user to insert tuples into the relation the insert privilege may also specify a list of attributes ; any inserts to the relation must specify only these attributes  and the system either gives each of the remaining attributes default values  if a default is defined for the attribute  or sets them to null the delete authorization on a relation allows a user to delete tuples from a relation the user name public refers to all current and future users of the system thus  privileges granted to public are implicitly granted to all current and future users by default  a user/role that is granted a privilege is not authorized to grant that privilege to another user/role sql allows a privilege grant to specify that the recipient may further grant the privilege to another user.we describe this feature in more detail in section 4.6.5 thesumit67.blogspot.com 4.6 authorization 145 it is worth noting that the sql authorization mechanism grants privileges on an entire relation  or on specified attributes of a relation however  it does not permit authorizations on specific tuples of a relation to revoke an authorization  we use the revoke statement it takes a form almost identical to that of grant  revoke < privilege list > on < relation name or view name > from < user/role list > ; thus  to revoke the privileges that we granted previously  we write revoke select on department from amit  satoshi ; revoke update  budget  on department from amit  satoshi ; revocation of privileges is more complex if the user fromwhom the privilege is revoked has granted the privilege to another user we return to this issue in section 4.6.5 4.6.2 roles consider the real-world roles of various people in a university each instructor must have the same types of authorizations on the same set of relations.whenever a new instructor is appointed  she will have to be given all these authorizations individually abetter approachwould be to specify the authorizations that every instructor is to be given  and to identify separately which database users are instructors the system can use these two pieces of information to determine the authorizations of each instructor.when a new instructor is hired  a user identifier must be allocated to him  and he must be identified as an instructor individual permissions given to instructors need not be specified again the notion of roles captures this concept a set of roles is created in the database authorizations can be granted to roles  in exactly the same fashion as they are granted to individual users each database user is granted a set of roles  which may be empty  that she is authorized to perform in our university database  examples of roles could include instructor  teaching assistant  student  dean  and department chair a less preferable alternative would be to create an instructor userid  and permit each instructor to connect to the database using the instructor userid the problem with this approach is that it would not be possible to identify exactly which instructor carried out a database update  leading to security risks the use of roles has the benefit of requiring users to connect to the database with their own userid any authorization that can be granted to a user can be granted to a role roles are granted to users just as authorizations are thesumit67.blogspot.com 146 chapter 4 intermediate sql roles can be created in sql as follows  create role instructor ; roles can then be granted privileges just as the users can  as illustrated in this statement  grant select on takes to instructor ; roles can be granted to users  as well as to other roles  as these statements show  grant dean to amit ; create role dean ; grant instructor to dean ; grant dean to satoshi ; thus the privileges of a user or a role consist of  ? all privileges directly granted to the user/role ? all privileges granted to roles that have been granted to the user/role note that there can be a chain of roles ; for example  the role teaching assistant may be granted to all instructors in turn the role instructor is granted to all deans thus  the dean role inherits all privileges granted to the roles instructor and to teaching assistant in addition to privileges granted directly to dean when a user logs in to the database system  the actions executed by the user during that session have all the privileges granted directly to the user  as well as all privileges granted to roles that are granted  directly or indirectly via other roles  to that user thus  if a user amit has been granted the role dean  user amit holds all privileges granted directly to amit  as well as privileges granted to dean  plus privileges granted to instructor  and teaching assistant if  as above  those roles were granted  directly or indirectly  to the role dean it is worth noting that the concept of role-based authorization is not specific to sql  and role-based authorization is used for access control in a wide variety of shared applications 4.6.3 authorization on views in our university example  consider a staff member who needs to know the salaries of all faculty in a particular department  say the geology department this staff member is not authorized to see information regarding faculty in other departments thus  the staffmembermust be denied direct access to the instructor relation but  if he is to have access to the information for the geology department  he might be granted access to a view that we shall call geo instructor  consisting thesumit67.blogspot.com 4.6 authorization 147 of only those instructor tuples pertaining to the geology department this view can be defined in sql as follows  create view geo instructor as  select * from instructor where dept name = ? geology ?  ; suppose that the staff member issues the following sql query  select * from geo instructor ; clearly  the staff member is authorized to see the result of this query however  when the query processor translates it into a query on the actual relations in the database  it produces a query on instructor thus  the system must check authorization on the clerk ? s query before it begins query processing a user who creates a view does not necessarily receive all privileges on that view she receives only those privileges that provide no additional authorization beyond those that she already had for example  a userwho creates a viewcannot be given update authorization on a view without having update authorization on the relations used to define the view if a user creates a view on which no authorization can be granted  the system will deny the view creation request in our geo instructor view example  the creator of the view must have select authorization on the instructor relation as we will see later  in section 5.2  sql supports the creation of functions and procedures  which may in turn contain queries and updates the execute privilege can be granted on a function or procedure  enabling a user to execute the function/procedure by default  just like views  functions and procedures have all the privileges that the creator of the function or procedure had in effect  the function or procedure runs as if it were invoked by the user who created the function although this behavior is appropriate in many situations  it is not always appropriate starting with sql  2003  if the function definition has an extra clause sql security invoker  then it is executed under the privileges of the user who invokes the function  rather than the privileges of the definer of the function this allows the creation of libraries of functions that can run under the same authorization as the invoker 4.6.4 authorizations on schema the sql standard specifies a primitive authorization mechanism for the database schema  only the owner of the schema can carry out any modification to the schema  such as creating or deleting relations  adding or dropping attributes of relations  and adding or dropping indices thesumit67.blogspot.com 148 chapter 4 intermediate sql however  sql includes a references privilege that permits a user to declare foreign keys when creating relations the sql references privilege is granted on specific attributes in a manner like that for the update privilege the following grant statement allows user mariano to create relations that reference the key branch name of the branch relation as a foreign key  grant references  dept name  on department to mariano ; initially  it may appear that there is no reason ever to prevent users from creating foreign keys referencing another relation however  recall that foreignkey constraints restrict deletion and update operations on the referenced relation suppose mariano creates a foreign key in a relation r referencing the dept name attribute of the department relation and then inserts a tuple into r pertaining to the geology department it is no longer possible to delete the geology department fromthe department relation without also modifying relation r  thus  the definition of a foreign key bymariano restricts future activity by other users ; therefore  there is a need for the references privilege continuing to use the example of the department relation  the references privilege on department is also required to create a check constraint on a relation r if the constraint has a subquery referencing department this is reasonable for the same reason as the one we gave for foreign-key constraints ; a check constraint that references a relation limits potential updates to that relation 4.6.5 transfer of privileges a user who has been granted some form of authorization may be allowed to pass on this authorization to other users by default  a user/role that is granted a privilege is not authorized to grant that privilege to another user/role ifwe wish to grant a privilege and to allow the recipient to pass the privilege on to other users  we append the with grant option clause to the appropriate grant command for example  ifwewish to allowamit the select privilege on department and allow amit to grant this privilege to others  we write  grant select on department to amit with grant option ; the creator of an object  relation/view/role  holds all privileges on the object  including the privilege to grant privileges to others consider  as an example  the granting of update authorization on the teaches relation of the university database assume that  initially  the database administrator grants update authorization on teaches to users u1  u2  and u3  who may in turn pass on this authorization to other users the passing of a specific authorization from one user to another can be represented by an authorization graph the nodes of this graph are the users consider the graph for update authorization on teaches the graph includes an edge ui ? uj if user ui grants update authorization on teaches to uj  the root of the graph is the database administrator in the sample graph in figure 4.10  thesumit67.blogspot.com 4.6 authorization 149 u3 dba u1 u2 u5 u4 figure 4.10 authorization-grant graph  u1,u2     ,u5 are users and dba refers to the database administrator   observe that user u5 is granted authorization by both u1 and u2 ; u4 is granted authorization by only u1 a user has an authorization if and only if there is a path from the root of the authorization graph  the node representing the database administrator  down to the node representing the user 4.6.6 revoking of privileges suppose that the database administrator decides to revoke the authorization of useru1 sinceu4 has authorization fromu1  that authorization should be revoked as well however  u5 was granted authorization by both u1 and u2 since the database administrator did not revoke update authorization on teaches from u2  u5 retains update authorization on teaches if u2 eventually revokes authorization from u5  then u5 loses the authorization a pair of devious users might attempt to defeat the rules for revocation of authorization by granting authorization to each other for example  if u2 is initially granted an authorization by the database administrator  and u2 further grants it to u3 suppose u3 now grants the privilege back to u2 if the database administrator revokes authorization from u2  it might appear that u2 retains authorization through u3 however  note that once the administrator revokes authorization from u2  there is no path in the authorization graph from the root to either u2 or to u3 thus  sql ensures that the authorization is revoked from both the users as we just saw  revocation of a privilege from a user/role may cause other users/roles also to lose that privilege this behavior is called cascading revocation in most database systems  cascading is the default behavior however  the revoke statement may specify restrict in order to prevent cascading revocation  revoke select on department from amit  satoshi restrict ; thesumit67.blogspot.com 150 chapter 4 intermediate sql in this case  the system returns an error if there are any cascading revocations  and does not carry out the revoke action the keyword cascade can be used instead of restrict to indicate that revocation should cascade ; however  it can be omitted  as we have done in the preceding examples  since it is the default behavior the following revoke statement revokes only the grant option  rather than the actual select privilege  revoke grant option for select on department from amit ; note that some database implementations do not support the above syntax ; instead  the privilege itself can be revoked  and then granted again without the grant option cascading revocation is inappropriate in many situations suppose satoshi has the role of dean  grants instructor to amit  and later the role dean is revoked from satoshi  perhaps because satoshi leaves the university  ; amit continues to be employed on the faculty  and should retain the instructor role to deal with the above situation  sql permits a privilege to be granted by a role rather than by a user sql has a notion of the current role associated with a session by default  the current role associated with a session is null  except in some special cases   the current role associated with a session can be set by executing set role role name the specified role must have been granted to the user  else the set role statement fails to grant a privilege with the grantor set to the current role associated with a session  we can add the clause  granted by current role to the grant statement  provided the current role is not null suppose the granting of the role instructor  or other privileges  to amit is done using the granted by current role clause  with the current role set to dean   instead of the grantor being the user satoshi then  revoking of roles/privileges  including the role dean  fromsatoshi will not result in revoking of privileges that had the grantor set to the role dean  even if satoshi was the user who executed the grant ; thus  amit would retain the instructor role even after satoshi ? s privileges are revoked 4.7 summary ? sql supports several types of joins including inner and outer joins and several types of join conditions ? view relations can be defined as relations containing the result of queries views are useful for hiding unneeded information  and for collecting together information from more than one relation into a single view thesumit67.blogspot.com review terms 151 ? transactions are a sequence of queries and updates that together carry out a task transactions can be committed  or rolled back ; when a transaction is rolled back  the effects of all updates performed by the transaction are undone ? integrity constraints ensure that changes made to the database by authorized users do not result in a loss of data consistency ? referential-integrity constraints ensure that a value that appears in one relation for a given set of attributes also appears for a certain set of attributes in another relation ? domain constraints specify the set of possible values that may be associated with an attribute such constraints may also prohibit the use of null values for particular attributes ? assertions are declarative expressions that state predicates that we require always to be true ? the sql data-definition language provides support for defining built-in domain types such as date and time  as well as user-defined domain types ? sql authorization mechanisms allow one to differentiate among the users of the database as far as the type of access they are permitted on various data values in the database ? a user who has been granted some form of authority may be allowed to pass on this authority to other users however  we must be careful about how authorization can be passed among users if we are to ensure that such authorization can be revoked at some future time ? roles help to assign a set of privileges to a user according to the role that the user plays in the organization review terms ? join types ? inner and outer join ? left  right and full outer join ? natural  using  and on ? view definition ? materialized views ? view update ? transactions ? commit work ? rollback work ? atomic transaction ? integrity constraints ? domain constraints ? unique constraint ? check clause ? referential integrity ? cascading deletes ? cascading updates thesumit67.blogspot.com 152 chapter 4 intermediate sql ? assertions ? date and time types ? default values ? indices ? large objects ? user-defined types ? domains ? catalogs ? schemas ? authorization ? privileges ? select ? insert ? update ? all privileges ? granting of privileges ? revoking of privileges ? privilege to grant privileges ? grant option ? roles ? authorization on views ? execute authorization ? invoker privileges ? row-level authorization practice exercises 4.1 write the following queries in sql  a display a list of all instructors  showing their id  name  and the number of sections that they have taught make sure to show the number of sections as 0 for instructors who have not taught any section your query should use an outerjoin  and should not use scalar subqueries b write the same query as above  but using a scalar subquery  without outerjoin c display the list of all course sections offered in spring 2010  along with the names of the instructors teaching the section if a section has more than one instructor  it should appear as many times in the result as it has instructors if it does not have any instructor  it should still appear in the result with the instructor name set to ? ? ?  d display the list of all departments  with the total number of instructors in each department  without using scalar subqueries make sure to correctly handle departments with no instructors 4.2 outer join expressions can be computed in sql without using the sql outer join operation to illustrate this fact  show how to rewrite each of the following sql queries without using the outer join expression a select * from student natural left outer join takes b select * from student natural full outer join takes thesumit67.blogspot.com practice exercises 153 4.3 suppose we have three relations r  a  b   s  b  c   and t  b  d   with all attributes declared as not null consider the expressions ? r natural left outer join  s natural left outer join t   and ?  r natural left outer join s  natural left outer join t a give instances of relations r  s and t such that in the result of the second expression  attribute c has a null value but attribute d has a non-null value b is the above pattern  with c null and d not null possible in the result of the first expression ? explain why or why not 4.4 testing sql queries  to test if a query specified in english has been correctly written in sql  the sql query is typically executed on multiple test databases  and a human checks if the sql query result on each test database matches the intention of the specification in english a in section 3.3.3 we saw an example of an erroneous sql query which was intended to find which courses had been taught by each instructor ; the query computed the natural join of instructor  teaches  and course  and as a result unintentionally equated the dept name attribute of instructor and course give an example of a dataset that would help catch this particular error b when creating test databases  it is important to create tuples in referenced relations that do not have any matching tuple in the referencing relation  for each foreign key explain why  using an example query on the university database c when creating test databases  it is important to create tupleswith null values for foreign key attributes  provided the attribute is nullable  sql allows foreign key attributes to take on null values  as long as they are not part of the primary key  and have not been declared as not null   explain why  using an example query on the university database hint  use the queries from exercise 4.1 4.5 show how to define the view student grades  id  gpa  giving the gradepoint average of each student  based on the query in exercise 3.2 ; recall that we used a relation grade points  grade  points  to get the numeric points associated with a letter grade make sure your view definition correctly handles the case of null values for the grade attribute of the takes relation 4.6 complete the sql ddl definition of the university database of figure 4.8 to include the relations student  takes  advisor  and prereq thesumit67.blogspot.com 154 chapter 4 intermediate sql employee  employee name  street  city  works  employee name  company name  salary  company  company name  city  manages  employee name  manager name  figure 4.11 employee database for figure 4.7 and 4.12 4.7 consider the relational database of figure 4.11 give an sql ddl definition of this database identify referential-integrity constraints that should hold  and include them in the ddl definition 4.8 as discussed in section 4.4.7  we expect the constraint ? an instructor can not teach sections in two different classrooms in a semester in the same time slot ? to hold a write an sql query that returns all  instructor  section  combinations that violate this constraint b write an sql assertion to enforce this constraint  as discussed in section 4.4.7  current generation database systems do not support such assertions  although they are part of the sql standard   4.9 sql allows a foreign-key dependency to refer to the same relation  as in the following example  create table manager  employee name varchar  20  not null manager name varchar  20  not null  primary key employee name  foreign key  manager name  references manager on delete cascade  here  employee name is a key to the table manager  meaning that each employee has atmost one manager the foreign-key clause requires that every manager also be an employee explain exactly what happens when a tuple in the relation manager is deleted 4.10 sql provides an n-ary operation called coalesce  which is defined as follows  coalesce  a1  a2      an  returns the first nonnull ai in the list a1  a2      an  and returns null if all of a1  a2      an are null let a and b be relations with the schemas a  name  address  title   and b  name  address  salary   respectively show how to express a natural full outer join b using the full outer-join operation with an on condition and the coalesce operation make sure that the result relation does not contain two copies of the attributes name and address  and that the solution is correct even if some tuples in a and b have null values for attributes name or address thesumit67.blogspot.com exercises 155 salaried worker  name  office  phone  salary  hourly worker  name  hourly wage  address  name  street  city  figure 4.12 employee database for exercise 4.16 4.11 some researchers have proposed the concept of marked nulls a marked null ? i is equal to itself  but if i  = j  then ? i  = ? j  one application of marked nulls is to allow certain updates through views consider the view instructor info  section 4.2   show how you can use marked nulls to allow the insertion of the tuple  99999  ? johnson ?  ? music ?  through instructor info exercises 4.12 for the database of figure 4.11  write a query to find those employees with no manager note that an employee may simply have no manager listed or may have a null manager write your query using an outer join and then write it again using no outer join at all 4.13 under what circumstances would the query select * from student natural full outer join takes natural full outer join course include tuples with null values for the title attribute ? 4.14 show how to define a view tot credits  year  num credits   giving the total number of credits taken by students in each year 4.15 show how to express the coalesce operation from exercise 4.10 using the case operation 4.16 referential-integrity constraints as defined in this chapter involve exactly two relations consider a database that includes the relations shown in figure 4.12 suppose thatwewish to require that every name that appears in address appears in either salaried worker or hourly worker  but not necessarily in both a propose a syntax for expressing such constraints b discuss the actions that the system must take to enforce a constraint of this form 4.17 explain why  when a manager  say satoshi  grants an authorization  the grant should be done by the manager role  rather than by the user satoshi thesumit67.blogspot.com 156 chapter 4 intermediate sql 4.18 suppose user a  who has all authorizations on a relation r  grants select on relation r to public with grant option suppose user b then grants select on r to a does this cause a cycle in the authorization graph ? explain why 4.19 database systems that store each relation in a separate operating-system file may use the operating system ? s authorization scheme  instead of defining a special scheme themselves discuss an advantage and a disadvantage of such an approach bibliographical notes see the bibliographic notes of chapter 3 for sql reference material the rules used by sql to determine the updatability of a view  and how updates are reflected on the underlying database relations  are defined by the sql  1999 standard  and are summarized in melton and simon  2001   thesumit67.blogspot.com chapter5 advanced sql in chapters 3 and 4  we provided detailed coverage of the basic structure of sql in this chapter  we cover some of themore advanced features of sql.1 we address the issue of how to access sql from a general-purpose programming language  which is very important for building applications that use a database to store and retrieve data we describe how procedural code can be executed within the database  either by extending the sql language to support procedural actions  or by allowing functions defined in procedural languages to be executed within the database we describe triggers  which can be used to specify actions that are to be carried out automatically on certain events such as insertion  deletion  or update of tuples in a specified relation we discuss recursive queries and advanced aggregation features supported by sql finally  we describe online analytic processing  olap  systems  which support interactive analysis of very large datasets 5.1 accessing sql from a programming language sql provides a powerful declarative query language writing queries in sql is usually much easier than coding the same queries in a general-purpose programming language however  a database programmer must have access to a general-purpose programming language for at least two reasons  1 not all queries can be expressed in sql  since sql does not provide the full expressive power of a general-purpose language that is  there exist queries that can be expressed in a language such as c  java  or cobol that can not be expressed in sql to write such queries  we can embed sql within a more powerful language 1note regarding chapter and section sequencing  database design ? chapters 7 and 8 ? can be studied independently of the material in this chapter it is quite possible to study database design first  and study this chapter later however  for courses with a programming emphasis  a richer variety of laboratory exercises is possible after studying section 5.1  and we recommend that it be covered before database design for such courses 157 thesumit67.blogspot.com 158 chapter 5 advanced sql 2 nondeclarative actions ? such as printing a report  interacting with a user  or sending the results of a query to a graphical user interface ? can not be done from within sql applications usually have several components  and querying or updating data is only one component ; other components are written in general-purpose programming languages for an integrated application  there must be a means to combine sql with a general-purpose programming language there are two approaches to accessing sql from a general-purpose programming language  ? dynamic sql  a general-purpose program can connect to and communicate with a database server using a collection of functions  for procedural languages  or methods  for object-oriented languages   dynamic sql allows the program to construct an sql query as a character string at runtime  submit the query  and then retrieve the result into program variables a tuple at a time the dynamic sql component of sql allows programs to construct and submit sql queries at runtime in this chapter  we look at two standards for connecting to an sql database and performing queries and updates one  jdbc  section 5.1.1   is an application program interface for the java language the other  odbc  section 5.1.2   is an application program interface originally developed for the c language  and subsequently extended to other languages such as c + +  c #  and visual basic ? embedded sql  like dynamic sql  embedded sql provides a means by which a program can interact with a database server however  under embedded sql  the sql statements are identified at compile time using a preprocessor the preprocessor submits the sql statements to the database system for precompilation and optimization ; then it replaces the sql statements in the application program with appropriate code and function calls before invoking the programming-language compiler section 5.1.3 covers embedded sql a major challenge in mixing sql with a general-purpose language is the mismatch in the ways these languages manipulate data in sql  the primary type of data is the relation sql statements operate on relations and return relations as a result programming languages normally operate on a variable at a time  and those variables correspond roughly to the value of an attribute in a tuple in a relation thus  integrating these two types of languages into a single application requires providing a mechanism to return the result of a query in a manner that the program can handle 5.1.1 jdbc the jdbc standard defines an application program interface  api  that java programs can use to connect to database servers  the word jdbc was originally thesumit67.blogspot.com 5.1 accessing sql from a programming language 159 public static void jdbcexample  string userid  string passwd   try  class.forname  " oracle.jdbc.driver.oracledriver "  ; connection conn = drivermanager.getconnection  " jdbc  oracle  thin  @ db.yale.edu  1521  univdb "  userid  passwd  ; statement stmt = conn.createstatement   ; try  stmt.executeupdate  " insert into instructor values  ? 77987 ?  ? kim ?  ? physics ?  98000  "  ;  catch  sqlexception sqle   system.out.println  " could not insert tuple " + sqle  ;  resultset rset = stmt.executequery  " select dept name  avg  salary  " + " from instructor " + " group by dept name "  ; while  rset.next     system.out.println  rset.getstring  " dept name "  + " " + rset.getfloat  2   ;  stmt.close   ; conn.close   ;  catch  exception sqle   system.out.println  " exception  " + sqle  ;   figure 5.1 an example of jdbc code an abbreviation for java database connectivity  but the full form is no longer used  figure 5.1 shows an example java program that uses the jdbc interface it illustrates how connections are opened  how statements are executed and results processed  and how connections are closed we discuss this example in detail in this section the java program must import java.sql *  which contains the interface definitions for the functionality provided by jdbc 5.1.1.1 connecting to the database the first step in accessing a database froma java program is to open a connection to the database this step is required to select which database to use  for example  thesumit67.blogspot.com 160 chapter 5 advanced sql an instance of oracle running on your machine  or a postgresql database running on anothermachine only after opening a connection can a java program execute sql statements a connection is opened using the getconnection method of the driver manager class  within java.sql   this method takes three parameters.2 ? the first parameter to the getconnection call is a string that specifies the url  ormachine name,where the server runs  in our example  db.yale.edu   along with possibly some other information such as the protocol to be used to communicate with the database  in our example  jdbc  oracle  thin  ; we shall shortly see why this is required   the port number the database system uses for communication  in our example  2000   and the specific database on the server to be used  in our example  univdb   note that jdbc specifies only the api  not the communication protocol.ajdbc driver may support multiple protocols  and we must specify one supported by both the database and the driver the protocol details are vendor specific ? the second parameter to getconnection is a database user identifier  which is a string ? the third parameter is a password  which is also a string  note that the need to specify a password within the jdbc code presents a security risk if an unauthorized person accesses your java code  in our example in the figure  we have created a connection object whose handle is conn each database product that supports jdbc  all the major database vendors do  provides a jdbc driver that must be dynamically loaded in order to access the database from java in fact  loading the driver must be done first  before connecting to the database this is done by invoking class.forname with one argument specifying a concrete class implementing the java.sql.driver interface  in the first line of the program in figure 5.1 this interface provides for the translation of productindependent jdbc calls into the product-specific calls needed by the specific database management system being used the example in the figure shows the oracle driver  oracle.jdbc.driver.oracledriver.3 the driver is available in a .jar file at vendor web sites and should be placed within the classpath so that the java compiler can access it the actual protocol used to exchange information with the database depends on the driver that is used  and is not defined by the jdbc standard some 2there are multiple versions of the getconnection method  which differ in the parameters that they accept we present the most commonly used version 3the equivalent driver names for other products are as follows  ibm db2  com.ibm.db2.jdbc.app.db2driver ; microsoft sql server  com.microsoft.sqlserver.jdbc.sqlserverdriver ; postgresql  org.postgresql.driver ; and mysql  com.mysql.jdbc.driver sun also offers a ? bridge driver ? that converts jdbc calls to odbc this should be used only for vendors that support odbc but not jdbc thesumit67.blogspot.com 5.1 accessing sql from a programming language 161 drivers support more than one protocol  and a suitable protocol must be chosen depending on what protocol the database that you are connecting to supports in our example  when opening a connection with the database  the string jdbc  oracle  thin  specifies a particular protocol supported by oracle 5.1.1.2 shipping sql statements to the database system once a database connection is open  the program can use it to send sql statements to the database system for execution this is done via an instance of the class statement a statement object is not the sql statement itself  but rather an object that allows the java program to invoke methods that ship an sql statement given as an argument for execution by the database system our example creates a statement handle  stmt  on the connection conn to execute a statement  we invoke either the executequery method or the executeupdate method  depending on whether the sql statement is a query  and  thus  returns a result set  or nonquery statement such as update  insert  delete  create table  etc in our example  stmt.executeupdate executes an update statement that inserts into the instructor relation it returns an integer giving the number of tuples inserted  updated  or deleted for ddl statements  the return value is zero the try      catch      construct permits us to catch any exceptions  error conditions  that arise when jdbc calls are made  and print an appropriate message to the user 5.1.1.3 retrieving the result of a query the example program executes a query by using stmt.executequery it retrieves the set of tuples in the result into a resultset object rset and fetches them one tuple at a time the next method on the result set tests whether or not there remains at least one unfetched tuple in the result set and if so  fetches it the return value of the next method is a boolean indicating whether it fetched a tuple attributes from the fetched tuple are retrieved using various methods whose names begin with get the method getstring can retrieve any of the basic sql data types  converting the value to a java string object   but more restrictive methods such as getfloat can be used as well the argument to the various get methods can either be an attribute name specified as a string  or an integer indicating the position of the desired attribute within the tuple figure 5.1 shows two ways of retrieving the values of attributes in a tuple  using the name of the attribute  dept name  and using the position of the attribute  2  to denote the second attribute   the statement and connection are both closed at the end of the java program note that it is important to close the connection because there is a limit imposed on the number of connections to the database ; unclosed connections may cause that limit to be exceeded if this happens  the application can not open any more connections to the database thesumit67.blogspot.com 162 chapter 5 advanced sql preparedstatement pstmt = conn.preparestatement  " insert into instructor values  ?  ?  ?  ?  "  ; pstmt.setstring  1  " 88877 "  ; pstmt.setstring  2  " perry "  ; pstmt.setstring  3  " finance "  ; pstmt.setint  4  125000  ; pstmt.executeupdate   ; pstmt.setstring  1  " 88878 "  ; pstmt.executeupdate   ; figure 5.2 prepared statements in jdbc code 5.1.1.4 prepared statements we can create a prepared statement in which some values are replaced by ? ? ?  thereby specifying that actual values will be provided later the database system compiles the query when it is prepared each time the query is executed  with new values to replace the ? ? ? s   the database system can reuse the previously compiled form of the query and apply the new values the code fragment in figure 5.2 shows how prepared statements can be used the preparestatement method of the connection class submits an sql statement for compilation it returns an object of class preparedstatement at this point  no sql statement has been executed the executequery and executeupdate methods of preparedstatement class do that but before they can be invoked  we must use methods of class preparedstatement that assign values for the ? ? ? parameters the setstring method and other similar methods such as setint for other basic sql types allow us to specify the values for the parameters the first argument specifies the ? ? ? parameter for which we are assigning a value  the first parameter is 1  unlike most other java constructs  which start with 0   the second argument specifies the value to be assigned in the example in the figure  we prepare an insert statement  set the ? ? ? parameters  and then invoke executeupdate the final two lines of our example show that parameter assignments remain unchanged until we specifically reassign them thus  the final statement  which invokes executeupdate  inserts the tuple  ? 88878 ?  ? perry ?  ? finance ?  125000   prepared statements allow for more efficient execution in cases where the same query can be compiled once and then run multiple times with different parameter values however  there is an even more significant advantage to prepared statements that makes them the preferred method of executing sql queries whenever a user-entered value is used  even if the query is to be run only once suppose that we read in a user-entered value and then use java string manipulation to construct the sql statement if the user enters certain special characters  such as a single quote  the resulting sql statement may be syntactically incorrect unless we take extraordinary care in checking the input the setstring method does this for us automatically and inserts the needed escape characters to ensure syntactic correctness thesumit67.blogspot.com 5.1 accessing sql from a programming language 163 in our example  suppose that the values for the variables id  name  dept name  and salary have been entered by a user  and a corresponding row is to be inserted into the instructor relation suppose that  instead of using a prepared statement  a query is constructed by concatenating the strings using the following java expression  " insert into instructor values  ? " + id + " ?  ? " + name + " ?  " + " ? + dept name + " ?  " ? balance + "  " and the query is executed directly using the executequery method of a statement object now  if the user typed a single quote in the id or name fields  the query stringwould have a syntax error it is quite possible that an instructor name may have a quotation mark in its name  for example  ? o ? henry ?   while the above example might be considered an annoyance  the situation can be much worse a technique called sql injection can be used by malicious hackers to steal data or damage the database suppose a java programinputs a string name and constructs the query  " select * from instructor where name = ? " + name + " ? " if the user  instead of entering a name  enters  x ? or ? y ? = ? y then the resulting statement becomes  " select * from instructor where name = ? " + " x ? or ? y ? = ? y " + " ? " which is  select * from instructor where name = ? x ? or ? y ? = ? y ? in the resulting query  the where clause is always true and the entire instructor relation is returned more clever malicious users could arrange to output even more data use of a prepared statement would prevent this problem because the input string would have escape characters inserted  so the resulting query becomes  " select * from instructor where name = ? x \ ? or \ ? y \ ? = \ ? y ? which is harmless and returns the empty relation older systems allow multiple statements to be executed in a single call  with statements separated by a semicolon this feature is being eliminated because the sql injection technique was used by malicious hackers to insert whole sql statements because these statements run with the privileges of the owner of the thesumit67.blogspot.com 164 chapter 5 advanced sql java program  devastating sql statements such as drop table could be executed developers of sql applications need to be wary of such potential security holes 5.1.1.5 callable statements jdbc also provides a callablestatement interface that allows invocation of sql stored procedures and functions  described later  in section 5.2   these play the same role for functions and procedures as preparestatement does for queries callablestatement cstmt1 = conn.preparecall  "  ? = call some function  ?   "  ; callablestatement cstmt2 = conn.preparecall  "  call some procedure  ?  ?   "  ; the data types of function return values and out parameters of procedures must be registered using the method registeroutparameter    and can be retrieved using get methods similar to those for result sets see a jdbc manual for more details 5.1.1.6 metadata features as we noted earlier  a java application program does not include declarations for data stored in the database those declarations are part of the sql ddl statements therefore  a java program that uses jdbc must either have assumptions about the database schema hard-coded into the program or determine that information directly from the database system at runtime the latter approach is usually preferable  since it makes the application program more robust to changes in the database schema recall that when we submit a query using the executequery method  the result of the query is contained in a resultset object the interface resultset has a method  getmetadata    that returns a resultsetmetadata object that contains metadata about the result set resultsetmetadata  in turn  hasmethods to find metadata information  such as the number of columns in the result  the name of a specified column  or the type of a specified column in this way  we can execute a query even if we have no idea of the schema of the result the java code segment below uses jdbc to print out the names and types of all columns of a result set the variable rs in the code below is assumed to refer to a resultset instance obtained by executing a query resultsetmetadata rsmd = rs.getmetadata   ; for  int i = 1 ; i < = rsmd.getcolumncount   ; i + +   system.out.println  rsmd.getcolumnname  i   ; system.out.println  rsmd.getcolumntypename  i   ;  the getcolumncount method returns the arity  number of attributes  of the result relation that allows us to iterate through each attribute  note that we start thesumit67.blogspot.com 5.1 accessing sql from a programming language 165 databasemetadata dbmd = conn.getmetadata   ; resultset rs = dbmd.getcolumns  null  " univdb "  " department "  " % "  ; // arguments to getcolumns  catalog  schema-pattern  table-pattern  // and column-pattern // returns  one row for each column ; row has a number of attributes // such as column name  type name while  rs.next     system.out.println  rs.getstring  " column name "   rs.getstring  " type name "  ;  figure 5.3 finding column information in jdbc using databasemetadata at 1  as is conventional in jdbc   for each attribute  we retrieve its name and data type using the methods getcolumnname and getcolumntypename  respectively the databasemetadata interface provides a way to find metadata about the database the interface connection has a method getmetadata that returns a databasemetadata object the databasemetadata interface in turn has a very large number of methods to get metadata about the database and the database system to which the application is connected for example  there are methods that return the product name and version number of the database system other methods allow the application to query the database system about its supported features still other methods return information about the database itself the code in figure 5.3 illustrates how to find information about columns  attributes  of relations in a database the variable conn is assumed to be a handle for an already opened database connection the method getcolumns takes four arguments  a catalog name  null signifies that the catalog name is to be ignored   a schema name pattern  a table name pattern  and a column name pattern the schema name  table name  and column name patterns can be used to specify a name or a pattern patterns can use the sql string matching special characters ? % ? and ? ? ; for instance  the pattern ? % ? matches all names only columns of tables of schemas satisfying the specified name or pattern are retrieved each row in the result set contains information about one column the rows have a number of columns such as the name of the catalog  schema  table and column  the type of the column  and so on examples of other methods provided by databasemetadata that provide information about the database include those for retrieval of metadata about relations  gettables     foreign-key references  getcrossreference     authorizations  database limits such as maximum number of connections  and so on the metadata interfaces can be used for a variety of tasks for example  they can be used to write a database browser that allows a user to find the tables in a database  examine their schema  examine rows in a table  apply selections to see desired rows  and so on the metadata information can be used to make code thesumit67.blogspot.com 166 chapter 5 advanced sql used for these tasks generic ; for example  code to display the rows in a relation can be written in such a way that it would work on all possible relations regardless of their schema similarly  it is possible to write code that takes a query string  executes the query  and prints out the results as a formatted table ; the code can work regardless of the actual query submitted 5.1.1.7 other features jdbc provides a number of other features  such as updatable result sets it can create an updatable result set from a query that performs a selection and/or a projection on a database relation an update to a tuple in the result set then results in an update to the corresponding tuple of the database relation recall from section 4.3 that a transaction allows multiple actions to be treated as a single atomic unit which can be committed or rolled back by default  each sql statement is treated as a separate transaction that is committed automatically the method setautocommit   in the jdbc connection interface allows this behavior to be turned on or off thus  if conn is an open connection  conn.setautocommit  false  turns off automatic commit transactions must then be committed or rolled back explicitly using either conn.commit   or conn.rollback    conn.setautocommit  true  turns on automatic commit jdbc provides interfaces to deal with large objects without requiring an entire large object to be created in memory to fetch large objects  the resultset interface provides methods getblob   and getclob   that are similar to the getstring   method  but return objects of type blob and clob  respectively these objects do not store the entire large object  but instead store ? locators ? for the large objects  that is  logical pointers to the actual large object in the database fetching data from these objects is very much like fetching data from a file or an input stream  and can be performed using methods such as getbytes and getsubstring conversely  to store large objects in the database  the preparedstatement class permits a database column whose type is blob to be linked to an input stream  such as a file that has been opened  using the method setblob  int parameterindex  inputstream inputstream  .when the prepared statement is executed  data are read from the input stream  and written to the blob in the database similarly  a clob column can be set using the setclob method  which takes as arguments a parameter index and a character stream jdbc includes a row set feature that allows result sets to be collected and shipped to other applications row sets can be scanned both backward and forward and can be modified because row sets are not part of the database itself once they are downloaded  we do not cover details of their use here 5.1.2 odbc the open database connectivity  odbc  standard defines an api that applications can use to open a connectionwith a database  send queries and updates  and get back results applications such as graphical user interfaces  statistics packthesumit67 blogspot.com 5.1 accessing sql from a programming language 167 void odbcexample    retcode error ; henv env ; / * environment * / hdbc conn ; / * database connection * / sqlallocenv  &env  ; sqlallocconnect  env  &conn  ; sqlconnect  conn  " db.yale.edu "  sql nts  " avi "  sql nts  " avipasswd "  sql nts  ;  char deptname  80  ; float salary ; int lenout1  lenout2 ; hstmt stmt ; char * sqlquery = " select dept name  sum  salary  from instructor group by dept name " ; sqlallocstmt  conn  &stmt  ; error = sqlexecdirect  stmt  sqlquery  sql nts  ; if  error = = sql success   sqlbindcol  stmt  1  sql c char  deptname  80  &lenout1  ; sqlbindcol  stmt  2  sql c float  &salary  0  &lenout2  ; while  sqlfetch  stmt  = = sql success   printf  " % s % g \ n "  depthname  salary  ;   sqlfreestmt  stmt  sql drop  ;  sqldisconnect  conn  ; sqlfreeconnect  conn  ; sqlfreeenv  env  ;  figure 5.4 odbc code example ages  and spreadsheets can make use of the same odbc api to connect to any database server that supports odbc each database system supporting odbc provides a library that must be linked with the client program when the client program makes an odbc api call  the code in the library communicateswith the server to carry out the requested action  and fetch results figure 5.4 shows an example of c code using the odbc api the first step in using odbc to communicate with a server is to set up a connection with the server to do so  the program first allocates an sql environment  then a thesumit67.blogspot.com 168 chapter 5 advanced sql database connection handle odbc defines the types henv  hdbc  and retcode the program then opens the database connection by using sqlconnect this call takes several parameters  including the connection handle  the server to which to connect  the user identifier  and the password for the database the constant sql nts denotes that the previous argument is a null-terminated string once the connection is set up  the program can send sql commands to the database by using sqlexecdirect c language variables can be bound to attributes of the query result  so that when a result tuple is fetched using sqlfetch  its attribute values are stored in correspondingcvariables the sqlbindcol function does this task ; the second argument identifies the position of the attribute in the query result  and the third argument indicates the type conversion required from sql to c the next argument gives the address of the variable for variable-length types like character arrays  the last two arguments give the maximum length of the variable and a location where the actual length is to be stored when a tuple is fetched a negative value returned for the length field indicates that the value is null for fixed-length types such as integer or float  the maximum length field is ignored  while a negative value returned for the length field indicates a null value the sqlfetch statement is in a while loop that is executed until sqlfetch returns a value other than sql success on each fetch  the program stores the values in c variables as specified by the calls on sqlbindcol and prints out these values at the end of the session  the program frees the statement handle  disconnects from the database  and frees up the connection and sql environment handles good programming style requires that the result of every function call must be checked to make sure there are no errors ; we have omitted most of these checks for brevity it ispossible tocreateansql statementwith parameters ; for example  consider the statement insert into department values  ?  ?  ?   the questionmarks are placeholders for values which will be supplied later the above statement can be ? prepared  ? that is  compiled at the database  and repeatedly executed by providing actual values for the placeholders ? in this case  by providing an department name  building  and budget for the relation department odbc defines functions for a variety of tasks  such as finding all the relations in the database and finding the names and types of columns of a query result or a relation in the database by default  each sql statement is treated as a separate transaction that is committed automatically the sqlsetconnectoption  conn  sql autocommit  0  turns off automatic commit on connection conn  and transactions must then be committed explicitly by sqltransact  conn  sql commit  or rolled back by sqltransact  conn  sql rollback   the odbc standard defines conformance levels  which specify subsets of the functionality defined by the standard an odbc implementation may provide only core level features  or it may provide more advanced  level 1 or level 2  features level 1 requires support for fetching information about the catalog  such as information about what relations are present and the types of their attributes thesumit67.blogspot.com 5.1 accessing sql from a programming language 169 ado.net the ado.net api  designed for the visual basic .net and c # languages  provides functions to access data,which at a high level are not dissimilar to the jdbc functions  although details differ like jdbc and odbc  the ado.net api allows access to results of sql queries  as well as to metadata  but is considerably simpler to use than odbc a database that supports odbc can be accessed using the ado.net api  and the ado.net calls are translated into odbc calls the ado.net api can also be used with some kinds of nonrelational data sources such as microsoft ? s ole-db  xml  covered in chapter 23   and more recently  the entity framework developed by microsoft see the bibliographic notes for more information on ado.net level 2 requires further features  such as the ability to send and retrieve arrays of parameter values and to retrieve more detailed catalog information the sql standard defines a call level interface  cli  that is similar to the odbc interface 5.1.3 embedded sql the sql standard defines embeddings of sql in a variety of programming languages  such as c  c + +  cobol  pascal  java  pl/i  and fortran a language in which sql queries are embedded is referred to as a host language  and the sql structures permitted in the host language constitute embedded sql programs written in the host language can use the embedded sql syntax to access and update data stored in a database an embedded sql program must be processed by a special preprocessor prior to compilation the preprocessor replaces embedded sql requests with host-language declarations and procedure calls that allow runtime execution of the database accesses then  the resulting program is compiled by the host-language compiler this is the main distinction between embedded sql and jdbc or odbc in jdbc  sql statements are interpreted at runtime  even if they are prepared first using the prepared statement feature   when embedded sql is used  some sql-related errors  including data-type errors  may be caught at compile time to identify embedded sql requests to the preprocessor  we use the exec sql statement ; it has the form  exec sql < embedded sql statement > ; the exact syntax for embedded sql requests depends on the language in which sql is embedded in some languages  such as cobol  the semicolon is replaced with end-exec thesumit67.blogspot.com 170 chapter 5 advanced sql we place the statement sql include sqlca in the program to identify the place where the preprocessor should insert the special variables used for communication between the program and the database system before executing any sql statements  the program must first connect to the database this is done using  exec sql connect to server user user-name using password ; here  server identifies the server to which a connection is to be established variables of the host language can be used within embedded sql statements  but they must be preceded by a colon    to distinguish them from sql variables variables used as above must be declared within a declare section  as illustrated below the syntax for declaring the variables  however  follows the usual host language syntax exec sql begin declare section ; int credit amount ; exec sql end declare section ; embedded sql statements are similar in form to regular sql statements there are  however  several important differences  as we note here to write a relational query  we use the declare cursor statement the result of the query is not yet computed rather  the program must use the open and fetch commands  discussed later in this section  to obtain the result tuples as we shall see  use of a cursor is analogous to iterating through a result set in jdbc consider the university schema assume that we have a host-language variable credit amount in our program  declared as we saw earlier  and that we wish to find the names of all students who have taken more than credit amount credit hours we can write this query as follows  exec sql declare c cursor for select id  name from student where tot cred >  credit amount ; the variable c in the preceding expression is called a cursor for the query.we use this variable to identify the query.we then use the open statement  which causes the query to be evaluated the open statement for our sample query is as follows  exec sql open c ; this statement causes the database system to execute the query and to save the resultswithin a temporary relation the query uses the value of the host-language variable  credit amount  at the time the open statement is executed thesumit67.blogspot.com 5.1 accessing sql from a programming language 171 if the sql query results in an error  the database system stores an error diagnostic in the sql communication-area  sqlca  variables we then use a series of fetch statements  each ofwhich causes the values of one tuple to be placed in host-language variables the fetch statement requires one host-language variable for each attribute of the result relation for our example query  we need one variable to hold the id value and another to hold the name value suppose that those variables are si and sn  respectively  and have been declared within a declare section then the statement  exec sql fetch c into  si   sn ; produces a tuple of the result relation the program can then manipulate the variables si and sn by using the features of the host programming language asingle fetch request returns only one tuple to obtain all tuples of the result  the program must contain a loop to iterate over all tuples embedded sql assists the programmer in managing this iteration although a relation is conceptually a set  the tuples of the result of a query are in some fixed physical order when the program executes an open statement on a cursor  the cursor is set to point to the first tuple of the result each time it executes a fetch statement  the cursor is updated to point to the next tuple of the result when no further tuples remain to be processed  the character array variable sqlstate in the sqlca is set to ? 02000 ?  meaning ? no more data ?  ; the exact syntax for accessing this variable depends on the specific database system you use thus  we can use a while loop  or equivalent loop  to process each tuple of the result we must use the close statement to tell the database system to delete the temporary relation that held the result of the query for our example  this statement takes the form exec sql close c ; embedded sql expressions for database modification  update  insert  and delete  do not return a result thus  they are somewhat simpler to express a database-modification request takes the form exec sql < any valid update  insert  or delete > ; host-language variables  preceded by a colon  may appear in the sql databasemodification expression if an error condition arises in the execution of the statement  a diagnostic is set in the sqlca database relations can also be updated through cursors for example  if we want to add 100 to the salary attribute of every instructor in the music department  we could declare a cursor as follows thesumit67.blogspot.com 172 chapter 5 advanced sql sqlj the java embedding of sql  called sqlj  provides the same features as other embedded sql implementations  but using a different syntax that more closely matches features already present in java  such as iterators for example  sqlj uses the syntax # sql instead of exec sql  and instead of cursors  uses the java iterator interface to fetch query results thus the result of executing a query is a java iterator  and the next   method of the java iterator interface can be used to step through the result tuples  just as the preceding examples use fetch on the cursor the iterator must have attributes declared  whose types match the types of the attributes in the sql query result the code snippet below illustrates the use of iterators # sql iterator deptinfoiter  string dept name  int avgsal  ; deptinfoiter iter = null ; # sql iter =  select dept name  avg  salary  from instructor group by dept name  ; while  iter.next     string deptname = iter.dept name   ; int avgsal = iter.avgsal   ; system.out.println  deptname + " " + avgsal  ;  iter.close   ; sqlj is supported by ibm db2 and oracle ; both provide translators that convert sqlj code into jdbc code the translator can connect to the database in order to check the syntactic correctness of queries at compile time  and to ensure that the sql types of query results are compatible with the java types of variables they are assigned to as of early 2009  sqlj is not supported by other database systems we do not describe sqlj in detail here ; see the bibliographic notes for more information exec sql declare c cursor for select * from instructor where dept name = ? music ? for update ; we then iterate through the tuples by performing fetch operations on the cursor  as illustrated earlier   and after fetching each tuple we execute the following code  thesumit67.blogspot.com 5.2 functions and procedures 173 exec sql update instructor set salary = salary + 100 where current of c ; transactions can be committed using exec sql commit  or rolled back using exec sql rollback queries in embedded sql are normally defined when the program is written there are rare situations where a query needs to be defined at runtime for example  an application interface may allow a user to specify selection conditions on one or more attributes of a relation  and may construct the where clause of an sql query at runtime  with conditions on only those attributes for which the user specifies selections in such cases  a query string can be constructed and prepared at runtime  using a statement of the form exec sql prepare < query-name > from  < variable >  and a cursor can be opened on the query name 5.2 functions and procedures we have already seen several functions that are built into the sql language in this section  we show how developers can write their own functions and procedures  store themin the database  and then invoke themfrom sql statements functions are particularly useful with specialized data types such as images and geometric objects for instance  a line-segment data type used in amapdatabasemayhave an associated function that checkswhether two line segments overlap  and an image data type may have associated functions to compare two images for similarity procedures and functions allow ? business logic ? to be stored in the database  and executed from sql statements for example  universities usually have many rules about how many courses a student can take in a given semester  the minimum number of courses a full-time instructor must teach in a year  the maximum number of majors a student can be enrolled in  and so on while such business logic can be encoded as programming-language procedures stored entirely outside the database  defining them as stored procedures in the database has several advantages for example  it allows multiple applications to access the procedures  and it allows a single point of change in case the business rules change  without changing other parts of the application application code can then call the stored procedures  instead of directly updating database relations sql allows the definition of functions  procedures  and methods these can be defined either by the procedural component of sql  or by an external programming language such as java  c  or c + +  we look at definitions in sql first  and then see how to use definitions in external languages in section 5.2.3 although the syntax we present here is defined by the sql standard  most databases implement nonstandard versions of this syntax for example  the procedural languages supported by oracle  pl/sql   microsoft sql server  transactsql   and postgresql  pl/pgsql  all differ fromthe standard syntaxwe present thesumit67.blogspot.com 174 chapter 5 advanced sql create function dept count  dept name varchar  20   returns integer begin declare d count integer ; select count  *  into d count from instructor where instructor.dept name = dept name return d count ; end figure 5.5 function defined in sql here.we illustrate some of the differences  for the case of oracle  later  page 178   see the respective system manuals for further details although parts of the syntax we present here may not be supported on such systems  the concepts we describe are applicable across implementations  although with a different syntax 5.2.1 declaring and invoking sql functions and procedures suppose that we want a function that  given the name of a department  returns the count of the number of instructors in that department we can define the function as shown in figure 5.5.4 this function can be used in a query that returns names and budgets of all departments with more than 12 instructors  select dept name  budget from instructor where dept count  dept name  > 12 ; the sql standard supports functions that can return tables as results ; such functions are called table functions.5 consider the function defined in figure 5.6 the function returns a table containing all the instructors of a particular department note that the function ? s parameter is referenced by prefixing it with the name of the function  instructor of.dept name   the function can be used in a query as follows  select * from table  instructor of  ? finance ?   ; this query returns all instructors of the ? finance ? department in the above simple case it is straightforward towrite this querywithout using table-valued functions in general  however  table-valued functions can be thought of as parameterized views that generalize the regular notion of views by allowing parameters 4if you are entering your own functions or procedures  you should write ? create or replace ? rather than create so that it is easy to modify your code  by replacing the function  during debugging 5this feature first appeared in sql  2003 thesumit67.blogspot.com 5.2 functions and procedures 175 create function instructors of  dept name varchar  20   returns table  id varchar  5   name varchar  20   dept name varchar  20   salary numeric  8,2   return table  select id  name  dept name  salary from instructor where instructor.dept name = instructor of.dept name  ; figure 5.6 table function in sql sql also supports procedures the dept count function could instead be written as a procedure  create procedure dept count proc  in dept name varchar  20   out d count integer  begin select count  *  into d count from instructor where instructor.dept name = dept count proc.dept name end the keywords in and out indicate  respectively  parameters that are expected to have values assigned to them and parameters whose values are set in the procedure in order to return results procedures can be invoked either from an sql procedure or from embedded sql by the call statement  declare d count integer ; call dept count proc  ? physics ?  d count  ; procedures and functions can be invoked from dynamic sql  as illustrated by the jdbc syntax in section 5.1.1.4 sql permits more than one procedure of the same name  so long as the number of arguments of the procedures with the same name is different the name  along with the number of arguments  is used to identify the procedure sql also permits more than one function with the same name  so long as the different functions with the same name either have different numbers of arguments  or for functions with the same number of arguments  they differ in the type of at least one argument thesumit67.blogspot.com 176 chapter 5 advanced sql 5.2.2 language constructs for procedures and functions sql supports constructs that give it almost all the power of a general-purpose programming language the part of the sql standard that dealswith these constructs is called the persistent storagemodule  psm   variables are declared using a declare statement and can have any valid sql data type assignments are performed using a set statement a compound statement is of the form begin    end  and it may contain multiple sql statements between the begin and the end local variables can be declared within a compound statement  as we have seen in section 5.2.1 a compound statement of the form begin atomic    end ensures that all the statements contained within it are executed as a single transaction sql  1999 supports the while statements and the repeat statements by the following syntax  while boolean expression do sequence of statements ; end while repeat sequence of statements ; until boolean expression end repeat there is also a for loop that permits iteration over all results of a query  declare n integer default 0 ; for r as select budget from department where dept name = ? music ? do set n = n r.budget end for the program fetches the query results one row at a time into the for loop variable  r  in the above example   the statement leave can be used to exit the loop  while iterate starts on the next tuple  from the beginning of the loop  skipping the remaining statements the conditional statements supported by sql include if-then-else statements by using this syntax  if boolean expression then statement or compound statement elseif boolean expression then statement or compound statement else statement or compound statement end if thesumit67.blogspot.com 5.2 functions and procedures 177 ? ? registers a student after ensuring classroom capacity is not exceeded ? ? returns 0 on success  and -1 if capacity is exceeded create function registerstudent  in s id varchar  5   in s courseid varchar  8   in s secid varchar  8   in s semester varchar  6   in s year numeric  4,0   out errormsg varchar  100  returns integer begin declare currenrol int ; select count  *  into currenrol from takes where course id = s courseid and sec id = s secid and semester = s semester and year = s year ; declare limit int ; select capacity into limit from classroom natural join section where course id = s courseid and sec id = s secid and semester = s semester and year = s year ; if  currenrol < limit  begin insert into takes values  s id  s courseid  s secid  s semester  s year  null  ; return  0  ; end ? ? otherwise  section capacity limit already reached set errormsg = ? enrollment limit reached for course ? | | s courseid | | ? section ? | | s secid ; return  -1  ; end ; figure 5.7 procedure to register a student for a course section sql also supports a case statement similar to the c/c + + language case statement  in addition to case expressions  which we saw in chapter 3   figure 5.7 provides a larger example of the use of procedural constructs in sql the function registerstudent defined in the figure  registers a student in a course section  after verifying that the number of students in the section does not exceed the capacity of the room allocated to the section the function returns an error code  with a value greater than or equal to 0 signifying success  and a negative value signifying an error condition  and a message indicating the reason for the failure is returned as an out parameter thesumit67.blogspot.com 178 chapter 5 advanced sql nonstandard syntax for procedures and functions although the sql standard defines the syntax for procedures and functions  most databases do not follow the standard strictly  and there is considerable variation in the syntax supported one of the reasons for this situation is that these databases typically introduced support for procedures and functions before the syntax was standardized  and they continue to support their original syntax it is not possible to list the syntax supported by each database here  but we illustrate a few of the differences in the case of oracle ? s pl/sql  by showing below a version of the function from figure 5.5  as it would be defined in pl/sql create or replace function dept count  dept name in instructor.dept name % type  return integer as d count integer ; begin select count  *  into d count from instructor where instructor.dept name = dept name ; return d count ; end ; while the two versions are similar in concept  there are a number of minor syntactic differences  some of which are evident when comparing the two versions of the function although not shown here  the syntax for control flow in pl/sql also has several differences from the syntax presented here observe that pl/sql allows a type to be specified as the type of an attribute of a relation  by adding the suffix % type on the other hand  pl/sql does not directly support the ability to return a table  although there is an indirect way of implementing this functionality by creating a table type the procedural languages supported by other databases also have a number of syntactic and semantic differences see the respective language references for more information the sql procedural language also supports the signaling of exception conditions  and declaring of handlers that can handle the exception  as in this code  declare out of classroom seats condition declare exit handler for out of classroom seats begin sequence of statements end thesumit67.blogspot.com 5.2 functions and procedures 179 the statements between the begin and the end can raise an exception by executing signal out of classroom seats the handler says that if the condition arises  the action to be taken is to exit the enclosing begin end statement alternative actionswould be continue  which continues execution from the next statement following the one that raised the exception in addition to explicitly defined conditions  there are also predefined conditions such as sqlexception  sqlwarning  and not found 5.2.3 external language routines although the procedural extensions to sql can be very useful  they are unfortunately not supported in a standard way across databases even the most basic features have different syntax or semantics in different database products as a result  programmers have to essentially learn a new language for each database product an alternative that is gaining in support is to define procedures in an imperative programming language  but allow them to be invoked from sql queries and trigger definitions sql allows us to define functions in a programming language such as java,c #  c or c + +  functions defined in this fashion can be more efficient than functions defined in sql  and computations that can not be carried out in sql can be executed by these functions external procedures and functions can be specified in this way  note that the exact syntax depends on the specific database system you use   create procedure dept count proc  in dept name varchar  20   out count integer  language c external name ? /usr/avi/bin/dept count proc ? create function dept count  dept name varchar  20   returns integer language c external name ? /usr/avi/bin/dept count ? in general  the external language procedures need to deal with null values in parameters  both in and out  and return values they also need to communicate failure/success status  to deal with exceptions this information can be communicated by extra parameters  an sqlstate value to indicate failure/success status  a parameter to store the return value of the function  and indicator variables for each parameter/function result to indicate if the value is null other mechanisms are possible to handle null values  for example by passing pointers instead of values the exact mechanisms depend on the database however  if a function does not deal with these situations  an extra line parameter style general can be added to the declaration to indicate that the external procedures/functions take only the arguments shown and do not handle null values or exceptions functions defined in a programming language and compiled outside the database system may be loaded and executed with the database-system code thesumit67.blogspot.com 180 chapter 5 advanced sql however  doing so carries the risk that a bug in the program can corrupt the database internal structures  and can bypass the access-control functionality of the database system database systems that are concerned more about efficient performance than about securitymay execute procedures in such a fashion database systems that are concerned about security may execute such code as part of a separate process  communicate the parameter values to it  and fetch results back  via interprocess communication however  the time overhead of interprocess communication is quite high ; on typical cpu architectures  tens to hundreds of thousands of instructions can execute in the time taken for one interprocess communication if the code is written in a ? safe ? language such as java or c #  there is another possibility  executing the code in a sandbox within the database query execution process itself the sandbox allows the java or c # code to access its own memory area  but prevents the code from reading or updating the memory of the query execution process  or accessing files in the file system  creating a sandbox is not possible for a language such as c  which allows unrestricted access to memory through pointers  avoiding interprocess communication reduces function call overhead greatly several database systems today support external language routines running in a sandbox within the query execution process for example  oracle and ibm db2 allow java functions to run as part of the database process microsoft sql server allows procedures compiled into the common language runtime  clr  to executewithin the database process ; such procedures could have been written  for example  in c # or visual basic postgresql allows functions defined in several languages  such as perl  python  and tcl 5.3 triggers a trigger is a statement that the system executes automatically as a side effect of a modification to the database to design a trigger mechanism  we must meet two requirements  1 specify when a trigger is to be executed this is broken up into an event that causes the trigger to be checked and a condition that must be satisfied for trigger execution to proceed 2 specify the actions to be taken when the trigger executes onceweenter a trigger into the database  the database system takes on the responsibility of executing it whenever the specified event occurs and the corresponding condition is satisfied 5.3.1 need for triggers triggers can be used to implement certain integrity constraints that can not be specified using the constraint mechanism of sql triggers are also useful mechathesumit67 blogspot.com 5.3 triggers 181 nisms for alerting humans or for starting certain tasks automatically when certain conditions are met as an illustration  we could design a trigger that  whenever a tuple is inserted into the takes relation  updates the tuple in the student relation for the student taking the course by adding the number of credits for the course to the student ? s total credits as another example  suppose a warehouse wishes to maintain a minimum inventory of each item ; when the inventory level of an item falls below the minimum level  an order can be placed automatically on an update of the inventory level of an item  the trigger compares the current inventory level with the minimum inventory level for the item  and if the level is at or below the minimum  a new order is created note that trigger systems cannotusually performupdates outside the database  and hence  in the inventory replenishment example  we can not use a trigger to place an order in the external world instead  we add an order to a relation holding reorders.we must create a separate permanently running system process that periodically scans that relation and places orders some database systems provide built-in support for sending email fromsql queries and triggers  using the above approach 5.3.2 triggers in sql we now consider how to implement triggers in sql the syntaxwe present here is defined by the sql standard  but most databases implement nonstandard versions of this syntax although the syntax we present here may not be supported on such systems  the concepts we describe are applicable across implementations we discuss nonstandard trigger implementations later in this section  page 184   figure 5.8 showshowtriggers can be used to ensure referential integrity on the time slot id attribute of the section relation the first trigger definition in the figure specifies that the trigger is initiated after any insert on the relation section and it ensures that the time slot id value being inserted is valid an sql insert statement could insert multiple tuples of the relation  and the for each row clause in the trigger codewould then explicitly iterate over each inserted row the referencing new row as clause creates a variable nrow  called a transition variable  that stores the value of an inserted row after the insertion the when statement specifies a condition the system executes the rest of the trigger body only for tuples that satisfy the condition the begin atomic    end clause can serve to collect multiple sql statements into a single compound statement in our example  though  there is only one statement  which rolls back the transaction that caused the trigger to get executed thus any transaction that violates the referential integrity constraint gets rolled back  ensuring the data in the database satisfies the constraint it is not sufficient to check referential integrity on inserts alone,we also need to consider updates of section  as well as deletes and updates to the referenced table time slot the second trigger definition in figure 5.8 considers the case of deletes to time slot this trigger checks that the time slot id of the tuple being deleted is either still present in time slot  or that no tuple in section contains that particular time slot id value ; otherwise  referential integrity would be violated thesumit67.blogspot.com 182 chapter 5 advanced sql create trigger timeslot check1 after insert on section referencing new row as nrow for each row when  nrow.time slot id not in  select time slot id from time slot   / * time slot id not present in time slot * / begin rollback end ; create trigger timeslot check2 after delete on timeslot referencing old row as orow for each row when  orow.time slot id not in  select time slot id from time slot  / * last tuple for time slot id deleted from time slot * / and orow.time slot id in  select time slot id from section   / * and time slot id still referenced from section * / begin rollback end ; figure 5.8 using triggers to maintain referential integrity to ensure referential integrity,wewould also have to create triggers to handle updates to section and time slot ; we describe next howtriggers can be executed on updates  but leave the definition of these triggers as an exercise to the reader for updates  the trigger can specify attributes whose update causes the trigger to execute ; updates to other attributes would not cause it to be executed for example  to specify that a trigger executes after an update to the grade attribute of the takes relation  we write  after update of takes on grade the referencing old row as clause can be used to create a variable storing the old value of an updated or deleted row the referencing new row as clause can be used with updates in addition to inserts figure 5.9 shows how a trigger can be used to keep the tot cred attribute value of student tuples up-to-date when the grade attribute is updated for a tuple in the takes relation the trigger is executed only when the grade attribute is updated from a value that is either null or ? f ?  to a grade that indicates the course is successfully completed the update statement is normal sql syntax except for the use of the variable nrow thesumit67.blogspot.com 5.3 triggers 183 create trigger credits earned after update of takes on  grade  referencing new row as nrow referencing old row as orow for each row when nrow.grade < > ? f ? and nrow.grade is not null and  orow.grade = ? f ? or orow.grade is null  begin atomic update student set tot cred = tot cred +  select credits from course where course.course id = nrow.course id  where student.id = nrow.id ; end ; figure 5.9 using a trigger to maintain credits earned values a more realistic implementation of this example trigger would also handle grade corrections that change a successful completion grade to a fail grade  and handle insertions into the takes relation where the grade indicates successful completion we leave these as an exercise for the reader as another example of the use of a trigger  the action on delete of a student tuple could be to check if the student has any entries in the takes relation  and if so  to delete them many database systems support a variety of other triggering events  such as when a user  application  logs on to the database  that is  opens a connection   the system shuts down  or changes are made to system settings triggers can be activated before the event  insert  delete  or update  instead of after the event triggers that execute before an event can serve as extra constraints that can prevent invalid updates  inserts  or deletes instead of letting the invalid action proceed and cause an error  the trigger might take action to correct the problem so that the update  insert  or delete becomes valid for example  if we attempt to insert an instructor into a department whose name does not appear in the department relation  the trigger could insert a tuple into the department relation for that department name before the insertion generates a foreign-key violation as another example  suppose the value of an inserted grade is blank  presumably to indicate the absence of a grade.we can define a trigger that replaces the value by the null value the set statement can be used to carry out such modifications an example of such a trigger appears in figure 5.10 instead of carrying out an action for each affected row  we can carry out a single action for the entire sql statement that caused the insert  delete  or update to do so  we use the for each statement clause instead of the for each row clause the clauses referencing old table as or referencing new table as can then be used to refer to temporary tables  called transition tables  containing all the affected rows transition tables can not be used with before triggers  but can be thesumit67.blogspot.com 184 chapter 5 advanced sql create trigger setnull before update on takes referencing new row as nrow for each row when  nrow.grade = ? ?  begin atomic set nrow.grade = null ; end ; figure 5.10 example of using set to change an inserted value used with after triggers  regardless of whether they are statement triggers or row triggers a single sql statement can then be used to carry out multiple actions on the basis of the transition tables nonstandard trigger syntax although the trigger syntax we describe here is part of the sql standard  and is supported by ibm db2  most other database systems have nonstandard syntax for specifying triggers  and may not implement all features in the sql standard we outline a few of the differences below ; see the respective systemmanuals for further details for example  in the oracle syntax  unlike the sql standard syntax  the keyword row does not appear in the referencing statement the keyword atomic does not appear after begin the reference to nrow in the select statement nested in the update statement must begin with a colon    to inform the system that the variable nrow is defined externally from the sql statement further  subqueries are not allowed in the when and if clauses it is possible towork around this problem by moving complex predicates from the when clause into a separate query that saves the result into a local variable  and then reference that variable in an if clause  and the body of the trigger then moves into the corresponding then clause further  in oracle  triggers are not allowed to execute a transaction rollback directly ; however  they can instead use a function called raise application error to not only roll back the transaction  but also return an error message to the user/application that performed the update as another example  in microsoft sql server the keyword on is used instead of after the referencing clause is omitted  and old and new rows are referenced by the tuple variables deleted and inserted further  the for each row clause is omitted  and when is replaced by if the before specification is not supported  but an instead of specification is supported in postgresql  triggers do not have a body  but instead invoke a procedure for each row  which can access variables new and old containing the old and new values of the row instead of performing a rollback  the trigger can raise an exception  with an associated error message thesumit67.blogspot.com 5.3 triggers 185 create trigger reorder after update of amount on inventory referencing old row as orow  new row as nrow for each row when nrow.level < =  select level from minlevel where minlevel.item = orow.item  and orow.level >  select level from minlevel where minlevel.item = orow.item  begin atomic insert into orders  select item  amount from reorder where reorder.item = orow.item  ; end ; figure 5.11 example of trigger for reordering an item triggers can be disabled or enabled ; by default they are enabled when they are created  but can be disabled by using alter trigger trigger name disable  some databases use alternative syntax such as disable trigger trigger name   a trigger that has been disabled can be enabled again a trigger can instead be dropped  which removes it permanently  by using the command drop trigger trigger name returning to ourwarehouse inventory example  supposewe have the following relations  ? inventory  item  level   which notes the current amount of the item in the warehouse ? minlevel  item  level   which notes the minimum amount of the item to be maintained ? reorder  item  amount   which notes the amount of the item to be orderedwhen its level falls below the minimum ? orders  item  amount   which notes the amount of the item to be ordered note that we have been careful to place an order only when the amount falls from above the minimum level to below the minimum level if we check only that the new value after an update is below the minimum level  we may place an order erroneously when the item has already been reordered.we can then use the trigger shown in figure 5.11 for reordering the item sql-based database systems use triggers widely  although before sql  1999 they were not part of the sql standard unfortunately  each database system implemented itsownsyntax for triggers  leading to incompatibilities the sql  1999 syntax for triggers that we use here is similar  but not identical  to the syntax in the ibm db2 and oracle database systems thesumit67.blogspot.com 186 chapter 5 advanced sql 5.3.3 when not to use triggers there are many good uses for triggers  such as those we have just seen in section 5.3.2  but some uses are best handled by alternative techniques for example  we could implement the on delete cascade feature of a foreign-key constraint by using a trigger  instead of using the cascade feature not only would this be more work to implement  but also  it would be much harder for a database user to understand the set of constraints implemented in the database as another example  triggers can be used to maintain materialized views for instance  if we wished to support very fast access to the total number of students registered for each course section  we could do this by creating a relation section registration  course id  sec id  semester  year  total students  defined by the query select course id  sec id  semester  year  count  id  as total students from takes group by course id  sec id  semester  year ; the value of total students for each course must be maintained up-to-date by triggers on insert  delete  or update of the takes relation such maintenance may require insertion  update or deletion of tuples fromsection registration  and triggers must be written accordingly however  many database systems now support materialized views  which are automatically maintained by the database system  see section 4.2.3   as a result  there is no need to write trigger code for maintaining such materialized views triggers have been used for maintaining copies  or replicas  of databases a collection of triggers on insert  delete  or update can be created on each relation to record the changes in relations called change or delta relations a separate process copies over the changes to the replica of the database modern database systems  however  provide built-in facilities for database replication  making triggers unnecessary for replication in most cases replicated databases are discussed in detail in chapter 19 another problem with triggers lies in unintended execution of the triggered action when data are loaded from a backup copy,6 or when database updates at a site are replicated on a backup site in such cases  the triggered action has already been executed  and typically should not be executed again when loading data  triggers can be disabled explicitly for backup replica systems that may have to take over from the primary system  triggers would have to be disabled initially  and enabled when the backup site takes over processing fromthe primary system as an alternative  some database systems allow triggers to be specified as not 6we discuss database backup and recovery from failures in detail in chapter 16 thesumit67.blogspot.com 5.4 recursive queries 187 course id prereq id bio-301 bio-101 bio-399 bio-101 cs-190 cs-101 cs-315 cs-101 cs-319 cs-101 cs-347 cs-101 ee-181 phy-101 figure 5.12 the prereq relation for replication  which ensures that they are not executed on the backup site during database replication other database systems provide a system variable that denotes that the database is a replica on which database actions are being replayed ; the trigger body should check this variable and exit if it is true both solutions remove the need for explicit disabling and enabling of triggers triggers should be written with great care  since a trigger error detected at runtime causes the failure of the action statement that set off the trigger furthermore  the action of one trigger can set off another trigger in the worst case  this could even lead to an infinite chain of triggering for example  suppose an insert trigger on a relation has an action that causes another  new  insert on the same relation the insert action then triggers yet another insert action  and so on ad infinitum some database systems limit the length of such chains of triggers  for example  to 16 or 32  and consider longer chains of triggering an error other systems flag as an error any trigger that attempts to reference the relation whose modification caused the trigger to execute in the first place triggers can serve a very useful purpose  but they are best avoided when alternatives exist many trigger applications can be substituted by appropriate use of stored procedures  which we discussed in section 5.2 5.4 recursive queries * * consider the instance of the relation prereq shown in figure 5.12 containing information about the various courses offered at the university and the prerequisite for each course.7 suppose now that we want to find out which courses are a prerequisite whether directly or indirectly  for a specific course ? say  cs-347 that is  we wish to find a course that is a direct prerequisite for cs-347  or is a prerequisite for a course that is a prerequisite for cs-347  and so on 7this instance of prereq differs from that used earlier for reasons that will become apparent as we use it to explain recursive queries thesumit67.blogspot.com 188 chapter 5 advanced sql thus  if cs-301 is a prerequisite for cs-347  and cs-201 is a prerequisite for cs-301  and cs-101 is a prerequisite for cs-201  then cs-301  cs-201  and cs-101 are all prerequisites for cs-347 the transitive closure of the relation prereq is a relation that contains all pairs  cid  pre  such that pre is a direct or indirect prerequisite of cid there are numerous applications that require computation of similar transitive closures on hierarchies for instance  organizations typically consist of several levels of organizational units.machines consist of parts that in turn have subparts  and so on ; for example  a bicycle may have subparts such as wheels and pedals  which in turn have subparts such as tires  rims  and spokes transitive closure can be used on such hierarchies to find  for example  all parts in a bicycle 5.4.1 transitive closure using iteration one way to write the above query is to use iteration  first find those courses that are a direct prerequisite of cs-347  then those courses that are a prerequisite of all the courses under the first set  and so on this iterative process continues until we reach an iteration where no courses are added figure 5.13 shows a function findallprereqs  cid  to carry out this task ; the function takes the course id of the course as a parameter  cid   computes the set of all direct and indirect prerequisites of that course  and returns the set the procedure uses three temporary tables  ? c prereq  stores the set of tuples to be returned ? new c prereq  stores the courses found in the previous iteration ? temp  used as temporary storage while sets of courses are manipulated note that sql allows the creation of temporary tables using the command create temporary table ; such tables are available only within the transaction executing the query  and are dropped when the transaction finishes moreover  if two instances of findallprereqs run concurrently  each gets its own copy of the temporary tables ; if they shared a copy  their result could be incorrect the procedure inserts all direct prerequisites of course cid into new c prereq before the repeat loop the repeat loop first adds all courses in new c prereq to c prereq.next  it computes prerequisites of all those courses in new c prereq  except those that have already been found to be prerequisites of cid  and stores them in the temporary table temp finally  it replaces the contents of new c prereq by the contents of temp the repeat loop terminates when it finds no new  indirect  prerequisites figure 5.14 shows the prerequisites that would be found in each iteration  if the procedure were called for the course named cs-347 we note that the use of the except clause in the function ensures that the functionworks even in the  abnormal  casewhere there is a cycle of prerequisites for example  if a is aprerequisite for b  b is aprerequisite for c  andc is a prerequisite for a  there is a cycle thesumit67.blogspot.com 5.4 recursive queries 189 create function findallprereqs  cid varchar  8   ? ? finds all courses that are prerequisite  directly or indirectly  for cid returns table  course id varchar  8   ? ? the relation prereq  course id  prereq id  specifies which course is ? ? directly a prerequisite for another course begin create temporary table c prereq  course id varchar  8   ; ? ? table c prereq stores the set of courses to be returned create temporary table new c prereq  course id varchar  8   ; ? ? table new c prereq contains courses found in the previous iteration create temporary table temp  course id varchar  8   ; ? ? table temp is used to store intermediate results insert into new c prereq select prereq id from prereq where course id = cid ; repeat insert into c prereq select course id from new c prereq ; insert into temp  select prereq.course id from new c prereq  prereq where new c prereq.course id = prereq.prereq id  except  select course id from c prereq  ; delete from new c prereq ; insert into new c prereq select * from temp ; delete from temp ; until not exists  select * from new c prereq  end repeat ; return table c prereq ; end figure 5.13 finding all prerequisites of a course while cycles may be unrealistic in course prerequisites  cycles are possible in other applications for instance  suppose we have a relation flights  to  from  that says which cities can be reached from which other cities by a direct flight.we can thesumit67.blogspot.com 190 chapter 5 advanced sql iteration number tuples in c1 01  cs-301  2  cs-301    cs-201  3  cs-301    cs-201  4  cs-301    cs-201    cs-101  5  cs-301    cs-201    cs-101  figure 5.14 prerequisites of cs-347 in iterations of function findallprereqs write code similar to that in the findallprereqs function  to find all cities that are reachable by a sequence of one or more flights from a given city all we have to do is to replace prereq by flight and replace attribute names correspondingly in this situation  there can be cycles of reachability  but the function would work correctly since it would eliminate cities that have already been seen 5.4.2 recursion in sql it is rather inconvenient to specify transitive closure using iteration there is an alternative approach  using recursive view definitions  that is easier to use we can use recursion to define the set of courses that are prerequisites of a particular course  say cs-347  as follows the courses that are prerequisites  directly or indirectly  of cs-347 are  1 courses that are prerequisites for cs-347 2 courses that are prerequisites for those courses that are prerequisites  directly or indirectly  for cs-347 note that case 2 is recursive  since it defines the set of courses that are prerequisites of cs-347 in terms of the set of courses that are prerequisites of cs-347 other examples of transitive closure  such as finding all subparts  direct or indirect  of a given part can also be defined in a similar manner  recursively since the sql  1999 version  the sql standard supports a limited form of recursion  using the with recursive clause  where a view  or temporary view  is expressed in terms of itself recursive queries can be used  for example  to express transitive closure concisely recall that the with clause is used to define a temporary view whose definition is available only to the query in which it is defined the additional keyword recursive specifies that the view is recursive for example  we can find every pair  cid,pre  such that pre is directly or indirectly a prerequisite for course cid  using the recursive sql view shown in figure 5.15 any recursive view must be defined as the union of two subqueries  a base query that is nonrecursive and a recursive query that uses the recursive view in the example in figure 5.15  the base query is the select on prereq while the recursive query computes the join of prereq and rec prereq thesumit67.blogspot.com 5.4 recursive queries 191 with recursive c prereq  course id  prereq id  as  select course id  prereq id from prereq union select prereq.prereq id  c prereq.course id from prereq  c prereq where prereq.course id = c prereq.prereq id  select * from c prereq ; figure 5.15 recursive query in sql the meaning of a recursive view is best understood as follows first compute the base query and add all the resultant tuples to the recursively defined view relation rec prereq  which is initially empty   next compute the recursive query using the current contents of the view relation  and add all the resulting tuples back to the view relation keep repeating the above step until no new tuples are added to the view relation the resultant view relation instance is called a fixed point of the recursive view definition  the term ? fixed ? refers to the fact that there is no further change  the view relation is thus defined to contain exactly the tuples in the fixed-point instance applying the above logic to our example  we first find all direct prerequisites of each course by executing the base query the recursive query adds one more level of courses in each iteration  until the maximum depth of the course-prereq relationship is reached at this point no new tuples are added to the view  and a fixed point is reached to find the prerequisites of a specific course  such ascs-347,wecanmodify the outer level query by adding awhere clause ? where rec prereq.course id = ? cs-347 ? ?  one way to evaluate the query with the selection is to compute the full contents of rec prereq using the iterative technique  and then select from this result only those tuples whose course id is cs-347 however  this would result in computing  course  prerequisite  pairs for all courses  all of which are irrelevant except for those for the course cs-347 in fact the database system is not required to use the above iterative technique to compute the full result of the recursive query and then perform the selection it may get the same result using other techniques that may be more efficient  such as that used in the function findallprereqs which we saw earlier see the bibliographic notes for references to more information on this topic there are some restrictions on the recursive query in a recursive view ; specifically  the query should be monotonic  that is  its result on a view relation instance v1 should be a superset of its result on a view relation instance v2 if v1 is a superset of v2 intuitively  if more tuples are added to the view relation  the recursive query should return at least the same set of tuples as before  and possibly return additional tuples thesumit67.blogspot.com 192 chapter 5 advanced sql in particular  recursive queries should not use any of the following constructs  since they would make the query nonmonotonic  ? aggregation on the recursive view ? not exists on a subquery that uses the recursive view ? set difference  except  whose right-hand side uses the recursive view for instance  if the recursive query was of the form r  v where v is the recursive view  if we add a tuple to v the result of the query can become smaller ; the query is therefore not monotonic the meaning of recursive views can be defined by the iterative procedure as long as the recursive query is monotonic ; if the recursive query is nonmonotonic  the meaning of the view is hard to define sql therefore requires the queries to be monotonic recursive queries are discussed in more detail in the context of the datalog query language  in section b.3.6 sql also allows creation of recursively defined permanent views by using create recursive view in place of with recursive some implementations support recursive queries using a different syntax ; see the respective system manuals for further details 5.5 advanced aggregation features * * the aggregation support in sql  which we have seen earlier  is quite powerful  and handles most commontaskswith ease.however  there are some tasks that are hard to implement efficiently with the basic aggregation features in this section  we study features that were added to sql to handle some such tasks 5.5.1 ranking finding the position of a value in a larger set is a common operation for instance  we maywish to assign students a rank in class based on their grade-point average  gpa   with the rank 1 going to the student with the highest gpa  the rank 2 to the student with the next highest gpa  and so on a related type of query is to find the percentile in which a value in a  multi  set belongs  for example  the bottom third  middle third  or top third while such queries can be expressed using the sql constructs we have seen so far  they are difficult to express and inefficient to evaluate programmers may resort to writing the query partly in sql and partly in a programming language.we study sql support for direct expression of these types of queries here in our university example  the takes relation shows the grade each student earned in each course taken to illustrate ranking  let us assume we have a view student grades  id  gpa  giving the grade-point average of each student.8 8the sql statement to create the view student grades is somewhat complex since we must convert the letter grades in the takes relation to numbers and weight the grades for each course by the number of credits for that course the definition of this view is the goal of exercise 4.5 thesumit67.blogspot.com 5.5 advanced aggregation features 193 ranking is done with an order by specification the following query gives the rank of each student  select id  rank   over  order by  gpa  desc  as s rank from student grades ; note that the order of tuples in the output is not defined  so they may not be sorted by rank an extra order by clause is needed to get them in sorted order  as shown below select id  rank   over  order by  gpa  desc  as s rank from student grades order by s rank ; a basic issue with ranking is how to dealwith the case of multiple tuples that are the same on the ordering attribute  s   in our example  this means deciding what to do if there are two students with the same gpa the rank function gives the same rank to all tuples that are equal on the order by attributes for instance  if the highest gpa is shared by two students  both would get rank 1 the next rank given would be 3  not 2  so if three students get the next highest gpa  they would all get rank 3  and the next student  s  would get rank 6  and so on there is also a dense rank function that does not create gaps in the ordering in the above example  the tuples with the second highest value all get rank 2  and tuples with the third highest value get rank 3  and so on it is possible to express the above query with the basic sql aggregation functions  using the following query  select id   1 +  select count  *  from student grades b where b.gpa > a.gpa   as s rank from student grades a order by s rank ; it should be clear that the rank of a student is merely 1 plus the number of students with a higher gpa  which is exactly what the above query specifies however  this computation of each student ? s rank takes time linear in the size of the relation  leading to an overall time quadratic in the size of the relation on large relations  the above query could take a very long time to execute in contrast  the system ? s implementation of the rank clause can sort the relation and compute the rank in much less time ranking can be done within partitions of the data for instance  suppose we wish to rank students by department rather than across the entire university assume that a view is defined like student grades but including the department name  dept grades  id  dept name  gpa   the following query then gives the rank of students within each section  thesumit67.blogspot.com 194 chapter 5 advanced sql select id  dept name  rank   over  partition by dept name order by gpa desc  as dept rank from dept grades order by dept name  dept rank ; the outer order by clause orders the result tuples by department name  and within each department by the rank multiple rank expressions can be used within a single select statement ; thus we can obtain the overall rank and the rank within the department by using two rank expressions in the same select clause when ranking  possibly with partitioning  occurs along with a group by clause  the group by clause is applied first  and partitioning and ranking are done on the results of the group by thus aggregate values can then be used for ranking.we could have written our ranking over the student grades view without using the view  using a single select clause we leave details as an exercise for you the ranking functions can be used to find the top n tuples by embedding a ranking query within an outer-level query ; we leave details as an exercise note that the bottom n is simply the same as the top n with a reverse sorting order several database systems provide nonstandard sql extensions to specify directly that only the top n results are required ; such extensions do not require the rank function and simplify the job of the optimizer for example  some databases allow a clause limit n to be added at the end of an sql query to specify that only the first n tuples should be output ; this clause is used in conjunction with an order by clause to fetch the top n tuples  as illustrated by the following query  which retrieves the ids and gpas of the top 10 students in order of gpa  select id  gpa  from student grades order by gpa limit 10 ; however  the limit clause does not support partitioning  so we can not get the top n within each partition without performing ranking ; further  if more than one student gets the same gpa  it is possible that one is included in the top 10  while another is excluded several other functions can be used in place of rank for instance  percent rank of a tuple gives the rank of the tuple as a fraction if there are n tuples in the partition9 and the rank of the tuple is r  then its percent rank is defined as  r -1  /  n-1   and as null if there is only one tuple in the partition   the function cume dist  short for cumulative distribution  for a tuple is defined as p/n where p is the number of tuples in the partitionwith ordering values preceding or equal to the ordering value of the tuple and n is the number of tuples in the partition the function row number sorts the rows and gives each row a unique number corre 9the entire set is treated as a single partition if no explicit partition is used thesumit67.blogspot.com 5.5 advanced aggregation features 195 sponding to its position in the sort order ; different rows with the same ordering value would get different row numbers  in a nondeterministic fashion finally  for a given constant n  the ranking function ntile  n  takes the tuples in each partition in the specified order and divides them into n buckets with equal numbers of tuples.10 for each tuple  ntile  n  then gives the number of the bucket in which it is placed  with bucket numbers starting with 1 this function is particularly useful for constructing histograms based on percentiles we can show the quartile into which each student falls based on gpa by the following query  select id  ntile  4  over  order by  gpa desc   as quartile from student grades ; the presence of null values can complicate the definition of rank  since it is not clear where they should occur first in the sort order sql permits the user to specify where they should occur by using nulls first or nulls last  for instance  select id  rank   over  order by gpa desc nulls last  as s rank from student grades ; 5.5.2 windowing window queries compute an aggregate function over ranges of tuples this is useful  for example  to compute an aggregate of a fixed range of time ; the time range is called a window windows may overlap  in which case a tuple may contribute to more than one window this is unlike the partitions we saw earlier  where a tuple could contribute to only one partition an example of the use of windowing is trend analysis consider our earlier sales example sales may fluctuate widely from day to day based on factors like weather  for example a snowstorm  flood  hurricane  or earthquake might reduce sales for a period of time   however  over a sufficiently long period of time  fluctuations might be less  continuing the example  sales may ? make up ? for weather-related downturns   stock market trend analysis is another example of the use of the windowing concept various ? moving averages ? are found on business and investmentweb sites it is relatively easy towrite an sql query using those featureswe have already studied to compute an aggregate over onewindow  for example  sales over a fixed 3-day period however  if we want to do this for every 3-day period  the query becomes cumbersome sql provides a windowing feature to support such queries suppose we are given a view tot credits  year  num credits  giving the total number of credits taken 10if the total number of tuples in a partition is not divisible by n  then the number of tuples in each bucket can differ by at most 1 tuples with the same value for the ordering attribute may be assigned to different buckets  nondeterministically  in order to make the number of tuples in each bucket equal thesumit67.blogspot.com 196 chapter 5 advanced sql by students in each year.11 note that this relation can contain at most one tuple for each year consider the following query  select year  avg  num credits  over  order by year rows 3 preceding  as avg total credits from tot credits ; this query computes averages over the 3 preceding tuples in the specified sort order thus  for 2009  if tuples for years 2008 and 2007 are present in the relation tot credits  with each year represented by only one tuple  the result of the window definition is the average of the values for years 2007  2008  and 2009 the averages each year would be computed in a similar manner for the earliest year in the relation tot credits  the average would be over only that year itself  while for the next year  the average would be over two years note that if the relation tot credits hasmore than one tuple for a specific year  there may bemultiple possible orderings of tuples  that are sorted by year in this case  the definition of preceding tuples is based on the implementation dependent sort order  and is not uniquely defined suppose that instead of going back a fixed number of tuples  we want the window to consist of all prior years that means the number of prior years considered is not fixed to get the average total credits over all prior years we write  select year  avg  num credits  over  order by year rows unbounded preceding  as avg total credits from tot credits ; it is possible to use the keyword following in place of preceding if we did this in our example the year value specifies the beginning of the window instead of the end similarly  we can specify a window beginning before the current tuple and ending after it  select year  avg  num credits  over  order by year rows between 3 preceding and 2 following  as avg total credits from tot credits ; instead of a specific count of tuples,we can specify a range based on the value of the order by attribute to specify a range going back 4 years and including the current year  we write  11we leave the definition of this view in terms of our university example as an exercise thesumit67.blogspot.com 5.6 olap 197 select year  avg  num credits  over  order by year range between year  4 and year  as avg total credits from tot credits ; be sure to note the use of the keyword range in the above example for the year 2010  data for years 2006 to 2010 inclusive would be included regardless of how many tuples actually exist for that range in our example  all tuples pertain to the entire university suppose instead  we have credit data for each department in a view tot credits dept  dept name  year  num credits  giving the total number of credits students took with the particular department in the specified year  again  we leave writing this view definition as an exercise  we can write windowing queries that treat each department separately by partitioning by dept name  select dept name  year  avg  num credits  over  partition by dept name order by year rows between 3 preceding and current row  as avg total credits from tot credits dept ; 5.6 olap * * an online analytical processing  olap  system is an interactive system that permits an analyst to view different summaries of multidimensional data the word online indicates that an analyst must be able to request new summaries and get responses online  within a few seconds  and should not be forced to wait for a long time to see the result of a query there are many olap products available  including some that ship with database products such as microsoft sql server  and oracle  and other standalone tools the initial versions of many olap tools assumed that data is memory resident data analysis on small amounts of data can in fact be performed using spreadsheet applications  such as excel however  olap on very large amounts of data requires that data be resident in a database  and requires support from the database for efficient preprocessing of data as well as for online query processing in this section  we study extensions of sql to support such tasks 5.6.1 online analytical processing consider an application where a shop wants to find out what kinds of clothes are popular let us suppose that clothes are characterized by their item name  color  and size  and that we have a relation sales with the schema sales  item name  color  clothes size  quantity  thesumit67.blogspot.com 198 chapter 5 advanced sql item name color clothes size quantity skirt dark small 2 skirt dark medium 5 skirt dark large 1 skirt pastel small 11 skirt pastel medium 9 skirt pastel large 15 skirt white small 2 skirt white medium 5 skirt white large 3 dress dark small 2 dress dark medium 6 dress dark large 12 dress pastel small 4 dress pastel medium 3 dress pastel large 3 dress white small 2 dress white medium 3 dress white large 0 shirt dark small 2 shirt dark medium 6 shirt dark large 6 shirt pastel small 4 shirt pastel medium 1 shirt pastel large 2 shirt white small 17 shirt white medium 1 shirt white large 10 pants dark small 14 pants dark medium 6 pants dark large 0 pants pastel small 1 pants pastel medium 0 pants pastel large 1 pants white small 3 pants white medium 0 pants white large 2 figure 5.16 an example of sales relation suppose that item name can take on the values  skirt  dress  shirt  pants   color can take on the values  dark  pastel  white   clothes size can take on values  small  medium  large   and quantity is an integer value representing the total number of items of a given  item name  color  clothes size   an instance of the sales relation is shown in figure 5.16 thesumit67.blogspot.com 5.6 olap 199 statistical analysis often requires grouping on multiple attributes given a relation used for data analysis  we can identify some of its attributes as measure attributes  since they measure some value  and can be aggregated upon for instance  the attribute quantity of the sales relation is a measure attribute  since it measures the number of units sold some  or all  of the other attributes of the relation are identified as dimension attributes  since they define the dimensions on which measure attributes  and summaries of measure attributes  are viewed in the sales relation  item name  color  and clothes size are dimension attributes  a more realistic version of the sales relation would have additional dimensions  such as time and sales location  and additional measures such as monetary value of the sale  data that can be modeled as dimension attributes and measure attributes are called multidimensional data to analyze the multidimensional data  a manager may want to see data laid out as shown in the table in figure 5.17 the table shows total quantities for different combinations of item name and color the value of clothes size is specified to be all  indicating that the displayed values are a summary across all values of clothes size  that is  we want to group the ? small ?  ? medium ?  and ? large ? items into one single group the table in figure 5.17 is an example of a cross-tabulation  or cross-tab  for short   also referred to as a pivot-table in general  a cross-tab is a table derived from a relation  say r   where values for one attribute of relation r  say a  form the row headers and values for another attribute of relation r  say b  form the column header for example  in figure 5.17  the attribute item name corresponds to a  with values ? dark ?  ? pastel ?  and ? white ?   and the attribute color corresponds to to b  with attributes ? skirt ?  ? dress ?  ? shirt ?  and ? pants ?   each cell in the pivot-table can be identified by  ai  b j   where ai is a value for a and b j a value for b the values of the various cells in the pivot-table are derived from the relation r as follows  if there is at most one tuple in r with any  ai  b j  value  the value in the cell is derived from that single tuple  if any  ; for instance  it could be the value of one or more other attributes of the tuple if there can be multiple tuples with an  ai  b j  value  the value in the cell must be derived skirt dress shirt pants color item_name clothes_size all dark pastel white total total 8 35 10 53 20 10 5 35 14 7 28 49 20 2 5 27 62 54 48 164 figure 5.17 cross tabulation of sales by item name and color thesumit67.blogspot.com 200 chapter 5 advanced sql 8 20 14 20 62 35 10 7 2 54 10 8 28 5 48 53 35 49 27 164 34 21 77 4 9 42 16 18 45 all large medium small skirt dress shirt pants all clothes_size 2 5 3 1 11 4 7 6 12 29 2 8 5 7 22 dark pastel white all item_name color figure 5.18 three-dimensional data cube by aggregation on the tuples with that value in our example  the aggregation used is the sum of the values for attribute quantity  across all values for clothes size  as indicated by ? clothes size  all ? above the cross-tab in figure 5.17 thus  the value for cell  skirt  pastel  is 35  since there are 3 tuples in the sales table thatmeet that criteria  with values 11  9  and 15 in our example  the cross-tab also has an extra column and an extra rowstoring the totals of the cells in the row/column most cross-tabs have such summary rows and columns the generalization of a cross-tab  which is two-dimensional  to n dimensions can be visualized as an n-dimensional cube  called the data cube figure 5.18 shows a data cube on the sales relation the data cube has three dimensions  item name  color  and clothes size  and the measure attribute is quantity each cell is identified by values for these three dimensions each cell in the data cube contains a value  just as in a cross-tab in figure 5.18  the value contained in a cell is shown on one of the faces of the cell ; other faces of the cell are shown blank if they are visible all cells contain values  even if they are not visible the value for a dimension may be all  in which case the cell contains a summary over all values of that dimension  as in the case of cross-tabs the number of different ways in which the tuples can be grouped for aggregation can be large in the example of figure 5.18  there are 3 colors  4 items  and 3 sizes resulting in a cube size of 3 ? 4 ? 3 = 36 including the summary values  we obtain a 4 ? 5 ? 4 cube  whose size is 80 in fact  for a table with n dimensions  aggregation can be performed with grouping on each of the 2n subsets of the n dimensions.12 12grouping on the set of all n dimensions is useful only if the table may have duplicates thesumit67.blogspot.com 5.6 olap 201 with an olap system  a data analyst can look at different cross-tabs on the same data by interactively selecting the attributes in the cross-tab each cross-tab is a two-dimensional view on a multidimensional data cube for instance  the analyst may select a cross-tab on item name and clothes size or a cross-tab on color and clothes size the operation of changing the dimensions used in a cross-tab is called pivoting olap systems allow an analyst to see a cross-tab on item name and color for a fixed value of clothes size  for example  large  instead of the sum across all sizes such an operation is referred to as slicing  since it can be thought of as viewing a slice of the data cube the operation is sometimes called dicing  particularly when values for multiple dimensions are fixed when a cross-tab is used to view a multidimensional cube  the values of dimension attributes that are not part of the cross-tab are shown above the crosstab the value of such an attribute can be all  as shown in figure 5.17  indicating that data in the cross-tab are a summary over all values for the attribute slicing/ dicing simply consists of selecting specific values for these attributes  which are then displayed on top of the cross-tab olap systems permit users to view data at any desired level of granularity the operation of moving from finer-granularity data to a coarser granularity  by means of aggregation  is called a rollup in our example  starting from the data cube on the sales table  we got our example cross-tab by rolling up on the attribute clothes size the opposite operation ? that of moving fromcoarser-granularity data to finer-granularity data ? is called a drill down clearly  finer-granularity data can not be generated from coarse-granularity data ; they must be generated either from the original data  or from even finer-granularity summary data analysts may wish to view a dimension at different levels of detail for instance  an attribute of type datetime contains a date and a time of day using time precise to a second  or less  may not be meaningful  an analyst who is interested in rough time of day may look at only the hour value an analyst who is interested in sales by day of the week may map the date to a day of the week and look only at that another analyst may be interested in aggregates over a month  or a quarter  or for an entire year the different levels of detail for an attribute can be organized into a hierarchy figure 5.19a shows a hierarchy on the datetime attribute as another example  figure 5.19b shows a hierarchy on location  with the city being at the bottom of the hierarchy  state above it  country at the next level  and region being the top level in our earlier example  clothes can be grouped by category  for instance  menswear or womenswear  ; category would then lie above item name in our hierarchy on clothes at the level of actual values  skirts and dresses would fall under the womenswear category and pants and shirts under the menswear category an analyst may be interested in viewing sales of clothes divided asmenswear and womenswear  and not interested in individual values after viewing the aggregates at the level of womenswear and menswear  an analyst may drill down the hierarchy to look at individual values an analyst looking at the detailed level may drill up the hierarchy and look at coarser-level aggregates both levels can be displayed on the same cross-tab  as in figure 5.20 thesumit67.blogspot.com 202 chapter 5 advanced sql hour of day date datetime day of week month quarter year state country region city  a  time hierarchy  b  location hierarchy figure 5.19 hierarchies on dimensions 5.6.2 cross-tab and relational tables a cross-tab is different from relational tables usually stored in databases  since the number of columns in the cross-tab depends on the actual data a change in the data values may result in adding more columns  which is not desirable for data storage however  a cross-tab view is desirable for display to users it is straightforward to represent a cross-tab without summary values in a relational form with a fixed number of columns a cross-tab with summary rows/columns can be represented by introducing a special value all to represent subtotals  as in figure 5.21 the sql standard actually uses the null value in place of all  but to avoid confusion with regular null values  we shall continue to use all womenswear category item_name color clothes_size  all dark pastel white total total skirt 8 8 10 53 dress 20 20 5 35 subtotal 28 28 15 88 menswear pants 14 14 28 49 shirt 20 20 5 27 subtotal 34 34 33 76 62 62 48 164 figure 5.20 cross tabulation of sales with hierarchy on item name thesumit67.blogspot.com 5.6 olap 203 item name color clothes size quantity skirt dark all 8 skirt pastel all 35 skirt white all 10 skirt all all 53 dress dark all 20 dress pastel all 10 dress white all 5 dress all all 35 shirt dark all 14 shirt pastel all 7 shirt white all 28 shirt all all 49 pants dark all 20 pants pastel all 2 pants white all 5 pants all all 27 all dark all 62 all pastel all 54 all white all 48 all all all 164 figure 5.21 relational representation of the data in figure 5.17 consider the tuples  skirt  all  all  53  and  dress  all  all  35  .we have obtained these tuples by eliminating individual tuples with different values for color and clothes size  and by replacing the value of quantity by an aggregate ? namely  the sum of the quantities the value all can be thought of as representing the set of all values for an attribute tuples with the value all for the color and clothes size dimensions can be obtained by an aggregation on the sales relation with a group by on the column item name similarly  a group by on color  clothes size can be used to get the tuples with the value all for item name  and a group by with no attributes  which can simply be omitted in sql  can be used to get the tuple with value all for item name  color  and clothes size hierarchies can also be represented by relations for example  the fact that skirts and dresses fall under the womenswear category  and the pants and shirts under the menswear category can be represented by a relation itemcategory  item name  category   this relation can be joined with the sales relation  to get a relation that includes the category for each item.aggregation on this joined relation allows us to get a cross-tab with hierarchy as another example  a hierarchy on city can be represented by a single relation city hierarchy  id  city  state  country  region   or by multiple relations  each mapping values in one level of the hierarchy to values at the next level.we assume here that cities have unique identifiers  stored in the attribute id  to avoid confusing between two cities with the same name  e.g  the springfield in missouri and the springfield in illinois thesumit67.blogspot.com 204 chapter 5 advanced sql olap implementation the earliest olap systems used multidimensional arrays in memory to store data cubes  and are referred to as multidimensional olap  molap  systems later  olap facilitieswere integrated into relational systems,with data stored in a relational database such systems are referred to as relational olap  rolap  systems hybrid systems  which store some summaries in memory and store the base data and other summaries in a relational database  are called hybrid olap  holap  systems many olap systems are implemented as client-server systems the server contains the relational database aswell as anymolap data cubes client systems obtain views of the data by communicating with the server ana ? iveway of computing the entire data cube  all groupings  on a relation is to use any standard algorithm for computing aggregate operations  one grouping at a time the na ? ive algorithm would require a large number of scans of the relation a simple optimization is to compute an aggregation on  say   item name  color  from an aggregation  item name  color  clothes size   instead of from the original relation for the standard sql aggregate functions,we can compute an aggregatewith grouping on a set of attributes afrom an aggregate with grouping on a set of attributes b if a ? b ; you can do so as an exercise  see exercise 5.24   but note that to compute avg  we additionally need the count value  for some nonstandard aggregate functions  such as median  aggregates can not be computed as above ; the optimization described here does not apply to such non-decomposable aggregate functions  the amount of data read drops significantly by computing an aggregate from another aggregate  instead of from the original relation further improvements are possible ; for instance  multiple groupings can be computed on a single scan of the data early olap implementations precomputed and stored entire data cubes  that is  groupings on all subsets of the dimension attributes precomputation allows olap queries to be answered within a few seconds  even on datasets that may contain millions of tuples adding up to gigabytes of data however  there are 2n groupings with n dimension attributes ; hierarchies on attributes increase the number further as a result  the entire data cube is often larger than the original relation that formed the data cube and in many cases it is not feasible to store the entire data cube instead of precomputing and storing all possible groupings  itmakes sense to precompute and store some of the groupings  and to compute others on demand instead of computing queries from the original relation  which may take a very long time,we can compute them fromother precomputed queries for instance  suppose that a query requires grouping by  item name  color   and this has not been precomputed the query result can be computed from summaries by  item name  color  clothes size   if that has been precomputed see the bibliographical notes for references on howto select a good set of groupings for precomputation  given limits on the storage available for precomputed results thesumit67.blogspot.com 5.6 olap 205 5.6.3 olap in sql several sql implementations  such as microsoft sql server  and oracle  support a pivot clause in sql  which allows creation of cross-tabs given the sales relation from figure 5.16  the query  select * from sales pivot  sum  quantity  for color in  ? dark ?  ? pastel ?  ? white ?   order by item name ; returns the cross-tab shown in figure 5.22 note that the for clause within the pivot clause specifies what values from the attribute color should appears as attribute names in the pivot result the attribute color itself is eliminated from the result  although all other attributes are retained  except that the values for the newly created attributes are specified to come from the attribute quantity in case more than one tuple contributes values to a given cell  the aggregate operation within the pivot clause specifies howthe values should be combined in the above example  the quantity values are summed up note that the pivot clause by itself does not compute the subtotals we saw in the pivot table from figure 5.17 however  we can first generate the relational representation shown in figure 5.21  as outlined shortly  and then apply the pivot clause on that representation to get an equivalent result in this case  the value all must also be listed in the for clause  and the order by clause needs to bemodified to order all at the end item name clothes size dark pastel white skirt small 2 11 2 skirt medium 5 9 5 skirt large 1 15 3 dress small 2 4 2 dress medium 6 3 3 dress large 12 3 0 shirt small 2 4 17 shirt medium 6 1 1 shirt large 6 2 10 pants small 14 1 3 pants medium 6 0 0 pants large 0 1 2 figure 5.22 result of sql pivot operation on the sales relation of figure 5.16 thesumit67.blogspot.com 206 chapter 5 advanced sql item name quantity skirt 53 dress 35 shirt 49 pants 27 figure 5.23 query result the data in a data cube can not be generated by a single sql query  using the basic group by constructs  since aggregates are computed for several different groupings of the dimension attributes for this reason  sql includes functions to form the grouping needed for olap.we discuss these below sql supports generalizations of the group by construct to perform the cube and rollup operations the cube and rollup constructs in the group by clause allow multiple group by queries to be run in a single query with the result returned as a single relation in a style similar to that of the relation of figure 5.21 consider again our retail shop example and the relation  sales  item name  color  clothes size  quantity  we can find the number of items sold in each item name by writing a simple group by query  select item name  sum  quantity  from sales group by item name ; the result of this query is shown in figure 5.23 note that this represents the same data as the last column of figure 5.17  or equivalently  the first row in the cube of figure 5.18   similarly  we can find the number of items sold in each color  etc by using multiple attributes in the group by clause  we can find how many items were sold with a certain set of properties for example  we can find a breakdown of sales by item-name and color by writing  select item name  color  sum  quantity  from sales group by item name  color ; the result of this query is shown in figure 5.24 note that this represents the same data as is shown in the the first 4 rows and first 4 columns of figure 5.17  or equivalently  the first 4 rows and columns in the cube of figure 5.18   if  however  we want to generate the entire data cube using this approach  we would have to write a separate query for each of the following sets of attributes  thesumit67.blogspot.com 5.6 olap 207 item name color quantity skirt dark 8 skirt pastel 35 skirt white 10 dress dark 20 dress pastel 10 dress white 5 shirt dark 14 shirt pastel 7 shirt white 28 pants dark 20 pants pastel 2 pants white 5 figure 5.24 query result   item name  color  clothes size    item name  color    item name  clothes size    color  clothes size    item name    color    clothes size      where   denotes an empty group by list the cube construct allows us to accomplish this in one query  select item name  color  clothes size  sum  quantity  from sales group by cube  item name  color  clothes size  ; the above query produces a relation whose schema is   item name  color  clothes size  sum  quantity   so that the result of this query is indeed a relation  tuples in the result contain null as the value of those attributes not present in a particular grouping for example  tuples produced by grouping on clothes size have a schema  clothes size  sum  quantity    they are converted to tuples on  item name  color  clothes size  sum  quantity   by inserting null for item name and color data cube relations are often very large the cube query above,with 3 possible colors  4 possible item names  and 3 sizes  has 80 tuples the relation of figure 5.21 is generated using grouping by item name and color it also uses all in place of null so as to be more readable to the average user to generate that relation in sql  we arrange to substitute all for null the query  select item name  color  sum  quantity  from sales group by cube  item name  color  ; thesumit67.blogspot.com 208 chapter 5 advanced sql the decode function the decode function allows substitution of values in an attribute of a tuple the general form of decode is  decode  value  match-1  replacement-1  match-2  replacement-2      match-n  replacement-n  default-replacement  ; it compares value against the match values and if a match is found  it replaces the attribute value with the corresponding replacement value if nomatch succeeds  then the attribute value is replaced with the default replacement value the decode function does not work as we might like for null values because  as we saw in section 3.6  predicates on nulls evaluate to unknown  which ultimately becomes false to deal with this  we apply the grouping function  which returns 1 if its argument is a null value generated by a cube or rollup and 0 otherwise then the relation in figure 5.21  with occurrences of all replaced by null  can be computed by the query  select decode  grouping  item name   1  ? all ?  item name  as item name decode  grouping  color   1  ? all ?  color  as color sum  quantity  as quantity from sales group by cube  item name  color  ; generates the relation of figure 5.21 with nulls the substitution of all is achieved using the sql decode and grouping functions the decode function is conceptually simple but its syntax is somewhat hard to read see blue box for details the rollup construct is the same as the cube construct except that rollup generates fewer group by queries we saw that group by cube  item name  color  clothes size  generated all 8 ways of forming a group by query using some  or all or none  of the attributes in  select item name  color  clothes size  sum  quantity  from sales group by rollup  item name  color  clothes size  ; group by rollup  item name  color  clothes size  generates only 4 groupings    item name  color  clothes size    item name  color    item name      notice that the order of the attributes in the rollup makes a difference ; the final attribute  clothes size  in our example  appears in only one grouping  the penultimate  second last  attribute in 2 groupings  and so on  with the first attribute appearing in all groups but one  the empty grouping   thesumit67.blogspot.com 5.7 summary 209 why might we want the specific groupings that are used in rollup ? these groups are of frequent practical interest for hierarchies  as in figure 5.19  for example   for the location hierarchy  region  country  state  city   we may want to group by region to get sales by region thenwe may want to ? drill down ? to the level of countries within each region  which means we would group by region  country drilling down further  we may wish to group by region  country  state and then by region  country  state  city the rollup construct allows us to specify this sequence of drilling down for further detail multiple rollups and cubes can be used in a single group by clause for instance  the following query  select item name  color  clothes size  sum  quantity  from sales group by rollup  item name   rollup  color  clothes size  ; generates the groupings    item name  color  clothes size    item name  color    item name    color  clothes size    color      to understand why  observe that rollup  item name  generates two groupings    item name       and rollup  color  clothes size  generates three groupings    color  clothes size    color       the cartesian product of the two gives us the six groupings shown neither the rollup nor the cube clause gives complete control on the groupings that are generated for instance  we can not use them to specify that we want only groupings   color  clothes size    clothes size  item name    such restricted groupings can be generated by using the grouping construct in the having clause ; we leave the details as an exercise for you 5.7 summary ? sql queries can be invoked fromhost languages  via embedded and dynamic sql the odbc and jdbc standards define application program interfaces to access sql databases from c and java language programs increasingly  programmers use these apis to access databases ? functions and procedures can be defined using sqlprocedural extensions that allow iteration and conditional  if-then-else  statements ? triggers define actions to be executed automatically when certain events occur and corresponding conditions are satisfied triggers have many uses  such as implementing business rules  audit logging  and even carrying out actions outside the database system.although triggers were not added to the thesumit67.blogspot.com 210 chapter 5 advanced sql sql standard until sql  1999  most database systems have long implemented triggers ? some queries  such as transitive closure  can be expressed either by using iteration or by using recursive sql queries recursion can be expressed using either recursive views or recursive with clause definitions ? sql supports several advanced aggregation features  including ranking and windowing queries that simplify the expression of some aggregates and allow more efficient evaluation ? online analytical processing  olap  tools help analysts view data summarized in different ways  so that they can gain insight into the functioning of an organization ? olap tools work on multidimensional data  characterized by dimension attributes and measure attributes ? the data cube consists of multidimensional data summarized in different ways precomputing the data cube helps speed up queries on summaries of data ? cross-tab displays permit users to view two dimensions of multidimensional data at a time  along with summaries of the data ? drill down  rollup  slicing  and dicing are among the operations that users perform with olap tools ? sql  starting with the sql  1999 standard  provides a variety of operators for data analysis  including cube and rollup operations some systems support a pivot clause  which allows easy creation of cross-tabs review terms ? jdbc ? odbc ? prepared statements ? accessing metadata ? sql injection ? embedded sql ? cursors ? updatable cursors ? dynamic sql ? sql functions ? stored procedures ? procedural constructs ? external language routines ? trigger ? before and after triggers ? transition variables and tables ? recursive queries ? monotonic queries ? ranking functions ? rank ? dense rank ? partition by ? windowing thesumit67.blogspot.com practice exercises 211 ? online analytical processing  olap  ? multidimensional data ? measure attributes ? dimension attributes ? pivoting ? data cube ? slicing and dicing ? rollup and drill down ? cross-tabulation practice exercises 5.1 describe the circumstances in which you would choose to use embedded sql rather than sql alone or only a general-purpose programming language 5.2 write a java function using jdbc metadata features that takes a resultset as an input parameter  and prints out the result in tabular form  with appropriate names as column headings 5.3 write a java function using jdbc metadata features that prints a list of all relations in the database  displaying for each relation the names and types of its attributes 5.4 show how to enforce the constraint ? an instructor can not teach in two different classrooms in a semester in the same time slot ? using a trigger  remember that the constraint can be violated by changes to the teaches relation as well as to the section relation   5.5 write triggers to enforce the referential integrity constraint from section to time slot  on updates to section  and time slot note that the ones we wrote in figure 5.8 do not cover the update operation 5.6 to maintain the tot cred attribute of the student relation  carry out the following  a modify the trigger on updates of takes  to handle all updates that can affect the value of tot cred b write a trigger to handle inserts to the takes relation c under what assumptions is it reasonable not to create triggers on the course relation ? 5.7 consider the bank database of figure 5.25 let us define a view branch cust as follows  create view branch cust as select branch name  customer name from depositor  account where depositor.account number = account.account number thesumit67.blogspot.com 212 chapter 5 advanced sql branch  branch name  branch city  assets  customer  customer name  customer street  cust omer city  loan  loan number  branch name  amount  borrower  customer name  loan number  account  account number  branch name  balance  depositor  customer name  account number  figure 5.25 banking database for exercises 5.7  5.8  and 5.28  suppose that the view is materialized ; that is  the view is computed and stored write triggers to maintain the view  that is  to keep it up-to-date on insertions to and deletions from depositor or account do not bother about updates 5.8 consider the bank database of figure 5.25 write an sql trigger to carry out the following action  on delete of an account  for each owner of the account  check if the owner has any remaining accounts  and if she does not  delete her from the depositor relation 5.9 show how to express group by cube  a  b  c  d  using rollup ; your answer should have only one group by clause 5.10 given a relation s  student  subject  marks   write a query to find the top n students by total marks  by using ranking 5.11 consider the sales relation from section 5.6.write an sql query to compute the cube operation on the relation  giving the relation in figure 5.21 do not use the cube construct exercises 5.12 consider the following relations for a company database  ? emp  ename  dname  salary  ? mgr  ename  mname  and the java code in figure 5.26  which uses the jdbc api assume that the userid  password  machine name  etc are all okay describe in concise english what the java program does  that is  produce an english sentence like ? it finds the manager of the toy department  ? not a line-by-line description of what each java statement does  5.13 suppose you were asked to define a class metadisplay in java  containing a method static void printtable  string r  ; the method takes a relation name r as input  executes the query ? select * from r ?  and prints the result out in nice tabular format  with the attribute names displayed in the header of the table thesumit67.blogspot.com exercises 213 import java.sql * ; public class mystery  public static void main  string   args   try  connection con = null ; class.forname  " oracle.jdbc.driver.oracledriver "  ; con = drivermanager.getconnection  " jdbc  oracle  thin  star/x @ //edgar.cse.lehigh.edu  1521/xe "  ; statement s = con.createstatement   ; string q ; string empname = " dog " ; boolean more ; resultset result ; do  q = " select mname from mgr where ename = ? " + empname + " ? " ; result = s.executequery  q  ; more = result.next   ; if  more   empname = result.getstring  " mname "  ; system.out.println  empname  ;   while  more  ; s.close   ; con.close   ;  catch  exception e   e.printstacktrace   ;    figure 5.26 java code for exercise 5.12 a what do you need to know about relation r to be able to print the result in the specified tabular format b what jdbc methods  s  can get you the required information ? c write the method printtable  string r  using the jdbc api 5.14 repeat exercise 5.13 using odbc  defining void printtable  char * r  as a function instead of a method 5.15 consider an employee database with two relations employee  employee name  street  city  works  employee name  company name  salary  where the primary keys are underlined write a query to find companies whose employees earn a higher salary  on average  than the average salary at ? first bank corporation ?  thesumit67.blogspot.com 214 chapter 5 advanced sql a using sql functions as appropriate b without using sql functions 5.16 rewrite the query in section 5.2.1 that returns the name and budget of all departments with more than 12 instructors  using the with clause instead of using a function call 5.17 compare the use of embedded sql with the use in sql of functions defined in a general-purpose programming language under what circumstances would you use each of these features ? 5.18 modify the recursive query in figure 5.15 to define a relation prereq depth  course id  prereq id  depth  where the attribute depth indicates how many levels of intermediate prerequisites are there between the course and the prerequisite direct prerequisites have a depth of 0 5.19 consider the relational schema part  part id  name  cost  subpart  part id  subpart id  count  a tuple  p1  p2  3  in the subpart relation denotes that the part with part-id p2 is a direct subpart of the part with part-id p1  and p1 has 3 copies of p2 note that p2 may itself have further subparts write a recursive sql query that outputs the names of all subparts of the part with part-id ? p-100 ?  5.20 consider again the relational schema from exercise 5.19 write a jdbc function using non-recursive sql to find the total cost of part ? p-100 ?  including the costs of all its subparts be sure to take into account the fact that a part may have multiple occurrences of a subpart you may use recursion in java if you wish 5.21 suppose there are two relations r and s  such that the foreign key b of r references the primary key aof s describe how the trigger mechanism can be used to implement the on delete cascade option,when a tuple is deleted from s 5.22 the execution of a trigger can cause another action to be triggered most database systems place a limit on how deep the nesting can be explain why they might place such a limit 5.23 consider the relation  r  shown in figure 5.27 give the result of the following query  thesumit67.blogspot.com exercises 215 building room number time slot id course id sec id garfield 359 a bio-101 1 garfield 359 b bio-101 2 saucon 651 a cs-101 2 saucon 550 c cs-319 1 painter 705 d mu-199 1 painter 403 d fin-201 1 figure 5.27 the relation r for exercise 5.23 select building  room number  time slot id  count  *  from r group by rollup  building  room number  time slot id  5.24 for each of the sql aggregate functions sum  count  min  and max  show how to compute the aggregate value on a multiset s1 ? s2  given the aggregate values on multisets s1 and s2 on the basis of the above  give expressions to compute aggregate values with grouping on a subset s of the attributes of a relation r  a  b,c  d  e   given aggregate values for grouping on attributes t ? s  for the following aggregate functions  a sum  count  min  and max b avg c standard deviation 5.25 in section 5.5.1  we used the student grades view of exercise 4.5 to write a query to find the rank of each student based on grade-point average modify that query to show only the top 10 students  that is  those students whose rank is 1 through 10   5.26 give an example of a pair of groupings that can not be expressed by using a single group by clause with cube and rollup 5.27 given relation s  a  b  c   show how to use the extended sql features to generate a histogram of c versus a  dividing a into 20 equal-sized partitions  that is  where each partition contains 5 percent of the tuples in s  sorted by a   5.28 consider the bank database of figure 5.25 and the balance attribute of the account relation write an sql query to compute a histogram of balance values  dividing the range 0 to the maximum account balance present  into three equal ranges thesumit67.blogspot.com 216 chapter 5 advanced sql tools most database vendors provide olap tools as part of their database systems  or as add-on applications these include olap tools from microsoft corp  oracle express  and informix metacube tools may be integrated with a larger ? business intelligence ? product such as ibm cognos many companies also provide analysis tools for specific applications  such as customer relationship management  for example  oracle siebel crm   bibliographical notes see the bibliographic notes of chapter 3 for references to sql standards and books on sql an excellent source for more  and up-to-date  information on jdbc  and on java in general  is java.sun.com/docs/books/tutorial references to books on java  including jdbc  are also available at this url the odbc api is described in microsoft  1997  and sanders  1998   melton and eisenberg  2000  provides a guide to sqlj  jdbc  and related technologies more information on odbc  ado  and ado.net can be found on msdn.microsoft.com/data in the context of functions and procedures in sql  many database products support features beyond those specified in the standards  and do not support many of the features of the standard more information on these features may be found in the sql user manuals of the respective products the original sql proposals for assertions and triggers are discussed in astrahan et al  1976  ,chamberlin et al  1976   and chamberlin et al  1981   melton and simon  2001   melton  2002   and eisenberg and melton  1999  provide textbook coverage of sql  1999  the version of the sql standard that first included triggers recursive query processing was first studied in detail in the context of a query language called datalog  which was based on mathematical logic and followed the syntax of the logic programming language prolog ramakrishnan and ullman  1995  provides a survey of results in this area  including techniques to optimize queries that select a subset of tuples from a recursively defined view gray et al  1995  and gray et al  1997  describe the data-cube operator efficient algorithms for computing data cubes are described by agarwal et al  1996   harinarayan et al  1996   and ross and srivastava  1997   descriptions of extended aggregation support in sql  1999 can be found in the product manuals of database systems such as oracle and ibm db2 there has been a substantial amount of research on the efficient processing of ? top-k ? queries that return only the top-k-ranked results a survey of that work appears in ilyas et al  2008   thesumit67.blogspot.com chapter6 formal relational query languages in chapters 2 through 5 we introduced the relational model and covered sql in great detail in this chapter we present the formal model upon which sql as well as other relational query languages are based we cover three formal languages.we start by presenting the relational algebra  which forms the basis of the widely used sql query language.we then cover the tuple relational calculus and the domain relational calculus  which are declarative query languages based on mathematical logic 6.1 the relational algebra the relational algebra is a procedural query language it consists of a set of operations that take one or two relations as input and produce a new relation as their result the fundamental operations in the relational algebra are select  project  union  set difference  cartesian product  and rename in addition to the fundamental operations  there are several other operations ? namely  set intersection  natural join  and assignment.we shall define these operations in terms of the fundamental operations 6.1.1 fundamental operations the select  project  and rename operations are called unary operations  because they operate on one relation the other three operations operate on pairs of relations and are  therefore  called binary operations 6.1.1.1 the select operation the select operation selects tuples that satisfy a given predicate we use the lowercase greek letter sigma    to denote selection the predicate appears as a subscript to   the argument relation is in parentheses after the   thus  to select 217 thesumit67.blogspot.com 218 chapter 6 formal relational query languages id name dept name salary 10101 srinivasan comp sci 65000 12121 wu finance 90000 15151 mozart music 40000 22222 einstein physics 95000 32343 el said history 60000 33456 gold physics 87000 45565 katz comp sci 75000 58583 califieri history 62000 76543 singh finance 80000 76766 crick biology 72000 83821 brandt comp sci 92000 98345 kim elec eng 80000 figure 6.1 the instructor relation those tuples of the instructor relation where the instructor is in the ? physics ? department  we write   dept name = ? physics ?  instructor  if the instructor relation is as shown in figure 6.1  then the relation that results from the preceding query is as shown in figure 6.2 we can find all instructors with salary greater than $ 90,000 by writing   salary > 90000  instructor  in general  we allow comparisons using =   =  <  =  >  and = in the selection predicate furthermore,we can combine several predicates into a larger predicate by using the connectives and  ?   or  ?   and not  ?   thus  to find the instructors in physics with a salary greater than $ 90,000  we write   dept name = ? physics ? ? salary > 90000  instructor  the selection predicate may include comparisons between two attributes to illustrate  consider the relation department to find all departments whose name is the same as their building name  we can write   dept name = building  department  id name dept name salary 22222 einstein physics 95000 33456 gold physics 87000 figure 6.2 result of  dept name = ? physics ?  instructor   thesumit67.blogspot.com 6.1 the relational algebra 219 sql versus relational algebra the term select in relational algebra has a different meaning than the one used in sql  which is an unfortunate historical fact in relational algebra  the term select corresponds to what we refer to in sql as where we emphasize the different interpretations here to minimize potential confusion 6.1.1.2 the project operation suppose we want to list all instructors ? id  name  and salary  but do not care about the dept name the project operation allows us to produce this relation the project operation is a unary operation that returns its argument relation  with certain attributes left out since a relation is a set  any duplicate rows are eliminated projection is denoted by the uppercase greek letter pi     we list those attributes that we wish to appear in the result as a subscript to   the argument relation follows in parentheses we write the query to produce such a list as   id  name  salary  instructor  figure 6.3 shows the relation that results from this query 6.1.1.3 composition of relational operations the fact that the result of a relational operation is itself a relation is important consider the more complicated query ? find the name of all instructors in the physics department ? we write  id name salary 10101 srinivasan 65000 12121 wu 90000 15151 mozart 40000 22222 einstein 95000 32343 el said 60000 33456 gold 87000 45565 katz 75000 58583 califieri 62000 76543 singh 80000 76766 crick 72000 83821 brandt 92000 98345 kim 80000 figure 6.3 result of  id  name  salary  instructor   thesumit67.blogspot.com 220 chapter 6 formal relational query languages  name   dept name = ? physics ?  instructor   notice that  instead of giving the name of a relation as the argument of the projection operation  we give an expression that evaluates to a relation in general  since the result of a relational-algebra operation is of the same type  relation  as its inputs  relational-algebra operations can be composed together into a relational-algebra expression composing relational-algebra operations into relational-algebra expressions is just like composing arithmetic operations  such as + ,  *  and ?  into arithmetic expressions.we study the formal definition of relational-algebra expressions in section 6.1.2 6.1.1.4 the union operation consider a query to find the set of all courses taught in the fall 2009 semester  the spring 2010 semester  or both the information is contained in the section relation  figure 6.4   to find the set of all courses taught in the fall 2009 semester  we write   course id   semester = ? fall ? ? year = 2009  section   to find the set of all courses taught in the spring 2010 semester  we write   course id   semester = ? spring ? ? year = 2010  section   to answer the query  we need the union of these two sets ; that is  we need all section ids that appear in either or both of the two relations we find these data course id sec id semester year building room number time slot id bio-101 1 summer 2009 painter 514 b bio-301 1 summer 2010 painter 514 a cs-101 1 fall 2009 packard 101 h cs-101 1 spring 2010 packard 101 f cs-190 1 spring 2009 taylor 3128 e cs-190 2 spring 2009 taylor 3128 a cs-315 1 spring 2010 watson 120 d cs-319 1 spring 2010 watson 100 b cs-319 2 spring 2010 taylor 3128 c cs-347 1 fall 2009 taylor 3128 a ee-181 1 spring 2009 taylor 3128 c fin-201 1 spring 2010 packard 101 b his-351 1 spring 2010 painter 514 c mu-199 1 spring 2010 packard 101 d phy-101 1 fall 2009 watson 100 a figure 6.4 the section relation thesumit67.blogspot.com 6.1 the relational algebra 221 course id cs-101 cs-315 cs-319 cs-347 fin-201 his-351 mu-199 phy-101 figure 6.5 courses offered in either fall 2009  spring 2010 or both semesters by the binary operation union  denoted  as in set theory  by ?  so the expression needed is   course id   semester = ? fall ? ? year = 2009  section   ?  course id   semester = ? spring ? ? year = 2010  section   the result relation for this query appears in figure 6.5 notice that there are 8 tuples in the result  even though there are 3 distinct courses offered in the fall 2009 semester and 6 distinct courses offered in the spring 2010 semester since relations are sets  duplicate values such as cs-101  which is offered in both semesters  are replaced by a single occurrence observe that  in our example  we took the union of two sets  both of which consisted of course id values in general  we must ensure that unions are taken between compatible relations for example  it would not make sense to take the union of the instructor relation and the student relation although both relations have four attributes  they differ on the salary and tot cred domains the union of these two attributes would not make sense in most situations therefore  for a union operation r ? s to be valid  we require that two conditions hold  1 the relations r and s must be of the same arity that is  they must have the same number of attributes 2 the domains of the ith attribute of r and the ith attribute of s must be the same  for all i note that r and s can be either database relations or temporary relations that are the result of relational-algebra expressions 6.1.1.5 the set-difference operation the set-difference operation  denoted by   allows us to find tuples that are in one relation but are not in another the expression r  s produces a relation containing those tuples in r but not in s thesumit67.blogspot.com 222 chapter 6 formal relational query languages course id cs-347 phy-101 figure 6.6 courses offered in the fall 2009 semester but not in spring 2010 semester we can find all the courses taught in the fall 2009 semester but not in spring 2010 semester by writing   course id   semester = ? fall ? ? year = 2009  section     course id   semester = ? spring ? ? year = 2010  section   the result relation for this query appears in figure 6.6 as with the union operation  we must ensure that set differences are taken between compatible relations therefore  for a set-difference operation r  s to be valid  we require that the relations r and s be of the same arity  and that the domains of the ith attribute of r and the ith attribute of s be the same  for all i 6.1.1.6 the cartesian-product operation the cartesian-product operation  denoted by a cross  ?   allows us to combine information from any two relations we write the cartesian product of relations r1 and r2 as r1 ? r2 recall that a relation is by definition a subset of a cartesian product of a set of domains from that definition  we should already have an intuition about the definition of the cartesian-product operation however  since the same attribute name may appear in both r1 and r2  we need to devise a naming schema to distinguish between these attributes we do so here by attaching to an attribute the name of the relation from which the attribute originally came for example  the relation schema for r = instructor ? teaches is   instructor.id  instructor.name  instructor.dept name  instructor.salary teaches.id  teaches.course id  teaches.sec id  teaches.semester  teaches.year  with this schema  we can distinguish instructor.id from teaches.id for those attributes that appear in only one of the two schemas  we shall usually drop the relation-name prefix this simplification does not lead to any ambiguity we can then write the relation schema for r as   instructor.id  name  dept name  salary teaches.id  course id  sec id  semester  year  this naming convention requires that the relations that are the arguments of the cartesian-product operation have distinct names this requirement causes problems in some cases  such as when the cartesian product of a relation with itself is desired.asimilar problem arises if we use the result of a relational-algebra expression in a cartesian product  since we shall need a name for the relation so thesumit67.blogspot.com 6.1 the relational algebra 223 id course id sec id semester year 10101 cs-101 1 fall 2009 10101 cs-315 1 spring 2010 10101 cs-347 1 fall 2009 12121 fin-201 1 spring 2010 15151 mu-199 1 spring 2010 22222 phy-101 1 fall 2009 32343 his-351 1 spring 2010 45565 cs-101 1 spring 2010 45565 cs-319 1 spring 2010 76766 bio-101 1 summer 2009 76766 bio-301 1 summer 2010 83821 cs-190 1 spring 2009 83821 cs-190 2 spring 2009 83821 cs-319 2 spring 2010 98345 ee-181 1 spring 2009 figure 6.7 the teaches relation that we can refer to the relation ? s attributes in section 6.1.1.7  we see how to avoid these problems by using the rename operation nowthatwe know the relation schema for r = instructor ? teaches,what tuples appear in r ? as you may suspect  we construct a tuple of r out of each possible pair of tuples  one from the instructor relation  figure 6.1  and one fromthe teaches relation  figure 6.7   thus  r is a large relation  as you can see from figure 6.8  which includes only a portion of the tuples that make up r.1 assume that we have n1 tuples in instructor and n2 tuples in teaches then  there are n1 * n2 ways of choosing a pair of tuples ? one tuple from each relation ; so there are n1 * n2 tuples in r in particular  note that for some tuples t in r  it may be that t  instructor.id   = t  teaches.id   in general  if we have relations r1  r1  and r2  r2   then r1 ? r2 is a relation whose schema is the concatenation of r1 and r2 relation r contains all tuples t for which there is a tuple t1 in r1 and a tuple t2 in r2 for which t  r1  = t1  r1  and t  r2  = t2  r2   suppose that we want to find the names of all instructors in the physics department together with the course id of all courses they taught we need the information in both the instructor relation and the teaches relation to do so if we write   dept name = ? physics ?  instructor ? teaches  then the result is the relation in figure 6.9 1note that we renamed instructor.id as inst.id to reduce the width of the tables in figures 6.8 and 6.9 thesumit67.blogspot.com 224 chapter 6 formal relational query languages inst.id name dept name salary teaches.id course id sec id semester year 10101 srinivasan physics 95000 10101 cs-101 1 fall 2009 10101 srinivasan physics 95000 10101 cs-315 1 spring 2010 10101 srinivasan physics 95000 10101 cs-347 1 fall 2009 10101 srinivasan physics 95000 10101 fin-201 1 spring 2010 10101 srinivasan physics 95000 15151 mu-199 1 spring 2010 10101 srinivasan physics 95000 22222 phy-101 1 fall 2009                   12121 wu physics 95000 10101 cs-101 1 fall 2009 12121 wu physics 95000 10101 cs-315 1 spring 2010 12121 wu physics 95000 10101 cs-347 1 fall 2009 12121 wu physics 95000 10101 fin-201 1 spring 2010 12121 wu physics 95000 15151 mu-199 1 spring 2010 12121 wu physics 95000 22222 phy-101 1 fall 2009                   15151 mozart physics 95000 10101 cs-101 1 fall 2009 15151 mozart physics 95000 10101 cs-315 1 spring 2010 15151 mozart physics 95000 10101 cs-347 1 fall 2009 15151 mozart physics 95000 10101 fin-201 1 spring 2010 15151 mozart physics 95000 15151 mu-199 1 spring 2010 15151 mozart physics 95000 22222 phy-101 1 fall 2009                   22222 einstein physics 95000 10101 cs-101 1 fall 2009 22222 einstein physics 95000 10101 cs-315 1 spring 2010 22222 einstein physics 95000 10101 cs-347 1 fall 2009 22222 einstein physics 95000 10101 fin-201 1 spring 2010 22222 einstein physics 95000 15151 mu-199 1 spring 2010 22222 einstein physics 95000 22222 phy-101 1 fall 2009                   figure 6.8 result of instructor ? teaches we have a relation that pertains only to instructors in the physics department however  the course id column may contain information about courses that were not taught by the corresponding instructor  if you do not see why that is true  recall that the cartesian product takes all possible pairings of one tuple from instructor with one tuple of teaches  since the cartesian-product operation associates every tuple of instructor with every tuple of teaches,we know that if a an instructor is in the physics department  and has taught a course  as recorded in the teaches relation   then there is some thesumit67.blogspot.com 6.1 the relational algebra 225 inst.id name dept name salary teaches.id course id sec id semester year 22222 einstein physics 95000 10101 cs-437 1 fall 2009 22222 einstein physics 95000 10101 cs-315 1 spring 2010 22222 einstein physics 95000 12121 fin-201 1 spring 2010 22222 einstein physics 95000 15151 mu-199 1 spring 2010 22222 einstein physics 95000 22222 phy-101 1 fall 2009 22222 einstein physics 95000 32343 his-351 1 spring 2010                   33456 gold physics 87000 10101 cs-437 1 fall 2009 33456 gold physics 87000 10101 cs-315 1 spring 2010 33456 gold physics 87000 12121 fin-201 1 spring 2010 33456 gold physics 87000 15151 mu-199 1 spring 2010 33456 gold physics 87000 22222 phy-101 1 fall 2009 33456 gold physics 87000 32343 his-351 1 spring 2010                   figure 6.9 result of  dept name = ? physics ?  instructor ? teaches   tuple in  dept name = ? physics ?  instructor ? teaches  that contains his name  and which satisfies instructor.id = teaches.id so  if we write   instructor.id = teaches.id   dept name = ? physics ?  instructor ? teaches   we get only those tuples of instructor ? teaches that pertain to instructors in physics and the courses that they taught finally  since we only want the names of all instructors in the physics department together with the course id of all courses they taught  we do a projection   name  course id   instructor.id = teaches.id   dept name = ? physics ?  instructor ? teaches    the result of this expression  shown in figure 6.10  is the correct answer to our query observe that although instructor gold is in the physics department  he does not teach any course  as recorded in the teaches relation   and therefore does not appear in the result name course id einstein phy-101 figure 6.10 result of  name  course id   instructor.id = teaches.id   dept name = ? physics ?  instructor ? teaches     thesumit67.blogspot.com 226 chapter 6 formal relational query languages note that there is often more than one way to write a query in relational algebra consider the following query   name  course id   instructor.id = teaches.id    dept name = ? physics ?  instructor   ? teaches   note the subtle difference between the two queries  in the query above  the selection that restricts dept name to physics is applied to instructor  and the cartesian product is applied subsequently ; in contrast  the cartesian product was applied before the selection in the earlier query.however  the two queries are equivalent ; that is  they give the same result on any database 6.1.1.7 the rename operation unlike relations in the database  the results of relational-algebra expressions do not have a name that we can use to refer to them it is useful to be able to give them names ; the rename operator  denoted by the lowercase greek letter rho     lets us do this given a relational-algebra expression e  the expression  x  e  returns the result of expression e under the name x a relation r by itself is considered a  trivial  relational-algebra expression thus  we can also apply the rename operation to a relation r to get the same relation under a new name asecond form of the rename operation is as follows  assume that a relationalalgebra expression e has arity n then  the expression  x  a1,a2,...,an   e  returns the result of expression e under the name x  and with the attributes renamed to a1  a2      an to illustrate renaming a relation  we consider the query ? find the highest salary in the university ? our strategy is to  1  compute first a temporary relation consisting of those salaries that are not the largest and  2  take the set difference between the relation  salary  instructor  and the temporary relation just computed  to obtain the result 1 step 1  to compute the temporary relation  we need to compare the values of all salaries we do this comparison by computing the cartesian product instructor ? instructor and forming a selection to compare the value of any two salaries appearing in one tuple first  we need to devise a mechanism to distinguish between the two salary attributes we shall use the rename operation to rename one reference to the instructor relation ; thus we can reference the relation twice without ambiguity thesumit67.blogspot.com 6.1 the relational algebra 227 salary 65000 90000 40000 60000 87000 75000 62000 72000 80000 92000 figure 6.11 result of the subexpression  instructor.salary   instructor.salary < d.salary  instructor ?  d  instructor     we can now write the temporary relation that consists of the salaries that are not the largest   instructor.salary   instructor.salary < d.salary  instructor ?  d  instructor    this expression gives those salaries in the instructor relation for which a larger salary appears somewhere in the instructor relation  renamed as d   the result contains all salaries except the largest one figure 6.11 shows this relation 2 step 2  the query to find the largest salary in the university can be written as   salary  instructor    instructor.salary   instructor.salary < d.salary  instructor ?  d  instructor    figure 6.12 shows the result of this query the rename operation is not strictly required  since it is possible to use a positional notation for attributes.we canname attributes of a relation implicitly by using a positional notation  where $ 1  $ 2     refer to the first attribute  the second attribute  and so on the positional notation also applies to results of relationalalgebra operations the following relational-algebra expression illustrates the salary 95000 figure 6.12 highest salary in the university thesumit67.blogspot.com 228 chapter 6 formal relational query languages use of positional notation towrite the expression we saw earlier  which computes salaries that are not the largest   $ 4   $ 4 < $ 8  instructor ? instructor   note that the cartesian product concatenates the attributes of the two relations thus  for the result of the cartesian product  instructor ? instructor   $ 4 refers to the salary attribute from the first occurrence of instructor  while $ 8 refers to the salary attribute from the second occurrence of instructor a positional notation can also be used to refer to relation names  if a binary operation needs to distinguish between its two operand relations for example  $ r1 could refer to the first operand relation  and $ r2 could refer to the second operand relation of a cartesian product however  the positional notation is inconvenient for humans  since the position of the attribute is a number  rather than an easy-to-remember attribute name hence  we do not use the positional notation in this textbook 6.1.2 formal definition of the relational algebra the operations in section 6.1.1 allow us to give a complete definition of an expression in the relational algebra a basic expression in the relational algebra consists of either one of the following  ? a relation in the database ? a constant relation a constant relation is written by listing its tuples within    for example   22222  einstein  physics  95000    76543  singh  finance  80000    a general expression in the relational algebra is constructed out of smaller subexpressions let e1 and e2 be relational-algebra expressions then  the following are all relational-algebra expressions  ? e1 ? e2 ? e1  e2 ? e1 ? e2 ?  p  e1   where p is a predicate on attributes in e1 ?  s  e1   where s is a list consisting of some of the attributes in e1 ?  x  e1   where x is the new name for the result of e1 6.1.3 additional relational-algebra operations the fundamental operations of the relational algebra are sufficient to express any relational-algebra query.however  ifwe restrict ourselves to just the fundamental operations  certain common queries are lengthy to express therefore  we define additional operations that do not add any power to the algebra  but simplify thesumit67.blogspot.com 6.1 the relational algebra 229 course id cs-101 figure 6.13 courses offered in both the fall 2009 and spring 2010 semesters common queries for each new operation  we give an equivalent expression that uses only the fundamental operations 6.1.3.1 the set-intersection operation the first additional relational-algebra operation that we shall define is set intersection  n   suppose that we wish to find the set of all courses taught in both the fall 2009 and the spring 2010 semesters using set intersection  we can write  course id   semester = ? fall ? ? year = 2009  section   n  course id   semester = ? spring ? ? year = 2010  section   the result relation for this query appears in figure 6.13 note that we can rewrite any relational-algebra expression that uses set intersection by replacing the intersection operation with a pair of set-difference operations as  r n s = r   r  s  thus  set intersection is not a fundamental operation and does not add any power to the relational algebra it is simply more convenient to write r n s than to write r   r  s   6.1.3.2 the natural-join operation it is often desirable to simplify certain queries that require a cartesian product usually  a query that involves a cartesian product includes a selection operation on the result of the cartesian product the selection operation most often requires that all attributes that are common to the relations that are involved in the cartesian product be equated in our example query from section 6.1.1.6 that combined information from the instructor and teaches tables  the matching condition required instructor.id to be equal to teaches.id these are the only attributes in the two relations that have the same name the natural join is a binary operation that allows us to combine certain selections and a cartesian product into one operation it is denoted by the join symbol   the natural-join operation forms a cartesian product of its two arguments  performs a selection forcing equality on those attributes that appear in both relation schemas  and finally removes duplicate attributes returning to the example of the relations instructor and teaches  computing instructor natural join teaches considers only those pairs of tuples where both the tuple from instructor and the thesumit67.blogspot.com 230 chapter 6 formal relational query languages id name dept name salary course id sec id semester year 10101 srinivasan comp sci 65000 cs-101 1 fall 2009 10101 srinivasan comp sci 65000 cs-315 1 spring 2010 10101 srinivasan comp sci 65000 cs-347 1 fall 2009 12121 wu finance 90000 fin-201 1 spring 2010 15151 mozart music 40000 mu-199 1 spring 2010 22222 einstein physics 95000 phy-101 1 fall 2009 32343 el said history 60000 his-351 1 spring 2010 45565 katz comp sci 75000 cs-101 1 spring 2010 45565 katz comp sci 75000 cs-319 1 spring 2010 76766 crick biology 72000 bio-101 1 summer 2009 76766 crick biology 72000 bio-301 1 summer 2010 83821 brandt comp sci 92000 cs-190 1 spring 2009 83821 brandt comp sci 92000 cs-190 2 spring 2009 83821 brandt comp sci 92000 cs-319 2 spring 2010 98345 kim elec eng 80000 ee-181 1 spring 2009 figure 6.14 the natural join of the instructor relation with the teaches relation tuple from teaches have the same value on the common attribute id the result relation  shown in figure 6.14  has only 13 tuples  the ones that give information about an instructor and a course that that instructor actually teaches notice that we do not repeat those attributes that appear in the schemas of both relations ; rather they appear only once notice also the order in which the attributes are listed  first the attributes common to the schemas of both relations  second those attributes unique to the schema of the first relation  and finally  those attributes unique to the schema of the second relation although the definition of natural join is complicated  the operation is easy to apply as an illustration  consider again the example ? find the names of all instructors together with the course id of all courses they taught ? we express this query by using the natural join as follows   name  course id  instructor  teaches  since the schemas for instructor and teaches have the attribute id in common  the natural-join operation considers only pairs of tuples that have the same value on id it combines each such pair of tuples into a single tuple on the union of the two schemas ; that is   id  name  dept name  salary  course id   after performing the projection  we obtain the relation in figure 6.15 consider two relation schemasrand s ? which are  of course  lists of attribute names if we consider the schemas to be sets  rather than lists,we can denote those attribute names that appear in both r and s by r n s  and denote those attribute names that appear in r  in s  or in both by r ? s similarly  those attribute names that appear in r but not s are denoted by r  s  whereas s  r denotes those thesumit67.blogspot.com 6.1 the relational algebra 231 name course id srinivasan cs-101 srinivasan cs-315 srinivasan cs-347 wu fin-201 mozart mu-199 einstein phy-101 el said his-351 katz cs-101 katz cs-319 crick bio-101 crick bio-301 brandt cs-190 brandt cs-319 kim ee-181 figure 6.15 result of  name  course id  instructor  teaches   attribute names that appear in s but not in r note that the union  intersection  and difference operations here are on sets of attributes  rather than on relations we are now ready for a formal definition of the natural join consider two relations r  r  and s  s   the natural join of r and s  denoted by r  s  is a relation on schema r ? s formally defined as follows  r  s =  r ? s   r.a1 = s.a1 ? r.a2 = s.a2 ?  ? r.an = s.an  r ? s   where r n s =  a1  a2      an   please note that if r  r  and s  s  are relations without any attributes in common  that is  r n s = ?  then r  s = r ? s let us consider one more example of the use of natural join  towrite the query ? find the names of all instructors in the comp sci department together with the course titles of all the courses that the instructors teach ?  name,title   dept name = ? comp sci ?  instructor  teaches  course   the result relation for this query appears in figure 6.16 notice that we wrote instructor  teaches  course without inserting parentheses to specify the order in which the natural-join operations on the three relations should be executed in the preceding case  there are two possibilities   instructor  teaches   course instructor   teaches  course  we did not specifywhich expressionwe intended  because the two are equivalent that is  the natural join is associative thesumit67.blogspot.com 232 chapter 6 formal relational query languages name title brandt game design brandt image processing katz image processing katz intro to computer science srinivasan intro to computer science srinivasan robotics srinivasan database system concepts figure 6.16 result of  name,title   dept name = ? comp sci ?  instructor  teaches  course    the theta join operation is a variant of the natural-join operation that allows us to combine a selection and a cartesian product into a single operation consider relations r  r  and s  s   and let  be a predicate on attributes in the schema r ? s the theta join operation r   s is defined as follows  r   s =    r ? s  6.1.3.3 the assignment operation it is convenient at times towrite a relational-algebra expression by assigning parts of it to temporary relation variables the assignment operation  denoted by ?  works like assignment in a programming language to illustrate this operation  consider the definition of the natural-join operation.we could write r  s as  temp1 ? r ? s temp2 ?  r.a1 = s.a1 ? r.a2 = s.a2 ?  ? r.an = s.an  temp1  result =  r ? s  temp2  the evaluation of an assignment does not result in any relation being displayed to the user rather  the result of the expression to the right of the ? is assigned to the relation variable on the left of the ?  this relation variable may be used in subsequent expressions with the assignment operation  a query can bewritten as a sequential program consisting of a series of assignments followed by an expression whose value is displayed as the result of the query for relational-algebra queries  assignment must always be made to a temporary relation variable.assignments to permanent relations constitute a database modification note that the assignment operation does not provide any additional power to the algebra it is  however  a convenient way to express complex queries 6.1.3.4 outer join operations the outer-join operation is an extension of the join operation to deal with missing information suppose that there is some instructor who teaches no courses then thesumit67.blogspot.com 6.1 the relational algebra 233 id name dept name salary course id sec id semester year 10101 srinivasan comp sci 65000 cs-101 1 fall 2009 10101 srinivasan comp sci 65000 cs-315 1 spring 2010 10101 srinivasan comp sci 65000 cs-347 1 fall 2009 12121 wu finance 90000 fin-201 1 spring 2010 15151 mozart music 40000 mu-199 1 spring 2010 22222 einstein physics 95000 phy-101 1 fall 2009 32343 el said history 60000 his-351 1 spring 2010 33456 gold physics 87000 null null null null 45565 katz comp sci 75000 cs-101 1 spring 2010 45565 katz comp sci 75000 cs-319 1 spring 2010 58583 califieri history 62000 null null null null 76543 singh finance 80000 null null null null 76766 crick biology 72000 bio-101 1 summer 2009 76766 crick biology 72000 bio-301 1 summer 2010 83821 brandt comp sci 92000 cs-190 1 spring 2009 83821 brandt comp sci 92000 cs-190 2 spring 2009 83821 brandt comp sci 92000 cs-319 2 spring 2010 98345 kim elec eng 80000 ee-181 1 spring 2009 figure 6.17 result of instructor  teaches the tuple in the instructor relation  figure 6.1  for that particular instructor would not satisfy the condition of a natural join with the teaches relation  figure 6.7  and that instructor ? s data would not appear in the result of the natural join  shown in figure 6.14 for example  instructors califieri  gold  and singh do not appear in the result of the natural join  since they do not teach any course more generally  some tuples in either or both of the relations being joined may be ? lost ? in this way the outer join operation works in a manner similar to the natural join operation we have already studied  but preserves those tuples that would be lost in an join by creating tuples in the result containing null values we can use the outer-join operation to avoid this loss of information there are actually three forms of the operation  left outer join  denoted  ; right outer join  denoted  ; and full outer join  denoted   all three forms of outer join compute the join  and add extra tuples to the result of the join for example  the results of the expression instructor  teaches and teaches  instructor appear in figures 6.17 and 6.18  respectively the left outer join    takes all tuples in the left relation that did not match with any tuple in the right relation  pads the tuples with null values for all other attributes from the right relation  and adds them to the result of the natural join in figure 6.17  tuple  58583  califieri  history  62000  null  null  null  null   is such a tuple all information from the left relation is present in the result of the left outer join thesumit67.blogspot.com 234 chapter 6 formal relational query languages id course id sec id semester year name dept name salary 10101 cs-101 1 fall 2009 srinivasan comp sci 65000 10101 cs-315 1 spring 2010 srinivasan comp sci 65000 10101 cs-347 1 fall 2009 srinivasan comp sci 65000 12121 fin-201 1 spring 2010 wu finance 90000 15151 mu-199 1 spring 2010 mozart music 40000 22222 phy-101 1 fall 2009 einstein physics 95000 32343 his-351 1 spring 2010 el said history 60000 33456 null null null null gold physics 87000 45565 cs-101 1 spring 2010 katz comp sci 75000 45565 cs-319 1 spring 2010 katz comp sci 75000 58583 null null null null califieri history 62000 76543 null null null null singh finance 80000 76766 bio-101 1 summer 2009 crick biology 72000 76766 bio-301 1 summer 2010 crick biology 72000 83821 cs-190 1 spring 2009 brandt comp sci 92000 83821 cs-190 2 spring 2009 brandt comp sci 92000 83821 cs-319 2 spring 2010 brandt comp sci 92000 98345 ee-181 1 spring 2009 kim elec eng 80000 figure 6.18 result of teaches  instructor  the right outer join    is symmetric with the left outer join  it pads tuples from the right relation that did not match any from the left relationwith nulls and adds them to the result of the natural join in figure 6.18  tuple  58583  null  null  null  null  califieri  history  62000   is such a tuple thus  all information fromthe right relation is present in the result of the right outer join the full outer join    does both the left and right outer join operations  padding tuples from the left relation that did not match any from the right relation  as well as tuples from the right relation that did not match any from the left relation  and adding them to the result of the join note that in going fromour left-outer-join example to our right-outer-join example  we chose to swap the order of the operands thus both examples preserve tuples from the instructor relation  and thus contain the same information in our example relations  teaches tuples always have matching instructor tuples  and thus teaches  instructor would give the same result as teaches  instructor if there were tuples in teaches without matching tuples in instructor  such tuples would appear padded with nulls in teaches  instructor as well as in teaches  instructor  further examples of outer joins  expressed in sql syntax  may be found in section 4.1.2 since outer-join operations may generate results containing null values  we need to specify how the different relational-algebra operations deal with null values section 3.6 dealt with this issue in the context of sql the same concepts apply for the case of relational algebra  and we omit details thesumit67.blogspot.com 6.1 the relational algebra 235 it is interesting to note that the outer-join operations can be expressed by the basic relational-algebra operations for instance  the left outer join operation  r  s  can be written as   r  s  ?  r   r  r  s   ?   null      null   where the constant relation   null      null   is on the schema s  r 6.1.4 extended relational-algebra operations we now describe relational-algebra operations that provide the ability to write queries that can not be expressed using the basic relational-algebra operations these operations are called extended relational-algebra operations 6.1.4.1 generalized projection the first operation is the generalized-projection operation  which extends the projection operation by allowing operations such as arithmetic and string functions to be used in the projection list the generalized-projection operation has the form   f1,f2,...,fn  e  where e is any relational-algebra expression  and each of f1  f2      fn is an arithmetic expression involving constants and attributes in the schema of e as a base case  the expression may be simply an attribute or a constant in general  an expression can use arithmetic operations such as + ,  *  and ? on numeric valued attributes  numeric constants  and on expressions that generate a numeric result generalized projection also permits operations on other data types  such as concatenation of strings for example  the expression   id,name,dept name,salary ? 12  instructor  gives the id  name  dept name  and the monthly salary of each instructor 6.1.4.2 aggregation the second extended relational-algebra operation is the aggregate operation g  which permits the use of aggregate functions such as min or average  on sets of values aggregate functions take a collection of values and return a single value as a result for example  the aggregate function sum takes a collection of values and returns the sum of the values thus  the function sum applied on the collection   1  1  3  4  4  11  thesumit67.blogspot.com 236 chapter 6 formal relational query languages returns the value 24 the aggregate function avg returns the average of the values when applied to the preceding collection  it returns the value 4 the aggregate function count returns the number of the elements in the collection  and returns 6 on the preceding collection other common aggregate functions include min and max  which return the minimum and maximum values in a collection ; they return 1 and 11  respectively  on the preceding collection the collections on which aggregate functions operate can have multiple occurrences of a value ; the order in which the values appear is not relevant such collections are called multisets sets are a special case of multisets where there is only one copy of each element to illustrate the concept of aggregation  we shall use the instructor relation suppose that we want to find out the sum of salaries of all instructors ; the relational-algebra expression for this query is  gsum  salary   instructor  the symbol g is the letter g in calligraphic font ; read it as ? calligraphic g ? the relational-algebra operation g signifies that aggregation is to be applied  and its subscript specifies the aggregate operation to be applied the result of the expression above is a relation with a single attribute  containing a single row with a numerical value corresponding to the sum of the salaries of all instructors there are cases where we must eliminate multiple occurrences of a value before computing an aggregate function if we do want to eliminate duplicates  we use the same function names as before  with the addition of the hyphenated string ? distinct ? appended to the end of the function name  for example  countdistinct   an example arises in the query ? find the total number of instructors who teach a course in the spring 2010 semester ? in this case  an instructor counts only once  regardless of the number of course sections that the instructor teaches the required information is contained in the relation teaches  and we write this query as follows  gcount-distinct  id    semester = ? spring ? ? year = 2010  teaches   the aggregate function count-distinct ensures that even if an instructor teaches more than one course  she is counted only once in the result there are circumstanceswhere we would like to apply the aggregate function not to a single set of tuples  but instead to a group of sets of tuples as an illustration  consider the query ? find the average salary in each department ? we write this query as follows  dept namegaverage  salary   instructor  figure 6.19 shows the tuples in the instructor relation grouped by the dept name attribute this is the first step in computing the query result the specified aggregate is computed for each group  and the result of the query is shown in figure 6.20 thesumit67.blogspot.com 6.1 the relational algebra 237 id name dept name salary 76766 crick biology 72000 45565 katz comp sci 75000 10101 srinivasan comp sci 65000 83821 brandt comp sci 92000 98345 kim elec eng 80000 12121 wu finance 90000 76543 singh finance 80000 32343 el said history 60000 58583 califieri history 62000 15151 mozart music 40000 33456 gold physics 87000 22222 einstein physics 95000 figure 6.19 tuples of the instructor relation  grouped by the dept name attribute in contrast  consider the query ? find the average salary of all instructors ? we write this query as follows  gaverage  salary   instructor  in this case the attribute dept name has been omitted from the left side of the g operator  so the entire relation is treated as a single group the general form of the aggregation operation g is as follows  g1,g2,...,gn gf1  a1   f2  a2    fm  am   e  where e is any relational-algebra expression ; g1  g2      gn constitute a list of attributes on which to group ; each fi is an aggregate function ; and each ai is an attribute name the meaning of the operation is as follows  the tuples in the result of expression e are partitioned into groups in such a way that  dept name salary biology 72000 comp sci 77333 elec eng 80000 finance 85000 history 61000 music 40000 physics 91000 figure 6.20 the result relation for the query ? find the average salary in each department ?  thesumit67.blogspot.com 238 chapter 6 formal relational query languages multiset relational algebra unlike the relational algebra  sql allows multiple copies of a tuple in an input relation as well as in a query result the sql standard defines howmany copies of each tuple are there in the output of a query  which depends in turn on how many copies of tuples are present in the input relations to model this behavior of sql  a version of relational algebra  called the multiset relational algebra  is defined to work on multisets  that is  sets that may contain duplicates the basic operations in the multiset relational algebra are defined as follows  1 if there are c1 copies of tuple t1 in r1  and t1 satisfies selection    then there are c1 copies of t1 in    r1   2 for each copy of tuple t1 in r1  there is a copy of tuple  a  t1  in  a  r1   where  a  t1  denotes the projection of the single tuple t1 3 if there are c1 copies of tuple t1 in r1 and c2 copies of tuple t2 in r2  there are c1 * c2 copies of the tuple t1.t2 in r1 ? r2 for example  suppose that relations r1 with schema  a  b  andr2 with schema  c  are the following multisets  r1 =   1  a    2  a   r2 =   2    3    3   then  b  r1  would be   a    a    whereas  b  r1  ? r2 would be    a  2    a  2    a  3    a  3    a  3    a  3   multiset union  intersection and set difference can also be defined in a similar way  following the corresponding definitions in sql  which we saw in section 3.5 there is no change in the definition of the aggregation operation 1 all tuples in a group have the same values for g1  g2      gn 2 tuples in different groups have different values for g1  g2      gn thus  the groups can be identified by the values of attributes g1  g2      gn for each group  g1  g2      gn   the result has a tuple  g1  g2      gn  a1  a2      am  where  for each i  ai is the result of applying the aggregate function fi on the multiset of values for attribute ai in the group asa special case of the aggregate operation  the list of attributesg1  g2      gn can be empty  in which case there is a single group containing all tuples in the relation this corresponds to aggregation without grouping thesumit67.blogspot.com 6.2 the tuple relational calculus 239 sql and relational algebra from a comparison of the relational algebra operations and the sql operations  it should be clear that there is a close connection between the two a typical sql query has the form  select a1  a2      an from r1  r2      rm where p each ai represents an attribute  and each ri a relation p is a predicate the query is equivalent to the multiset relational-algebra expression   a1  a2,...,an   p  r1 ? r2 ? ? ? ? ? rm   if the where clause is omitted  the predicate p is true more complex sql queries can also be rewritten in relational algebra for example  the query  select a1  a2  sum  a3  from r1  r2      rm where p group by a1  a2 is equivalent to  a1  a2 gsum  a3    a1  a2  an   p  r1 ? r2 ? ? ? ? ? rm    join expressions in the from clause can be written using equivalent join expressions in relational algebra ; we leave the details as an exercise for the reader however  subqueries in the where or select clause can not be rewritten into relational algebra in such a straightforward manner  since there is no relationalalgebra operation equivalent to the subquery construct extensions of relational algebra have been proposed for this task  but are beyond the scope of this book 6.2 the tuple relational calculus when we write a relational-algebra expression  we provide a sequence of procedures that generates the answer to our query the tuple relational calculus  by contrast  is a nonprocedural query language it describes the desired information without giving a specific procedure for obtaining that information a query in the tuple relational calculus is expressed as   t | p  t   thesumit67.blogspot.com 240 chapter 6 formal relational query languages that is  it is the set of all tuples t such that predicate p is true for t following our earlier notation  we use t  a  to denote the value of tuple t on attribute a  and we use t ? r to denote that tuple t is in relation r before we give a formal definition of the tuple relational calculus  we return to some of the queries for which we wrote relational-algebra expressions in section 6.1.1 6.2.1 example queries find the id  name  dept name  salary for instructors whose salary is greater than $ 80,000   t | t ? instructor ? t  salary  > 80000  suppose that we want only the id attribute  rather than all attributes of the instructor relation to write this query in the tuple relational calculus  we need to write an expression for a relation on the schema  id  .we need those tuples on  id  such that there is a tuple in instructor with the salary attribute > 80000 to express this request  we need the construct ? there exists ? from mathematical logic the notation  ? t ? r  q  t   means ? there exists a tuple t in relation r such that predicate q  t  is true ? using this notation  we can write the query ? find the instructor id for each instructor with a salary greater than $ 80,000 ? as   t | ? s ? instructor  t  id  = s  id  ? s  salary  > 80000   in english  we read the preceding expression as ? the set of all tuples t such that there exists a tuple s in relation instructor for which the values of t and s for the id attribute are equal  and the value of s for the salary attribute is greater than $ 80,000 ? tuple variable t is defined on only the id attribute  since that is the only attribute having a condition specified for t thus  the result is a relation on  id   consider the query ? find the names of all instructors whose department is in the watson building ? this query is slightly more complex than the previous queries  since it involves two relations  instructor and department as we shall see  however  all it requires is that we have two ? there exists ? clauses in our tuple-relational-calculus expression  connected by and  ?  .we write the query as follows   t | ? s ? instructor  t  name  = s  name  ? ? u ? department  u  dept name  = s  dept name  ? u  building  = ? watson ?    thesumit67.blogspot.com 6.2 the tuple relational calculus 241 name einstein crick gold figure 6.21 names of all instructors whose department is in the watson building tuple variable u is restricted to departments that are located in thewatson building  while tuple variable s is restricted to instructors whose dept name matches that of tuple variable u figure 6.21 shows the result of this query to find the set of all courses taught in the fall 2009 semester  the spring 2010 semester  or both  we used the union operation in the relational algebra in the tuple relational calculus  we shall need two ? there exists ? clauses  connected by or  ?    t | ? s ? section  t  course id  = s  course id   ? s  semester  = ? fall ? ? s  year  = 2009   ? ? u ? section  u  course id  = t  course id    ? u  semester  = ? spring ? ? u  year  = 2010   this expression gives us the set of all course id tuples for which at least one of the following holds  ? the course id appears in some tuple of the section relation with semester = fall and year = 2009 ? the course id appears in some tuple of the section relation with semester = spring and year = 2010 if the same course is offered in both the fall 2009 and spring 2010 semesters  its course id appears only once in the result  because the mathematical definition of a set does not allow duplicate members the result of this query appeared earlier in figure 6.5 if we now want only those course id values for courses that are offered in both the fall 2009 and spring 2010 semesters  all we need to do is to change the or  ?  to and  ?  in the preceding expression  t | ? s ? section  t  course id  = s  course id   ? s  semester  = ? fall ? ? s  year  = 2009   ? ? u ? section  u  course id  = t  course id    ? u  semester  = ? spring ? ? u  year  = 2010   the result of this query appeared in figure 6.13 now consider the query ? find all the courses taught in the fall 2009 semester but not in spring 2010 semester ? the tuple-relational-calculus expression for this thesumit67.blogspot.com 242 chapter 6 formal relational query languages query is similar to the expressions that we have just seen  except for the use of the not  ?  symbol   t | ? s ? section  t  course id  = s  course id   ? s  semester  = ? fall ? ? s  year  = 2009   ? ? ? u ? section  u  course id  = t  course id    ? u  semester  = ? spring ? ? u  year  = 2010   this tuple-relational-calculus expression uses the ? s ? section      clause to require that a particular course id is taught in the fall 2009 semester  and it uses the ? ? u ? section      clause to eliminate those course id values that appear in some tuple of the section relation as having been taught in the spring 2010 semester the query that we shall consider next uses implication  denoted by ?  the formula p ? q means ? p implies q ? ; that is  ? if p is true  then q must be true ? note that p ? q is logically equivalent to ? p ? q the use of implication rather than not and or often suggests a more intuitive interpretation of a query in english consider the query that ? find all students who have taken all courses offered in the biology department ? to write this query in the tuple relational calculus  we introduce the ? for all ? construct  denoted by ?  the notation  ? t ? r  q  t   means ? q is true for all tuples t in relation r ? we write the expression for our query as follows   t | ? r ? student  r  id  = t  id   ?  ? u ? course  u  dept name  = ? biology ? ? ? s ? takes  t  id  = s  id  ? s  course id  = u  course id     in english  we interpret this expression as ? the set of all students  that is   id  tuples t  suchthat  for all tuples u in the course relation  if the value of u on attribute dept name is ? biology ?  then there exists a tuple in the takes relation that includes the student id and the course id ? note that there is a subtlety in the above query  if there is no course offered in the biology department  all student ids satisfy the condition the first line of the query expression is critical in this case ? without the condition ? r ? student  r  id  = t  id   if there is no course offered in the biology department  any value of t  including values that are not student ids in the student relation  would qualify thesumit67.blogspot.com 6.2 the tuple relational calculus 243 6.2.2 formal definition we are now ready for a formal definition a tuple-relational-calculus expression is of the form   t | p  t   where p is a formula several tuple variables may appear in a formula a tuple variable is said to be a free variable unless it is quantified by a ? or ?  thus  in  t ? instructor ? ? s ? department  t  dept name  = s  dept name   t is a free variable tuple variable s is said to be a bound variable a tuple-relational-calculus formula is built up out of atoms an atomhas one of the following forms  ? s ? r  where s is a tuple variable and r is a relation  we do not allow use of the / ? operator   ? s  x   u  y   where s and u are tuple variables  x is an attribute on which s is defined  y is an attribute on which u is defined  and  is a comparison operator  <  =  =   =  >  =  ; we require that attributes x and y have domains whose members can be compared by   ? s  x   c  where s is a tuple variable  x is an attribute on which s is defined   is a comparison operator  and c is a constant in the domain of attribute x we build up formulae from atoms by using the following rules  ? an atom is a formula ? if p1 is a formula  then so are ? p1 and  p1   ? if p1 and p2 are formulae  then so are p1 ? p2  p1 ? p2  and p1 ? p2 ? if p1  s  is a formula containing a free tuple variable s  and r is a relation  then ? s ? r  p1  s   and ? s ? r  p1  s   are also formulae as we could for the relational algebra  we can write equivalent expressions that are not identical in appearance in the tuple relational calculus  these equivalences include the following three rules  1 p1 ? p2 is equivalent to ?  ?  p1  ? ?  p2    2 ? t ? r  p1  t   is equivalent to ? ? t ? r  ? p1  t    3 p1 ? p2 is equivalent to ?  p1  ? p2 thesumit67.blogspot.com 244 chapter 6 formal relational query languages 6.2.3 safety of expressions there is one final issue to be addressed a tuple-relational-calculus expression may generate an infinite relation suppose that we write the expression   t | ?  t ? instructor   there are infinitely many tuples that are not in instructor most of these tuples contain values that do not even appear in the database ! clearly  we do not wish to allow such expressions to help us define a restriction of the tuple relational calculus  we introduce the concept of the domain of a tuple relational formula  p intuitively  the domain of p  denoted dom  p   is the set of all values referenced by p they include values mentioned in p itself  as well as values that appear in a tuple of a relation mentioned in p thus  thedomainof p is the set of all values that appear explicitly in p or that appear in one or more relations whose names appear in p for example  dom  t ? instructor ? t  salary  > 80000  is the set containing 80000 as well as the set of all values appearing in any attribute of any tuple in the instructor relation similarly  dom  ?  t ? instructor   is also the set of all values appearing in instructor  since the relation instructor is mentioned in the expression we say that an expression  t | p  t   is safe if all values that appear in the result are values from dom  p   the expression  t | ?  t ? instructor   is not safe note that dom  ?  t ? instructor   is the set of all values appearing in instructor however  it is possible to have a tuple t not in instructor that contains values that do not appear in instructor the other examples of tuple-relational-calculus expressions that we have written in this section are safe the number of tuples that satisfy an unsafe expression  such as  t | ?  t ? instructor    could be infinite  whereas safe expressions are guaranteed to have finite results the class of tuple-relational-calculus expressions that are allowed is therefore restricted to those that are safe 6.2.4 expressive power of languages the tuple relational calculus restricted to safe expressions is equivalent in expressive power to the basic relational algebra  with the operators ? ,  ?    and   but without the extended relational operations such as generalized projection and aggregation  g    thus  for every relational-algebra expression using only the basic operations  there is an equivalent expression in the tuple relational calculus  and for every tuple-relational-calculus expression  there is an equivalent relationalalgebra expression.we shall not prove this assertion here ; the bibliographic notes contain references to the proof some parts of the proof are included in the exercises we note that the tuple relational calculus does not have any equivalent of the aggregate operation  but it can be extended to support aggregation extending the tuple relational calculus to handle arithmetic expressions is straightforward thesumit67.blogspot.com 6.3 the domain relational calculus 245 6.3 the domain relational calculus a second form of relational calculus  called domain relational calculus  uses domain variables that take on values froman attributes domain  rather than values for an entire tuple the domain relational calculus  however  is closely related to the tuple relational calculus domain relational calculus serves as the theoretical basis of the widely used qbe language  see appendix b.1   just as relational algebra serves as the basis for the sql language 6.3.1 formal definition an expression in the domain relational calculus is of the form  < x1  x2      xn > | p  x1  x2      xn   where x1  x2      xn represent domain variables p represents a formula composed of atoms  as was the case in the tuple relational calculus an atom in the domain relational calculus has one of the following forms  ? < x1  x2      xn > ? r  where r is a relation on n attributes and x1  x2      xn are domain variables or domain constants ? x  y  where x and y are domain variables and  is a comparison operator  <  =  =   =  >  =   we require that attributes x and y have domains that can be compared by   ? x  c  where x is a domain variable   is a comparison operator  and c is a constant in the domain of the attribute for which x is a domain variable we build up formulae from atoms by using the following rules  ? an atom is a formula ? if p1 is a formula  then so are ? p1 and  p1   ? if p1 and p2 are formulae  then so are p1 ? p2  p1 ? p2  and p1 ? p2 ? if p1  x  is a formula in x  where x is a free domain variable  then ? x  p1  x   and ? x  p1  x   are also formulae as a notational shorthand,wewrite ? a  b  c  p  a  b  c   for ? a  ? b  ? c  p  a  b  c      thesumit67.blogspot.com 246 chapter 6 formal relational query languages 6.3.2 example queries we now give domain-relational-calculus queries for the examples that we considered earlier note the similarity of these expressions and the corresponding tuple-relational-calculus expressions ? find the instructor id  name  dept name  andsalary for instructors whose salary is greater than $ 80,000   < i  n  d  s > | < i  n  d  s > ? instructor ? s > 80000  ? find all instructor id for instructors whose salary is greater than $ 80,000   < n > | ? i  d  s  < i  n  d  s > ? instructor ? s > 80000   although the second query appears similar to the one that we wrote for the tuple relational calculus  there is an important difference in the tuple calculus  when we write ? s for some tuple variable s  we bind it immediately to a relation by writing ? s ? r  however  when we write ? n in the domain calculus  n refers not to a tuple  but rather to a domain value thus  the domain of variable n is unconstrained until the subformula < i  n  d  s > ? instructor constrains n to instructor names that appear in the instructor relation we now give several examples of queries in the domain relational calculus ? find the names of all instructors in the physics department together with the course id of all courses they teach   < n  c > | ? i  a  < i  c  a  s  y > ? teaches ? ? d  s  < i  n  d  s > ? instructor ? d = ? physics ?    ? find the set of all courses taught in the fall 2009 semester  the spring 2010 semester  or both   < c > | ? s  < c  a  s  y  b  r  t > ? section ? s = ? fall ? ? y = ? 2009 ? ? ? u  < c  a  s  y  b  r  t > ? section ? s = ? spring ? ? y = ? 2010 ? ? find all students who have taken all courses offered in the biology department   < i > | ? n  d  t  < i  n  d  t > ? student  ? ? x  y  z,w  < x  y  z,w > ? course ? z = ? biology ? ? ? a  b  < a  x  b  r  p  q > ? takes ? < c  a > ? depositor    thesumit67.blogspot.com 6.3 the domain relational calculus 247 note that as was the case for tuple-relational-calculus  if no courses are offered in the biology department  all students would be in the result 6.3.3 safety of expressions we noted that  in the tuple relational calculus  section 6.2   it is possible to write expressions that may generate an infinite relation that led us to define safety for tuple-relational-calculus expressions a similar situation arises for the domain relational calculus an expression such as  < i  n  d  s > | ?  < i  n  d  s > ? instructor   is unsafe  because it allows values in the result that are not in the domain of the expression for the domain relational calculus,wemust be concerned also about the form of formulae within ? there exists ? and ? for all ? clauses consider the expression  < x > | ? y  < x  y > ? r  ? ? z  ?  < x  z > ? r  ? p  x  z    where p is some formula involving x and z we can test the first part of the formula  ? y  < x  y > ? r   by considering only the values in r however  to test the second part of the formula  ? z  ?  < x  z > ? r  ? p  x  z    we must consider values for z that do not appear in r since all relations are finite  an infinite number of values do not appear in r thus  it is not possible  in general  to test the second part of the formula without considering an infinite number of potential values for z instead  we add restrictions to prohibit expressions such as the preceding one in the tuple relational calculus,we restricted any existentially quantified variable to range over a specific relation sincewedid not do so in the domain calculus  we add rules to the definition of safety to deal with cases like our example we say that an expression  < x1  x2      xn > | p  x1  x2      xn   is safe if all of the following hold  1 all values that appear in tuples of the expression are values from dom  p   2 for every ? there exists ? subformula of the form ? x  p1  x    the subformula is true if and only if there is a value x in dom  p1  such that p1  x  is true 3 for every ? for all ? subformula of the form ? x  p1  x    the subformula is true if and only if p1  x  is true for all values x from dom  p1   the purpose of the additional rules is to ensure that we can test ? for all ? and ? there exists ? subformulae without having to test infinitely many possibilities consider the second rule in the definition of safety for ? x  p1  x   to be true  thesumit67.blogspot.com 248 chapter 6 formal relational query languages we need to find only one x for which p1  x  is true in general  there would be infinitely many values to test however  if the expression is safe  we know that we can restrict our attention to values from dom  p1   this restriction reduces to a finite number the tuples we must consider the situation for subformulae of the form ? x  p1  x   is similar to assert that ? x  p1  x   is true  we must  in general  test all possible values  so we must examine infinitely many values as before  if we know that the expression is safe  it is sufficient for us to test p1  x  for those values taken from dom  p1   all the domain-relational-calculus expressions that we have written in the example queries of this section are safe  except for the example unsafe query we saw earlier 6.3.4 expressive power of languages when the domain relational calculus is restricted to safe expressions  it is equivalent in expressive power to the tuple relational calculus restricted to safe expressions since we noted earlier that the restricted tuple relational calculus is equivalent to the relational algebra  all three of the following are equivalent  ? the basic relational algebra  without the extended relational-algebra operations  ? the tuple relational calculus restricted to safe expressions ? the domain relational calculus restricted to safe expressions we note that the domain relational calculus also does not have any equivalent of the aggregate operation  but it can be extended to support aggregation  and extending it to handle arithmetic expressions is straightforward 6.4 summary ? the relational algebra defines a set of algebraic operations that operate on tables  and output tables as their results these operations can be combined to get expressions that express desired queries the algebra defines the basic operations used within relational query languages ? the operations in relational algebra can be divided into  ? basic operations ? additional operations that can be expressed in terms of the basic operations ? extended operations  some of which add further expressive power to relational algebra ? the relational algebra is a terse  formal language that is inappropriate for casual users of a database system commercial database systems  therefore  thesumit67.blogspot.com practice exercises 249 use languages with more ? syntactic sugar ? in chapters 3 through 5  we cover the most influential language ? sql  which is based on relational algebra ? the tuple relational calculus and the domain relational calculus are nonprocedural languages that represent the basic power required in a relational query language the basic relational algebra is a procedural language that is equivalent in power to both forms of the relational calculus when they are restricted to safe expressions ? the relational calculi are terse  formal languages that are inappropriate for casual users of a database system these two formal languages form the basis for two more user-friendly languages  qbe and datalog  that we cover in appendix b review terms ? relational algebra ? relational-algebra operations ? select  ? project  ? union ? ? set difference  ? cartesian product ? ? rename  ? additional operations ? set intersection n ? natural join  ? assignment operation ? outer join  left outer join   right outer join   full outer join  ? multisets ? grouping ? null value ? tuple relational calculus ? domain relational calculus ? safety of expressions ? expressive power of languages practice exercises 6.1 write the following queries in relational algebra  using the university schema a find the titles of courses in the comp sci department that have 3 credits b find the ids of all students who were taught by an instructor named einstein ; make sure there are no duplicates in the result c find the highest salary of any instructor d find all instructors earning the highest salary  there may be more than one with the same salary   thesumit67.blogspot.com 250 chapter 6 formal relational query languages employee  person name  street  city  works  person name  company name  salary  company  company name  city  manages  person name  manager name  figure 6.22 relational database for exercises 6.2  6.8  6.11  6.13  and 6.15 e find the enrollment of each section that was offered in autumn 2009 f find the maximum enrollment  across all sections  in autumn 2009 g find the sections that had themaximum enrollment in autumn 2009 6.2 consider the relational database of figure 6.22  where the primary keys are underlined give an expression in the relational algebra to express each of the following queries  a find the names of all employees who live in the same city and on the same street as do their managers b find the names of all employees in this database who do not work for ? first bank corporation ?  c find the names of all employeeswho earnmore than every employee of ? small bank corporation ?  6.3 the natural outer-join operations extend the natural-join operation so that tuples from the participating relations are not lost in the result of the join describe how the theta-join operation can be extended so that tuples from the left  right  or both relations are not lost from the result of a theta join 6.4  division operation   the division operator of relational algebra  ? ? ?  is defined as follows let r  r  and s  s  be relations  and let s ? r ; that is  every attribute of schema s is also in schema r then r ? s is a relation on schema r  s  that is  on the schema containing all attributes of schema r that are not in schema s   a tuple t is in r ? s if and only if both of two conditions hold  ? t is in  r-s  r  ? for every tuple ts in s  there is a tuple tr in r satisfying both of the following  a tr  s  = ts  s  b tr  r  s  = t given the above definition  a write a relational algebra expression using the division operator to find the ids of all students who have taken all comp sci courses  hint  project takes to just id and course id  and generate the set of thesumit67.blogspot.com practice exercises 251 all comp sci course ids using a select expression  before doing the division  b show how to write the above query in relational algebra  without using division  by doing so  you would have shown how to define the division operation using the other relational algebra operations  6.5 let the following relation schemas be given  r =  a  b,c  s =  d  e  f  let relations r  r  and s  s  be given give an expression in the tuple relational calculus that is equivalent to each of the following  a  a  r  b  b = 17  r  c r ? s d  a,f   c = d  r ? s   6.6 let r =  a  b  c   and let r1 and r2 both be relations on schema r give an expression in the domain relational calculus that is equivalent to each of the following  a  a  r1  b  b = 17  r1  c r1 ? r2 d r1 n r2 e r1  r2 f  a,b  r1    b,c  r2  6.7 let r =  a  b  and s =  a,c   and let r  r  and s  s  be relations.write expressions in relational algebra for each of the following queries  a  < a > | ? b  < a  b > ? r ? b = 7   b  < a  b  c > | < a  b > ? r ? < a  c > ? s  c  < a > | ? c  < a  c > ? s ? ? b1  b2  < a  b1 > ? r ? < c  b2 > ? r ? b1 > b2    6.8 consider the relational database of figure 6.22 where the primary keys are underlined give an expression in tuple relational calculus for each of the following queries  thesumit67.blogspot.com 252 chapter 6 formal relational query languages a find all employees who work directly for ? jones ? b find all cities of residence of all employees who work directly for ? jones ? c find the name of the manager of the manager of ? jones ? d find those employees who earn more than all employees living in the city ? mumbai ? 6.9 describe how to translate join expressions in sql to relational algebra exercises 6.10 write the following queries in relational algebra  using the university schema a find the names of all studentswho have taken at least one comp sci course b find the ids and names of all students who have not taken any course offering before spring 2009 c for each department  find the maximum salary of instructors in that department you may assume that every department has at least one instructor d find the lowest  across all departments  of the per-department maximum salary computed by the preceding query 6.11 consider the relational database of figure 6.22  where the primary keys are underlined give an expression in the relational algebra to express each of the following queries  a find the names of all employees who work for ? first bank corporation ?  b find the names and cities of residence of all employeeswho work for ? first bank corporation ?  c find the names  street addresses  and cities of residence of all employees who work for ? first bank corporation ? and earn more than $ 10,000 d find the names of all employees in this database who live in the same city as the company for which they work e assume the companies may be located in several cities find all companies located in every city in which ? small bank corporation ? is located thesumit67.blogspot.com exercises 253 6.12 using the university example  write relational-algebra queries to find the course sections taught by more than one instructor in the following ways  a using an aggregate function b without using any aggregate functions 6.13 consider the relational database of figure 6.22 give a relational-algebra expression for each of the following queries  a find the company with the most employees b find the company with the smallest payroll c find those companies whose employees earn a higher salary  on average  than the average salary at first bank corporation 6.14 consider the following relational schema for a library  member  memb no  name  dob  books  isbn  title  authors  publisher  borrowed  memb no  isbn  date  write the following queries in relational algebra a find the names of members who have borrowed any book published by ? mcgraw-hill ?  b find the name of members who have borrowed all books published by ? mcgraw-hill ?  c find the name and membership number of members who have borrowed more than five different books published by ? mcgraw-hill ?  d for each publisher  find the name and membership number of members who have borrowed more than five books of that publisher e find the average number of books borrowed per member take into account that if an member does not borrow any books  then that member does not appear in the borrowed relation at all 6.15 consider the employee database of figure 6.22 give expressions in tuple relational calculus and domain relational calculus for each of the following queries  a find the names of all employees who work for ? first bank corporation ?  b find the names and cities of residence of all employeeswho work for ? first bank corporation ?  c find the names  street addresses  and cities of residence of all employees who work for ? first bank corporation ? and earn more than $ 10,000 thesumit67.blogspot.com 254 chapter 6 formal relational query languages d find all employees who live in the same city as that in which the company for which they work is located e find all employees who live in the same city and on the same street as their managers f find all employees in the database who do not work for ? first bank corporation ?  g find all employees who earn more than every employee of ? small bank corporation ?  h assume that the companies may be located in several cities find all companies located in every city in which ? small bank corporation ? is located 6.16 let r =  a  b  and s =  a  c   and let r  r  and s  s  be relations write relational-algebra expressions equivalent to the following domainrelational calculus expressions  a  < a > | ? b  < a  b > ? r ? b = 17   b  < a  b  c > | < a  b > ? r ? < a  c > ? s  c  < a > | ? b  < a  b > ? r  ? ? c  ? d  < d  c > ? s  ? < a  c > ? s   d  < a > | ? c  < a  c > ? s ? ? b1  b2  < a  b1 > ? r ? < c  b2 > ? r ? b1 > b2    6.17 repeat exercise 6.16  writing sql queries instead of relational-algebra expressions 6.18 let r =  a  b  and s =  a  c   and let r  r  and s  s  be relations using the special constant null  write tuple-relational-calculus expressions equivalent to each of the following  a r  s b r  s c r  s 6.19 give a tuple-relational-calculus expression to find the maximum value in relation r  a   bibliographical notes the original definition of relational algebra is in codd  1970   extensions to the relational model and discussions of incorporation of null values in the relational algebra  the rm/t model   as well as outer joins  are in codd  1979   codd  1990  is a compendium of e f codd ? s papers on the relational model outer joins are also discussed in date  1993b   thesumit67.blogspot.com bibliographical notes 255 the original definition of tuple relational calculus is in codd  1972   a formal proof of the equivalence of tuple relational calculus and relational algebra is in codd  1972   several extensions to the relational calculus have been proposed klug  1982  and escobar-molano et al  1993  describe extensions to scalar aggregate functions thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com part 2 database design database systems are designed to manage large bodies of information these large bodies of information do not exist in isolation they are part of the operation of some enterprise whose end product may be information from the database or may be some device or service for which the database plays only a supporting role the first two chapters of this part focus on the design of database schemas the entity-relationship  e-r  model described in chapter 7 is a high-level data model instead of representing all data in tables  it distinguishes between basic objects  called entities  and relationships among these objects it is often used as a first step in database-schema design relational database design ? the design of the relational schema ? was covered informally in earlier chapters there are  however  principles that can be used to distinguish good database designs from bad ones these are formalized by means of several ? normal forms ? that offer different trade-offs between the possibility of inconsistencies and the efficiency of certain queries chapter 8 describes the formal design of relational schemas the design of a complete database application environment that meets the needs of the enterprise being modeled requires attention to a broader set of issues  many of which are covered in chapter 9 this chapter first covers the design of web-based interfaces to applications the chapter then describes how large applications are architected using multiple layers of abstraction finally  the chapter provides a detailed discussion of security at the application and database levels 257 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter7 database design and the e-r model up to this point in the text  we have assumed a given database schema and studied how queries and updates are expressed.we now consider how to design a database schema in the first place in this chapter  we focus on the entityrelationship data model  e-r   which provides a means of identifying entities to be represented in the database and how those entities are related ultimately  the database design will be expressed in terms of a relational database design and an associated set of constraints we show in this chapter how an e-r design can be transformed into a set of relation schemas and how some of the constraints can be captured in that design then  in chapter 8  we consider in detail whether a set of relation schemas is a good or bad database design and study the process of creating good designs using a broader set of constraints these two chapters cover the fundamental concepts of database design 7.1 overview of the design process the task of creating a database application is a complex one  involving design of the database schema  design of the programs that access and update the data  and design of a security scheme to control access to data the needs of the users play a central role in the design process in this chapter  we focus on the design of the database schema  although we briefly outline some of the other design tasks later in the chapter the design of a complete database application environment that meets the needs of the enterprise being modeled requires attention to a broad set of issues these additional aspects of the expected use of the database influence a variety of design choices at the physical  logical  and view levels 7.1.1 design phases for small applications  it may be feasible for a database designer who understands the application requirements to decide directly on the relations to be created  259 thesumit67.blogspot.com 260 chapter 7 database design and the e-r model their attributes  and constraints on the relations however  such a direct design process is difficult for real-world applications  since they are often highly complex often no one person understands the complete data needs of an application the database designer must interact with users of the application to understand the needs of the application  represent them in a high-level fashion that can be understood by the users  and then translate the requirements into lower levels of the design a high-level data model serves the database designer by providing a conceptual framework in which to specify  in a systematic fashion  the data requirements of the database users  and a database structure that fulfills these requirements ? the initial phase of database design is to characterize fully the data needs of the prospective database users the database designer needs to interact extensively with domain experts and users to carry out this task the outcome of this phase is a specification of user requirements while there are techniques for diagrammatically representing user requirements  in this chapter we restrict ourselves to textual descriptions of user requirements ? next  the designer chooses a data model and  by applying the concepts of the chosen data model  translates these requirements into a conceptual schema of the database the schema developed at this conceptual-design phase provides a detailed overview of the enterprise the entity-relationship model  which we study in the rest of this chapter  is typically used to represent the conceptual design stated in terms of the entity-relationship model  the conceptual schema specifies the entities that are represented in the database  the attributes of the entities  the relationships among the entities  and constraints on the entities and relationships typically  the conceptual-design phase results in the creation of an entity-relationship diagram that provides a graphic representation of the schema the designer reviews the schema to confirm that all data requirements are indeed satisfied and are not in conflict with one another she can also examine the design to remove any redundant features her focus at this point is on describing the data and their relationships  rather than on specifying physical storage details ? a fully developed conceptual schema also indicates the functional requirements of the enterprise in a specification of functional requirements  users describe the kinds of operations  or transactions  that will be performed on the data example operations include modifying or updating data  searching for and retrieving specific data  and deleting data at this stage of conceptual design  the designer can review the schema to ensure it meets functional requirements ? the process of moving from an abstract data model to the implementation of the database proceeds in two final design phases ? in the logical-design phase  the designer maps the high-level conceptual schema onto the implementation data model of the database system that thesumit67.blogspot.com 7.1 overview of the design process 261 will be used the implementation data model is typically the relational data model  and this step typically consists of mapping the conceptual schemadefined using the entity-relationshipmodel into a relation schema ? finally  the designer uses the resulting system-specific database schema in the subsequent physical-design phase  in which the physical features of the database are specified these features include the form of file organization and choice of index structures  discussed in chapters 10 and 11 the physical schema of a database can be changed relatively easily after an application has been built however  changes to the logical schema are usually harder to carry out  since they may affect a number of queries and updates scattered across application code it is therefore important to carry out the database design phase with care  before building the rest of the database application 7.1.2 design alternatives a major part of the database design process is deciding how to represent in the design the various types of ? things ? such as people  places  products  and the like we use the term entity to refer to any such distinctly identifiable item in a university database  examples of entities would include instructors  students  departments  courses  and course offerings.1 the various entities are related to each other in a variety of ways  all of which need to be captured in the database design for example  a student takes a course offering,while an instructor teaches a course offering ; teaches and takes are examples of relationships between entities in designing a database schema  we must ensure that we avoid two major pitfalls  1 redundancy  a bad design may repeat information for example  if we store the course identifier and title of a course with each course offering  the title would be stored redundantly  that is  multiple times  unnecessarily  with each course offering it would suffice to store only the course identifier with each course offering  and to associate the titlewith the course identifier only once  in a course entity redundancy can also occur in a relational schema in the university example we have used so far  we have a relation with section information and a separate relation with course information suppose that instead we have a single relation where we repeat all of the course information  course id  title  dept name  credits  once for each section  offering  of the course clearly  information about courses would then be stored redundantly the biggest problem with such redundant representation of information is that the copies of a piece of information can become inconsistent if the 1a course may have run in multiple semesters  as well as multiple times in a semester we refer to each such offering of a course as a section thesumit67.blogspot.com 262 chapter 7 database design and the e-r model information is updated without taking precautions to update all copies of the information for example  different offerings of a course may have the same course identifier  but may have different titles it would then become unclear what the correct title of the course is ideally  information should appear in exactly one place 2 incompleteness  a bad design may make certain aspects of the enterprise difficult or impossible to model for example  suppose that  as in case  1  above  we only had entities corresponding to course offering  without having an entity corresponding to courses equivalently  in terms of relations  suppose we have a single relation where we repeat all of the course information once for each section that the course is offered it would then be impossible to represent information about a new course  unless that course is offered we might try to make do with the problematic design by storing null values for the section information such a work-around is not only unattractive  but may be prevented by primary-key constraints avoiding bad designs is not enough there may be a large number of good designs from which we must choose as a simple example  consider a customer whobuys a product is the sale of this product a relationship between the customer and the product ? alternatively  is the sale itself an entity that is related both to the customer and to the product ? this choice  though simple  may make an important difference in what aspects of the enterprise can bemodeledwell considering the need to make choices such as this for the large number of entities and relationships in a real-world enterprise  it is not hard to see that database design can be a challenging problem indeed we shall see that it requires a combination of both science and ? good taste ? 7.2 the entity-relationship model the entity-relationship  e-r  data model was developed to facilitate database design by allowing specification of an enterprise schema that represents the overall logical structure of a database the e-r model is very useful in mapping the meanings and interactions of real-world enterprises onto a conceptual schema because of this usefulness  many database-design tools draw on concepts from the e-r model the e-r data model employs three basic concepts  entity sets  relationship sets  and attributes  which we study first the e-r model also has an associated diagrammatic representation  the e-r diagram  which we study later in this chapter 7.2.1 entity sets an entity is a ? thing ? or ? object ? in the real world that is distinguishable from all other objects for example  each person in a university is an entity an entity has a set of properties  and the values for some set of properties may uniquely identify an entity for instance  a person may have a person id property whose thesumit67.blogspot.com 7.2 the entity-relationship model 263 value uniquely identifies that person thus  the value 677-89-9011 for person id would uniquely identify one particular person in the university similarly  courses can be thought of as entities  and course id uniquely identifies a course entity in the university an entity may be concrete  such as a person or a book  or it may be abstract  such as a course  a course offering  or a flight reservation an entity set is a set of entities of the same type that share the same properties  or attributes the set of all people who are instructors at a given university  for example  can be defined as the entity set instructor similarly  the entity set student might represent the set of all students in the university in the process of modeling  we often use the term entity set in the abstract  without referring to a particular set of individual entities we use the term extension of the entity set to refer to the actual collection of entities belonging to the entity set thus  the set of actual instructors in the university forms the extension of the entity set instructor the above distinction is similar to the difference between a relation and a relation instance  which we saw in chapter 2 entity sets do not need to be disjoint for example  it is possible to define the entity set of all people in a university  person   a person entity may be an instructor entity  a student entity  both  or neither an entity is represented by a set of attributes attributes are descriptive properties possessed by each member of an entity set the designation of an attribute for an entity set expresses that the database stores similar information concerning each entity in the entity set ; however  each entity may have its own value for each attribute possible attributes of the instructor entity set are id  name  dept name  and salary in real life  there would be further attributes  such as street number  apartment number  state  postal code  and country  but we omit them to keep our examples simple possible attributes of the course entity set are course id  title  dept name  and credits each entity has a value for each of its attributes for instance  a particular instructor entity may have the value 12121 for id  the valuewu for name  the value finance for dept name  and the value 90000 for salary the id attribute is used to identify instructors uniquely  since there may be more than one instructor with the same name in the united states  many enterprises find it convenient to use the social-security number of a person2 as an attribute whose value uniquely identifies the person in general the enterprise would have to create and assign a unique identifier for each instructor a database thus includes a collection of entity sets  each of which contains any number of entities of the same type figure 7.1 shows part of a university database that consists of two entity sets  instructor and student to keep the figure simple  only some of the attributes of the two entity sets are shown a database for a university may include a number of other entity sets for example  in addition to keeping track of instructors and students  the university also has information about courses  which are represented by the entity set course 2in the united states  the government assigns to each person in the country a unique number  called a social-security number  to identify that person uniquely each person is supposed to have only one social-security number  and no two people are supposed to have the same social-security number thesumit67.blogspot.com 264 chapter 7 database design and the e-r model instructor student 22222 einstein katz kim crick srinivasan singh 45565 98345 76766 10101 76543 12345 98988 76653 23121 00128 76543 shankar tanaka aoi chavez peltier zhang brown 44553 figure 7.1 entity sets instructor and student with attributes course id  title  dept name and credits in a real setting  a university database may keep dozens of entity sets 7.2.2 relationship sets a relationship is an association among several entities for example  we can define a relationship advisor that associates instructor katz with student shankar this relationship specifies that katz is an advisor to student shankar a relationship set is a set of relationships of the same type formally  it is a mathematical relation on n = 2  possibly nondistinct  entity sets if e1  e2      en are entity sets  then a relationship set r is a subset of   e1  e2      en  | e1 ? e1  e2 ? e2      en ? en  where  e1  e2      en  is a relationship consider the two entity sets instructor and student in figure 7.1 we define the relationship set advisor to denote the association between instructors and students figure 7.2 depicts this association as another example  consider the two entity sets student and section we can define the relationship set takes to denote the association between a student and the course sections in which that student is enrolled the association between entity sets is referred to as participation ; that is  the entity sets e1  e2      en participate in relationship set r a relationship instance in an e-r schema represents an association between the named entities in the real-world enterprise that is being modeled as an illustration  the individual instructor entity katz,who has instructor id 45565  and the student entity shankar  who has student id 12345  participate in a relationship instance of advisor this relationship instance represents that in the university  the instructor katz is advising student shankar the function that an entity plays in a relationship is called that entity ? s role since entity sets participating in a relationship set are generally distinct  roles thesumit67.blogspot.com 7.2 the entity-relationship model 265 instructor student 76766 crick katz srinivasan kim singh einstein 45565 10101 98345 76543 22222 98988 12345 00128 76543 76653 23121 44553 tanaka shankar zhang brown aoi chavez peltier figure 7.2 relationship set advisor are implicit and are not usually specified however  they are useful when the meaning of a relationship needs clarification such is the casewhen the entity sets of a relationship set are not distinct ; that is  the same entity set participates in a relationship set more than once  in different roles in this type of relationship set  sometimes called a recursive relationship set  explicit role names are necessary to specify howan entity participates in a relationship instance for example  consider the entity set course that records information about all the courses offered in the university to depict the situation where one course  c2  is a prerequisite for another course  c1  we have relationship set prereq that is modeled by ordered pairs of course entities the first course of a pair takes the role of coursec1,whereas the second takes the role of prerequisite course c2 in this way  all relationships of prereq are characterized by  c1  c2  pairs ;  c2  c1  pairs are excluded a relationship may also have attributes called descriptive attributes consider a relationship set advisor with entity sets instructor and student we could associate the attribute date with that relationship to specify the date when an instructor became the advisor of a student the advisor relationship among the entities corresponding to instructor katz and student shankar has the value ? 10 june 2007 ? for attribute date  which means that katz became shankar ? s advisor on 10 june 2007 figure 7.3 shows the relationship set advisor with a descriptive attribute date please note that katz advises two students with two different advising dates as a more realistic example of descriptive attributes for relationships  consider the entity sets student and section  which participate in a relationship set takes we may wish to store a descriptive attribute grade with the relationship to record the grade that a student got in the class.we may also store a descriptive attribute for credit to record whether a student has taken the course for credit  or is auditing  or sitting in on  the course a relationship instance in a given relationship set must be uniquely identifiable from its participating entities  without using the descriptive attributes to understand this point  suppose we want to model all the dateswhen an instructor thesumit67.blogspot.com 266 chapter 7 database design and the e-r model instructor student 76766 crick katz srinivasan kim singh einstein 45565 10101 98345 76543 22222 98988 12345 00128 76543 44553 tanaka shankar zhang brown aoi chavez peltier 3 may 2008 10 june 2007 12 june 2006 6 june 2009 30 june 2007 31 may 2007 4 may 2006 76653 23121 figure 7.3 date as attribute of the advisor relationship set became an advisor of a particular student the single-valued attribute date can store a single date only we can not represent multiple dates by multiple relationship instances between the same instructor and a student  since the relationship instances would not be uniquely identifiable using only the participating entities the right way to handle this case is to create a multivalued attribute date  which can store all the dates it is possible to have more than one relationship set involving the same entity sets in our example  the instructor and student entity sets participate in the relationship set advisor additionally  suppose each student must have another instructorwho serves as a department advisor  undergraduate or graduate   then the instructor and student entity sets may participate in another relationship set  dept advisor the relationship sets advisor and dept advisor provide examples of a binary relationship set ? that is  one that involves two entity sets most of the relationship sets in a database system are binary occasionally  however  relationship sets involve more than two entity sets as an example  suppose that we have an entity set project that represents all the research projects carried out in the university consider the entity sets instructor  student  and project each project can have multiple associated students and multiple associated instructors furthermore  each student working on a projectmust have an associated instructor who guides the student on the project for now,we ignore the first two relationships  between project and instructor  and between project and student instead  we focus on the information about which instructor is guiding which student on a particular project to represent this information  we relate the three entity sets through the relationship set proj guide  which indicates that a particular student is guided by a particular instructor on a particular project note that a student could have different instructors as guides for different projects,which can not be captured by a binary relationship between students and instructors thesumit67.blogspot.com 7.2 the entity-relationship model 267 the number of entity sets that participate in a relationship set is the degree of the relationship set a binary relationship set is of degree 2 ; a ternary relationship set is of degree 3 7.2.3 attributes for each attribute  there is a set of permitted values  called the domain  or value set  of that attribute the domain of attribute course id might be the set of all text strings of a certain length similarly  the domain of attribute semester might be strings from the set  fall,winter  spring  summer   formally  an attribute of an entity set is a function that maps from the entity set into a domain since an entity set may have several attributes  each entity can be described by a set of  attribute  data value  pairs  one pair for each attribute of the entity set for example  a particular instructor entity may be described by the set   id  76766    name  crick    dept name  biology    salary  72000    meaning that the entity describes a person named crick whose instructor id is 76766  who is a member of the biology department with salary of $ 72,000 we can see  at this point  an integration of the abstract schema with the actual enterprise being modeled the attribute values describing an entity constitute a significant portion of the data stored in the database an attribute  as used in the e-r model  can be characterized by the following attribute types ? simple and composite attributes in our examples thus far  the attributes have been simple ; that is  they have not been divided into subparts composite attributes  on the other hand  can be divided into subparts  that is  other attributes   for example  an attribute name could be structured as a composite attribute consisting of first name  middle initial  andlast name usingcomposite attributes in a design schema is a good choice if a user will wish to refer to an entire attribute on some occasions  and to only a component of the attribute on other occasions suppose we were to to add an address to the student entity-set the address can be defined as the composite attribute address with the attributes street  city  state  and zip code.3 composite attributes help us to group together related attributes  making the modeling cleaner note also that a composite attribute may appear as a hierarchy in the composite attribute address  its component attribute street can be further divided into street number  street name  and apartment number figure 7.4 depicts these examples of composite attributes for the instructor entity set ? single-valued and multivalued attributes the attributes in our examples all have a single value for a particular entity for instance  the student id attribute for a specific student entity refers to only one student id such attributes are said to be single valued there may be instances where an attribute has a set of values for a specific entity suppose we add to the instructor entity set 3we assume the address format used in the united states  which includes a numeric postal code called a zip code thesumit67.blogspot.com 268 chapter 7 database design and the e-r model name address first_name middle_initial last_name street city state postal_code street_number street_name apartment_number composite attributes component attributes figure 7.4 composite attributes instructor name and address a phone number attribute an instructor may have zero  one  or several phone numbers  and different instructors may have different numbers of phones this type of attribute is said to be multivalued as another example  we could add to the instructor entity set an attribute dependent name listing all the dependents this attribute would be multivalued  since any particular instructor may have zero  one  or more dependents to denote that an attribute is multivalued,we enclose it in braces  for example  phone number  or  dependent name   where appropriate  upper and lower bounds may be placed on the number of values in a multivalued attribute for example  a university may limit the number of phone numbers recorded for a single instructor to two placing bounds in this case expresses that the phone number attribute of the instructor entity set may have between zero and two values ? derived attribute the value for this type of attribute can be derived from the values of other related attributes or entities for instance  let us say that the instructor entity set has an attribute students advised  which represents how many students an instructor advises.we can derive the value for this attribute by counting the number of student entities associated with that instructor as another example  suppose that the instructor entity set has an attribute age that indicates the instructor ? s age if the instructor entity set also has an attribute date of birth  we can calculate age from date of birth and the current date thus  age is a derived attribute in this case  date of birth may be referred to as a base attribute  or a stored attribute the value of a derived attribute is not stored but is computed when required an attribute takes a null value when an entity does not have a value for it the null value may indicate ? not applicable ? ? that is  that the value does not exist for the entity for example  one may have no middle name null can also designate that an attribute value is unknown an unknown value may be either missing  the value does exist  but we do not have that information  or not known  we do not know whether or not the value actually exists   for instance  if the name value for a particular instructor is null  we assume that the value is missing  since every instructor must have a name a null value for the apartment number attribute could mean that the address does not include thesumit67.blogspot.com 7.3 constraints 269 an apartment number  not applicable   that an apartment number exists but we do not know what it is  missing   or that we do not know whether or not an apartment number is part of the instructor ? s address  unknown   7.3 constraints an e-r enterprise schema may define certain constraints to which the contents of a database must conform in this section  we examine mapping cardinalities and participation constraints 7.3.1 mapping cardinalities mapping cardinalities  or cardinality ratios  express the number of entities to which another entity can be associated via a relationship set mapping cardinalities are most useful in describing binary relationship sets  although they can contribute to the description of relationship sets that involve more than two entity sets in this section  we shall concentrate on only binary relationship sets for a binary relationship set r between entity sets a and b  the mapping cardinality must be one of the following  ? one-to-one an entity in a is associated with at most one entity in b  and an entity in b is associated with at most one entity in a  see figure 7.5a  ? one-to-many an entity in a is associated with any number  zero or more  of entities in b an entity in b  however  can be associated with at most one entity in a  see figure 7.5b   a   b  a1 a2 a3 a4 b1 b2 b3 a2 a1 a3 b1 b2 b3 b4 b5 a b a b figure 7.5 mapping cardinalities  a  one-to-one  b  one-to-many thesumit67.blogspot.com 270 chapter 7 database design and the e-r model aa 2 a3 a5 a1 a2 a4 a2 a1 a3 a4 b1 b2 b3 a b a b b1 b2 b3 b4  a   b  figure 7.6 mapping cardinalities  a  many-to-one  b  many-to-many ? many-to-one an entity in a is associated with at most one entity in b an entity in b  however  can be associated with any number  zero or more  of entities in a  see figure 7.6a  ? many-to-many an entity in a is associated with any number  zero or more  of entities in b  and an entity in b is associated with any number  zero or more  of entities in a  see figure 7.6b  the appropriate mapping cardinality for a particular relationship set obviously depends on the real-world situation that the relationship set is modeling as an illustration  consider the advisor relationship set if  in a particular university  a student can be advised by only one instructor  and an instructor can advise several students  then the relationship set from instructor to student is one-to-many if a student can be advised by several instructors  as in the case of students advised jointly   the relationship set is many-to-many 7.3.2 participation constraints the participation of an entity set e in a relationship set r is said to be total if every entity in e participates in at least one relationship in r if only some entities in e participate in relationships in r  the participation of entity set e in relationship r is said to be partial in figure 7.5a  the participation of b in the relationship set is total while the participation of ain the relationship set is partial in figure 7.5b  the participation of both aand b in the relationship set are total for example  we expect every student entity to be related to at least one instructor through the advisor relationship therefore the participation of student in the relationship set advisor is total in contrast  an instructor need not advise any students hence  it is possible that only some of the instructor entities are related to the student entity set through the advisor relationship  and the participation of instructor in the advisor relationship set is therefore partial thesumit67.blogspot.com 7.3 constraints 271 7.3.3 keys we must have a way to specify how entities within a given entity set are distinguished conceptually  individual entities are distinct ; from a database perspective  however  the differences among them must be expressed in terms of their attributes therefore  the values of the attribute values of an entity must be such that they can uniquely identify the entity in other words  no two entities in an entity set are allowed to have exactly the same value for all attributes the notion of a key for a relation schema  as defined in section 2.3  applies directly to entity sets that is  a key for an entity is a set of attributes that suffice to distinguish entities from each other the concepts of superkey  candidate key  and primary key are applicable to entity sets just as they are applicable to relation schemas keys also help to identify relationships uniquely  and thus distinguish relationships from each other below  we define the corresponding notions of keys for relationships the primary key of an entity set allows us to distinguish among the various entities of the set.we need a similarmechanism to distinguish among the various relationships of a relationship set let r be a relationship set involving entity sets e1  e2      en let primarykey  ei  denote the set of attributes that forms the primary key for entity set ei  assume for now that the attribute names of all primary keys are unique the composition of the primary key for a relationship set depends on the set of attributes associated with the relationship set r if the relationship set r has no attributes associated with it  then the set of attributes primary-key  e1  ? primary-key  e2  ? ? ? ? ? primary-key  en  describes an individual relationship in set r if the relationship set r has attributes a1  a2      am associated with it  then the set of attributes primary-key  e1  ? primary-key  e2  ? ? ? ? ? primary-key  en  ?  a1  a2      am  describes an individual relationship in set r in both of the above cases  the set of attributes primary-key  e1  ? primary-key  e2  ? ? ? ? ? primary-key  en  forms a superkey for the relationship set if the attribute names of primary keys are not unique across entity sets  the attributes are renamed to distinguish them ; the name of the entity set combined with the name of the attribute would form a unique name if an entity set participates more than once in a relationship set  as in the prereq relationship in thesumit67.blogspot.com 272 chapter 7 database design and the e-r model section 7.2.2   the role name is used instead of the name of the entity set  to form a unique attribute name the structure of the primary key for the relationship set depends on the mapping cardinality of the relationship set as an illustration  consider the entity sets instructor and student  and the relationship set advisor  with attribute date  in section 7.2.2 suppose that the relationship set is many-to-many then the primary key of advisor consists of the union of the primary keys of instructor and student if the relationship is many-to-one from student to instructor ? that is  each student can have have at most one advisor ? then the primary key of advisor is simply the primary key of student however  if an instructor can advise only one student ? that is  if the advisor relationship is many-to-one from instructor to student ? then the primary key of advisor is simply the primary key of instructor for one-to-one relationships either candidate key can be used as the primary key for nonbinary relationships  if no cardinality constraints are present then the superkey formed as described earlier in this section is the only candidate key  and it is chosen as the primary key the choice of the primary key is more complicated if cardinality constraints are present since we have not discussed how to specify cardinality constraints on nonbinary relations,we do not discuss this issue further in this chapter.we consider the issue inmore detail later  in sections 7.5.5 and 8.4 7.4 removing redundant attributes in entity sets when we design a database using the e-r model  we usually start by identifying those entity sets that should be included for example  in the university organization we have discussed thus far  we decided to include such entity sets as student  instructor  etc once the entity sets are decided upon  we must choose the appropriate attributes these attributes are supposed to represent the various values we want to capture in the database in the university organization  we decided that for the instructor entity set  we will include the attributes id  name  dept name  and salary we could have added the attributes  phone number  office number  home page  etc the choice of what attributes to include is up to the designer  who has a good understanding of the structure of the enterprise once the entities and their corresponding attributes are chosen  the relationship sets among the various entities are formed these relationship sets may result in a situation where attributes in the various entity sets are redundant and need to be removed from the original entity sets to illustrate  consider the entity sets instructor and department  ? the entity set instructor includes the attributes id  name  dept name  and salary  with id forming the primary key ? the entity set department includes the attributes dept name  building  and budget  with dept name forming the primary key we model the fact that each instructor has an associated department using a relationship set inst dept relating instructor and department thesumit67.blogspot.com 7.4 removing redundant attributes in entity sets 273 the attribute dept name appears in both entity sets since it is the primary key for the entity set department  it is redundant in the entity set instructor and needs to be removed removing the attribute dept name from the instructor entity set may appear rather unintuitive  since the relation instructor that we used in the earlier chapters had an attribute dept name as we shall see later  when we create a relational schema from the e-r diagram  the attribute dept name in fact gets added to the relation instructor  but only if each instructor has at most one associated department if an instructor has more than one associated department  the relationship between instructors and departments is recorded in a separate relation inst dept treating the connection between instructors and departments uniformly as a relationship  rather than as an attribute of instructor  makes the logical relationship explicit  and helps avoid a premature assumption that each instructor is associated with only one department similarly  the student entity set is related to the department entity set through the relationship set student dept and thus there is no need for a dept name attribute in student as another example  consider course offerings  sections  along with the time slots of the offerings each time slot is identified by a time slot id  and has associated with it a set of weekly meetings  each identified by a day of the week  start time  and end time.we decide tomodel the set ofweeklymeeting times as amultivalued composite attribute suppose we model entity sets section and time slot as follows  ? the entity set section includes the attributes course id  sec id  semester  year  building  room number  and time slot id  with  course id  sec id  year  semester  forming the primary key ? the entity set time slot includes the attributes time slot id,which is the primary key,4 and a multivalued composite attribute   day  start time  end time   .5 these entities are related through the relationship set sec time slot the attribute time slot id appears in both entity sets since it is the primary key for the entity set time slot  it is redundant in the entity set section and needs to be removed as a final example  suppose we have an entity set classroom  with attributes building  room number  and capacity  with building and room number forming the primary key suppose also that we have a relationship set sec class that relates section to classroom then the attributes  building  room number  are redundant in the entity set section a good entity-relationship design does not contain redundant attributes for our university example  we list the entity sets and their attributes below  with primary keys underlined  4we shall see later on that the primary key for the relation created from the entity set time slot includes day and start time ; however  day and start time do not form part of the primary key of the entity set time slot 5we could optionally give a name  such as meeting  for the composite attribute containing day  start time  and end time thesumit67.blogspot.com 274 chapter 7 database design and the e-r model ? classroom  with attributes  building  room number  capacity   ? department  with attributes  dept name  building  budget   ? course  with attributes  course id  title  credits   ? instructor  with attributes  id  name  salary   ? section  with attributes  course id  sec id  semester  year   ? student  with attributes  id  name  tot cred   ? time slot  with attributes  time slot id    day  start time  end time     the relationship sets in our design are listed below  ? inst dept  relating instructors with departments ? stud dept  relating students with departments ? teaches  relating instructors with sections ? takes  relating students with sections  with a descriptive attribute grade ? course dept  relating courses with departments ? sec course  relating sections with courses ? sec class  relating sections with classrooms ? sec time slot  relating sections with time slots ? advisor  relating students with instructors ? prereq  relating courses with prerequisite courses you can verify that none of the entity sets has any attribute that is made redundant by one of the relationship sets further  you can verify that all the information  other than constraints  in the relational schema for our university database  which we saw earlier in figure 2.8 in chapter 2  has been captured by the above design  but with several attributes in the relational design replaced by relationships in the e-r design 7.5 entity-relationship diagrams as we saw briefly in section 1.3.3  an e-r diagram can express the overall logical structure of a database graphically e-r diagrams are simple and clear ? qualities that may well account in large part for the widespread use of the e-r model 7.5.1 basic structure an e-r diagram consists of the following major components  thesumit67.blogspot.com 7.5 entity-relationship diagrams 275 instructor id name salary student id name tot_cred advisor figure 7.7 e-r diagram corresponding to instructors and students ? rectangles divided into two parts represent entity sets the first part  which in this textbook is shaded blue  contains the name of the entity set the second part contains the names of all the attributes of the entity set ? diamonds represent relationship sets ? undivided rectangles represent the attributes of a relationship set.attributes that are part of the primary key are underlined ? lines link entity sets to relationship sets ? dashed lines link attributes of a relationship set to the relationship set ? double lines indicate total participation of an entity in a relationship set ? double diamonds represent identifying relationship sets linked to weak entity sets  we discuss identifying relationship sets and weak entity sets later  in section 7.5.6   consider the e-r diagram in figure 7.7  which consists of two entity sets  instructor and student related through a binary relationship set advisor.the attributes associated with instructor are id  name  and salary the attributes associated with student are id  name  and tot cred in figure 7.7  attributes of an entity set that are members of the primary key are underlined if a relationship set has some attributes associated with it  thenwe enclose the attributes in a rectangle and link the rectangle with a dashed line to the diamond representing that relationship set for example  in figure 7.8  we have the date descriptive attribute attached to the relationship set advisor to specify the date on which an instructor became the advisor id name salary id name tot_cred date instructor student advisor figure 7.8 e-r diagram with an attribute attached to a relationship set thesumit67.blogspot.com 276 chapter 7 database design and the e-r model instructor student id name salary instructor id name salary instructor id name salary id name tot_cred student id name tot_cred student id name tot_cred  a   b   c  advisor advisor advisor figure 7.9 relationships  a  one-to-one  b  one-to-many  c  many-to-many 7.5.2 mapping cardinality the relationship set advisor  between the instructor and student entity sets may be one-to-one  one-to-many  many-to-one  or many-to-many to distinguish among these types,we draw either a directed line  ?  or an undirected line  ?  between the relationship set and the entity set in question  as follows  ? one-to-one  we draw a directed line from the relationship set advisor to both entity sets instructor and student  see figure 7.9a   this indicates that an instructor may advise at most one student  and a student may have at most one advisor ? one-to-many  we draw a directed line from the relationship set advisor to the entity set instructor and an undirected line to the entity set student  see figure 7.9b   this indicates that an instructor may advise many students  but a studentmay have at most one advisor ? many-to-one  we draw an undirected line from the relationship set advisor to the entity set instructor and a directed line to the entity set student this indicates that an instructor may advise at most one student  but a student may have many advisors ? many-to-many  we draw an undirected line from the relationship set advisor to both entity sets instructor and student  see figure 7.9c   this indicates that thesumit67.blogspot.com 7.5 entity-relationship diagrams 277 an instructor may advise many students  and a student may have many advisors e-r diagrams also provide a way to indicate more complex constraints on the number of times each entity participates in relationships in a relationship set a line may have an associated minimum and maximum cardinality  shown in the form l..h  where l is the minimum and h the maximum cardinality a minimum value of 1 indicates total participation of the entity set in the relationship set ; that is  each entity in the entity set occurs in at least one relationship in that relationship set a maximum value of 1 indicates that the entity participates in at most one relationship  while a maximum value * indicates no limit for example  consider figure 7.10 the line between advisor and student has a cardinality constraint of 1..1  meaning the minimum and the maximum cardinality are both 1 that is  each student must have exactly one advisor the limit 0 * on the line between advisor and instructor indicates that an instructor can have zero or more students thus  the relationship advisor is one-to-many from instructor to student  and further the participation of student in advisor is total  implying that a student must have an advisor it is easy to misinterpret the 0 * on the left edge and think that the relationship advisor is many-to-one frominstructor to student ? this is exactly the reverse of the correct interpretation if both edges have a maximum value of 1  the relationship is one-to-one if we had specified a cardinality limit of 1 * on the left edge  we would be saying that each instructor must advise at least one student the e-r diagram in figure 7.10 could alternatively have been drawn with a double line from student to advisor  and an arrow on the line from advisor to instructor  in place of the cardinality constraints shown this alternative diagram would enforce exactly the same constraints as the constraints shown in the figure 7.5.3 complex attributes figure 7.11 shows howcomposite attributes can be represented in the e-r notation here  a composite attribute name  with component attributes first name  middle initial  and last name replaces the simple attribute name of instructor as another example  suppose we were to add an address to the instructor entity-set the address can be defined as the composite attribute address with the attributes instructor id name salary student id name tot_cred 0 * advisor 1..1 figure 7.10 cardinality limits on relationship sets thesumit67.blogspot.com 278 chapter 7 database design and the e-r model instructor id name first_name middle_initial last_name address street street_number street_name apt_number city state zip  phone_number  date_of_birth age   figure 7.11 e-r diagram with composite  multivalued  and derived attributes street  city  state  and zip code the attribute street is itself a composite attribute whose component attributes are street number  street name  and apartment number figure 7.11 also illustrates a multivalued attribute phone number  denoted by ?  phone number  ?  and a derived attribute age  depicted by a ? age   ?  7.5.4 roles we indicate roles in e-r diagrams by labeling the lines that connect diamonds to rectangles figure 7.12 shows the role indicators course id and prereq id between the course entity set and the prereq relationship set 7.5.5 nonbinary relationship sets nonbinary relationship sets can be specified easily in an e-r diagram figure 7.13 consists of the three entity sets instructor  student  and project  related through the relationship set proj guide course course_id title credits course_id prereq_id prereq figure 7.12 e-r diagram with role indicators thesumit67.blogspot.com 7.5 entity-relationship diagrams 279 instructor id name salary student id name tot_cred    project proj_guide figure 7.13 e-r diagram with a ternary relationship we can specify some types of many-to-one relationships in the case of nonbinary relationship sets suppose a student can have at most one instructor as a guide on a project this constraint can be specified by an arrow pointing to instructor on the edge from proj guide we permit at most one arrow out of a relationship set  since an e-r diagram with two or more arrows out of a nonbinary relationship set can be interpreted in twoways suppose there is a relationship set r between entity sets a1  a2      an  and the only arrows are on the edges to entity sets ai + 1  ai + 2      an then  the two possible interpretations are  1 a particular combination of entities from a1  a2      ai can be associated with at most one combination of entities from ai + 1  ai + 2      an thus  the primary key for the relationship r can be constructed by the union of the primary keys of a1  a2      ai  2 for each entity set ak  i < k = n  each combination of the entities from the other entity sets can be associated with at most one entity from ak each set  a1  a2      ak-1  ak + 1      an   for i < k = n  then forms a candidate key each of these interpretations has been used in different books and systems to avoid confusion  we permit only one arrow out of a relationship set  in which case the two interpretations are equivalent in chapter 8  section 8.4   we study functional dependencies  which allow either of these interpretations to be specified in an unambiguous manner 7.5.6 weak entity sets consider a section entity  which is uniquely identified by a course identifier  semester  year  and section identifier clearly  section entities are related to course entities suppose we create a relationship set sec course between entity sets section and course now  observe that the information in sec course is redundant  since section already has an attribute course id  which identifies the course with which the section is related one option to deal with this redundancy is to get rid of the thesumit67.blogspot.com 280 chapter 7 database design and the e-r model relationship sec course ; however  by doing so the relationship between section and course becomes implicit in an attribute  which is not desirable an alternative way to deal with this redundancy is to not store the attribute course id in the section entity and to only store the remaining attributes sec id  year  and semester.6 however  the entity set section then does not have enough attributes to identify a particular section entity uniquely ; although each section entity is distinct  sections for different courses may share the same sec id  year  and semester to deal with this problem  we treat the relationship sec course as a special relationship that provides extra information  in this case the course id  required to identify section entities uniquely the notion of weak entity set formalizes the above intuition an entity set that does not have sufficient attributes to form a primary key is termed a weak entity set an entity set that has a primary key is termed a strong entity set for a weak entity set to be meaningful  it must be associated with another entity set  called the identifying or owner entity set every weak entity must be associated with an identifying entity ; that is  the weak entity set is said to be existence dependent on the identifying entity set the identifying entity set is said to own the weak entity set that it identifies the relationship associating the weak entity set with the identifying entity set is called the identifying relationship the identifying relationship is many-to-one from the weak entity set to the identifying entity set  and the participation of the weak entity set in the relationship is total the identifying relationship set should not have any descriptive attributes  since any such attributes can instead be associated with theweak entity set in our example  the identifying entity set for section is course  and the relationship sec course  which associates section entities with their corresponding course entities  is the identifying relationship although a weak entity set does not have a primary key  we nevertheless need a means of distinguishing among all those entities in the weak entity set that depend on one particular strong entity the discriminator of a weak entity set is a set of attributes that allows this distinction to be made for example  the discriminator of the weak entity set section consists of the attributes sec id  year  and semester  since  for each course  this set of attributes uniquely identifies one single section for that course the discriminator of a weak entity set is also called the partial key of the entity set the primary key of a weak entity set is formed by the primary key of the identifying entity set  plus the weak entity set ? s discriminator in the case of the entity set section  its primary key is  course id  sec id  year  semester   where course id is the primary key of the identifying entity set  namely course  and  sec id  year  semester  distinguishes section entities for the same course note that we could have chosen to make sec id globally unique across all courses offered in the university  in which case the section entity set would have 6note that the relational schema we eventually create from the entity set section does have the attribute course id  for reasons that will become clear later  even though we have dropped the attribute course id from the entity set section thesumit67.blogspot.com 7.5 entity-relationship diagrams 281 course course_id title credits section sec_id semester year sec_course figure 7.14 e-r diagram with a weak entity set had a primary key however  conceptually  a section is still dependent on a course for its existence  which is made explicit by making it a weak entity set in e-r diagrams  a weak entity set is depicted via a rectangle  like a strong entity set  but there are two main differences  ? the discriminator of a weak entity is underlined with a dashed  rather than a solid  line ? the relationship set connecting the weak entity set to the identifying strong entity set is depicted by a double diamond in figure 7.14  the weak entity set section depends on the strong entity set course via the relationship set sec course the figure also illustrates the use of double lines to indicate total participation ; the participation of the  weak  entity set section in the relationship sec course is total  meaning that every section must be related via sec course to some course finally  the arrow from sec course to course indicates that each section is related to a single course a weak entity set can participate in relationships other than the identifying relationship for instance  the section entity could participate in a relationship with the time slot entity set  identifying the time when a particular class section meets a weak entity set may participate as owner in an identifying relationship with anotherweak entity set it is also possible to have a weak entity setwith more than one identifying entity set a particular weak entity would then be identified by a combination of entities  one from each identifying entity set the primary key of the weak entity set would consist of the union of the primary keys of the identifying entity sets  plus the discriminator of the weak entity set in some cases  the database designer may choose to express a weak entity set as a multivalued composite attribute of the owner entity set in our example  this alternativewould require that the entity set course have amultivalued  composite attribute section a weak entity set may be more appropriately modeled as an attribute if it participates in only the identifying relationship  and if it has few attributes conversely  a weak entity set representation more aptly models a situation where the set participates in relationships other than the identifying relationship  and where the weak entity set has several attributes it is clear that section violates the requirements for being modeled as a multivalued composite attribute  and is modeled more aptly as a weak entity set thesumit67.blogspot.com 282 chapter 7 database design and the e-r model 7.5.7 e-r diagram for the university enterprise in figure 7.15  we show an e-r diagram that corresponds to the university enterprise that we have been using thus far in the text this e-r diagram is equivalent to the textual description of the university e-r model that we saw in section 7.4  but with several additional constraints  and section now being a weak entity in ouruniversity database,we have a constraint that each instructor must have exactly one associated department.as a result  there is a double line in figure 7.15 between instructor and inst dept  indicating total participation of instructor in inst dept ; that is  each instructor must be associated with a department further  there is an arrow from inst dept to department  indicating that each instructor can have at most one associated department course time_slot student id name salary id name tot_cred course_id title credits time_slot_id  day start_time end_time  course_id prereq_id advisor teaches takes sec_course sec_time_slot grade prereq inst_dept stud_dept instructor department dept_name building budget section sec_id semester year course_dept sec_class classroom building room_number capacity figure 7.15 e-r diagram for a university enterprise thesumit67.blogspot.com 7.6 reduction to relational schemas 283 similarly  entity sets course and student have double lines to relationship sets course dept and stud dept respectively  as also entity set section to relationship set sec time slot the first two relationships  in turn  have an arrow pointing to the other relationship  department  while the third relationship has an arrow pointing to time slot further  figure 7.15 shows that the relationship set takes has a descriptive attribute grade  and that each student has at most one advisor the figure also shows that section is now a weak entity set  with attributes sec id  semester  and year forming the discriminator ; sec course is the identifying relationship set relating weak entity set section to the strong entity set course in section 7.6  we shall show how this e-r diagram can be used to derive the various relation schemas we use 7.6 reduction to relational schemas we can represent a database that conforms to an e-r database schema by a collection of relation schemas for each entity set and for each relationship set in the database design  there is a unique relation schema to which we assign the name of the corresponding entity set or relationship set both the e-r model and the relational database model are abstract  logical representations of real-world enterprises because the two models employ similar design principles  we can convert an e-r design into a relational design in this section  we describe how an e-r schema can be represented by relation schemas and how constraints arising from the e-r design can be mapped to constraints on relation schemas 7.6.1 representation of strong entity sets with simple attributes let e be a strong entity set with only simple descriptive attributes a1  a2      an we represent this entity by a schema called e with n distinct attributes each tuple in a relation on this schema corresponds to one entity of the entity set e for schemas derived from strong entity sets  the primary key of the entity set serves as the primary key of the resulting schema this follows directly from the fact that each tuple corresponds to a specific entity in the entity set as an illustration  consider the entity set student of the e-r diagram in figure 7.15 this entity set has three attributes  id  name  tot cred we represent this entity set by a schema called student with three attributes  student  id  name  tot cred  note that since student id is the primary key of the entity set  it is also the primary key of the relation schema continuing with our example  for the e-r diagram in figure 7.15  all the strong entity sets  except time slot  have only simple attributes the schemas derived from these strong entity sets are  thesumit67.blogspot.com 284 chapter 7 database design and the e-r model classroom  building  room number  capacity  department  dept name  building  budget  course  course id  title  credits  instructor  id  name  salary  student  id  name  tot cred  as you can see  both the instructor and student schemas are different from the schemas we have used in the previous chapters  they do not contain the attribute dept name  .we shall revisit this issue shortly 7.6.2 representation of strong entity sets with complex attributes when a strong entity set has nonsimple attributes  things are a bit more complex we handle composite attributes by creating a separate attribute for each of the component attributes ; we do not create a separate attribute for the composite attribute itself to illustrate  consider the version of the instructor entity set depicted in figure 7.11 for the composite attribute name  the schema generated for instructor contains the attributes first name  middle name  and last name ; there is no separate attribute or schema for name similarly  for the composite attribute address  the schema generated contains the attributes street  city  state  and zip code since street is a composite attribute it is replaced by street number  street name  and apt number.we revisit this matter in section 8.2 multivalued attributes are treated differently from other attributes we have seen that attributes in an e-r diagram generally map directly into attributes for the appropriate relation schemas multivalued attributes  however  are an exception ; new relation schemas are created for these attributes  as we shall see shortly derived attributes are not explicitly represented in the relational data model however  they can be represented as ? methods ? in other data models such as the object-relational data model  which is described later in chapter 22 the relational schema derived from the version of entity set instructor with complex attributes  without including the multivalued attribute  is thus  instructor  id  first name  middle name  last name  street number  street name  apt number  city  state  zip code  date of birth  for a multivalued attributem  we create a relationschema r with an attribute a that corresponds to m and attributes corresponding to the primary key of the entity set or relationship set of which m is an attribute as an illustration  consider the e-r diagram in figure 7.11 that depicts the entity set instructor  which includes the multivalued attribute phone number the primary key of instructor is id for this multivalued attribute  we create a relation schema instructor phone  id  phone number  thesumit67.blogspot.com 7.6 reduction to relational schemas 285 each phone number of an instructor is represented as a unique tuple in the relation on this schema thus  if we had an instructor with id 22222  and phone numbers 555-1234 and 555-4321  the relation instructor phone would have two tuples  22222  555-1234  and  22222  555-4321   we create a primary key of the relation schema consisting of all attributes of the schema in the above example  the primary key consists of both attributes of the relation instructor phone in addition  we create a foreign-key constraint on the relation schema created from the multivalued attribute  with the attribute generated from the primary key of the entity set referencing the relation generated from the entity set in the above example  the foreign-key constraint on the instructor phone relation would be that attribute id references the instructor relation in the case that an entity set consists of only two attributes ? a single primarykey attribute b and a single multivalued attributem ? the relation schema for the entity set would contain only one attribute  namely the primary-key attribute b we can drop this relation  while retaining the relation schema with the attribute b and attribute a that corresponds to m to illustrate  consider the entity set time slot depicted in figure 7.15 here  time slot id is the primary key of the time slot entity set and there is a single multivalued attribute that happens also to be composite the entity set can be represented by just the following schema created from the multivalued composite attribute  time slot  time slot id  day  start time  end time  although not represented as a constraint on the e-r diagram  we know that there can not be two meetings of a class that start at the same time of the same day-ofthe week but end at different times ; based on this constraint  end time has been omitted from the primary key of the time slot schema the relation created from the entity set would have only a single attribute time slot id ; the optimization of dropping this relation has the benefit of simplifying the resultant database schema  although it has a drawback related to foreign keys  which we briefly discuss in section 7.6.4 7.6.3 representation of weak entity sets let a be a weak entity set with attributes a1  a2      am let b be the strong entity set on which a depends let the primary key of b consist of attributes b1  b2      bn we represent the entity set a by a relation schema called a with one attribute for each member of the set   a1  a2      am  ?  b1  b2      bn  for schemas derived from a weak entity set  the combination of the primary key of the strong entity set and the discriminator of the weak entity set serves as the primary key of the schema in addition to creating a primary key  we also create a foreign-key constraint on the relation a  specifying that the thesumit67.blogspot.com 286 chapter 7 database design and the e-r model attributes b1  b2      bn reference the primary key of the relation b the foreignkey constraint ensures that for each tuple representing a weak entity  there is a corresponding tuple representing the corresponding strong entity as an illustration  consider the weak entity set section in the e-r diagram of figure 7.15 this entity set has the attributes  sec id  semester  and year the primary key of the course entity set  on which section depends  is course id thus  we represent section by a schema with the following attributes  section  course id  sec id  semester  year  the primary key consists of the primary key of the entity set course  along with the discriminator of section  which is sec id  semester  and year we also create a foreign-key constraint on the section schema  with the attribute course id referencing the primary key of the course schema  and the integrity constraint ? on delete cascade ? .7 because of the ? on delete cascade ? specification on the foreign key constraint  if a course entity is deleted  then so are all the associated section entities 7.6.4 representation of relationship sets let r be a relationship set  let a1  a2      am be the set of attributes formed by the union of the primary keys of each of the entity sets participating in r  and let the descriptive attributes  if any  of r be b1  b2      bn.we represent this relationship set by a relation schema called r with one attribute for each member of the set   a1  a2      am  ?  b1  b2      bn  we described earlier  in section 7.3.3  how to choose a primary key for a binary relationship set as we saw in that section  taking all the primary-key attributes from all the related entity sets serves to identify a particular tuple  but for one-toone  many-to-one  and one-to-many relationship sets  this turns out to be a larger set of attributes than we need in the primary key the primary key is instead chosen as follows  ? for a binary many-to-many relationship  the union of the primary-key attributes from the participating entity sets becomes the primary key ? for a binary one-to-one relationship set  the primary key of either entity set can be chosen as the primary key the choice can be made arbitrarily ? for a binary many-to-one or one-to-many relationship set  the primary key of the entity set on the ? many ? side of the relationship set serves as the primary key 7the ? on delete cascade ? feature of foreign key constraints in sql is described in section 4.4.5 thesumit67.blogspot.com 7.6 reduction to relational schemas 287 ? for an n-ary relationship setwithout any arrows on its edges  the union of the primary key-attributes fromthe participating entity sets becomes the primary key ? for an n-ary relationship set with an arrow on one of its edges  the primary keys of the entity sets not on the ? arrow ? side of the relationship set serve as the primary key for the schema recall that we allowed only one arrow out of a relationship set we also create foreign-key constraints on the relation schema r as follows  for each entity set ei related to relationship set r  we create a foreign-key constraint from relation schema r  with the attributes of r that were derived from primary-key attributes of ei referencing the primary key of the relation schema representing ei  as an illustration  consider the relationship set advisor in the e-r diagram of figure 7.15 this relationship set involves the following two entity sets  ? instructor with the primary key id ? student with the primary key id since the relationship set has no attributes  the advisor schema has two attributes  the primary keys of instructor and student since both attributes have the same name  we rename them i id and s id since the advisor relationship set is manyto one from student to instructor the primary key for the advisor relation schema is s id we also create two foreign-key constraints on the advisor relation  with attribute i id referencing the primary key of instructor and attribute s id referencing the primary key of student continuing with our example  for the e-r diagram in figure 7.15  the schemas derived from a relationship set are depicted in figure 7.16 observe that for the case of the relationship set prereq  the role indicators associatedwith the relationship are used as attribute names  since both roles refer to the same relation course similar to the case of advisor  the primary key for each of the relations sec course  sec time slot  sec class  inst dept  stud dept and course dept consists of the primary key of only one of the two related entity sets  since each of the corresponding relationships is many-to-one foreign keys are not shown in figure 7.16  but for each of the relations in the figure there are two foreign-key constraints  referencing the two relations created from the two related entity sets thus  for example  sec course has foreign keys referencing section and classroom  teaches has foreign keys referencing instructor and section  and takes has foreign keys referencing student and section the optimization that allowed us to create only a single relation schema from the entity set time slot  which had a multivalued attribute  prevents the creation of a foreign key from the relation schema sec time slot to the relation created from entity set time slot  since we dropped the relation created from the entity set time thesumit67.blogspot.com 288 chapter 7 database design and the e-r model teaches  id  course id  sec id  semester  year  takes  id  course id  sec id  semester  year  grade  prereq  course id  prereq id  advisor  s id  i id  sec course  course id  sec id  semester  year  sec time slot  course id  sec id  semester  year  time slot id  sec class  course id  sec id  semester  year  building  room number  inst dept  id  dept name  stud dept  id  dept name  course dept  course id  dept name  figure 7.16 schemas derived from relationship sets in the e-r diagram in figure 7.15 slot.we retained the relation created from the multivalued attribute  and named it time slot  but this relation may potentially have no tuples corresponding to a time slot id  or may have multiple tuples corresponding to a time slot id ; thus  time slot id in sec time slot can not reference this relation the astute readermaywonderwhy we have not seen the schemas sec course  sec time slot  sec class  inst dept  stud dept  and course dept in the previous chapters the reason is that the algorithm we have presented thus far results in some schemas that can be either eliminated or combined with other schemas we explore this issue next 7.6.4.1 redundancy of schemas a relationship set linking a weak entity set to the corresponding strong entity set is treated specially as we noted in section 7.5.6  these relationships are many-toone and have no descriptive attributes furthermore  the primary key of a weak entity set includes the primary key of the strong entity set in the e-r diagram of figure 7.14  the weak entity set section is dependent on the strong entity set course via the relationship set sec course the primary key of section is  course id  sec id  semester  year  and the primary key of course is course id since sec course has no descriptive attributes  the sec course schema has attributes course id  sec id  semester  and year the schema for the entity set section includes the attributes course id  sec id  semester  and year  among others   every  course id  sec id  semester  year  combination in a sec course relation would also be present in the relation on schema section  and vice versa thus  the sec course schema is redundant in general  the schema for the relationship set linking a weak entity set to its corresponding strong entity set is redundant and does not need to be present in a relational database design based upon an e-r diagram 7.6.4.2 combination of schemas consider a many-to-one relationship set ab from entity set a to entity set b using our relational-schema construction algorithm outlined previously  we get thesumit67.blogspot.com 7.6 reduction to relational schemas 289 three schemas  a  b  and ab suppose further that the participation of a in the relationship is total ; that is  every entity a in the entity set b must participate in the relationship ab then we can combine the schemas a and ab to form a single schema consisting of the union of attributes of both schemas the primary key of the combined schema is the primary key of the entity set into whose schema the relationship set schema was merged to illustrate  let ? s examine the various relations in the e-r diagram of figure 7.15 that satisfy the above criteria  ? inst dept the schemas instructor and department correspond to the entity sets a and b  respectively thus  the schema inst dept can be combined with the instructor schema the resulting instructor schema consists of the attributes  id  name  dept name  salary   ? stud dept the schemas student and department correspond to the entity sets a and b  respectively thus  the schema stud dept can be combined with the student schema the resulting student schema consists of the attributes  id  name  dept name  tot cred   ? course dept the schemas course and department correspond to the entity sets a and b  respectively thus  the schema course dept can be combinedwith the course schema the resulting course schema consists of the attributes  course id  title  dept name  credits   ? sec class the schemas section and classroom correspond to the entity sets a and b  respectively thus  the schema sec class can be combined with the section schema the resulting section schema consists of the attributes  course id  sec id  semester  year  building  room number   ? sec time slot the schemas section and time slot correspond to the entity sets a and b respectively  thus  the schema sec time slot can be combined with the section schema obtained in the previous step the resulting section schema consists of the attributes  course id  sec id  semester  year  building  room number  time slot id   in the case of one-to-one relationships  the relation schema for the relationship set can be combined with the schemas for either of the entity sets we can combine schemas even if the participation is partial by using null values in the above example  if inst dept were partial  then we would store null values for the dept name attribute for those instructors who have no associated department finally  we consider the foreign-key constraints that would have appeared in the schema representing the relationship set there would have been foreign-key constraints referencing each of the entity sets participating in the relationship set we drop the constraint referencing the entity set into whose schema the relationship set schema is merged  and add the other foreign-key constraints to the combined schema for example  inst dept has a foreign key constraint of the attribute dept name referencing the department relation this foreign constraint is thesumit67.blogspot.com 290 chapter 7 database design and the e-r model added to the instructor relation when the schema for inst dept is merged into instructor 7.7 entity-relationship design issues the notions of an entity set and a relationship set are not precise  and it is possible to define a set of entities and the relationships among them in a number of different ways in this section  we examine basic issues in the design of an e-r database schema section 7.10 covers the design process in further detail 7.7.1 use of entity sets versus attributes consider the entity set instructor with the additional attribute phone number  figure 7.17a  it can easily be argued that a phone is an entity in its own right with attributes phone number and location ; the location may be the office or homewhere the phone is located  with mobile  cell  phones perhaps represented by the value ? mobile ? if we take this point of view  we do not add the attribute phone number to the instructor rather  we create  ? a phone entity set with attributes phone number and location ? a relationship set inst phone  denoting the association between instructors and the phones that they have this alternative is shown in figure 7.17b what  then  is themain difference between these two definitions of an instructor ? treating a phone as an attribute phone number implies that instructors have precisely one phone number each treating a phone as an entity phone permits instructors to have several phone numbers  including zero  associatedwith them however  we could instead easily define phone number as a multivalued attribute to allow multiple phones per instructor the main difference then is that treating a phone as an entity better models a situation where one may want to keep extra information about a phone  such as its location  or its type  mobile  ip phone  or plain old phone   or all who share instructor id name salary phone phone_number location instructor id name salary phone_number  a   b  inst_phone figure 7.17 alternatives for adding phone to the instructor entity set thesumit67.blogspot.com 7.7 entity-relationship design issues 291 the phone thus  treating phone as an entity is more general than treating it as an attribute and is appropriate when the generality may be useful in contrast  it would not be appropriate to treat the attribute name  of an instructor  as an entity ; it is difficult to argue that name is an entity in its own right  in contrast to the phone   thus  it is appropriate to have name as an attribute of the instructor entity set two natural questions thus arise  what constitutes an attribute  and what constitutes an entity set ? unfortunately  there are no simple answers the distinctionsmainly depend on the structure of the real-world enterprise beingmodeled  and on the semantics associated with the attribute in question a common mistake is to use the primary key of an entity set as an attribute of another entity set  instead of using a relationship for example  it is incorrect to model the id of a student as an attribute of an instructor even if each instructor advises only one student the relationship advisor is the correct way to represent the connection between students and instructors  since it makes their connection explicit  rather than implicit via an attribute another related mistake that people sometimes make is to designate the primary-key attributes of the related entity sets as attributes of the relationship set for example  id  the primary-key attributes of student  and id  the primary key of instructor  should not appear as attributes of the relationship advisor this should not be done since the primary-key attributes are already implicit in the relationship set.8 7.7.2 use of entity sets versus relationship sets it is not always clear whether an object is best expressed by an entity set or a relationship set in figure 7.15  we used the takes relationship set to model the situationwhere a student takes a  section of a  course.an alternative is to imagine that there is a course-registration record for each course that each student takes then  we have an entity set to represent the course-registration record let us call that entity set registration each registration entity is related to exactly one student and to exactly one section  so we have two relationship sets  one to relate courseregistration records to students and one to relate course-registration records to sections in figure 7.18,weshow the entity sets section and student fromfigure 7.15 with the takes relationship set replaced by one entity set and two relationship sets  ? registration  the entity set representing course-registration records ? section reg  the relationship set relating registration and course ? student reg  the relationship set relating registration and student note that we use double lines to indicate total participation by registration entities 8when we create a relation schema fromthe e-r schema  the attributes may appear in a schema created fromthe advisor relationship set  as we shall see later ; however  they should not appear in the advisor relationship set thesumit67.blogspot.com 292 chapter 7 database design and the e-r model registration    section sec_id semester year student id name tot_cred section_reg student_reg figure 7.18 replacement of takes by registration and two relationship sets both the approach of figure 7.15 and that of figure 7.18 accurately represent the university ? s information  but the use of takes is more compact and probably preferable however  if the registrar ? s office associates other information with a course-registration record  it might be best to make it an entity in its own right one possible guideline in determining whether to use an entity set or a relationship set is to designate a relationship set to describe an action that occurs between entities this approach can also be useful in deciding whether certain attributes may be more appropriately expressed as relationships 7.7.3 binary versus n-ary relationship sets relationships in databases are often binary some relationships that appear to be nonbinary could actually be better represented by several binary relationships for instance  one could create a ternary relationship parent  relating a child to his/hermother and father.however  such a relationship could also be represented by two binary relationships  mother and father  relating a child to his/her mother and father separately using the two relationships mother and father provides us a record of a child ? s mother  even if we are not aware of the father ? s identity ; a null value would be required if the ternary relationship parent is used using binary relationship sets is preferable in this case in fact  it is always possible to replace a nonbinary  n-ary  for n > 2  relationship set by a number of distinct binary relationship sets for simplicity  consider the abstract ternary  n = 3  relationship set r  relating entity sets a  b  and c we replace the relationship set r by an entity set e  and create three relationship sets as shown in figure 7.19  ? ra  relating e and a ? rb  relating e and b ? rc  relating e and c thesumit67.blogspot.com 7.7 entity-relationship design issues 293 b r c a b e c a ra rb rc  a   b  figure 7.19 ternary relationship versus three binary relationships if the relationship set r had any attributes  these are assigned to entity set e ; further  a special identifying attribute is created for e  since it must be possible to distinguish different entities in an entity set on the basis of their attribute values   for each relationship  ai  bi  ci  in the relationship set r  we create a new entity ei in the entity set e then  in each of the three new relationship sets  we insert a relationship as follows  ?  ei  ai  in ra ?  ei  bi  in rb ?  ei  ci  in rc we can generalize this process in a straightforward manner to n-ary relationship sets thus  conceptually  we can restrict the e-r model to include only binary relationship sets however  this restriction is not always desirable ? an identifying attribute may have to be created for the entity set created to represent the relationship set this attribute  alongwith the extra relationship sets required  increases the complexity of the design and  as we shall see in section 7.6  overall storage requirements ? an n-ary relationship set shows more clearly that several entities participate in a single relationship ? there may not be a way to translate constraints on the ternary relationship into constraints on the binary relationships for example  consider a constraint that says that r is many-to-one from a  b to c ; that is  each pair of entities from aand b is associated with at most one c entity this constraint can not be expressed by using cardinality constraints on the relationship sets ra  rb  and rc consider the relationship set proj guide in section 7.2.2  relating instructor  student  and project we can not directly split proj guide into binary relationships between instructor and project and between instructor and student if we did so  thesumit67.blogspot.com 294 chapter 7 database design and the e-r model instructor student 76766 crick katz srinivasan kim singh einstein 45565 10101 98345 76543 22222 98988 12345 00128 76543 76653 23121 44553 tanaka shankar zhang brown aoi chavez peltier may 2009 june 2007 june 2006 june 2009 june 2007 may 2007 may 2006 figure 7.20 date as an attribute of the student entity set we would be able to record that instructor katz works on projects a and b with students shankar and zhang ; however  we would not be able to record that katz works on project a with student shankar and works on project b with student zhang  but does not work on project a with zhang or on project b with shankar the relationship set proj guide can be split into binary relationships by creating a new entity set as described above.however  doing sowould not be very natural 7.7.4 placement of relationship attributes the cardinality ratio of a relationship can affect the placement of relationship attributes thus  attributes of one-to-one or one-to-many relationship sets can be associated with one of the participating entity sets  rather than with the relationship set for instance  let us specify that advisor is a one-to-many relationship set such that one instructor may advise several students  but each student can be advised by only a single instructor in this case  the attribute date  which specifies when the instructor became the advisor of a student  could be associated with the student entity set  as figure 7.20 depicts  to keep the figure simple  only some of the attributes of the two entity sets are shown  since each student entity participates in a relationship with at most one instance of instructor  making this attribute designation has the same meaning aswould placing date with the advisor relationship set attributes of a one-to-many relationship set can be repositioned to only the entity set on the ? many ? side of the relationship for one-to-one relationship sets  on the other hand  the relationship attribute can be associated with either one of the participating entities the design decision of where to place descriptive attributes in such cases ? as a relationship or entity attribute ? should reflect the characteristics of the enterprise being modeled the designer may choose to retain date as an attribute of advisor to express explicitly that the date refers to the advising relationship and not some other aspect of the student ? s university status  for example  date of acceptance to the university   thesumit67.blogspot.com 7.8 extended e-r features 295 the choice of attribute placement is more clear-cut for many-to-many relationship sets returning to our example  let us specify the perhaps more realistic case that advisor is a many-to-many relationship set expressing that an instructor may advise one or more students  and that a student may be advised by one or more instructors if we are to express the date on which a specific instructor became the advisor of a specific student  date must be an attribute of the advisor relationship set  rather than either one of the participating entities if date were an attribute of student  for instance,we could not determine which instructor became the advisor on that particular date.when an attribute is determined by the combination of participating entity sets  rather than by either entity separately  that attribute must be associated with the many-to-many relationship set figure 7.3 depicts the placement of date as a relationship attribute ; again  to keep the figure simple  only some of the attributes of the two entity sets are shown 7.8 extended e-r features although the basic e-r concepts can model most database features  some aspects of a database may be more aptly expressed by certain extensions to the basic e-r model in this section  we discuss the extended e-r features of specialization  generalization  higher and lower-level entity sets  attribute inheritance  and aggregation to help with the discussions  we shall use a slightly more elaborate database schema for the university in particular  we shall model the various people within a university by defining an entity set person  with attributes id  name  and address 7.8.1 specialization an entity set may include subgroupings of entities that are distinct in some way from other entities in the set for instance  a subset of entities within an entity set may have attributes that are not shared by all the entities in the entity set the e-r model provides a means for representing these distinctive entity groupings as an example  the entity set person may be further classified as one of the following  ? employee ? student each of these person types is described by a set of attributes that includes all the attributes of entity set person plus possibly additional attributes for example  employee entities may be described further by the attribute salary  whereas student entities may be described further by the attribute tot cred the process of designating subgroupings within an entity set is called specialization the specialization of person allows us to distinguish among person entities according to whether they correspond to employees or students  in general  a person could be an employee  a student  both  or neither thesumit67.blogspot.com 296 chapter 7 database design and the e-r model as another example  suppose the university divides students into two categories  graduate and undergraduate graduate students have an office assigned to them undergraduate students are assigned to a residential college each of these student types is described by a set of attributes that includes all the attributes of the entity set student plus additional attributes theuniversitycouldcreate twospecializationsof student  namelygraduate and undergraduate as we saw earlier  student entities are described by the attributes id  name  address  and tot cred the entity set graduate would have all the attributes of student and an additional attribute office number the entity set undergraduate would have all the attributes of student  and an additional attribute residential college we can apply specialization repeatedly to refine a design for instance  university employees may be further classified as one of the following  ? instructor ? secretary each of these employee types is described by a set of attributes that includes all the attributes of entity set employee plus additional attributes for example  instructor entities may be described further by the attribute rank while secretary entities are described by the attribute hours per week further  secretary entities may participate in a relationship secretary for between the secretary and employee entity sets  which identifies the employees who are assisted by a secretary an entity set may be specialized by more than one distinguishing feature in our example  the distinguishing feature among employee entities is the job the employee performs another  coexistent  specialization could be based on whether the person is a temporary  limited term  employee or a permanent employee  resulting in the entity sets temporary employee and permanent employee when more than one specialization is formed on an entity set  a particular entity may belong to multiple specializations for instance  a given employee may be a temporary employee who is a secretary in terms of an e-r diagram  specialization is depicted by a hollow arrow-head pointing from the specialized entity to the other entity  see figure 7.21  .we refer to this relationship as the isa relationship  which stands for ? is a ? and represents  for example  that an instructor ? is a ? employee the way we depict specialization in an e-r diagram depends on whether an entity may belong to multiple specialized entity sets or if it must belong to at most one specialized entity set the former case  multiple sets permitted  is called overlapping specialization,while the latter case  at most one permitted  is called disjoint specialization for an overlapping specialization  as is the case for student and employee as specializations of person   two separate arrows are used for a disjoint specialization  as is the case for instructor and secretary as specializations of employee   a single arrow is used the specialization relationship may also be referred to as a superclass-subclass relationship higher and lower-level entity thesumit67.blogspot.com 7.8 extended e-r features 297 person id name address student instructor rank secretary hours_per_week employee salary tot_credits figure 7.21 specialization and generalization sets are depicted as regular entity sets ? that is  as rectangles containing the name of the entity set 7.8.2 generalization the refinement from an initial entity set into successive levels of entity subgroupings represents a top-down design process inwhich distinctions are made explicit the design process may also proceed in a bottom-up manner  in which multiple entity sets are synthesized into a higher-level entity set on the basis of common features the database designer may have first identified  ? instructor entity set with attributes instructor id  instructor name  instructor salary  and rank ? secretary entity set with attributes secretary id  secretary name  secretary salary  and hours per week there are similarities between the instructor entity set and the secretary entity set in the sense that they have several attributes that are conceptually the same across the two entity sets  namely  the identifier  name  and salary attributes this commonality can be expressed by generalization  which is a containment relationship that exists between a higher-level entity set and one ormore lower-level entity sets in our example  employee is the higher-level entity set and instructor and secretary are lower-level entity sets in this case  attributes that are conceptually the same had different names in the two lower-level entity sets to create a generalization  the attributes must be given a common name and represented with the higher-level entity person we can use the attribute names id  name  address  as we saw in the example in section 7.8.1 thesumit67.blogspot.com 298 chapter 7 database design and the e-r model higher and lower-level entity sets also may be designated by the terms superclass and subclass  respectively the person entity set is the superclass of the employee and student subclasses for all practical purposes  generalization is a simple inversion of specialization we apply both processes  in combination  in the course of designing the e-r schema for an enterprise in terms of the e-r diagram itself  we do not distinguish between specialization and generalization new levels of entity representation are distinguished  specialization  or synthesized  generalization  as the design schema comes to express fully the database application and the user requirements of the database differences in the two approaches may be characterized by their starting point and overall goal specialization stems from a single entity set ; it emphasizes differences among entities within the set by creating distinct lower-level entity sets these lowerlevel entity sets may have attributes  or may participate in relationships  that do not apply to all the entities in the higher-level entity set indeed  the reason a designer applies specialization is to represent such distinctive features if student and employee have exactly the same attributes as person entities  and participate in exactly the same relationships as person entities  there would be no need to specialize the person entity set generalization proceeds from the recognition that a number of entity sets share some common features  namely  they are described by the same attributes and participate in the same relationship sets   on the basis of their commonalities  generalization synthesizes these entity sets into a single  higher-level entity set generalization is used to emphasize the similarities among lower-level entity sets and to hide the differences ; it also permits an economy of representation in that shared attributes are not repeated 7.8.3 attribute inheritance acrucial property of the higher and lower-level entities created by specialization and generalization is attribute inheritance the attributes of the higher-level entity sets are said to be inherited by the lower-level entity sets for example  student and employee inherit the attributes of person thus  student is described by its id  name  and address attributes  and additionally a tot cred attribute ; employee is described by its id  name  andaddress attributes  and additionally a salary attribute attribute inheritance applies through all tiers of lower-level entity sets ; thus  instructor and secretary  which are subclasses of employee  inherit the attributes id  name  and address from person  in addition to inheriting salary from employee alower-level entity set  or subclass  also inherits participation in the relationship sets inwhich its higher-level entity  or superclass  participates like attribute inheritance  participation inheritance applies through all tiers of lower-level entity sets for example  suppose the person entity set participates in a relationship person dept with department then  the student  employee  instructor and secretary entity sets  which are subclasses of the person entity set  also implicitly participate in the person dept relationship with department the above entity sets can participate in any relationships in which the person entity set participates thesumit67.blogspot.com 7.8 extended e-r features 299 whether a given portion of an e-r model was arrived at by specialization or generalization  the outcome is basically the same  ? a higher-level entity set with attributes and relationships that apply to all of its lower-level entity sets ? lower-level entity sets with distinctive features that apply only within a particular lower-level entity set in what follows  althoughwe often refer to only generalization  the properties that we discuss belong fully to both processes figure 7.21 depicts a hierarchy of entity sets in the figure  employee is a lower-level entity set of person and a higher-level entity set of the instructor and secretary entity sets in a hierarchy  a given entity set may be involved as a lowerlevel entity set in only one isa relationship ; that is  entity sets in this diagram have only single inheritance if an entity set is a lower-level entity set in more than one isa relationship  then the entity set has multiple inheritance  and the resulting structure is said to be a lattice 7.8.4 constraints on generalizations to model an enterprise more accurately  the database designer may choose to place certain constraints on a particular generalization one type of constraint involves determiningwhich entities can bemembers of a given lower-level entity set such membership may be one of the following  ? condition-defined in condition-defined lower-level entity sets,membership is evaluated on the basis ofwhether or not an entity satisfies an explicit condition or predicate for example  assume that the higher-level entity set student has the attribute student type all student entities are evaluated on the defining student type attribute only those entities that satisfy the condition student type = ? graduate ? are allowed to belong to the lower-level entity set graduate student all entities that satisfy the condition student type = ? undergraduate ? are included in undergraduate student since all the lower-level entities are evaluated on the basis of the same attribute  in this case  on student type   this type of generalization is said to be attribute-defined ? user-defined user-defined lower-level entity sets are not constrained by a membership condition ; rather  the database user assigns entities to a given entity set for instance  let us assume that  after 3 months of employment  university employees are assigned to one of four work teams we therefore represent the teams as four lower-level entity sets of the higher-level employee entity set a given employee is not assigned to a specific team entity automatically on the basis of an explicit defining condition instead  the user in charge of this decision makes the team assignment on an individual basis the assignment is implemented by an operation that adds an entity to an entity set thesumit67.blogspot.com 300 chapter 7 database design and the e-r model a second type of constraint relates to whether or not entities may belong to more than one lower-level entity set within a single generalization the lowerlevel entity sets may be one of the following  ? disjoint a disjointness constraint requires that an entity belong to no more than one lower-level entity set in our example  student entity can satisfy only one condition for the student type attribute ; an entity can be either a graduate student or an undergraduate student  but can not be both ? overlapping in overlapping generalizations  the same entity may belong to more than one lower-level entity set within a single generalization for an illustration  consider the employee work-team example  and assume that certain employees participate inmore than onework team.agiven employee may therefore appear in more than one of the team entity sets that are lowerlevel entity sets of employee thus  the generalization is overlapping in figure 7.21  we assume a person may be both an employee and a student we show this overlapping generalization via separate arrows  one from employee to person and another from student to person however  the generalization of instructor and secretaries is disjoint.we show this using a single arrow afinal constraint  the completeness constraint on a generalization or specialization  specifieswhether or not an entity in the higher-level entity setmust belong to at least one of the lower-level entity setswithin the generalization/specialization this constraint may be one of the following  ? total generalization or specialization each higher-level entity must belong to a lower-level entity set ? partial generalization or specialization some higher-level entities may not belong to any lower-level entity set partial generalization is the default.we can specify total generalization in an e-r diagram by adding the keyword ? total ? in the diagram and drawing a dashed line from the keyword to the corresponding hollow arrow-head to which it applies  for a total generalization   or to the set of hollow arrow-heads to which it applies  for an overlapping generalization   the student generalization is total  all student entities must be either graduate or undergraduate because the higher-level entity set arrived at through generalization is generally composed of only those entities in the lower-level entity sets  the completeness constraint for a generalized higher-level entity set is usually total when the generalization is partial  a higher-level entity is not constrained to appear in a lower-level entity set the work team entity sets illustrate a partial specialization since employees are assigned to a team only after 3 months on the job  some employee entities may not be members of any of the lower-level team entity sets we may characterize the team entity sets more fully as a partial  overlapping specialization of employee the generalization of graduate student and undergradthesumit67 blogspot.com 7.8 extended e-r features 301 uate student into student is a total  disjoint generalization the completeness and disjointness constraints  however  do not depend on each other constraint patterns may also be partial-disjoint and total-overlapping we can see that certain insertion and deletion requirements follow from the constraints that apply to a given generalization or specialization for instance  when a total completeness constraint is in place  an entity inserted into a higherlevel entity set must also be inserted into at least one of the lower-level entity sets with a condition-defined constraint  all higher-level entities that satisfy the condition must be inserted into that lower-level entity set finally  an entity that is deleted from a higher-level entity set also is deleted from all the associated lower-level entity sets to which it belongs 7.8.5 aggregation one limitation of the e-r model is that it can not express relationships among relationships to illustrate the need for such a construct  consider the ternary relationship proj guide  which we saw earlier  between an instructor  student and project  see figure 7.13   now suppose that each instructor guiding a student on a project is required to file a monthly evaluation report.we model the evaluation report as an entity evaluation  with a primary key evaluation id one alternative for recording the  student  project  instructor  combination to which an evaluation corresponds is to create a quaternary  4-way  relationship set eval for between instructor  student  project  and evaluation  a quaternary relationship is required ? a binary relationship between student and evaluation  for example  would not permit us to represent the  project  instructor  combination to which an evaluation corresponds  using the basic e-r modeling constructs  we obtain the e-r diagram of figure 7.22  we have omitted the attributes of the entity sets  for simplicity  it appears that the relationship sets proj guide and eval for can be combined into one single relationship set nevertheless  we should not combine them into a single relationship  since some instructor  student  project combinations may not have an associated evaluation there is redundant information in the resultant figure  however  since every instructor  student  project combination in eval for must also be in proj guide if the evaluation were a value rather than a entity  we could instead make evaluation a multivalued composite attribute of the relationship set proj guide however  this alternative may not be be an option if an evaluation may also be related to other entities ; for example  each evaluation report may be associated with a secretary who is responsible for further processing of the evaluation report to make scholarship payments the best way to model a situation such as the one just described is to use aggregation aggregation is an abstraction through which relationships are treated as higher-level entities thus  for our example  we regard the relationship set proj guide  relating the entity sets instructor  student  and project  as a higher-level entity set called proj guide such an entity set is treated in the same manner as is any other entity set.we can then create a binary relationship eval for between proj thesumit67.blogspot.com 302 chapter 7 database design and the e-r model project evaluation instructor student eval_ for proj_ guide figure 7.22 e-r diagram with redundant relationships guide and evaluation to represent which  student  project  instructor  combination an evaluation is for figure 7.23 shows a notation for aggregation commonly used to represent this situation 7.8.6 reduction to relation schemas we are in a position now to describe how the extended e-r features can be translated into relation schemas 7.8.6.1 representation of generalization there are two different methods of designing relation schemas for an e-r diagram that includes generalization althoughwerefer to the generalization in figure 7.21 in this discussion  we simplify it by including only the first tier of lower-level entity sets ? that is  employee and student we assume that id is the primary key of person 1 create a schema for the higher-level entity set for each lower-level entity set  create a schema that includes an attribute for each of the attributes of that entity set plus one for each attribute of the primary key of the higher-level entity set thus  for the e-r diagram of figure 7.21  ignoring the instructor and secretary entity sets  we have three schemas  person  id  name  street  city  employee  id  salary  student  id  tot cred  thesumit67.blogspot.com 7.8 extended e-r features 303 evaluation proj_ guide instructor student eval_ for project figure 7.23 e-r diagram with aggregation the primary-key attributes of the higher-level entity set become primarykey attributes of the higher-level entity set as well as all lower-level entity sets these can be seen underlined in the above example in addition  we create foreign-key constraints on the lower-level entity sets  with their primary-key attributes referencing the primary key of the relation created from the higher-level entity set in the above example  the id attribute of employee would reference the primary key of person  and similarly for student 2 an alternative representation is possible  if the generalization is disjoint and complete ? that is  if no entity is a member of two lower-level entity sets directly below a higher-level entity set  and if every entity in the higher-level entity set is also a member of one of the lower-level entity sets here  we do not create a schema for the higher-level entity set instead  for each lowerlevel entity set  we create a schema that includes an attribute for each of the attributes of that entity set plus one for each attribute of the higher-level entity set then  for the e-r diagram of figure 7.21  we have two schemas  employee  id  name  street  city  salary  student  id  name  street  city  tot cred  both these schemas have id,which is the primary-key attribute of the higherlevel entity set person  as their primary key thesumit67.blogspot.com 304 chapter 7 database design and the e-r model one drawback of the second method lies in defining foreign-key constraints to illustrate the problem  suppose we had a relationship set r involving entity set person with the first method  when we create a relation schema r from the relationship set  we would also define a foreign-key constraint on r  referencing the schema person.unfortunately,with the secondmethod,we do not have a single relation to which a foreign-key constraint on r can refer to avoid this problem  we need to create a relation schema person containing at least the primary-key attributes of the person entity if the second method were used for an overlapping generalization  some values would be stored multiple times  unnecessarily for instance  if a person is both an employee and a student  values for street and city would be stored twice if the generalization were disjoint but not complete ? that is  if some person is neither an employee nor a student ? then an extra schema person  id  name  street  city  would be required to represent such people however  the problem with foreignkey constraints mentioned above would remain as an attempt to work around the problem  suppose employees and students are additionally represented in the person relation unfortunately  name  street  and city information would then be stored redundantly in the person relation and the student relation for students  and similarly in the person relation and the employee relation for employees that suggests storing name  street  and city information only in the person relation and removing that information from student and employee if we do that  the result is exactly the first method we presented 7.8.6.2 representation of aggregation designing schemas for an e-r diagram containing aggregation is straightforward consider the diagram of figure 7.23 the schema for the relationship set eval for between the aggregation of proj guide and the entity set evaluation includes an attribute for each attribute in the primary keys of the entity set evaluation  and the relationship set proj guide it also includes an attribute for any descriptive attributes  if they exist  of the relationship set eval for we then transform the relationship sets and entity sets within the aggregated entity set following the rules we have already defined the rules we saw earlier for creating primary-key and foreign-key constraints on relationship sets can be applied to relationship sets involving aggregations as well  with the aggregation treated like any other entity set the primary key of the aggregation is the primary key of its defining relationship set no separate relation is required to represent the aggregation ; the relation created from the defining relationship is used instead 7.9 alternative notations for modeling data a diagrammatic representation of the data model of an application is a very important part of designing a database schema creation of a database schema thesumit67.blogspot.com 7.9 alternative notations formodeling data 305 requires not only data modeling experts  but also domain experts who know the requirements of the application but may not be familiar with data modeling an intuitive diagrammatic representation is particularly important since it eases communication of information between these groups of experts a number of alternative notations for modeling data have been proposed  of which e-r diagrams and uml class diagrams are the most widely used there is no universal standard for e-r diagram notation  and different books and e-r diagram software use different notations we have chosen a particular notation e r r r r r rolename r e a1 a2 a2.1 a2.2  a3  a4 e r l..h e e1 e2 e3 e1 e2 e3 e1 e2 e3 entity set relationship set identifying relationship set for weak entity set primary key many-to-many relationship many-to-one relationship one-to-one relationship cardinality limits discriminating a  ribute of weak entity set total participation of entity set in relationship a  ributes  simple  a1   composite  a2  and multivalued  a3  derived  a4  isa  generalization or specialization disjoint generalization total  disjoint  generalization role indicator total a1 e a1 e r e   figure 7.24 symbols used in the e-r notation thesumit67.blogspot.com 306 chapter 7 database design and the e-r model in this sixth edition of this book which actually differs from the notation we used in earlier editions  for reasons that we explain later in this section in the rest of this section  we study some of the alternative e-r diagram notations  as well as the uml class diagram notation to aid in comparison of our notation with these alternatives  figure 7.24 summarizes the set of symbols we have used in our e-r diagram notation 7.9.1 alternative e-r notations figure 7.25 indicates some of the alternative e-r notations that are widely used one alternative representation of attributes of entities is to show them in ovals connected to the box representing the entity ; primary key attributes are indicated by underlining them the above notation is shown at the top of the figure relationship attributes can be similarly represented  by connecting the ovals to the diamond representing the relationship participation in r  total  e1  and partial  e2  e1 r e2 e1 e2 r r entity set e with simple a  ribute a1  composite a  ribute a2  multivalued a  ribute a3  derived a  ribute a4  and primary key a1 many-to-many relationship one-to-one relationship many-to-one relationship r r * * * 1 1 1 r e1 e1 e1 e2 e2 e2 e1 e2 generalization isa isa total weak entity set generalization a1 a2 a3 a2.1 a2.2 e a4 e1 r e2 e1 r e2 figure 7.25 alternative e-r notations thesumit67.blogspot.com 7.9 alternative notations formodeling data 307 cardinality constraints on relationships can be indicated in several different ways  as shown in figure 7.25 in one alternative  shown on the left side of the figure  labels * and 1 on the edges out of the relationship are used for depicting many-to-many  one-to-one  and many-to-one relationships the case of one-tomany is symmetric to many-to-one  and is not shown in another alternative notation shown on the right side of the figure  relationship sets are represented by lines between entity sets  without diamonds ; only binary relationships can be modeled thus cardinality constraints in such a notation are shown by ? crow ? s-foot ? notation  as in the figure in a relationship r between e1 and e2  crow ? s feet on both sides indicates a many-to-many relationship  while crow ? s feet on just the e1 side indicates a many-to-one relationship from e1 to e2 total participation is specified in this notation by a vertical bar note however  that in a relationship r between entities e1 and e2  if the participation of e1 in r is total  the vertical bar is placed on the opposite side  adjacent to entity e2 similarly  partial participation is indicated by using a circle  again on the opposite side the bottom part of figure 7.25 shows an alternative representation of generalization  using triangles instead of hollow arrow-heads in prior editions of this text up to the fifth edition  we used ovals to represent attributes  with triangles representing generalization  as shown in figure 7.25 the notation using ovals for attributes and diamonds for relationships is close to the original form of e-r diagrams used by chen in his paper that introduced the notion of e-r modeling that notation is now referred to as chen ? s notation the u.s national institute for standards and technology defined a standard called idef1x in 1993 idef1x uses the crow ? s-foot notation  with vertical bars on the relationship edge to denote total participation and hollow circles to denote partial participation  and includes other notations that we have not shown with the growth in the use of unified markup language  uml   described later in section 7.9.2  we have chosen to update our e-r notation to make it closer to the form of uml class diagrams ; the connections will become clear in section 7.9.2 in comparison with our previous notation  our new notation provides a more compact representation of attributes  and is also closer to the notation supported by many e-r modeling tools  in addition to being closer to the uml class diagram notation there are a variety of tools for constructing e-r diagrams  each of which has its own notational variants some of the tools even provide a choice between several e-r notation variants see the references in the bibliographic notes for more information one key difference between entity sets in an e-r diagram and the relation schemas created from such entities is that attributes in the relational schema corresponding to e-r relationships  such as the dept name attribute of instructor  are not shown in the entity set in the e-r diagram some data modeling tools allow users to choose between two views of the same entity  one an entity view without such attributes  and other a relational view with such attributes thesumit67.blogspot.com 308 chapter 7 database design and the e-r model 7.9.2 the unified modeling language uml entity-relationship diagrams help model the data representation component of a software system data representation  however  forms only one part of an overall system design other components include models of user interactions with the system  specification of functional modules of the system and their interaction  etc the unified modeling language  uml  is a standard developed under the auspices of the object management group  omg  for creating specifications of various components of a software system some of the parts of uml are  ? class diagram a class diagram is similar to an e-r diagram later in this section we illustrate a few features of class diagrams and how they relate to e-r diagrams ? use case diagram use case diagrams show the interaction between users and the system  in particular the steps of tasks that users perform  such as withdrawing money or registering for a course   ? activity diagram.activity diagrams depict the flow of tasks between various components of a system ? implementation diagram implementation diagrams show the system components and their interconnections  both at the software component level and the hardware component level we do not attempt to provide detailed coverage of the different parts of uml here see the bibliographic notes for references on uml instead we illustrate some features of that part of uml that relates to data modeling through examples figure 7.26 shows several e-r diagram constructs and their equivalent uml class diagram constructs.we describe these constructs below uml actually models objects  whereas e-r models entities objects are like entities  and have attributes  but additionally provide a set of functions  called methods  that can be invoked to compute values on the basis of attributes of the objects  or to update the object itself class diagrams can depict methods in addition to attributes.we cover objects in chapter 22 uml does not support composite or multivalued attributes  and derived attributes are equivalent to methods that take no parameters since classes support encapsulation  uml allows attributes and methods to be prefixed with a ? + ?  ?  ?  or ? # ?  which denote respectively public  private and protected access private attributes can only be used in methods of the class,while protected attributes can be used only in methods of the class and its subclasses ; these should be familiar to anyone who knows java  c + + or c #  in uml terminology  relationship sets are referred to as associations ; we shall refer to them as relationship sets for consistency with e-r terminology.we represent binary relationship sets in uml by just drawing a line connecting the entity sets.we write the relationship set name adjacent to the line.we may also specify the role played by an entity set in a relationship set by writing the role name on the line  adjacent to the entity set alternatively  we may write the relationship set name in a box  along with attributes of the relationship set  and connect the thesumit67.blogspot.com 7.9 alternative notations formodeling data 309 ? a1 + m1 e e2 e3 e1 e2 e3 e1 e2 e3 binary relationship class with simple a  ributes and methods  a  ribute prefixes  + = public  ? = private  # = protected  overlapping generalization disjoint generalization a1 m1 e entity with a  ributes  simple  composite  multivalued  derived  r e1 role1 role2 e2 relationship a  ributes e1 e2 role1 role2 a1 r r cardinality e1 e2 constraints r 0 * 0..1 e1 0..1 0 * e2 r e3 e1 e2 r e3 e1 n-ary e2 relationships e1 e2 e3 overlapping disjoint er diagram notation equivalent in uml e1 role1 r role2 e2 e1 role1 r role2 e2 a1     e1 figure 7.26 symbols used in the uml class diagram notation box by a dotted line to the line depicting the relationship set this box can then be treated as an entity set  in the same way as an aggregation in e-r diagrams  and can participate in relationships with other entity sets since uml version 1.3  uml supports nonbinary relationships  using the same diamond notation used in e-r diagrams nonbinary relationships could not be directly represented in earlier versions of uml ? they had to be converted to binary relationships by the technique we have seen earlier in section 7.7.3 uml allows the diamond notation to be used even for binary relationships  but most designers use the line notation cardinality constraints are specified in uml in the same way as in e-r diagrams  in the forml..h,wherel denotes the minimum and h the maximum number of relationships an entity can participate in however  you should be aware that the positioning of the constraints is exactly the reverse of the positioning of constraints in e-r diagrams  as shown in figure 7.26 the constraint 0 * on the e2 thesumit67.blogspot.com 310 chapter 7 database design and the e-r model side and 0..1 on the e1 side means that each e2 entity can participate in at most one relationship  whereas each e1 entity can participate in many relationships ; in other words  the relationship is many-to-one from e2 to e1 single values such as 1 or * may bewritten on edges ; the single value 1 on an edge is treated as equivalent to 1..1  while * is equivalent to 0 *  uml supports generalization ; the notation is basically the same as in our e-r notation  including the representation of disjoint and overlapping generalizations uml class diagrams include several other notations that do not correspond to the e-r notations we have seen for example  a line between two entity sets with a small diamond at one end specifies that the entity on the diamond side contains the other entity  containment is called ? aggregation ? in uml terminology ; do not confuse this use of aggregationwith the sense inwhich it is used in the e-r model   for example  a vehicle entity may contain an engine entity uml class diagrams also provide notations to represent object-oriented language features such as interfaces see the references in the bibliographic notes for more information on uml class diagrams 7.10 other aspects of database design our extensive discussion of schema design in this chapter may create the false impression that schema design is the only component of a database design there are indeed several other considerations that we addressmore fully in subsequent chapters  and survey briefly here 7.10.1 data constraints and relational database design we have seen a variety of data constraints that can be expressed using sql  including primary-key constraints  foreign-key constraints  check constraints  assertions  and triggers constraints serve several purposes the most obvious one is the automation of consistency preservation by expressing constraints in the sql data-definition language  the designer is able to ensure that the database system itself enforces the constraints this is more reliable than relying on each application program individually to enforce constraints it also provides a central location for the update of constraints and the addition of new ones a further advantage of stating constraints explicitly is that certain constraints are particularly useful in designing relational database schemas if we know  for example  that a social-security number uniquely identifies a person  then we can use a person ? s social-security number to link data related to that person even if these data appear in multiple relations contrast that with  for example  eye color  which is not a unique identifier eye color could not be used to link data pertaining to a specific person across relations because that person ? s data could not be distinguished from data pertaining to other peoplewith the same eye color in section 7.6  we generated a set of relation schemas for a given e-r design using the constraints specified in the design in chapter 8  we formalize this idea and related ones  and show how they can assist in the design of relational thesumit67.blogspot.com 7.10 other aspects of database design 311 database schemas the formal approach to relational database design allows us to state in a precise manner when a given design is a good one and to transform poor designs into better ones we shall see that the process of starting with an entity-relationship design and generating relation schemas algorithmically from that design provides a good start to the design process data constraints are useful as well in determining the physical structure of data it may be useful to store data that are closely related to each other in physical proximity on disk so as to gain efficiencies in disk access certain index structures work better when the index is on a primary key constraint enforcement comes at a potentially high price in performance each time the database is updated for each update  the system must check all of the constraints and either reject updates that fail the constraints or execute appropriate triggers the significance of the performance penalty depends not only on the frequency of update but also on how the database is designed indeed efficiency of the testing of certain types of constraints is an important aspect of the discussion of relational database schema design in chapter 8 7.10.2 usage requirements  queries  performance database system performance is a critical aspect of most enterprise information systems performance pertains not only to the efficient use of the computing and storage hardware being used  but also to the efficiency of people who interact with the system and of processes that depend upon database data there are two main metrics for performance  ? throughput ? the number of queries or updates  often referred to as transactions  that can be processed on average per unit of time ? response time ? the amount of time a single transaction takes from start to finish in either the average case or the worst case systems that process large numbers of transactions in a batch style focus on having high throughput systems that interact with people or time-critical systems often focus on response time these two metrics are not equivalent high throughput arises from obtaining high utilization of system components doing so may result in certain transactions being delayed until such time that they can be run more efficiently those delayed transactions suffer poor response time most commercial database systems historically have focused on throughput ; however  a variety of applications includingweb-based applications and telecommunication information systems require good response time on average and a reasonable bound on worst-case response time anunderstanding of types of queries that are expected to be themost frequent helps in the design process queries that involve joins require more resources to evaluate than those that do not in cases where a join is required  the database administrator may choose to create an index that facilitates evaluation of that join for queries ? whether a join is involved or not ? indices can be created to speed evaluation of selection predicates  sql where clause  that are likely to appear thesumit67.blogspot.com 312 chapter 7 database design and the e-r model another aspect of queries that affects the choice of indices is the relative mix of update and read operations while an index may speed queries  it also slows updates  which are forced to do extra work to maintain the accuracy of the index 7.10.3 authorization requirements authorization constraints affect design of the database aswell because sql allows access to be granted to users on the basis of components of the logical design of the database a relation schema may need to be decomposed into two or more schemas to facilitate the granting of access rights in sql for example  an employee record may include data relating to payroll  job functions  and medical benefits because different administrative units of the enterprise may manage each of these types of data  some users will need access to payroll data while being denied access to the job data  medical data  etc if these data are all in one relation  the desired division of access  though still feasible through the use of views  is more cumbersome division of data in this manner becomes even more critical when the data are distributed across systems in a computer network  an issue we consider in chapter 19 7.10.4 data flow  workflow database applications are often part of a larger enterprise application that interacts not only with the database system but also with various specialized applications for example  in a manufacturing company  a computer-aided design  cad  system may assist in the design of new products the cad system may extract data from the database via an sql statement  process the data internally  perhaps interacting with a product designer  and then update the database during this process  control of the data may pass among several product designers as well as other people as another example  consider a travel-expense report it is created by an employee returning from a business trip  possibly by means of a special software package  and is subsequently routed to the employee ? smanager  perhaps other higher-level managers  and eventually to the accounting department for payment  at which point it interacts with the enterprise ? s accounting information systems   the term workflow refers to the combination of data and tasks involved in processes like those of the preceding examples.workflows interactwith the database system as they move among users and users perform their tasks on theworkflow in addition to the data on which workflows operate  the database may store data about the workflow itself  including the tasks making up a workflow and how they are to be routed among users.workflows thus specify a series of queries and updates to the database that may be taken into account as part of the databasedesign process put in other terms  modeling the enterprise requires us not only to understand the semantics of the data but also the business processes that use those data thesumit67.blogspot.com 7.11 summary 313 7.10.5 other issues in database design database design is usually not a one-time activity the needs of an organization evolve continually  and the data that it needs to store also evolve correspondingly during the initial database-design phases  or during the development of an application  the database designer may realize that changes are required at the conceptual  logical  or physical schema levels changes in the schema can affect all aspects of the database application a good database design anticipates future needs of an organization  and ensures that the schema requires minimal changes as the needs evolve it is important to distinguish between fundamental constraints that are expected to be permanent and constraints that are anticipated to change for example  the constraint that an instructor-id identify a unique instructor is fundamental on the other hand  a university may have a policy that an instructor can have only one department  which may change at a later date if joint appointments are allowed a database design that only allows one department per instructor might requiremajor changes if joint appointments are allowed such joint appointments can be represented by adding an extra relationship  without modifying the instructor relation  as long as each instructor has only one primary department affiliation ; a policy change that allows more than one primary affiliation may require a larger change in the database design a good design should account not only for current policies  but should also avoid or minimize changes due to changes that are anticipated  or have a reasonable chance of happening furthermore  the enterprise that the database is serving likely interacts with other enterprises and  therefore  multiple databases may need to interact conversion of data between different schemas is an important problem in real-world applications various solutions have been proposed for this problem the xml data model  which we study in chapter 23  is widely used for representing data when it is exchanged between different applications finally  it is worth noting that database design is a human-oriented activity in two senses  the end users of the system are people  even if an application sits between the database and the end users  ; and the database designer needs to interact extensivelywith experts in the application domain to understand the data requirements of the application all of the people involved with the data have needs and preferences that should be taken into account in order for a database design and deployment to succeed within the enterprise 7.11 summary ? database design mainly involves the design of the database schema the entity-relationship  e-r  data model is a widely used data model for database design it provides a convenient graphical representation to view data  relationships  and constraints ? the e-r model is intended primarily for the database-design process it was developed to facilitate database design by allowing the specification of an thesumit67.blogspot.com 314 chapter 7 database design and the e-r model enterprise schema such a schema represents the overall logical structure of the database this overall structure can be expressed graphically by an e-r diagram ? an entity is an object that exists in the real world and is distinguishable from other objects.we express the distinction by associating with each entity a set of attributes that describes the object ? a relationship is an association among several entities a relationship set is a collection of relationships of the same type  and an entity set is a collection of entities of the same type ? the terms superkey  candidate key  and primary key apply to entity and relationship sets as they do for relation schemas identifying the primary key of a relationship set requires some care  since it is composed of attributes from one or more of the related entity sets ? mapping cardinalities express the number of entities towhich another entity can be associated via a relationship set ? an entity set that does not have sufficient attributes to form a primary key is termed a weak entity set an entity set that has a primary key is termed a strong entity set ? the various features of the e-r model offer the database designer numerous choices in how to best represent the enterprise being modeled concepts and objects may  in certain cases  be represented by entities  relationships  or attributes aspects of the overall structure of the enterprise may be best described by using weak entity sets  generalization  specialization  or aggregation often  the designer must weigh the merits of a simple  compact model versus those of a more precise  but more complex  one ? a database design specified by an e-r diagram can be represented by a collection of relation schemas for each entity set and for each relationship set in the database  there is a unique relation schema that is assigned the name of the corresponding entity set or relationship set this forms the basis for deriving a relational database design from an e-r diagram ? specialization and generalization define a containment relationship between a higher-level entity set and one or more lower-level entity sets specialization is the result of taking a subset of a higher-level entity set to form a lower-level entity set generalization is the result of taking the union of two or more disjoint  lower-level  entity sets to produce a higher-level entity set the attributes of higher-level entity sets are inherited by lower-level entity sets ? aggregation is an abstraction in which relationship sets  along with their associated entity sets  are treated as higher-level entity sets  and can participate in relationships ? uml is a popular modeling language uml class diagrams are widely used for modeling classes  as well as for general purpose data modeling thesumit67.blogspot.com practice exercises 315 review terms ? entity-relationship data model ? entity and entity set ? attributes ? domain ? simple and composite attributes ? single-valued and multivalued attributes ? null value ? derived attribute ? superkey  candidate key  and primary key ? relationship and relationship set ? binary relationship set ? degree of relationship set ? descriptive attributes ? superkey  candidate key  and primary key ? role ? recursive relationship set ? e-r diagram ? mapping cardinality  ? one-to-one relationship ? one-to-many relationship ? many-to-one relationship ? many-to-many relationship ? participation ? total participation ? partial participation ? weak entity sets and strong entity sets ? discriminator attributes ? identifying relationship ? specialization and generalization ? superclass and subclass ? attribute inheritance ? single and multiple inheritance ? condition-defined and userdefined membership ? disjoint and overlapping generalization ? total and partial generalization ? aggregation ? uml ? uml class diagram practice exercises 7.1 construct an e-r diagram for a car insurance company whose customers own one or more cars each each car has associated with it zero to any number of recorded accidents each insurance policy covers one or more cars  and has one or more premium payments associated with it each payment is for a particular period of time  and has an associated due date  and the date when the payment was received 7.2 consider a database used to record the marks that students get in different exams of different course offerings  sections   thesumit67.blogspot.com 316 chapter 7 database design and the e-r model a construct an e-r diagram that models exams as entities  and uses a ternary relationship  for the database b construct an alternative e-r diagram that uses only a binary relationship between student and section.make sure that only one relationship exists between a particular student and section pair  yet you can represent the marks that a student gets in different exams 7.3 design an e-r diagram for keeping track of the exploits of your favorite sports team you should store the matches played  the scores in each match  the players in each match  and individual player statistics for each match summary statistics should be modeled as derived attributes 7.4 consider an e-r diagram inwhich the same entity set appears several times  with its attributes repeated in more than one occurrence why is allowing this redundancy a bad practice that one should avoid ? 7.5 an e-r diagram can be viewed as a graph.what do the following mean in terms of the structure of an enterprise schema ? a the graph is disconnected b the graph has a cycle 7.6 consider the representation of a ternary relationship using binary relationships as described in section 7.7.3 and illustrated in figure 7.27b  attributes not shown   b c a b e c a ra rb rc  a   b   c  a b c r rbc rab rac figure 7.27 e-r diagram for practice exercise 7.6 and exercise 7.24 thesumit67.blogspot.com practice exercises 317 a show a simple instance of e  a  b,c  ra  rb  and rc that can not correspond to any instance of a  b,c  and r b modify the e-r diagram of figure 7.27b to introduce constraints that will guarantee that any instance of e  a  b,c  ra  rb  and rc that satisfies the constraints will correspond to an instance of a  b,c  and r c modify the translation above to handle total participation constraints on the ternary relationship d the above representation requires that we create a primary-key attribute for e show how to treat e as a weak entity set so that a primary-key attribute is not required 7.7 a weak entity set can always be made into a strong entity set by adding to its attributes the primary-key attributes of its identifying entity set outline what sort of redundancy will result if we do so 7.8 consider a relation such as sec course  generated from a many-to-one relationship sec course do the primary and foreign key constraints created on the relation enforce the many-to-one cardinality constraint ? explain why 7.9 suppose the advisor relationship were one-to-one what extra constraints are required on the relation advisorto ensure that the one-to-one cardinality constraint is enforced ? 7.10 consider a many-to-one relationship r between entity sets aand b suppose the relation created from r is combined with the relation created from a in sql  attributes participating in a foreign key constraint can be null explain how a constraint on total participation of a in r can be enforced using not null constraints in sql 7.11 in sql  foreign key constraints can only reference the primary key attributes of the referenced relation  or other attributes declared to be a super key using the unique constraint as a result  total participation constraints on a many-to-many relationship  or on the ? one ? side of a one-to-many relationship  can not be enforced on the relations created from the relationship  using primary key  foreign key and not null constraints on the relations a explain why b explain how to enforce total participation constraints using complex check constraints or assertions  see section 4.4.7    unfortunately  these features are not supported on any widely used database currently  7.12 figure 7.28 shows a lattice structure of generalization and specialization  attributes not shown   for entity sets a  b  and c  explain how attributes thesumit67.blogspot.com 318 chapter 7 database design and the e-r model x y a b c figure 7.28 e-r diagram for practice exercise 7.12 are inherited from the higher-level entity sets x and y discuss how to handle a case where an attribute of x has the same name as some attribute of y 7.13 temporal changes  an e-r diagram usually models the state of an enterprise at a point in time suppose we wish to track temporal changes  that is  changes to data over time for example  zhang may have been a student between 1 september 2005 31 may 2009  while shankar may have had instructor einstein as advisor from 31 may 2008 to 5 december 2008  and again from 1 june 2009 to 5 january 2010 similarly  attribute values of an entity or relationship  such as title and credits of course  salary  or even name of instructor  and tot cred of student  can change over time one way to model temporal changes is as follows.we define a new data type called valid time  which is a time-interval  or a set of time-intervals we then associate a valid time attribute with each entity and relationship  recording the time periods during which the entity or relationship is valid the end-time of an interval can be infinity ; for example  if shankar became a student on 2 september 2008  and is still a student  we can represent the end-time of the valid time interval as infinity for the shankar entity similarly  we model attributes that can change over time as a set of values  each with its own valid time a draw an e-r diagram with the student and instructor entities  and the advisor relationship  with the above extensions to track temporal changes b convert the above e-r diagram into a set of relations it should be clear that the set of relations generated above is rather complex  leading to difficulties in tasks such as writing queries in sql an alternative approach  which is used more widely is to ignore temporal changes when designing the e-r model  in particular  temporal changes to attribute values   and to modify the relations generated from the e-r model to track temporal changes  as discussed later in section 8.9 thesumit67.blogspot.com exercises 319 exercises 7.14 explain the distinctions among the terms primary key  candidate key  and superkey 7.15 construct an e-r diagram for a hospital with a set of patients and a set of medical doctors associate with each patient a log of the various tests and examinations conducted 7.16 construct appropriate relation schemas for each of the e-r diagrams in practice exercises 7.1 to 7.3 7.17 extend the e-r diagram of practice exercise 7.3 to track the same information for all teams in a league 7.18 explain the difference between a weak and a strong entity set 7.19 we can convert any weak entity set to a strong entity set by simply adding appropriate attributes.why  then  do we have weak entity sets ? 7.20 consider the e-r diagram in figure 7.29  which models an online bookstore a list the entity sets and their primary keys b suppose the bookstore adds blu-ray discs and downloadable video to its collection the same item may be present in one or both formats  with differing prices extend the e-r diagram to model this addition  ignoring the effect on shopping baskets c now extend the e-r diagram  using generalization  to model the case where a shopping basket may contain any combination of books  blu-ray discs  or downloadable video 7.21 design a database for an automobile company to provide to its dealers to assist them in maintaining customer records and dealer inventory and to assist sales staff in ordering cars each vehicle is identified by a vehicle identification number  vin   each individual vehicle is a particular model of a particular brand offered by the company  e.g  the xf is a model of the car brand jaguar of tata motors   each model can be offered with a variety of options  but an individual car may have only some  or none  of the available options the database needs to store information about models  brands  and options  as well as information about individual dealers  customers  and cars your design should include an e-r diagram  a set of relational schemas  and a list of constraints  including primary-key and foreign-key constraints 7.22 design a database for a world-wide package delivery company  e.g  dhl or fedex   the databasemust be able to keep track of customers  who ship items  and customers  who receive items  ; some customers may do both thesumit67.blogspot.com 320 chapter 7 database design and the e-r model author name address url written_by published_by contains number number stocks book shopping_basket basket_id warehouse basket_of isbn title year price code address phone publisher name address phone url customer email name address phone figure 7.29 e-r diagram for exercise 7.20 each package must be identifiable and trackable  so the database must be able to store the location of the package and its history of locations locations include trucks  planes  airports  and warehouses your design should include an e-r diagram  a set of relational schemas  and a list of constraints  including primary-key and foreign-key constraints 7.23 design a database for an airline the database must keep track of customers and their reservations  flights and their status  seat assignments on individual flights  and the schedule and routing of future flights your design should include an e-r diagram  a set of relational schemas  and a list of constraints  including primary-key and foreign-key constraints 7.24 in section 7.7.3  we represented a ternary relationship  repeated in figure 7.27a  using binary relationships  as shown in figure 7.27b consider the alternative shown in figure 7.27c discuss the relative merits of these two alternative representations of a ternary relationship by binary relationships thesumit67.blogspot.com bibliographical notes 321 7.25 consider the relation schemas shown in section 7.6  which were generated from the e-r diagram in figure 7.15 for each schema  specifywhat foreignkey constraints  if any  should be created 7.26 design a generalization ? specialization hierarchy for a motor vehicle sales company the company sells motorcycles  passenger cars  vans  and buses justify your placement of attributes at each level of the hierarchy explain why they should not be placed at a higher or lower level 7.27 explain the distinction between condition-defined and user-defined constraints which of these constraints can the system check automatically ? explain your answer 7.28 explain the distinction between disjoint and overlapping constraints 7.29 explain the distinction between total and partial constraints tools many database systems provide tools for database design that support e-r diagrams these tools help a designer create e-rdiagrams  and they can automatically create corresponding tables in a database see bibliographic notes of chapter 1 for references to database-system vendors ? web sites there are also several database-independent data modeling tools that support e-r diagrams and uml class diagrams the drawing tool dia  which is available as freeware  supports e-r diagrams and uml class diagrams commercial tools include ibm rational rose  www.ibm.com/software/rational   microsoft visio  see www.microsoft.com/office/visio   ca ? s erwin  www.ca.com/us/datamodeling aspx   poseidon for uml  www.gentleware.com   and smartdraw  www.smartdraw.com   bibliographical notes the e-r data modelwas introduced by chen  1976  .alogical design methodology for relational databases using the extended e-r model is presented by teorey et al  1986   the integration definition for information modeling  idef1x  standard nist  1993  released by the united states national institute of standards and technology  nist  defined standards for e-r diagrams however  a variety of e-r notations are in use today thalheim  2000  provides a detailed textbook coverage of research in e-r modeling basic textbook discussions are offered by batini et al  1992  and elmasri and navathe  2006   davis et al  1983  provides a collection of papers on the e-r model as of 2009  the current uml version was 2.2  with uml version 2.3 near final adoption see www.uml.org for more information on uml standards and tools thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter8 relational database design in this chapter  we consider the problem of designing a schema for a relational database many of the issues in doing so are similar to design issues we considered in chapter 7 using the e-r model in general  the goal of relational database design is to generate a set of relation schemas that allows us to store information without unnecessary redundancy  yet also allows us to retrieve information easily this is accomplished by designing schemas that are in an appropriate normal form to determine whether a relation schema is in one of the desirable normal forms  we need information about the real-world enterprise that we are modeling with the database some of this information exists in a well-designed e-r diagram  but additional information about the enterprise may be needed as well in this chapter  we introduce a formal approach to relational database design based on the notion of functional dependencies we then define normal forms in terms of functional dependencies and other types of data dependencies first  however  we view the problem of relational design from the standpoint of the schemas derived from a given entity-relationship design 8.1 features of good relational designs our study of entity-relationship design inchapter 7 provides an excellent starting point for creating a relational database design we saw in section 7.6 that it is possible to generate a set of relation schemas directly from the e-r design obviously  the goodness  or badness  of the resulting set of schemas depends on how good the e-r design was in the first place later in this chapter  we shall study precise ways of assessing the desirability of a collection of relation schemas however  we can go a long way toward a good design using concepts we have already studied for ease of reference  we repeat the schemas for the university database in figure 8.1 323 thesumit67.blogspot.com 324 chapter 8 relational database design classroom  building  room number  capacity  department  dept name  building  budget  course  course id  title  dept name  credits  instructor  id  name  dept name  salary  section  course id  sec id  semester  year  building  room number  time slot id  teaches  id  course id  sec id  semester  year  student  id  name  dept name  tot cred  takes  id  course id  sec id  semester  year  grade  advisor  s id  i id  time slot  time slot id  day  start time  end time  prereq  course id  prereq id  figure 8.1 schema for the university database 8.1.1 design alternative  larger schemas now  let us explore features of this relational database design as well as some alternatives suppose that instead of having the schemas instructor and department  we have the schema  inst dept  id  name  salary  dept name  building  budget  this represents the result of a natural join on the relations corresponding to instructor and department this seems like a good idea because some queries can be expressed using fewer joins  until we think carefully about the facts about the university that led to our e-r design let us consider the instance of the inst dept relation shown in figure 8.2.notice that we have to repeat the department information  ? building ? and ? budget ?  once for each instructor in the department for example  the information about the comp sci department  taylor  100000  is included in the tuples of instructors katz  srinivasan  and brandt it is important that all these tuples agree as to the budget amount since otherwise our database would be inconsistent in our original design using instructor and department  we stored the amount of each budget exactly once this suggests that using inst dept is a bad idea since it stores the budget amounts redundantly and runs the risk that some user might update the budget amount in one tuple but not all  and thus create inconsistency even if we decided to live with the redundancy problem  there is still another problem with the inst dept schema suppose we are creating a new department in the university in the alternative design above  we can not represent directly the information concerning a department  dept name  building  budget  unless that department has at least one instructor at the university this is because tuples in the inst dept table require values for id  name  andsalary thismeans thatwecannot record information about the newly created department until the first instructor thesumit67.blogspot.com 8.1 features of good relational designs 325 id name salary dept name building budget 22222 einstein 95000 physics watson 70000 12121 wu 90000 finance painter 120000 32343 el said 60000 history painter 50000 45565 katz 75000 comp sci taylor 100000 98345 kim 80000 elec eng taylor 85000 76766 crick 72000 biology watson 90000 10101 srinivasan 65000 comp sci taylor 100000 58583 califieri 62000 history painter 50000 83821 brandt 92000 comp sci taylor 100000 15151 mozart 40000 music packard 80000 33456 gold 87000 physics watson 70000 76543 singh 80000 finance painter 120000 figure 8.2 the inst dept table is hired for the new department in the old design  the schema department can handle this  but under the revised design  we would have to create a tuple with a null value for building and budget in some cases null values are troublesome  as we saw in our study of sql however  if we decide that this is not a problem to us in this case  then we can proceed to use the revised design 8.1.2 design alternative  smaller schemas suppose again that  somehow  we had started out with the schema inst dept how would we recognize that it requires repetition of information and should be split into the two schemas instructor and department ? by observing the contents of actual relations on schema inst dept  we could note the repetition of information resulting from having to list the building and budget once for each instructor associatedwith a department however  this is an unreliable process a real-world database has a large number of schemas and an even larger number of attributes the number of tuples can be in the millions or higher.discovering repetitionwould be costly there is an evenmore fundamental problemwith this approach it does not allow us to determine whether the lack of repetition is just a ? lucky ? special case orwhether it is amanifestation of a general rule in our example  howwouldwe know that in our university organization  each department  identified by its department name  must reside in a single building and must have a single budget amount ? is the fact that the budget amount for the comp sci department appears three times with the same budget amount just a coincidence ? we can not answer these questions without going back to the enterprise itself and understanding its rules in particular  we would need to discover that the university requires that every department  identified by its department name  must have only one building and one budget value in the case of inst dept  our process of creating an e-r design successfully avoided the creation of this schema however  this fortuitous situation does not thesumit67.blogspot.com 326 chapter 8 relational database design always occur therefore  we need to allow the database designer to specify rules such as ? each specific value for dept name corresponds to atmost one budget ? even in cases where dept name is not the primary key for the schema in question in other words  we need to write a rule that says ? if there were a schema  dept name  budget   then dept name is able to serve as the primary key ? this rule is specified as a functional dependency dept name ? budget given such a rule  we now have sufficient information to recognize the problem of the inst dept schema because dept name can not be the primary key for inst dept  because a department may need several tuples in the relation on schema inst dept   the amount of a budgetmay have to be repeated observations such as these and the rules  functional dependencies in particular  that result from them allow the database designer to recognize situations where a schema ought to be split  or decomposed  into two or more schemas it is not hard to see that the right way to decompose inst dept is into schemas instructor and department as in the original design finding the right decomposition is much harder for schemaswith a large number of attributes and several functional dependencies to deal with this  we shall rely on a formal methodology that we develop later in this chapter not all decompositions of schemas are helpful consider an extreme case where all we had were schemas consisting of one attribute no interesting relationships of any kind could be expressed.now consider a less extreme casewhere we choose to decompose the employee schema  section 7.8   employee  id  name  street  city  salary  into the following two schemas  employee1  id  name  employee2  name  street  city  salary  the flaw in this decomposition arises from the possibility that the enterprise has two employees with the same name this is not unlikely in practice  as many cultures have certain highly popular names of course each person would have a unique employee-id  which is why id can serve as the primary key as an example  let us assume two employees  both named kim  work at the university and have the following tuples in the relation on schema employee in the original design   57766  kim  main  perryridge  75000   98776  kim  north  hampton  67000  thesumit67.blogspot.com 8.2 atomic domains and first normal form 327 id name street city salary   57766 98776   kim kim main north perryridge hampton 75000 67000 id name   57766 98776   kim kim name street city salary 75000 67000 main north perryridge hampton   kim kim   id name street city salary employee   57766 57766 98776 98776   75000 67000 75000 67000 perryridge hampton perryridge hampton main north main north kim kim kim kim natural join figure 8.3 loss of information via a bad decomposition figure 8.3 shows these tuples  the resulting tuples using the schemas resulting from the decomposition  and the result if we attempted to regenerate the original tuples using a natural join as we see in the figure  the two original tuples appear in the result along with two new tuples that incorrectly mix data values pertaining to the two employees named kim although we have more tuples  we actually have less information in the following sense we can indicate that a certain street  city  and salary pertain to someone named kim  but we are unable to distinguish which of the kims thus  our decomposition is unable to represent certain important facts about the university employees clearly  we would like to avoid such decompositions we shall refer to such decompositions as being lossy decompositions  and  conversely  to those that are not as lossless decompositions 8.2 atomic domains and first normal form the e-r model allows entity sets and relationship sets to have attributes that have some degree of substructure specifically  it allows multivalued attributes such as thesumit67.blogspot.com 328 chapter 8 relational database design phone number in figure 7.11 and composite attributes  such as an attribute address with component attributes street  city  state  and zip   when we create tables from e-r designs that contain these types of attributes  we eliminate this substructure for composite attributes  we let each component be an attribute in its own right formultivalued attributes,we create one tuple for each item in amultivalued set in the relationalmodel  we formalize this idea that attributes do not have any substructure a domain is atomic if elements of the domain are considered to be indivisible units we say that a relation schema r is in first normal form  1nf  if the domains of all attributes of r are atomic aset of names is an example of a nonatomic value for example  if the schema of a relation employee included an attribute children whose domain elements are sets of names  the schema would not be in first normal form composite attributes  such as an attribute address with component attributes street  city  state  and zip also have nonatomic domains integers are assumed to be atomic  so the set of integers is an atomic domain ; however  the set of all sets of integers is a nonatomic domain the distinction is that we do not normally consider integers to have subparts  but we consider sets of integers to have subparts ? namely  the integers making up the set but the important issue is not what the domain itself is  but rather how we use domain elements in our database the domain of all integers would be nonatomic if we considered each integer to be an ordered list of digits as a practical illustration of the above point  consider an organization that assigns employees identification numbers of the following form  the first two letters specify the department and the remaining four digits are a unique number within the department for the employee examples of such numbers would be ? cs001 ? and ? ee1127 ?  such identification numbers can be divided into smaller units  and are therefore nonatomic if a relation schema had an attribute whose domain consists of identification numbers encoded as above  the schema would not be in first normal form when such identification numbers are used  the department of an employee can be found by writing code that breaks up the structure of an identification number doing so requires extra programming  and information gets encoded in the application program rather than in the database further problems arise if such identification numbers are used as primary keys  when an employee changes departments  the employee ? s identification number must be changed everywhere it occurs  which can be a difficult task  or the code that interprets the number would give a wrong result from the above discussion  it may appear that our use of course identifiers such as ? cs-101 ?  where ? cs ? indicates the computer science department  means that the domain of course identifiers is not atomic such a domain is not atomic as far as humans using the system are concerned.however  the database application still treats the domain as atomic  as long as it does not attempt to split the identifier and interpret parts of the identifier as a department abbreviation the course schema stores the department name as a separate attribute  and the database application can use this attribute value to find the department of a course  instead thesumit67.blogspot.com 8.3 decomposition using functional dependencies 329 of interpreting particular characters of the course identifier thus  our university schema can be considered to be in first normal form the use of set-valued attributes can lead to designswith redundant storage of data  which in turn can result in inconsistencies for instance  instead of having the relationship between instructors and sections being represented as a separate relation teaches  a database designer may be tempted to store a set of course section identifiers with each instructor and a set of instructor identifierswith each section  the primary keys of section and instructor are used as identifiers  whenever data pertaining to which instructor teaches which section is changed  the update has to be performed at two places  in the set of instructors for the section  and the set of sections for the instructor failure to perform both updates can leave the database in an inconsistent state keeping only one of these sets  that either the set of instructors of a section  or the set of sections of an instructor  would avoid repeated information ; however keeping only one of thesewould complicate some queries  and it is unclear which of the two to retain some types of nonatomic values can be useful  although they should be used with care for example  composite-valued attributes are often useful  and setvalued attributes are also useful in many cases  which is why both are supported in the e-r model in many domains where entities have a complex structure  forcing a first normal form representation represents an unnecessary burden on the application programmer  who has to write code to convert data into atomic form there is also the runtime overhead of converting data back and forth from the atomic form support for nonatomic values can thus be very useful in such domains in fact  modern database systems do support many types of nonatomic values  aswe shall see inchapter 22.however  in this chapterwe restrict ourselves to relations in first normal form and  thus  all domains are atomic 8.3 decomposition using functional dependencies in section 8.1,we noted that there is a formal methodology for evaluatingwhether a relational schema should be decomposed this methodology is based upon the concepts of keys and functional dependencies in discussing algorithms for relational database design  we shall need to talk about arbitrary relations and their schema  rather than talking only about examples recalling our introduction to the relational model in chapter 2  we summarize our notation here ? in general  we use greek letters for sets of attributes  for example     we use a lowercase roman letter followed by an uppercase roman letter in parentheses to refer to a relation schema  for example  r  r    we use the notation r  r  to show that the schema is for relation r  with r denoting the set of attributes  but at times simplify our notation to use just r when the relation name does not matter to us of course  a relation schema is a set of attributes  but not all sets of attributes are schemas when we use a lowercase greek letter  we are referring to a set thesumit67.blogspot.com 330 chapter 8 relational database design of attributes that may or may not be a schema a roman letter is used when we wish to indicate that the set of attributes is definitely a schema ? when a set of attributes is a superkey,we denote it by k a superkey pertains to a specific relation schema  so we use the terminology ? k is a superkey of r  r   ? ? we use a lowercase name for relations in our examples  these names are intended to be realistic  for example  instructor   while in our definitions and algorithms  we use single letters  like r  ? a relation  of course  has a particular value at any given time ; we refer to that as an instance and use the term ? instance of r ?  when it is clear that we are talking about an instance,we may use simply the relation name  for example  r   8.3.1 keys and functional dependencies a database models a set of entities and relationships in the real world there are usually a variety of constraints  rules  on the data in the real world for example  some of the constraints that are expected to hold in a university database are  1 students and instructors are uniquely identified by their id 2 each student and instructor has only one name 3 each instructor and student is  primarily  associated with only one department 1 4 each department has only one value for its budget  and only one associated building an instance of a relation that satisfies all such real-world constraints is called a legal instance of the relation ; a legal instance of a database is one where all the relation instances are legal instances some of the most commonly used types of real-world constraints can be represented formally as keys  superkeys  candidate keys and primary keys   or as functional dependencies  which we define below in section 2.3  we defined the notion of a superkey as a set of one or more attributes that  taken collectively  allows us to identify uniquely a tuple in the relation.we restate that definition here as follows  let r  r  be a relation schema a subset k of r is a superkey of r  r  if  in any legal instance of r  r   for all pairs t1 and t2 of tuples in the instance of r if t1  = t2  then t1  k   = t2  k   that is  no two tuples in any legal instance of relation r  r  may have the same value on 1an instructor or a student can be associated with more than one department  for example as an adjunct faculty  or as a minor department our simplified university schema models only the primary department associated with each instructor or student a real university schema would capture secondary associations in other relations thesumit67.blogspot.com 8.3 decomposition using functional dependencies 331 attribute set k clearly  if no two tuples in r have the same value on k  then a k-value uniquely identifies a tuple in r  whereas a superkey is a set of attributes that uniquely identifies an entire tuple  a functional dependency allows us to express constraints that uniquely identify the values of certain attributes consider a relation schema r  r   and let  ? r and  ? r ? given an instance of r  r   we say that the instance satisfies the functional dependency  ?  if for all pairs of tuples t1 and t2 in the instance such that t1    = t2     it is also the case that t1    = t2     ? we say that the functional dependency  ?  holds on schema r  r  if  in every legal instance of r  r  it satisfies the functional dependency using the functional-dependency notation  we say that k is a superkey of r  r  if the functional dependency k ? r holds on r  r   in other words  k is a superkey if  for every legal instance of r  r   for every pair of tuples t1 and t2 from the instance  whenever t1  k  = t2  k   it is also the case that t1  r  = t2  r   that is  t1 = t2  .2 functional dependencies allow us to express constraints that we can not express with superkeys in section 8.1.2  we considered the schema  inst dept  id  name  salary  dept name  building  budget  in which the functional dependency dept name ? budget holds because for each department  identified by dept name  there is a unique budget amount we denote the fact that the pair of attributes  id  dept name  forms a superkey for inst dept by writing  id  dept name ? name  salary  building  budget we shall use functional dependencies in two ways  1 to test instances of relations to see whether they satisfy a given set f of functional dependencies 2 to specify constraints on the set of legal relations.we shall thus concern ourselveswith only those relation instances that satisfy a given set of functional dependencies if we wish to constrain ourselves to relations on schema r  r  that satisfy a set f of functional dependencies  we say that f holds on r  r   let us consider the instance of relation r of figure 8.4  to see which functional dependencies are satisfied observe that a ? c is satisfied there are two tuples 2note that we assume here that relations are sets sql deals with multisets  and a primary key declaration in sql for a set of attributes k requires not only that t1 = t2 if t1  k  = t2  k   but also that there be no duplicate tuples sql also requires that attributes in the set k can not be assigned a null value thesumit67.blogspot.com 332 chapter 8 relational database design a b c d a1 b1 c1 d1 a1 b2 c1 d2 a2 b2 c2 d2 a2 b3 c2 d3 a3 b3 c2 d4 figure 8.4 sample instance of relation r that have an a value of a1 these tuples have the same c value ? namely  c1 similarly  the two tuples with an a value of a2 have the same c value  c2 there are no other pairs of distinct tuples that have the same a value the functional dependency c ? a is not satisfied  however to see that it is not  consider the tuples t1 =  a2  b3  c2  d3  and t2 =  a3  b3  c2  d4   these two tuples have the same c values  c2  but they have different a values  a2 and a3  respectively thus  we have found a pair of tuples t1 and t2 such that t1  c  = t2  c   but t1  a   = t2  a   some functional dependencies are said to be trivial because they are satisfied by all relations for example  a ? a is satisfied by all relations involving attribute a reading the definition of functional dependency literally,we see that  for all tuples t1 and t2 such that t1  a  = t2  a   it is the case that t1  a  = t2  a   similarly  ab ? ais satisfied by all relations involving attribute a in general  a functional dependency of the form  ?  is trivial if  ?   it is important to realize that an instance of a relation may satisfy some functional dependencies that are not required to hold on the relation ? s schema in the instance of the classroom relation of figure 8.5  we see that room number ? capacity is satisfied however  we believe that  in the real world  two classrooms in different buildings can have the same room number but with different room capacity thus  it is possible  at some time  to have an instance of the classroom relation in which room number ? capacity is not satisfied so  we would not include room number ? capacity in the set of functional dependencies that hold on the schema for the classroom relation however  we would expect the functional dependency building  room number ? capacity to hold on the classroom schema given that a set of functional dependencies f holds on a relation r  r   it may be possible to infer that certain other functional dependencies must also hold on building room number capacity packard 101 500 painter 514 10 taylor 3128 70 watson 100 30 watson 120 50 figure 8.5 an instance of the classroom relation thesumit67.blogspot.com 8.3 decomposition using functional dependencies 333 the relation for example  given a schema r  a  b,c   if functional dependencies a ? b and b ? c  hold on r  we can infer the functional dependency a ? c must also hold on r  this is because  given any value of a there can be only one corresponding value for b  and for that value of b  there can only be one corresponding value for c we study later  in section 8.4.1  how to make such inferences we will use the notation f + to denote the closure of the set f  that is  the set of all functional dependencies that can be inferred given the set f clearly f + contains all of the functional dependencies in f 8.3.2 boyce ? codd normal form one of the more desirable normal forms that we can obtain is boyce ? codd normal form  bcnf   it eliminates all redundancy that can be discovered based on functional dependencies  though  as we shall see in section 8.6  there may be other types of redundancy remaining.arelation schema r is in bcnf with respect to a set f of functional dependencies if  for all functional dependencies in f + of the form  ?   where  ? r and  ? r  at least one of the following holds  ?  ?  is a trivial functional dependency  that is   ?    ?  is a superkey for schema r a database design is in bcnf if each member of the set of relation schemas that constitutes the design is in bcnf we have already seen in section 8.1 an example of a relational schema that is not in bcnf  inst dept  id  name  salary  dept name  building  budget  the functional dependency dept name ? budget holds on inst dept  but dept name is not a superkey  because  a department may have a number of different instructors   in section 8.1.2  we saw that the decomposition of inst dept into instructor and department is a better design the instructor schema is in bcnf all of the nontrivial functional dependencies that hold  such as  id ? name  dept name  salary include id on the left side of the arrow  and id is a superkey  actually  in this case  the primary key  for instructor  in other words  there is no nontrivial functional dependency with any combination of name  dept name  and salary  without id  on the side  thus  instructor is in bcnf similarly  the department schema is in bcnf because all of the nontrivial functional dependencies that hold  such as  dept name ? building  budget thesumit67.blogspot.com 334 chapter 8 relational database design include dept name on the left side of the arrow  and dept name is a superkey  and the primary key  for department thus  department is in bcnf we now state a general rule for decomposing that are not in bcnf let r be a schema that is not in bcnf then there is at least one nontrivial functional dependency  ?  such that  is not a superkey for r.we replace r in our design with two schemas  ?   ?   ?  r        in the case of inst dept above   = dept name   =  building  budget   and inst dept is replaced by ?   ?   =  dept name  building,budget  ?  r        =  id  name  dept name  salary  in this example  it turns out that    =   we need to state the rule as we did so as to deal correctly with functional dependencies that have attributes that appear on both sides of the arrow the technical reasons for this are covered later in section 8.5.1 when we decompose a schema that is not in bcnf  itmay be that one ormore of the resulting schemas are not in bcnf in such cases  further decomposition is required  the eventual result of which is a set of bcnf schemas 8.3.3 bcnf and dependency preservation we have seen several ways in which to express database consistency constraints  primary-key constraints  functional dependencies  check constraints  assertions  and triggers testing these constraints each time the database is updated can be costly and  therefore  it is useful to design the database in a way that constraints can be tested efficiently in particular  if testing a functional dependency can be done by considering just one relation  then the cost of testing this constraint is low we shall see that  in some cases  decomposition into bcnf can prevent efficient testing of certain functional dependencies to illustrate this  suppose that we make a small change to our university organization in the design of figure 7.15  a student may have only one advisor this follows from the relationship set advisor being many-to-one from student to advisor the ? small ? change we shall make is that an instructor can be associated with only a single department and a student may have more than one advisor  but at most one froma given department.3 one way to implement this change using the e-r design is by replacing the advisor relationship set with a ternary relationship set  dept advisor  involving entity sets instructor  student  and department that is many-to-one from the pair 3such an arrangement makes sense for students with a double major thesumit67.blogspot.com 8.3 decomposition using functional dependencies 335 dept_name building budget department dept_advisor instructor id name salary student id name tot_cred figure 8.6 the dept advisor relationship set  student  instructor  to departmentas shown infigure8.6.the e-r diagram specifies the constraint that ? a student may have more than one advisor  but at most one corresponding to a given department ?  with this new e-r diagram  the schemas for the instructor  department  and student are unchanged however  the schema derived from dept advisor is now  dept advisor  s id  i id  dept name  although not specified in the e-r diagram  suppose we have the additional constraint that ? an instructor can act as advisor for only a single department ? then  the following functional dependencies hold on dept advisor  i id ? dept name s id  dept name ? i id the first functional dependency follows from our requirement that ? an instructor can act as an advisor for only one department ? the second functional dependency follows from our requirement that ? a student may have at most one advisor for a given department ? notice that with this design  we are forced to repeat the department name once for each time an instructor participates in a dept advisor relationship.we see that dept advisor is not in bcnf because i id is not a superkey following our rule for bcnf decomposition  we get   s id  i id   i id  dept name  both the above schemas are bcnf  in fact  you can verify that any schema with only two attributes is in bcnf by definition  note however  that in our bcnf design  there is no schema that includes all the attributes appearing in the functional dependency s id  dept name ? i id thesumit67.blogspot.com 336 chapter 8 relational database design because our design makes it computationally hard to enforce this functional dependency  we say our design is not dependency preserving.4 because dependency preservation is usually considered desirable  we consider another normal form,weaker than bcnf  thatwill allow us to preserve dependencies that normal form is called third normal form.5 8.3.4 third normal form bcnf requires that all nontrivial dependencies be of the form  ?   where  is a superkey third normal form  3nf  relaxes this constraint slightly by allowing certain nontrivial functionaldependencieswhose left side is not a superkey.before we define 3nf  we recall that a candidate key is a minimal superkey ? that is  a superkey no proper subset of which is also a superkey arelation schemaris in third normal form with respect to a set f of functional dependencies if  for all functional dependencies in f + of the form  ?   where  ? r and  ? r  at least one of the following holds  ?  ?  is a trivial functional dependency ?  is a superkey for r ? each attribute a in    is contained in a candidate key for r note that the third condition above does not say that a single candidate key must contain all the attributes in    ; each attribute a in    may be contained in a different candidate key the first two alternatives are the same as the two alternatives in the definition of bcnf the third alternative of the 3nf definition seems rather unintuitive  and it is not obvious why it is useful it represents  in some sense  a minimal relaxation of the bcnf conditions that helps ensure that every schema has a dependencypreserving decomposition into 3nf its purpose will become more clear later  when we study decomposition into 3nf observe that any schema that satisfies bcnf also satisfies 3nf  since each of its functional dependencies would satisfy one of the first two alternatives bcnf is therefore a more restrictive normal form than is 3nf the definition of 3nf allows certain functional dependencies that are not allowed in bcnf a dependency  ?  that satisfies only the third alternative of the 3nf definition is not allowed in bcnf  but is allowed in 3nf.6 now  let us again consider the dept advisor relationship set  which has the following functional dependencies  4technically  it is possible that a dependency whose attributes do not all appear in any one schema is still implicitly enforced  because of the presence of other dependencies that imply it logically.we address that case later  in section 8.4.5 5youmay have noted thatwe skipped second normal form it is of historical significance only and is not used in practice 6these dependencies are examples of transitive dependencies  see practice exercise 8.16   the original definition of 3nf was in terms of transitive dependencies the definition we use is equivalent but easier to understand thesumit67.blogspot.com 8.3 decomposition using functional dependencies 337 i id ? dept name s id  dept name ? i id in section 8.3.3 we argued that the functional dependency ? i id ? dept name ? caused the dept advisor schema not to be in bcnf note that here  = i id   = dept name  and    = dept name since the functional dependency s id  dept name ? i id holds on dept advisor  the attribute dept name is contained in a candidate key and  therefore  dept advisor is in 3nf we have seen the trade-off that must be made between bcnf and 3nf when there is no dependency-preserving bcnf design these trade-offs are described in more detail in section 8.5.4 8.3.5 higher normal forms using functional dependencies to decompose schemas may not be sufficient to avoid unnecessary repetition of information in certain cases consider a slight variation in the instructor entity-set definition in which we record with each instructor a set of children ? s names and a set of phone numbers the phone numbers may be shared by multiple people thus  phone number and child name would bemultivalued attributes and  following our rules for generating schemas from an e-r design  we would have two schemas  one for each of the multivalued attributes  phone number and child name   id  child name   id  phone number  if we were to combine these schemas to get  id  child name  phone number  we would find the result to be in bcnf because only nontrivial functional dependencies hold as a result we might think that such a combination is a good idea however  such a combination is a bad idea  as we can see by considering the example of an instructor with two children and two phone numbers for example  let the instructor with id 99999 have two children named ? david ? and ? william ? and two phone numbers  512-555-1234 and 512-555-4321 in the combined schema  we must repeat the phone numbers once for each dependent   99999  david  512-555-1234   99999  david  512-555-4321   99999  william  512-555-1234   99999  william  512-555-4321  if we did not repeat the phone numbers  and stored only the first and last tuple,wewould have recorded the dependent names and the phone numbers  but thesumit67.blogspot.com 338 chapter 8 relational database design the resultant tuples would imply that david corresponded to 512-555-1234  while william corresponded to 512-555-4321 as we know  this would be incorrect because normal forms based on functional dependencies are not sufficient to deal with situations like this  other dependencies and normal forms have been defined.we cover these in sections 8.6 and 8.7 8.4 functional-dependency theory we have seen in our examples that it is useful to be able to reason systematically about functional dependencies as part of a process of testing schemas for bcnf or 3nf 8.4.1 closure of a set of functional dependencies we shall see that  given a set f of functional dependencies on a schema  we can prove that certain other functional dependencies also hold on the schema we say that such functional dependencies are ? logically implied ? by f when testing for normal forms  it is not sufficient to consider the given set of functional dependencies ; rather  we need to consider all functional dependencies that hold on the schema more formally  given a relational schema r  r   a functional dependency f on r is logically implied by a set of functional dependencies f on r if every instance of r  r  that satisfies f also satisfies f  suppose we are given a relation schema r  a  b  c  g  h  i  and the set of functional dependencies  a ? b a ? c cg ? h cg ? i b ? h the functional dependency  a ? h is logically implied that is  we can show that  whenever a relation satisfies our given set of functional dependencies,a ? hmust also be satisfied by that relation suppose that t1 and t2 are tuples such that  t1  a  = t2  a  since we are given that a ? b  it follows from the definition of functional dependency that  t1  b  = t2  b  thesumit67.blogspot.com 8.4 functional-dependency theory 339 then  since we are given that b ? h  it follows from the definition of functional dependency that  t1  h  = t2  h  therefore  we have shown that  whenever t1 and t2 are tuples such that t1  a  = t2  a   it must be that t1  h  = t2  h   but that is exactly the definition of a ? h let f be a set of functional dependencies the closure of f  denoted by f +  is the set of all functional dependencies logically implied by f given f  we can compute f + directly from the formal definition of functional dependency if f were large  this process would be lengthy and difficult such a computation of f + requires arguments of the type just used to show that a ? h is in the closure of our example set of dependencies axioms  or rules of inference  provide a simpler technique for reasoning about functional dependencies in the rules that follow  we use greek letters            for sets of attributes  and uppercase roman letters from the beginning of the alphabet for individual attributes.we use   to denote  ?   we can use the following three rules to find logically implied functional dependencies by applying these rules repeatedly  we can find all of f +  given f this collection of rules is called armstrong ? s axioms in honor of the person who first proposed it ? reflexivity rule if  is a set of attributes and  ?   then  ?  holds ? augmentation rule if  ?  holds and  is a set of attributes  then   ?   holds ? transitivity rule if  ?  holds and  ?  holds  then  ?  holds armstrong ? s axioms are sound  because they do not generate any incorrect functional dependencies they are complete  because  for a given set f of functional dependencies  they allow us to generate all f +  the bibliographical notes provide references for proofs of soundness and completeness although armstrong ? s axioms are complete  it is tiresome to use them directly for the computation of f +  to simplify matters further  we list additional rules it is possible to use armstrong ? s axioms to prove that these rules are sound  see practice exercises 8.4 and 8.5 and exercise 8.26   ? union rule if  ?  holds and  ?  holds  then  ?   holds ? decomposition rule if  ?   holds  then  ?  holds and  ?  holds ? pseudotransitivity rule if  ?  holds and   ?  holds  then   ?  holds let us apply our rules to the example of schema r =  a  b  c  g  h  i  andthe set f of functional dependencies  a ? b  a ? c  cg ? h  cg ? i  b ? h   we list several members of f + here  thesumit67.blogspot.com 340 chapter 8 relational database design ? a ? h since a ? b and b ? h hold  we apply the transitivity rule observe that it was much easier to use armstrong ? s axioms to show that a ? h holds than it was to argue directly from the definitions  as we did earlier in this section ? cg ? hi since cg ? h and cg ? i  the union rule implies that cg ? hi  ? ag ? i since a ? c and cg ? i  the pseudotransitivity rule implies that ag ? i holds another way of finding that ag ? i holds is as follows  we use the augmentation rule on a ? c to infer ag ? cg applying the transitivity rule to this dependency and cg ? i  we infer ag ? i  figure 8.7 shows a procedure that demonstrates formally how to use armstrong ? s axioms to compute f +  in this procedure,when a functional dependency is added to f +  it may be already present  and in that case there is no change to f + .we shall see an alternative way of computing f + in section 8.4.2 the left-hand and right-hand sides of a functional dependency are both subsets of r since a set of size n has 2n subsets  there are a total of 2n ? 2n = 22n possible functional dependencies  where n is the number of attributes in r each iteration of the repeat loop of the procedure  except the last iteration  adds at least one functional dependency to f +  thus  the procedure is guaranteed to terminate 8.4.2 closure of attribute sets we say that an attribute b is functionally determined by  if  ? b to test whether a set  is a superkey  we must devise an algorithm for computing the set of attributes functionally determined by   one way of doing this is to compute f +  take all functional dependencies with  as the left-hand side  and take the union of the right-hand sides of all such dependencies however  doing so can be expensive  since f + can be large an efficient algorithm for computing the set of attributes functionally determined by  is useful not only for testing whether  is a superkey  but also for several other tasks  as we shall see later in this section f + = f repeat for each functional dependency f in f + apply reflexivity and augmentation rules on f add the resulting functional dependencies to f + for each pair of functional dependencies f1 and f2 in f + if f1 and f2 can be combined using transitivity add the resulting functional dependency to f + until f + does not change any further figure 8.7 a procedure to compute f +  thesumit67.blogspot.com 8.4 functional-dependency theory 341 result  =  ; repeat for each functional dependency  ?  in f do begin if  ? result then result  = result ?  ; end until  result does not change  figure 8.8 an algorithm to compute  +  the closure of  under f let  be a set of attributes we call the set of all attributes functionally determined by  under a set f of functional dependencies the closure of  under f ; we denote it by  +  figure 8.8 shows an algorithm  written in pseudocode  to compute  +  the input is a set f of functional dependencies and the set  of attributes the output is stored in the variable result to illustrate how the algorithm works  we shall use it to compute  ag  + with the functional dependencies defined in section 8.4.1 we start with result = ag the first time that we execute the repeat loop to test each functional dependency  we find that  ? a ? b causes us to include b in result to see this fact  we observe that a ? b is in f  a ? result  which is ag   so result  = result ? b ? a ? c causes result to become abcg ? cg ? h causes result to become abcgh ? cg ? i causes result to become abcghi the second time that we execute the repeat loop  no new attributes are added to result  and the algorithm terminates let us see why the algorithm of figure 8.8 is correct the first step is correct  since  ?  always holds  by the reflexivity rule  .we claim that  for any subset  of result   ?   since we start the repeat loop with  ? result being true  we can add  to result only if  ? result and  ?   but then result ?  by the reflexivity rule  so  ?  by transitivity another application of transitivity shows that  ?   using  ?  and  ?    the union rule implies that  ? result ?   so  functionally determines any new result generated in the repeat loop thus  any attribute returned by the algorithm is in  +  it is easy to see that the algorithm finds all of  +  if there is an attribute in  + that is not yet in result at any point during the execution  then there must be a functional dependency  ?  for which  ? result  and at least one attribute in  is not in result when the algorithm terminates  all such functional dependencies have been processed  and the attributes in  added to result ; we can thus be sure that all attributes in  + are in result thesumit67.blogspot.com 342 chapter 8 relational database design it turns out that  in the worst case  this algorithm may take an amount of time quadratic in the size of f there is a faster  although slightly more complex  algorithm that runs in time linear in the size of f ; that algorithm is presented as part of practice exercise 8.8 there are several uses of the attribute closure algorithm  ? to test if  is a superkey,wecompute  +  andcheck if  + contains all attributes in r ? we can check if a functional dependency  ?  holds  or  in other words  is in f +   by checking if  ?  +  that is  we compute  + by using attribute closure  and then check if it contains   this test is particularly useful  as we shall see later in this chapter ? it gives us an alternative way to compute f +  for each  ? r  we find the closure  +  and for each s ?  +  we output a functional dependency  ? s 8.4.3 canonical cover suppose that we have a set of functional dependencies f on a relation schema whenever a user performs an update on the relation  the database system must ensure that the update does not violate any functional dependencies  that is  all the functional dependencies in f are satisfied in the new database state the system must roll back the update if it violates any functional dependencies in the set f we can reduce the effort spent in checking for violations by testing a simplified set of functional dependencies that has the same closure as the given set any database that satisfies the simplified set of functional dependencies also satisfies the original set  and vice versa  since the two sets have the same closure.however  the simplified set is easier to test we shall see how the simplified set can be constructed in a moment first  we need some definitions an attribute of a functional dependency is said to be extraneous if we can remove it without changing the closure of the set of functional dependencies the formal definition of extraneous attributes is as follows  consider a set f of functional dependencies and the functional dependency  ?  in f ? attribute a is extraneous in  if a ?   and f logically implies  f    ?    ?     a  ?    ? attributeais extraneous in  if a ?   and the set of functional dependencies  f    ?    ?   ?    a   logically implies f for example  suppose we have the functional dependencies ab ? c and a ? c in f then  b is extraneous in ab ? c as another example  suppose we have the functional dependencies ab ? cd and a ? c in f then c would be extraneous in the right-hand side of ab ? cd beware of the direction of the implications when using the definition of extraneous attributes  if you exchange the left-hand side with the right-hand side  thesumit67.blogspot.com 8.4 functional-dependency theory 343 fc = f repeat use the union rule to replace any dependencies in fc of the form  1 ?  1 and  1 ?  2 with  1 ?  1  2 find a functional dependency  ?  in fc with an extraneous attribute either in  or in   / * note  the test for extraneous attributes is done using fc  not f * / if an extraneous attribute is found  delete it from  ?  in fc  until  fc does not change  figure 8.9 computing canonical cover the implication will always hold that is   f    ?    ?     a  ?   always logically implies f  and also f always logically implies  f    ?    ?   ?    a    here is how we can test efficiently if an attribute is extraneous let r be the relation schema  and let f be the given set of functional dependencies that hold on r consider an attribute ain a dependency  ?   ? if a ?   to check if ais extraneous  consider the set f =  f    ?    ?   ?    a   and check if  ? acan be inferred from f  todo so  compute  +  the closure of   under f ; if  + includes a  then ais extraneous in   ? if a ?   to check if ais extraneous  let  =    a   and check if  ?  can be inferred from f to do so  compute  +  the closure of   under f ; if  + includes all attributes in   then ais extraneous in   for example  suppose f contains ab ? cd  a ? e  and e ? c to check if c is extraneous in ab ? cd  we compute the attribute closure of ab under f =  ab ? d  a ? e  and e ? c   the closure is abcde  which includes cd  so we infer that c is extraneous a canonical cover fc for f is a set of dependencies such that f logically implies all dependencies in fc  and fc logically implies all dependencies in f furthermore  fc must have the following properties  ? no functional dependency in fc contains an extraneous attribute ? each left side of a functional dependency in fc is unique that is  there are no two dependencies  1 ?  1 and  2 ?  2 in fc such that  1 =  2 a canonical cover for a set of functional dependencies f can be computed as depicted in figure 8.9 it is important to note that when checking if an attribute is extraneous  the check uses the dependencies in the current value of fc  and not the dependencies in f if a functional dependency contains only one attribute thesumit67.blogspot.com 344 chapter 8 relational database design in its right-hand side  for example a ? c  and that attribute is found to be extraneous  we would get a functional dependency with an empty right-hand side such functional dependencies should be deleted the canonical cover of f  fc  can be shown to have the same closure as f ; hence  testing whether fc is satisfied is equivalent to testing whether f is satisfied however  fc is minimal in a certain sense ? it does not contain extraneous attributes  and it combines functional dependencies with the same left side it is cheaper to test fc than it is to test f itself consider the following set f of functional dependencies on schema  a  b,c   a ? bc b ? c a ? b ab ? c let us compute the canonical cover for f ? there are two functional dependencies with the same set of attributes on the left side of the arrow  a ? bc a ? b we combine these functional dependencies into a ? bc ? a is extraneous in ab ? c because f logically implies  f   ab ? c   ?  b ? c   this assertion is true because b ? c is already in our set of functional dependencies ? c is extraneous in a ? bc  since a ? bc is logically implied by a ? b and b ? c thus  our canonical cover is  a ? b b ? c given a set f of functional dependencies  it may be that an entire functional dependency in the set is extraneous  in the sense that dropping it does not change the closure of f we can show that a canonical cover fc of f contains no such extraneous functional dependency suppose that  to the contrary  there were such an extraneous functional dependency in fc  the right-side attributes of the dependency would then be extraneous  which is not possible by the definition of canonical covers a canonical cover might not be unique for instance  consider the set of functional dependencies f =  a ? bc  b ? ac  and c ? ab   if we apply thesumit67.blogspot.com 8.4 functional-dependency theory 345 the extraneity test to a ? bc  we find that both b and c are extraneous under f however  it is incorrect to delete both ! the algorithm for finding the canonical cover picks one of the two  and deletes it then  1 if c is deleted  we get the set f =  a ? b  b ? ac  andc ? ab   now  b is not extraneous in the side of a ? b under f  continuing the algorithm  we find aand b are extraneous in the right-hand side of c ? ab  leading to two canonical covers fc =  a ? b  b ? c  c ? a  fc =  a ? b  b ? ac  c ? b   2 if b is deleted  we get the set  a ? c  b ? ac  and c ? ab   this case is symmetrical to the previous case  leading to the canonical covers fc =  a ? c  c ? b  and b ? a  fc =  a ? c  b ? c  and c ? ab   as an exercise  can you find one more canonical cover for f ? 8.4.4 lossless decomposition let r  r  be a relation schema  and let f be a set of functional dependencies on r  r   let r1 and r2 form a decomposition of r.we say that the decomposition is a lossless decomposition if there is no loss of information by replacing r  r  with two relation schemas r1  r1  andr2  r2   more precisely  we say the decomposition is lossless if  for all legal database instances  that is  database instances that satisfy the specified functional dependencies and other constraints   relation r contains the same set of tuples as the result of the following sql query  select * from  select r1 from r  natural join  select r2 from r  this is stated more succinctly in the relational algebra as   r1  r    r2  r  = r in other words  if we project r onto r1 and r2  and compute the natural join of the projection results  we get back exactly r  a decomposition that is not a lossless decomposition is called a lossy decomposition the terms lossless-join decomposition and lossy-join decomposition are sometimes used in place of lossless decomposition and lossy decomposition as an example of a lossy decomposition  recall the decomposition of the employee schema into  thesumit67.blogspot.com 346 chapter 8 relational database design employee1  id  name  employee2  name  street  city  salary  that we saw earlier in section 8.1.2 as we saw in figure 8.3  the result of employee1  employee2 is a superset of the original relation employee  but the decomposition is lossy since the join result has lost information about which employee identifiers correspond to which addresses and salaries  in the case where two or more employees have the same name we can use functional dependencies to show when certain decompositions are lossless let r  r1  r2  and f be as above r1 and r2 form a lossless decomposition of r if at least one of the following functional dependencies is in f +  ? r1 n r2 ? r1 ? r1 n r2 ? r2 in other words  if r1 n r2 forms a superkey of either r1 or r2  the decomposition of r is a lossless decomposition.we can use attribute closure to test efficiently for superkeys  as we have seen earlier to illustrate this  consider the schema inst dept  id  name  salary  dept name  building  budget  that we decomposed in section 8.1.2 into the instructor and department schemas  instructor  id  name  dept name  salary  department  dept name  building  budget  consider the intersection of these two schemas  which is dept name we see that because dept name ? dept name  building  budget  the lossless-decomposition rule is satisfied for the general case of decomposition of a schema into multiple schemas at once  the test for lossless decomposition is more complicated see the bibliographical notes for references on the topic while the test for binary decomposition is clearly a sufficient condition for lossless decomposition  it is a necessary condition only if all constraints are functional dependencies we shall see other types of constraints later  in particular  a type of constraint called multivalued dependencies discussed in section 8.6.1   that can ensure that a decomposition is lossless even if no functional dependencies are present 8.4.5 dependency preservation using the theory of functional dependencies  it is easier to characterize dependency preservation than using the ad-hoc approach we took in section 8.3.3 thesumit67.blogspot.com 8.4 functional-dependency theory 347 let f be a set of functional dependencies on a schema r  and let r1  r2      rn be a decomposition of r the restriction of f to ri is the set fi of all functional dependencies in f + that include only attributes of ri  since all functional dependencies in a restriction involve attributes of only one relation schema  it is possible to test such a dependency for satisfaction by checking only one relation note that the definition of restriction uses all dependencies in f +  not just those in f for instance  suppose f =  a ? b  b ? c   and we have a decomposition into ac and ab the restriction of f to ac includes a ? c  since a ? c is in f +  even though it is not in f the set of restrictions f1  f2      fn is the set of dependencies that can be checked efficiently we now must ask whether testing only the restrictions is sufficient let f = f1 ? f2 ? ? ? ? ? fn f is a set of functional dependencies on schema r  but  in general  f  = f however  even if f  = f  it may be that f + = f +  if the latter is true  then every dependency in f is logically implied by f  and  if we verify that f is satisfied  we have verified that f is satisfied.we say that a decomposition having the property f + = f + is a dependency-preserving decomposition figure 8.10 shows an algorithm for testing dependency preservation the input is a set d =  r1  r2      rn  of decomposed relation schemas  and a set f of functional dependencies this algorithm is expensive since it requires computation of f +  instead of applying the algorithm of figure 8.10  we consider two alternatives first  note that if each member of f can be tested on one of the relations of the decomposition  then the decomposition is dependency preserving this is an easy way to show dependency preservation ; however  it does not always work there are cases where  even though the decomposition is dependency preserving  there is a dependency in f that can not be tested in any one relation in the decomposition thus  this alternative test can be used only as a sufficient condition that is easy compute f + ; for each schema ri in d do begin fi  = the restriction of f + to ri ; end f  = ? for each restriction fi do begin f = f ? fi end compute f + ; if  f + = f +  then return  true  else return  false  ; figure 8.10 testing for dependency preservation thesumit67.blogspot.com 348 chapter 8 relational database design to check ; if it fails we can not conclude that the decomposition is not dependency preserving ; instead we will have to apply the general test we nowgive a secondalternative test for dependency preservation that avoids computing f +  we explain the intuition behind the test after presenting the test the test applies the following procedure to each  ?  in f result =  repeat for each ri in the decomposition t =  result n ri  + n ri result = result ? t until  result does not change  the attribute closure here is under the set of functional dependencies f if result contains all attributes in   then the functional dependency  ?  is preserved the decomposition is dependency preserving if and only if the procedure shows that all the dependencies in f are preserved the two key ideas behind the above test are as follows  ? the first idea is to test each functional dependency  ?  in f to see if it is preserved in f  where f is as defined in figure 8.10   to do so  we compute the closure of  under f ; the dependency is preserved exactly when the closure includes   the decomposition is dependency preserving if  and only if  all the dependencies in f are found to be preserved ? the second idea is to use a modified form of the attribute-closure algorithm to compute closure under f ,without actually first computing f  wewish to avoid computing f since computing it is quite expensive note that f is the union of fi  where fi is the restriction of f on ri  the algorithm computes the attribute closure of  result n ri  with respect to f  intersects the closure with ri  and adds the resultant set of attributes to result ; this sequence of steps is equivalent to computing the closure of result under fi  repeating this step for each i inside the while loop gives the closure of result under f  to understand why this modified attribute-closure approach works correctly  we note that for any  ? ri   ?  + is a functional dependency in f +  and  ?  + n ri is a functional dependency that is in fi  the restriction of f + to ri  conversely  if  ?  were in fi  then  would be a subset of  + n ri  this test takes polynomial time  instead of the exponential time required to compute f +  8.5 algorithms for decomposition real-world database schemas are much larger than the examples that fit in the pages of a book for this reason,we need algorithms for the generation of designs thesumit67.blogspot.com 8.5 algorithms for decomposition 349 that are in appropriate normal form in this section  we present algorithms for bcnf and 3nf 8.5.1 bcnf decomposition the definition of bcnf can be used directly to test if a relation is in bcnf.however  computation of f + can be a tedious task we first describe below simplified tests for verifying if a relation is in bcnf if a relation is not in bcnf  it can be decomposed to create relations that are in bcnf later in this section  we describe an algorithm to create a lossless decomposition of a relation  such that the decomposition is in bcnf 8.5.1.1 testing for bcnf testing of a relation schema r to see if it satisfies bcnf can be simplified in some cases  ? to check if a nontrivial dependency  ?  causes a violation of bcnf  compute  +  the attribute closure of    and verify that it includes all attributes of r ; that is  it is a superkey of r ? to check if a relation schema r is in bcnf  it suffices to check only the dependencies in the given set f for violation of bcnf  rather than check all dependencies in f +  we can show that if none of the dependencies in f causes a violation of bcnf  then none of the dependencies in f + will cause a violation of bcnf  either unfortunately  the latter procedure does notworkwhen a relation is decomposed that is  it does not suffice to use f when we test a relation ri  in a decomposition of r  for violation of bcnf for example  consider relation schema r  a  b,c  d  e   with functional dependencies f containing a ? b and bc ? d suppose this were decomposed into r1  a  b  and r2  a,c  d  e   now  neither of the dependencies in f contains only attributes from  a,c  d  e  so we might be misled into thinking r2 satisfies bcnf in fact  there is a dependency ac ? d in f +  which can be inferred using the pseudotransitivity rule from the two dependencies in f  that shows that r2 is not in bcnf thus  we may need a dependency that is in f +  but is not in f  to show that a decomposed relation is not in bcnf an alternative bcnf test is sometimes easier than computing every dependency in f +  to check if a relation ri in a decomposition of r is in bcnf  we apply this test  ? for every subset  of attributes in ri  check that  +  the attribute closure of  under f  either includes no attribute of ri    or includes all attributes of ri  thesumit67.blogspot.com 350 chapter 8 relational database design result  =  r  ; done  = false ; compute f + ; while  not done  do if  there is a schema ri in result that is not in bcnf  then begin let  ?  be a nontrivial functional dependency that holds on ri such that  ? ri is not in f +  and  n  = ? ; result  =  result  ri  ?  ri    ?      ; end else done  = true ; figure 8.11 bcnf decomposition algorithm if the condition is violated by some set of attributes  in ri  consider the following functional dependency  which can be shown to be present in f +   ?   +    n ri  the above dependency shows that ri violates bcnf 8.5.1.2 bcnf decomposition algorithm we are now able to state a general method to decompose a relation schema so as to satisfy bcnf figure 8.11 shows an algorithm for this task if r is not in bcnf  we can decompose r into a collection of bcnf schemas r1  r2      rn by the algorithm the algorithm uses dependencies that demonstrate violation of bcnf to perform the decomposition the decomposition that the algorithm generates is not only in bcnf  but is also a lossless decomposition to see why our algorithm generates only lossless decompositions  we note that  when we replace a schema ri with  ri    and       the dependency  ?  holds  and  ri    n      =   if we did not require  n  = ?  then those attributes in  n  would not appear in the schema  ri    and the dependency  ?  would no longer hold it is easy to see that our decomposition of inst dept in section 8.3.2 would result from applying the algorithm the functional dependency dept name ? building  budget satisfies the  n  = ? condition and would therefore be chosen to decompose the schema the bcnf decomposition algorithm takes time exponential in the size of the initial schema  since the algorithm for checking if a relation in the decomposition satisfies bcnf can take exponential time the bibliographical notes provide references to an algorithm that can compute a bcnf decomposition in polynomial time.however  the algorithmmay ? overnormalize  ? that is  decompose a relation unnecessarily as a longer example of the use of the bcnf decomposition algorithm  suppose we have a database design using the class schema below  thesumit67.blogspot.com 8.5 algorithms for decomposition 351 class  course id  title  dept name  credits  sec id  semester  year  building  room number  capacity  time slot id  the set of functional dependencies that we require to hold on class are  course id ? title  dept name  credits building  room number ? capacity course id  sec id  semester  year ? building  room number  time slot id a candidate key for this schema is  course id  sec id  semester  year   we can apply the algorithm of figure 8.11 to the class example as follows  ? the functional dependency  course id ? title  dept name  credits holds  but course id is not a superkey thus  class is not in bcnf we replace class by  course  course id  title  dept name  credits  class-1  course id  sec id  semester  year  building  room number capacity  time slot id  the only nontrivial functional dependencies that hold on course include course id on the left side of the arrow since course id is a key for course  the relation course is in bcnf ? a candidate key for class-1 is  course id  sec id  semester  year   the functional dependency  building  room number ? capacity holds on class-1  but  building  room number  is not a superkey for class-1 we replace class-1 by  classroom  building  room number  capacity  section  course id  sec id  semester  year  building  room number  time slot id  classroom and section are in bcnf thus  the decomposition of class results in the three relation schemas course  classroom  and section  each of which is in bcnf these correspond to the schemas that we have used in this  and previous  chapters you can verify that the decomposition is lossless and dependency preserving thesumit67.blogspot.com 352 chapter 8 relational database design let fc be a canonical cover for f ; i  = 0 ; for each functional dependency  ?  in fc i  = i + 1 ; ri  =   ; if none of the schemas rj  j = 1  2      i contains a candidate key for r then i  = i + 1 ; ri  = any candidate key for r ; / * optionally  remove redundant relations * / repeat if any schema rj is contained in another schema rk then / * delete rj * / rj  = ri ; i  = i  1 ; until no more rjs can be deleted return  r1  r2      ri  figure 8.12 dependency-preserving  lossless decomposition into 3nf 8.5.2 3nf decomposition figure 8.12 shows an algorithm for finding a dependency-preserving  lossless decomposition into 3nf the set of dependencies fc used in the algorithm is a canonical cover for f note that the algorithm considers the set of schemas rj  j = 1  2      i ; initially i = 0  and in this case the set is empty let us apply this algorithm to our example of section 8.3.4  where we showed that  dept advisor  s id  i id  dept name  is in 3nf even though it is not in bcnf the algorithm uses the following functional dependencies in f  f1  i id ? dept name f2  s id  dept name ? i id there are no extraneous attributes in any of the functional dependencies in f  so fc contains f1 and f2 the algorithm then generates as r1 the schema   i id dept name   and as r2 the schema  s id  dept name  i id   the algorithm then finds that r2 contains a candidate key  so no further relation schema is created the resultant set of schemas can contain redundant schemas,with one schema rk containing all the attributes of another schema rj  for example  r2 above contains all the attributes from r1 the algorithm deletes all such schemas that are contained in another schema any dependencies that could be tested on an thesumit67.blogspot.com 8.5 algorithms for decomposition 353 rj that is deleted can also be tested on the corresponding relation rk  and the decomposition is lossless even if rj is deleted now let us consider again the class schema of section 8.5.1.2 and apply the 3nf decomposition algorithm the set of functional dependencies we listed there happen to be a canonical cover as a result  the algorithm gives us the same three schemas course  classroom  and section the above example illustrates an interesting property of the 3nf algorithm sometimes  the result is not only in 3nf  but also in bcnf this suggests an alternative method of generating a bcnf design first use the 3nf algorithm then  for any schema in the 3nf design that is not in bcnf  decompose using the bcnf algorithm if the result is not dependency-preserving  revert to the 3nf design 8.5.3 correctness of the 3nf algorithm the 3nf algorithm ensures the preservation of dependencies by explicitly building a schema for each dependency in a canonical cover it ensures that the decomposition is a lossless decomposition by guaranteeing that at least one schema contains a candidate key for the schema being decomposed practice exercise 8.14 provides some insight into the proof that this suffices to guarantee a lossless decomposition this algorithm is also called the 3nf synthesis algorithm  since it takes a set of dependencies and adds one schema at a time  instead of decomposing the initial schema repeatedly the result is not uniquely defined  since a set of functional dependencies can have more than one canonical cover  and  further  in some cases  the result of the algorithm depends on the order inwhich it considers the dependencies in fc  the algorithm may decompose a relation even if it is already in 3nf ; however  the decomposition is still guaranteed to be in 3nf if a relation ri is in the decomposition generated by the synthesis algorithm  then ri is in 3nf recall thatwhenwe test for 3nf it suffices to consider functional dependencies whose right-hand side is a single attribute therefore  to see that ri is in 3nf you must convince yourself that any functional dependency  ? b that holds on ri satisfies the definition of 3nf assume that the dependency that generated ri in the synthesis algorithm is  ?   now  b must be in  or   since b is in ri and  ?  generated ri  let us consider the three possible cases  ? b is in both  and   in this case  the dependency  ?  would not have been in fc since b would be extraneous in   thus  this case can not hold ? b is in  but not   consider two cases  ?  is a superkey the second condition of 3nf is satisfied ?  is not a superkey then  must contain some attribute not in   now  since  ? b is in f +  it must be derivable from fc by using the attribute closure algorithm on   the derivation could not have used  ?  ? if it had been used   must be contained in the attribute closure of   which is not possible  since we assumed  is not a superkey now  using  ?     b   and  ? b  we can derive  ? b  since  ?    and  thesumit67.blogspot.com 354 chapter 8 relational database design can not contain b because  ? b is nontrivial   this would imply that b is extraneous in the right-hand side of  ?   which is not possible since  ?  is in the canonical cover fc thus  if b is in   then  must be a superkey  and the second condition of 3nf must be satisfied ? b is in  but not   since  is a candidate key  the third alternative in the definition of 3nf is satisfied interestingly  the algorithm we described for decomposition into 3nf can be implemented in polynomial time  even though testing a given relation to see if it satisfies 3nf is np-hard  which means that it is very unlikely that a polynomialtime algorithm will ever be invented for this task   8.5.4 comparison of bcnf and 3nf of the two normal forms for relational database schemas  3nf and bcnf there are advantages to 3nf in that we know that it is always possible to obtain a 3nf design without sacrificing losslessness or dependency preservation nevertheless  there are disadvantages to 3nf  we may have to use null values to represent some of the possiblemeaningful relationships among data items  and there is the problem of repetition of information our goals of database design with functional dependencies are  1 bcnf 2 losslessness 3 dependency preservation since it is not always possible to satisfy all three  we may be forced to choose between bcnf and dependency preservation with 3nf it is worth noting that sql does not provide a way of specifying functional dependencies  except for the special case of declaring superkeys by using the primary key or unique constraints it is possible  although a little complicated  to write assertions that enforce a functional dependency  see practice exercise 8.9  ; unfortunately  currently no database system supports the complex assertions that are required to enforce a functional dependency  and the assertions would be expensive to test thus even if we had a dependency-preserving decomposition  if we use standard sql we can test efficiently only those functional dependencies whose left-hand side is a key although testing functional dependencies may involve a join if the decomposition is not dependency preserving  we could in principle reduce the cost by using materialized views  which many database systems support  provided the database system supports primary key constraints on materialized views given a bcnf decomposition that is not dependency preserving  we consider each dependency in a canonical cover fc that is not preserved in the decomposition for each such dependency  ?   we define a materialized view that computes a thesumit67.blogspot.com 8.6 decomposition using multivalued dependencies 355 join of all relations in the decomposition  and projects the result on    the functional dependency can be tested easily on the materialized view  using one of the constraints unique    or primary key     on the negative side  there is a space and time overhead due to the materialized view  but on the positive side  the application programmer need not worry about writing code to keep redundant data consistent on updates ; it is the job of the database system to maintain the materialized view  that is  keep it up to date when the database is updated  later in the book  in section 13.5  we outline how a database system can perform materialized view maintenance efficiently  unfortunately  most current database systems do not support constraints on materialized views although the oracle database does support constraints on materialized views  by default it performs view maintenance when the view is accessed  not when the underlying relation is updated ; 7 as a result a constraint violation may get detected well after the update has been performed  which makes the detection useless thus  in case we are not able to get a dependency-preserving bcnf decomposition  it is generally preferable to opt for bcnf  since checking functional dependencies other than primary key constraints is difficult in sql 8.6 decomposition using multivalued dependencies some relation schemas  even though they are in bcnf  do not seem to be sufficiently normalized  in the sense that they still suffer fromthe problem of repetition of information consider a variation of the university organization where an instructor may be associated with multiple departments inst  id  dept name  name  street  city  the astute reader will recognize this schema as a non-bcnf schema because of the functional dependency id ? name  street  city and because id is not a key for inst further assume that an instructor may have several addresses  say  a winter home and a summer home   then  we no longer wish to enforce the functional dependency ? id ? street  city ?  though  of course  we still want to enforce ? id ? name ?  that is  the university is not dealing with instructors who operate under multiple aliases !   following the bcnf decomposition algorithm  we obtain two schemas  7at least as of oracle version 10g thesumit67.blogspot.com 356 chapter 8 relational database design r1  id  name  r2  id  dept name  street  city  both of these are in bcnf  recall that an instructor can be associated with multiple departments and a department may have several instructors  and therefore  neither ? id ? dept name ? nor ? dept name ? id ? hold   despite r2 being in bcnf  there is redundancy.we repeat the address information of each residence of an instructor once for each department with which the instructor is associated we could solve this problem by decomposing r2 further into  r21  dept name  id  r22  id  street  city  but there is no constraint that leads us to do this to deal with this problem  we must define a new form of constraint  called a multivalued dependency as we did for functional dependencies  we shall use multivalued dependencies to define a normal form for relation schemas this normal form  called fourth normal form  4nf   is more restrictive than bcnf we shall see that every 4nf schema is also in bcnf but there are bcnf schemas that are not in 4nf 8.6.1 multivalued dependencies functional dependencies rule out certain tuples frombeing in a relation if a ? b  then we can not have two tuples with the same a value but different b values multivalued dependencies  on the other hand  do not rule out the existence of certain tuples instead  they require that other tuples of a certain form be present in the relation for this reason  functional dependencies sometimes are referred to as equality-generating dependencies  and multivalued dependencies are referred to as tuple-generating dependencies let r  r  be a relation schema and let  ? r and  ? r the multivalued dependency  ? ?  holds on r if  in any legal instance of relation r  r   for all pairs of tuples t1 and t2 in r such that t1    = t2     there exist tuples t3 and t4 in r such that t1    = t2    = t3    = t4    t3    = t1    t3  r    = t2  r    t4    = t2    t4  r    = t1  r    thesumit67.blogspot.com 8.6 decomposition using multivalued dependencies 357 a ? r ? a ? ? t1 t2 t3 t4 a1  ai a1  ai a1  ai a1  ai ai + 1  aj bi + 1  bj ai + 1  aj bi + 1  bj aj + 1  an bj + 1  bn bj + 1  bn aj + 1  an figure 8.13 tabular representation of  ? ?   this definition is less complicated than it appears to be figure 8.13 gives a tabular picture of t1  t2  t3  and t4 intuitively  the multivalued dependency  ? ?  says that the relationship between  and  is independent of the relationship between  and r   if the multivalued dependency  ? ?  is satisfied by all relations on schema r  then  ? ?  is a trivial multivalued dependency on schema r thus   ? ?  is trivial if  ?  or  ?  = r to illustrate the difference between functional and multivalued dependencies  we consider the schema r2 again  and an example relation on that schema shown in figure 8.14.we must repeat the department name once for each address that an instructor has  and we must repeat the address for each department with which an instructor is associated this repetition is unnecessary  since the relationship between an instructor and his address is independent of the relationship between that instructor and a department if an instructor with id 22222 is associated with the physics department,wewant that department to be associated with all of that instructor ? s addresses thus  the relation of figure 8.15 is illegal to make this relation legal  we need to add the tuples  physics  22222  main  manchester  and  math  22222  north  rye  to the relation of figure 8.15 comparing the preceding example with our definition of multivalued dependency  we see that we want the multivalued dependency  id ? ? street  city to hold  themultivalued dependency id ? ? dept name will do as well.we shall soon see that they are equivalent  as with functional dependencies  we shall use multivalued dependencies in two ways  1 to test relations to determine whether they are legal under a given set of functional and multivalued dependencies id dept name street city 22222 physics north rye 22222 physics main manchester 12121 finance lake horseneck figure 8.14 an example of redundancy in a relation on a bcnf schema thesumit67.blogspot.com 358 chapter 8 relational database design id dept name street city 22222 physics north rye 22222 math main manchester figure 8.15 an illegal r2 relation 2 to specify constraints on the set of legal relations ; we shall thus concern ourselves with only those relations that satisfy a given set of functional and multivalued dependencies note that  if a relation r fails to satisfy a given multivalued dependency  we can construct a relation r that does satisfy the multivalued dependency by adding tuples to r let d denote a set of functional and multivalued dependencies the closure d + of d is the set of all functional and multivalued dependencies logically implied by d as we did for functional dependencies  we can compute d + from d  using the formal definitions of functional dependencies and multivalued dependencies we can manage with such reasoning for very simple multivalued dependencies luckily  multivalued dependencies that occur in practice appear to be quite simple for complex dependencies  it is better to reason about sets of dependencies by using a system of inference rules from the definition of multivalued dependency  we can derive the following rules for    ? r  ? if  ?   then  ? ?   in other words  every functional dependency is also a multivalued dependency ? if  ? ?   then  ? ? r     appendix c.1.1 outlines a system of inference rules for multivalued dependencies 8.6.2 fourth normal form consider again our example of the bcnf schema  r2  id  dept name  street  city  in which the multivalued dependency ? id ? ? street  city ? holds we saw in the opening paragraphs of section 8.6 that  although this schema is in bcnf  the design is not ideal  since we must repeat an instructor ? s address information for each department.we shall see thatwe can use the given multivalued dependency to improve the database design  by decomposing this schema into a fourth normal form decomposition a relation schema r  r  is in fourth normal form  4nf  with respect to a set d of functional and multivalued dependencies if  for all multivalued dependencies thesumit67.blogspot.com 8.6 decomposition using multivalued dependencies 359 in d + of the form  ? ?   where  ? r and  ? r  at least one of the following holds  ?  ? ?  is a trivial multivalued dependency ?  is a superkey for r a database design is in 4nf if each member of the set of relation schemas that constitutes the design is in 4nf note that the definition of 4nf differs from the definition of bcnf in only the use of multivalued dependencies every 4nf schema is in bcnf to see this fact  we note that  if a schema r  r  is not in bcnf  then there is a nontrivial functional dependency  ?  holding on r  where  is not a superkey since  ?  implies  ? ?   r  r  can not be in 4nf let r  r  be a relation schema  and let r1  r1   r2  r2       rn  rn  be a decomposition of r  r   to check if each relation schema ri in the decomposition is in 4nf  we need to find what multivalued dependencies hold on each ri recall that  for a set f of functional dependencies  the restriction fi of f to ri is all functional dependencies in f + that include only attributes of ri  now consider a set d of both functional and multivalued dependencies the restriction of d to ri is the set di consisting of  1 all functional dependencies in d + that include only attributes of ri  2 all multivalued dependencies of the form   ? ?  n ri where  ? ri and  ? ?  is in d +  8.6.3 4nf decomposition the analogy between 4nf and bcnf applies to the algorithm for decomposing a schema into 4nf figure 8.16 shows the 4nf decomposition algorithm it is identical to the bcnf decomposition algorithm of figure 8.11  except that it uses multivalued dependencies and uses the restriction of d + to ri  if we apply the algorithm of figure 8.16 to  id  dept name  street  city   we find that id ? ? dept name is a nontrivial multivalued dependency  and id is not a superkey for the schema following the algorithm  we replace it by two schemas  r21  id  dept name  r22  id  street  city  this pair of schemas  which is in 4nf  eliminates the redundancy we encountered earlier as was the case when we were dealing solely with functional dependencies  we are interested in decompositions that are lossless and that preserve dependenthesumit67 blogspot.com 360 chapter 8 relational database design result  =  r  ; done  = false ; compute d + ; given schema ri  let di denote the restriction of d + to ri while  not done  do if  there is a schema ri in result that is not in 4nf w.r.t di  then begin let  ? ?  be a nontrivial multivalued dependency that holds on ri such that  ? ri is not in di  and  n  = ? ; result  =  result  ri  ?  ri    ?      ; end else done  = true ; figure 8.16 4nf decomposition algorithm cies the following fact about multivalued dependencies and losslessness shows that the algorithm of figure 8.16 generates only lossless decompositions  ? let r  r  be a relation schema  and letd be a set of functional and multivalued dependencies on r let r1  r1  and r2  r2  form a decomposition of r this decomposition is lossless of r if and only if at least one of the following multivalued dependencies is in d +  r1 n r2 ? ? r1 r1 n r2 ? ? r2 recall that we stated in section 8.4.4 that  if r1 n r2 ? r1 or r1 n r2 ? r2  then r1  r1  and r2  r2  are a lossless decomposition r  r   the preceding fact about multivalued dependencies is a more general statement about losslessness it says that  for every lossless decomposition of r  r  into two schemas r1  r1  and r2  r2   one of the two dependencies r1 n r2 ? ? r1 or r1 n r2 ? ? r2 must hold the issue of dependency preservationwhenwe decompose a relation schema becomes more complicated in the presence of multivalued dependencies appendix c.1.2 pursues this topic 8.7 more normal forms the fourth normal form is by no means the ? ultimate ? normal form as we saw earlier  multivalued dependencies help us understand and eliminate some forms of repetition of information that can not be understood in terms of functional dependencies there are types of constraints called join dependencies that generalize multivalued dependencies  and lead to another normal form called project-join normal form  pjnf   pjnf is called fifth normal form in some books   there is a class of even more general constraints that leads to a normal form called domain-key normal form  dknf   thesumit67.blogspot.com 8.8 database-design process 361 a practical problem with the use of these generalized constraints is that they are not only hard to reason with  but there is also no set of sound and complete inference rules for reasoning about the constraints hence pjnf and dknf are used quite rarely appendix c provides more details about these normal forms conspicuous by its absence from our discussion of normal forms is second normal form  2nf  .we have not discussed it  because it is of historical interest only.we simply define it  and let you experiment with it in practice exercise 8.17 8.8 database-design process so far we have looked at detailed issues about normal forms and normalization in this section  we study how normalization fits into the overall database-design process earlier in the chapter  starting in section 8.3  we assumed that a relation schema r  r  is given  and proceeded to normalize it there are several ways in which we could have come up with the schema r  r   1 r  r  could have been generated in converting an e-r diagram to a set of relation schemas 2 r  r  could have been a single relation schema containing all attributes that are of interest the normalization process then breaks up r  r  into smaller schemas 3 r  r  could have been the result of an ad-hoc design of relations that we then test to verify that it satisfies a desired normal form in the rest of this section,we examine the implications of these approaches.we also examine some practical issues in database design  including denormalization for performance and examples of bad design that are not detected by normalization 8.8.1 e-r model and normalization when we define an e-r diagram carefully  identifying all entities correctly  the relation schemas generated from the e-r diagram should not need much further normalization.however  there can be functional dependencies between attributes of an entity for instance  suppose an instructor entity set had attributes dept name and dept address  and there is a functional dependency dept name ? dept address we would then need to normalize the relation generated from instructor most examples of such dependencies arise out of poor e-r diagram design in the above example  if we had designed the e-r diagram correctly  we would have created a department entity set with attribute dept address and a relationship set between instructor and department similarly  a relationship set involving more than two entity sets may result in a schema that may not be in a desirable normal form since most relationship sets are binary  such cases are relatively rare  in thesumit67.blogspot.com 362 chapter 8 relational database design fact  some e-r-diagram variants actually make it difficult or impossible to specify nonbinary relationship sets  functional dependencies can help us detect poor e-r design if the generated relation schemas are not in desired normal form  the problemcan be fixed in the er diagram that is  normalization can be done formally as part of data modeling alternatively  normalization can be left to the designer ? s intuition during e-r modeling  and can be done formally on the relation schemas generated from the e-r model a careful reader will have noted that in order for us to illustrate a need for multivalued dependencies and fourth normal form,we had to beginwith schemas that were not derived from our e-r design indeed  the process of creating an e-r design tends to generate 4nf designs if a multivalued dependency holds and is not implied by the corresponding functional dependency  it usually arises from one of the following sources  ? a many-to-many relationship set ? a multivalued attribute of an entity set for a many-to-many relationship set each related entity set has its ownschema and there is an additional schema for the relationship set for a multivalued attribute  a separate schema is created consisting of that attribute and the primary key of the entity set  as in the case of the phone number attribute of the entity set instructor   the universal-relation approach to relational database design starts with an assumption that there is one single relation schema containing all attributes of interest this single schema defines how users and applications interact with the database 8.8.2 naming of attributes and relationships a desirable feature of a database design is the unique-role assumption  which means that each attribute name has a unique meaning in the database this prevents us from using the same attribute to mean different things in different schemas for example  we might otherwise consider using the attribute number for phone number in the instructor schema and for room number in the classroom schema the join of a relation on schema instructor with one on classroom is meaningless.while users and application developers canwork carefully to ensure use of the right number in each circumstance  having a different attribute name for phone number and for room number serves to reduce user errors while it is a good idea to keep names for incompatible attributes distinct  if attributes of different relations have the same meaning  it may be a good idea to use the same attribute name for this reason we used the same attribute name ? name ? for both the instructor and the student entity sets if this was not the case  that is  we used different naming conventions for the instructor and student names   then ifwewished to generalize these entity sets by creating a person entity set  we would have to rename the attribute thus  even if we did not currently thesumit67.blogspot.com 8.8 database-design process 363 have a generalization of student and instructor  if we foresee such a possibility it is best to use the same name in both entity sets  and relations   although technically  the order of attribute names in a schema does not matter  it is convention to list primary-key attributes first this makes reading default output  as from select *  easier in large database schemas  relationship sets  and schemas derived therefrom  are often named via a concatenation of the names of related entity sets  perhaps with an intervening hyphen or underscore we have used a few such names  for example inst sec and student sec we used the names teaches and takes instead of using the longer concatenated names this was acceptable since it is not hard for you to remember the associated entity sets for a few relationship sets.we can not always create relationship-set names by simple concatenation ; for example  a manager or works-for relationship between employees would not make much sense if itwere calledemployee employee ! similarly  if there are multiple relationship sets possible between a pair of entity sets  the relationship-set names must include extra parts to identify the relationship set different organizations have different conventions for naming entity sets for example,we may call an entity set of students student or students.we have chosen to use the singular form in our database designs using either singular or plural is acceptable  as long as the convention is used consistently across all entity sets as schemas grow larger  with increasing numbers of relationship sets  using consistent naming of attributes  relationships  and entities makes lifemuch easier for the database designer and application programmers 8.8.3 denormalization for performance occasionally database designers choose a schemathat has redundant information ; that is  it is not normalized they use the redundancy to improve performance for specific applications the penalty paid for not using a normalized schema is the extra work  in terms of coding time and execution time  to keep redundant data consistent for instance  suppose all course prerequisites have to be displayed along with a course information  every time a course is accessed in our normalized schema  this requires a join of course with prereq one alternative to computing the join on the fly is to store a relation containing all the attributes of course and prereq this makes displaying the ? full ? course information faster however  the information for a course is repeated for every course prerequisite  and all copiesmust be updated by the application  whenever a course prerequisite is added or dropped the process of taking a normalized schema and making it nonnormalized is called denormalization  and designers use it to tune performance of systems to support time-critical operations a better alternative  supported by many database systems today  is to use the normalized schema  and additionally store the join of course and prereq as a materialized view  recall that amaterialized view is a viewwhose result is stored in the database and brought up to date when the relations used in the view are updated  like denormalization  using materialized views does have space and thesumit67.blogspot.com 364 chapter 8 relational database design time overhead ; however  it has the advantage that keeping the view up to date is the job of the database system  not the application programmer 8.8.4 other design issues there are some aspects of database design that are not addressed by normalization  and can thus lead to bad database design data pertaining to time or to ranges of time have several such issues we give examples here ; obviously  such designs should be avoided consider a university database  where we want to store the total number of instructors in each department in different years a relation total inst  dept name  year  size  could be used to store the desired information the only functional dependency on this relation is dept name  year ? size  and the relation is in bcnf an alternative design is to use multiple relations  each storing the size information for a different year let us say the years of interest are 2007  2008  and 2009 ; we would then have relations of the form total inst 2007  total inst 2008  total inst 2009  all of which are on the schema  dept name  size   the only functional dependency here on each relation would be dept name ? size  so these relations are also in bcnf however  this alternative design is clearly a bad idea ? we would have to create a new relation every year  and we would also have to write new queries every year  to take each new relation into account queries would also be more complicated since they may have to refer to many relations yet another way of representing the same data is to have a single relation dept year  dept name  total inst 2007  total inst 2008  total inst 2009   here the only functional dependencies are from dept name to the other attributes  and again the relation is in bcnf this design is also a bad idea since it has problems similar to the previous design ? namely we would have to modify the relation schema andwrite newqueries every year querieswould also be more complicated  since they may have to refer to many attributes representations such as those in the dept year relation  with one column for each value of an attribute  are called crosstabs ; they are widely used in spreadsheets and reports and in data analysis tools while such representations are useful for display to users  for the reasons just given  they are not desirable in a database design sql includes features to convert data from a normal relational representation to a crosstab  for display  as we discussed in section 5.6.1 8.9 modeling temporal data suppose we retain data in our university organization showing not only the address of each instructor  but also all former addresses of which the university is aware we may then ask queries such as ? find all instructors who lived in princeton in 1981 ? in this case  we may have multiple addresses for instructors each address has an associated start and end date  indicating when the instructor was resident at that address a special value for the end date  e.g  null  or a value thesumit67.blogspot.com 8.9 modeling temporal data 365 well into the future such as 9999-12-31  can be used to indicate that the instructor is still resident at that address in general  temporal data are data that have an associated time interval during which they are valid.8 we use the term snapshot of data to mean the value of the data at a particular point in time thus a snapshot of course data gives the values of all attributes  such as title and department  of all courses at a particular point in time modeling temporal data is a challenging problem for several reasons for example  suppose we have an instructor entity setwithwhich we wish to associate a time-varying address to add temporal information to an address  we would then have to create a multivalued attribute  each of whose values is a composite value containing an address and a time interval in addition to time-varying attribute values  entities may themselves have an associated valid time for example  a student entity may have a valid time from the date the student entered the university to the date the student graduated  or left the university   relationships too may have associated valid times for example  the prereq relationship may record when a course became a prerequisite for another course we would thus have to add valid time intervals to attribute values  entity sets  and relationship sets adding such detail to an e-r diagram makes it very difficult to create and to comprehend there have been several proposals to extend the e-r notation to specify in a simple manner that an attribute value or relationship is time varying  but there are no accepted standards when we track data values across time  functional dependencies that we assumed to hold  such as  id ? street  city may no longer hold the following constraint  expressed in english  would hold instead  ? an instructor id has only one street and city value for any given time t ? functional dependencies that hold at a particular point in time are called temporal functional dependencies formally  a temporal functional dependency x  ? y holds on a relation schema r  r  if  for all legal instances of r  r   all snapshots of r satisfy the functional dependency x ? y we could extend the theory of relational database design to take temporal functional dependencies into account however  reasoning with regular functional dependencies is difficult enough already  and few designers are prepared to deal with temporal functional dependencies in practice  database designers fall back to simpler approaches to designing temporal databases one commonly used approach is to design the entire database  including e-r design and relational design  ignoring temporal changes  equivalently  taking only a snapshot into consideration   after this  the designer 8there are other models of temporal data that distinguish between valid time and transaction time  the latter recording when a fact was recorded in the database we ignore such details for simplicity thesumit67.blogspot.com 366 chapter 8 relational database design studies the various relations and decides which relations require temporal variation to be tracked the next step is to add valid time information to each such relation  by adding start and end time as attributes for example  consider the course relation the title of the course may change over time  which can be handled by adding a valid time range ; the resultant schema would be course  course id  title  dept name  start  end  an instance of this relation might have two records  cs-101  ? introduction to programming ?  1985-01-01  2000-12-31  and  cs-101  ? introduction to c ?  2001-01-01  9999-12-31   if the relation is updated by changing the course title to ? introduction to java  ? the time ? 9999-12-31 ? would be updated to the time untilwhich the old value  ? introduction to c ?  is valid  and a new tuple would be added containing the new title  ? introduction to java ?   with an appropriate start time if another relation had a foreign key referencing a temporal relation  the database designer has to decide if the reference is to the current version of the data or to the data as of a particular point in time for example  we may extend the department relation to track changes in the building or budget of a department across time  but a reference from the instructor or student relation may not care about the history of the building or budget  but may instead implicitly refer to the temporally current record for the corresponding dept name on the other hand  a record in a student ? s transcript should refer to the course title at the time when the student took the course in this latter case  the referencing relation must also record time information  to identify a particular record fromthe course relation in our example  the year and semester when the course was taken can be mapped to a representative time/date value such as midnight of the start date of the semester ; the resulting time/date value is used to identify a particular record from the temporal version of the course relation  from which the title is retrieved the original primary key for a temporal relation would no longer uniquely identify a tuple to resolve this problem  we could add the start and end time attributes to the primary key however  some problems remain  ? it is possible to store data with overlapping intervals  which the primary-key constraint would not catch if the system supports a native valid time type  it can detect and prevent such overlapping time intervals ? to specify a foreign key referencing such a relation  the referencing tuples would have to include the start and end-time attributes as part of their foreign key  and the values must match that in the referenced tuple further  if the referenced tuple is updated  and the end time which was in the future is updated   the update must propagate to all the referencing tuples if the system supports temporal data in a better fashion  we can allow the referencing tuple to specify a point in time  rather than a range  and rely on the system to ensure that there is a tuple in the referenced relation whose valid time interval contains the point in time for example  a transcript record thesumit67.blogspot.com 8.10 summary 367 may specify a course id and a time  say the start date of a semester   which is enough to identify the correct record in the course relation as a common special case  if all references to temporal data refer to only the current data  a simpler solution is to not add time information to the relation  but instead create a corresponding history relation that has temporal information  for past values for example  in our bank database  we could use the design we have created  ignoring temporal changes  to store only the current information all historical information is moved to historical relations thus  the instructor relation may store only the current address  while a relation instructor history may contain all the attributes of instructor  with additional start time and end time attributes although we have not provided any formal way to deal with temporal data  the issues we have discussed and the examples we have provided should help you in designing a database that records temporal data further issues in handling temporal data  including temporal queries  are covered later  in section 25.2 8.10 summary ? we showed pitfalls in database design  and how to systematically design a database schema that avoids the pitfalls the pitfalls included repeated information and inability to represent some information ? we showed the development of a relational database design from an e-r design  when schemas may be combined safely  and when a schema should be decomposed all valid decompositions must be lossless ? we described the assumptions of atomic domains and first normal form ? we introduced the concept of functional dependencies  and used it to present two normal forms  boyce ? codd normal form  bcnf  and third normal form  3nf   ? if the decomposition is dependency preserving  given a database update  all functional dependencies can be verifiable from individual relations  without computing a join of relations in the decomposition ? we showed how to reason with functional dependencies we placed special emphasis on what dependencies are logically implied by a set of dependencies we also defined the notion of a canonical cover,which is aminimal set of functional dependencies equivalent to a given set of functional dependencies ? we outlined an algorithm for decomposing relations into bcnf there are relations for which there is no dependency-preserving bcnf decomposition ? we used the canonical covers to decompose a relation into 3nf  which is a small relaxation of the bcnf condition relations in 3nf may have some redundancy  but there is always a dependency-preserving decomposition into 3nf thesumit67.blogspot.com 368 chapter 8 relational database design ? we presented the notion of multivalued dependencies  which specify constraints that can not be specified with functional dependencies alone we defined fourth normal form  4nf  with multivalued dependencies appendix c.1.1 gives details on reasoning about multivalued dependencies ? other normal forms  such as pjnf and dknf  eliminate more subtle forms of redundancy however  these are hard to work with and are rarely used appendix c gives details on these normal forms ? in reviewing the issues in this chapter  note that the reason we could define rigorous approaches to relational database design is that the relational data model rests on a firm mathematical foundation that is one of the primary advantages of the relationalmodel comparedwith the other data models that we have studied review terms ? e-r model and normalization ? decomposition ? functional dependencies ? lossless decomposition ? atomic domains ? first normal form  1nf  ? legal relations ? superkey ? r satisfies f ? f holds on r ? boyce ? codd normal form  bcnf  ? dependency preservation ? third normal form  3nf  ? trivial functional dependencies ? closure of a set of functional dependencies ? armstrong ? s axioms ? closure of attribute sets ? restriction of f to ri ? canonical cover ? extraneous attributes ? bcnf decomposition algorithm ? 3nf decomposition algorithm ? multivalued dependencies ? fourth normal form  4nf  ? restriction of a multivalued dependency ? project-join normal form  pjnf  ? domain-key normal form  dknf  ? universal relation ? unique-role assumption ? denormalization practice exercises 8.1 suppose that we decompose the schema r  a  b  c  d  e  into r1  a  b  c  r2  a  d  e  thesumit67.blogspot.com practice exercises 369 show that this decomposition is a lossless decomposition if the following set f of functional dependencies holds  a ? bc cd ? e b ? d e ? a 8.2 list all functional dependencies satisfied by the relation of figure 8.17 8.3 explain how functional dependencies can be used to indicate the following  ? a one-to-one relationship set exists between entity sets student and instructor ? a many-to-one relationship set exists between entity sets student and instructor 8.4 use armstrong ? s axioms to prove the soundness of the union rule  hint  use the augmentation rule to show that  if  ?   then  ?    apply the augmentation rule again  using  ?   and then apply the transitivity rule  8.5 use armstrong ? s axioms to prove the soundness of the pseudotransitivity rule 8.6 compute the closure of the following set f of functional dependencies for relation schema r  a  b  c  d  e   a ? bc cd ? e b ? d e ? a list the candidate keys for r 8.7 using the functional dependencies of practice exercise 8.6  compute the canonical cover fc  a b c a1 b1 c1 a1 b1 c2 a2 b1 c1 a2 b1 c3 figure 8.17 relation of practice exercise 8.2 thesumit67.blogspot.com 370 chapter 8 relational database design 8.8 consider the algorithm in figure 8.18 to compute  +  show that this algorithm is more efficient than the one presented in figure 8.8  section 8.4.2  and that it computes  + correctly 8.9 given the database schema r  a  b  c   and a relation r on the schema r  write an sql query to test whether the functional dependency b ? c holds on relation r also write an sql assertion that enforces the functional dependency ; assume that no null values are present  although part of the sql standard  such assertions are not supported by any database implementation currently  8.10 our discussion of lossless-join decomposition implicitly assumed that attributes on the left-hand side of a functional dependency can not take on null values what could go wrong on decomposition  if this property is violated ? 8.11 in the bcnf decomposition algorithm  suppose you use a functional dependency  ?  to decompose a relation schema r        into r1      and r2       a what primary and foreign-key constraint do you expect to hold on the decomposed relations ? b give an example of an inconsistency that can arise due to an erroneous update  if the foreign-key constraint were not enforced on the decomposed relations above c when a relation is decomposed into 3nf using the algorithm in section 8.5.2  what primary and foreign key dependencies would you expect will hold on the decomposed schema ? 8.12 let r1  r2      rn be a decomposition of schema u let u  u  be a relation  and let ri =  ri  u   show that u ? r1  r2  ? ? ?  rn 8.13 show that the decomposition in practice exercise 8.1 is not a dependencypreserving decomposition 8.14 show that it is possible to ensure that a dependency-preserving decomposition into 3nf is a lossless decomposition by guaranteeing that at least one schema contains a candidate key for the schema being decomposed  hint  show that the join of all the projections onto the schemas of the decomposition can not have more tuples than the original relation  8.15 give an example of a relation schema r and set f of functional dependencies such that there are at least three distinct lossless decompositions of r into bcnf thesumit67.blogspot.com practice exercises 371 result  = ? ; / * fdcount is an array whose ith element contains the number of attributes on the left side of the ith fd that are not yet known to be in  + * / for i  = 1 to | f | do begin let  ?  denote the ith fd ; fdcount  i   = |  | ; end / * appears is an array with one entry for each attribute the entry for attribute a is a list of integers each integer i on the list indicates that a appears on the left side of the ith fd * / for each attribute a do begin appears  a   = ni l ; for i  = 1 to | f | do begin let  ?  denote the ith fd ; if a ?  then add i to appears  a  ; end end addin    ; return  result  ; procedure addin    ; for each attribute a in  do begin if a  ? result then begin result  = result ?  a  ; for each element i of appears  a  do begin fdcount  i   = fdcount  i   1 ; if fdcount  i   = 0 then begin let  ?  denote the ith fd ; addin    ; end end end end figure 8.18 an algorithm to compute  +  thesumit67.blogspot.com 372 chapter 8 relational database design 8.16 let a prime attribute be one that appears in at least one candidate key let  and  be sets of attributes such that  ?  holds  but  ?  does not hold let a be an attribute that is not in   is not in   and for which  ? a holds we say that a is transitively dependent on   we can restate our definition of 3nf as follows  a relation schema r is in 3nf with respect to a set f of functional dependencies if there are no nonprime attributes a in r for which a is transitively dependent on a key for r show that this new definition is equivalent to the original one 8.17 a functional dependency  ?  is called a partial dependency if there is a proper subset  of  such that  ?  .we say that  is partially dependent on   a relation schema r is in second normal form  2nf  if each attribute a in r meets one of the following criteria  ? it appears in a candidate key ? it is not partially dependent on a candidate key show that every 3nf schema is in 2nf  hint  show that every partial dependency is a transitive dependency  8.18 give an example of a relation schema r and a set of dependencies such that r is in bcnf but is not in 4nf exercises 8.19 give a lossless-join decomposition into bcnf of schema r of practice exercise 8.1 8.20 give a lossless-join  dependency-preserving decomposition into 3nf of schema r of practice exercise 8.1 8.21 normalize the following schema  with given constraints  to 4nf books  accessionno  isbn  title  author  publisher  users  userid  name  deptid  deptname  accessionno ? isbn isbn ? title isbn ? publisher isbn ? ? author userid ? name userid ? deptid deptid ? deptname 8.22 explain what is meant by repetition of information and inability to represent information explain why each of these properties may indicate a bad relational database design thesumit67.blogspot.com exercises 373 8.23 why are certain functional dependencies called trivial functional dependencies ? 8.24 use the definition of functional dependency to argue that each of armstrong ? s axioms  reflexivity  augmentation  and transitivity  is sound 8.25 consider the following proposed rule for functional dependencies  if  ?  and  ?   then  ?   prove that this rule is not sound by showing a relation r that satisfies  ?  and  ?   but does not satisfy  ?   8.26 usearmstrong ? s axioms to prove the soundness of the decomposition rule 8.27 using the functional dependencies of practice exercise 8.6  compute b +  8.28 show that the following decomposition of the schema r of practice exercise 8.1 is not a lossless decomposition   a  b  c   c  d  e  hint  give an example of a relation r on schema r such that  a  b,c  r    c  d  e  r   = r 8.29 consider the following set f of functional dependencies on the relation schema r  a  b  c  d  e  f   a ? bcd bc ? de b ? d d ? a a compute b +  b prove  using armstrong ? s axioms  that af is a superkey c compute a canonical cover for the above set of functional dependencies f ; give each step of your derivation with an explanation d give a 3nf decomposition of r based on the canonical cover e give a bcnf decomposition of r using the original set of functional dependencies f can you get the same bcnf decomposition of r as above  using the canonical cover ? 8.30 list the three design goals for relational databases  and explain why each is desirable thesumit67.blogspot.com 374 chapter 8 relational database design 8.31 in designing a relational database  why might we choose a non-bcnf design ? 8.32 given the three goals of relational database design  is there any reason to design a database schema that is in 2nf  but is in no higher-order normal form ?  see practice exercise 8.17 for the definition of 2nf  8.33 given a relational schema r  a  b,c  d   does a ? ? bc logically imply a ? ? b and a ? ? c ? if yes prove it  else give a counter example 8.34 explain why 4nf is a normal form more desirable than bcnf bibliographical notes the first discussion of relational database design theory appeared in an early paper by codd  1970   in that paper  codd also introduced functional dependencies and first  second  and third normal forms armstrong ? s axioms were introduced in armstrong  1974   significant development of relational database theory occurred in the late 1970s these results are collected in several texts on database theory including maier  1983   atzeni and antonellis  1993   and abiteboul et al  1995   bcnf was introduced in codd  1972   biskup et al  1979  give the algorithm we used to find a lossless dependency-preserving decomposition into 3nf fundamental results on the lossless decomposition property appear in aho et al  1979a   beeri et al  1977  gives a set of axioms for multivalued dependencies  and proves that the authors ? axioms are sound and complete the notions of 4nf  pjnf  and dknf are from fagin  1977   fagin  1979   and fagin  1981   respectively see the bibliographical notes of appendix c for further references to literature on normalization jensen et al  1994  presents a glossary of temporal-database concepts a survey of extensions to e-r modeling to handle temporal data is presented by gregersen and jensen  1999   tansel et al  1993  covers temporal database theory  design  and implementation jensen et al  1996  describes extensions of dependency theory to temporal data thesumit67.blogspot.com chapter9 application design and development practically all use of databases occurs from within application programs correspondingly  almost all user interaction with databases is indirect  via application programs not surprisingly  therefore  database systems have long supported tools such as form and gui builders  which help in rapid development of applications that interface with users in recent years  the web has become the most widely used user interface to databases in this chapter  we study tools and technologies that are used to build applications  focussing on interactive applications that use databases to store data after an introduction to application programs and user interfaces in section 9.1,we focus on developing applicationswithweb-based interfaces.we start with an overview ofweb technologies in section 9.2  and discuss the java servlets technology  which is widely used for buildingweb applications  in section 9.3 a short overview of web application architectures in presented section 9.4 in section 9.5  we discuss tools for rapid application development  while in section 9.6 we cover performance issues in building largeweb applications in section 9.7  we discuss issues in application security we conclude the chapter with section 9.8  which covers encryption and its use in applications 9.1 application programs and user interfaces although many people interact with databases  very few people use a query language to interact with a database system directly the most common way in which users interact with databases is through an application program that provides a user interface at the front end  and interfaces with a database at the back end such applications take input from users  typically through a formsbased interface  and either enter data into a database or extract information from a database based on the user input  and generate output  which is displayed to the user as an example of an application  consider a university registration system like other such applications  the registration system first requires you to identify 375 thesumit67.blogspot.com 376 chapter 9 application design and development  a  mainframe era  b  personal computer era  c  web era web application server database internet terminals web browsers mainframe computer propietary network or dial up phone lines local area network desktop pcs database application program application program figure 9.1 application architectures in different eras and authenticate yourself  typically by a user name and password the application then uses your identity to extract information  such as your name and the courses for which you have registered  from the database and displays the information the application provides a number of interfaces that let you register for courses and query a variety of other information such as course and instructor information organizations use such applications to automate a variety of tasks  such as sales  purchases  accounting and payroll  human-resourcesmanagement  and inventory management  among many others application programsmay be used evenwhen it is not apparent that they are being used for example  a news site may provide a page that is transparently customized to individual users  even if the user does not explicitly fill any forms when interacting with the site to do so  it actually runs an application program that generates a customized page for each user ; customization can  for example  be based on the history of articles browsed by the user a typical application program includes a front-end component  which deals with the user interface  a back-end component  which communicates with a database  and a middle layer  which contains ? business logic  ? that is  code that executes specific requests for information or updates  enforcing rules of business such as what actions should be carried out to execute a given task  or who can carry out what task application architectures have evolved over time  as illustrated in figure 9.1 applications such as airline reservations have been around since the 1960s in the early days of computer applications  applications ran on a large ? mainframe ? computer  and users interacted with the application through terminals  some of which even supported forms with the widespread use of personal computers  many organizations used a different architecture for internal applications  with applications running on the user ? s computer  and accessing a central database this architecture  often called a ? client ? server ? architecture  allowed the creation of powerful graphical user interfaces  which earlier terminal-based applications did not support however  software had to be installed on each user ? smachine to run an application  making tasks such as upgrades harder even in the personal computer era  when client ? server architectures became popular  mainframe architecture continued to be the thesumit67.blogspot.com 9.2 web fundamentals 377 choice for applications such as airline reservations  which are used from a large number of geographically distributed locations in the past 15 years  web browsers have become the universal front end to database applications  connecting to the back end through the internet browsers use a standardized syntax  the hypertext markup language  html  standard  which supports both formatted display of information  and creation of formsbased interfaces the html standard is independent of the operating system or browser  and pretty much every computer today has a web browser installed thus a web-based application can be accessed from any computer that is connected to the internet unlike client ? server architectures  there is no need to install any applicationspecific software on client machines in order to use web-based applications however  sophisticated user interfaces  supporting features well beyond what is possible using plain html  are now widely used  and are built with the scripting language javascript  which is supported by most web browsers javascript programs  unlike programs written in c  can be run in a safe mode  guaranteeing they can not cause security problems javascript programs are downloaded transparently to the browser and do not need any explicit software installation on the user ? s computer while theweb browser provides the front end for user interaction  application programs constitute the back end typically  requests froma browser are sent to a web server,which in turn executes an application program to process the request a variety of technologies are available for creating application programs that run at the back end  including java servlets  java server pages  jsp   active server page  asp   or scripting languages such as php  perl  or python in the rest of this chapter,we describe how to build such applications  starting with web technologies and tools for building web interfaces  and technologies for building application programs  and then covering application architectures  and performance and security issues in building applications 9.2 web fundamentals in this section  we review some of the fundamental technology behind theworld wide web  for readers who are not familiar with the technology underlying the web 9.2.1 uniform resource locators a uniform resource locator  url  is a globally unique name for each document that can be accessed on the web an example of a url is  http  //www.acm.org/sigmod the first part of the url indicates how the document is to be accessed  ? http ? indicates that the document is to be accessed by the hypertext transfer protocol thesumit67.blogspot.com 378 chapter 9 application design and development < html > < body > < table border > < tr > < th > id < /th > < th > name < /th > < th > department < /th > < /tr > < tr > < td > 00128 < /td > < td > zhang < /td > < td > comp sci < /td > < /tr > < tr > < td > 12345 < /td > < td > shankar < /td > < td > comp sci < /td > < /tr > < tr > < td > 19991 < /td > < td > brandt < /td > < td > history < /td > < /tr > < /table > < /body > < /html > figure 9.2 tabular data in html format  http   which is a protocol for transferring html documents the second part gives the name of a machine that has aweb server the rest of the url is the path name of the file on the machine  or other unique identifier of a document within the machine a url can contain the identifier of a program located on the web server machine  as well as arguments to be given to the program an example of such a url is http  //www.google.com/search ? q = silberschatz which says that the program search on the server www.google.com should be executedwith the argument q = silberschatz.on receiving a request for such a url  the web server executes the program  using the given arguments the program returns an html document to the web server  which sends it back to the front end 9.2.2 hypertext markup language figure 9.2 is an example of a table represented in the htmlformat,while figure 9.3 shows the displayed image generated by a browser fromthe html representation of the table the html source shows a few of the html tags every html page should be enclosed in an html tag  while the body of the page is enclosed in a body tag a table is specified by a table tag  which contains rows specified by a tr tag the header row of the table has table cells specified by a th tag  while regular id 00128 12345 19991 zhang shankar brandt name comp sci comp sci history department figure 9.3 display of html source from figure 9.2 thesumit67.blogspot.com 9.2 web fundamentals 379 < html > < body > < form action = " personquery " method = get > search for  < select name = " persontype " > < option value = " student " selected > student < /option > < option value = " instructor " > instructor < /option > < /select > < br > name  < input type = text size = 20 name = " name " > < input type = submit value = " submit " > < /form > < /body > < /html > figure 9.4 an html form rows have table cells specified by a td tag we do not go into more details about the tags here ; see the bibliographic notes for references containing more detailed descriptions of html figure 9.4 shows how to specify an html form that allows users to select the person type  student or instructor  from a menu and to input a number in a text box figure 9.5 shows how the above form is displayed in a web browser two methods of accepting input are illustrated in the form  but html also supports several other input methods the action attribute of the form tag specifies that when the form is submitted  by clicking on the submit button   the form data should be sent to the url personquery  the url is relative to that of the page   theweb server is configured such thatwhen this url is accessed  a corresponding application program is invoked  with the user-provided values for the arguments persontype and name  specified in the select and input fields   the application program generates an html document  which is then sent back and displayed to the user ; we shall see how to construct such programs later in this chapter http defines two ways in which values entered by a user at the browser can be sent to theweb server the get method encodes the values as part of the url for example  if the google search page used a form with an input parameter named q with the get method  and the user typed in the string ? silberschatz ? and submitted the form  the browser would request the following url from the web server  http  //www.google.com/search ? q = silberschatz search for  name  student submit figure 9.5 display of html source from figure 9.4 thesumit67.blogspot.com 380 chapter 9 application design and development the post method would instead send a request for the url http  //www.google.com  and send the parameter values as part of the http protocol exchange between the web server and the browser the form in figure 9.4 specifies that the form uses the get method although html code can be created using a plain text editor  there are a number of editors that permit direct creation of html text by using a graphical interface such editors allow constructs such as forms  menus  and tables to be inserted into the html document from a menu of choices  instead of manually typing in the code to generate the constructs html supports stylesheets  which can alter the default definitions of how an html formatting construct is displayed  as well as other display attributes such as background color of the page the cascading stylesheet  css  standard allows the same stylesheet to be used for multiple html documents  giving a distinctive but uniform look to all the pages on a web site 9.2.3 web servers and sessions aweb server is a program running on the server machine,which accepts requests from aweb browser and sends back results in the form of html documents the browser and web server communicate via http web servers provide powerful features  beyond the simple transfer of documents the most important feature is the ability to execute programs  with arguments supplied by the user  and to deliver the results back as an html document as a result  a web server can act as an intermediary to provide access to a variety of information services a new service can be created by creating and installing an application program that provides the service the common gateway interface  cgi  standard defines how the web server communicates with application programs the application program typically communicates with a database server  through odbc  jdbc  or other protocols  in order to get or store data http browser server web server application server database server data network network figure 9.6 three-layer web application architecture thesumit67.blogspot.com 9.2 web fundamentals 381 http browser server data database server web server and application server network figure 9.7 two-layer web application architecture figure 9.6 shows aweb application built using a three-layer architecture  with a web server  an application server  and a database server using multiple levels of servers increases system overhead ; the cgi interface starts a new process to service each request  which results in even greater overhead most web applications today therefore use a two-layer web application architecture  where the application program runs within the web server  as in figure 9.7 we study systems based on the two-layer architecture in more detail in subsequent sections there is no continuous connection between the client and the web server ; when aweb server receives a request  a connection is temporarily created to send the request and receive the response from the web server but the connection may then be closed  and the next request could come over a new connection in contrast  when a user logs on to a computer  or connects to a database using odbc or jdbc  a session is created  and session information is retained at the server and the client until the session is terminated ? information such as the user-identifier of the user and session options that the user has set one important reason that http is connectionless is that most computers have limits on the number of simultaneous connections they can accommodate  and if a large number of sites on the web open connections to a single server  this limit would be exceeded  denying service to further users with a connectionless protocol  the connection can be broken as soon as a request is satisfied  leaving connections available for other requests.1 most web applications  however  need session information to allow meaningful user interaction for instance  applications typically restrict access to information  and therefore need to authenticate users authentication should be 1for performance reasons  connections may be kept open for a short while  to allow subsequent requests to reuse the connection however  there is no guarantee that the connection will be kept open  and applications must be designed assuming the connection may be closed as soon as a request is serviced thesumit67.blogspot.com 382 chapter 9 application design and development done once per session  and further interactions in the session should not require reauthentication to implement sessions in spite of connections getting closed  extra information has to be stored at the client and returned with each request in a session ; the server uses this information to identify that a request is part of a user session extra information about the session also has to be maintained at the server this extra information is usually maintained in the form of a cookie at the client ; a cookie is simply a small piece of text containing identifying information and with an associated name for example  google.com may set a cookie with the name prefs  which encodes preferences set by the user such as the preferred language and the number of answers displayed per page on each search request  google.com can retrieve the cookie named prefs from the user ? s browser  and display results according to the specified preferences a domain  web site  is permitted to retrieve only cookies that it has set  not cookies set by other domains  and cookie names can be reused across domains for the purpose of tracking a user session  an application may generate a session identifier  usually a random number not currently in use as a session identifier   and send a cookie named  for instance  sessionid containing the session identifier the session identifier is also stored locally at the server when a request comes in  the application server requests the cookie named sessionid from the client if the client does not have the cookie stored  or returns a value that is not currently recorded as a valid session identifier at the server  the application concludes that the request is not part of a current session if the cookie value matches a stored session identifier  the request is identified as part of an ongoing session if an application needs to identify users securely  it can set the cookie only after authenticating the user ; for example a user may be authenticated only when a valid user name and password are submitted.2 for applications that do not require high security  such as publicly available news sites  cookies can be stored permanently at the browser and at the server ; they identify the user on subsequent visits to the same site,without any identification information being typed in for applications that require higher security  the server may invalidate  drop  the session after a time-out period  or when the user logs out  typically a user logs out by clicking on a logout button  which submits a logout form  whose action is to invalidate the current session  invalidating a session merely consists of dropping the session identifier from the list of active sessions at the application server 2the user identifier could be stored at the client end  in a cookie named  for example  userid such cookies can be used for low-security applications  such as free web sites identifying their users however  for applications that require a higher level of security  such a mechanism creates a security risk  the value of a cookie can be changed at the browser by a malicious user  who can then masquerade as a different user setting a cookie  named sessionid  for example  to a randomly generated session identifier  from a large space of numbers  makes it highly improbable that a user can masquerade as  that is  pretend to be  another user a sequentially generated session identifier  on the other hand  would be susceptible to masquerading thesumit67.blogspot.com 9.3 servlets and jsp 383 9.3 servlets and jsp in a two-layer web architecture  an application runs as part of the web server itself one way of implementing such an architecture is to load java programs into the web server the java servlet specification defines an application programming interface for communication between the web server and the application program the httpservlet class in java implements the servlet api specification ; servlet classes used to implement specific functions are defined as subclasses of this class.3 often the word servlet is used to refer to a java program  and class  that implements the servlet interface figure 9.8 shows a servlet example ; we explain it in detail shortly the code for a servlet is loaded into theweb serverwhen the server is started  or when the server receives a remote http request to execute a particular servlet the task of a servlet is to process such a request  which may involve accessing a database to retrieve necessary information  and dynamically generate an html page to be returned to the client browser 9.3.1 a servlet example servlets are commonly used to generate dynamic responses to http requests they can access inputs provided through html forms  apply ? business logic ? to decide what response to provide  and then generate html output to be sent back to the browser figure 9.8 shows an example of servlet code to implement the form in figure 9.4 the servlet is called personqueryservlet  while the form specifies that ? action = " personquery "  ? the web server must be told that this servlet is to be used to handle requests for personquery the form specifies that the http get mechanism is used for transmitting parameters so the doget   method of the servlet  as defined in the code  is invoked each request results in a new thread within which the call is executed  so multiple requests can be handled in parallel any values from the form menus and input fields on the web page  as well as cookies  pass through an object of the httpservletrequest class that is created for the request  and the reply to the request passes through an object of the class httpservletresponse the doget   method in the example extracts values of the parameter ? s type and number by using request.getparameter    and uses these values to run a query against a database the code used to access the database and to get attribute values from the query result is not shown ; refer to section 5.1.1.4 for details of how to use jdbc to access a database the servlet code returns the results of the query to the requester by outputting them to the httpservletresponse object response outputting the results is to response is implemented by first getting a printwriter object out from response  and then printing the result in html format to out 3the servlet interface can also support non-http requests  although our example uses only http thesumit67.blogspot.com 384 chapter 9 application design and development import java.io * ; import javax.servlet * ; import javax.servlet.http * ; public class personqueryservlet extends httpservlet  public void doget  httpservletrequest request  httpservletresponse response  throws servletexception  ioexception  response.setcontenttype  " text/html "  ; printwriter out = response.getwriter   ; out.println  " < head > < title > query result < /title > < /head > "  ; out.println  " < body > "  ; string persontype = request.getparameter  " persontype "  ; string number = request.getparameter  " name "  ; if  persontype.equals  " student "     code to find students with the specified name   using jdbc to communicate with the database  out.println  " < table border cols = 3 > "  ; out.println  " < tr > < td > id < /td > < td > name  < /td > " + " < td > department < /td > < /tr > "  ; for   each result     retrieve id  name and dept name  into variables id  name and deptname out.println  " < tr > < td > " + id + " < /td > " + " < td > " + name + " < /td > " + " < td > " + deptname + " < /td > < /tr > "  ;  ; out.println  " < /table > "  ;  else   as above  but for instructors   out.println  " < /body > "  ; out.close   ;   figure 9.8 example of servlet code 9.3.2 servlet sessions recall that the interaction between a browser and aweb server is stateless that is  each time the browser makes a request to the server  the browser needs to connect to the server  request some information  then disconnect from the server cookies can be used to recognize that a request is from the same browser session as an thesumit67.blogspot.com 9.3 servlets and jsp 385 earlier request.however  cookies form a low-level mechanism  and programmers require a better abstraction to deal with sessions the servlet api provides a method of tracking a session and storing information pertaining to it invocation of the method getsession  false  of the class httpservletrequest retrieves the httpsession object corresponding to the browser that sent the request an argument value of true would have specified that a new session object must be created if the request is a new request when the getsession   method is invoked  the server first asks the client to return a cookie with a specified name if the client does not have a cookie of that name  or returns a value that does not match any ongoing session  then the request is not part of an ongoing session in this case  getsession   would return a null value  and the servlet could direct the user to a login page the login page could allow the user to provide a user name and password the servlet corresponding to the login page could verify that the password matches the user  for example  by looking up authentication information in the database   if the user is properly authenticated  the login servlet would execute getsession  true   which would return a new session object to create a new session theweb server would internally carry out the following tasks  set a cookie  called  for example  sessionid  with a session identifier as its associated value at the client browser  create a newsession object  and associate the session identifier value with the session object the servlet code can also store and look up  attribute-name  value  pairs in the httpsession object  to maintain state across multiple requests within a session for example  after the user is authenticated and the session object has been created  the login servlet could store the user-id of the user as a session parameter by executing the method session.setattribute  ? userid ?  userid  on the session object returned by getsession   ; the java variable userid is assumed to contain the user identifier if the request was part of an ongoing session  the browser would have returned the cookie value  and the corresponding session object is returned by getsession    the servlet can then retrieve session parameters such as user-id from the session object by executing the method session.getattribute  ? userid ?  on the session object returned above if the attribute userid is not set  the function would return a null value  which would indicate that the client user has not been authenticated 9.3.3 servlet life cycle the life cycle of a servlet is controlled by theweb server in which the servlet has been deployed when there is a client request for a specific servlet  the server thesumit67.blogspot.com 386 chapter 9 application design and development first checks if an instance of the servlet exists or not if not  the web server loads the servlet class into the java virtual machine  jvm   and creates an instance of the servlet class in addition  the server calls the init   method to initialize the servlet instance notice that each servlet instance is initialized only once when it is loaded after making sure the servlet instance does exist  the server invokes the service method of the servlet  with a request object and a response object as parameters by default  the server creates a new thread to execute the service method ; thus  multiple requests on a servlet can execute in parallel  without having to wait for earlier requests to complete execution the service method calls doget or dopost as appropriate whenno longer required  a servlet can be shut down by calling the destroy   method the server can be set up to automatically shut down a servlet if no requests have been made on a servlet within a time-out period ; the time-out period is a server parameter that can be set as appropriate for the application 9.3.4 servlet support many application servers provide built-in support for servlets one of the most popular is the tomcat server from the apache jakarta project other application servers that support servlets include glassfish  jboss  beaweblogic application server  oracle application server  and ibm ? s websphere application server the bestway to develop servlet applications is by using an ide such as eclipse or netbeans  which come with tomcat or glassfish servers built in application servers usually provide a variety of useful services  in addition to basic servlet support they allow applications to be deployed or stopped  and provide functionality to monitor the status of the application server  including performance statistics if a servlet file is modified  some application servers can detect this and recompile and reload the servlet transparently many application servers also allow the server to run on multiple machines in parallel to improve performance  and route requests to an appropriate copy many application servers also support the java 2 enterprise edition  j2ee  platform  which provides support and apis for a variety of tasks  such as for handling objects  parallel processing across multiple application servers  and for handling xml data  xml is described later in chapter 23   9.3.5 server-side scripting writing even a simple web application in a programming language such as java or c is a time-consuming task that requires many lines of code and programmers who are familiar with the intricacies of the language an alternative approach  that of server-side scripting  provides a much easier method for creating many applications scripting languages provide constructs that can be embeddedwithin html documents in server-side scripting  before delivering a web page  the server executes the scripts embeddedwithin the html contents of the page each piece of script,when executed  can generate text that is added to the page  ormay even delete content from the page   the source code of the scripts is removed thesumit67.blogspot.com 9.3 servlets and jsp 387 < html > < head > < title > hello < /title > < /head > < body > < % if  request.getparameter  ? name ?  = = null   out.println  ? hello world ?  ;  else  out.println  ? hello  ? + request.getparameter  ? name ?   ;  % > < /body > < /html > figure 9.9 a jsp page with embedded java code from the page  so the client may not even be aware that the page originally had any code in it the executed script may contain sql code that is executed against a database some of the widely used scripting languages include java server pages  jsp  from sun  active server pages  asp  and its successor asp.net from microsoft  the php scripting language  the coldfusion markup language  cfml   and ruby on rails many scripting languages also allow code written in languages such as java  c #  vbscript  perl  and python to be embedded into or invoked from html pages for instance  jsp allows java code to be embedded in html pages  while microsoft ? s asp.net and asp support embedded c # and vbscript many of these languages come with libraries and tools  that together constitute a framework for web application development we briefly describe below java server pages  jsp   a scripting language that allows html programmers to mix static html with dynamically generated html the motivation is that  for many dynamicweb pages  most of their content is still static  that is  the same content is present whenever the page is generated   the dynamic content of the web pages  which are generated  for example  on the basis of form parameters  is often a small part of the page creating such pages by writing servlet code results in a large amount of html being coded as java strings jsp instead allows java code to be embedded in static html ; the embedded java code generates the dynamic part of the page jsp scripts are actually translated into servlet code that is then compiled  but the application programmer is saved the trouble of writing much of the java code to create the servlet figure 9.9 shows the source text of an jsp page that includes embedded java code the java code in the script is distinguished from the surrounding html code by being enclosed in < %    % >  the code uses request.getparameter   to get the value of the attribute name when a jsp page is requested by a browser  the application server generates html output from the page  which is sent back to the browser the html part of the jsp page is output as is.4 wherever java code is embedded within < %    % >  4jsp allows a more complex embedding  where html code is within a java if-else statement  and gets output conditionally depending on whether the if condition evaluates to true or not.we omit details here thesumit67.blogspot.com 388 chapter 9 application design and development php php is a scripting language that is widely used for server-side scripting php code can be intermixed with html in a manner similar to jsp the characters ? < ? php ? indicate the start of php code  while the characters ? ? > ? indicate the end of php code the following code performs the same actions as the jsp code in figure 9.9 < html > < head > < title > hello < /title > < /head > < body > < ? php if  ! isset  $ request  ? name ?     echo ? hello world ? ;  else  echo ? hello  ?  $ request  ? name ?  ;  ? > < /body > < /html > the array $ request contains the request parameters note that the array is indexed by the parameter name ; in php arrays can be indexed by arbitrary strings  not just numbers the function isset checks if the element of the array has been initialized the echo function prints its argument to the output html the operator ?  ? between two strings concatenates the strings a suitably configuredweb server would interpret any file whose name ends in ? .php ? to be a php file if the file is requested  the web server process it in a manner similar to how jsp files are processed  and returns the generated html to the browser a number of libraries are available for the php language  including libraries for database access using odbc  similar to jdbc in java   the code is replaced in the html output by the text it prints to the object out in the jsp code in the above figure  if no value was entered for the form parameter name  the script prints ? hello world ? ; if a value was entered  the script prints ? hello ? followed by the name a more realistic example may perform more complex actions  such as looking up values from a database using jdbc jsp also supports the concept of a tag library  which allows the use of tags that look much like html tags  but are interpreted at the server  and are replaced by appropriately generated html code jsp provides a standard set of tags that define variables and control flow  iterators  if-then-else   along with an expression language based on javascript  but interpreted at the server   the set of tags is extensible  and a number of tag libraries have been implemented for example  there is a tag library that supports paginated display of large data sets  and a library that simplifies display and parsing of dates and times see the bibliographic notes for references to more information on jsp tag libraries thesumit67.blogspot.com 9.3 servlets and jsp 389 9.3.6 client-side scripting embedding of program code in documents allows web pages to be active  carrying out activities such as animation by executing programs at the local site  instead of just presenting passive text and graphics the primary use of such programs is flexible interaction with the user  beyond the limited interaction power provided by html and html forms further  executing programs at the client site speeds up interaction greatly  compared to every interaction being sent to a server site for processing a danger in supporting such programs is that  if the design of the system is done carelessly  program code embedded in a web page  or equivalently  in an email message  can perform malicious actions on the user ? s computer the malicious actions could range from reading private information  to deleting or modifying information on the computer  up to taking control of the computer and propagating the code to other computers  through email  for example   a number of email viruses have spread widely in recent years in this way one of the reasons that the java language became very popular is that it provides a safe mode for executing programs on users ? computers java code can be compiled into platform-independent ? byte-code ? that can be executed on any browser that supports java unlike local programs  java programs  applets  downloaded as part of aweb page have no authority to perform any actions that could be destructive they are permitted to display data on the screen  or tomake a network connection to the server from which the web page was downloaded  in order to fetch more information however  they are not permitted to access local files  to execute any system programs  or to make network connections to any other computers while java is a full-fledged programming language  there are simpler languages  called scripting languages  that can enrich user interaction  while providing the same protection as java these languages provide constructs that can be embedded with an html document client-side scripting languages are languages designed to be executed on the client ? sweb browser of these  the javascript language is by far the most widely used the current generation of web interfaces uses the javascript scripting language extensively to construct sophisticated user interfaces javascript is used for a variety of tasks for example  functions written in javascript can be used to perform error checks  validation  on user input  such as a date string being properly formatted  or a value entered  such as age  being in an appropriate range these checks are carried out on the browser as data is entered  even before the data are sent to the web server figure 9.10 shows an example of a javascript function used to validate a form input the function is declared in the head section of the html document the function checks that the credits entered for a course is a number greater than 0  and less than 16 the form tag specifies that the validation function is to be invoked when the form is submitted if the validation fails  an alert box is shown to the user  and if it succeeds  the form is submitted to the server javascript can be used to modify dynamically the html code being displayed the browser parses html code into an in-memory tree structure defined by thesumit67.blogspot.com 390 chapter 9 application design and development < html > < head > < script type = " text/javascript " > function validate    var credits = document.getelementbyid  " credits "  .value ; if  isnan  credits  | | credits < = 0 | | credits > = 16   alert  " credits must be a number greater than 0 and less than 16 "  ; return false   < /script > < /head > < body > < form action = " createcourse " onsubmit = " return validate   " > title  < input type = " text " id = " title " size = " 20 " > < br / > credits  < input type = " text " id = " credits " size = " 2 " > < br / > < input type = " submit " value = " submit " > < /form > < /body > < /html > figure 9.10 example of javascript used to validate form input a standard called the document object model  dom   javascript code can modify the tree structure to carry out certain operations for example  suppose a user needs to enter a number of rows of data  for example multiple items in a single bill a table containing text boxes and other form input methods can be used to gather user input the table may have a default size  but if more rows are needed  the user may click on a button labeled  for example  ? add item ? this button can be set up to invoke a javascript function that modifies the dom tree by adding an extra row in the table although the javascript language has been standardized  there are differences between browsers  particularly in the details of the dom model as a result  javascript code that works on one browser may not work on another to avoid such problems  it is best to use a javascript library  such as yahoo ? s yui library  which allows code to be written in a browser independent way internally  the functions in the library can find out which browser is in use  and send appropriately generated javascript to the browser see the tools section at the end of the chapter for more information on yui and other libraries today  javascript is widely used to create dynamicweb pages  using several technologies that are collectively called ajax programs written in javascript communicate with the web server asynchronously  that is  in the background  without blocking user interaction with theweb browser   and can fetch data and display it thesumit67.blogspot.com 9.4 application architectures 391 as an example of the use of ajax  consider aweb site with a form that allows you to select a country  and once a country has been selected  you are allowed to select a state from a list of states in that country until the country is selected  the drop-down list of states is empty the ajax framework allows the list of states to be downloaded from theweb site in the background when the country is selected  and as soon as the list has been fetched  it is added to the drop-down list  which allows you to select the state there are also special-purpose scripting languages for specialized tasks such as animation  for example  flash and shockwave  and three-dimensional modeling  virtual reality markup language  vrml    flash is very widely used today not only for animation  but also for handling streaming video content 9.4 application architectures to handle their complexity  large applications are often broken into several layers  ? the presentation or user interface layer  which deals with user interaction a single application may have several different versions of this layer  corresponding to distinct kinds of interfaces such as web browsers  and user interfaces of mobile phones  which have much smaller screens in many implementations  the presentation/user-interface layer is itself conceptually broken up into layers  based on the model-view-controller  mvc  architecture the model corresponds to the business-logic layer  described below the view defines the presentation of data ; a single underlying model can have different views depending on the specific software/device used to access the application the controller receives events  user actions   executes actions on the model  and returns a view to the user the mvc architecture is used in a number of web application frameworks  which are discussed later in section 9.5.2 ? the business-logic layer  which provides a high-level view of data and actions on data we discuss the business-logic layer in more detail in section 9.4.1 ? the data access layer  which provides the interface between the business-logic layer and the underlying database many applications use an object-oriented language to code the business-logic layer  and use an object-oriented model of data  while the underlying database is a relational database in such cases  the data-access layer also provides the mapping fromthe object-oriented data model used by the business logic to the relational model supported by the database.we discuss such mappings in more detail in section 9.4.2 figure 9.11 shows the above layers  along with a sequence of steps taken to process a request from the web browser the labels on the arrows in the figure indicate the order of the steps when the request is received by the application server  the controller sends a request to the model the model processes the thesumit67.blogspot.com 392 chapter 9 application design and development web browser internet 1 8 7 6 5 2 3 4 database web/application server view controller model data access layer figure 9.11 web application architecture request  using business logic  which may involve updating objects that are part of the model  followed by creating a result object the model in turn uses the dataaccess layer to update or retrieve information from a database the result object created by the model is sent to the view module  which creates an html view of the result  to be displayed on the web browser the view may be tailored based on the characteristics of the device used to view the result  for example whether it is a computer monitor with a large screen  or a small screen on a phone 9.4.1 the business-logic layer the business-logic layer of an application for managing a university may provide abstractions of entities such as students  instructors  courses  sections  etc  and actions such as admitting a student to the university  enrolling a student in a course  and so on the code implementing these actions ensures that business rules are satisfied ; for example the code would ensure that a student can enroll for a course only if she has already completed course prerequisites  and has paid her tuition fees in addition  the business logic includes workflows  which describe how a particular task that involves multiple participants is handled for example  if a candidate applies to the university  there is a workflow that defines who should see and approve the application first  and if approved in the first step  who should see the application next  and so on until either an offer is made to the student  or a rejection note is sent out.workflow management also needs to deal with error situations ; for example  if a deadline for approval/rejection is not met  a supervisor may need to be informed so she can intervene and ensure the application is processed.workflows are discussed in more detail in section 26.2 thesumit67.blogspot.com 9.4 application architectures 393 9.4.2 the data-access layer and object-relational mapping in the simplest scenario  where the business-logic layer uses the same datamodel as the database  the data-access layer simply hides the details of interfacing with the database however  when the business-logic layer is written using an objectoriented programming language  it is natural to model data as objects  with methods invoked on objects in early implementations  programmers had towrite code for creating objects by fetching data from the database  and for storing updated objects back in the database however  such manual conversions between data models is cumbersome and error prone one approach to handling this problem was to develop a database system that natively stores objects  and relationships between objects such databases  called object-oriented databases  are discussed in more detail in chapter 22 however  object-oriented databases did not achieve commercial success for a variety of technical and commercial reasons an alternative approach is to use traditional relational databases to store data  but to automate the mapping of data in relations to in-memory objects  which are created on demand  since memory is usually not sufficient to store all data in the database   as well as the reverse mapping to store updated objects back as relations in the database several systems have been developed to implement such object-relational mappings the hibernate system is widely used for mapping from java objects to relations in hibernate  the mapping from each java class to one or more relations is specified in a mapping file the mapping file can specify  for example  that a java class called student is mapped to the relation student  with the java attribute id mapped to the attribute student.id  and so on information about the database  such as the host on which it is running  and user name and password for connecting to the database  etc  are specified in a properties file the program has to open a session  which sets up the connection to the database once the session is set up  a student object stud created in java can be stored in the database by invoking session.save  stud   the hibernate code generates the sql commands required to store corresponding data in the student relation a list of objects can be retrieved from the database by executing a query written in the hibernate query language ; this is similar to executing a query using jdbc  which returns a resultset containing a set of tuples alternatively  a single object can be retrieved by providing its primary key the retrieved objects can be updated inmemory ; when the transaction on the ongoing hibernate session is committed  hibernate automatically saves the updated objects by making corresponding updates on relations in the database while entities in an e-r model naturally correspond to objects in an objectoriented language such as java  relationships often do not hibernate supports the ability to map such relationships as sets associated with objects for example  the takes relationship between student and section can be modeled by associating a set of sections with each student  and a set of students with each section once the appropriate mapping is specified  hibernate populates these sets automatically from the database relation takes  and updates to the sets are reflected back to the database relation on commit thesumit67.blogspot.com 394 chapter 9 application design and development hibernate example as an example of the use of hibernate  we create a java class corresponding to the student relation as follows public class student  string id ; string name ; string department ; int tot cred ; student  string id  string name  string dept  int totcreds  ; // constructor  to be precise  the class attributes should be declared as private  and getter/setter methods should be provided to access the attributes  but we omit these details the mapping of the class attributes of student to attributes of the relation student is specified in a mapping file  in an xml format again  we omit details the following code snippet then creates a student object and saves it to the database session session = getsessionfactory   .opensession   ; transaction txn = session.begintransaction   ; student stud = new student  " 12328 "  " john smith "  " comp sci "  0  ; session.save  stud  ; txn.commit   ; session.close   ; hibernate automatically generates the required sql insert statement to create a student tuple in the database to retrieve students  we can use the following code snippet session session = getsessionfactory   .opensession   ; transaction txn = session.begintransaction   ; list students = session.find  " from student as s order by s.id asc "  ; for  iterator iter = students.iterator   ; iter.hasnext   ;   student stud =  student  iter.next   ;  print out the student information   txn.commit   ; session.close   ; the above code snippet uses a query in hibernate ? s hql query language the hql query is automatically translated to sql by hibernate and executed  and the results are converted into a list of student objects the for loop iterates over the objects in this list and prints them out thesumit67.blogspot.com 9.4 application architectures 395 the above features help to provide the programmer a high-level model of data without bothering about the details of the relational storage.however,hibernate  like other object-relational mapping systems  also allows programmers direct sql access to the underlying relations such direct access is particularly important for writing complex queries used for report generation microsoft has developed a data model called the entity data model  which can be viewed as a variant of the entity-relationship model  and an associated framework called the ado.net entity framework  which can map data between the entity data model and a relational database the framework also provides an sql-like query language called entity sql  which operates directly on the entity data model 9.4.3 web services in the past  most web applications used only data available at the application server and its associated database in recent years  a wide variety of data is available on the web that is intended to be processed by a program  rather than displayed directly to the user ; the program may be running as part of a back-end application  or may be a script running in the browser such data are typically accessed using what is in effect aweb application programming interface ; that is  a function call request is sent using the http protocol  executed at an application server  and the results sent back to the calling program a system that supports such an interface is called a web service two approaches are widely used to implement web services in the simpler approach  called representation state transfer  or rest   web service function calls are executed by a standard http request to a url at an application server  with parameters sent as standard http request parameters the application server executes the request  which may involve updating the database at the server   generates and encodes the result  and returns the result as the result of the http request the server can use any encoding for a particular requested url ; xml  and an encoding of javascript objects called javascript object notation  json   are widely used the requestor parses the returned page to access the returned data in many applications of such restfulweb services  that is,web services using rest   the requestor is javascript code running in aweb browser ; the code updates the browser screen using the result of the function call for example  when you scroll the display on amap interface on theweb  the part of the map that needs to be newly displayed may be fetched by javascript code using a restful interface  and then displayed on the screen a more complex approach  sometimes referred to as ? bigweb services  ? uses xml encoding of parameters as well as results  has a formal definition of theweb api using a special language  and uses a protocol layer built on top of the http protocol this approach is described in more detail in section 23.7.3 9.4.4 disconnected operation many applications wish to support some operations even when a client is disconnected from the application server for example  a student may wish to fill thesumit67.blogspot.com 396 chapter 9 application design and development an application form even if her laptop is disconnected from the network  but have it saved back when the laptop is reconnected as another example  if an email client is built as a web application  a user may wish to compose an email even if her laptop is disconnected from the network  and have it sent when it is reconnected building such applications requires local storage  preferably in the form of a database  in the client machine the gears software  originally developed by google  is a browser plug-in that provides a database  a local web server  and support for parallel execution of javascript at the client the software works identically on multiple operating system/browser platforms  allowing applications to support rich functionalitywithout installation of any software  other than gears itself   adobe ? s air software also provides similar functionality for building internet applications that can run outside the web browser 9.5 rapid application development if web applications are built without using tools or libraries for constructing the user interface  the programming effort required to construct the user interface can be significantly more than that required for business logic and database access several approaches have been developed to reduce the effort required to build applications  ? provide a library of functions to generate user-interface elements with minimal programming ? provide drag-and-drop features in an integrated development environment that allows user-interface elements to be dragged from a menu into a design view of a page the integrated development environment generates code that creates the user-interface element by invoking library functions ? automatically generate code for the user interface from a declarative specification all these approaches have been used for creating user interfaces  well before the web was created  as part of rapid application development  rad  tools  and are now used extensively for creating web applications as well examples of tools designed for rapid development of interfaces for database applications include oracle forms  sybase powerbuilder  and oracle application express  apex   in addition  tools designed for web application development  such asvisual studio and netbeansvisualweb  support several features designed for rapid development ofweb interfaces for database backed applications we study tools for construction of user interfaces in section 9.5.1  and study frameworks that support automatic code generation from a system model  in section 9.5.2 9.5.1 tools for building user interfaces many html constructs are best generated by using appropriately defined functions  instead of being written as part of the code of eachweb page for example  thesumit67.blogspot.com 9.5 rapid application development 397 address forms typically require a menu containing country or state names instead of writing lengthy html code to create the required menu each time it is used  it is preferable to define a function that outputs the menu  and to call the function wherever required menus are often best generated from data in the database  such as a table containing country names or state names the function generating the menu executes a database query and populates the menu  using the query result adding a country or state then requires only a change to the database  not to the application code this approach has the potential drawback of requiring increased database interaction  but such overhead can be minimized by caching query results at the application server forms to input dates and times  or inputs that require validation  are similarly best generated by calling appropriately defined functions such functions can output javascript code to perform validation at the browser displaying a set of results from a query is a common task for many database applications it is possible to build a generic function that takes an sql query  or resultset  as argument  and display the tuples in the query result  or resultset  in a tabular form jdbc metadata calls can be used to find information such as the number of columns and the names and types of the columns in the query result ; this information is then used to display the query result to handle situations where the query result is very large  such a query result display function can provide for pagination of results the function can display a fixed number of records in a page and provide controls to step to the next or previous page or jump to a particular page of the results there is unfortunately no  widely used  standard java api for functions to carry out the user-interface tasks described above building such a library can be an interesting programming project however  there are other tools  such as the javaserver faces  jsf  framework  that support the features listed above the jsf framework includes a jsp tag library that implements these features the netbeans ide has a component called visualweb that builds on jsf  providing a visual development environmentwhere user interface components can be dragged and dropped into a page  and their properties customized for example  jsf provides components to create dropdown menus  or display a table  which can be configured to get their data from a database query jsf also supports validation specification on components  for example to make a selection or input mandatory  or to constrain a number or a date to be in a specified range microsoft ? s active server pages  asp   and its more recent version  active server pages.net  asp.net   is a widely used alternative to jsp/java asp.net is similar to jsp  in that code in a language such as visual basic or c # can be embedded within html code in addition  asp.net provides a variety of controls  scripting commands  that are interpreted at the server  and generate html that is then sent to the client these controls can significantly simplify the construction of web interfaces.we provide a brief overview of the benefits that these controls offer thesumit67.blogspot.com 398 chapter 9 application design and development for example  controls such as drop-downmenus and list boxes can be associated with a dataset object the dataset object is similar to a jdbc resultset object  and is typically created by executing a query on the database the html menu contents are then generated from the dataset object ? s contents ; for example  a query may retrieve the names of all departments in an organization into the dataset  and the associated menu would contain these names thus  menus that depend on database contents can be created in a convenient manner with very little programming validator controls can be added to form input fields ; these declaratively specify validity constraints such as value ranges  or whether the input is a required input for which a value must be provided by the user the server creates appropriate html code combined with javascript to perform the validation at the user ? s browser error messages to be displayed on invalid input can be associated with each validator control user actions can be specified to have an associated action at the server for example  a menu control can specify that selecting a value from a menu has an associated server-side action  javascript code is generated to detect the selection event and initiate the server-side action   visual basic/c # code that displays data pertaining to the selected value can be associated with the action at the server thus  selecting a value from a menu can result in associated data on the page getting updated  without requiring the user to click on a submit button the datagrid control provides a very convenient way of displaying query results a datagrid is associated with a dataset object  which is typically the result of a query the server generates html code that displays the query result as a table column headings are generated automatically from query result metadata in addition  datagrids provide several features  such as pagination  and allow the user to sort the result on chosen columns all the html code as well as server-side functionality to implement these features is generated automatically by the server the datagrid even allows users to edit the data and submit changes back to the server the application developer can specify a function  to be executed when a row is edited  that can perform the update on the database microsoft visual studio provides a graphical user interface for creating asp pages using these features  further reducing the programming effort see the bibliographic notes for references to more information on asp.net 9.5.2 web application frameworks there are a variety of web application development frameworks that provide several commonly used features such as  ? an object-oriented model with an object-relational mapping to store data in a relational database  as we saw in section 9.4.2   ? a  relatively  declarativeway of specifying a formwith validation constraints on user inputs  from which the system generates html and javascript/ajax code to implement the form thesumit67.blogspot.com 9.5 rapid application development 399 ? a template scripting system  similar to jsp   ? a controller that maps user interaction events such as form submits to appropriate functions that handle the event the controller also manages authentication and sessions some frameworks also provide tools for managing authorizations thus  these frameworks provide a variety of features that are required to build web applications  in an integratedmanner by generating forms from declarative specifications  andmanaging data access transparently  the frameworks minimize the amount of coding that a web application programmer has to carry out there are a large number of such frameworks  based on different languages some of the more widely used frameworks include ruby on rails  which is based on the ruby programming language  jboss seam  apache struts  swing  tapestry  andwebobjects  all based on java/jsp some of these  such as ruby on rails and jboss seam provide a tool that can automatically create simple crud web interfaces ; that is  interfaces that support create  read  update and delete of objects/tuples  by generating code from an object model or a database such tools are particularly useful to get simple applications running quickly  and the generated code can be edited to build more sophisticatedweb interfaces 9.5.3 report generators report generators are tools to generate human-readable summary reports from a database they integrate querying the database with the creation of formatted text and summary charts  such as bar or pie charts   for example  a report may show the total sales in each of the past 2 months for each sales region the application developer can specify report formats by using the formatting facilities of the report generator variables can be used to store parameters such as the month and the year and to define fields in the report tables  graphs  bar charts  or other graphics can be defined via queries on the database the query definitions can make use of the parameter values stored in the variables once we have defined a report structure on a report-generator facility  we can store it and can execute it at any time to generate a report report-generator systems provide a variety of facilities for structuring tabular output  such as defining table and column headers  displaying subtotals for each group in a table  automatically splitting long tables into multiple pages  and displaying subtotals at the end of each page figure 9.12 is an example of a formatted report the data in the report are generated by aggregation on information about orders report-generation tools are available froma variety of vendors  such as crystal reports and microsoft  sql server reporting services   several application suites  such as microsoft office  provide a way of embedding formatted query results from a database directly into a document chart-generation facilities provided by crystal reports  or by spreadsheets such as excel can be used to access data from databases  and generate tabular depictions of data or graphical depictions using charts or graphs such charts can be embeddedwithin text documents thesumit67.blogspot.com 400 chapter 9 application design and development region category sales north computer hardware 1,000,000 computer so  ware 500,000 all categories 1,500,000 south computer hardware 200,000 computer so  ware 400,000 all categories 600,000 2,100,000 acme supply company  inc quarterly sales report period  jan 1 to march 31  2009 total sales subtotal figure 9.12 a formatted report the charts are created initially from data generated by executing queries against the database ; the queries can be re-executed and the charts regenerated when required  to generate a current version of the overall report in addition to generating static reports  report-generation tools support the creation of interactive reports for example  a user can ? drill down ? into areas of interest  for example move froman aggregate view showing the total sales across an entire year to the monthly sales figures for a particular year such operations were discussed earlier  in section 5.6 9.6 application performance web sites may be accessed by millions of people from across the globe  at rates of thousands of requests per second  or even more  for themost popular sites ensuring that requests are served with low response times is a major challenge forweb site developers to do so  application developers try to speed up the processing of individual requests by using techniques such as caching  and exploit parallel processing by using multiple application servers we describe these techniques briefly below tuning of database applications is described in more detail later  in chapter 24  section 24.1   9.6.1 reducing overhead by caching caching techniques of various types are used to exploit commonalities between transactions for instance  suppose the application code for servicing each user request needs to contact a database through jdbc creatinganewjdbc connection may take several milliseconds  so opening a new connection for each user request is not a good idea if very high transaction rates are to be supported thesumit67.blogspot.com 9.6 application performance 401 the connection pooling method is used to reduce this overhead ; it works as follows the connection pool manager  a part of the application server  creates a pool  that is  a set  of open odbc/jdbc connections instead of opening a new connection to the database  the code servicing a user request  typically a servlet  asks for  requests  a connection from the connection pool and returns the connection to the pool when the code  servlet  completes its processing if the pool has no unused connections when a connection is requested  a new connection is opened to the database  taking care not to exceed the maximum number of connections that the database system can support concurrently   if there are many open connections that have not been used for a while  the connection pool manager may close some of the open database connections many application servers  and newer odbc/jdbc drivers provide a built-in connection pool manager a common error that many programmers make when creating web applications is to forget to close an opened jdbc connection  or equivalently  when connection pooling is used  to forget to return the connection to the connection pool   each request then opens a newconnection to the database  and the database soon reaches the limit of how many open connections it can have at a time such problems often do not show up on small-scale testing  since databases often allow hundreds of open connections  but show up only on intensive usage some programmers assume that connections  like memory allocated by java programs  are garbage collected automatically unfortunately  this does not happen  and programmers are responsible for closing connections that they have opened certain requestsmay result in exactly the same query being resubmitted to the database the cost of communication with the database can be greatly reduced by caching the results of earlier queries and reusing them  so long as the query result has not changed at the database some web servers support such query-result caching ; caching can otherwise be done explicitly in application code costs can be further reduced by caching the final web page that is sent in response to a request if a new request comeswith exactly the same parameters as a previous request  the request does not perform any updates  and the resultant web page is in the cache  then it can be reused to avoid the cost of recomputing the page caching can be done at the level of fragments of web pages  which are then assembled to create completeweb pages cached query results and cachedweb pages are forms of materialized views if the underlying database data change  the cached results must be discarded  or recomputed  or even incrementally updated  as in materialized-viewmaintenance  described later  in section 13.5   some database systems  such as microsoft sql server  provide a way for the application server to register a query with the database  and get a notification from the database when the result of the query changes such a notification mechanism can be used to ensure that query results cached at the application server are up-to-date 9.6.2 parallel processing a commonly used approach to handling such very heavy loads is to use a large number of application servers running in parallel  each handling a fraction of the thesumit67.blogspot.com 402 chapter 9 application design and development requests.aweb server or a network router can be used to route each client request to one of the application servers all requests froma particular client session must go to the same application server  since the server maintains state for a client session this property can be ensured  for example  by routing all requests froma particular ip address to the same application server the underlying database is  however  shared by all the application servers  so that users see a consistent view of the database with the above architecture  the database could easily become the bottleneck  since it is shared application designers pay particular attention to minimizing the number of requests to the database  by caching query results at the application server  as discussed earlier in addition  parallel database systems  described in chapter 18  are used when required 9.7 application security application security has to deal with several security threats and issues beyond those handled by sql authorization the first point where security has to be enforced is in the application to do so  applications must authenticate users  and ensure that users are only allowed to carry out authorized tasks there are many ways in which an application ? s security can be compromised  even if the database system is itself secure  due to badly written application code in this section  we first describe several security loopholes that can permit hackers to carry out actions that bypass the authentication and authorization checks carried out by the application  and explain howto prevent such loopholes later in the section  we describe techniques for secure authentication  and for finegrained authorization we then describe audit trails that can help in recovering from unauthorized access and from erroneous updates we conclude the section by describing issues in data privacy 9.7.1 sql injection in sql injection attacks  the attacker manages to get an application to execute an sql query created by the attacker in section 5.1.1.4  we saw an example of an sql injection vulnerability if user inputs are concatenated directly with an sql query and submitted to the database as another example of sql injection vulnerability  consider the form source text shown in figure 9.4 suppose the corresponding servlet shown in figure 9.8 creates an sql query string using the following java expression  string query = ? select * from student where name like ? % ? + name + ? % ? ? where name is a variable containing the string input by the user  and then executes the query on the database a malicious attacker using theweb form can then type thesumit67.blogspot.com 9.7 application security 403 a string such as ? ? ; < some sql statement > ;  ?  where < some sql statement > denotes any sql statement that the attacker desires  in place of a valid student name the servlet would then execute the following string select * from student where name like ? ? ; < some sql statement > ;  ? the quote inserted by the attacker closes the string  the following semicolon terminates the query  and the following text inserted by the attacker gets interpreted as a second sql query,while the closing quote has been commented out thus  the malicious user has managed to insert an arbitrary sql statement that is executed by the application the statement can cause significant damage  since it can perform any action on the database  bypassing all security measures implemented in the application code as discussed in section 5.1.1.4  to avoid such attacks  it is best to use prepared statements to execute sql queries when setting a parameter of a prepared query  jdbc automatically adds escape characters so that the user-supplied quote would no longer be able to terminate the string equivalently  a function that adds such escape characters could be applied on input strings before they are concatenated with the sql query  instead of using prepared statements another source of sql-injection risk comes from applications that create queries dynamically  based on selection conditions and ordering attributes specified in a form for example  an application may allow a user to specify what attribute should be used for sorting the results of a query an appropriate sql query is constructed  based on the attribute specified suppose the application takes the attribute name from a form  in the variable orderattribute  and creates a query string such as the following  string query = ? select * from takes order by ? + orderattribute ; a malicious user can send an arbitrary string in place of a meaningful orderattribute value  even if the html form used to get the input tried to restrict the allowed values by providing a menu to avoid this kind of sql injection  the application should ensure that the orderattribute variable value is one of the allowed values  in our example  attribute names   before appending it 9.7.2 cross site scripting and request forgery aweb site that allows users to enter text  such as a comment or a name  and then stores it and later displays it to other users  is potentially vulnerable to a kind of attack called a cross-site scripting  xss  attack in such an attack  a malicious user enters code written in a client-side scripting language such as javascript or flash instead of entering a valid name or comment when a different user views the entered text  the browser would execute the script  which can carry out actions such as sending private cookie information back to the malicious user  or even executing an action on a differentweb server that the user may be logged into thesumit67.blogspot.com 404 chapter 9 application design and development for example  suppose the user happens to be logged into her bank account at the time the script executes the script could send cookie information related to the bank account login back to themalicious user  who could use the information to connect to the bank ? sweb server  fooling it into believing that the connection is from the original user or  the script could access appropriate pages on the bank ? s web site  with appropriately set parameters  to execute a money transfer in fact this particular problem can occur even without scripting by simply using a line of code such as < img src = " http  //mybank.com/transfermoney ? amount = 1000&toaccount = 14523 " > assuming that the url mybank.com/transfermoney accepts the specified parameters  and carries out a money transfer this latter kind of vulnerability is also called cross-site request forgery or xsrf  sometimes also called csrf   xss can be done in other ways  such as luring a user into visiting a web site that has malicious scripts embedded in its pages there are other more complex kinds of xss and xsrf attacks  which we shall not get into here to protect against such attacks  two things need to be done  ? prevent yourweb site from being used to launch xss or xsrf attacks the simplest technique is to disallow any html tagswhatsoever in text input by users there are functions that detect  or strip all such tags these functions can be used to prevent html tags  and as a result  any scripts  from being displayed to other users in some cases html formatting is useful  and in that case functions that parse the text and allow limited html constructs  but disallow other dangerous constructs can be used instead ; these must be designed carefully  since something as innocuous as an image include could potentially be dangerous in case there is a bug in the image display software that can be exploited ? protect yourweb site from xss or xsrf attacks launched from other sites if the user has logged into your web site  and visits a different web site vulnerable to xss  the malicious code executing on the user ? s browser could execute actions on yourweb site  or pass session information related to your web site back to the malicious user who could try to exploit it this can not be prevented altogether  but you can take a few steps to minimize the risk ? the http protocol allows a server to check the referer of a page access  that is  the url of the page that had the link that the user clicked on to initiate the page access by checking that the referer is valid  for example  that the referer url is a page on the same web site  xss attacks that originated on a differentweb page accessed by the user can be prevented ? instead of using only the cookie to identify a session  the session could also be restricted to the ip address from which it was originally authenticated thesumit67.blogspot.com 9.7 application security 405 as a result  even if a malicious user gets a cookie  he may not be able to log in from a different computer ? never use a get method to perform any updates this prevents attacks using < img src  > such as the one we saw earlier in fact  the http standard recommends that get methods should never perform any updates  for other reasons such as a page refresh repeating an action that should have happened only once 9.7.3 password leakage another problemthat application developersmust dealwith is storing passwords in clear text in the application code for example  programs such as jsp scripts often contain passwords in clear text if such scripts are stored in a directory accessible by a web server  an external user may be able to access the source code of the script  and get access to the password for the database account used by the application to avoid such problems  many application servers provide mechanisms to store passwords in encrypted form  which the server decrypts before passing it on to the database such a feature removes the need for storing passwords as clear text in application programs however  if the decryption key is also vulnerable to being exposed  this approach is not fully effective as another measure against compromised database passwords  many database systems allow access to the database to be restricted to a given set of internet addresses  typically  the machines running the application servers attempts to connect to the database from other internet addresses are rejected thus  unless the malicious user is able to log into the application server  she can not do any damage even if she gains access to the database password 9.7.4 application authentication authentication refers to the task of verifying the identity of a person/software connecting to an application the simplest form of authentication consists of a secret password that must be presented when a user connects to the application unfortunately  passwords are easily compromised  for example  by guessing  or by sniffing of packets on the network if the passwords are not sent encrypted more robust schemes are needed for critical applications  such as online bank accounts encryption is the basis for more robust authentication schemes authentication through encryption is addressed in section 9.8.3 many applications use two-factor authentication  where two independent factors  that is  pieces of information or processes  are used to identify a user the two factors should not share a common vulnerability ; for example  if a system merely required two passwords  both could be vulnerable to leakage in the same manner  by network sniffing  or by a virus on the computer used by the user  for example   while biometrics such as fingerprints or iris scanners can be used in situations where a user is physically present at the point of authentication  they are not very meaningful across a network thesumit67.blogspot.com 406 chapter 9 application design and development passwords are used as the first factor in most such two-factor authentication schemes smart cards or other encryption devices connected through the usb interface  which can be used for authentication based on encryption techniques  see section 9.8.3   are widely used as second factors one-time password devices  which generate a new pseudo-random number  say  every minute are also widely used as a second factor each user is given one of these devices  and must enter the number displayed by the device at the time of authentication  along with the password  to authenticate himself each device generates a different sequence of pseudo-random numbers the application server can generate the same sequence of pseudo-random numbers as the device given to the user  stopping at the number that would be displayed at the time of authentication  and verify that the numbers match this scheme requires that the clock in the device and at the server are synchronized reasonably closely yet another second-factor approach is to send an sms with a  randomly generated  one-time password to the user ? s phone  whose number is registered earlier  whenever the user wishes to log in to the application the user must possess a phonewith that number to receive the sms  and then enter the one-time password  along with her regular password  to be authenticated it is worth noting that even with two-factor authentication  users may still be vulnerable to man-in-the-middle attacks in such attacks  a user attempting to connect to the application is diverted to a fake web site  which accepts the password  including second factor passwords  from the user  and uses it immediately to authenticate to the original application the https protocol  described later in section 9.8.3.2  is used to authenticate the web site to the user  so the user does not connect to a fake site believing it to be the intended site   the https protocol also encrypts data  and prevents man-in-the-middle attacks when users access multipleweb sites  it is often annoying for a user to have to authenticate herself to each site separately  oftenwith different passwords on each site there are systems that allow the user to authenticate herself to one central authentication service  and otherweb sites and applications can authenticate the user through the central authentication service ; the same password can then be used to access multiple sites the ldap protocol is widely used to implement such a central point of authentication ; organizations implement an ldap server containing user names and password information  and applications use the ldap server to authenticate users in addition to authenticating users  a central authentication service can provide other services  for example  providing information about the user such as name  email  and address information  to the application this obviates the need to enter this information separately in each application ldap can be used for this task  as described later in section 19.10.2 other directory systems such microsoft ? s active directories  also provide mechanisms for authenticating users as well as for providing user information a single sign-on system further allows the user to be authenticated once  and multiple applications can then verify the user ? s identity through an authentication service without requiring reauthentication in other words  once a user is logged thesumit67.blogspot.com 9.7 application security 407 in at one site  he does not have to enter his user name and password at other sites that use the same single sign-on service such single sign-on mechanisms have long been used in network authentication protocols such as kerberos  and implementations are now available forweb applications the security assertion markup language  saml  is a standard for exchanging authentication and authorization information between different security domains  to provide cross-organization single sign-on for example  suppose an application needs to provide access to all students froma particular university  say yale the university can set up a web-based service that carries out authentication suppose a user connects to the application with a username such as ? joe @ yale.edu ?  the application  instead of directly authenticating a user  diverts the user to yale university ? s authentication service  which authenticates the user  and then tells the application who the user is and may provide some additional information such as the category of the user  student or instructor  or other relevant information the user ? s password and other authentication factors are never revealed to the application  and the user need not register explicitly with the application however  the application must trust the university ? s authentication service when authenticating a user the openid standard is an alternative for single sign-on across organizations  and has seen increasing acceptance in recent years a large number of popular web sites  such as google,microsoft  yahoo !  among many others  act as openid authentication providers any application that acts as an openid client can then use any of these providers to authenticate a user ; for example  a user who has a yahoo ! account can choose yahoo ! as the authentication provider the user is redirected to yahoo ! for authentication  and on successful authentication is transparently redirected back to the application  and can then continue using the application 9.7.5 application-level authorization although the sql standard supports a fairly flexible system of authorization based on roles  described in section 4.6   the sql authorization model plays a very limited role in managing user authorizations in a typical application for instance  suppose you want all students to be able to see their own grades  but not the grades of anyone else such authorization can not be specified in sql for at least two reasons  1 lack of end-user information with the growth in the web  database accesses come primarily fromweb application servers the end users typically do not have individual user identifiers on the database itself  and indeed there may only be a single user identifier in the database corresponding to all users of an application server thus  authorization specification in sql can not be used in the above scenario it is possible for an application server to authenticate end users  and then pass the authentication information on to the database in this section we thesumit67.blogspot.com 408 chapter 9 application design and development will assume that the function syscontext.user id   returns the identifier of the application user on whose behalf a query is being executed.5 2 lack of fine-grained authorization authorization must be at the level of individual tuples  if we are to authorize students to see only their own grades such authorization is not possible in the current sql standard,which permits authorization only on an entire relation or view  or on specified attributes of relations or views we could try to get around this limitation by creating for each student a view on the takes relation that shows only that student ? s grades while this would work in principle  it would be extremely cumbersome since we would have to create one such view for every single student enrolled in the university  which is completely impractical.6 an alternative is to create a view of the form create view studenttakes as select * from takes where takes.id = syscontext.user id   users are then given authorization to this view  rather than to the underlying takes relation.however  queries executed on behalf of students must nowbe written on the view studenttakes  rather than on the original takes relation  whereas queries executed on behalf of instructorsmay need to use a different view the task of developing applications becomesmore complex as a result the task of authorization is today typically carried out entirely in the application  bypassing the authorization facilities of sql at the application level  users are authorized to access specific interfaces  and may further be restricted to view or update certain data items only while carrying out authorization in the application gives a great deal of flexibility to application developers  there are problems  too ? the code for checking authorization becomes intermixed with the rest of the application code ? implementing authorization through application code  rather than specifying it declaratively in sql  makes it hard to ensure the absence of loopholes because of an oversight  one of the application programs may not check for authorization  allowing unauthorized users access to confidential data 5in oracle  a jdbc connection using oracle ? s jdbc drivers can set the end user identifier using the method oracleconnection.setclientidentifier  userid   and an sql query can use the function sys context  ? userenv ?  ? client identifier ?  to retrieve the user identifier 6database systems are designed to manage large relations  but manage schema information such as views in a way that assumes smaller data volumes so as to enhance overall performance thesumit67.blogspot.com 9.7 application security 409 verifying that all application programs make all required authorization checks involves reading through all the application-server code  a formidable task in a large system in other words  applications have a very large ? surface area  ? making the task of protecting the application significantly harder and in fact  security loopholes have been found in a variety of real-life applications in contrast  if a database directly supported fine-grained authorization  authorization policies could be specified and enforced at the sql-level  which has a much smaller surface area even if some of the application interfaces inadvertently omit required authorization checks  the sql-level authorization could prevent unauthorized actions from being executed some database systems provide mechanisms for fine-grained authorization for example  the oracle virtual private database  vpd  allows a system administrator to associate a function with a relation ; the function returns a predicate that must be added to any query that uses the relation  different functions can be defined for relations that are being updated   for example  using our syntax for retrieving application user identifiers  the function for the takes relation can return a predicate such as  id = sys context.user id   this predicate is added to the where clause of every query that uses the takes relation as a result  assuming that the application program sets the user id value to the student ? s id   each student can see only the tuples corresponding to courses that she took thus  vpd provides authorization at the level of specific tuples  or rows  of a relation  and is therefore said to be a row-level authorization mechanism a potential pitfall with adding a predicate as described above is that it may change the meaning of a query significantly for example  if a user wrote a query to find the average grade over all courses  she would end up getting the average of her grades  not all grades although the system would give the ? right ? answer for the rewritten query  that answer would not correspond to the query the user may have thought she was submitting see the bibliographic notes for pointers to more information on oracle vpd 9.7.6 audit trails an audit trail is a log of all changes  inserts  deletes  and updates  to the application data  along with information such as which user performed the change and when the change was performed if application security is breached  or even if security was not breached  but some update was carried out erroneously  an audit trail can  a  help find out what happened  and who may have carried out the actions  and  b  aid in fixing the damage caused by the security breach or erroneous update for example  if a student ? s grade is found to be incorrect  the audit log can be examined to locate when and how the grade was updated  as well as to find which user carried out the updates the university could then also use the audit thesumit67.blogspot.com 410 chapter 9 application design and development trail to trace all the updates performed by this user  in order to find other incorrect or fraudulent updates  and then correct them audit trails can also be used to detect security breacheswhere a user ? s account is compromised and accessed by an intruder for example  each time a user logs in  shemay be informed about all updates in the audit trail that were done from that login in the recent past ; if the user sees a update that she did not carry out  it is likely the account has been compromised it is possible to create a database-level audit trail by defining appropriate triggers on relation updates  using system-defined variables that identify the user name and time   however  many database systems provide built-in mechanisms to create audit trails that are much more convenient to use details of how to create audit trails vary across database systems  and you should refer to the database-system manuals for details database-level audit trails are usually insufficient for applications  since they are usually unable to track who was the end user of the application further  updates are recorded at a low level  in terms of updates to tuples of a relation  rather than at a higher level  in terms of the business logic applications therefore usually create a higher-level audit trail  recording  for example  what action was carried out  by whom  when  and from which ip address the request originated a related issue is that of protecting the audit trail itself from being modified or deleted by users who breach application security one possible solution is to copy the audit trail to a different machine  to which the intruder would not have access  with each record in the trail copied as soon as it is generated 9.7.7 privacy in a world where an increasing amount of personal data are available online  people are increasingly worried about the privacy of their data for example  most people would want their personal medical data to be kept private and not revealed publicly however  the medical data must be made available to doctors and emergencymedical technicianswho treat the patient many countries have laws on privacy of such data that define when and to whom the data may be revealed violation of privacy law can result in criminal penalties in some countries applications that access such private data must be built carefully  keeping the privacy laws in mind on the other hand  aggregated private data can play an important role in many tasks such as detecting drug side effects  or in detecting the spread of epidemics how to make such data available to researchers carrying out such tasks  without compromising the privacy of individuals  is an important realworld problem as an example  suppose a hospital hides the name of the patient  but provides a researcher with the date of birth and the zip code  postal code  of the patient  both of which may be useful to the researcher   just these two pieces of information can be used to uniquely identify the patient in many cases  using information from an external database   compromising his privacy in this particular situation  one solution would be to give the year of birth but not the thesumit67.blogspot.com 9.8 encryption and its applications 411 date of birth  along with the zip code  which may suffice for the researcher this would not provide enough information to uniquely identify most individuals.7 as another example  web sites often collect personal data such as address  telephone  email  and credit-card information such information may be required to carry out a transaction such as purchasing an item from a store however  the customer may not want the information to be made available to other organizations  or may want part of the information  such as credit-card numbers  to be erased after some period of time as a way to prevent it from falling into unauthorized hands in the event of a security breach many web sites allow customers to specify their privacy preferences  and must then ensure that these preferences are respected 9.8 encryption and its applications encryption refers to the process of transforming data into a form that is unreadable  unless the reverse process of decryption is applied encryption algorithms use an encryption key to perform encryption  and require a decryption key  which could be the same as the encryption key depending on the encryption algorithm used  to perform decryption the oldest uses of encryption were for transmitting messages  encrypted using a secret key known only to the sender and the intended receiver even if themessage is intercepted by an enemy  the enemy  not knowing the key  will not be able to decrypt and understand the message encryption is widely used today for protecting data in transit in a variety of applications such as data transfer on the internet  and on cellular phone networks encryption is also used to carry out other tasks  such as authentication  as we will see in section 9.8.3 in the context of databases  encryption is used to store data in a secure way  so that even if the data is acquired by an unauthorized user  for example  a laptop computer containing the data is stolen   the data will not be accessible without a decryption key many databases today store sensitive customer information  such as creditcard numbers  names  fingerprints  signatures  and identification numbers such as  in the united states  social-security numbers a criminal who gets access to such data can use it for a variety of illegal activities such as purchasing goods using a credit-card number  or even acquiring a credit card in someone else ? s name organizations such as credit-card companies use knowledge of personal information as a way of identifying who is requesting a service or goods leakage of such personal information allows a criminal to impersonate someone else and get access to service or goods ; such impersonation is referred to as identity theft thus  applications that store such sensitive data must take great care to protect them from theft 7for extremely old people  who are relatively rare  even the year of birth plus postal code may be enough to uniquely identify the individual  so a range of values  such as 80 years or older  may be provided instead of the actual age for people older than 80 years thesumit67.blogspot.com 412 chapter 9 application design and development to reduce the chance of sensitive information being acquired by criminals  many countries and states today require by law that any database storing such sensitive information must store the information in an encrypted form.abusiness that does not protect its data thus could be held criminally liable in case of data theft thus  encryption is a critical component of any application that stores such sensitive information 9.8.1 encryption techniques there are a vast number of techniques for the encryption of data simple encryption techniques may not provide adequate security  since it may be easy for an unauthorized user to break the code as an example of a weak encryption technique  consider the substitution of each character with the next character in the alphabet thus  perryridge becomes qfsszsjehf if an unauthorized user sees only ? qfsszsjehf  ? she probably has insufficient information to break the code however  if the intruder sees a large number of encrypted branch names  she could use statistical data regarding the relative frequency of characters to guess what substitution is being made  for example  e is the most common letter in english text  followed by t  a  o  n  i  and so on   a good encryption technique has the following properties  ? it is relatively simple for authorized users to encrypt and decrypt data ? it depends not on the secrecy of the algorithm  but rather on a parameter of the algorithm called the encryption key  which is used to encrypt data in a symmetric-key encryption technique  the encryption key is also used to decrypt data in contrast  in public-key  also known as asymmetric-key  encryption techniques  there are two different keys  the public key and the private key  used to encrypt and decrypt the data ? its decryption key is extremely difficult for an intruder to determine  even if the intruder has access to encrypted data in the case of asymmetric-key encryption  it is extremely difficult to infer the private key even if the public key is available the advanced encryption standard  aes  is a symmetric-key encryption algorithm that was adopted as an encryption standard by the u.s government in 2000  and is now widely used the standard is based on the rijndael algorithm  named for the inventors v rijmen and j daemen   the algorithm operates on a 128-bit block of data at a time  while the key can be 128  192  or 256 bits in length thesumit67.blogspot.com 9.8 encryption and its applications 413 the algorithm runs a series of steps to jumble up the bits in a data block in a way that can be reversed during decryption  and performs an xor operation with a 128-bit ? round key ? that is derived from the encryption key a new round key is generated from the encryption key for each block of data that is encrypted during decryption  the round keys are generated again from the encryption key and the encryption process is reversed to recover the original data an earlier standard called the data encryption standard  des   adopted in 1977  was very widely used earlier for any symmetric-key encryption scheme to work  authorized users must be provided with the encryption key via a secure mechanism this requirement is a major weakness  since the scheme is no more secure than the security of the mechanism by which the encryption key is transmitted public-key encryption is an alternative scheme that avoids some of the problems faced by symmetric-key encryption techniques it is based on two keys  a public key and a private key each user ui has a public key ei and a private key di  all public keys are published  they can be seen by anyone each private key is known to only the one user to whom the key belongs if user u1 wants to store encrypted data  u1 encrypts them using public key e1 decryption requires the private key d1 because the encryption key for each user is public  it is possible to exchange information securely by this scheme if user u1 wants to share data with u2  u1 encrypts the data using e2  the public key of u2 since only user u2 knows how to decrypt the data  information can be transferred securely for public-key encryption to work  there must be a scheme for encryption such that it is infeasible  that is  extremely hard  to deduce the private key  given the public key such a scheme does exist and is based on these conditions  ? there is an efficient algorithm for testing whether or not a number is prime ? no efficient algorithm is known for finding the prime factors of a number for purposes of this scheme  data are treated as a collection of integers we create a public key by computing the product of two large prime numbers  p1 and p2 the private key consists of the pair  p1  p2   the decryption algorithm can not be used successfully if only the product p1p2 is known ; it needs the individual values p1 and p2 since all that is published is the product p1p2  an unauthorized user would need to be able to factor p1p2 to steal data by choosing p1 and p2 to be sufficiently large  over 100 digits   we can make the cost of factoring p1p2 prohibitively high  on the order of years of computation time  on even the fastest computers   the details of public-key encryption and the mathematical justification of this technique ? s properties are referenced in the bibliographic notes although public-key encryption by this scheme is secure  it is also computationally very expensive a hybrid scheme widely used for secure communication is as follows  a symmetric encryption key  based  for example  on aes  is ranthesumit67 blogspot.com 414 chapter 9 application design and development domly generated and exchanged in a secure manner using a public-key encryption scheme  and symmetric-key encryption using that key is used on the data transmitted subsequently encryption of small values  such as identifiers or names  is made complicated by the possibility of dictionary attacks  particularly if the encryption key is publicly available for example  if date-of-birth fields are encrypted  an attacker trying to decrypt a particular encrypted value e could try encrypting every possible date of birth until he finds one whose encrypted value matches e even if the encryption key is not publicly available  statistical information about data distributions can be used to figure out what an encrypted value represents in some cases  such as age or zip code for example  if the age 18 is the most common age in a database  the encrypted age value that occurs most often can be inferred to represent 18 dictionary attacks can be deterred by adding extra random bits to the end of the value before encryption  and removing them after decryption   such extra bits  referred to as an initialization vector in aes  or as salt bits in other contexts  provide good protection against dictionary attack 9.8.2 encryption support in databases many file systems and database systems today support encryption of data such encryption protects the data from someone who is able to access the data  but is not able to access the decryption key in the case of file-system encryption  the data to be encrypted are usually large files and directories containing information about files in the context of databases  encryption can be done at several different levels at the lowest level  the disk blocks containing database data can be encrypted  using a key available to the database-system software when a block is retrieved fromdisk  it is first decrypted and then used in the usual fashion such disk-blocklevel encryption protects against attackers who can access the disk contents but do not have access to the encryption key at the next higher level  specified  or all  attributes of a relation can be stored in encrypted form in this case  each attribute of a relation could have a different encryption key many databases today support encryption at the level of specified attributes as well as at the level of an entire relation  or all relations in a database encryption of specified attributes minimizes the overhead of decryption  by allowing applications to encrypt only attributes that contain sensitive values such as credit-card numbers however  when individual attributes or relations are encrypted  databases typically do not allow primary and foreign key attributes to be encrypted  and do not support indexing on encrypted attributes encryption also then needs to use extra random bits to prevent dictionary attacks  as described earlier a decryption key is obviously required to get access to encrypted data a single master encryption keymaybe used for all the encrypted data ; with attribute level encryption  different encryption keys could be used for different attributes thesumit67.blogspot.com 9.8 encryption and its applications 415 in this case  the decryption keys for different attributes can be stored in a file or relation  often referred to as ? wallet ?   which is itself encrypted using a master key a connection to the database that needs to access encrypted attributes must then provide the master key ; unless this is provided  the connection will not be able to access encrypted data the master key would be stored in the application program  typically on a different computer   or memorized by the database user  and provided when the user connects to the database encryption at the database level has the advantage of requiring relatively low time and space overhead  and does not require modification of applications for example  if data in a laptop computer database need to be protected from theft of the computer itself  such encryption can be used similarly  someone who gets access to backup tapes of a database would not be able to access the data contained in the backups without knowing the decryption key an alternative to performing encryption in the database is to perform it before the data are sent to the database the application must then encrypt the data before sending it to the database  and decrypt the data when it is retrieved this approach to data encryption requires significant modifications to be done to the application  unlike encryption performed in a database system 9.8.3 encryption and authentication password-based authentication is used widely by operating systems as well as databases however  the use of passwords has some drawbacks  especially over a network if an eavesdropper is able to ? sniff ? the data being sent over the network  she may be able to find the password as it is being sent across the network once the eavesdropper has a user name and password  she can connect to the database  pretending to be the legitimate user a more secure scheme involves a challenge ? response system the database system sends a challenge string to the user the user encrypts the challenge string using a secret password as encryption key and then returns the result the database system can verify the authenticity of the user by decrypting the string with the same secret password and checking the resultwith the original challenge string this scheme ensures that no passwords travel across the network public-key systems can be used for encryption in challenge ? response systems the database system encrypts a challenge string using the user ? s public key and sends it to the user the user decrypts the string using her private key  and returns the result to the database system the database system then checks the response this scheme has the added benefit of not storing the secret password in the database  where it could potentially be seen by system administrators storing the private key of a user on a computer  even a personal computer  has the risk that if the computer is compromised  the key may be revealed to an attacker who can then masquerade as the user smart cards provide a solution to this problem in a smart card  the key can be stored on an embedded chip ; the operating system of the smart card guarantees that the key can never be read  but thesumit67.blogspot.com 416 chapter 9 application design and development allows data to be sent to the card for encryption or decryption  using the private key.8 9.8.3.1 digital signatures another interesting application of public-key encryption is in digital signatures to verify authenticity of data ; digital signatures play the electronic role of physical signatures on documents the private key is used to ? sign  ? that is  encrypt  data  and the signed data can be made public anyone can verify the signature by decrypting the data using the public key  but no one could have generated the signed data without having the private key  note the reversal of the roles of the public and private keys in this scheme  thus  we can authenticate the data ; that is,we can verify that the datawere indeed created by the personwho is supposed to have created them furthermore  digital signatures also serve to ensure nonrepudiation that is  in case the person who created the data later claims she did not create it  the electronic equivalent of claiming not to have signed the check   we can prove that that person must have created the data  unless her private key was leaked to others   9.8.3.2 digital certificates authentication is  in general  a two-way process  where each of a pair of interacting entities authenticates itself to the other such pairwise authentication is needed even when a client contacts a web site  to prevent a malicious site from masquerading as a legal web site such masquerading could be done  for example  if the network routerswere compromised  and data rerouted to themalicious site for a user to ensure that she is interacting with an authentic web site  she must have the site ? s public key this raises the problem of howthe user can get the public key ? if it is stored on theweb site  the malicious site could supply a different key  and the user would have no way of verifying if the supplied public key is itself authentic authentication can be handled by a system of digital certificates  whereby public keys are signed by a certification agency  whose public key is well known for example  the public keys of the root certification authorities are stored in standardweb browsers a certificate issued by them can be verified by using the stored public keys a two-level system would place an excessive burden of creating certificates on the root certification authorities  so a multilevel system is used instead  with one or more root certification authorities and a tree of certification authorities below each root each authority  other than the root authorities  has a digital certificate issued by its parent a digital certificate issued by a certification authority a consists of a public key ka and an encrypted text e that can be decoded by using the public key 8smart cards provide other functionality too  such as the ability to store cash digitally and make payments  which is not relevant in our context thesumit67.blogspot.com 9.9 summary 417 ka the encrypted text contains the name of the party to whom the certificate was issued and her public key kc  in case the certification authority a is not a root certification authority  the encrypted text also contains the digital certificate issued to a by its parent certification authority ; this certificate authenticates the key ka itself  that certificate may in turn contain a certificate from a further parent authority  and so on  to verify a certificate  the encrypted text e is decrypted by using the public key ka to retrieve the name of the party  that is  the name of the organization owning the web site  ; additionally  if a is not a root authority whose public key is known to the verifier  the public key ka is verified recursively by using the digital certificate contained within e ; recursion terminates when a certificate issued by the root authority is reached verifying the certificate establishes the chain through which a particular site was authenticated  and provides the name and authenticated public key for the site digital certificates are widely used to authenticate web sites to users  to prevent malicious sites from masquerading as other web sites in the https protocol  the secure version of the http protocol   the site provides its digital certificate to the browser  which then displays it to the user if the user accepts the certificate  the browser then uses the provided public key to encrypt data a malicious site will have access to the certificate  but not the private key  and will thus not be able to decrypt the data sent by the browser only the authentic site  which has the corresponding private key  can decrypt the data sent by the browser we note that public-/private-key encryption and decryption costs are much higher than encryption/decryption costs using symmetric private keys to reduce encryption costs  https actually creates a one-time symmetric key after authentication  and uses it to encrypt data for the rest of the session digital certificates can also be used for authenticating users the user must submit a digital certificate containing her public key to a site  which verifies that the certificate has been signed by a trusted authority the user ? s public key can then be used in a challenge ? response system to ensure that the user possesses the corresponding private key  thereby authenticating the user 9.9 summary ? application programs that use databases as back ends and interact with users have been around since the 1960s application architectures have evolved over this period today most applications use web browsers as their front end  and a database as their back end  with an application server in between ? html provides the ability to define interfaces that combine hyperlinks with forms facilities web browsers communicate with web servers by the http protocol web servers can pass on requests to application programs  and return the results to the browser ? web servers execute application programs to implement desired functionality servlets are a widely used mechanism to write application programs thesumit67.blogspot.com 418 chapter 9 application design and development that run as part of the web server process  in order to reduce overhead there are also many server-side scripting languages that are interpreted by theweb server and provide application-program functionality as part of the web server ? there are several client-side scripting languages ? javascript is the most widely used ? that provide richer user interaction at the browser end ? complex applications usually have a multilayer architecture  including a model implementing business logic  a controller  and a view mechanism to display results they may also include a data access layer that implements an object-relational mapping many applications implement and use web services  allowing functions to be invoked over http ? a number of tools have been developed for rapid application development  and in particular to reduce the effort required to build user interfaces ? techniques such as caching of various forms  including query result caching and connection pooling  and parallel processing are used to improve application performance ? application developers must pay careful attention to security  to prevent attacks such as sql injection attacks and cross-site scripting attacks ? sql authorization mechanisms are coarse grained and of limited value to applications that deal with large numbers of users today application programs implement fine-grained  tuple-level authorization  dealingwith a large number of application users  completely outside the database system database extensions to provide tuple-level access control and to deal with large numbers of application users have been developed  but are not standard as yet ? protecting the privacy of data is an important task for database applications many countries have legal requirements on protection of certain kinds of data  such as credit-card information or medical data ? encryption plays a key role in protecting information and in authentication of users andweb sites symmetric-key encryption and public-key encryption are two contrasting but widely used approaches to encryption encryption of certain sensitive data stored in databases is a legal requirement in many countries and states ? encryption also plays a key role in authentication of users to applications  of web sites to users  and for digital signatures review terms ? application programs ? web interfaces to databases ? hypertext markup language  html  thesumit67.blogspot.com practice exercises 419 ? hyperlinks ? uniform resource locator  url  ? forms ? hypertext transfer protocol  http  ? common gateway interface  cgi  ? connectionless protocols ? cookie ? session ? servlets and servlet sessions ? server-side scripting ? jsp ? php ? asp.net ? client-side scripting ? javascript ? document object model  dom  ? applets ? application architecture ? presentation layer ? model-view-controller  mvc  architecture ? business-logic layer ? data-access layer ? object-relational mapping ? hibernate ? web services ? restful services ? rapid application development ? web application frameworks ? report generators ? connection pooling ? query result caching ? application security ? sql injection ? cross-site scripting  xss  ? cross-site request forgery  xsrf  ? authentication ? two-factor authentication ? man-in-the-middle attack ? central authentication ? single sign-on ? openid ? virtual private database  vpd  ? audit trail ? encryption ? symmetric-key encryption ? public-key encryption ? dictionary attack ? challenge ? response ? digital signatures ? digital certificates practice exercises 9.1 what is the main reason why servlets give better performance than programs that use the common gateway interface  cgi   even though java programs generally run slower than c or c + + programs ? 9.2 list some benefits and drawbacks of connectionless protocols over protocols that maintain connections 9.3 consider a carelessly writtenweb application for an online-shopping site  which stores the price of each item as a hidden form variable in the web page sent to the customer ; when the customer submits the form  the inthesumit67 blogspot.com 420 chapter 9 application design and development formation from the hidden form variable is used to compute the bill for the customer what is the loophole in this scheme ?  there was a real instance where the loophole was exploited by some customers of an onlineshopping site  before the problem was detected and fixed  9.4 consider another carelessly written web application  which uses a servlet that checks if there was an active session  but does not check if the user is authorized to access that page  instead depending on the fact that a link to the page is shown only to authorized users what is the risk with this scheme ?  therewas a real instancewhere applicants to a college admissions site could  after logging into the web site  exploit this loophole and view information they were not authorized to see ; the unauthorized access was however detected  and those who accessed the information were punished by being denied admission  9.5 list three ways in which caching can be used to speed up web server performance 9.6 the netstat command  available on linux and on windows  shows the active network connections on a computer explain how this command can be used to find out if a particularweb page is not closing connections that it opened  or if connection pooling is used  not returning connections to the connection pool you should account for the fact that with connection pooling  the connection may not get closed immediately 9.7 testing for sql-injection vulnerability  a suggest an approach for testing an application to find if it is vulnerable to sql injection attacks on text input b can sql injection occur with other forms of input ? if so  how would you test for vulnerability ? 9.8 a database relation may have the values of certain attributes encrypted for security why do database systems not support indexing on encrypted attributes ? using your answer to this question  explain why database systems do not allow encryption of primary-key attributes 9.9 exercise 9.8 addresses the problem of encryption of certain attributes.however  some database systems support encryption of entire databases explain how the problems raised in exercise 9.8 are avoided when the entire database is encrypted 9.10 suppose someone impersonates a company and gets a certificate from a certificate-issuing authority.what is the effect on things  such as purchase orders or programs  certified by the impersonated company  and on things certified by other companies ? 9.11 perhaps the most important data items in any database system are the passwords that control access to the database suggest a scheme for the secure storage of passwords be sure that your scheme allows the system thesumit67.blogspot.com exercises 421 to test passwords supplied by users who are attempting to log into the system exercises 9.12 write a servlet and associated html code for the following very simple application  a user is allowed to submit a form containing a value  say n  and should get a response containing n ? * ? symbols 9.13 write a servlet and associated html code for the following simple application  a user is allowed to submit a form containing a number  say n  and should get a response saying how many times the value n has been submitted previously the number of times each value has been submitted previously should be stored in a database 9.14 write a servlet that authenticates a user  based on user names and passwords stored in a database relation   and sets a session variable called userid after authentication 9.15 what is an sql injection attack ? explain how it works  and what precautions must be taken to prevent sql injection attacks 9.16 write pseudocode to manage a connection pool your pseudocode must include a function to create a pool  providing a database connection string  database user name  and password as parameters   a function to request a connection from the pool  a connection to release a connection to the pool  and a function to close the connection pool 9.17 explain the terms crud and rest 9.18 many web sites today provide rich user-interfaces using ajax list two features each of which reveals if a site uses ajax  without having to look at the source code using the above features  find three sites which use ajax ; you can view the html source of the page to check if the site is actually using ajax 9.19 xss attacks  a what is an xss attack ? b how can the referer field be used to detect some xss attacks ? 9.20 what is multi-factor authentication ? how does it help safeguard against stolen passwords ? 9.21 consider the oracle virtual private database  vpd  feature described in section 9.7.5  and an application based on our university schema a what predicate  using a subquery  should be generated to allow each faculty member to see only takes tuples corresponding to course sections that they have taught ? thesumit67.blogspot.com 422 chapter 9 application design and development b give an sql query such that the querywith the predicate added gives a result that is a subset of the original query result without the added predicate c give an sql query such that the querywith the predicate added gives a result containing a tuple that is not in the result of the original query without the added predicate 9.22 what are two advantages of encrypting data stored in the database ? 9.23 suppose you wish to create an audit trail of changes to the takes relation a define triggers to create an audit trail  logging the information into a relation called  for example  takes trail the logged information should include the user-id  assume a function user id   provides this information  and a timestamp  in addition to old and new values you must also provide the schema of the takes trail relation b can the above implementation guarantee that updates made by a malicious database administrator  or someone who manages to get the administrator ? s password  will be in the audit trail ? explain your answer 9.24 hackers may be able to fool you into believing that theirweb site is actually a web site  such as a bank or credit card web site  that you trust this may be done by misleading email  or even by breaking into the network infrastructure and rerouting network traffic destined for  say mybank.com  to the hacker ? s site if you enter your user name and password on the hacker ? s site  the site can record it  and use it later to break into your account at the real site.when you use a url such as https  //mybank.com  the https protocol is used to prevent such attacks explain how the protocol might use digital certificates to verify authenticity of the site 9.25 explain what is a challenge ? response system for authentication why is it more secure than a traditional password-based system ? project suggestions each of the following is a large project  which can be a semester-long project done by a group of students the difficulty of the project can be adjusted easily by adding or deleting features project 9.1 pick your favorite interactive web site  such as bebo  blogger  facebook  flickr  last.fm  twitter  wikipedia ; these are just a few examples  there are many more most of these sites manage a large amount of data  and use databases to store and process the data implement a subset of the functionality of theweb site you picked clearly  implementing even a significant subset of the features of such a site is well beyond a course project  thesumit67.blogspot.com project suggestions 423 but it is possible to find a set of features that is interesting to implement  yet small enough for a course project most of today ? s popular web sites make extensive use of javascript to create rich interfaces you may wish to go easy on this for your project  at least initially  since it takes time to build such intefaces  and then add more features to your interfaces  as time permits make use of web application development frameworks  or javascript libraries available on theweb  such as the yahoo user interface library  to speed up your development project 9.2 create a ? mashup ? which uses web services such as google or yahoomaps apis to create an interactiveweb sites for example  themap apis provide a way to display a map on the web page  with other information overlayed on the maps you could implement a restaurant recommendation system  with users contributing information about restaurants such as location  cuisine  price range  and ratings results of user searches could be displayed on the map you could allow wikipedia-like features  such as allowing users to add information and edit information added by other users  along with moderators who can weed out malicious updates you could also implement social features  such as giving more importance to ratings provided by your friends project 9.3 your university probably uses a course-management systems such as moodle  blackboard  or webct implement a subset of the functionality of such a course-management system for example  you can provide assignment submission and grading functionality  including mechanisms for students and teachers/teaching-assistants to discuss grading of a particular assignment you could also provide polls and other mechanisms for getting feedback project 9.4 consider the e-r schema of practice exercise 7.3  chapter 7   which represents information about teams in a league design and implement a web-based system to enter  update  and view the data project 9.5 design and implement a shopping cart system that lets shoppers collect items into a shopping cart  you can decide what information is to be supplied for each item  and purchased together you can extend and use the e-r schema of exercise 7.20 of chapter 7 you should check for availability of the item and deal with nonavailable items as you feel appropriate project 9.6 design and implement a web-based system to record student registration and grade information for courses at a university project 9.7 design and implement a system that permits recording of course performance information ? specifically  the marks given to each student in each assignment or exam of a course  and computation of a  weighted  sum of marks to get the total course marks the number of assignments/exams should not be predefined ; that is  more assignments/exams can be added at any time the system should also support grading  permitting cutoffs to be specified for various grades thesumit67.blogspot.com 424 chapter 9 application design and development you may also wish to integrate it with the student registration system of project 9.6  perhaps being implemented by another project team   project 9.8 design and implement a web-based system for booking classrooms at your university periodic booking  fixed days/times each week for a whole semester  must be supported cancellation of specific lectures in a periodic booking should also be supported you may also wish to integrate it with the student registration system of project 9.6  perhaps being implemented by another project team  so that classrooms can be booked for courses  and cancellations of a lecture or addition of extra lectures can be noted at a single interface  and will be reflected in the classroom booking and communicated to students via email project 9.9 design and implement a system for managing online multiple-choice tests you should support distributed contribution of questions  by teaching assistants  for example   editing of questions by whoever is in charge of the course  and creation of tests from the available set of questions you should also be able to administer tests online  either at a fixed time for all students  or at any time but with a time limit from start to finish  support one or both   and give students feedback on their scores at the end of the allotted time project 9.10 design and implement a system for managing email customer service incoming mail goes to a common pool there is a set of customer service agents who reply to email if the email is part of an ongoing series of replies  tracked using the in-reply-to field of email  the mail should preferably be replied to by the same agent who replied earlier the system should track all incoming mail and replies  so an agent can see the history of questions from a customer before replying to an email project 9.11 design and implement a simple electronicmarketplace where items can be listed for sale or for purchase under various categories  which should form a hierarchy   you may also wish to support alerting services  whereby a user can register interest in items in a particular category  perhaps with other constraints as well  without publicly advertising her interest  and is notified when such an item is listed for sale project 9.12 design and implement aweb-based newsgroup system.users should be able to subscribe to newsgroups  and browse articles in newsgroups the system tracks which articles were read by a user  so they are not displayed again also provide search for old articles you may also wish to provide a rating service for articles  so that articles with high rating are highlighted  permitting the busy reader to skip low-rated articles project 9.13 design and implement a web-based system for managing a sports ? ladder ? many people register  and may be given some initial rankings  perhaps based on past performance   anyone can challenge anyone else to a match  and the rankings are adjusted according to the result one simple system for adjusting rankings just moves the winner ahead of the loser in thesumit67.blogspot.com project suggestions 425 the rank order  in case the winner was behind earlier you can try to invent more complicated rank-adjustment systems project 9.14 design and implement a publication-listing service the service should permit entering of information about publications  such as title  authors  year  where the publication appeared  and pages authors should be a separate entity with attributes such as name  institution  department  email  address  and home page your application should support multiple views on the same data for instance  you should provide all publications by a given author  sorted by year  for example   or all publications by authors from a given institution or department you should also support search by keywords  on the overall database as well as within each of the views project 9.15 a common task in any organization is to collect structured information from a group of people for example  a manager may need to ask employees to enter their vacation plans  a professor may wish to collect feedback on a particular topic from students  or a student organizing an event may wish to allow other students to register for the event  or someone may wish to conduct an online vote on some topic create a system that will allow users to easily create information collection events.when creating an event  the event creator must definewho is eligible to participate ; to do so  your system must maintain user information  and allow predicates defining a subset of users the event creator should be able to specify a set of inputs  with types  default values  and validation checks  that the users will have to provide the event should have an associated deadline  and the system should have the ability to send reminders to users who have not yet submitted their information the event creator may be given the option of automatic enforcement of the deadline based on a specified date/time  or choosing to login and declare the deadline is over statistics about the submissions should be generated ? to do so  the event creator may be allowed to create simple summaries on the entered information the event creator may choose to make some of the summaries public  viewable by all users  either continually  e.g  how many people have responded  or after the deadline  e.g  what was the average feedback score   project 9.16 create a library of functions to simplify creation of web interfaces you must implement at least the following functions  a function to display a jdbc result set  with tabular formatting   functions to create different types of text and numeric inputs  with validation criteria such as input type and optional range  enforced at the client by appropriate javascript code   functions to input date and time values  with default values   and functions to create menu items based on a result set for extra credit  allow the user to set style parameters such as colors and fonts  and provide pagination support in the tables  hidden form parameters can be used to specifywhich page is to be displayed   build a sample database application to illustrate the use of these functions thesumit67.blogspot.com 426 chapter 9 application design and development project 9.17 design and implement aweb-based multiuser calendar system the system must track appointments for eachperson  includingmultioccurrence events  such asweekly meetings  and shared events  where an update made by the event creator gets reflected to all those who share the event   provide interfaces to schedule multiuser events  where an event creator can add a number of users who are invited to the event provide email notification of events for extra credits implement a web service that can be used by a reminder program running on the client machine tools development of a web application requires several software tools such as an application server  a compiler  and an editor for a programming language such as java or c #  and other optional tools such as a web server there are several integrated development environments that provide support for web application development the two most popular open-source ides are eclipse  developed by ibm  and netbeans  developed by sun microsystems microsoft ? s visual studio is the most widely used ide in thewindows world the apache tomcat  jakarta.apache.org   glassfish  glassfish.dev.java.net   jboss  jboss.org   and caucho ? s resin  www.caucho.com   are application servers that support servlets and jsp the apacheweb server  apache.org  is themostwidely used web server today microsoft ? s iis  internet information services  is aweb and application server that is widely used on microsoftwindows platforms  supporting microsoft ? s asp.net  msdn.microsoft.com/asp.net/   ibm ? s websphere  www.software.ibm.com  software provides a variety of software tools for web application development and deployment  including an application server  an ide  application integration middleware  business process management software and system management tools some of the above tools are open-source software that can be used free of cost  some are free for noncommercial use or for personal use  while others need to be paid for see the respectiveweb sites for more information the yahoo ! user interface  yui  javascript library  developer.yahoo.com/yui  is widely used for creating javascript programs thatwork across multiple browsers bibliographical notes information about servlets  including tutorials  standard specifications  and software  is available on java.sun.com/products/servlet information about jsp is available at java.sun.com/products/jsp information on jsp tag libraries can also be found at this url information about the .net framework and about web application development using asp.net can be found at msdn.microsoft.com atreya et al  2002  provide textbook coverage of digital signatures  including x.509 digital certificates and public-key infrastructure thesumit67.blogspot.com part 3 data storage and querying although a database system provides a high-level view of data  ultimately data have to be stored as bits on one or more storage devices a vast majority of databases today store data on magnetic disk  and  increasingly  on flash storage  and fetch data into main memory for processing  or copy data onto tapes and other backup devices for archival storage the physical characteristics of storage devices play a major role in the way data are stored  in particular because access to a random piece of data on disk is much slower than memory access  disk access takes tens of milliseconds,whereas memory access takes a tenth of a microsecond chapter 10 begins with an overview of physical storage media  including mechanisms to minimize the chance of data loss due to device failures the chapter then describes howrecords aremapped to files,which in turn are mapped to bits on the disk many queries reference only a small proportion of the records in a file an index is a structure that helps locate desired records of a relation quickly  without examining all records the index in this textbook is an example  although  unlike database indices  it is meant for human use chapter 11 describes several types of indices used in database systems user queries have to be executed on the database contents  which reside on storage devices it is usually convenient to break up queries into smaller operations  roughly corresponding to the relational-algebra operations chapter 12 describes how queries are processed  presenting algorithms for implementing individual operations  and then outlining how the operations are executed in synchrony  to process a query there are many alternativeways of processing a query,which can havewidely varying costs query optimization refers to the process of finding the lowest-cost method of evaluating a given query chapter 13 describes the process of query optimization 427 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter10 storage and file structure in preceding chapters,we have emphasized the higher-level models of a database for example  at the conceptual or logical level  we viewed the database  in the relational model  as a collection of tables indeed  the logical model of the database is the correct level for database users to focus on this is because the goal of a database system is to simplify and facilitate access to data ; users of the system should not be burdened unnecessarily with the physical details of the implementation of the system in this chapter  however  as well as in chapters 11  12  and 13  we probe below the higher levels as we describe various methods for implementing the data models and languages presented in preceding chapters we start with characteristics of the underlying storage media  such as disk and tape systems we then define various data structures that allow fast access to data we consider several alternative structures  each best suited to a different kind of access to data the final choice of data structure needs to be made on the basis of the expected use of the system and of the physical characteristics of the specific machine 10.1 overview of physical storage media several types of data storage exist inmost computer systems these storagemedia are classified by the speed with which data can be accessed  by the cost per unit of data to buy the medium  and by the medium ? s reliability among the media typically available are these  ? cache the cache is the fastest and most costly form of storage cachememory is relatively small ; its use is managed by the computer system hardware we shall not be concerned about managing cache storage in the database system it is  however  worth noting that database implementors do pay attention to cache effects when designing query processing data structures and algorithms ? main memory the storage medium used for data that are available to be operated on is main memory the general-purpose machine instructions operate 429 thesumit67.blogspot.com 430 chapter 10 storage and file structure on main memory although main memory may contain several gigabytes of data on a personal computer  or even hundreds of gigabytes of data in large server systems  it is generally too small  or too expensive  for storing the entire database the contents of main memory are usually lost if a power failure or system crash occurs ? flash memory flash memory differs from main memory in that stored data are retained even if power is turned off  or fails   there are two types of flash memory  called nand and nor flash of these  nand flash has a much higher storage capacity for a given cost  and is widely used for data storage in devices such as cameras  music players  and cell phones  and increasingly  in laptop computers as well flash memory has a lower cost per byte than main memory  in addition to being nonvolatile ; that is  it retains stored data even if power is switched off flash memory is alsowidely used for storing data in ? usb keys  ? which can be plugged into the universal serial bus  usb  slots of computing devices such usb keys have become a popular means of transporting data between computer systems  ? floppy disks ? played the same role in earlier days  but their limited capacity has made them obsolete now   flash memory is also increasingly used as a replacement for magnetic disks for storing moderate amounts of data such disk-drive replacements are called solid-state drives as of 2009  a 64 gb solid-state hard drive costs less than $ 200  and capacities range up to 160 gb further  flash memory is increasingly being used in server systems to improve performance by caching frequently used data  since it provides faster access than disk  with larger storage capacity than main memory  for a given cost   ? magnetic-disk storage the primary medium for the long-term online storage of data is the magnetic disk usually  the entire database is stored on magnetic disk the system must move the data from disk to main memory so that they can be accessed after the system has performed the designated operations  the data that have been modified must be written to disk as of 2009  the size of magnetic disks ranges from 80 gigabytes to 1.5 terabytes  and a 1 terabyte disk costs about $ 100 disk capacities have been growing at about 50 percent per year  and we can expect disks of much larger capacity every year disk storage survives power failures and system crashes disk-storage devices themselves may sometimes fail and thus destroy data  but such failures usually occur much less frequently than do system crashes ? optical storage the most popular forms of optical storage are the compact disk  cd   which can hold about 700 megabytes of data and has a playtime of about 80 minutes  and the digital video disk  dvd   which can hold 4.7 or 8.5 gigabytes of data per side of the disk  or up to 17 gigabytes on a two-sided disk   the expression digital versatile disk is also used in place of digital video disk  since dvds can hold any digital data  not just video data data are stored optically on a disk  and are read by a laser a higher capacity format called blu-ray dvd can store 27 gigabytes per layer  or 54 gigabytes in a double-layer disk thesumit67.blogspot.com 10.1 overview of physical storage media 431 the optical disks used in read-only compact disks  cd-rom  or read-only digital video disks  dvd-rom  can not be written  but are supplied with data prerecorded there are also ? record-once ? versions of compact disk  called cd-r  and digital video disk  called dvd-r and dvd + r   which can be written only once ; such disks are also called write-once  read-many  worm  disks there are also ? multiple-write ? versions of compact disk  called cd-rw  and digital video disk  dvd-rw  dvd + rw  and dvd-ram   which can be written multiple times optical disk jukebox systems contain a few drives and numerous disks that can be loaded into one of the drives automatically  by a robot arm  on demand ? tape storage tape storage is used primarily for backup and archival data although magnetic tape is cheaper than disks  access to data is much slower  because the tape must be accessed sequentially from the beginning for this reason  tape storage is referred to as sequential-access storage in contrast  disk storage is referred to as direct-access storage because it is possible to read data from any location on disk tapes have a high capacity  40 to 300-gigabyte tapes are currently available   and can be removed from the tape drive  so they are well suited to cheap archival storage tape libraries  jukeboxes  are used to hold exceptionally large collections of data such as data from satellites,which could include as much as hundreds of terabytes  1 terabyte = 1012 bytes   or even multiple petabytes  1 petabyte = 1015 bytes  of data in a few cases the various storage media can be organized in a hierarchy  figure 10.1  according to their speed and their cost the higher levels are expensive  but are fast as we move down the hierarchy  the cost per bit decreases  whereas the access time increases this trade-off is reasonable ; if a given storage system were both faster and less expensive than another ? other properties being the same ? then there would be no reason to use the slower  more expensive memory in fact  many early storage devices  including paper tape and core memories  are relegated to museums now that magnetic tape and semiconductor memory have become faster and cheaper magnetic tapes themselves were used to store active data back when disks were expensive and had low storage capacity today  almost all active data are stored on disks  except in very rare cases where they are stored on tape or in optical jukeboxes the fastest storage media ? for example  cache and main memory ? are referred to as primary storage the media in the next level in the hierarchy ? for example  magnetic disks ? are referred to as secondary storage  or online storage the media in the lowest level in the hierarchy ? for example  magnetic tape and optical-disk jukeboxes ? are referred to as tertiary storage  or offline storage in addition to the speed and cost of the various storage systems  there is also the issue of storage volatility volatile storage loses its contents when the power to the device is removed in the hierarchy shown in figure 10.1  the storage systems from main memory up are volatile  whereas the storage systems below thesumit67.blogspot.com 432 chapter 10 storage and file structure cache main memory flash memory magnetic disk optical disk magnetic tapes figure 10.1 storage device hierarchy main memory are nonvolatile data must be written to nonvolatile storage for safekeeping.we shall return to this subject in chapter 16 10.2 magnetic disk and flash storage magnetic disks provide the bulk of secondary storage for modern computer systems although disk capacities have been growing year after year  the storage requirements of large applications have also been growing very fast  in some cases even faster than the growth rate of disk capacities a very large database may require hundreds of disks in recent years  flash-memory storage sizes have grown rapidly  and flash storage is increasingly becoming a competitor to magnetic disk storage for several applications 10.2.1 physical characteristics of disks physically  disks are relatively simple  figure 10.2   each disk platter has a flat  circular shape its two surfaces are covered with a magnetic material  and information is recorded on the surfaces platters are made from rigid metal or glass when the disk is in use  a drive motor spins it at a constant high speed  usually 60  90  or 120 revolutions per second  but disks running at 250 revolutions per second are available   there is a read ? write head positioned just above the surface of the platter the disk surface is logically divided into tracks  which are subdivided into sectors a sector is the smallest unit of information that can be read from or written to the disk in currently available disks  sector sizes are thesumit67.blogspot.com 10.2 magnetic disk and flash storage 433 track t sector s spindle cylinder c platter arm read ? write head arm assembly rotation figure 10.2 moving head disk mechanism typically 512 bytes ; there are about 50,000 to 100,000 tracks per platter  and 1 to 5 platters per disk the inner tracks  closer to the spindle  are of smaller length  and in current-generation disks  the outer tracks contain more sectors than the inner tracks ; typical numbers are around 500 to 1000 sectors per track in the inner tracks  and around 1000 to 2000 sectors per track in the outer tracks the numbers vary among different models ; higher-capacity models usually have more sectors per track and more tracks on each platter the read ? write head stores information on a sector magnetically as reversals of the direction of magnetization of the magnetic material each side of a platter of a disk has a read ? write head that moves across the platter to access different tracks a disk typically contains many platters  and the read ? write heads of all the tracks are mounted on a single assembly called a disk arm  and move together the disk platters mounted on a spindle and the heads mounted on a disk arm are together known as head ? disk assemblies since the heads on all the platters move together  when the head on one platter is on the ith track  the heads on all other platters are also on the ith track of their respective platters.hence  the ith tracks of all the platters together are called the ith cylinder today  disks with a platter diameter of 312 inches dominate the market they have a lower cost and faster seek times  due to smaller seek distances  than do the larger-diameter disks  up to 14 inches  that were common earlier  yet they provide high storage capacity disks with even smaller diameters are used in portable devices such as laptop computers  and some handheld computers and portable music players the read ? write heads are kept as close as possible to the disk surface to increase the recording density the head typically floats or flies only microns thesumit67.blogspot.com 434 chapter 10 storage and file structure from the disk surface ; the spinning of the disk creates a small breeze  and the head assembly is shaped so that the breeze keeps the head floating just above the disk surface because the head floats so close to the surface  platters must be machined carefully to be flat head crashes can be a problem if the head contacts the disk surface  the head can scrape the recording medium off the disk  destroying the data that had been there in older-generation disks  the head touching the surface caused the removed medium to become airborne and to come between the other heads and their platters  causing more crashes ; a head crash could thus result in failure of the entire disk current-generation disk drives use a thin film of magnetic metal as recording medium they are much less susceptible to failure by head crashes than the older oxide-coated disks a disk controller interfaces between the computer system and the actual hardware of the disk drive ; in modern disk systems  the disk controller is implemented within the disk drive unit a disk controller accepts high-level commands to read or write a sector  and initiates actions  such as moving the disk arm to the right track and actually reading or writing the data disk controllers also attach checksums to each sector that is written ; the checksum is computed from the data written to the sector when the sector is read back  the controller computes the checksum again from the retrieved data and compares it with the stored checksum ; if the data are corrupted  with a high probability the newly computed checksum will not match the stored checksum if such an error occurs  the controller will retry the read several times ; if the error continues to occur  the controller will signal a read failure another interesting task that disk controllers perform is remapping of bad sectors if the controller detects that a sector is damaged when the disk is initially formatted  or when an attempt is made towrite the sector  it can logically map the sector to a different physical location  allocated from a pool of extra sectors set aside for this purpose   the remapping is noted on disk or in nonvolatile memory  and the write is carried out on the new location disks are connected to a computer system through a high-speed interconnection there are a number of common interfaces for connecting disks to computers of which the most commonly used today are  1  sata  which stands for serial ata,1 and a newer version of sata called sata ii or sata3gb  older versions of the ata standard called pata  or parallel ata  and ide  were widely used earlier  and are still available    2  small-computer-system interconnect  scsi ; pronounced ? scuzzy ?    3  sas  which stands for serial attached scsi   and  4  the fibre channel interface portable external disk systems often use the usb interface or the ieee 1394 firewire interface while disks are usually connected directly by cables to the disk interface of the computer system  they can be situated remotely and connected by a high-speed network to the disk controller in the storage area network  san  architecture  large numbers of disks are connected by a high-speed network to a number 1ata is a storage-device connection standard from the 1980s thesumit67.blogspot.com 10.2 magnetic disk and flash storage 435 of server computers the disks are usually organized locally using a storage organization technique called redundant arrays of independent disks  raid   described later  in section 10.3   to give the servers a logical view of a very large and very reliable disk the computer and the disk subsystem continue to use the scsi  sas  or fiber channel interface protocols to talk with each other  although they may be separated by a network remote access to disks across a storage area network means that disks can be shared by multiple computers that could run different parts of an application in parallel remote access also means that disks containing important data can be kept in a central server room where they can be monitored and maintained by system administrators  instead of being scattered in different parts of an organization network attached storage  nas  is an alternative to san nas is much like san  except that instead of the networked storage appearing to be a large disk  it provides a file system interface using networked file system protocols such as nfs or cifs 10.2.2 performance measures of disks the main measures of the qualities of a disk are capacity  access time  data-transfer rate  and reliability access time is the time from when a read or write request is issued to when data transfer begins to access  that is  to read or write  data on a given sector of a disk  the arm first must move so that it is positioned over the correct track  and then must wait for the sector to appear under it as the disk rotates the time for repositioning the arm is called the seek time  and it increases with the distance that the arm must move typical seek times range from 2 to 30 milliseconds  depending on how far the track is from the initial arm position smaller disks tend to have lower seek times since the head has to travel a smaller distance the average seek time is the average of the seek times  measured over a sequence of  uniformly distributed  random requests if all tracks have the same number of sectors  and we disregard the time required for the head to start moving and to stop moving  we can show that the average seek time is one-third the worst-case seek time taking these factors into account  the average seek time is around one-half of themaximum seek time.average seek times currently range between 4 and 10 milliseconds  depending on the disk model once the head has reached the desired track  the time spent waiting for the sector to be accessed to appear under the head is called the rotational latency time rotational speeds of disks today range from 5400 rotations per minute  90 rotations per second  up to 15,000 rotations per minute  250 rotations per second   or  equivalently  4 milliseconds to 11.1 milliseconds per rotation on an average  one-half of a rotation of the disk is required for the beginning of the desired sector to appear under the head thus  the average latency time of the disk is one-half the time for a full rotation of the disk the access time is then the sum of the seek time and the latency  and ranges from 8 to 20milliseconds once the first sector of the data to be accessed has come under the head  data transfer begins the data-transfer rate is the rate at which thesumit67.blogspot.com 436 chapter 10 storage and file structure data can be retrieved from or stored to the disk current disk systems support maximum transfer rates of 25 to 100 megabytes per second ; transfer rates are significantly lower than the maximum transfer rates for inner tracks of the disk  since they have fewer sectors for example  a disk with a maximum transfer rate of 100 megabytes per second may have a sustained transfer rate of around 30 megabytes per second on its inner tracks the final commonly usedmeasure of a disk is the mean time to failure  mttf   which is a measure of the reliability of the disk the mean time to failure of a disk  or of any other system  is the amount of time that  on average  we can expect the system to run continuously without any failure according to vendors ? claims  the mean time to failure of disks today ranges from 500,000 to 1,200,000 hours ? about 57 to 136 years in practice the claimed mean time to failure is computed on the probability of failure when the disk is new ? the figure means that given 1000 relatively new disks  if the mttf is 1,200,000 hours  on an average one of them will fail in 1200 hours a mean time to failure of 1,200,000 hours does not imply that the disk can be expected to function for 136 years ! most disks have an expected life span of about 5 years  and have significantly higher rates of failure once they become more than a few years old disk drives for desktop machines typically support the serial ata  sata  interface  which supports 150 megabytes per second  or the sata-ii 3gb interface  which supports 300 megabytes per second the pata 5 interface supported transfer rates of 133 megabytes per second disk drives designed for server systems typically support the ultra320 scsi interface  which provides transfer rates of up to 320 megabytes per second  or the serial attached scsi  sas  interface  versions ofwhich provide transfer rates of 3 or 6 gigabits per second storage area network  san  devices  which are connected to servers by a network  typically use fiber channel fc 2-gb or 4-gb interface  which provides transfer rates of up to 256 or 512 megabytes per second the transfer rate of an interface is shared between all disks attached to the interface  except for the serial interfaces which allow only one disk to be connected to each interface 10.2.3 optimization of disk-block access requests for disk i/o are generated both by the file system and by the virtual memory manager found in most operating systems each request specifies the address on the disk to be referenced ; that address is in the form of a block number a block is a logical unit consisting of a fixed number of contiguous sectors block sizes range from 512 bytes to several kilobytes data are transferred between disk and main memory in units of blocks the term page is often used to refer to blocks  although in a few contexts  such as flash memory  they refer to different things a sequence of requests for blocks from disk may be classified as a sequential access pattern or a random access pattern in a sequential access pattern  successive requests are for successive block numbers,which are on the same track  or on adjacent tracks to read blocks in sequential access  a disk seek may be required for the first block  but successive requests would either not require a seek  or thesumit67.blogspot.com 10.2 magnetic disk and flash storage 437 require a seek to an adjacent track  which is faster than a seek to a track that is farther away in contrast  in a random access pattern  successive requests are for blocks that are randomly located on disk each such request would require a seek the number of random block accesses that can be satisfied by a single disk in a second depends on the seek time  and is typically about 100 to 200 accesses per second since only a small amount  one block  of data is read per seek  the transfer rate is significantly lower with a random access pattern than with a sequential access pattern a number of techniques have been developed for improving the speed of access to blocks ? buffering blocks that are read from disk are stored temporarily in an inmemory buffer  to satisfy future requests buffering is done by both the operating system and the database system database buffering is discussed in more detail in section 10.8 ? read-ahead.when a disk block is accessed  consecutive blocks fromthe same track are read into an in-memory buffer even if there is no pending request for the blocks in the case of sequential access  such read-ahead ensures that many blocks are already in memorywhen they are requested  and minimizes the time wasted in disk seeks and rotational latency per block read operating systems also routinely perform read-ahead for consecutive blocks of an operating system file read-ahead is  however  not very useful for random block accesses ? scheduling if several blocks from a cylinder need to be transferred from disk to main memory  we may be able to save access time by requesting the blocks in the order in which they will pass under the heads if the desired blocks are on different cylinders  it is advantageous to request the blocks in an order thatminimizes disk-armmovement disk-arm ? scheduling algorithms attempt to order accesses to tracks in a fashion that increases the number of accesses that can be processed a commonly used algorithm is the elevator algorithm  which works in the same way many elevators do suppose that  initially  the arm is moving from the innermost track toward the outside of the disk under the elevator algorithm ? s control  for each track for which there is an access request  the arm stops at that track  services requests for the track  and then continues moving outward until there are no waiting requests for tracks farther out at this point  the arm changes direction  and moves toward the inside  again stopping at each track for which there is a request  until it reaches a track where there is no request for tracks farther toward the center then  it reverses direction and starts a new cycle disk controllers usually perform the task of reordering read requests to improve performance  since they are intimately aware of the organization of blocks on disk  of the rotational position of the disk platters  and of the position of the disk arm thesumit67.blogspot.com 438 chapter 10 storage and file structure ? file organization to reduce block-access time  we can organize blocks on disk in a way that corresponds closely to the way we expect data to be accessed for example  if we expect a file to be accessed sequentially  then we should ideally keep all the blocks of the file sequentially on adjacent cylinders older operating systems  such as the ibm mainframe operating systems  provided programmers fine control on placement of files  allowing a programmer to reserve a set of cylinders for storing a file however  this control places a burden on the programmer or system administrator to decide  for example  how many cylinders to allocate for a file  and may require costly reorganization if data are inserted to or deleted from the file subsequent operating systems  such as unix and microsoft windows  hide the disk organization from users  and manage the allocation internally although they do not guarantee that all blocks of a file are laid out sequentially  they allocate multiple consecutive blocks  an extent  at a time to a file sequential access to the file then only needs one seek per extent  instead of one seek per block over time  a sequential file that has multiple small appends may become fragmented ; that is  its blocks become scattered all over the disk to reduce fragmentation  the system can make a backup copy of the data on disk and restore the entire disk the restore operation writes back the blocks of each file contiguously  or nearly so   some systems  such as different versions of the windows operating system  have utilities that scan the disk and then move blocks to decrease the fragmentation the performance increases realized from these techniques can be large ? nonvolatile write buffers since the contents of main memory are lost in a power failure  information about database updates has to be recorded on disk to survive possible system crashes for this reason  the performance of update-intensive database applications  such as transaction-processing systems  is heavily dependent on the speed of disk writes we can use nonvolatile random-access memory  nvram  to speed up disk writes drastically the contents of nvram are not lost in power failure a common way to implement nvram is to use battery ? backed-up ram  although flash memory is also increasingly being used for nonvolatile write buffering the idea is that  when the database system  or the operating system  requests that a block be written to disk  the disk controller writes the block to an nvram buffer  and immediately notifies the operating system that the write completed successfully the controller writes the data to their destination on disk whenever the disk does not have any other requests  or when the nvram buffer becomes full when the database system requests a block write  it notices a delay only if the nvram buffer is full on recovery from a system crash  any pending buffered writes in the nvram are written back to the disk nvram buffers are found in certain high end disks  but are more frequently found in ? raid controllers ? ; we study raid in section 10.3 ? log disk another approach to reducing write latencies is to use a log disk ? that is  a disk devoted to writing a sequential log ? in much the same way as a nonvolatile ram buffer all access to the log disk is sequential  essentially thesumit67.blogspot.com 10.2 magnetic disk and flash storage 439 eliminating seek time  and several consecutive blocks can be written at once  making writes to the log disk several times faster than random writes as before  the data have to be written to their actual location on disk as well  but the log disk can do the write later  without the database system having to wait for the write to complete furthermore  the log disk can reorder the writes to minimize disk-arm movement if the system crashes before some writes to the actual disk location have completed  when the system comes back up it reads the log disk to find thosewrites that had not been completed  and carries them out then file systems that support log disks as above are called journaling file systems journaling file systems can be implemented even without a separate log disk  keeping data and the log on the same disk doing so reduces the monetary cost  at the expense of lower performance most modern file systems implement journaling  and use the log disk when writing internal file system information such as file allocation information earlier-generation file systems allowed write reordering without using a log disk  and ran the risk that the file system data structures on disk would be corrupted if the system crashed suppose  for example  that a file system used a linked list  and inserted a new node at the end by first writing the data for the new node  then updating the pointer from the previous node suppose also that the writes were reordered  so the pointer was updated first  and the system crashes before the new node is written the contents of the node would then be whatever junk was on disk earlier  resulting in a corrupted data structure to deal with the possibility of such data structure corruption  earliergeneration file systems had to perform a file system consistency check on system restart  to ensure that the data structures were consistent and if they were not  extra steps had to be taken to restore them to consistency these checks resulted in long delays in system restart after a crash  and the delays became worse as disk systems grewto higher capacities journaling file systems allow quick restartwithout the need for such file system consistency checks however  writes performed by applications are usually not written to the log disk database systems implement their own forms of logging  which we study later in chapter 16 10.2.4 flash storage as mentioned in section 10.1  there are two types of flash memory  nor flash and nand flash nor flash allows random access to individual words of memory  and has read time comparable to main memory however  unlike nor flash  reading from nand flash requires an entire page of data  typically consisting of between 512 and 4096 bytes  to be fetched from nand flash into main memory pages in a nand flash are thus similar to sectors in a magnetic disk but nand flash is significantly cheaper than nor flash  and has much higher storage capacity  and is by far the more widely used thesumit67.blogspot.com 440 chapter 10 storage and file structure storage systems built using nand flash provide the same block-oriented interface as disk storage compared to magnetic disks  flash memory can provide much faster random access  a page of data can be retrieved in around 1 or 2 microseconds from flash  whereas a random access on disk would take 5 to 10 milliseconds flash memory has a lower transfer rate than magnetic disks  with 20 megabytes per second being common some more recent flash memories have increased transfer rates of 100 to 200 megabytes per second however  solid state drives use multiple flash memory chips in parallel  to increase transfer rates to over 200 megabytes per second  which is faster than transfer rates of most disks writes to flash memory are a little more complicated a write to a page of flash memory typically takes a few microseconds however  once written  a page of flash memory can not be directly overwritten instead  it has to be erased and rewritten subsequently the erase operation can be performed on a number of pages  called an erase block  at once  and takes about 1 to 2 milliseconds the size of an erase block  often referred to as just ? block ? in flash literature  is usually significantly larger than the block size of the storage system further  there is a limit to how many times a flash page can be erased  typically around 100,000 to 1,000,000 times once this limit is reached  errors in storing bits are likely to occur flash memory systems limit the impact of both the slow erase speed and the update limits by mapping logical page numbers to physical page numbers.when a logical page is updated  it can be remapped to any already erased physical page  and the original location can be erased later each physical page has a small area of memory where its logical address is stored ; if the logical address is remapped to a different physical page  the original physical page is marked as deleted thus by scanning the physical pages  we can find where each logical page resides the logical-to-physical page mapping is replicated in an in-memory translation table for quick access blocks containing multiple deleted pages are periodically erased  taking care to first copy nondeleted pages in those blocks to a different block  the translation table is updated for these nondeleted pages   since each physical page can be updated only a fixed number of times  physical pages that have been erased many times are assigned ? cold data  ? that is  data that are rarely updated  while pages that have not been erased many times are used to store ? hot data  ? that is  data that are updated frequently this principle of evenly distributing erase operations across physical blocks is called wear leveling  and is usually performed transparently by flash-memory controllers if a physical page is damaged due to an excessive number of updates  it can be removed from usage  without affecting the flash memory as a whole all the above actions are carried out by a layer of software called the flash translation layer ; above this layer  flash storage looks identical to magnetic disk storage  providing the same page/sector-oriented interface  except that flash storage is much faster file systems and database storage structures can thus see an identical logical view of the underlying storage structure  regardless of whether it is flash or magnetic storage hybrid disk drives are hard-disk systems that combine magnetic storage with a smaller amount of flash memory  which is used as a cache for frequently thesumit67.blogspot.com 10.3 raid 441 accessed data frequently accessed data that are rarely updated are ideal for caching in flash memory 10.3 raid the data-storage requirements of some applications  in particularweb  database  and multimedia applications  have been growing so fast that a large number of disks are needed to store their data  even though disk-drive capacities have been growing very fast having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written  if the disks are operated in parallel several independent reads or writes can also be performed in parallel furthermore  this setup offers the potential for improving the reliability of data storage  because redundant information can be stored on multiple disks thus  failure of one disk does not lead to loss of data a variety of disk-organization techniques  collectively called redundant arrays of independent disks  raid   have been proposed to achieve improved performance and reliability in the past  system designers viewed storage systems composed of several small  cheap disks as a cost-effective alternative to using large  expensive disks ; the cost per megabyte of the smaller disks was less than that of larger disks in fact  the i in raid  which now stands for independent  originally stood for inexpensive today  however  all disks are physically small  and larger-capacity disks actually have a lower cost permegabyte raid systems are used for their higher reliability and higher performance rate  rather than for economic reasons another key justification for raid use is easier management and operations 10.3.1 improvement of reliability via redundancy let us first consider reliability the chance that at least one disk out of a set of n disks will fail is much higher than the chance that a specific single disk will fail suppose that the mean time to failure of a disk is 100,000 hours  or slightly over 11 years then  the mean time to failure of some disk in an array of 100 disks will be 100,000/100 = 1000 hours  or around 42 days  which is not long at all ! if we store only one copy of the data  then each disk failure will result in loss of a significant amount of data  as discussed in section 10.2.1   such a high frequency of data loss is unacceptable the solution to the problem of reliability is to introduce redundancy ; that is  we store extra information that is not needed normally  but that can be used in the event of failure of a disk to rebuild the lost information thus  even if a disk fails  data are not lost  so the effective mean time to failure is increased  provided that we count only failures that lead to loss of data or to nonavailability of data the simplest  but most expensive  approach to introducing redundancy is to duplicate every disk this technique is called mirroring  or  sometimes  shadowing   a logical disk then consists of two physical disks  and every write is carried thesumit67.blogspot.com 442 chapter 10 storage and file structure out on both disks if one of the disks fails  the data can be read from the other data will be lost only if the second disk fails before the first failed disk is repaired the mean time to failure  where failure is the loss of data  of a mirrored disk depends on the mean time to failure of the individual disks  as well as on the mean time to repair  which is the time it takes  on an average  to replace a failed disk and to restore the data on it suppose that the failures of the two disks are independent ; that is  there is no connection between the failure of one disk and the failure of the other then  if the mean time to failure of a single disk is 100,000 hours  and the mean time to repair is 10 hours  the mean time to data loss of a mirrored disk system is 100  0002/  2 * 10  = 500 * 106 hours  or 57,000 years !  we do not go into the derivations here ; references in the bibliographical notes provide the details  you should be aware that the assumption of independence of disk failures is not valid power failures  and natural disasters such as earthquakes  fires  and floods  may result in damage to both disks at the same time as disks age  the probability of failure increases  increasing the chance that a second disk will fail while the first is being repaired in spite of all these considerations  however  mirrored-disk systems offer much higher reliability than do single-disk systems mirrored-disk systems with mean time to data loss of about 500,000 to 1,000,000 hours  or 55 to 110 years  are available today power failures are a particular source of concern  since they occur far more frequently than do natural disasters power failures are not a concern if there is no data transfer to disk in progress when they occur however  even with mirroring of disks  if writes are in progress to the same block in both disks  and power fails before both blocks are fully written  the two blocks can be in an inconsistent state the solution to this problem is to write one copy first  then the next  so that one of the two copies is always consistent some extra actions are required when we restart after a power failure  to recover from incomplete writes this matter is examined in practice exercise 10.3 10.3.2 improvement in performance via parallelism now let us consider the benefit of parallel access to multiple disks with disk mirroring  the rate at which read requests can be handled is doubled  since read requests can be sent to either disk  as long as both disks in a pair are functional  as is almost always the case   the transfer rate of each read is the same as in a single-disk system  but the number of reads per unit time has doubled with multiple disks  we can improve the transfer rate as well  or instead  by striping data across multiple disks in its simplest form  data striping consists of splitting the bits of each byte across multiple disks ; such striping is called bitlevel striping for example  if we have an array of eight disks  we write bit i of each byte to disk i the array of eight disks can be treated as a single disk with sectors that are eight times the normal size  and  more important  that has eight times the transfer rate in such an organization  every disk participates in every access  read or write   so the number of accesses that can be processed per second is about the same as on a single disk  but each access can read eight times asmany thesumit67.blogspot.com 10.3 raid 443 data in the same time as on a single disk bit-level striping can be generalized to a number of disks that either is a multiple of 8 or a factor of 8 for example  if we use an array of four disks  bits i and 4 + i of each byte go to disk i block-level striping stripes blocks across multiple disks it treats the array of disks as a single large disk  and it gives blocks logical numbers ; we assume the block numbers start from 0.with an array of n disks  block-level striping assigns logical block i of the disk array to disk  i mod n  + 1 ; it uses the  i/n  th physical  a  raid 0  nonredundant striping  b  raid 1  mirrored disks  c  raid 2  memory-style error-correcting codes  d  raid 3  bit-interleaved parity  e  raid 4  block-interleaved parity  f  raid 5  block-interleaved distributed parity  g  raid 6  p + q redundancy p p p p p p p p p p c c c c p p p p p p figure 10.3 raid levels thesumit67.blogspot.com 444 chapter 10 storage and file structure block of the disk to store logical block i for example  with 8 disks  logical block 0 is stored in physical block 0 of disk 1  while logical block 11 is stored in physical block 1 of disk 4 when reading a large file  block-level striping fetches n blocks at a time in parallel fromthe n disks  giving a high data-transfer rate for large reads when a single block is read  the data-transfer rate is the same as on one disk  but the remaining n  1 disks are free to perform other actions block-level striping is the most commonly used form of data striping other levels of striping  such as bytes of a sector or sectors of a block  also are possible in summary  there are two main goals of parallelism in a disk system  1 load-balance multiple small accesses  block accesses   so that the throughput of such accesses increases 2 parallelize large accesses so that the response time of large accesses is reduced 10.3.3 raid levels mirroring provides high reliability  but it is expensive striping provides high data-transfer rates  but does not improve reliability various alternative schemes aim to provide redundancy at lower cost by combining disk striping with ? parity ? bits  which we describe next   these schemes have different cost ? performance trade-offs the schemes are classified into raid levels  as in figure 10.3  in the figure  p indicates error-correcting bits  and c indicates a second copy of the data  for all levels  the figure depicts four disks ? worth of data  and the extra disks depicted are used to store redundant information for failure recovery ? raid level 0 refers to disk arrays with striping at the level of blocks  but without any redundancy  such asmirroring or parity bits   figure 10.3a shows an array of size 4 ? raid level 1 refers to disk mirroring with block striping figure 10.3b shows a mirrored organization that holds four disks ? worth of data note that some vendors use the term raid level 1 + 0 or raid level 10 to refer to mirroring with striping  and use the term raid level 1 to refer to mirroring without striping mirroring without striping can also be used with arrays of disks  to give the appearance of a single large  reliable disk  if each disk has mblocks  logical blocks 0 to m 1 are stored on disk 0  mto 2m 1 on disk 1  the second disk   and so on  and each disk is mirrored.2 ? raid level 2  known as memory-style error-correcting-code  ecc  organization  employs parity bits memory systems have long used parity bits for 2note that some vendors use the term raid 0 + 1 to refer to a version of raid that uses striping to create a raid 0 array  and mirrors the array onto another array  with the difference from raid 1 being that if a disk fails  the raid 0 array containing the disk becomes unusable the mirrored array can still be used  so there is no loss of data this arrangement is inferior to raid 1 when a disk has failed  since the other disks in the raid 0 array can continue to be used in raid 1  but remain idle in raid 0 + 1 thesumit67.blogspot.com 10.3 raid 445 error detection and correction each byte in a memory system may have a parity bit associated with it that records whether the numbers of bits in the byte that are set to 1 is even  parity = 0  or odd  parity = 1   if one of the bits in the byte gets damaged  either a 1 becomes a 0  or a 0 becomes a 1   the parity of the byte changes and thuswill not match the stored parity similarly  if the stored parity bit gets damaged  it will not match the computed parity thus  all 1-bit errorswill be detected by the memory system error-correcting schemes store 2 or more extra bits  and can reconstruct the data if a single bit gets damaged the idea of error-correcting codes can be used directly in disk arrays by striping bytes across disks for example  the first bit of each byte could be stored in disk 0  the second bit in disk 1  and so on until the eighth bit is stored in disk 7  and the error-correction bits are stored in further disks figure 10.3c shows the level 2 scheme the disks labeled p store the errorcorrection bits if one of the disks fails  the remaining bits of the byte and the associated error-correction bits can be read fromother disks  and can be used to reconstruct the damaged data figure 10.3c shows an array of size 4 ; note raid level 2 requires only three disks ? overhead for four disks of data  unlike raid level 1  which required four disks ? overhead ? raid level 3  bit-interleaved parity organization  improves on level 2 by exploiting the fact that disk controllers  unlike memory systems  can detect whether a sector has been read correctly  so a single parity bit can be used for error correction  as well as for detection the idea is as follows  if one of the sectors gets damaged  the system knows exactly which sector it is  and  for each bit in the sector  the system can figure out whether it is a 1 or a 0 by computing the parity of the corresponding bits from sectors in the other disks if the parity of the remaining bits is equal to the stored parity  the missing bit is 0 ; otherwise  it is 1 raid level 3 is as good as level 2  but is less expensive in the number of extra disks  it has only a one-disk overhead   so level 2 is not used in practice figure 10.3d shows the level 3 scheme raid level 3 has two benefits over level 1 it needs only one parity disk for several regular disks  whereas level 1 needs one mirror disk for every disk  and thus level 3 reduces the storage overhead since reads and writes of a byte are spread out over multiple disks  with n-way striping of data  the transfer rate for reading or writing a single block is n times faster than a raid level 1 organization using n-way striping on the other hand  raid level 3 supports a lower number of i/o operations per second  since every disk has to participate in every i/o request ? raid level 4  block-interleaved parity organization  uses block-level striping  like raid 0  and in addition keeps a parity block on a separate disk for corresponding blocks from n other disks this scheme is shown pictorially in figure 10.3e if one of the disks fails  the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk thesumit67.blogspot.com 446 chapter 10 storage and file structure ablock read accesses only one disk  allowing other requests to be processed by the other disks thus  the data-transfer rate for each access is slower  but multiple read accesses can proceed in parallel  leading to a higher overall i/o rate the transfer rates for large reads is high  since all the disks can be read in parallel ; large writes also have high transfer rates  since the data and parity can be written in parallel small independent writes  on the other hand  can not be performed in parallel awrite of a block has to access the disk on which the block is stored  as well as the parity disk  since the parity block has to be updated moreover  both the old value of the parity block and the old value of the block being written have to be read for the new parity to be computed thus  a single write requires four disk accesses  two to read the two old blocks  and two to write the two blocks ? raid level 5  block-interleaved distributed parity  improves on level 4 by partitioning data and parity among all n + 1 disks  instead of storing data in ndisks and parity in one disk in level 5  all disks can participate in satisfying read requests  unlike raid level 4  where the parity disk can not participate  so level 5 increases the total number of requests that can be met in a given amount of time for each set of n logical blocks  one of the disks stores the parity  and the other n disks store the blocks figure 10.3f shows the setup the p ? s are distributed across all the disks for example  with an array of 5 disks  the parity block  labeled pk  for logical blocks 4k  4k + 1  4k + 2  4k + 3 is stored in disk k mod 5 ; the corresponding blocks of the other four disks store the 4 data blocks 4k to 4k + 3 the following table indicates how the first 20 blocks  numbered 0 to 19  and their parity blocks are laid out the pattern shown gets repeated on further blocks p0 48 12 16 0 p1 9 13 17 15 p2 14 18 26 10 p3 19 37 11 15 p4 note that a parity block can not store parity for blocks in the same disk  since then a disk failure would result in loss of data as well as of parity  and hence would not be recoverable level 5 subsumes level 4  since it offers better read ? write performance at the same cost  so level 4 is not used in practice ? raid level 6  the p + q redundancy scheme  is much like raid level 5  but stores extra redundant information to guard against multiple disk failures instead of using parity  level 6 uses error-correcting codes such as the reed ? solomon codes  see the bibliographical notes   in the scheme in figure 10.3g  2 bits of redundant data are stored for every 4 bits of data ? unlike 1 parity bit in level 5 ? and the system can tolerate two disk failures thesumit67.blogspot.com 10.3 raid 447 finally  we note that several variations have been proposed to the basic raid schemes described here  and different vendors use different terminologies for the variants 10.3.4 choice of raid level the factors to be taken into account in choosing a raid level are  ? monetary cost of extra disk-storage requirements ? performance requirements in terms of number of i/o operations ? performance when a disk has failed ? performance during rebuild  that is  while the data in a failed disk are being rebuilt on a new disk   the time to rebuild the data of a failed disk can be significant  and it varies with the raid level that is used rebuilding is easiest for raid level 1  since data can be copied from another disk ; for the other levels  we need to access all the other disks in the array to rebuild data of a failed disk the rebuild performance of a raid system may be an important factor if continuous availability of data is required  as it is in high-performance database systems furthermore  since rebuild time can form a significant part of the repair time  rebuild performance also influences the mean time to data loss raid level 0 is used in high-performance applications where data safety is not critical since raid levels 2 and 4 are subsumed by raid levels 3 and 5  the choice of raid levels is restricted to the remaining levels bit striping  level 3  is inferior to block striping  level 5   since block striping gives as good data-transfer rates for large transfers  while using fewer disks for small transfers for small transfers  the disk access time dominates anyway  so the benefit of parallel reads diminishes in fact  level 3 may perform worse than level 5 for a small transfer  since the transfer completes only when corresponding sectors on all disks have been fetched ; the average latency for the disk array thus becomes very close to the worst-case latency for a single disk  negating the benefits of higher transfer rates level 6 is not supported currently by many raid implementations  but it offers better reliability than level 5 and can be used in applications where data safety is very important the choice between raid level 1 and level 5 is harder to make raid level 1 is popular for applications such as storage of log files in a database system  since it offers the best write performance raid level 5 has a lower storage overhead than level 1  but has a higher time overhead for writes for applications where data are read frequently  and written rarely  level 5 is the preferred choice disk-storage capacities have been growing at a rate of over 50 percent per year for many years  and the cost per byte has been falling at the same rate as a result  for many existing database applications with moderate storage requirements  the monetary cost of the extra disk storage needed formirroring has become relatively small  the extra monetary cost  however  remains a significant issue for storagethesumit67 blogspot.com 448 chapter 10 storage and file structure intensive applications such as video data storage   access speeds have improved at a much slower rate  around a factor of 3 over 10 years   while the number of i/o operations required per second has increased tremendously  particularly for web application servers raid level 5  which increases the number of i/o operations needed to write a single logical block  pays a significant time penalty in terms of write performance raid level 1 is therefore the raid level of choice for many applications with moderate storage requirements and high i/o requirements raid system designers have to make several other decisions as well for example  how many disks should there be in an array ? how many bits should be protected by each parity bit ? if there are more disks in an array  data-transfer rates are higher  but the system will be more expensive if there are more bits protected by a parity bit  the space overhead due to parity bits is lower  but there is an increased chance that a second disk will fail before the first failed disk is repaired  and that will result in data loss 10.3.5 hardware issues another issue in the choice of raid implementations is at the level of hardware raid can be implemented with no change at the hardware level  using only software modification such raid implementations are called software raid however  there are significant benefits to be had by building special-purpose hardware to support raid,which we outline below ; systemswith special hardware support are called hardware raid systems hardware raid implementations can use nonvolatile ram to record writes before they are performed in case of power failure  when the system comes back up  it retrieves information about any incomplete writes from nonvolatile ram and then completes thewrites.without such hardware support  extra work needs to be done to detect blocks that may have been partially written before power failure  see practice exercise 10.3   even if all writes are completed properly  there is a small chance of a sector in a disk becoming unreadable at some point  even though it was successfully written earlier reasons for loss of data on individual sectors could range from manufacturing defects  to data corruption on a track when an adjacent track is written repeatedly such loss of data that were successfully written earlier is sometimes referred to as a latent failure  or as bit rot.when such a failure happens  if it is detected early the data can be recovered from the remaining disks in the raid organization however  if such a failure remains undetected  a single disk failure could lead to data loss if a sector in one of the other disks has a latent failure to minimize the chance of such data loss  good raid controllers perform scrubbing ; that is  during periods when disks are idle  every sector of every disk is read  and if any sector is found to be unreadable  the data are recovered from the remaining disks in the raid organization  and the sector is written back  if the physical sector is damaged  the disk controllerwould remap the logical sector address to a different physical sector on disk  thesumit67.blogspot.com 10.4 tertiary storage 449 some hardware raid implementations permit hot swapping ; that is  faulty disks can be removed and replaced by new ones without turning power off hot swapping reduces the mean time to repair  since replacement of a disk does not have to wait until a time when the system can be shut down in fact many critical systems today run on a 24 ? 7 schedule ; that is  they run 24 hours a day  7 days a week  providing no time for shutting down and replacing a failed disk further  many raid implementations assign a spare disk for each array  or for a set of disk arrays   if a disk fails  the spare disk is immediately used as a replacement as a result  the mean time to repair is reduced greatly  minimizing the chance of any data loss the failed disk can be replaced at leisure the power supply  or the disk controller  or even the system interconnection in a raid system could become a single point of failure that could stop functioning of the raid system to avoid this possibility  good raid implementations have multiple redundant power supplies  with battery backups so they continue to function even if power fails   such raid systems have multiple disk interfaces  andmultiple interconnections to connect the raid system to the computer system  or to a network of computer systems   thus  failure of any single component will not stop the functioning of the raid system 10.3.6 other raid applications the concepts of raid have been generalized to other storage devices  including arrays of tapes  and even to the broadcast of data over wireless systems when applied to arrays of tapes  the raid structures are able to recover data even if one of the tapes in an array of tapes is damaged when applied to broadcast of data  a block of data is split into short units and is broadcast along with a parity unit ; if one of the units is not received for any reason  it can be reconstructed from the other units 10.4 tertiary storage in a large database system  some of the data may have to reside on tertiary storage the two most common tertiary storage media are optical disks and magnetic tapes 10.4.1 optical disks compact disks have been a popular medium for distributing software  multimedia data such as audio and images  and other electronically published information they have a storage capacity of 640 to 700 megabytes  and they are cheap to mass-produce digital video disks  dvds  have now replaced compact disks in applications that require larger amounts of data disks in the dvd-5 format can store 4.7 gigabytes of data  in one recording layer   while disks in the dvd-9 format can store 8.5 gigabytes of data  in two recording layers   recording on both sides of a disk yields even larger capacities ; dvd-10 and dvd-18 formats  which are the two-sided versions of dvd-5 and dvd-9  can store 9.4 gigabytes thesumit67.blogspot.com 450 chapter 10 storage and file structure and 17 gigabytes  respectively the blu-ray dvd format has a significantly higher capacity of 27 to 54 gigabytes per disk cdanddvddrives have muchlonger seek times  100 milliseconds is common  than do magnetic-disk drives  since the head assembly is heavier rotational speeds are typically lower than those of magnetic disks  although the faster cd and dvd drives have rotation speeds of about 3000 rotations per minute  which is comparable to speeds of lower-end magnetic-disk drives rotational speeds of cd drives originally corresponded to the audio cd standards  and the speeds of dvd drives originally corresponded to the dvd video standards  but currentgeneration drives rotate at many times the standard rate data-transfer rates are somewhat less than for magnetic disks current cd drives read at around 3 to 6 megabytes per second  and current dvd drives read at 8 to 20 megabytes per second like magnetic-disk drives  optical disks store more data in outside tracks and less data in inner tracks the transfer rate of optical drives is characterized as n ?  which means the drive supports transfers at n times the standard rate ; rates of around 50 ? for cd and 16 ? for dvd are now common the record-once versions of optical disks  cd-r  dvd-r  and dvd + r  are popular for distribution of data and particularly for archival storage of data because they have a high capacity  have a longer lifetime than magnetic disks  and can be removed and stored at a remote location since they can not be overwritten  they can be used to store information that should not be modified  such as audit trails the multiple-write versions  cd-rw  dvd-rw  dvd + rw  and dvd-ram  are also used for archival purposes jukeboxes are devices that store a large number of optical disks  up to several hundred  and load them automatically on demand to one of a small number of drives  usually 1 to 10   the aggregate storage capacity of such a system can be many terabytes when a disk is accessed  it is loaded by a mechanical arm from a rack onto a drive  any disk that was already in the drive must first be placed back on the rack   the disk load/unload time is usually of the order of a few seconds ? very much longer than disk access times 10.4.2 magnetic tapes although magnetic tapes are relatively permanent  and can hold large volumes of data  they are slow in comparison to magnetic and optical disks even more important  magnetic tapes are limited to sequential access thus  they can not provide random access for secondary-storage requirements  although historically  prior to the use of magnetic disks  tapes were used as a secondary-storage medium tapes are used mainly for backup  for storage of infrequently used information  and as an off-line medium for transferring information from one system to another tapes are also used for storing large volumes of data  such as video or image data  that either do not need to be accessible quickly or are so voluminous that magnetic-disk storage would be too expensive a tape is kept in a spool  and is wound or rewound past a read ? write head moving to the correct spot on a tape can take seconds or even minutes  rather than thesumit67.blogspot.com 10.5 file organization 451 milliseconds ; once positioned  however  tape drives can write data at densities and speeds approaching those of disk drives capacities vary  depending on the length and width of the tape and on the density at which the head can read and write the market is currently fragmented among a wide variety of tape formats currently available tape capacities range from a few gigabytes with the digital audio tape  dat  format  10 to 40 gigabytes with the digital linear tape  dlt  format  100 gigabytes and higher with the ultrium format  to 330 gigabytes with ampex helical scan tape formats data-transfer rates are of the order of a few to tens of megabytes per second tape devices are quite reliable  and good tape drive systems perform a read of the just-written data to ensure that it has been recorded correctly tapes  however  have limits on the number of times that they can be read or written reliably tape jukeboxes  like optical disk jukeboxes  hold large numbers of tapes  with a few drives onto which the tapes can be mounted ; they are used for storing large volumes of data  ranging up to many petabytes  1015 bytes   with access times on the order of seconds to a fewminutes.applications that need such enormous data storage include imaging systems that gather data from remote-sensing satellites  and large video libraries for television broadcasters some tape formats  such as the accelis format  support faster seek times  of the order of tens of seconds   and are intended for applications that retrieve information from jukeboxes most other tape formats provide larger capacities  at the cost of slower access ; such formats are ideal for data backup  where fast seeks are not important tape drives have been unable to keep up with the enormous improvements in disk drive capacity and corresponding reduction in storage cost while the cost of tapes is low  the cost of tape drives and tape libraries is significantly higher than the cost of a disk drive  a tape library capable of storing a few terabytes can costs tens of thousands of dollars backing up data to disk drives has become a cost-effective alternative to tape backup for a number of applications 10.5 file organization a database is mapped into a number of different files that are maintained by the underlying operating system these files reside permanently on disks a file is organized logically as a sequence of records these records are mapped onto disk blocks files are provided as a basic construct in operating systems  so we shall assume the existence of an underlying file system we need to consider ways of representing logical data models in terms of files each file is also logically partitioned into fixed-length storage units called blocks  which are the units of both storage allocation and data transfer most databases use block sizes of 4 to 8 kilobytes by default  but many databases allow the block size to be specified when a database instance is created larger block sizes can be useful in some database applications a block may contain several records ; the exact set of records that a block contains is determined by the form of physical data organization being used.we thesumit67.blogspot.com 452 chapter 10 storage and file structure shall assume that no record is larger than a block this assumption is realistic for most data-processing applications  such as our university example there are certainly several kinds of large data items  such as images  that can be significantly larger than a block we briefly discuss how to handle such large data items later  in section 10.5.2  by storing large data items separately  and storing a pointer to the data item in the record in addition  we shall require that each record is entirely contained in a single block ; that is  no record is contained partly in one block  and partly in another this restriction simplifies and speeds up access to data items in a relational database  tuples of distinct relations are generally of different sizes one approach to mapping the database to files is to use several files  and to store records of only one fixed length in any given file an alternative is to structure our files so that we can accommodate multiple lengths for records ; however  files of fixed-length records are easier to implement than are files of variable-length records.many of the techniques used for the former can be applied to the variable-length case thus  we begin by considering a file of fixed-length records  and consider storage of variable-length records later 10.5.1 fixed-length records as an example  let us consider a file of instructor records for our university database each record of this file is defined  in pseudocode  as  type instructor = record id varchar  5  ; name varchar  20  ; dept name varchar  20  ; salary numeric  8,2  ; end assume that each character occupies 1 byte and that numeric  8,2  occupies 8 bytes suppose that instead of allocating a variable amount of bytes for the attributes id  name  and dept name  we allocate the maximum number of bytes that each attribute can hold then  the instructor record is 53 bytes long a simple approach is to use the first 53 bytes for the first record  the next 53 bytes for the second record  and so on  figure 10.4   however  there are two problems with this simple approach  1 unless the block size happens to be a multiple of 53  which is unlikely   some records will cross block boundaries that is  part of the record will be stored in one block and part in another it would thus require two block accesses to read or write such a record 2 it is difficult to delete a record from this structure the space occupied by the record to be deleted must be filled with some other record of the file  or we must have a way of marking deleted records so that they can be ignored thesumit67.blogspot.com 10.5 file organization 453 srinivasan comp sci 65000 wu finance 90000 mozart music 40000 einstein physics 95000 el said history 60000 gold physics 87000 katz comp sci 75000 califieri history 62000 singh finance 80000 crick biology 72000 brandt comp sci 92000 15151 10101 12121 22222 32343 33456 45565 58583 76543 76766 83821 98345 kim elec eng 80000 record 0 record 1 record 2 record 3 record 4 record 5 record 6 record 7 record 8 record 9 record 10 record 11 figure 10.4 file containing instructor records to avoid the first problem  we allocate only as many records to a block as would fit entirely in the block  this number can be computed easily by dividing the block size by the record size  and discarding the fractional part   any remaining bytes of each block are left unused when a record is deleted,we could move the record that came after it into the space formerly occupied by the deleted record  and so on  until every record following the deleted record has been moved ahead  figure 10.5   such an approach requires moving a large number of records it might be easier simply to move the final record of the file into the space occupied by the deleted record  figure 10.6   it is undesirable tomove records to occupy the space freed by a deleted record  since doing so requires additional block accesses since insertions tend to be more frequent than deletions  it is acceptable to leave open the space occupied by the srinivasan comp sci 65000 wu finance 90000 mozart music 40000 el said history 60000 gold physics 87000 katz comp sci 75000 califieri history 62000 singh finance 80000 crick biology 72000 brandt comp sci 92000 15151 10101 12121 32343 33456 45565 58583 76543 76766 83821 98345 kim elec eng 80000 record 0 record 1 record 2 record 4 record 5 record 6 record 7 record 8 record 9 record 10 record 11 figure 10.5 file of figure 10.4  with record 3 deleted and all records moved thesumit67.blogspot.com 454 chapter 10 storage and file structure srinivasan comp sci 65000 wu finance 90000 mozart music 40000 el said history 60000 gold physics 87000 katz comp sci 75000 califieri history 62000 singh finance 80000 crick biology 72000 brandt comp sci 92000 15151 10101 12121 32343 33456 45565 58583 76543 76766 83821 record 0 record 1 record 2 record 4 record 5 record 6 record 7 record 8 record 9 record 10 record 11 98345 kim elec eng 80000 figure 10.6 file of figure 10.4  with record 3 deleted and final record moved deleted record  and to wait for a subsequent insertion before reusing the space a simple marker on a deleted record is not sufficient  since it is hard to find this available space when an insertion is being done thus  we need to introduce an additional structure at the beginning of the file  we allocate a certain number of bytes as a file header the header will contain a variety of information about the file for now  all we need to store there is the address of the first record whose contents are deleted we use this first record to store the address of the second available record  and so on intuitively  we can think of these stored addresses as pointers  since they point to the location of a record the deleted records thus form a linked list  which is often referred to as a free list figure 10.7 shows the file of figure 10.4  with the free list  after records 1  4  and 6 have been deleted on insertion of a new record  we use the record pointed to by the header we change the header pointer to point to the next available record if no space is available  we add the new record to the end of the file insertion and deletion for files of fixed-length records are simple to implement  because the space made available by a deleted record is exactly the space needed to insert a record if we allow records of variable length in a file  this match no longer holds an inserted record may not fit in the space left free by a deleted record  or it may fill only part of that space 10.5.2 variable-length records variable-length records arise in database systems in several ways  ? storage of multiple record types in a file ? record types that allow variable lengths for one or more fields ? record types that allow repeating fields  such as arrays or multisets thesumit67.blogspot.com 10.5 file organization 455 header record 0 record 1 record 2 record 3 record 4 record 5 record 6 record 7 record 8 record 9 record 10 record 11 72000 92000 80000 65000 40000 95000 87000 62000 76766 83821 98345 10101 15151 22222 33456 58583 76543 crick brandt kim srinivasan mozart einstein gold califieri singh biology elec eng comp sci comp sci music physics physics history finance 80000 figure 10.7 file of figure 10.4  with free list after deletion of records 1  4  and 6 different techniques for implementing variable-length records exist two different problems must be solved by any such technique  ? how to represent a single record in such a way that individual attributes can be extracted easily ? how to store variable-length records within a block  such that records in a block can be extracted easily the representation of a record with variable-length attributes typically has two parts  an initial part with fixed length attributes  followed by data for variablelength attributes fixed-length attributes  such as numeric values  dates  or fixedlength character strings are allocated as many bytes as required to store their value variable-length attributes  such as varchar types  are represented in the initial part of the record by a pair  offset  length   where offset denotes where the data for that attribute begins within the record  and length is the length in bytes of the variable-sized attribute the values for these attributes are stored consecutively  after the initial fixed-length part of the record thus  the initial part of the record stores a fixed size of information about each attribute  whether it is fixed-length or variable-length an example of such a record representation is shown in figure 10.8 the figure shows an instructor record,whose first three attributes id  name  and dept name are variable-length strings  and whose fourth attribute salary is a fixed-sized number we assume that the offset and length values are stored in two bytes each  for a total of 4 bytes per attribute the salary attribute is assumed to be stored in 8 bytes  and each string takes as many bytes as it has characters thesumit67.blogspot.com 456 chapter 10 storage and file structure 21  5 26  10 36  10 65000 10101 srinivasan comp sci bytes 0 4 8 12 20 21 26 36 45 0000 null bitmap  stored in 1 byte  figure 10.8 representation of variable-length record the figure also illustrates the use of a null bitmap  which indicates which attributes of the record have a null value in this particular record  if the salary were null  the fourth bit of the bitmapwould be set to 1  and the salary value stored in bytes 12 through 19 would be ignored since the record has four attributes  the null bitmap for this record fits in 1 byte  although more bytes may be required with more attributes in some representations  the null bitmap is stored at the beginning of the record  and for attributes that are null  no data  value  or offset/length  are stored at all such a representation would save some storage space  at the cost of extra work to extract attributes of the record this representation is particularly useful for certain applications where records have a large number of fields  most of which are null we next address the problem of storing variable-length records in a block the slotted-page structure is commonly used for organizing records within a block  and is shown in figure 10.9.3 there is a header at the beginning of each block  containing the following information  1 the number of record entries in the header 2 the end of free space in the block 3 an array whose entries contain the location and size of each record the actual records are allocated contiguously in the block  starting from the end of the block the free space in the block is contiguous  between the final entry in the header array  and the first record if a record is inserted  space is allocated for it at the end of free space  and an entry containing its size and location is added to the header if a record is deleted  the space that it occupies is freed  and its entry is set to deleted  its size is set to -1  for example   further  the records in the block before the deleted record are moved  so that the free space created by the deletion gets occupied  and all free space is again between the final entry in the header array and the first record the end-of-free-space pointer in the header is appropriately updated as well records can be grown or shrunk by similar techniques  as long as there is space in the block the cost of moving the records is not too high  since the size of a block is limited  typical values are around 4 to 8 kilobytes the slotted-page structure requires that there be no pointers that point directly to records instead  pointers must point to the entry in the header that contains the 3here  ? page ? is synonymous with ? block ? thesumit67.blogspot.com 10.6 organization of records in files 457 size # entries location block header records free space end of free space figure 10.9 slotted-page structure actual location of the record this level of indirection allows records to bemoved to prevent fragmentation of space inside a block  while supporting indirect pointers to the record databases often store data that can be much larger than a disk block for instance  an image or an audio recording may be multiple megabytes in size  while a video object may be multiple gigabytes in size recall that sql supports the types blob and clob  which store binary and character large objects most relational databases restrict the size of a record to be no larger than the size of a block  to simplify buffer management and free-space management large objects are often stored in a special file  or collection of files  instead of being stored with the other  short  attributes of records in which they occur a  logical  pointer to the object is then stored in the record containing the large object large objects are often represented using b + -tree file organizations  which we study in section 11.4.1 b + -tree file organizations permit us to read an entire object  or specified byte ranges in the object  as well as to insert and delete parts of the object 10.6 organization of records in files so far  we have studied how records are represented in a file structure a relation is a set of records given a set of records  the next question is how to organize them in a file several of the possible ways of organizing records in files are  ? heap file organization any record can be placed anywhere in the file where there is space for the record there is no ordering of records typically  there is a single file for each relation ? sequential file organization records are stored in sequential order  according to the value of a ? search key ? of each record section 10.6.1 describes this organization ? hashing file organization a hash function is computed on some attribute of each record the result of the hash function specifies in which block of the thesumit67.blogspot.com 458 chapter 10 storage and file structure 10101 srinivasan 45565 katz 58583 califieri 76543 singh 76766 crick 83821 brandt 98345 kim 12121 wu 15151 mozart 22222 einstein 32343 el said 33456 gold comp sci comp sci comp sci history finance biology elec eng finance music physics history physics 65000 75000 62000 80000 72000 92000 80000 90000 40000 95000 60000 87000 figure 10.10 sequential file for instructor records file the record should be placed chapter 11 describes this organization ; it is closely related to the indexing structures described in that chapter generally  a separate file is used to store the records of each relation.however  in a multitable clustering file organization  records of several different relations are stored in the same file ; further  related records of the different relations are stored on the same block  so that one i/o operation fetches related records from all the relations for example  records of the two relations can be considered to be related if they would match in a join of the two relations section 10.6.2 describes this organization 10.6.1 sequential file organization a sequential file is designed for efficient processing of records in sorted order based on some search key a search key is any attribute or set of attributes ; it need not be the primary key  or even a superkey to permit fast retrieval of records in search-key order  we chain together records by pointers the pointer in each record points to the next record in search-key order furthermore  tominimize the number of block accesses in sequential file processing,we store records physically in search-key order  or as close to search-key order as possible figure 10.10 shows a sequential file of instructor records taken from our university example in that example  the records are stored in search-key order  using id as the search key the sequential file organization allows records to be read in sorted order ; that can be useful for display purposes  as well as for certain query-processing algorithms that we shall study in chapter 12 it is difficult  however  to maintain physical sequential order as records are inserted and deleted  since it is costly to move many records as a result of a single thesumit67.blogspot.com 10.6 organization of records in files 459 10101 45565 76543 76766 83821 98345 12121 15151 22222 32343 33456 58583 srinivasan katz singh crick brandt kim wu mozart einstein el said gold califieri comp sci comp sci finance biology comp sci elec eng finance music physics history physics history 65000 75000 80000 72000 92000 80000 90000 40000 95000 60000 87000 62000 32222 verdi music 48000 figure 10.11 sequential file after an insertion insertion or deletion.we can manage deletion by using pointer chains  as we saw previously for insertion  we apply the following rules  1 locate the record in the file that comes before the record to be inserted in search-key order 2 if there is a free record  that is  space left after a deletion  within the same block as this record  insert the new record there otherwise  insert the new record in an overflow block in either case  adjust the pointers so as to chain together the records in search-key order figure 10.11 shows the file of figure 10.10 after the insertion of the record  32222  verdi  music  48000   the structure in figure 10.11 allows fast insertion of new records  but forces sequential file-processing applications to process records in an order that does not match the physical order of the records if relatively few records need to be stored in overflow blocks  this approach works well eventually  however  the correspondence between search-key order and physical order may be totally lost over a period of time  in which case sequential processing will become much less efficient at this point  the file should be reorganized so that it is once again physically in sequential order such reorganizations are costly  and must be done during times when the system load is low the frequency with which reorganizations are needed depends on the frequency of insertion of new records in the extreme case in which insertions rarely occur  it is possible always to keep the file in physically sorted order in such a case  the pointer field in figure 10.10 is not needed thesumit67.blogspot.com 460 chapter 10 storage and file structure 10.6.2 multitable clustering file organization many relational database systems store each relation in a separate file  so that they can take full advantage of the file system that the operating system provides usually  tuples of a relation can be represented as fixed-length records thus  relations can be mapped to a simple file structure this simple implementation of a relational database system is well suited to low-cost database implementations as in  for example  embedded systems or portable devices in such systems  the size of the database is small  so little is gained from a sophisticated file structure furthermore  in such environments  it is essential that the overall size of the object code for the database system be small a simple file structure reduces the amount of code needed to implement the system this simple approach to relational database implementation becomes less satisfactory as the size of the database increases.we have seen that there are performance advantages to be gained from careful assignment of records to blocks  and from careful organization of the blocks themselves clearly  a more complicated file structuremay be beneficial  even ifwe retain the strategy of storing each relation in a separate file however  many large-scale database systems do not rely directly on the underlying operating system for file management instead  one large operatingsystem file is allocated to the database system the database system stores all relations in this one file  and manages the file itself even if multiple relations are stored in a single file  by default most databases store records of only one relation in a given block this simplifies data management however  in some cases it can be useful to store records of more than one relation in a single block to see the advantage of storing records of multiple relations in one block  consider the following sql query for the university database  select dept name  building  budget  id  name  salary from department natural join instructor ; this query computes a join of the department and instructor relations thus  for each tuple of department  the system must locate the instructor tuples with the same value for dept name ideally  these records will be located with the help of indices  which we shall discuss in chapter 11 regardless of how these records are located  however  they need to be transferred fromdisk intomainmemory in the worst case  each record will reside on a different block  forcing us to do one block read for each record required by the query dept name building budget comp sci taylor 100000 physics watson 70000 figure 10.12 the department relation thesumit67.blogspot.com 10.6 organization of records in files 461 id name dept name salary 10101 srinivasan comp sci 65000 33456 gold physics 87000 45565 katz comp sci 75000 83821 brandt comp sci 92000 figure 10.13 the instructor relation as a concrete example  consider the department and instructor relations of figures 10.12 and 10.13  respectively  for brevity  we include only a subset of the tuples of the relations we have used thus far   in figure 10.14  we show a file structure designed for efficient execution of queries involving the natural join of department and instructor the instructor tuples for each id are stored near the department tuple for the corresponding dept name this structure mixes together tuples of two relations  but allows for efficient processing of the join when a tuple of the department relation is read  the entire block containing that tuple is copied from disk into main memory since the corresponding instructor tuples are stored on the disk near the department tuple  the block containing the department tuple contains tuples of the instructor relation needed to process the query if a department has so many instructors that the instructor records do not fit in one block  the remaining records appear on nearby blocks a multitable clustering file organization is a file organization  such as that illustrated in figure 10.14  that stores related records of two or more relations in each block such a file organization allows us to read records that would satisfy the join condition by using one block read thus  we are able to process this particular query more efficiently in the representation shown in figure 10.14  the dept name attribute is omitted from instructor records since it can be inferred from the associated department record ; the attribute may be retained in some implementations  to simplify access to the attributes.we assume that each record contains the identifier of the relation to which it belongs  although this is not shown in figure 10.14 our use of clustering of multiple tables into a single file has enhanced processing of a particular join  that of department and instructor   but it results in slowing processing of other types of queries for example  katz srinivasan taylor brandt watson gold 100000 70000 87000 92000 75000 65000 comp sci 45564 10101 83821 physics 33456 figure 10.14 multitable clustering file structure thesumit67.blogspot.com 462 chapter 10 storage and file structure katz srinivasan taylor brandt watson gold 100000 70000 87000 92000 75000 65000 comp sci 45564 10101 83821 physics 33456 figure 10.15 multitable clustering file structure with pointer chains select * from department ; requires more block accesses than it did in the scheme under which we stored each relation in a separate file  since each block now contains significantly fewer department records to locate efficiently all tuples of the department relation in the structure of figure 10.14  we can chain together all the records of that relation using pointers  as in figure 10.15 when multitable clustering is to be used depends on the types of queries that the database designer believes to be most frequent careful use of multitable clustering can produce significant performance gains in query processing 10.7 data-dictionary storage so far  we have considered only the representation of the relations themselves a relational database system needs to maintain data about the relations  such as the schema of the relations in general  such ? data about data ? is referred to as metadata relational schemas and other metadata about relations are stored in a structure called the data dictionary or system catalog.among the types of information that the system must store are these  ? names of the relations ? names of the attributes of each relation ? domains and lengths of attributes ? names of views defined on the database  and definitions of those views ? integrity constraints  for example  key constraints   in addition  many systems keep the following data on users of the system  ? names of authorized users ? authorization and accounting information about users thesumit67.blogspot.com 10.7 data-dictionary storage 463 ? passwords or other information used to authenticate users further  the database may store statistical and descriptive data about the relations  such as  ? number of tuples in each relation ? method of storage for each relation  for example  clustered or nonclustered   the data dictionary may also note the storage organization  sequential  hash  or heap  of relations  and the location where each relation is stored  ? if relations are stored in operating system files  the dictionary would note the names of the file  or files  containing each relation ? if the database stores all relations in a single file  the dictionary may note the blocks containing records of each relation in a data structure such as a linked list in chapter 11  in which we study indices  we shall see a need to store information about each index on each of the relations  ? name of the index ? name of the relation being indexed ? attributes on which the index is defined ? type of index formed all this metadata information constitutes  in effect  a miniature database some database systems store such metadata by using special-purpose data structures and code it is generally preferable to store the data about the database as relations in the database itself by using database relations to store system metadata  we simplify the overall structure of the system and harness the full power of the database for fast access to system data the exact choice of how to represent system metadata by relations must be made by the system designers one possible representation  with primary keys underlined  is shown in figure 10.16 in this representation  the attribute index attributes of the relation index metadata is assumed to contain a list of one or more attributes  which can be represented by a character string such as ? dept name  building ?  the index metadata relation is thus not in first normal form ; it can be normalized  but the above representation is likely to be more efficient to access the data dictionary is often stored in a nonnormalized form to achieve fast access whenever the database system needs to retrieve records from a relation  it must first consult the relation metadata relation to find the location and storage organization of the relation  and then fetch records using this information however  the storage organization and location of the relation metadata relation itself thesumit67.blogspot.com 464 chapter 10 storage and file structure relation_metadata relation_name number_of_a  ributes storage_organization location index_metadata index_name relation_name index_type index_a  ributes view_metadata view_name definition a  ribute_metadata relation_name a  ribute_name domain_type position length user_metadata user_name encrypted_password group figure 10.16 relational schema representing system metadata must be recorded elsewhere  for example  in the database code itself  or in a fixed location in the database   since we need this information to find the contents of relation metadata 10.8 database buffer a major goal of the database system is to minimize the number of block transfers between the disk and memory one way to reduce the number of disk accesses is to keep as many blocks as possible in main memory the goal is to maximize the chance that  when a block is accessed  it is already in main memory  and  thus  no disk access is required since it is not possible to keep all blocks inmainmemory  we need to manage the allocation of the space available in main memory for the storage of blocks the buffer is that part of main memory available for storage of copies of disk blocks there is always a copy kept on disk of every block  but the copy on disk may be a version of the block older than the version in the buffer the subsystem responsible for the allocation of buffer space is called the buffer manager 10.8.1 buffer manager programs in a database system make requests  that is  calls  on the buffer manager when they need a block from disk if the block is already in the buffer  the buffer manager passes the address of the block in main memory to the requester if the thesumit67.blogspot.com 10.8 database buffer 465 block is not in the buffer  the buffer manager first allocates space in the buffer for the block  throwing out some other block  if necessary  to make space for the new block the thrown-out block is written back to disk only if it has been modified since the most recent time that it waswritten to the disk then  the buffer manager reads in the requested block from the disk to the buffer  and passes the address of the block in main memory to the requester the internal actions of the buffer manager are transparent to the programs that issue disk-block requests if you are familiar with operating-system concepts  you will note that the buffer manager appears to be nothing more than a virtual-memory manager  like those found in most operating systems one difference is that the size of the database might be larger than the hardware address space of a machine  so memory addresses are not sufficient to address all disk blocks further  to serve the database system well  the buffer manager must use techniques more sophisticated than typical virtual-memory management schemes  ? buffer replacement strategy when there is no room left in the buffer  a block must be removed from the buffer before a new one can be read in most operating systems use a least recently used  lru  scheme  in which the block that was referenced least recently is written back to disk and is removed from the buffer this simple approach can be improved on for database applications ? pinned blocks for the database system to be able to recover from crashes  chapter 16   it is necessary to restrict those times when a block may be written back to disk for instance  most recovery systems require that a block should not be written to disk while an update on the block is in progress a block that is not allowed to be written back to disk is said to be pinned although many operating systems do not support pinned blocks  such a feature is essential for a database system that is resilient to crashes ? forced output of blocks there are situations inwhich it is necessary to write back the block to disk  even though the buffer space that it occupies is not needed this write is called the forced output of a block we shall see the reason for forced output in chapter 16 ; briefly  main-memory contents and thus buffer contents are lost in a crash  whereas data on disk usually survive a crash 10.8.2 buffer-replacement policies the goal of a replacement strategy for blocks in the buffer is to minimize accesses to the disk for general-purpose programs  it is not possible to predict accurately which blockswill be referenced therefore  operating systems use the past pattern of block references as a predictor of future references the assumption generally made is that blocks that have been referenced recently are likely to be referenced again therefore  if a block must be replaced  the least recently referenced block is replaced this approach is called the least recently used  lru  block-replacement scheme thesumit67.blogspot.com 466 chapter 10 storage and file structure for each tuple i of instructor do for each tuple d of department do if i  dept name  = d  dept name  then begin let x be a tuple defined as follows  x  id   = i  id  x  dept name   = i  dept name  x  name   = i  name  x  salary   = i  salary  x  building   = d  building  x  budget   = d  budget  include tuple x as part of result of instructor  department end end end figure 10.17 procedure for computing join lru is an acceptable replacement scheme in operating systems however  a database system is able to predict the pattern of future references more accurately than an operating system a user request to the database system involves several steps the database system is often able to determine in advance which blocks will be needed by looking at each of the steps required to perform the userrequested operation thus  unlike operating systems  which must rely on the past to predict the future  database systems may have information regarding at least the short-term future to illustrate how information about future block access allows us to improve the lru strategy  consider the processing of the sql query  select * from instructor natural join department ; assume that the strategy chosen to process this request is given by the pseudocode program shown in figure 10.17  we shall study other  more efficient  strategies in chapter 12  assume that the two relations of this example are stored in separate files in this example  we can see that  once a tuple of instructor has been processed  that tuple is not needed again therefore  once processing of an entire block of instructor tuples is completed  that block is no longer needed in main memory  even though it has been used recently the buffer manager should be instructed to free the space occupied by an instructor block as soon as the final tuple has been processed this buffer-management strategy is called the toss-immediate strategy now consider blocks containing department tuples.we need to examine every block of department tuples once for each tuple of the instructor relation when thesumit67.blogspot.com 10.8 database buffer 467 processing of a department block is completed  we know that that block will not be accessed again until all other department blocks have been processed thus  the most recently used department block will be the final block to be re-referenced  and the least recently used department block is the block that will be referenced next this assumption set is the exact opposite of the one that forms the basis for the lru strategy indeed  the optimal strategy for block replacement for the above procedure is the most recently used  mru  strategy if a department blockmust be removed from the buffer  the mru strategy chooses the most recently used block  blocks are not eligible for replacement while they are being used   for the mru strategy to work correctly for our example  the system must pin the department block currently being processed after the final department tuple has been processed  the block is unpinned  and it becomes themost recently used block in addition to using knowledge that the system may have about the request being processed  the buffer manager can use statistical information about the probability that a requestwill reference a particular relation for example  the data dictionary that  as we will see in detail in section 10.7  keeps track of the logical schema of the relations as well as their physical storage information is one of the most frequently accessed parts of the database thus  the buffer manager should try not to remove data-dictionary blocks from main memory  unless other factors dictate that it do so in chapter 11  we discuss indices for files since an index for a file may be accessed more frequently than the file itself  the buffer manager should  in general  not remove index blocks from main memory if alternatives are available the ideal database block-replacement strategy needs knowledge of the database operations ? both those being performed and those that will be performed in the future no single strategy is known that handles all the possible scenarios well indeed  a surprisingly large number of database systems use lru  despite that strategy ? s faults the practice questions and exercises explore alternative strategies the strategy that the buffer manager uses for block replacement is influenced by factors other than the time at which the block will be referenced again if the system is processing requests by several users concurrently  the concurrencycontrol subsystem  chapter 15  may need to delay certain requests  to ensure preservation of database consistency if the buffer manager is given information from the concurrency-control subsystem indicating which requests are being delayed  it can use this information to alter its block-replacement strategy specifically  blocks needed by active  nondelayed  requests can be retained in the buffer at the expense of blocks needed by the delayed requests the crash-recovery subsystem  chapter 16  imposes stringent constraints on block replacement if a block has been modified  the buffer manager is not allowed to write back the new version of the block in the buffer to disk  since that would destroy the old version instead  the block manager must seek permission from the crash-recovery subsystem before writing out a block the crash-recovery subsystem may demand that certain other blocks be force-output before it grants permission to the buffer manager to output the block requested in chapter 16  thesumit67.blogspot.com 468 chapter 10 storage and file structure we define precisely the interaction between the buffer manager and the crashrecovery subsystem 10.9 summary ? several types of data storage exist in most computer systems they are classified by the speed with which they can access data  by their cost per unit of data to buy the memory  and by their reliability among the media available are cache  main memory  flash memory  magnetic disks  optical disks  and magnetic tapes ? two factors determine the reliability of storage media  whether a power failure or system crash causes data to be lost  and what the likelihood is of physical failure of the storage device ? we can reduce the likelihood of physical failure by retaining multiple copies of data for disks  we can use mirroring or we can use more sophisticated methods based on redundant arrays of independent disks  raid   by striping data across disks  these methods offer high throughput rates on large accesses ; by introducing redundancy across disks  they improve reliability greatly several different raid organizations are possible  each with different cost  performance  and reliability characteristics raid level 1  mirroring  and raid level 5 are the most commonly used ? we can organize a file logically as a sequence of records mapped onto disk blocks one approach to mapping the database to files is to use several files  and to store records of only one fixed length in any given file an alternative is to structure files so that they can accommodate multiple lengths for records the slotted-page method is widely used to handle varying length records within a disk block ? since data are transferred between disk storage and main memory in units of a block  it is worthwhile to assign file records to blocks in such a way that a single block contains related records if we can access several of the records we want with only one block access  we save disk accesses since disk accesses are usually the bottleneck in the performance of a database system  careful assignment of records to blocks can pay significant performance dividends ? the data dictionary  also referred to as the system catalog  keeps track of metadata  that is data about data  such as relation names  attribute names and types  storage information  integrity constraints  and user information ? one way to reduce the number of disk accesses is to keep as many blocks as possible in main memory since it is not possible to keep all blocks in main memory  we need to manage the allocation of the space available in main memory for the storage of blocks the buffer is that part of main memory available for storage of copies of disk blocks the subsystem responsible for the allocation of buffer space is called the buffer manager thesumit67.blogspot.com review terms 469 review terms ? physical storage media ? cache ? main memory ? flash memory ? magnetic disk ? optical storage ? magnetic disk ? platter ? hard disks ? floppy disks ? tracks ? sectors ? read ? write head ? disk arm ? cylinder ? disk controller ? checksums ? remapping of bad sectors ? performance measures of disks ? access time ? seek time ? rotational latency ? data-transfer rate ? mean time to failure  mttf  ? disk block ? optimization of disk-block access ? disk-arm scheduling ? elevator algorithm ? file organization ? defragmenting ? nonvolatile write buffers ? nonvolatile random-access memory  nvram  ? log disk ? redundant arrays of independent disks  raid  ? mirroring ? data striping ? bit-level striping ? block-level striping ? raid levels ? level 0  block striping  no redundancy  ? level 1  block striping  mirroring  ? level 3  bit striping  parity  ? level 5  block striping  distributed parity  ? level 6  block striping  p + q redundancy  ? rebuild performance ? software raid ? hardware raid ? hot swapping ? tertiary storage ? optical disks ? magnetic tapes ? jukeboxes ? file ? file organization ? file header thesumit67.blogspot.com 470 chapter 10 storage and file structure ? free list ? variable-length records ? slotted-page structure ? large objects ? heap file organization ? sequential file organization ? hashing file organization ? multitable clustering file organization ? search key ? data dictionary ? system catalog ? buffer ? buffer manager ? pinned blocks ? forced output of blocks ? buffer-replacement policies ? least recently used  lru  ? toss-immediate ? most recently used  mru  practice exercises 10.1 consider the data and parity-block arrangement on four disks depicted in figure 10.18 the bi s represent data blocks ; the pi s represent parity blocks parity block pi is the parity block for data blocks b4i-3 to b4i what  if any  problem might this arrangement present ? 10.2 flash storage  a how is the flash translation table  which is used to map logical page numbers to physical page numbers  created in memory ? b suppose you have a 64 gigabyte flash storage system  with a 4096 byte page size how big would the flash translation table be  assuming each page has a 32 bit address  and the table is stored as an array c suggest how to reduce the size of the translation table if very often long ranges of consecutive logical page numbers are mapped to consecutive physical page numbers disk 1 disk 2 disk 3 disk 4 b1 p1 b8 ? b2 b5 p2 ? b3 b6 b9 ? b4 b7 b10 ? figure 10.18 data and parity block arrangement thesumit67.blogspot.com practice exercises 471 10.3 apower failure that occurs while a disk block is beingwritten could result in the block being only partially written assume that partially written blocks can be detected an atomic block write is one where either the disk block is fully written or nothing is written  i.e  there are no partial writes   suggest schemes for getting the effect of atomic block writes with the following raid schemes your schemes should involve work on recovery from failure a raid level 1  mirroring  b raid level 5  block interleaved  distributed parity  10.4 consider the deletion of record 5 from the file of figure 10.6 compare the relative merits of the following techniques for implementing the deletion  a move record 6 to the space occupied by record 5  and move record 7 to the space occupied by record 6 b move record 7 to the space occupied by record 5 c mark record 5 as deleted  and move no records 10.5 show the structure of the file of figure 10.7 after each of the following steps  a insert  24556  turnamian  finance  98000   b delete record 2 c insert  34556  thompson  music  67000   10.6 consider the relations section and takes give an example instance of these two relations  with three sections  each of which has five students give a file structure of these relations that uses multitable clustering 10.7 consider the following bitmap technique for tracking free space in a file for each block in the file  two bits are maintained in the bitmap if the block is between 0 and 30 percent full the bits are 00  between 30 and 60 percent the bits are 01  between 60 and 90 percent the bits are 10  and above 90 percent the bits are 11 such bitmaps can be kept in memory even for quite large files a describe howto keep the bitmap up to date on record insertions and deletions b outline the benefit of the bitmap technique over free lists in searching for free space and in updating free space information 10.8 it is important to be able to quickly find out if a block is present in the buffer  and if so where in the buffer it resides given that database buffer sizes are very large  what  in-memory  data structure would you use for the above task ? thesumit67.blogspot.com 472 chapter 10 storage and file structure 10.9 give an example of a relational-algebra expression and a query-processing strategy in each of the following situations  a mru is preferable to lru b lru is preferable to mru exercises 10.10 list the physical storage media available on the computers you use routinely give the speed with which data can be accessed on each medium 10.11 how does the remapping of bad sectors by disk controllers affect dataretrieval rates ? 10.12 raid systems typically allow you to replace failed disks without stopping access to the system thus  the data in the failed disk must be rebuilt and written to the replacement diskwhile the system is in operation which of the raid levels yields the least amount of interference between the rebuild and ongoing disk accesses ? explain your answer 10.13 what is scrubbing  in the context of raid systems  and why is scrubbing important ? 10.14 in the variable-length record representation  a null bitmap is used to indicate if an attribute has the null value a for variable length fields  if the value is null  what would be stored in the offset and length fields ? b in some applications  tuples have a very large number of attributes  most of which are null can you modify the record representation such that the only overhead for a null attribute is the single bit in the null bitmap 10.15 explain why the allocation of records to blocks affects database-system performance significantly 10.16 if possible  determine the buffer-management strategy used by the operating system running on your local computer system andwhat mechanisms it provides to control replacement of pages discuss how the control on replacement that it provides would be useful for the implementation of database systems 10.17 list two advantages and two disadvantages of each of the following strategies for storing a relational database  a store each relation in one file b store multiple relations  perhaps even the entire database  in one file thesumit67.blogspot.com bibliographical notes 473 10.18 in the sequential file organization  why is an overflow block used even if there is  at the moment  only one overflow record ? 10.19 give a normalized version of the index metadata relation  and explainwhy using the normalized version would result in worse performance 10.20 if you have data that should not be lost on disk failure  and the data are write intensive  how would you store the data ? 10.21 in earlier generation disks the number of sectors per track was the same across all tracks current generation disks have more sectors per track on outer tracks  and fewer sectors per track on inner tracks  since they are shorter in length  .what is the effect of such a change on each of the three main indicators of disk speed ? 10.22 standard buffer managers assume each block is of the same size and costs the same to read consider a buffer manager that  instead of lru  uses the rate of reference to objects  that is  how often an object has been accessed in the last n seconds suppose we want to store in the buffer objects of varying sizes  and varying read costs  such asweb pages,whose read cost depends on the site from which they are fetched   suggest how a buffer manager may choose which block to evict from the buffer bibliographical notes hennessy et al  2006  is a popular textbook on computer architecture  which includes coverage of hardware aspects of translation look-aside buffers  caches  and memory-management units rosch  2003  presents an excellent overview of computer hardware  including extensive coverage of all types of storage technology such as magnetic disks  optical disks  tapes  and storage interfaces patterson  2004  provides a good discussion on how latency improvements have lagged behind bandwidth  transfer rate  improvements with the rapid increase in cpu speeds  cache memory located along with the cpu has become much faster than main memory although database systems do not control what data is kept in cache  there is an increasing motivation to organize data in memory and write programs in such a way that cache utilization is maximized work in this area includes rao and ross  2000   ailamaki et al  2001   zhou and ross  2004   garcia and korth  2005   and cieslewicz et al  2009   the specifications of current-generation disk drives can be obtained from the web sites of their manufacturers  such ashitachi  lacie  iomega  seagate  maxtor  and western digital discussions of redundant arrays of inexpensive disks  raid  are presented by patterson et al  1988   chen et al  1994  presents an excellent survey of raid principles and implementation reed ? solomon codes are covered in pless  1998   buffering data in mobile systems is discussed in imielinski and badrinath  1994   imielinski and korth  1996   and chandrasekaran et al  2003   thesumit67.blogspot.com 474 chapter 10 storage and file structure the storage structure of specific database systems  such as ibm db2  oracle  microsoft sql server  and postgresql are documented in their respective system manuals buffer management is discussed in most operating-system texts  including in silberschatz et al  2008   chou and dewitt  1985  presents algorithms for buffer management in database systems  and describes a performance evaluation thesumit67.blogspot.com chapter11 indexing and hashing many queries reference only a small proportion of the records in a file for example  a query like ? find all instructors in the physics department ? or ? find the total number of credits earned by the student with id 22201 ? references only a fraction of the student records it is inefficient for the system to read every tuple in the instructor relation to check if the dept name value is ? physics ?  likewise  it is inefficient to read the entire student relation just to find the one tuple for the id ? 32556  ?  ideally  the system should be able to locate these records directly to allow these forms of access  we design additional structures that we associate with files 11.1 basic concepts an index for a file in a database system works in much the same way as the index in this textbook if we want to learn about a particular topic  specified by a word or a phrase  in this textbook  we can search for the topic in the index at the back of the book  find the pages where it occurs  and then read the pages to find the information for which we are looking the words in the index are in sorted order  making it easy to find the word we want moreover  the index is much smaller than the book  further reducing the effort needed database-system indices play the same role as book indices in libraries for example  to retrieve a student record given an id  the database system would look up an index to find on which disk block the corresponding record resides  and then fetch the disk block  to get the appropriate student record keeping a sorted list of students ? id would not work well on very large databases with thousands of students  since the index would itself be very big ; further  even though keeping the index sorted reduces the search time  finding a student can still be rather time-consuming instead  more sophisticated indexing techniques may be used we shall discuss several of these techniques in this chapter there are two basic kinds of indices  ? ordered indices based on a sorted ordering of the values 475 thesumit67.blogspot.com 476 chapter 11 indexing and hashing ? hash indices based on a uniform distribution of values across a range of buckets the bucket to which a value is assigned is determined by a function  called a hash function we shall consider several techniques for both ordered indexing and hashing no one technique is the best rather  each technique is best suited to particular database applications each technique must be evaluated on the basis of these factors  ? access types  the types of access that are supported efficiently access types can include finding records with a specified attribute value and finding records whose attribute values fall in a specified range ? access time  the time it takes to find a particular data item  or set of items  using the technique in question ? insertion time  the time it takes to insert a newdata item this value includes the time it takes to find the correct place to insert the new data item  as well as the time it takes to update the index structure ? deletion time  the time it takes to delete a data item this value includes the time it takes to find the item to be deleted  as well as the time it takes to update the index structure ? space overhead  the additional space occupied by an index structure provided that the amount of additional space is moderate  it is usually worthwhile to sacrifice the space to achieve improved performance we often want to have more than one index for a file for example  we may wish to search for a book by author  by subject  or by title an attribute or set of attributes used to look up records in a file is called a search key note that this definition of key differs from that used in primary key  candidate key  and superkey this duplicate meaning for key is  unfortunately  well established in practice using our notion of a search key  we see that if there are several indices on a file  there are several search keys 11.2 ordered indices to gain fast random access to records in a file  we can use an index structure each index structure is associated with a particular search key just like the index of a book or a library catalog  an ordered index stores the values of the search keys in sorted order  and associates with each search key the records that contain it the records in the indexed filemay themselves be stored in some sorted order  just as books in a library are stored according to some attribute such as the dewey decimal number a file may have several indices  on different search keys if the file containing the records is sequentially ordered  a clustering index is an index whose search key also defines the sequential order of the file clustering indices thesumit67.blogspot.com 11.2 ordered indices 477 10101 srinivasan 45565 katz 58583 califieri 76543 singh 76766 crick 83821 brandt 98345 kim 12121 wu 15151 mozart 22222 einstein 32343 el said 33456 gold comp sci comp sci comp sci history finance biology elec eng finance music physics history physics 65000 75000 62000 80000 72000 92000 80000 90000 40000 95000 60000 87000 figure 11.1 sequential file for instructor records are also called primary indices ; the term primary index may appear to denote an index on a primary key  but such indices can in fact be built on any search key the search key of a clustering index is often the primary key  although that is not necessarily so indices whose search key specifies an order different from the sequential order of the file are called nonclustering indices  or secondary indices the terms ? clustered ? and ? nonclustered ? are often used in place of ? clustering ? and ? nonclustering ? in sections 11.2.1 through 11.2.3  we assume that all files are ordered sequentially on some search key such files  with a clustering index on the search key  are called index-sequential files they represent one of the oldest index schemes used in database systems they are designed for applications that require both sequential processing of the entire file and random access to individual records in section 11.2.4 we cover secondary indices figure 11.1 shows a sequential file of instructor records taken from our university example in the example of figure 11.1  the records are stored in sorted order of instructor id  which is used as the search key 11.2.1 dense and sparse indices an index entry  or index record  consists of a search-key value and pointers to one or more records with that value as their search-key value the pointer to a record consists of the identifier of a disk block and an offset within the disk block to identify the record within the block there are two types of ordered indices that we can use  thesumit67.blogspot.com 478 chapter 11 indexing and hashing 10101 12121 15151 22222 32343 33456 45565 58583 76543 76766 83821 98345 10101 srinivasan 45565 katz 58583 califieri 76543 singh 76766 crick 83821 brandt 98345 kim 12121 wu 15151 mozart 22222 einstein 32343 el said 33456 gold comp sci comp sci comp sci history finance biology elec eng finance music physics history physics 65000 75000 62000 80000 72000 92000 80000 90000 40000 95000 60000 87000 figure 11.2 dense index ? dense index  in a dense index  an index entry appears for every search-key value in the file in a dense clustering index  the index record contains the search-key value and a pointer to the first data record with that search-key value the rest of the recordswith the same search-key valuewould be stored sequentially after the first record  since  because the index is a clustering one  records are sorted on the same search key in a dense nonclustering index  the index must store a list of pointers to all records with the same search-key value ? sparse index  in a sparse index  an index entry appears for only some of the search-key values sparse indices can be used only if the relation is stored in sorted order of the search key  that is  if the index is a clustering index as is true in dense indices  each index entry contains a search-key value and a pointer to the first data record with that search-key value to locate a record  we find the index entry with the largest search-key value that is less than or equal to the search-key value for which we are looking.we start at the record pointed to by that index entry  and follow the pointers in the file untilwe find the desired record figures 11.2 and 11.3 show dense and sparse indices  respectively  for the instructor file suppose that we are looking up the record of instructor with id ? 22222 ?  using the dense index of figure 11.2  we follow the pointer directly to the desired record since id is a primary key  there exists only one such record and the search is complete if we are using the sparse index  figure 11.3   we do not find an index entry for ? 22222 ?  since the last entry  in numerical order  before ? 22222 ? is ? 10101 ?  we follow that pointer we then read the instructor file in sequential order until we find the desired record thesumit67.blogspot.com 11.2 ordered indices 479 consider a  printed  dictionary the header of each page lists the first word alphabetically on that page the words at the top of each page of the book index together form a sparse index on the contents of the dictionary pages as another example  suppose that the search-key value is not not a primary key figure 11.4 shows a dense clustering index for the instructor file with the search key being dept name observe that in this case the instructor file is sorted on the search key dept name  instead of id  otherwise the index on dept name would be a nonclustering index suppose that we are looking up records for the history department using the dense index of figure 11.4  we follow the pointer directly to the first history record.we process this record  and follow the pointer in that record to locate the next record in search-key  dept name  order we continue processing records until we encounter a record for a department other than history as we have seen  it is generally faster to locate a record if we have a dense index rather than a sparse index however  sparse indices have advantages over dense indices in that they require less space and they impose less maintenance overhead for insertions and deletions there is a trade-off that the system designer must make between access time and space overhead although the decision regarding this trade-off depends on the specific application  a good compromise is to have a sparse index with one index entry per block the reason this design is a good trade-off is that the dominant cost in processing a database request is the time that it takes to bring a block from disk into main memory once we have brought in the block  the time to scan the entire block is negligible using this sparse index  we locate the block containing the record that we are seeking thus  unless the record is on an overflow block  see section 10.6.1   we minimize block accesses while keeping the size of the index  and thus our space overhead  as small as possible 10101 32343 76766 10101 srinivasan 45565 katz 58583 califieri 76543 singh 76766 crick 83821 brandt 98345 kim 12121 wu 15151 mozart 22222 einstein 32343 el said 33456 gold comp sci comp sci comp sci history finance biology elec eng finance music physics history physics 65000 75000 62000 80000 72000 92000 80000 90000 40000 95000 60000 87000 figure 11.3 sparse index thesumit67.blogspot.com 480 chapter 11 indexing and hashing biology comp sci elec eng finance history music physics 76766 crick 76543 singh 32343 el said 58583 califieri 15151 mozart 22222 einstein 33465 gold 10101 srinivasan 45565 katz 83821 brandt 98345 kim 12121 wu biology physics finance history history music physics comp sci comp sci comp sci elec eng finance 72000 80000 60000 62000 40000 95000 87000 65000 75000 92000 80000 90000 figure 11.4 dense index with search key dept name for the preceding technique to be fully general  we must consider the case where records for one search-key value occupy several blocks it is easy to modify our scheme to handle this situation 11.2.2 multilevel indices suppose we build a dense index on a relation with 1,000,000 tuples index entries are smaller than data records  so let us assume that 100 index entries fit on a 4 kilobyte block thus  our index occupies 10,000 blocks if the relation instead had 100,000,000 tuples  the index would instead occupy 1,000,000 blocks  or 4 gigabytes of space such large indices are stored as sequential files on disk if an index is small enough to be kept entirely in main memory  the search time to find an entry is low however  if the index is so large that not all of it can be kept in memory  index blocks must be fetched from disk when required  even if an index is smaller than the main memory of a computer  main memory is also required for a number of other tasks  so it may not be possible to keep the entire index in memory  the search for an entry in the index then requires several disk-block reads binary search can be used on the index file to locate an entry  but the search still has a large cost if the index would occupy b blocks  binary search requires as many as  log2  b   blocks to be read   x  denotes the least integer that is greater than or equal to x ; that is  we round upward  for a 10,000-block index  binary search requires 14 block reads on a disk system where a block read takes on average 10 milliseconds  the index search will take 140 milliseconds this may not seem much  but we would be able to carry out only seven index searches a second  whereas a more efficient search mechanism would let us carry out far more searches per second  as we shall see shortly note that  if overflow blocks have been used  binary search is not possible in that case  a sequential search is typically used  and that requires b block reads  which will take even longer thus  the process of searching a large index may be costly thesumit67.blogspot.com 11.2 ordered indices 481 ? ? ? ? outer index index block 0 index block 1 data block 0 data block 1 inner index figure 11.5 two-level sparse index to deal with this problem  we treat the index just as we would treat any other sequential file  and construct a sparse outer index on the original index  whichwe now call the inner index  as shown in figure 11.5 note that the index entries are always in sorted order  allowing the outer index to be sparse to locate a record  we first use binary search on the outer index to find the record for the largest search-key value less than or equal to the one that we desire the pointer points to a block of the inner index we scan this block until we find the record that has the largest search-key value less than or equal to the one that we desire the pointer in this record points to the block of the file that contains the record for which we are looking in our example  an inner index with 10,000 blocks would require 10,000 entries in the outer index  which would occupy just 100 blocks if we assume that the outer index is already in main memory  we would read only one index block for a search using a multilevel index  rather than the 14 blocks we read with binary search as a result  we can perform 14 times as many index searches per second if our file is extremely large  even the outer index may grow too large to fit in main memory with a 100,000,000 tuple relation  the inner index would occupy thesumit67.blogspot.com 482 chapter 11 indexing and hashing 1,000,000 blocks  and the outer index occupies 10,000 blocks  or 40 megabytes since there are many demands on main memory  it may not be possible to reserve that much main memory just for this particular outer index in such a case  we can create yet another level of index indeed  we can repeat this process as many times as necessary indices with two or more levels are called multilevel indices searching for records with a multilevel index requires significantly fewer i/o operations than does searching for records by binary search.1 multilevel indices are closely related to tree structures  such as the binary trees used for in-memory indexing we shall examine the relationship later  in section 11.3 11.2.3 index update regardless ofwhat form of index is used  every index must be updatedwhenever a record is either inserted into or deleted from the file further  in case a record in the file is updated  any index whose search-key attribute is affected by the update must also be updated ; for example  if the department of an instructor is changed  an index on the dept name attribute of instructor must be updated correspondingly such a record update can be modeled as a deletion of the old record  followed by an insertion of the new value of the record  which results in an index deletion followed by an index insertion as a result we only need to consider insertion and deletion on an index  and do not need to consider updates explicitly we first describe algorithms for updating single-level indices ? insertion first  the system performs a lookup using the search-key value that appears in the record to be inserted the actions the system takes next depend on whether the index is dense or sparse  ? dense indices  1 if the search-key value does not appear in the index  the systeminserts an index entry with the search-key value in the index at the appropriate position 2 otherwise the following actions are taken  a if the index entry stores pointers to all recordswith the same searchkey value  the system adds a pointer to the new record in the index entry b otherwise  the index entry stores a pointer to only the first record with the search-key value the system then places the record being inserted after the other records with the same search-key values ? sparse indices  we assume that the index stores an entry for each block if the system creates a new block  it inserts the first search-key value  in 1in the early days of disk-based indices  each level of the index corresponded to a unit of physical storage thus  wemay have indices at the track  cylinder  and disk levels such a hierarchy does not make sense today since disk subsystems hide the physical details of disk storage  and the number of disks and platters per disk is very small compared to the number of cylinders or bytes per track thesumit67.blogspot.com 11.2 ordered indices 483 search-key order  appearing in the new block into the index on the other hand  if the new record has the least search-key value in its block  the system updates the index entry pointing to the block ; if not  the system makes no change to the index ? deletion to delete a record  the system first looks up the record to be deleted the actions the system takes next depend on whether the index is dense or sparse  ? dense indices  1 if the deleted record was the only record with its particular search-key value  then the system deletes the corresponding index entry from the index 2 otherwise the following actions are taken  a if the index entry stores pointers to all recordswith the same searchkey value  the system deletes the pointer to the deleted record from the index entry b otherwise  the index entry stores a pointer to only the first record with the search-key value in this case  if the deleted record was the first record with the search-key value  the system updates the index entry to point to the next record ? sparse indices  1 if the index does not contain an index entry with the search-key value of the deleted record  nothing needs to be done to the index 2 otherwise the system takes the following actions  a if the deleted record was the only record with its search key  the system replaces the corresponding index record with an index record for the next search-key value  in search-key order   if the next search-key value already has an index entry  the entry is deleted instead of being replaced b otherwise  if the index entry for the search-key value points to the record being deleted  the system updates the index entry to point to the next record with the same search-key value insertion and deletion algorithms formultilevel indices are a simple extension of the scheme just described on deletion or insertion  the system updates the lowest-level index as described as far as the second level is concerned  the lowest-level index ismerely a file containing records ? thus  if there is any change in the lowest-level index  the system updates the second-level index as described the same technique applies to further levels of the index  if there are any 11.2.4 secondary indices secondary indices must be dense,with an index entry for every search-key value  and a pointer to every record in the file a clustering index may be sparse  storing thesumit67.blogspot.com 484 chapter 11 indexing and hashing only some of the search-key values  since it is always possible to find records with intermediate search-key values by a sequential access to a part of the file  as described earlier if a secondary index stores only some of the search-key values  records with intermediate search-key values may be anywhere in the file and  in general  we can not find them without searching the entire file a secondary index on a candidate key looks just like a dense clustering index  except that the records pointed to by successive values in the index are not stored sequentially in general  however  secondary indices may have a different structure from clustering indices if the search key of a clustering index is not a candidate key  it suffices if the index points to the first record with a particular value for the search key  since the other records can be fetched by a sequential scan of the file in contrast  if the search key of a secondary index is not a candidate key  it is not enough to point to just the first record with each search-key value the remaining recordswith the same search-key value could be anywhere in the file  since the records are ordered by the search key of the clustering index  rather than by the search key of the secondary index therefore  a secondary indexmust contain pointers to all the records we can use an extra level of indirection to implement secondary indices on search keys that are not candidate keys the pointers in such a secondary index do not point directly to the file instead  each points to a bucket that contains pointers to the file figure 11.6 shows the structure of a secondary index that uses an extra level of indirection on the instructor file  on the search key salary a sequential scan in clustering index order is efficient because records in the file are stored physically in the same order as the index order however  we can not  except in rare special cases  store a file physically ordered by both the search key of the clustering index and the search key of a secondary index 40000 60000 62000 65000 72000 75000 80000 87000 90000 92000 95000 10101 srinivasan comp sci 65000 12121 wu finance 90000 15151 mozart music 40000 22222 einstein physics 95000 32343 el said history 60000 33456 gold physics 87000 45565 katz comp sci 75000 58583 califieri history 62000 76543 singh finance 80000 76766 crick biology 72000 83821 brandt comp sci 92000 98345 kim elec eng 80000 figure 11.6 secondary index on instructor file  on noncandidate key salary thesumit67.blogspot.com 11.3 b + -tree index files 485 automatic creation of indices if a relation is declared to have a primary key  most database implementations automatically create an index on the primary key.whenever a tuple is inserted into the relation  the index can be used to check that the primary key constraint is not violated  that is  there are no duplicates on the primary key value  .without the index on the primary key  whenever a tuple is inserted  the entire relation would have to be read to ensure that the primary-key constraint is satisfied because secondary-key order and physical-key order differ  if we attempt to scan the file sequentially in secondary-key order  the reading of each record is likely to require the reading of a new block from disk  which is very slow the procedure described earlier for deletion and insertion can also be applied to secondary indices ; the actions taken are those described for dense indices storing a pointer to every record in the file if a file hasmultiple indices,whenever the file is modified  every index must be updated secondary indices improve the performance of queries that use keys other than the search key of the clustering index however  they impose a significant overhead on modification of the database the designer of a database decides which secondary indices are desirable on the basis of an estimate of the relative frequency of queries and modifications 11.2.5 indices on multiple keys although the examples we have seen so far have had a single attribute in a search key  in general a search key can have more than one attribute a search key containing more than one attribute is referred to as a composite search key the structure of the index is the same as that of any other index  the only difference being that the search key is not a single attribute  but rather is a list of attributes the search key can be represented as a tuple of values  of the form  a1      an   where the indexed attributes are a1      an the ordering of search-key values is the lexicographic ordering for example  for the case of two attribute search keys   a1  a2  <  b1  b2  if either a1 < b1 or a1 = b1 and a2 < b2 lexicographic ordering is basically the same as alphabetic ordering of words as an example  consider an index on the takes relation  on the composite search key  course id  semester  year   such an index would be useful to find all students who have registered for a particular course in a particular semester/year an ordered index on a composite key can also be used to answer several other kinds of queries efficiently  as we shall see later in section 11.5.2 11.3 b + -tree index files the main disadvantage of the index-sequential file organization is that performance degrades as the file grows  both for index lookups and for sequential scans thesumit67.blogspot.com 486 chapter 11 indexing and hashing through the data although this degradation can be remedied by reorganization of the file  frequent reorganizations are undesirable theb + -tree index structure is themostwidely used of several index structures that maintain their efficiency despite insertion and deletion of data a b + -tree index takes the form of a balanced tree in which every path from the root of the tree to a leaf of the tree is of the same length each nonleaf node in the tree has between  n/2  and n children  where n is fixed for a particular tree we shall see that the b + -tree structure imposes performance overhead on insertion and deletion  and adds space overhead the overhead is acceptable even for frequently modified files  since the cost of file reorganization is avoided furthermore  since nodes may be as much as half empty  if they have the minimum number of children   there is some wasted space this space overhead  too  is acceptable given the performance benefits of the b + -tree structure 11.3.1 structure of a b + -tree a b + -tree index is a multilevel index  but it has a structure that differs from that of the multilevel index-sequential file figure 11.7 shows a typical node of a b +  tree it contains up to n  1 search-key values k1  k2      kn-1  and n pointers p1  p2      pn the search-key valueswithin a node are kept in sorted order ; thus  if i < j  then ki < k j  we consider first the structure of the leaf nodes for i = 1  2      n-1  pointer pi points to a file record with search-key value ki pointer pn has a special purpose that we shall discuss shortly figure 11.8 shows one leaf node of a b + -tree for the instructor file  in which we have chosen n to be 4  and the search key is name now that we have seen the structure of a leaf node  let us consider how search-key values are assigned to particular nodes each leaf can hold up to n  1 values we allow leaf nodes to contain as few as   n  1  /2  values with n = 4 in our example b + -tree  each leaf must contain at least 2 values  and at most 3 values the ranges of values in each leaf do not overlap  except if there are duplicate search-key values  in which case a value may be present in more than one leaf specifically  if li and l j are leaf nodes and i < j  then every search-key value in li is less than or equal to every search-key value in l j if the b + -tree index is used as a dense index  as is usually the case  every search-key value must appear in some leaf node now we can explain the use of the pointer pn since there is a linear order on the leaves based on the search-key values that they contain  we use pn to chain p1 k1 p2 ? pn-1 kn-1 pn figure 11.7 typical node of a b + -tree thesumit67.blogspot.com 11.3 b + -tree index files 487 leaf node pointer to next leaf node instructor file brandt srinivasan califieri crick comp sci 65000 wu finance 90000 mozart music 40000 einstein physics 95000 el said history 80000 gold physics 87000 katz comp sci 75000 califieri history 60000 singh finance 80000 crick biology 72000 brandt comp sci 92000 15151 10101 12121 22222 32343 33456 45565 58583 76543 76766 83821 98345 kim elec eng 80000 figure 11.8 a leaf node for instructor b + -tree index  n = 4   together the leaf nodes in search-key order this ordering allows for efficient sequential processing of the file the nonleaf nodes of the b + -tree form a multilevel  sparse  index on the leaf nodes the structure of nonleaf nodes is the same as that for leaf nodes  except that all pointers are pointers to tree nodes a nonleaf node may hold up to n pointers  and must hold at least  n/2  pointers the number of pointers in a node is called the fanout of the node nonleaf nodes are also referred to as internal nodes let us consider a node containing m pointers  m = n   for i = 2  3     ,m  1  pointer pi points to the subtree that contains search-key values less than ki and greater than or equal to ki -1 pointer pm points to the part of the subtree that contains those key values greater than or equal to km-1  and pointer p1 points to the part of the subtree that contains those search-key values less than k1 unlike other nonleaf nodes  the root node can hold fewer than  n/2  pointers ; however  it must hold at least two pointers  unless the tree consists of only one node it is always possible to construct a b + -tree  for any n  that satisfies the preceding requirements figure 11.9 shows a complete b + -tree for the instructor file  with n = 4   we have shown instructor names abbreviated to 3 characters in order to depict the tree clearly ; in reality  the tree nodes would contain the full names we have also omitted null pointers for simplicity ; any pointer field in the figure that does not have an arrow is understood to have a null value figure 11.10 shows another b + -tree for the instructor file  this time with n = 6 as before  we have abbreviated instructor names only for clarity of presentation thesumit67.blogspot.com 488 chapter 11 indexing and hashing gold katz kim mozart singh srinivasan wu internal nodes root node leaf nodes einstein einstein el said gold mozart srinivasan srinivasan comp sci 65000 wu finance 90000 mozart music 40000 einstein physics 95000 el said history 80000 gold physics 87000 katz comp sci 75000 califieri history 60000 singh finance 80000 crick biology 72000 brandt comp sci 92000 15151 10101 brandt califieri crick 12121 22222 32343 33456 45565 58583 76543 76766 83821 98345 kim elec eng 80000 figure 11.9 b + -tree for instructor file  n = 4   observe that the height of this tree is less than that of the previous tree  which had n = 4 these examples of b + -trees are all balanced that is  the length of every path from the root to a leaf node is the same this property is a requirement for a b +  tree indeed  the ? b ? in b + -tree stands for ? balanced ? it is the balance property of b + -trees that ensures good performance for lookup  insertion  and deletion 11.3.2 queries on b + -trees let us consider how we process queries on a b + -tree suppose that we wish to find records with a search-key value of v figure 11.11 presents pseudocode for a function find   to carry out this task intuitively  the function starts at the root of the tree  and traverses the tree down until it reaches a leaf node that would contain the specified value if it exists in the tree specifically  starting with the root as the current node  the function repeats the following steps until a leaf node is reached first  the current node is examined  looking for the smallest i such that search-key value ki is greater brandt califieri crick einstein el said gold katz kim mozart singh srinivasan wu el said mozart figure 11.10 b + -tree for instructor file with n = 6 thesumit67.blogspot.com 11.3 b + -tree index files 489 function find  value v  / * returns leaf node c and index i such that c.pi points to first record * with search key value v * / set c = root node while  c is not a leaf node  begin let i = smallest number such that v = c.ki if there is no such number i then begin let pm = last non-null pointer in the node set c = c.pm end else if  v = c.ki  then set c = c.pi + 1 else c = c.pi / * v < c.ki * / end / * c is a leaf node * / let i be the least value such that ki = v if there is such a value i then return  c  i  else return null ; / * no record with key value v exists * / procedure printall  value v  / * prints all records with search key value v * / set done = false ; set  l  i  = find  v  ; if   l  i  is null  return repeat repeat print record pointed to by l.pi set i = i + 1 until  i > number of keys in l or l.ki > v  if  i > number of keys in l  then l = l.pn else set done = true ; until  done or l is null  figure 11.11 querying a b + -tree than or equal to v suppose such a value is found ; then  if ki is equal to v  the current node is set to the node pointed to by pi + 1  otherwise ki > v  and the current node is set to the node pointed to by pi if no such value ki is found  then clearly v > km-1  where pm is the last nonnull pointer in the node in this case the current node is set to that pointed to by pm the above procedure is repeated  traversing down the tree until a leaf node is reached at the leaf node  if there is a search-key value equal to v  let ki be the first such value ; pointer pi directs us to a record with search-key value ki  the function thesumit67.blogspot.com 490 chapter 11 indexing and hashing then returns the leaf node l and the index i if no search-key with value v is found in the leaf node  no record with key value v exists in the relation  and function find returns null  to indicate failure if there is at most one record with a search key value v  for example  if the index is on a primary key  the procedure that calls the find function simply uses the pointer l.pi to retrieve the record and is done however  in case there may be more than one matching record  the remaining records also need to be fetched procedure printall shown in figure 11.11 shows how to fetch all records with a specified search key v the procedure first steps through the remaining keys in the node l  to find other records with search-key value v if node l contains at least one search-key value greater than v  then there are no more records matching v otherwise  the next leaf  pointed to by pn may contain further entries for v the node pointed to by pn must then be searched to find further records with search-key value v if the highest search-key value in the node pointed to by pn is also v  further leaves may have to be traversed  in order to find all matching records the repeat loop in printall carries out the task of traversing leaf nodes until all matching records have been found areal implementationwould provide a version of find supporting an iterator interface similar to that provided by the jdbc resultset  which we saw in section 5.1.1 such an iterator interface would provide a method next    which can be called repeatedly to fetch successive records with the specified search-key the next   method would step through the entries at the leaf level  in a manner similar to printall  but each call takes only one step  and records where it left off  so that successive calls next step through successive records.we omit details for simplicity  and leave the pseudocode for the iterator interface as an exercise for the interested reader b + -trees can also be used to find all records with search key values in a specified range  l,u   for example  with a b + -tree on attribute salary of instructor  we can find all instructor records with salary in a specified range such as  50000  100000   in other words  all salaries between 50000 and 100000   such queries are called range queries to execute such queries  we can create a procedure printrange  l,u   whose body is the same as printall except for these differences  printrange calls find  l   instead of find  v   and then steps through records as in procedure printall  but with the stopping condition being that l.ki > u  instead of l.ki > v in processing a query  we traverse a path in the tree from the root to some leaf node if there are n records in the file  the path is no longer than  log  n/2   n    in practice  only a few nodes need to be accessed typically  a node is made to be the same size as a disk block  which is typically 4 kilobytes.with a search-key size of 12 bytes  and a disk-pointer size of 8 bytes  n is around 200 even with a more conservative estimate of 32 bytes for the search-key size  n is around 100 with n = 100  if we have 1 million search-key values in the file  a lookup requires only  log50  1,000,000   = 4 nodes to be accessed thus  at most four blocks need to be read from disk for the lookup the root node of the tree is usually heavily accessed and is likely to be in the buffer  so typically only three or fewer blocks need to be read from disk thesumit67.blogspot.com 11.3 b + -tree index files 491 animportant difference betweenb + -tree structures and in-memory tree structures  such as binary trees  is the size of a node  and as a result  the height of the tree in a binary tree  each node is small  and has at most two pointers in a b + -tree  each node is large ? typically a disk block ? and a node can have a large number of pointers thus  b + -trees tend to be fat and short  unlike thin and tall binary trees in a balanced binary tree  the path for a lookup can be of length  log2  n    where n is the number of records in the file being indexed.with n = 1,000,000 as in the previous example  a balanced binary tree requires around 20 node accesses if each node were on a different disk block  20 block reads would be required to process a lookup  in contrast to the four block reads for the b + -tree the difference is significant  since each block read could require a disk arm seek  and a block read together with the disk arm seek takes about 10 milliseconds on a typical disk 11.3.3 updates on b + -trees when a record is inserted into  or deleted from a relation  indices on the relation must be updated correspondingly recall that updates to a record can be modeled as a deletion of the old record followed by insertion of the updated record hence we only consider the case of insertion and deletion insertion and deletion are more complicated than lookup  since it may be necessary to split a node that becomes too large as the result of an insertion  or to coalesce nodes  that is  combine nodes  if a node becomes too small  fewer than  n/2  pointers   furthermore  when a node is split or a pair of nodes is combined,wemust ensure that balance is preserved to introduce the idea behind insertion and deletion in a b + -tree,we shall assume temporarily that nodes never become too large or too small under this assumption  insertion and deletion are performed as defined next ? insertion using the same technique as for lookup from the find   function  figure 11.11  ,we first findthe leaf node inwhich the search-keyvaluewould appear.we then insert an entry  that is  a search-key value and record pointer pair  in the leaf node  positioning it such that the search keys are still in order ? deletion using the same technique as for lookup  we find the leaf node containing the entry to be deleted  by performing a lookup on the search-key value of the deleted record ; if there are multiple entrieswith the same searchkey value  we search across all entries with the same search-key value until we find the entry that points to the record being deleted.we then remove the entry from the leaf node all entries in the leaf node that are to the right of the deleted entry are shifted left by one position  so that there are no gaps in the entries after the entry is deleted we now consider the general case of insertion and deletion  dealingwith node splitting and node coalescing  thesumit67.blogspot.com 492 chapter 11 indexing and hashing adams brandt califieri crick figure 11.12 split of leaf node on insertion of ? adams ? 11.3.3.1 insertion we now consider an example of insertion in which a node must be split assume that a record is inserted on the instructor relation  with the name value being adams.we then need to insert an entry for ? adams ? into theb + -tree of figure 11.9 using the algorithm for lookup  we find that ? adams ? should appear in the leaf node containing ? brandt ?  ? califieri ?  and ? crick ? there is no room in this leaf to insert the search-key value ? adams ? therefore  the node is split into two nodes figure 11.12 shows the two leaf nodes that result from the split of the leaf node on inserting ? adams ?  the search-key values ? adams ? and ? brandt ? are in one leaf  and ? califieri ? and ? crick ? are in the other in general  we take the n search-key values  the n  1 values in the leaf node plus the value being inserted   and put the first  n/2  in the existing node and the remaining values in a newly created node having split a leaf node  we must insert the new leaf node into the b + -tree structure in our example  the new node has ? califieri ? as its smallest search-key value.we need to insert an entry with this search-key value  and a pointer to the new node  into the parent of the leaf node that was split the b + -tree of figure 11.13 shows the result of the insertion it was possible to perform this insertion with no further node split  because therewas room in the parent node for the new entry if there were no room  the parent would have had to be split  requiring an entry to be added to its parent in the worst case  all nodes along the path to the root must be split if the root itself is split  the entire tree becomes deeper splitting of a nonleaf node is a little different from splitting of a leaf node figure 11.14 shows the result of inserting a record with search key ? lamport ? into the tree shown in figure 11.13 the leaf node inwhich ? lamport ? is to be inserted already has entries ? gold ?  ? katz ?  and ? kim ?  and as a result the leaf node has to be split the new right-hand-side node resulting from the split contains the search-key values ? kim ? and ? lamport ?  an entry  kim  n1  must then be added adams brandt einstein el said gold katz kim mozart singh srinivasan wu gold srinivasan mozart califieri einstein califieri crick figure 11.13 insertion of ? adams ? into the b + -tree of figure 11.9 thesumit67.blogspot.com 11.3 b + -tree index files 493 srinivasan gold califieri einstein mozart kim adams brandt califieri crick einstein el said gold katz kim lamport mozart singh srinivasan wu figure 11.14 insertion of ? lamport ? into the b + -tree of figure 11.13 to the parent node  where n1 is a pointer to the new node  however  there is no space in the parent node to add a new entry  and the parent node has to be split to do so  the parent node is conceptually expanded temporarily  the entry added  and the overfull node is then immediately split when an overfull nonleaf node is split  the child pointers are divided among the original and the newly created nodes ; in our example  the original node is left with the first three pointers  and the newly created node to the right gets the remaining two pointers the search key values are  however  handled a little differently the search key values that lie between the pointersmoved to the right node  in our example  the value ? kim ?  are moved along with the pointers  while those that lie between the pointers that stay on the left  in our example  ? califieri ? and ? einstein ?  remain undisturbed however  the search key value that lies between the pointers that stay on the left  and the pointers that move to the right node is treated differently in our example  the search key value ? gold ? lies between the three pointers that went to the left node  and the two pointers that went to the right node the value ? gold ? is not added to either of the split nodes instead  an entry  gold  n2  is added to the parent node  where n2 is a pointer to the newly created node that resulted from the split in this case  the parent node is the root  and it has enough space for the new entry the general technique for insertion into a b + -tree is to determine the leaf node l into which insertion must occur if a split results  insert the new node into the parent of node l if this insertion causes a split  proceed recursively up the tree until either an insertion does not cause a split or a new root is created figure 11.15 outlines the insertion algorithm in pseudocode the procedure insert inserts a key-value pointer pair into the index  using two subsidiary procedures insert in leaf and insert in parent in the pseudocode  l  n  p and t denote pointers to nodes  with l beingused todenote a leafnode l.ki and l.pi denote the ith value and the ith pointer in node l  respectively ; t.ki and t.pi are used similarly the pseudocode also makes use of the function parent  n  to find the parent of a node n we can compute a list of nodes in the path from the root to the leaf while initially finding the leaf node  and can use it later to find the parent of any node in the path efficiently the procedure insert in parent takes as parameters n  k   n   where node n was split into n and n   with k  being the least value in n   the procedure thesumit67.blogspot.com 494 chapter 11 indexing and hashing procedure insert  value k  pointer p  if  tree is empty  create an empty leaf node l  which is also the root else find the leaf node l that should contain key value k if  l has less than n  1 key values  then insert in leaf  l  k  p  else begin / * l has n  1 key values already  split it * / create node l  copy l.p1    l.kn-1 to a block of memory t that can hold n  pointer  key-value  pairs insert in leaf  t  k  p  set l  .pn = l.pn ; set l.pn = l  erase l.p1 through l.kn-1 from l copy t.p1 through t.k  n/2  from t into l starting at l.p1 copy t.p  n/2  + 1 through t.kn from t into l  starting at l  .p1 let k  be the smallest key-value in l  insert in parent  l  k   l   end procedure insert in leaf  node l  value k  pointer p  if  k < l.k1  then insert p  k into l just before l.p1 else begin let ki be the highest value in l that is less than k insert p  k into l just after t.ki end procedure insert in parent  node n  value k   node n   if  n is the root of the tree  then begin create a new node r containing n  k   n  / * n and n  are pointers * / make r the root of the tree return end let p = parent  n  if  p has less than n pointers  then insert  k   n   in p just after n else begin / * split p * / copy p to a block of memory t that can hold p and  k   n   insert  k   n   into t just after n erase all entries from p ; create node p  copy t.p1    t.p  n/2  into p let k   = t.k  n/2  copy t.p  n/2  + 1    t.pn + 1 into p  insert in parent  p  k    p   end figure 11.15 insertion of entry in a b + -tree thesumit67.blogspot.com 11.3 b + -tree index files 495 adams brandt califieri crick einstein el said gold katz kim mozart singh wu califieri gold einstein mozart figure 11.16 deletion of ? srinivasan ? from the b + -tree of figure 11.13 modifies the parent of n to record the split the procedures insert into index and insert in parent use a temporary area of memory t to store the contents of a node being split the procedures can be modified to copy data from the node being split directly to the newly created node  reducing the time required for copying data however  the use of the temporary space t simplifies the procedures 11.3.3.2 deletion we now consider deletions that cause tree nodes to contain too few pointers first  let us delete ? srinivasan ? from the b + -tree of figure 11.13 the resulting b + -tree appears in figure 11.16.we now consider how the deletion is performed.we first locate the entry for ? srinivasan ? by using our lookup algorithm when we delete the entry for ? srinivasan ? from its leaf node  the node is left with only one entry  ? wu ?  since  in our example  n = 4 and 1 <   n  1  /2   we must eithermerge the node with a sibling node  or redistribute the entries between the nodes  to ensure that each node is at least half-full in our example  the underfull nodewith the entry for ? wu ? can be merged with its left sibling node we merge the nodes by moving the entries from both the nodes into the left sibling  and deleting the now empty right sibling once the node is deleted  we must also delete the entry in the parent node that pointed to the just deleted node in our example  the entry to be deleted is  srinivasan  n3   where n3 is a pointer to the leaf containing ? srinivasan ?   in this case the entry to be deleted in the nonleaf node happens to be the same value as that deleted from the leaf ; that would not be the case for most deletions  after deleting the above entry  the parent node  which had a search key value ? srinivasan ? and two pointers  now has one pointer  the leftmost pointer in the node  and no search-key values since 1 <  n/2  for n = 4  the parent node is underfull  for larger n  a node that becomes underfull would still have some values as well as pointers  in this case  we look at a sibling node ; in our example  the only sibling is the nonleaf node containing the search keys ? califieri ?  ? einstein ?  and ? gold ?  if possible  we try to coalesce the node with its sibling in this case  coalescing is not possible  since the node and its sibling together have five pointers  against a maximum of four the solution in this case is to redistribute the pointers between thesumit67.blogspot.com 496 chapter 11 indexing and hashing adams brandt califieri crick einstein el said gold katz kim mozart califieri einstein kim gold figure 11.17 deletion of ? singh ? and ? wu ? from the b + -tree of figure 11.16 the node and its sibling  such that each has at least  n/2  = 2 child pointers to do so  we move the rightmost pointer from the left sibling  the one pointing to the leaf node containing ? mozart ?  to the underfull right sibling however  the underfull right siblingwould now have two pointers  namely its leftmost pointer  and the newly moved pointer  with no value separating them in fact  the value separating them is not present in either of the nodes  but is present in the parent node  between the pointers from the parent to the node and its sibling in our example  the value ? mozart ? separates the two pointers  and is present in the right sibling after the redistribution redistribution of the pointers also means that the value ? mozart ? in the parent no longer correctly separates search-key values in the two siblings in fact  the value that now correctly separates search-key values in the two sibling nodes is the value ? gold ?  which was in the left sibling before redistribution as a result  as can be seen in the b + -tree in figure 11.16  after redistribution of pointers between siblings  the value ? gold ? has moved up into the parent  while the value that was there earlier  ? mozart ?  has moved down into the right sibling we next delete the search-key values ? singh ? and ? wu ? from the b + -tree of figure 11.16 the result is shown in figure 11.17 the deletion of the first of these values does not make the leaf node underfull  but the deletion of the second value does it is not possible to merge the underfull node with its sibling  so a redistribution of values is carried out  moving the search-key value ? kim ? into the node containing ? mozart ?  resulting in the tree shown in figure 11.17 the value separating the two siblings has been updated in the parent  from ? mozart ? to ? kim ?  nowwe delete ? gold ? from the above tree ; the result is shown in figure 11.18 this results in an underfull leaf  which can now be merged with its sibling the resultant deletion of an entry from the parent node  the nonleaf node containing ? kim ?  makes the parent underfull  it is left with just one pointer   this time around  the parent node can be merged with its sibling this merge results in the search-key value ? gold ? moving down from the parent into the merged node as a result of this merge  an entry is deleted from its parent  which happens to be the root of the tree and as a result of that deletion  the root is left with only one child pointer and no search-key value  violating the condition that the root thesumit67.blogspot.com 11.3 b + -tree index files 497 adams brandt einstein el said katz kim mozart califieri gold califieri einstein crick figure 11.18 deletion of ? gold ? from the b + -tree of figure 11.17 have at least two children as a result  the root node is deleted and its sole child becomes the root  and the depth of the b + -tree has been decreased by 1 it is worth noting that  as a result of deletion  a key value that is present in a nonleaf node of the b + -tree may not be present at any leaf of the tree for example  in figure 11.18  the value ? gold ? has been deleted from the leaf level  but is still present in a nonleaf node in general  to delete a value in a b + -tree  we perform a lookup on the value and delete it if the node is too small  we delete it from its parent this deletion results in recursive application of the deletion algorithm until the root is reached  a parent remains adequately full after deletion  or redistribution is applied figure 11.19 outlines the pseudocode for deletion from a b + -tree the procedure swap variables  n  n   merely swaps the values of the  pointer  variables n and n  ; this swap has no effect on the tree itself the pseudocode uses the condition ? too few pointers/values ? for nonleaf nodes  this criterion means less than  n/2  pointers ; for leaf nodes  it means less than   n  1  /2  values the pseudocode redistributes entries by borrowing a single entry from an adjacent node we can also redistribute entries by repartitioning entries equally between the two nodes the pseudocode refers to deleting an entry  k  p  from a node in the case of leaf nodes  the pointer to an entry actually precedes the key value  so the pointer p precedes the key value k for nonleaf nodes  p follows the key value k 11.3.4 nonunique search keys if a relation can have more than one record containing the same search key value  that is  two ormore records can have the same values for the indexed attributes   the search key is said to be a nonunique search key one problem with nonunique search keys is in the efficiency of record deletion suppose a particular search-key value occurs a large number of times  and one of the records with that search key is to be deleted the deletion may have to search through a number of entries  potentially across multiple leaf nodes  to find the entry corresponding to the particular record being deleted a simple solution to this problem  used by most database systems  is to make search keys unique by creating a composite search key containing the original search key and another attribute  which together are unique across all records the extra attribute can be a record-id  which is a pointer to the record  or any other attribute whose value is unique among all records with the same searchthesumit67 blogspot.com 498 chapter 11 indexing and hashing procedure delete  value k  pointer p  find the leaf node l that contains  k  p  delete entry  l  k  p  procedure delete entry  node n  value k  pointer p  delete  k  p  from n if  n is the root and n has only one remaining child  then make the child of n the new root of the tree and delete n else if  n has too few values/pointers  then begin let n  be the previous or next child of parent  n  let k  be the value between pointers n and n  in parent  n  if  entries in n and n  can fit in a single node  then begin / * coalesce nodes * / if  n is a predecessor of n   then swap variables  n  n   if  n is not a leaf  then append k  and all pointers and values in n to n  else append all  ki  pi  pairs in n to n  ; set n  .pn = n.pn delete entry  parent  n   k   n  ; delete node n end else begin / * redistribution  borrow an entry from n  * / if  n  is a predecessor of n  then begin if  n is a nonleaf node  then begin let m be such that n  .pm is the last pointer in n  remove  n  .km-1  n  .pm  from n  insert  n  .pm  k   as the first pointer and value in n  by shifting other pointers and values right replace k  in parent  n  by n  .km-1 end else begin let m be such that  n  .pm  n  .km  is the last pointer/value pair in n  remove  n  .pm  n  .km  from n  insert  n  .pm  n  .km  as the first pointer and value in n  by shifting other pointers and values right replace k  in parent  n  by n  .km end end else    symmetric to the then case    end end figure 11.19 deletion of entry from a b + -tree key value the extra attribute is called a uniquifier attribute when a record is to be deleted  the composite search-key value is computed from the record  and then used to look up the index since the value is unique  the corresponding leafthesumit67 blogspot.com 11.3 b + -tree index files 499 level entry can be found with a single traversal from root to leaf  with no further accesses at the leaf level as a result  record deletion can be done efficiently a search with the original search-key attribute simply ignores the value of the uniquifier attribute when comparing search-key values with nonunique search keys  our b + -tree structure stores each key value as many times as there are records containing that value an alternative is to store each key value only once in the tree  and to keep a bucket  or list  of record pointers with a search-key value  to handle nonunique search keys this approach is more space efficient since it stores the key value only once ; however  it creates several complications when b + -trees are implemented if the buckets are kept in the leaf node  extra code is needed to deal with variable-size buckets  and to deal with buckets that grow larger than the size of the leaf node if the buckets are stored in separate blocks  an extra i/o operation may be required to fetch records in addition to these problems  the bucket approach also has the problem of inefficiency for record deletion if a search-key value occurs a large number of times 11.3.5 complexity of b + -tree updates although insertion and deletion operations on b + -trees are complicated  they require relatively few i/o operations  which is an important benefit since i/o operations are expensive it can be shown that the number of i/o operations needed in the worst case for an insertion is proportional to log  n/2   n   where n is the maximum number of pointers in a node  and n is the number of records in the file being indexed the worst-case complexity of the deletion procedure is also proportional to log  n/2   n   provided there are no duplicate values for the search key if there are duplicate values  deletion may have to search across multiple records with the same search-key value to find the correct entry to be deleted  which can be inefficient however  making the search key unique by adding a uniquifier attribute  as described in section 11.3.4  ensures the worst-case complexity of deletion is the same even if the original search key is nonunique in other words  the cost of insertion and deletion operations in terms of i/o operations is proportional to the height of the b + -tree  and is therefore low it is the speed of operation on b + -trees that makes them a frequently used index structure in database implementations in practice  operations on b + -trees result in fewer i/o operations than the worst-case bounds with fanout of 100  and assuming accesses to leaf nodes are uniformly distributed  the parent of a leaf node is 100 times more likely to get accessed than the leaf node conversely  with the same fanout  the total number of nonleaf nodes in a b + -tree would be just a little more than 1/100th of the number of leaf nodes as a result  with memory sizes of several gigabytes being common today  for b + -trees that are used frequently  even if the relation is very large it is quite likely that most of the nonleaf nodes are already in the database buffer when they are accessed thus  typically only one or two i/o operations are required to perform a lookup for updates  the probability of a node split thesumit67.blogspot.com 500 chapter 11 indexing and hashing occurring is correspondingly very small depending on the ordering of inserts  with a fanout of 100  only between 1 in 100 to 1 in 50 insertions will result in a node split  requiring more than one block to be written as a result  on an average an insert will require just a little more than one i/o operation to write updated blocks although b + -trees only guarantee that nodes will be at least half full  if entries are inserted in random order  nodes can be expected to be more than two-thirds full on average if entries are inserted in sorted order  on the other hand  nodes will be only half full  we leave it as an exercise to the reader to figure out why nodes would be only half full in the latter case  11.4 b + -tree extensions in this section  we discuss several extensions and variations of the b + -tree index structure 11.4.1 b + -tree file organization as mentioned in section 11.3  the main drawback of index-sequential file organization is the degradation of performance as the file grows  with growth  an increasing percentage of index entries and actual records become out of order  and are stored in overflow blocks.we solve the degradation of index lookups by using b + -tree indices on the file we solve the degradation problem for storing the actual records by using the leaf level of the b + -tree to organize the blocks containing the actual records we use the b + -tree structure not only as an index  but also as an organizer for records in a file in a b + -tree file organization  the leaf nodes of the tree store records  instead of storing pointers to records figure 11.20 shows an example of a b + -tree file organization since records are usually larger than pointers  the maximum number of records that can be stored in a leaf node is less than the number of pointers in a nonleaf node however  the leaf nodes are still required to be at least half full i c k m  a,4   b,8   c,1   d,9   e,4   f,7   g,3   h,3   i,4   j,8   k,1   l,6   m,4   n,8   p,6  f figure 11.20 b + -tree file organization thesumit67.blogspot.com 11.4 b + -tree extensions 501 insertion and deletion of records from a b + -tree file organization are handled in the same way as insertion and deletion of entries in a b + -tree index when a record with a given key value v is inserted  the system locates the block that should contain the record by searching the b + -tree for the largest key in the tree that is = v if the block located has enough free space for the record  the system stores the record in the block otherwise  as in b + -tree insertion  the systemsplits the block in two  and redistributes the records in it  in the b + -tree ? key order  to create space for the new record the split propagates up the b + -tree in the normal fashion when we delete a record  the system first removes it from the block containing it if a block b becomes less than half full as a result  the records in b are redistributed with the records in an adjacent block b   assuming fixed-sized records  each block will hold at least one-half as many records as the maximum that it can hold the system updates the nonleaf nodes of the b + -tree in the usual fashion when we use a b + -tree for file organization  space utilization is particularly important  since the space occupied by the records is likely to be much more than the space occupied by keys and pointers.we can improve the utilization of space in a b + -tree by involving more sibling nodes in redistribution during splits and merges the technique is applicable to both leaf nodes and nonleaf nodes  and works as follows  during insertion  if a node is full the system attempts to redistribute some of its entries to one of the adjacent nodes  to make space for a new entry if this attempt fails because the adjacent nodes are themselves full  the system splits the node  and splits the entries evenly among one of the adjacent nodes and the two nodes that it obtained by splitting the original node since the three nodes together contain one more record than can fit in two nodes  each node will be about two-thirds full more precisely  each node will have at least 2n/3  entries  where n is the maximum number of entries that the node can hold  x  denotes the greatest integer that is less than or equal to x ; that is  we drop the fractional part  if any  during deletion of a record  if the occupancy of a node falls below 2n/3   the system attempts to borrow an entry from one of the sibling nodes if both sibling nodes have 2n/3  records  instead of borrowing an entry  the system redistributes the entries in the node and in the two siblings evenly between two of the nodes  and deletes the third node we can use this approach because the total number of entries is 3 2n/3   1  which is less than 2n with three adjacent nodes used for redistribution  each node can be guaranteed to have 3n/4  entries in general  if m nodes  m  1 siblings  are involved in redistribution  each node can be guaranteed to contain at least  m  1  n/m  entries however  the cost of update becomes higher as more sibling nodes are involved in the redistribution note that in a b + -tree index or file organization  leaf nodes that are adjacent to each other in the tree may be located at different places on disk when a file organization is newly created on a set of records  it is possible to allocate blocks that are mostly contiguous on disk to leaf nodes that are contiguous in the tree thus a sequential scan of leaf nodeswould correspond to a mostly sequential scan on disk as insertions and deletions occur on the tree  sequentiality is increasingly thesumit67.blogspot.com 502 chapter 11 indexing and hashing lost  and sequential access has to wait for disk seeks increasingly often an index rebuild may be required to restore sequentiality b + -tree file organizations can also be used to store large objects  such as sql clobs and blobs  which may be larger than a disk block  and as large as multiple gigabytes such large objects can be stored by splitting them into sequences of smaller records that are organized in a b + -tree file organization the records can be sequentially numbered  or numbered by the byte offset of the record within the large object  and the record number can be used as the search key 11.4.2 secondary indices and record relocation some file organizations  such as the b + -tree file organization  may change the location of records evenwhen the records have not been updated as an example  when a leaf node is split in a b + -tree file organization  a number of records are moved to a new node in such cases  all secondary indices that store pointers to the relocated records would have to be updated  even though the values in the recordsmay not have changed each leaf node may contain a fairly large number of records  and each of them may be in different locations on each secondary index thus a leaf-node split may require tens or even hundreds of i/o operations to update all affected secondary indices  making it a very expensive operation a widely used solution for this problem is as follows  in secondary indices  in place of pointers to the indexed records  we store the values of the primaryindex search-key attributes for example  supposewe have a primary index on the attribute id of relation instructor ; then a secondary index on dept name would store with each department name a list of instructor ? s id values of the corresponding records  instead of storing pointers to the records relocation of records because of leaf-node splits then does not require any update on any such secondary index however  locating a record using the secondary index now requires two steps  first we use the secondary index to find the primary-index search-key values  and then we use the primary index to find the corresponding records the above approach thus greatly reduces the cost of index update due to file reorganization  although it increases the cost of accessing data using a secondary index 11.4.3 indexing strings creating b + -tree indices on string-valued attributes raises two problems the first problem is that strings can be of variable length the second problem is that strings can be long  leading to a low fanout and a correspondingly increased tree height with variable-length search keys  different nodes can have different fanouts even if they are full a node must then be split if it is full  that is  there is no space to add a new entry  regardless of how many search entries it has similarly  nodes can be merged or entries redistributed depending on what fraction of the space in the nodes is used  instead of being based on the maximum number of entries that the node can hold thesumit67.blogspot.com 11.4 b + -tree extensions 503 the fanout of nodes can be increased by using a technique called prefix compression with prefix compression  we do not store the entire search key value at nonleaf nodes we only store a prefix of each search key value that is sufficient to distinguish between the key values in the subtrees that it separates for example  if we had an index on names  the key value at a nonleaf node could be a prefix of a name ; it may suffice to store ? silb ? at a nonleaf node  instead of the full ? silberschatz ? if the closest values in the two subtrees that it separates are  say  ? silas ? and ? silver ? respectively 11.4.4 bulk loading of b + -tree indices as we saw earlier  insertion of a record in a b + -tree requires a number of i/o operations that in the worst case is proportional to the height of the tree  which is usually fairly small  typically five or less  even for large relations   now consider the case where a b + -tree is being built on a large relation suppose the relation is significantly larger than main memory  and we are constructing a nonclustering index on the relation such that the index is also larger than main memory in this case  as we scan the relation and add entries to the b + -tree  it is quite likely that each leaf node accessed is not in the database buffer when it is accessed  since there is no particular ordering of the entries.with such randomly ordered accesses to blocks  each time an entry is added to the leaf  a disk seek will be required to fetch the block containing the leaf node the block will probably be evicted from the disk buffer before another entry is added to the block  leading to another disk seek to write the block back to disk thus a random read and a random write operation may be required for each entry inserted for example  if the relation has 100 million records  and each i/o operation takes about 10 milliseconds  it would take at least 1 million seconds to build the index  counting only the cost of reading leaf nodes  not even counting the cost of writing the updated nodes back to disk this is clearly a very large amount of time ; in contrast  if each record occupies 100 bytes  and the disk subsystem can transfer data at 50 megabytes per second  it would take just 200 seconds to read the entire relation insertion of a large number of entries at a time into an index is referred to as bulk loading of the index an efficientway to perform bulk loading of an index is as follows first  create a temporary file containing index entries for the relation  then sort the file on the search key of the index being constructed  and finally scan the sorted file and insert the entries into the index there are efficient algorithms for sorting large relations  which are described later in section 12.4  which can sort even a large file with an i/o cost comparable to that of reading the file a few times  assuming a reasonable amount of main memory is available there is a significant benefit to sorting the entries before inserting them into the b + -tree when the entries are inserted in sorted order  all entries that go to a particular leaf node will appear consecutively  and the leaf needs to be written out only once ; nodes will never have to be read from disk during bulk load  if the b + -tree was empty to start with each leaf node will thus incur only one i/o operation even though many entries may be inserted into the node if each leaf thesumit67.blogspot.com 504 chapter 11 indexing and hashing contains 100 entries  the leaf level will contain 1 million nodes  resulting in only 1 million i/o operations for creating the leaf level even these i/o operations can be expected to be sequential  if successive leaf nodes are allocated on successive disk blocks  and few disk seeks would be required with current disks  1 millisecond per block is a reasonable estimate for mostly sequential i/o operations  in contrast to 10 milliseconds per block for random i/o operations we shall study the cost of sorting a large relation later  in section 12.4  but as a rough estimate  the index which would have taken a million seconds to build otherwise  can be constructed in well under 1000 seconds by sorting the entries before inserting them into the b + -tree  in contrast to more than 1,000,000 seconds for inserting in random order if the b + -tree is initially empty  it can be constructed faster by building it bottom-up  from the leaf level  instead of using the usual insert procedure in bottom-up b + -tree construction  after sorting the entries as we just described  we break up the sorted entries into blocks  keeping as many entries in a block as can fit in the block ; the resulting blocks form the leaf level of the b + -tree the minimum value in each block  alongwith the pointer to the block  is used to create entries in the next level of the b + -tree  pointing to the leaf blocks each further level of the tree is similarly constructed using the minimum values associated with each node one level below  until the root is created we leave details as an exercise for the reader most database systems implement efficient techniques based on sorting of entries  and bottom-up construction  when creating an index on a relation  although they use the normal insertion procedure when tuples are added one at a time to a relation with an existing index some database systems recommend that if a very large number of tuples are added at once to an already existing relation  indices on the relation  other than any index on the primary key  should be dropped  and then re-created after the tuples are inserted  to take advantage of efficient bulk-loading techniques 11.4.5 b-tree index files b-tree indices are similar to b + -tree indices the primary distinction between the two approaches is that a b-tree eliminates the redundant storage of search-key values in the b + -tree of figure 11.13  the search keys ? califieri ?  ? einstein ?  ? gold ?  ? mozart ?  and ? srinivasan ? appear in nonleaf nodes  in addition to appearing in the leaf nodes every search-key value appears in some leaf node ; several are repeated in nonleaf nodes a b-tree allows search-key values to appear only once  if they are unique   unlike a b + -tree  where a value may appear in a nonleaf node  in addition to appearing in a leaf node figure 11.21 shows a b-tree that represents the same search keys as the b + -tree of figure 11.13 since search keys are not repeated in the b-tree  we may be able to store the index in fewer tree nodes than in the corresponding b + -tree index however  since search keys that appear in nonleaf nodes appear nowhere else in the b-tree  we are forced to include an additional thesumit67.blogspot.com 11.4 b + -tree extensions 505 brandt califieri crick el said gold kim mozart srinivasan wu einstein katz singh einstein record katz record singh record brandt record califieri record  and soon for other records figure 11.21 b-tree equivalent of b + -tree in figure 11.13 pointer field for each search key in a nonleaf node these additional pointers point to either file records or buckets for the associated search key it is worth noting that many database system manuals  articles in industry literature  and industry professionals use the term b-tree to refer to the data structure that we call the b + -tree in fact  it would be fair to say that in current usage  the term b-tree is assumed to be synonymous with b + -tree however  in this book we use the terms b-tree and b + -tree as they were originally defined  to avoid confusion between the two data structures a generalized b-tree leaf node appears in figure 11.22a ; a nonleaf node appears in figure 11.22b leaf nodes are the same as in b + -trees in nonleaf nodes  the pointers pi are the tree pointers that we used also for b + -trees  while the pointers bi are bucket or file-record pointers in the generalized b-tree in the figure  there are n-1 keys in the leaf node  but there are m-1 keys in the nonleaf node this discrepancy occurs because nonleaf nodes must include pointers bi  thus reducing the number of search keys that can be held in these nodes clearly  m < n  but the exact relationship between m and n depends on the relative size of search keys and pointers the number of nodes accessed in a lookup in a b-tree depends on where the search key is located a lookup on a b + -tree requires traversal of a path from the root of the tree to some leaf node in contrast  it is sometimes possible to find the desired value in a b-tree before reaching a leaf node however  roughly n times as many keys are stored in the leaf level of a b-tree as in the nonleaf levels  and  since n is typically large  the benefit of finding certain values early is relatively p1 k1 p2 ? pn-1 kn-1 pn p1 b1 k1 p2 b2 k2 ? pm-1 bm-1 km-1 pm  a   b  figure 11.22 typical nodes of a b-tree  a  leaf node  b  nonleaf node thesumit67.blogspot.com 506 chapter 11 indexing and hashing small moreover  the fact that fewer search keys appear in a nonleaf b-tree node  compared to b + -trees  implies that a b-tree has a smaller fanout and therefore may have depth greater than that of the corresponding b + -tree thus  lookup in a b-tree is faster for some search keys but slower for others  although  in general  lookup time is still proportional to the logarithm of the number of search keys deletion in a b-tree is more complicated in a b + -tree  the deleted entry always appears in a leaf in a b-tree  the deleted entry may appear in a nonleaf node the proper value must be selected as a replacement from the subtree of the node containing the deleted entry specifically  if search key ki is deleted  the smallest search key appearing in the subtree of pointer pi + 1 must be moved to the field formerly occupied by ki  further actions need to be taken if the leaf node now has too few entries in contrast  insertion in a b-tree is only slightly more complicated than is insertion in a b + -tree the space advantages of b-trees are marginal for large indices  and usually do not outweigh the disadvantages that we have noted thus  pretty much all database-system implementations use the b + -tree data structure  even if  as we discussed earlier  they refer to the data structure as a b-tree 11.4.6 flash memory in our description of indexing so far  we have assumed that data are resident on magnetic disks although this assumption continues to be true for the most part  flash memory capacities have grown significantly  and the cost of flash memory per gigabyte has dropped equally significantly  making flash memory storage a serious contender for replacing magnetic-disk storage for many applications a natural question is  how would this change affect the index structure flash-memory storage is structured as blocks  and the b + -tree index structure can be used for flash-memory storage the benefit of themuch faster access speeds is clear for index lookups instead of requiring an average of 10 milliseconds to seek to and read a block  a random block can be read in about a microsecond from flash-memory thus lookups run significantly faster than with disk-based data the optimum b + -tree node size for flash-memory is typically smaller than that with disk the only real drawback with flash memory is that it does not permit inplace updates to data at the physical level  although it appears to do so logically every update turns into a copy + write of an entire flash-memory block  requiring the old copy of the block to be erased subsequently ; a block erase takes about 1 millisecond there is ongoing research aimed at developing index structures that can reduce the number of block erases meanwhile  standard b + -tree indices can continue to be used even on flash-memory storage  with acceptable update performance  and significantly improved lookup performance compared to disk storage 11.5 multiple-key access until now  we have assumed implicitly that only one index on one attribute is used to process a query on a relation however  for certain types of queries  it is thesumit67.blogspot.com 11.5 multiple-key access 507 advantageous to use multiple indices if they exist  or to use an index built on a multiattribute search key 11.5.1 using multiple single-key indices assume that the instructor file has two indices  one for dept name and one for salary consider the following query  ? find all instructors in the finance department with salary equal to $ 80,000 ? we write select id from instructor where dept name = ? finance ? and salary = 80000 ; there are three strategies possible for processing this query  1 use the index on dept name to find all records pertaining to the finance department examine each such record to see whether salary = 80000 2 use the index on salary to find all records pertaining to instructors with salary of $ 80,000 examine each such record to see whether the department name is ? finance ?  3 use the index on dept name to find pointers to all records pertaining to the finance department also  use the index on salary to find pointers to all records pertaining to instructors with a salary of $ 80,000 take the intersection of these two sets of pointers those pointers that are in the intersection point to records pertaining to instructors of the finance department and with salary of $ 80,000 the third strategy is the only one of the three that takes advantage of the existence of multiple indices however  even this strategy may be a poor choice if all of the following hold  ? there are many records pertaining to the finance department ? there are many records pertaining to instructors with a salary of $ 80,000 ? there are only a few records pertaining to both the finance department and instructors with a salary of $ 80,000 if these conditions hold  we must scan a large number of pointers to produce a small result an index structure called a ? bitmap index ? can in some cases greatly speed up the intersection operation used in the third strategy bitmap indices are outlined in section 11.9 thesumit67.blogspot.com 508 chapter 11 indexing and hashing 11.5.2 indices on multiple keys an alternative strategy for this case is to create and use an index on a composite search key  dept name  salary  ? that is  the search key consisting of the department name concatenated with the instructor salary we can use an ordered  b + -tree  index on the above composite search key to answer efficiently queries of the form select id from instructor where dept name = ? finance ? and salary = 80000 ; queries such as the following query  which specifies an equality condition on the first attribute of the search key  dept name  and a range on the second attribute of the search key  salary   can also be handled efficiently since they correspond to a range query on the search attribute select id from instructor where dept name = ? finance ? and salary < 80000 ; we can even use an ordered index on the search key  dept name  salary  to answer the following query on only one attribute efficiently  select id from instructor where dept name = ? finance ? ; an equality condition dept name = ? finance ? is equivalent to a range query on the range with lower end  finance  -8  and upper end  finance  + 8   range queries on just the dept name attribute can be handled in a similar manner the use of an ordered-index structure on a composite search key  however  has a few shortcomings as an illustration  consider the query select id from instructor where dept name < ? finance ? and salary < 80000 ; we can answer this query by using an ordered index on the search key  dept name  salary   for each value of dept name that is less than ? finance ? in alphabetic order  the system locates records with a salary value of 80000 however  each record is likely to be in a different disk block  because of the ordering of records in the file  leading to many i/o operations the difference between this query and the previous two queries is that the condition on the first attribute  dept name  is a comparison condition  rather than thesumit67.blogspot.com 11.6 static hashing 509 an equality condition the condition does not correspond to a range query on the search key to speed the processing of general composite search-key queries  which can involve one or more comparison operations   we can use several special structures we shall consider bitmap indices in section 11.9 there is another structure  called the r-tree  that can be used for this purpose the r-tree is an extension of the b + -tree to handle indexing on multiple dimensions since the r-tree is used primarily with geographical data types  we describe the structure in chapter 25 11.5.3 covering indices covering indices are indices that store the values of some attributes  other than the search-key attributes  along with the pointers to the record storing extra attribute values is useful with secondary indices  since they allow us to answer some queries using just the index  without even looking up the actual records for example  suppose that we have a nonclustering index on the id attribute of the instructor relation ifwe store the value of the salary attribute along with the record pointer  we can answer queries that require the salary  but not the other attribute  dept name  without accessing the instructor record the same effect could be obtained by creating an index on the search key  id  salary   but a covering index reduces the size of the search key  allowing a larger fanout in the nonleaf nodes  and potentially reducing the height of the index 11.6 static hashing one disadvantage of sequential file organization is that we must access an index structure to locate data  or must use binary search  and that results in more i/o operations file organizations based on the technique of hashing allow us to avoid accessing an index structure hashing also provides a way of constructing indices.we study file organizations and indices based on hashing in the following sections in our description of hashing  we shall use the term bucket to denote a unit of storage that can store one or more records a bucket is typically a disk block  but could be chosen to be smaller or larger than a disk block formally  let k denote the set of all search-key values  and let b denote the set of all bucket addresses a hash function h is a function from k to b let h denote a hash function to insert a record with search key ki  we compute h  ki   which gives the address of the bucket for that record assume for now that there is space in the bucket to store the record then  the record is stored in that bucket to perform a lookup on a search-key value ki  we simply compute h  ki   then search the bucket with that address suppose that two search keys  k5 and k7  have the same hash value ; that is  h  k5  = h  k7   if we perform a lookup on k5  the bucket h  k5  contains records with search-key values k5 and records thesumit67.blogspot.com 510 chapter 11 indexing and hashing with search-key values k7 thus  we have to check the search-key value of every record in the bucket to verify that the record is one that we want deletion is equally straightforward if the search-key value of the record to be deleted is ki  we compute h  ki   then search the corresponding bucket for that record  and delete the record from the bucket hashing can be used for two different purposes in a hash file organization  we obtain the address of the disk block containing a desired record directly by computing a function on the search-key value of the record in a hash index organization we organize the search keys  with their associated pointers  into a hash file structure 11.6.1 hash functions the worst possible hash function maps all search-key values to the same bucket such a function is undesirable because all the records have to be kept in the same bucket a lookup has to examine every such record to find the one desired an ideal hash function distributes the stored keys uniformly across all the buckets  so that every bucket has the same number of records since we do not know at design time precisely which search-key values will be stored in the file  we want to choose a hash function that assigns search-key values to buckets in such a way that the distribution has these qualities  ? the distribution is uniform that is  the hash function assigns each bucket the same number of search-key values from the set of all possible search-key values ? the distribution is random that is  in the average case  each bucket will have nearly the same number of values assigned to it  regardless of the actual distribution of search-key values more precisely  the hash value will not be correlated to any externally visible ordering on the search-key values  such as alphabetic ordering or ordering by the length of the search keys ; the hash function will appear to be random as an illustration of these principles  let us choose a hash function for the instructor file using the search key dept name the hash function that we choose must have the desirable properties not only on the example instructor file that we have been using  but also on an instructor file of realistic size for a large university with many departments assume that we decide to have 26 buckets  and we define a hash function that maps names beginning with the ith letter of the alphabet to the ith bucket this hash function has the virtue of simplicity  but it fails to provide a uniform distribution  since we expect more names to begin with such letters as b and r than q and x  for example now suppose that we want a hash function on the search key salary suppose that the minimum salary is $ 30,000 and the maximum salary is $ 130,000  and we use a hash function that divides the values into 10 ranges  $ 30,000 ? $ 40,000  $ 40,001 ? $ 50,000 and so on the distribution of search-key values is uniform  since thesumit67.blogspot.com 11.6 static hashing 511 bucket 0 bucket 1 bucket 2 bucket 3 bucket 4 bucket 5 bucket 6 bucket 7 45565 15151 mozart music 40000 80000 12121 wu finance 90000 76543 singh finance 10101 srinivasan comp sci katz comp sci 75000 92000 32343 65000 58583 el said califieri history history 80000 60000 einstein gold kim 22222 33456 98345 physics physics elec eng 95000 87000 80000 83821 brandt comp sci 76766 crick biology 72000 figure 11.23 hash organization of instructor file  with dept name as the key each bucket has the same number of different salary values   but is not random recordswith salaries between $ 60,001 and $ 70,000 are far more common than are records with salaries between $ 30,001 and $ 40,000 as a result  the distribution of records is not uniform ? some buckets receive more records than others do if the function has a random distribution  even if there are such correlations in the search keys  the randomness of the distribution will make it very likely that all buckets will have roughly the same number of records  as long as each search key occurs in only a small fraction of the records  if a single search key occurs in a large fraction of the records  the bucket containing it is likely to have more records than other buckets  regardless of the hash function used  typical hash functions perform computation on the internal binary machine representation of characters in the search key a simple hash function of this type first computes the sum of the binary representations of the characters of a key  then returns the sum modulo the number of buckets figure 11.23 shows the application of such a scheme  with eight buckets  to the instructor file  under the assumption that the ith letter in the alphabet is represented by the integer i the following hash function is a better alternative for hashing strings let s be a string of length n  and let s  i  denote the ith byte of the string the hash function is defined as  s  0  * 31  n-1  + s  1  * 31  n-2  + ? ? ? + s  n  1  thesumit67.blogspot.com 512 chapter 11 indexing and hashing the function can be implemented efficiently by setting the hash result initially to 0  and iterating from the first to the last character of the string  at each step multiplying the hash value by 31 and then adding the next character  treated as an integer   the above expression would appear to result in a very large number  but it is actually computed with fixed-size positive integers ; the result of each multiplication and addition is thus automatically computed modulo the largest possible integer value plus 1 the result of the above function modulo the number of buckets can then be used for indexing hash functions require careful design a bad hash function may result in lookup taking time proportional to the number of search keys in the file a welldesigned function gives an average-case lookup time that is a  small  constant  independent of the number of search keys in the file 11.6.2 handling of bucket overflows so far  we have assumed that  when a record is inserted  the bucket to which it is mapped has space to store the record if the bucket does not have enough space  a bucket overflow is said to occur bucket overflow can occur for several reasons  ? insufficient buckets the number of buckets  which we denote nb  must be chosen such that nb > nr / fr  where nr denotes the total number of records that will be stored and fr denotes the number of records that will fit in a bucket this designation  of course  assumes that the total number of records is known when the hash function is chosen ? skew some buckets are assigned more records than are others  so a bucket may overflow even when other buckets still have space this situation is called bucket skew skew can occur for two reasons  1 multiple records may have the same search key 2 the chosen hash function may result in nonuniform distribution of search keys so that the probability of bucket overflow is reduced  the number of buckets is chosen to be  nr / fr  *  1 + d   where d is a fudge factor  typically around 0.2 some space is wasted  about 20 percent of the space in the buckets will be empty but the benefit is that the probability of overflow is reduced despite allocation of a few more buckets than required  bucket overflow can still occur we handle bucket overflow by using overflow buckets if a record must be inserted into a bucket b  and b is already full  the system provides an overflow bucket for b  and inserts the record into the overflow bucket if the overflow bucket is also full  the system provides another overflow bucket  and so on.all the overflow buckets of a given bucket are chained together in a linked list  as in figure 11.24 overflow handling using such a linked list is called overflow chaining we must change the lookup algorithm slightly to handle overflow chaining as before  the system uses the hash function on the search key to identify a bucket thesumit67.blogspot.com 11.6 static hashing 513 overflow buckets for bucket 1 bucket 0 bucket 1 bucket 2 bucket 3 figure 11.24 overflow chaining in a hash structure b the system must examine all the records in bucket b to see whether they match the search key  as before in addition  if bucket b has overflow buckets  the system must examine the records in all the overflow buckets also the form of hash structure that we have just described is sometimes referred to as closed hashing under an alternative approach  called open hashing  the set of buckets is fixed  and there are no overflow chains instead  if a bucket is full  the system inserts records in some other bucket in the initial set of buckets b one policy is to use the next bucket  in cyclic order  that has space ; this policy is called linear probing other policies  such as computing further hash functions  are also used open hashing has been used to construct symbol tables for compilers and assemblers  but closed hashing is preferable for database systems the reason is that deletion under open hashing is troublesome usually  compilers and assemblers perform only lookup and insertion operations on their symbol tables however  in a database system  it is important to be able to handle deletion as well as insertion thus  open hashing is of only minor importance in database implementation an important drawback to the form of hashing that we have described is that we must choose the hash function when we implement the system  and it can not be changed easily thereafter if the file being indexed grows or shrinks since the function h maps search-key values to a fixed set b of bucket addresses  we waste space if b is made large to handle future growth of the file if b is too small  the buckets contain records of many different search-key values  and bucket overflows can occur as the file grows  performance suffers we study later  in section 11.7  how the number of buckets and the hash function can be changed dynamically thesumit67.blogspot.com 514 chapter 11 indexing and hashing bucket 0 bucket 1 bucket 2 bucket 3 bucket 4 bucket 5 bucket 6 76766 45565 76543 10101 15151 33456 58583 83821 22222 98345 bucket 7 12121 32343 76766 crick 76543 singh 32343 el said 58583 califieri 15151 mozart 22222 einstein 33465 gold 10101 srinivasan 45565 katz 83821 brandt 98345 kim 12121 wu biology physics finance history history music physics comp sci comp sci comp sci elec eng finance 72000 80000 60000 62000 40000 95000 87000 65000 75000 92000 80000 90000 figure 11.25 hash index on search key id of instructor file 11.6.3 hash indices hashing can be used not only for file organization  but also for index-structure creation a hash index organizes the search keys  with their associated pointers  into a hash file structure we construct a hash index as follows we apply a hash function on a search key to identify a bucket  and store the key and its associated pointers in the bucket  or in overflow buckets   figure 11.25 shows a secondary hash index on the instructor file  for the search key id the hash function in the figure computes the sum of the digits of the id modulo 8 the hash index has eight buckets  each of size 2  realistic indices would  of course  have much larger bucket sizes   one of the buckets has three keys mapped to it  so it has an overflow bucket in this example  id is a primary key for instructor  so each search key has thesumit67.blogspot.com 11.7 dynamic hashing 515 only one associated pointer in general  multiple pointers can be associated with each key we use the term hash index to denote hash file structures as well as secondary hash indices strictly speaking  hash indices are only secondary index structures a hash index is never needed as a clustering index structure  since  if a file itself is organized by hashing  there is no need for a separate hash index structure on it however  since hash file organization provides the same direct access to records that indexing provides  we pretend that a file organized by hashing also has a clustering hash index on it 11.7 dynamic hashing as we have seen  the need to fix the set b of bucket addresses presents a serious problemwith the static hashing technique of the previous section.most databases grow larger over time ifwe are to use static hashing for such a database  we have three classes of options  1 choose a hash function based on the current file size this option will result in performance degradation as the database grows 2 choose a hash function based on the anticipated size of the file at some point in the future although performance degradation is avoided  a significant amount of space may be wasted initially 3 periodically reorganize the hash structure in response to file growth such a reorganization involves choosing a new hash function  recomputing the hash function on every record in the file  and generating new bucket assignments this reorganization is a massive  time-consuming operation furthermore  it is necessary to forbid access to the file during reorganization several dynamic hashing techniques allow the hash function to be modified dynamically to accommodate the growth or shrinkage of the database in this section we describe one form of dynamic hashing  called extendable hashing the bibliographical notes provide references to other forms of dynamic hashing 11.7.1 data structure extendable hashing copes with changes in database size by splitting and coalescing buckets as the database grows and shrinks as a result  space efficiency is retained moreover  since the reorganization is performed on only one bucket at a time  the resulting performance overhead is acceptably low with extendable hashing  we choose a hash function h with the desirable properties of uniformity and randomness however  this hash function generates values over a relatively large range ? namely  b-bit binary integers a typical value for b is 32 thesumit67.blogspot.com 516 chapter 11 indexing and hashing i i1 i2 i3 bucket 1 bucket 2 bucket 3 00 01 10 11 bucket address table hash prefix ? ? figure 11.26 general extendable hash structure we do not create a bucket for each hash value indeed  232 is over 4 billion  and that many buckets is unreasonable for all but the largest databases instead  we create buckets on demand  as records are inserted into the file we do not use the entire b bits of the hash value initially at any point  we use i bits  where 0 = i = b these i bits are used as an offset into an additional table of bucket addresses the value of i grows and shrinks with the size of the database figure 11.26 shows a general extendable hash structure the i appearing above the bucket address table in the figure indicates that i bits of the hash value h  k  are required to determine the correct bucket for k this number will  of course  change as the file grows although i bits are required to find the correct entry in the bucket address table  several consecutive table entries may point to the same bucket all such entries will have a common hash prefix  but the length of this prefix may be less than i therefore,we associate with each bucket an integer giving the length of the common hash prefix in figure 11.26 the integer associated with bucket j is shown as i j  the number of bucket-address-table entries that point to bucket j is 2  i -i j  11.7.2 queries and updates we now see how to perform lookup  insertion  and deletion on an extendable hash structure thesumit67.blogspot.com 11.7 dynamic hashing 517 to locate the bucket containing search-key value kl  the system takes the first i high-order bits of h  kl   looks at the corresponding table entry for this bit string  and follows the bucket pointer in the table entry to insert a record with search-key value kl  the system follows the same procedure for lookup as before  ending up in some bucket ? say  j if there is room in the bucket  the system inserts the record in the bucket if  on the other hand  the bucket is full  it must split the bucket and redistribute the current records  plus the new one to split the bucket  the system must first determine from the hash value whether it needs to increase the number of bits that it uses ? if i = i j  only one entry in the bucket address table points to bucket j therefore  the system needs to increase the size of the bucket address table so that it can include pointers to the two buckets that result fromsplitting bucket j it does so by considering an additional bit of the hash value it increments the value of i by 1  thus doubling the size of the bucket address table it replaces each entry by two entries  both of which contain the same pointer as the original entry now two entries in the bucket address table point to bucket j the system allocates a new bucket  bucket z   and sets the second entry to point to the new bucket it sets i j and iz to i next  it rehashes each record in bucket j and  depending on the first i bits  remember the system has added 1 to i   either keeps it in bucket j or allocates it to the newly created bucket the system now reattempts the insertion of the new record usually  the attemptwill succeed.however  if all the records in bucket j  aswell as thenew record  have the same hash-value prefix  it will be necessary to split a bucket again  since all the records in bucket j and the new record are assigned to the same bucket if the hash function has been chosen carefully  it is unlikely that a single insertion will require that a bucket be split more than once  unless there are a large number of records with the same search key if all the records in bucket j have the same search-key value  no amount of splitting will help in such cases  overflow buckets are used to store the records  as in static hashing ? if i > i j  then more than one entry in the bucket address table points to bucket j thus  the system can split bucket j without increasing the size of the bucket address table observe that all the entries that point to bucket j correspond to hash prefixes that have the same value on the leftmost i j bits the system allocates a new bucket  bucket z   and sets i j and iz to the value that results from adding 1 to the original i j value next  the system needs to adjust the entries in the bucket address table that previously pointed to bucket j  note that with the new value for i j  not all the entries correspond to hash prefixes that have the same value on the leftmost i j bits  the system leaves the first half of the entries as they were  pointing to bucket j   and sets all the remaining entries to point to the newly created bucket  bucket z   next  as in the previous case  the system rehashes each record in bucket j  and allocates it either to bucket j or to the newly created bucket z thesumit67.blogspot.com 518 chapter 11 indexing and hashing dept_name h  dept_name  biology 0010 ? ? 1101 ? ? 1111 ? ? 1011 ? ? 0010 ? ? 1100 ? ? 0011 ? ? 0000 comp ? ? sci 1111 ? ? 0001 ? ? 0010 ? ? 0100 ? ? 1001 ? ? 0011 ? ? 0110 ? ? 1101 elec ? ? eng 0100 ? ? 0011 ? ? 1010 ? ? 1100 ? ? 1100 ? ? 0110 ? ? 1101 ? ? 1111 finance 1010 ? ? 0011 ? ? 1010 ? ? 0000 ? ? 1100 ? ? 0110 ? ? 1001 ? ? 1111 history 1100 ? ? 0111 ? ? 1110 ? ? 1101 ? ? 1011 ? ? 1111 ? ? 0011 ? ? 1010 music 0011 ? ? 0101 ? ? 1010 ? ? 0110 ? ? 1100 ? ? 1001 ? ? 1110 ? ? 1011 physics ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1001 ? ? 1000 ? ? 0011 ? ? 1111 ? ? 1001 ? ? 1100 ? ? 0000 ? ? 0001 figure 11.27 hash function for dept name the system then reattempts the insert in the unlikely case that it again fails  it applies one of the two cases  i = i j or i > i j  as appropriate note that  in both cases  the system needs to recompute the hash function on only the records in bucket j to delete a record with search-key value kl  the system follows the same procedure for lookup as before  ending up in some bucket ? say  j it removes both the search key from the bucket and the record from the file the bucket  too  is removed if it becomes empty note that  at this point  several buckets can be coalesced  and the size of the bucket address table can be cut in half the procedure for deciding on which buckets can be coalesced and how to coalesce buckets is left to you to do as an exercise the conditions under which the bucket address table can be reduced in size are also left to you as an exercise unlike coalescing of buckets  changing the size of the bucket address table is a rather expensive operation if the table is large therefore it may be worthwhile to reduce the bucket-address-table size only if the number of buckets reduces greatly to illustrate the operation of insertion  we use the instructor file in figure 11.1 and assume that the search key is dept name with the 32-bit hash values as appear in figure 11.27 assume that  initially  the file is empty  as in figure 11.28 we insert the records one by one to illustrate all the features of extendable hashing in a small structure  we shall make the unrealistic assumption that a bucket can hold only two records 0 0 bucket address table bucket 1 hash prefix figure 11.28 initial extendable hash structure thesumit67.blogspot.com 11.7 dynamic hashing 519 1 1 bucket address table hash prefix 1 15151 music 40000 10101 12121 srinivasan 90000 wu 90000 mozart comp sci finance figure 11.29 hash structure after three insertions we insert the record  10101  srinivasan,comp sci  65000   the bucket address table contains a pointer to the one bucket  and the system inserts the record.next  we insert the record  12121  wu  finance  90000   the system also places this record in the one bucket of our structure when we attempt to insert the next record  15151  mozart  music  40000   we find that the bucket is full since i = i0  we need to increase the number of bits that we use from the hash value we now use 1 bit  allowing us 21 = 2 buckets this increase in the number of bits necessitates doubling the size of the bucket address table to two entries the system splits the bucket  placing in the new bucket those records whose search key has a hash value beginning with 1  and leaving in the original bucket the other records figure 11.29 shows the state of our structure after the split next,we insert  22222  einstein  physics  95000   since the first bit of h  physics  is 1  we must insert this record into the bucket pointed to by the ? 1 ? entry in the 2 1 2 2 bucket address table hash prefix 15151 mozart music 40000 12121 wu finance 90000 10101 srinivasan comp sci 65000 22222 einstein physics 95000 figure 11.30 hash structure after four insertions thesumit67.blogspot.com 520 chapter 11 indexing and hashing 3 1 3 3 bucket address table hash prefix 2 22222 33456 physics 95000 physics 87000 15151 mozart music 40000 einstein gold 12121 wu finance 90000 10101 32343 srinivasan el said comp sci history 60000 65000 figure 11.31 hash structure after six insertions 3 1 3 3 bucket address table hash prefix 3 22222 33456 physics 95000 physics 87000 15151 mozart music 40000 einstein gold 12121 wu finance 90000 10101 45565 srinivasan katz comp sci comp sci 75000 65000 32343 el said history 60000 3 figure 11.32 hash structure after seven insertions thesumit67.blogspot.com 11.7 dynamic hashing 521 bucket address table once again  we find the bucket full and i = i1 we increase the number of bits that we use from the hash to 2 this increase in the number of bits necessitates doubling the size of the bucket address table to four entries  as in figure 11.30 since the bucket of figure 11.29 for hash prefix 0 was not split  the two entries of the bucket address table of 00 and 01 both point to this bucket for each record in the bucket of figure 11.29 for hash prefix 1  the bucket being split   the system examines the first 2 bits of the hash value to determine which bucket of the new structure should hold it next,we insert  32343  el said,history  60000  ,which goes in the same bucket as comp sci the following insertion of  33456  gold  physics  87000  results in a bucket overflow  leading to an increase in the number of bits  and a doubling of the size of the bucket address table  see figure 11.31   the insertion of  45565  katz  comp sci  75000  leads to another bucket overflow ; this overflow  however  can be handled without increasing the number of bits  since the bucket in question has two pointers pointing to it  see figure 11.32   next  we insert the records of ? califieri ?  ? singh ?  and ? crick ? without any bucket overflow the insertion of the third comp sci record  83821  brandt  comp sci  92000   however  leads to another overflow this overflow can not be handled by increasing the number of bits  since there are three records with exactly the same hash value hence the system uses an overflow bucket  as in 3 bucket address table hash prefix 2 3 3 3 22222 33456 physics 95000 physics 87000 music biology 15151 40000 72000 mozart einstein gold 12121 wu finance 90000 10101 45565 srinivasan katz comp sci comp sci 75000 65000 76766 crick 76543 singh finance 83821 brandt comp sci 92000 32343 58583 el said califieri history history 60000 62000 80000 3 figure 11.33 hash structure after eleven insertions thesumit67.blogspot.com 522 chapter 11 indexing and hashing 3 2 3 3 bucket address table hash prefix 3 22222 33456 physics 95000 physics 87000 15151 mozart music 40000 einstein gold 12121 wu finance 90000 10101 45565 srinivasan katz comp sci comp sci 75000 65000 76766 crick biology 72000 76543 singh finance 98345 kim elec eng 80000 83821 brandt comp sci 92000 32343 58583 el said califieri history history 60000 62000 2 80000 3 figure 11.34 extendable hash structure for the instructor file figure 11.33.we continue in this manner until we have inserted all the instructor records of figure 11.1 the resulting structure appears in figure 11.34 11.7.3 static hashing versus dynamic hashing we now examine the advantages and disadvantages of extendable hashing  compared with static hashing the main advantage of extendable hashing is that performance does not degrade as the file grows furthermore  there is minimal space overhead although the bucket address table incurs additional overhead  it contains one pointer for each hash value for the current prefix length this table is thus small the main space saving of extendable hashing over other forms of hashing is that no buckets need to be reserved for future growth ; rather  buckets can be allocated dynamically a disadvantage of extendable hashing is that lookup involves an additional level of indirection  since the system must access the bucket address table before accessing the bucket itself this extra reference has only a minor effect on performance although the hash structures that we discussed in section 11.6 do not thesumit67.blogspot.com 11.8 comparison of ordered indexing and hashing 523 have this extra level of indirection  they lose their minor performance advantage as they become full thus  extendable hashing appears to be a highly attractive technique  provided that we are willing to accept the added complexity involved in its implementation the bibliographical notes reference more detailed descriptions of extendable hashing implementation the bibliographical notes also provide references to another form of dynamic hashing called linear hashing  which avoids the extra level of indirection associated with extendable hashing  at the possible cost of more overflow buckets 11.8 comparison of ordered indexing and hashing we have seen several ordered-indexing schemes and several hashing schemes we can organize files of records as ordered files by using index-sequential organization or b + -tree organizations alternatively  we can organize the files by using hashing finally  we can organize them as heap files  where the records are not ordered in any particular way each scheme has advantages in certain situations a database-system implementor could provide many schemes  leaving the final decision of which schemes to use to the database designer however  such an approach requires the implementor to write more code  adding both to the cost of the system and to the space that the system occupies most database systems support b + -trees and may additionally support some form of hash file organization or hash indices to make a choice of file organization and indexing techniques for a relation  the implementor or the database designer must consider the following issues  ? is the cost of periodic reorganization of the index or hash organization acceptable ? ? what is the relative frequency of insertion and deletion ? ? is it desirable to optimize average access time at the expense of increasing the worst-case access time ? ? what types of queries are users likely to pose ? we have already examined the first three of these issues  first in our review of the relativemerits of specific indexing techniques  and again in our discussion of hashing techniques the fourth issue  the expected type of query  is critical to the choice of ordered indexing or hashing if most queries are of the form  select a1  a2      an from r where ai = c ; thesumit67.blogspot.com 524 chapter 11 indexing and hashing then  to process this query  the system will perform a lookup on an ordered index or a hash structure for attribute ai  for value c for queries of this form  a hashing scheme is preferable an ordered-index lookup requires time proportional to the log of the number of values in r for ai  in a hash structure  however  the average lookup time is a constant independent of the size of the database the only advantage to an index over a hash structure for this form of query is that the worst-case lookup time is proportional to the log of the number of values in r for ai  by contrast  for hashing  the worst-case lookup time is proportional to the number of values in r for ai  however  the worst-case lookup time is unlikely to occur with hashing  and hashing is preferable in this case ordered-index techniques are preferable to hashing in cases where the query specifies a range of values such a query takes the following form  select a1  a2    an from r where ai = c2 and ai = c1 ; in other words  the preceding query finds all the records with ai values between c1 and c2 let us consider how we process this query using an ordered index first  we perform a lookup on value c1 once we have found the bucket for value c1  we follow the pointer chain in the index to read the next bucket in order  and we continue in this manner until we reach c2 if  instead of an ordered index  we have a hash structure  we can perform a lookup on c1 and can locate the corresponding bucket ? but it is not easy  in general  to determine the next bucket that must be examined the difficulty arises because a good hash function assigns values randomly to buckets thus  there is no simple notion of ? next bucket in sorted order ? the reason we can not chain buckets together in sorted order on ai is that each bucket is assigned many searchkey values since values are scattered randomly by the hash function  the values in the specified range are likely to be scattered across many or all of the buckets therefore  we have to read all the buckets to find the required search keys usually the designer will choose ordered indexing unless it is known in advance that range queries will be infrequent  in which case hashing would be chosen hash organizations are particularly useful for temporary files created during query processing  if lookups based on a key value are required  but no range queries will be performed 11.9 bitmap indices bitmap indices are a specialized type of index designed for easy querying on multiple keys  although each bitmap index is built on a single key for bitmap indices to be used  records in a relation must be numbered sequentially  starting from  say  0 given a number n  it must be easy to retrieve thesumit67.blogspot.com 11.9 bitmap indices 525 the record numbered n this is particularly easy to achieve if records are fixed in size  and allocated on consecutive blocks of a file the record number can then be translated easily into a block number and a number that identifies the record within the block consider a relation r  with an attribute a that can take on only one of a small number  for example  2 to 20  values for instance  a relation instructor info may have an attribute gender  which can take only values m  male  or f  female   another example would be an attribute income level  where income has been broken up into 5 levels  l1  $ 0-9999  l2  $ 10,000-19,999  l3  20,000-39,999  l4  40,000-74,999  and l5  75,000-8 here  the raw data can take on many values  but a data analyst has split the values into a small number of ranges to simplify analysis of the data 11.9.1 bitmap index structure a bitmap is simply an array of bits in its simplest form  a bitmap index on the attribute a of relation r consists of one bitmap for each value that a can take each bitmap has as many bits as the number of records in the relation the ith bit of the bitmap for value vj is set to 1 if the record numbered i has the value vj for attribute a all other bits of the bitmap are set to 0 in our example  there is one bitmap for the value m and one for f the ith bit of the bitmap for m is set to 1 if the gender value of the record numbered i is m all other bits of the bitmap for m are set to 0 similarly  the bitmap for f has the value 1 for bits corresponding to records with the value f for the gender attribute ; all other bits have the value 0 figure 11.35 shows an example of bitmap indices on a relation instructor info we now consider when bitmaps are useful the simplest way of retrieving all records with value m  or value f  would be to simply read all records of the relation and select those records with value m  or f  respectively   the bitmap index doesn ? t really help to speed up such a selection.while it would allow us to id gender income_level 76766 22222 12121 15151 58583 m m f f f l1 l1 l2 l4 l3 record number 1 0 2 3 4 m f bitmaps for gender 10010 01101 bitmaps for income_level l1 l2 l3 l4 l5 10100 01000 00001 00010 00000 figure 11.35 bitmap indices on relation instructor info thesumit67.blogspot.com 526 chapter 11 indexing and hashing read only those records for a specific gender  it is likely that every disk block for the filewould have to be read anyway in fact  bitmap indices are useful for selections mainly when there are selections on multiple keys suppose we create a bitmap index on attribute income level  which we described earlier  in addition to the bitmap index on gender consider now a query that selects womenwith income in the range $ 10,000 to $ 19  999 this query can be expressed as select * from r where gender = ? f ? and income level = ? l2 ? ; to evaluate this selection  we fetch the bitmaps for gender value f and the bitmap for income level value l2  and perform an intersection  logical-and  of the two bitmaps in other words  we compute a new bitmap where bit i has value 1 if the ith bit of the two bitmaps are both 1  and has a value 0 otherwise in the example in figure 11.35  the intersection of the bitmap for gender = f  01101  and the bitmap for income level = l2  01000  gives the bitmap 01000 since the first attribute can take two values  and the second can take five values  we would expect only about 1 in 10 records  on an average  to satisfy a combined condition on the two attributes if there are further conditions  the fraction of records satisfying all the conditions is likely to be quite small the system can then compute the query result by finding all bits with value 1 in the intersection bitmap and retrieving the corresponding records if the fraction is large  scanning the entire relation would remain the cheaper alternative another important use of bitmaps is to count the number of tuples satisfying a given selection such queries are important for data analysis for instance  if we wish to find out how many women have an income level l2  we compute the intersection of the two bitmaps and then count the number of bits that are 1 in the intersection bitmap.we can thus get the desired result from the bitmap index  without even accessing the relation bitmap indices are generally quite small compared to the actual relation size records are typically at least tens of bytes to hundreds of bytes long  whereas a single bit represents the record in a bitmap thus the space occupied by a single bitmap is usually less than 1 percent of the space occupied by the relation for instance  if the record size for a given relation is 100 bytes  then the space occupied by a single bitmap would be 18 of 1 percent of the space occupied by the relation if an attribute a of the relation can take on only one of eight values  a bitmap index on attribute awould consist of eight bitmaps  which together occupy only 1 percent of the size of the relation deletion of records creates gaps in the sequence of records  since shifting records  or record numbers  to fill gaps would be extremely expensive to recognize deleted records,we can store an existence bitmap  inwhich bit i is 0 if record i does not exist and 1 otherwise we shall see the need for existence bitmaps in section 11.9.2 insertion of records should not affect the sequence numbering of thesumit67.blogspot.com 11.9 bitmap indices 527 other records therefore  we can do insertion either by appending records to the end of the file or by replacing deleted records 11.9.2 efficient implementation of bitmap operations we can compute the intersection of two bitmaps easily by using a for loop  the ith iteration of the loop computes the and of the ith bits of the two bitmaps we can speed up computation of the intersection greatly by using bit-wise and instructions supported by most computer instruction sets a word usually consists of 32 or 64 bits  depending on the architecture of the computer a bit-wise and instruction takes two words as input and outputs a word where each bit is the logical and of the bits in corresponding positions of the input words.what is important to note is that a single bit-wise and instruction can compute the intersection of 32 or 64 bits at once if a relation had 1 million records  each bitmap would contain 1 million bits  or equivalently 128 kilobytes only 31,250 instructions are needed to compute the intersection of two bitmaps for our relation  assuming a 32-bit word length thus  computing bitmap intersections is an extremely fast operation just as bitmap intersection is useful for computing the and of two conditions  bitmap union is useful for computing the or of two conditions the procedure for bitmap union is exactly the same as for intersection  except we use bit-wise or instructions instead of bit-wise and instructions the complement operation can be used to compute a predicate involving the negation of a condition  such as not  income-level = l1   the complement of a bitmap is generated by complementing every bit of the bitmap  the complement of 1 is 0 and the complement of 0 is 1   it may appear that not  income level = l1  can be implemented by just computing the complement of the bitmap for income level l1 if some records have been deleted  however  just computing the complement of a bitmap is not sufficient bits corresponding to such records would be 0 in the original bitmap  but would become 1 in the complement  although the records don ? t exist a similar problem also arises when the value of an attribute is null for instance  if the value of income level is null  the bit would be 0 in the original bitmap for value l1  and 1 in the complement bitmap to make sure that the bits corresponding to deleted records are set to 0 in the result  the complement bitmap must be intersected with the existence bitmap to turn off the bits for deleted records similarly  to handle null values  the complement bitmap must also be intersected with the complement of the bitmap for the value null.2 counting the number of bits that are 1 in a bitmap can be done quickly by a clever technique we can maintain an array with 256 entries  where the ith entry stores the number of bits that are 1 in the binary representation of i set the total count initially to 0.we take each byte of the bitmap  use it to index into this array  and add the stored count to the total count the number of addition operations is 2handling predicates such as is unknown would cause further complications  which would in general require use of an extra bitmap to track which operation results are unknown thesumit67.blogspot.com 528 chapter 11 indexing and hashing 18 of the number of tuples  and thus the counting process is very efficient a large array  using 216 = 65,536 entries   indexed by pairs of bytes  would give even higher speedup  but at a higher storage cost 11.9.3 bitmaps and b + -trees bitmaps can be combined with regular b + -tree indices for relations where a few attribute values are extremely common  and other values also occur  but much less frequently in a b + -tree index leaf  for each valuewewould normally maintain a list of all records with that value for the indexed attribute each element of the list would be a record identifier  consisting of at least 32 bits  and usually more for a value that occurs in many records  we store a bitmap instead of a list of records suppose a particular value vi occurs in 1 16 of the records of a relation let n be the number of records in the relation  and assume that a record has a 64-bit number identifying it the bitmap needs only 1 bit per record  or n bits in total in contrast  the list representation requires 64 bits per record where the value occurs  or 64 * n/16 = 4n bits thus  a bitmap is preferable for representing the list of records for value vi  in our example  with a 64-bit record identifier   if fewer than 1 in 64 records have a particular value  the list representation is preferable for identifying records with that value  since it uses fewer bits than the bitmap representation if more than 1 in 64 records have that value  the bitmap representation is preferable thus  bitmaps can be used as a compressed storage mechanism at the leaf nodes of b + -trees for those values that occur very frequently 11.10 index definition in sql the sql standard does not provide any way for the database user or administrator to control what indices are created and maintained in the database system indices are not required for correctness  since they are redundant data structures however  indices are important for efficient processing of transactions  including both update transactions and queries indices are also important for efficient enforcement of integrity constraints in principle  a database system can decide automatically what indices to create however  because of the space cost of indices  as well as the effect of indices on update processing  it is not easy to automatically make the right choices about what indices to maintain therefore  most sql implementations provide the programmer control over creation and removal of indices via data-definitionlanguage commands we illustrate the syntax of these commands next although the syntax that we show is widely used and supported by many database systems  it is not part of the sql standard the sql standard does not support control of the physical database schema ; it restricts itself to the logical database schema we create an index with the create index command  which takes the form  thesumit67.blogspot.com 11.11 summary 529 create index < index-name > on < relation-name >  < attribute-list >  ; the attribute-list is the list of attributes of the relations that form the search key for the index to define an index named dept index on the instructor relation with dept name as the search key  we write  create index dept index on instructor  dept name  ; if we wish to declare that the search key is a candidate key  we add the attribute unique to the index definition thus  the command  create unique index dept index on instructor  dept name  ; declares dept name to be a candidate key for instructor  which is probably notwhat we actually would want for our university database   if  at the time we enter the create unique index command  dept name is not a candidate key  the system will display an error message  and the attempt to create the index will fail if the index-creation attempt succeeds  any subsequent attempt to insert a tuple that violates the key declaration will fail note that the unique feature is redundant if the database system supports the unique declaration of the sql standard many database systems also provide a way to specify the type of index to be used  such as b + -tree or hashing   some database systems also permit one of the indices on a relation to be declared to be clustered ; the system then stores the relation sorted by the search-key of the clustered index the index name we specified for an index is required to drop an index the drop index command takes the form  drop index < index-name > ; 11.11 summary ? many queries reference only a small proportion of the records in a file to reduce the overhead in searching for these records  we can construct indices for the files that store the database ? index-sequential files are one of the oldest index schemes used in database systems to permit fast retrieval of records in search-key order  records are stored sequentially  and out-of-order records are chained together to allow fast random access  we use an index structure ? there are two types of indices that we can use  dense indices and sparse indices dense indices contain entries for every search-key value  whereas sparse indices contain entries only for some search-key values thesumit67.blogspot.com 530 chapter 11 indexing and hashing ? if the sort order of a search key matches the sort order of a relation  an index on the search key is called a clustering index the other indices are called nonclustering or secondary indices secondary indices improve the performance of queries that use search keys other than the search key of the clustering index however  they impose an overhead on modification of the database ? the primary disadvantage of the index-sequential file organization is that performance degrades as the file grows to overcome this deficiency  we can use a b + -tree index ? a b + -tree index takes the form of a balanced tree  in which every path from the root of the tree to a leaf of the tree is of the same length the height of a b + -tree is proportional to the logarithm to the base n of the number of records in the relation  where each nonleaf node stores n pointers ; the value of n is often around 50 or 100 b + -trees are much shorter than other balanced binary-tree structures such as avl trees  and therefore require fewer disk accesses to locate records ? lookup on b + -trees is straightforward and efficient insertion and deletion  however  are somewhat more complicated  but still efficient the number of operations required for lookup  insertion  and deletion on b + -trees is proportional to the logarithm to the base n of the number of records in the relation  where each nonleaf node stores n pointers ? we can use b + -trees for indexing a file containing records  as well as to organize records into a file ? b-tree indices are similar to b + -tree indices the primary advantage of a b-tree is that the b-tree eliminates the redundant storage of search-key values the major disadvantages are overall complexity and reduced fanout for a given node size system designers almost universally prefer b + -tree indices over b-tree indices in practice ? sequential file organizations require an index structure to locate data file organizations based on hashing  by contrast  allow us to find the address of a data item directly by computing a function on the search-key value of the desired record since we do not know at design time precisely which searchkey values will be stored in the file  a good hash function to choose is one that assigns search-key values to buckets such that the distribution is both uniform and random ? static hashing uses hash functions in which the set of bucket addresses is fixed such hash functions can not easily accommodate databases that grow significantly larger over time there are several dynamic hashing techniques that allow the hash function to be modified one example is extendable hashing  which copes with changes in database size by splitting and coalescing buckets as the database grows and shrinks thesumit67.blogspot.com review terms 531 ? we can also use hashing to create secondary indices ; such indices are called hash indices for notational convenience  we assume hash file organizations have an implicit hash index on the search key used for hashing ? ordered indices such as b + -trees and hash indices can be used for selections based on equality conditions involving single attributes when multiple attributes are involved in a selection condition  we can intersect record identifiers retrieved from multiple indices ? bitmap indices provide a very compact representation for indexing attributes with very few distinct values intersection operations are extremely fast on bitmaps  making them ideal for supporting queries on multiple attributes review terms ? access types ? access time ? insertion time ? deletion time ? space overhead ? ordered index ? clustering index ? primary index ? nonclustering index ? secondary index ? index-sequential file ? index entry/record ? dense index ? sparse index ? multilevel index ? composite key ? sequential scan ? b + -tree index ? leaf node ? nonleaf node ? balanced tree ? range query ? node split ? node coalesce ? nonunique search key ? b + -tree file organization ? bulk load ? bottom-up b + -tree construction ? b-tree index ? static hashing ? hash file organization ? hash index ? bucket ? hash function ? bucket overflow ? skew ? closed hashing ? dynamic hashing ? extendable hashing ? multiple-key access ? indices on multiple keys ? bitmap index ? bitmap operations ? intersection ? union ? complement ? existence bitmap thesumit67.blogspot.com 532 chapter 11 indexing and hashing practice exercises 11.1 indices speed query processing  but it is usually a bad idea to create indices on every attribute  and every combinations of attributes  that is a potential search keys explain why 11.2 is it possible in general to have two clustering indices on the same relation for different search keys ? explain your answer 11.3 construct a b + -tree for the following set of key values   2  3  5  7  11  17  19  23  29  31  assume that the tree is initially empty and values are added in ascending order construct b + -trees for the cases where the number of pointers that will fit in one node is as follows  a four b six c eight 11.4 for each b + -tree of practice exercise 11.3  show the form of the tree after each of the following series of operations  a insert 9 b insert 10 c insert 8 d delete 23 e delete 19 11.5 consider the modified redistribution scheme for b + -trees described on page 501 what is the expected height of the tree as a function of n ? 11.6 suppose that we are using extendable hashing on a file that contains records with the following search-key values  2  3  5  7  11  17  19  23  29  31 show the extendable hash structure for this file if the hash function is h  x  = xmod 8 and buckets can hold three records 11.7 show how the extendable hash structure of practice exercise 11.6 changes as the result of each of the following steps  a delete 11 b delete 31 thesumit67.blogspot.com practice exercises 533 c insert 1 d insert 15 11.8 give pseudocode for a b + -tree function finditerator    which is like the function find    except that it returns an iterator object  as described in section 11.3.2 also give pseudocode for the iterator class  including the variables in the iterator object  and the next   method 11.9 give pseudocode for deletion of entries froman extendable hash structure  including details of when and how to coalesce buckets do not bother about reducing the size of the bucket address table 11.10 suggest an efficient way to test if the bucket address table in extendable hashing can be reduced in size  by storing an extra count with the bucket address table give details of how the count should be maintained when buckets are split  coalesced  or deleted  note  reducing the size of the bucket address table is an expensive operation  and subsequent inserts may cause the table to grow again therefore  it is best not to reduce the size as soon as it is possible to do so  but instead do it only if the number of index entries becomes small compared to the bucket-address-table size  11.11 consider the instructor relation shown in figure 11.1 a construct a bitmap index on the attribute salary  dividing salary values into 4 ranges  below 50000  50000 to below 60000  60000 to below 70000  and 70000 and above b consider a query that requests all instructors in the finance department with salary of 80000 or more outline the steps in answering the query  and show the final and intermediate bitmaps constructed to answer the query 11.12 what would the occupancy of each leaf node of a b + -tree be  if index entries are inserted in sorted order ? explain why 11.13 suppose you have a relation r with nr tuples on which a secondary b + -tree is to be constructed a give a formula for the cost of building the b + -tree index by inserting one record at a time assume each block will hold an average of f entries  and that all levels of the tree above the leaf are in memory b assuming a random disk access takes 10 milliseconds  what is the cost of index construction on a relation with 10 million records ? c write pseudocode for bottom-up construction of a b + -tree  which was outlined in section 11.4.4 you can assume that a function to efficiently sort a large file is available 11.14 why might the leaf nodes of a b + -tree file organization lose sequentiality ? thesumit67.blogspot.com 534 chapter 11 indexing and hashing a suggest how the file organization may be reorganized to restore sequentiality b an alternative to reorganization is to allocate leaf pages in units of n blocks  for some reasonably large n when the first leaf of a b +  tree is allocated  only one block of an n-block unit is used  and the remaining pages are free if a page splits  and its n-block unit has a free page  that space is used for the new page if the n-block unit is full  another n-block unit is allocated  and the first n/2 leaf pages are placed in one n-block unit  and the remaining in the second n-block unit for simplicity  assume that there are no delete operations i what is the worst case occupancy of allocated space  assuming no delete operations  after the first n-block unit is full ii is it possible that leaf nodes allocated to an n-node block unit are not consecutive  that is  is it possible that two leaf nodes are allocated to one n-node block  but another leaf node in between the two is allocated to a different n-node block ? iii under the reasonable assumption that buffer space is sufficient to store a n-page block  how many seeks would be required for a leaf-level scan of the b + -tree  in the worst case ? compare this number with the worst case if leaf pages are allocated a block at a time iv the technique of redistributing values to siblings to improve space utilization is likely to be more efficientwhen usedwith the above allocation scheme for leaf blocks explain why exercises 11.15 when is it preferable to use a dense index rather than a sparse index ? explain your answer 11.16 what is the difference between a clustering index and a secondary index ? 11.17 for each b + -tree of practice exercise 11.3  show the steps involved in the following queries  a find records with a search-key value of 11 b find records with a search-key value between 7 and 17  inclusive 11.18 the solution presented in section 11.3.4 to deal with nonunique search keys added an extra attribute to the search key what effect could this change have on the height of the b + -tree ? 11.19 explain the distinction between closed and open hashing discuss the relative merits of each technique in database applications thesumit67.blogspot.com exercises 535 11.20 what are the causes of bucket overflow in a hash file organization ? what can be done to reduce the occurrence of bucket overflows ? 11.21 why is a hash structure not the best choice for a search key on which range queries are likely ? 11.22 suppose there is a relation r  a  b,c   with a b + -tree index with search key  a  b   a what is theworst-case cost of finding records satisfying 10 < a < 50 using this index  in terms of the number of records retrieved n1 and the height h of the tree ? b what is the worst-case cost of finding records satisfying 10 < a < 50 ? 5 < b < 10 using this index  in terms of the number of records n2 that satisfy this selection  as well as n1 and h defined above ? c under what conditions on n1 and n2 would the index be an efficient way of finding records satisfying 10 < a < 50 ? 5 < b < 10 ? 11.23 suppose you have to create a b + -tree index on a large number of names  where the maximum size of a name may be quite large  say 40 characters  and the average name is itself large  say 10 characters   explain how prefix compression can be used to maximize the average fanout of nonleaf nodes 11.24 suppose a relation is stored in a b + -tree file organization suppose secondary indices stored record identifiers that are pointers to records on disk a what would be the effect on the secondary indices if a node split happens in the file organization ? b what would be the cost of updating all affected records in a secondary index ? c how does using the search key of the file organization as a logical record identifier solve this problem ? d what is the extracost due to theuseof suchlogical recordidentifiers ? 11.25 show how to compute existence bitmaps from other bitmaps make sure that your technique works even in the presence of null values  by using a bitmap for the value null 11.26 how does data encryption affect index schemes ? in particular  how might it affect schemes that attempt to store data in sorted order ? 11.27 our description of static hashing assumes that a large contiguous stretch of disk blocks can be allocated to a static hash table suppose you can allocate only c contiguous blocks suggest how to implement the hash table  if it can be much larger than c blocks access to a block should still be efficient thesumit67.blogspot.com 536 chapter 11 indexing and hashing bibliographical notes discussions of the basic data structures in indexing and hashing can be found in cormen et al  1990   b-tree indices were first introduced in bayer  1972  and bayer and mccreight  1972   b + -trees are discussed in comer  1979   bayer and unterauer  1977   and knuth  1973   the bibliographical notes in chapter 15 provide references to research on allowing concurrent accesses and updates on b + -trees gray and reuter  1993  provide a good description of issues in the implementation of b + -trees several alternative tree and treelike search structures have been proposed tries are trees whose structure is based on the ? digits ? of keys  for example  a dictionary thumb index  which has one entry for each letter   such trees may not be balanced in the sense that b + -trees are tries are discussed by ramesh et al  1989   orenstein  1982   litwin  1981   and fredkin  1960   related work includes the digital b-trees of lomet  1981   knuth  1973  analyzes a large number of different hashing techniques several dynamic hashing schemes exist extendable hashing was introduced by fagin et al  1979   linear hashing was introduced by litwin  1978  and litwin  1980   a performance comparison with extendable hashing is given by rathi et al  1990   an alternative given by ramakrishna and larson  1989  allows retrieval in a single disk access at the price of a high overhead for a small fraction of database modifications partitioned hashing is an extension of hashing tomultiple attributes  and is covered in rivest  1976   burkhard  1976   and burkhard  1979   vitter  2001  provides an extensive survey of external-memory data structures and algorithms bitmap indices  and variants called bit-sliced indices and projection indices  are described in o ? neil and quass  1997   they were first introduced in the ibm model 204 file manager on theas 400 platform they provide very large speedups on certain types of queries  and are today implemented on most database systems research on bitmap indices includes wu and buchmann  1998   chan and ioannidis  1998   chan and ioannidis  1999   and johnson  1999   thesumit67.blogspot.com chapter12 query processing query processing refers to the range of activities involved in extracting data from a database the activities include translation of queries in high-level database languages into expressions that can be used at the physical level of the file system  a variety of query-optimizing transformations  and actual evaluation of queries 12.1 overview the steps involved in processing a query appear in figure 12.1 the basic steps are  1 parsing and translation 2 optimization 3 evaluation before query processing can begin  the system must translate the query into a usable form a language such as sql is suitable for human use  but is ill suited to be the system ? s internal representation of a query a more useful internal representation is one based on the extended relational algebra thus  the first action the system must take in query processing is to translate a given query into its internal form this translation process is similar to the work performed by the parser of a compiler in generating the internal form of the query  the parser checks the syntax of the user ? s query  verifies that the relation names appearing in the query are names of the relations in the database  and so on the system constructs a parse-tree representation of the query  which it then translates into a relational-algebra expression if the query was expressed in terms of a view  the translation phase also replaces all uses of the view by the relational-algebra expression that defines the view.1 most compiler texts cover parsing in detail 1for materialized views  the expression defining the view has already been evaluated and stored therefore  the stored relation can be used  instead of uses of the view being replaced by the expression defining the view recursive views are handled differently  via a fixed-point procedure  as discussed in section 5.4 and appendix b.3.6 537 thesumit67.blogspot.com 538 chapter 12 query processing query output query parser and translator evaluation engine relational-algebra expression execution plan optimizer data statistics about data figure 12.1 steps in query processing given a query  there are generally a variety of methods for computing the answer for example  we have seen that  in sql  a query could be expressed in several different ways each sql query can itself be translated into a relationalalgebra expression in one of several ways furthermore  the relational-algebra representation of a query specifies only partially howto evaluate a query ; there are usually several ways to evaluate relational-algebra expressions as an illustration  consider the query  select salary from instructor where salary < 75000 ; this query can be translated into either of the following relational-algebra expressions  ?  salary < 75000   salary  instructor   ?  salary   salary < 75000  instructor   further  we can execute each relational-algebra operation by one of several different algorithms for example  to implement the preceding selection  we can search every tuple in instructor to find tuples with salary less than 75000 if a b + -tree index is available on the attribute salary  we can use the index instead to locate the tuples to specify fully how to evaluate a query  we need not only to provide the relational-algebra expression  but also to annotate it with instructions specifying how to evaluate each operation annotations may state the algorithm to be used thesumit67.blogspot.com 12.1 overview 539 salary salary < 75000 ; use index 1 instructor s p figure 12.2 a query-evaluation plan for a specific operation  or the particular index or indices to use a relationalalgebra operation annotated with instructions on how to evaluate it is called an evaluation primitive a sequence of primitive operations that can be used to evaluate a query is a query-execution plan or query-evaluation plan figure 12.2 illustrates an evaluation plan for our example query  in which a particular index  denoted in the figure as ? index 1 ?  is specified for the selection operation the query-execution engine takes a query-evaluation plan  executes that plan  and returns the answers to the query the different evaluation plans for a given query can have different costs we do not expect users to write their queries in a way that suggests the most efficient evaluation plan rather  it is the responsibility of the system to construct a queryevaluation plan that minimizes the cost of query evaluation ; this task is called query optimization chapter 13 describes query optimization in detail once the query plan is chosen  the query is evaluated with that plan  and the result of the query is output the sequence of steps already described for processing a query is representative ; not all databases exactly follow those steps for instance  instead of using the relational-algebra representation  several databases use an annotated parsetree representation based on the structure of the given sql query however  the concepts that we describe here form the basis of query processing in databases in order to optimize a query  a query optimizer must know the cost of each operation although the exact cost is hard to compute  since it depends on many parameters such as actual memory available to the operation  it is possible to get a rough estimate of execution cost for each operation in this chapter we study how to evaluate individual operations in a query plan  and how to estimate their cost ; we return to query optimization in chapter 13 section 12.2 outlines how we measure the cost of a query sections 12.3 through 12.6 cover the evaluation of individual relational-algebra operations several operations may be grouped together into a pipeline  in which each of the operations starts working on its input tuples even as they are being generated by another operation in section 12.7  we examine how to coordinate the execution of multiple operations in a query evaluation plan  in particular  how to use pipelined operations to avoid writing intermediate results to disk thesumit67.blogspot.com 540 chapter 12 query processing 12.2 measures of query cost there are multiple possible evaluation plans for a query  and it is important to be able to compare the alternatives in terms of their  estimated  cost  and choose the best plan to do so  we must estimate the cost of individual operations  and combine them to get the cost of a query evaluation plan thus  as we study evaluation algorithms for each operation later in this chapter  we also outline how to estimate the cost of the operation the cost of query evaluation can be measured in terms of a number of different resources  including disk accesses  cpu time to execute a query  and  in a distributed or parallel database system  the cost of communication  which we discuss later  in chapters 18 and 19   in large database systems  the cost to access data from disk is usually the most important cost  since disk accesses are slow compared to in-memory operations moreover  cpu speeds have been improving much faster than have disk speeds thus  it is likely that the time spent in disk activity will continue to dominate the total time to execute a query the cpu time taken for a task is harder to estimate since it depends on low-level details of the execution code although real-life query optimizers do take cpu costs into account  for simplicity in this book we ignore cpu costs and use only disk-access costs to measure the cost of a query-evaluation plan we use the number of block transfers from disk and the number of disk seeks to estimate the cost of a query-evaluation plan if the disk subsystem takes an average of tt seconds to transfer a block of data  and has an average block-access time  disk seek time plus rotational latency  of ts seconds  then an operation that transfers b blocks and performs s seeks would take b * tt + s * ts seconds the values of tt and ts must be calibrated for the disk system used  but typical values for high-end disks today would be ts = 4 milliseconds and tt = 0.1 milliseconds  assuming a 4-kilobyte block size and a transfer rate of 40 megabytes per second.2 we can refine our cost estimates further by distinguishing block reads from block writes  since block writes are typically about twice as expensive as reads  this is because disk systems read sectors back after they are written to verify that the write was successful   for simplicity  we ignore this detail  and leave it to you to work out more precise cost estimates for various operations the cost estimates we give do not include the cost ofwriting the final result of an operation back to disk these are taken into account separatelywhere required the costs of all the algorithms that we consider depend on the size of the buffer in main memory in the best case  all data can be read into the buffers  and the disk does not need to be accessed again in the worst case  we assume that the buffer can hold only a few blocks of data ? approximately one block per relation when presenting cost estimates  we generally assume the worst case 2some database systems perform test seeks and block transfers to estimate average seek and block transfer costs  as part of the software installation process thesumit67.blogspot.com 12.3 selection operation 541 in addition  although we assume that data must be read from disk initially  it is possible that a block that is accessed is already present in the in-memory buffer again  for simplicity  we ignore this effect ; as a result  the actual disk-access cost during the execution of a plan may be less than the estimated cost the response time for a query-evaluation plan  that is  the wall-clock time required to execute the plan   assuming no other activity is going on in the computer  would account for all these costs  and could be used as a measure of the cost of the plan unfortunately  the response time of a plan is very hard to estimate without actually executing the plan  for the following reasons  1 the response time depends on the contents of the buffer when the query begins execution ; this information is not available when the query is optimized  and is hard to account for even if it were available 2 in a system with multiple disks  the response time depends on how accesses are distributed among disks  which is hard to estimate without detailed knowledge of data layout on disk interestingly  a plan may get a better response time at the cost of extra resource consumption for example  if a system has multiple disks  a plan athat requires extra disk reads  but performs the reads in parallel across multiple disks may finish faster than another plan b that has fewer disk reads  but from only one disk however  if many instances of a query using plan a run concurrently  the overall response time mayactually be more than if the same instances are executed using plan b  since plan agenerates more load on the disks as a result  instead of trying to minimize the response time  optimizers generally try to minimize the total resource consumption of a query plan our model of estimating the total disk access time  including seek and data transfer  is an example of such a resource consumption ? based model of query cost 12.3 selection operation in query processing  the file scan is the lowest-level operator to access data file scans are search algorithms that locate and retrieve records that fulfill a selection condition in relational systems  a file scan allows an entire relation to be read in those cases where the relation is stored in a single  dedicated file 12.3.1 selections using file scans and indices consider a selection operation on a relation whose tuples are stored together in one file the most straightforward way of performing a selection is as follows  ? a1  linear search   in a linear search  the system scans each file block and tests all records to see whether they satisfy the selection condition an initial seek is required to access the first block of the file in case blocks of the file thesumit67.blogspot.com 542 chapter 12 query processing are not stored contiguously  extra seeks may be required  but we ignore this effect for simplicity although it may be slower than other algorithms for implementing selection  the linear-search algorithm can be applied to any file  regardless of the ordering of the file  or the availability of indices  or the nature of the selection operation the other algorithms that we shall study are not applicable in all cases  but when applicable they are generally faster than linear search cost estimates for linear scan  as well as for other selection algorithms  are shown in figure 12.3 in the figure  we use hi to represent the height of the b +  tree real-life optimizers usually assume that the root of the tree is present in the in-memory buffer since it is frequently accessed some optimizers even assume that all but the leaf level of the tree is present in memory  since they are accessed relatively frequently  and usually less than 1 percent of the nodes of a b + -tree are nonleaf nodes the cost formulae can be modified appropriately index structures are referred to as access paths  since they provide a path through which data can be located and accessed in chapter 11  we pointed out that it is efficient to read the records of a file in an order corresponding closely to physical order recall that a primary index  also referred to as a clustering index  is an index that allows the records of a file to be read in an order that corresponds to the physical order in the file an index that is not a primary index is called a secondary index search algorithms that use an index are referred to as index scans we use the selection predicate to guide us in the choice of the index to use in processing the query search algorithms that use an index are  ? a2  primary index  equality on key   for an equality comparison on a key attributewith a primary index,we can use the index to retrieve a single record that satisfies the corresponding equality condition cost estimates are shown in figure 12.3 ? a3  primary index  equality on nonkey   we can retrieve multiple records by using a primary index when the selection condition specifies an equality comparison on a nonkey attribute  a the only difference from the previous case is that multiple records may need to be fetched however  the records must be stored consecutively in the file since the file is sorted on the search key cost estimates are shown in figure 12.3 ? a4  secondary index  equality   selections specifying an equality condition can use a secondary index this strategy can retrieve a single record if the equality condition is on a key ; multiple records may be retrieved if the indexing field is not a key in the first case  only one record is retrieved the time cost in this case is the same as that for a primary index  casea2   in the second case  each record may be resident on a different block  which may result in one i/o operation per retrieved record,with each i/o operation requiring a seek and a block transfer the worst-case time cost in this case is thesumit67.blogspot.com 12.3 selection operation 543 algorithm cost reason a1 linear search ts + br * tt one initial seek plus br block transfers  where br denotes the number of blocks in the file a1 linear search  equality on key average case ts +  br /2  * tt since at most one record satisfies condition  scan can be terminated as soon as the required record is found in the worst case  br blocks transfers are still required a2 primary b + -tree index  equality on key  hi + 1  *  tt + ts   where hi denotes the height of the index  index lookup traverses the height of the tree plus one i/o to fetch the record ; each of these i/o operations requires a seek and a block transfer a3 primary b + -tree index  equality on nonkey hi *  tt + ts  + b * tt one seek for each level of the tree  one seek for the first block here b is the number of blocks containing records with the specified search key  all of which are read these blocks are leaf blocks assumed to be stored sequentially  since it is a primary index  and don ? t require additional seeks a4 secondary b + -tree index  equality on key  hi + 1  *  tt + ts  this case is similar to primary index a4 secondary b + -tree index  equality on nonkey  hi + n  *  tt + ts   where n is the number of records fetched  here  cost of index traversal is the same as for a3  but each record may be on a different block  requiring a seek per record cost is potentially very high if n is large a5 primary b + -tree index  comparison hi *  tt + ts  + b * tt identical to the case of a3  equality on nonkey a6 secondary b + -tree index  comparison  hi + n  *  tt + ts  identical to the case of a4  equality on nonkey figure 12.3 cost estimates for selection algorithms  hi + n  *  ts + tt   where n is the number of records fetched  if each record is in a different disk block  and the block fetches are randomly ordered the worst-case cost could become even worse than that of linear search if a large number of records are retrieved thesumit67.blogspot.com 544 chapter 12 query processing if the in-memory buffer is large  the block containing the record may already be in the buffer it is possible to construct an estimate of the average or expected cost of the selection by taking into account the probability of the block containing the record already being in the buffer for large buffers  that estimate will be much less than the worst-case estimate in certain algorithms  including a2  the use of a b + -tree file organization can save one access since records are stored at the leaf-level of the tree as described in section 11.4.2  when records are stored in a b + -tree file organization or other file organizations that may require relocation of records  secondary indices usually do not store pointers to the records.3 instead  secondary indices store the values of the attributes used as the search key in a b + -tree file organization accessing a record through such a secondary index is then more expensive  first the secondary index is searched to find the primary index search-key values  then the primary index is looked up to find the records the cost formulae described for secondary indices have to be modified appropriately if such indices are used 12.3.2 selections involving comparisons consider a selection of the form  a = v  r   we can implement the selection either by using linear search or by using indices in one of the following ways  ? a5  primary index  comparison   a primary ordered index  for example  a primary b + -tree index  can be used when the selection condition is a comparison for comparison conditions of the form a > v or a = v  a primary index on acan be used to direct the retrieval of tuples  as follows  for a = v  we look up the value v in the index to find the first tuple in the file that has a value of a = v a file scan starting from that tuple up to the end of the file returns all tuples that satisfy the condition for a > v  the file scan startswith the first tuple such that a > v the cost estimate for this case is identical to that for case a3 for comparisons of the form a < v or a = v  an index lookup is not required for a < v  we use a simple file scan starting from the beginning of the file  and continuing up to  but not including  the first tuple with attribute a = v the case of a = v is similar  except that the scan continues up to  but not including  the first tuple with attribute a > v in either case  the index is not useful ? a6  secondary index  comparison   we can use a secondary ordered index to guide retrieval for comparison conditions involving <  =  =  or >  the lowest-level index blocks are scanned  either from the smallest value up to v  for < and =   or from v up to the maximum value  for > and =   3recall that if b + -tree file organizations are used to store relations  records may be moved between blocks when leaf nodes are split or merged  and when records are redistributed thesumit67.blogspot.com 12.3 selection operation 545 the secondary index provides pointers to the records  but to get the actual records we have to fetch the records by using the pointers this step may require an i/o operation for each record fetched  since consecutive records may be on different disk blocks ; as before  each i/o operation requires a disk seek and a block transfer if the number of retrieved records is large  using the secondary index may be even more expensive than using linear search therefore the secondary index should be used only if very few records are selected 12.3.3 implementation of complex selections so far  we have considered only simple selection conditions of the form a op b  where op is an equality or comparison operation.we now consider more complex selection predicates ? conjunction  a conjunctive selection is a selection of the form    1 ?  2 ? ? ? ? ?  n  r  ? disjunction  a disjunctive selection is a selection of the form    1 ?  2 ? ? ? ? ?  n  r  a disjunctive condition is satisfied by the union of all records satisfying the individual  simple conditions  i  ? negation  the result of a selection  ?   r  is the set of tuples of r for which the condition  evaluates to false in the absence of nulls  this set is simply the set of tuples in r that are not in    r   we can implement a selection operation involving either a conjunction or a disjunction of simple conditions by using one of the following algorithms  ? a7  conjunctive selection using one index   we first determine whether an access path is available for an attribute in one of the simple conditions if one is  one of the selection algorithms a2 through a6 can retrieve records satisfying that condition.we complete the operation by testing  in thememory buffer  whether or not each retrieved record satisfies the remaining simple conditions to reduce the cost  we choose a  i and one of algorithms a1 through a6 for which the combination results in the least cost for   i  r   the cost of algorithm a7 is given by the cost of the chosen algorithm ? a8  conjunctive selection using composite index   an appropriate composite index  that is  an index on multiple attributes  may be available for some conjunctive selections if the selection specifies an equality condition on two or more attributes  and a composite index exists on these combined attribute thesumit67.blogspot.com 546 chapter 12 query processing fields  then the index can be searched directly the type of index determines which of algorithms a2  a3  or a4 will be used ? a9  conjunctive selection by intersection of identifiers   another alternative for implementing conjunctive selection operations involves the use of record pointers or record identifiers this algorithm requires indices with record pointers  on the fields involved in the individual conditions the algorithm scans each index for pointers to tuples that satisfy an individual condition the intersection of all the retrieved pointers is the set of pointers to tuples that satisfy the conjunctive condition the algorithm then uses the pointers to retrieve the actual records if indices are not available on all the individual conditions  then the algorithm tests the retrieved records against the remaining conditions the cost of algorithm a9is the sum of the costs of the individual index scans  plus the cost of retrieving the records in the intersection of the retrieved lists of pointers this cost can be reduced by sorting the list of pointers and retrieving records in the sorted order thereby   1  all pointers to records in a block come together  hence all selected records in the block can be retrieved using a single i/o operation  and  2  blocks are read in sorted order  minimizing disk-arm movement section 12.4 describes sorting algorithms ? a10  disjunctive selection by union of identifiers   if access paths are available on all the conditions of a disjunctive selection  each index is scanned for pointers to tuples that satisfy the individual condition the union of all the retrieved pointers yields the set of pointers to all tuples that satisfy the disjunctive condition.we then use the pointers to retrieve the actual records however  if even one of the conditions does not have an access path  we have to perform a linear scan of the relation to find tuples that satisfy the condition therefore  if there is even one such condition in the disjunct  the most efficient access method is a linear scan  with the disjunctive condition tested on each tuple during the scan the implementation of selectionswith negation conditions is left to you as an exercise  practice exercise 12.6   12.4 sorting sorting of data plays an important role in database systems for two reasons first  sql queries can specify that the output be sorted second  and equally important for query processing  several of the relational operations  such as joins  can be implemented efficiently if the input relations are first sorted thus  we discuss sorting here before discussing the join operation in section 12.5 we can sort a relation by building an index on the sort key  and then using that index to read the relation in sorted order however  such a process orders the relation only logically  through an index  rather than physically hence  the reading of tuples in the sorted order may lead to a disk access  disk seek plus thesumit67.blogspot.com 12.4 sorting 547 block transfer  for each record  which can be very expensive  since the number of records can be much larger than the number of blocks for this reason  it may be desirable to order the records physically the problem of sorting has been studied extensively  both for relations that fit entirely in main memory and for relations that are bigger than memory in the first case  standard sorting techniques such as quick-sort can be used here  we discuss how to handle the second case 12.4.1 external sort-merge algorithm sorting of relations that do not fit in memory is called external sorting the most commonly used technique for external sorting is the external sort ? merge algorithm.we describe the external sort ? merge algorithm next let mdenote the number of blocks in the main-memory buffer available for sorting  that is  the number of disk blocks whose contents can be buffered in available main memory 1 in the first stage  a number of sorted runs are created ; each run is sorted  but contains only some of the records of the relation i = 0 ; repeat read mblocks of the relation  or the rest of the relation  whichever is smaller ; sort the in-memory part of the relation ; write the sorted data to run file ri ; i = i + 1 ; until the end of the relation 2 in the second stage  the runs are merged suppose  for now  that the total number of runs  n  is less than m  so that we can allocate one block to each run and have space left to hold one block of output the merge stage operates as follows  read one block of each of the n files ri into a buffer block in memory ; repeat choose the first tuple  in sort order  among all buffer blocks ; write the tuple to the output  and delete it from the buffer block ; if the buffer block of any run ri is empty and not end-of-file  ri  then read the next block of ri into the buffer block ; until all input buffer blocks are empty the output of the merge stage is the sorted relation the output file is buffered to reduce the number of disk write operations the preceding merge operation is a generalization of the two-way merge used by the standard in-memory sort ? merge algorithm ; it merges n runs  so it is called an n-way merge thesumit67.blogspot.com 548 chapter 12 query processing g a d 31 c 33 b 14 e 16 r 16 d 21 m 3 p 2 d 7 a 14 a 14 a 19 b 14 c 33 d 7 d 21 d 31 e 16 g 24 m 3 p 2 r 16 a 19 b 14 c 33 d 31 e 16 g 24 a 14 d 7 d 21 m 3 p 2 r 16 a 19 d 31 g 24 b 14 c 33 e 16 d 21 m 3 r 16 a 14 d 7 p 2 initial relation create runs merge pass ? 1 merge pass ? 2 runs runs sorted output 24 19 figure 12.4 external sorting using sort ? merge in general  if the relation is much larger than memory  there may be m or more runs generated in the first stage  and it is not possible to allocate a block for each run during the merge stage in this case  the merge operation proceeds in multiple passes since there is enough memory for m 1 input buffer blocks  each merge can take m 1 runs as input the initial pass functions in this way  it merges the first m 1 runs  as described in item 2 above  to get a single run for the next pass then  it merges the next m  1 runs similarly  and so on  until it has processed all the initial runs at this point  the number of runs has been reduced by a factor of m 1 if this reduced number of runs is still greater than or equal to m  another pass ismade  with the runs created by the first pass as input each pass reduces the number of runs by a factor of m 1 the passes repeat as many times as required  until the number of runs is less than m ; a final pass then generates the sorted output figure 12.4 illustrates the steps of the external sort ? merge for an example relation for illustration purposes  we assume that only one tuple fits in a block  fr = 1   and we assume that memory holds at most three blocks during the merge stage  two blocks are used for input and one for output 12.4.2 cost analysis of external sort-merge we compute the disk-access cost for the external sort ? merge in this way  let br denote the number of blocks containing records of relation r  the first stage thesumit67.blogspot.com 12.5 join operation 549 reads every block of the relation and writes them out again  giving a total of 2br block transfers the initial number of runs is br /m   since the number of runs decreases by a factor of m  1 in each merge pass  the total number of merge passes required is logm-1  br /m    each of these passes reads every block of the relation once and writes it out once  with two exceptions first  the final pass can produce the sorted output without writing its result to disk second  there may be runs that are not read in or written out during a pass ? for example  if there are m runs to be merged in a pass  m 1 are read in and merged  and one run is not accessed during the pass ignoring the  relatively small  savings due to the latter effect  the total number of block transfers for external sorting of the relation is  br  2 logm-1  br /m   + 1  applying this equation to the example in figure 12.4  we get a total of 12 *  4 + 1  = 60 block transfers  as you can verify from the figure note that the above numbers do not include the cost of writing out the final result we also need to add the disk-seek costs run generation requires seeks for reading data for each of the runs aswell as for writing the runs during the merge phase  if data are read bb blocks at a time from each run  that is  bb buffer blocks are allocated to each run   then each merge pass would require around br /bb  seeks for reading data.4 although the output is written sequentially  if it is on the same disk as the input runs the head may have moved away between writes of consecutive blocks thus we would have to add a total of 2 br /bb  seeks for each merge pass  except the final pass  since we assume the final result is not written back to disk   the total number of seeks is then  2 br /m  + br /bb   2 logm-1  br /m    1  applying this equation to the example in figure 12.4  we get a total of 8 + 12 *  2 * 2  1  = 44 disk seeks if we set the number of buffer blocks per run  bb to 1 12.5 join operation in this section  we study several algorithms for computing the join of relations  and we analyze their respective costs we use the term equi-join to refer to a join of the form r  r.a = s.b s  where a and b are attributes or sets of attributes of relations r and s  respectively we use as a running example the expression  student  takes 4to be more precise  since we read each run separately and may get fewer than bb blocks when reading the end of a run  we may require an extra seek for each run we ignore this detail for simplicity thesumit67.blogspot.com 550 chapter 12 query processing using the same relation schemas that we used in chapter 2 we assume the following information about the two relations  ? number of records of student  nstudent = 5  000 ? number of blocks of student  bstudent = 100 ? number of records of takes  ntakes = 10  000 ? number of blocks of takes  btakes = 400 12.5.1 nested-loop join figure 12.5 shows a simple algorithm to compute the theta join  r   s  of two relations r and s this algorithm is called the nested-loop join algorithm  since it basically consists of a pair of nested for loops relation r is called the outer relation and relation s the inner relation of the join  since the loop for r encloses the loop for s the algorithm uses the notation tr ? ts  where tr and ts are tuples ; tr ? ts denotes the tuple constructed by concatenating the attribute values of tuples tr and ts  like the linear file-scan algorithm for selection  the nested-loop join algorithm requires no indices  and it can be used regardless of what the join condition is extending the algorithm to compute the natural join is straightforward  since the natural join can be expressed as a theta join followed by elimination of repeated attributes by a projection the only change required is an extra step of deleting repeated attributes from the tuple tr ? ts  before adding it to the result the nested-loop join algorithm is expensive  since it examines every pair of tuples in the two relations consider the cost of the nested-loop join algorithm the number of pairs of tuples to be considered is nr * ns  where nr denotes the number of tuples in r  and ns denotes the number of tuples in s for each record in r,we have to performa complete scan on s in the worst case  the buffer can hold only one block of each relation  and a total of nr * bs + br block transfers would be required  where br and bs denote the number of blocks containing tuples of r and s  respectively we need only one seek for each scan on the inner relation s since it is read sequentially  and a total of br seeks to read r  leading to a total of nr + br seeks in the best case  there is enough space for both relations to fit simultaneously in memory  so each block would have to be read only once ; hence  only br + bs block transfers would be required  along with 2 seeks for each tuple tr in r do begin for each tuple ts in s do begin test pair  tr  ts  to see if they satisfy the join condition  if they do  add tr ? ts to the result ; end end figure 12.5 nested-loop join thesumit67.blogspot.com 12.5 join operation 551 for each block br of r do begin for each block bs of s do begin for each tuple tr in br do begin for each tuple ts in bs do begin test pair  tr  ts  to see if they satisfy the join condition if they do  add tr ? ts to the result ; end end end end figure 12.6 block nested-loop join if one of the relations fits entirely in main memory  it is beneficial to use that relation as the inner relation  since the inner relation would then be read only once therefore  if s is small enough to fit in main memory  our strategy requires only a total br + bs block transfers and 2 seeks ? the same cost as that for the case where both relations fit in memory now consider the natural join of student and takes assume for now that we have no indiceswhatsoever on either relation  and that we are not willing to create any index.we can use the nested loops to compute the join ; assume that student is the outer relation and takes is the inner relation in the join.wewill have to examine 5000 * 10  000 = 50 * 106 pairs of tuples in the worst case  the number of block transfers is 5000 * 400 + 100 = 2,000,100  plus 5000 + 100 = 5100 seeks in the bestcase scenario  however  we can read both relations only once  and perform the computation this computation requires at most 100 + 400 = 500 block transfers  plus 2 seeks ? a significant improvement over the worst-case scenario if we had used takes as the relation for the outer loop and student for the inner loop  the worst-case cost of our final strategy would have been 10,000 * 100 + 400 = 1,000,400 block transfers  plus 10,400 disk seeks the number of block transfers is significantly less  and although the number of seeks is higher  the overall cost is reduced  assuming ts = 4 milliseconds and tt = 0.1 milliseconds 12.5.2 block nested-loop join if the buffer is too small to hold either relation entirely in memory  we can still obtain a major saving in block accesses if we process the relations on a per-block basis  rather than on a per-tuple basis figure 12.6 shows block nested-loop join  which is a variant of the nested-loop join where every block of the inner relation is paired with every block of the outer relation.within each pair of blocks  every tuple in one block is paired with every tuple in the other block  to generate all pairs of tuples as before  all pairs of tuples that satisfy the join condition are added to the result the primary difference in cost between the block nested-loop join and the basic nested-loop join is that  in the worst case  each block in the inner relation s is read only once for each block in the outer relation  instead of once for each tuple thesumit67.blogspot.com 552 chapter 12 query processing in the outer relation thus  in the worst case  there will be a total of br * bs + br block transfers  where br and bs denote the number of blocks containing records of r and s  respectively each scan of the inner relation requires one seek  and the scan of the outer relation requires one seek per block  leading to a total of 2 * br seeks clearly  it is more efficient to use the smaller relation as the outer relation  in case neither of the relations fits in memory in the best case  where the inner relation fits in memory  there will be br + bs block transfers and just 2 seeks  we would choose the smaller relation as the inner relation in this case   now return to our example of computing student  takes  using the block nested-loop join algorithm in the worst case  we have to read each block of takes once for each block of student thus  in the worst case  a total of 100 * 400 + 100 = 40,100 block transfers plus 2 * 100 = 200seeks are required this cost is a significant improvement over the 5000 * 400 + 100 = 2,000,100 block transfers plus 5100 seeks needed in the worst case for the basic nested-loop join the best-case cost remains the same ? namely  100 + 400 = 500 block transfers and 2 seeks the performance of the nested-loop and block nested-loop procedures can be further improved  ? if the join attributes in a natural join or an equi-join form a key on the inner relation  then for each outer relation tuple the inner loop can terminate as soon as the first match is found ? in the block nested-loop algorithm  instead of using disk blocks as the blocking unit for the outer relation  we can use the biggest size that can fit in memory  while leaving enough space for the buffers of the inner relation and the output in other words  if memory has mblocks  we read in m-2 blocks of the outer relation at a time  and when we read each block of the inner relation we join it with all the m-2 blocks of the outer relation this change reduces the number of scans of the inner relation from br to br /  m  2    where br is the number of blocks of the outer relation the total cost is then br /  m 2   * bs + br block transfers and 2 br /  m 2   seeks ? we can scan the inner loop alternately forward and backward this scanning method orders the requests for disk blocks so that the data remaining in the buffer from the previous scan can be reused  thus reducing the number of disk accesses needed ? if an index is available on the inner loop ? s join attribute  we can replace file scans with more efficient index lookups section 12.5.3 describes this optimization 12.5.3 indexed nested-loop join in a nested-loop join  figure 12.5   if an index is available on the inner loop ? s join attribute  index lookups can replace file scans for each tuple tr in the outer relation r  the index is used to look up tuples in s that will satisfy the join condition with tuple tr  thesumit67.blogspot.com 12.5 join operation 553 this join method is called an indexed nested-loop join ; it can be used with existing indices  as well as with temporary indices created for the sole purpose of evaluating the join looking up tuples in s that will satisfy the join conditions with a given tuple tr is essentially a selection on s for example  consider student  takes suppose that we have a student tuple with id ? 00128 ?  then  the relevant tuples in takes are those that satisfy the selection ? id = 00128 ?  the cost of an indexed nested-loop join can be computed as follows  for each tuple in the outer relation r  a lookup is performed on the index for s  and the relevant tuples are retrieved in the worst case  there is space in the buffer for only one block of r and one block of the index then  br i/o operations are needed to read relation r  where br denotes the number of blocks containing records of r ; each i/o requires a seek and a block transfer  since the disk head may have moved in between each i/o for each tuple in r  we perform an index lookup on s then  the time cost of the join can be computed as br  tt + ts  + nr * c  where nr is the number of records in relation r  and c is the cost of a single selection on s using the join condition we have seen in section 12.3 how to estimate the cost of a single selection algorithm  possibly using indices  ; that estimate gives us the value of c the cost formula indicates that  if indices are available on both relations r and s  it is generally most efficient to use the one with fewer tuples as the outer relation for example  consider an indexed nested-loop join of student  takes  with student as the outer relation suppose also that takes has a primary b + -tree index on the join attribute id,which contains 20 entries on average in each index node since takes has 10,000 tuples  the height of the tree is 4  and one more access is needed to find the actual data since nstudent is 5000  the total cost is 100 + 5000 * 5 = 25,100 disk accesses  each of which requires a seek and a block transfer in contrast  aswe saw before  40,100 block transfers plus 200 seeks were needed for a block nestedloop join although the number of block transfers has been reduced  the seek cost has actually increased  increasing the total cost since a seek is considerably more expensive than a block transfer however  if we had a selection on the student relation that reduces the number of rows significantly  indexed nested-loop join could be significantly faster than block nested-loop join 12.5.4 merge join the merge-join algorithm  also called the sort-merge-join algorithm  can be used to compute natural joins and equi-joins let r  r  and s  s  be the relations whose natural join is to be computed  and let r n s denote their common attributes suppose that both relations are sorted on the attributes r n s then  their join can be computed by a process much like the merge stage in the merge ? sort algorithm 12.5.4.1 merge-join algorithm figure 12.7 shows the merge-join algorithm in the algorithm  joinattrs refers to the attributes in r n s  and tr  ts  where tr and ts are tuples that have the same thesumit67.blogspot.com 554 chapter 12 query processing pr  = address of first tuple of r ; ps  = address of first tuple of s ; while  ps = null and pr = null  do begin ts  = tuple to which ps points ; ss  =  ts  ; set ps to point to next tuple of s ; done  = false ; while  not done and ps = null  do begin ts  = tuple to which ps points ; if  ts  joinattrs  = ts  joinattrs   then begin ss  = ss ?  ts  ; set ps to point to next tuple of s ; end else done  = true ; end tr  = tuple to which pr points ; while  pr = null and tr  joinattrs  < ts  joinattrs   do begin set pr to point to next tuple of r ; tr  = tuple to which pr points ; end while  pr = null and tr  joinattrs  = ts  joinattrs   do begin for each ts in ss do begin add ts  tr to result ; end set pr to point to next tuple of r ; tr  = tuple to which pr points ; end end figure 12.7 merge join values for joinattrs  denotes the concatenation of the attributes of the tuples  followed by projecting out repeated attributes the merge-join algorithm associates one pointer with each relation these pointers point initially to the first tuple of the respective relations as the algorithm proceeds  the pointers move through the relation a group of tuples of one relation with the same value on the join attributes is read into ss  the algorithm in figure 12.7 requires that every set of tuples ss fit in main memory ; we discuss extensions of the algorithm to avoid this requirement shortly then  the corresponding tuples  if any  of the other relation are read in  and are processed as they are read thesumit67.blogspot.com 12.5 join operation 555 a 3 b 1 d 8 d 13 f 7 m 5 q 6 a a b g c l d n m b a1 a2 a1 a3 pr ps r s figure 12.8 sorted relations for merge join figure 12.8 shows two relations that are sorted on their join attribute a1 it is instructive to go through the steps of the merge-join algorithm on the relations shown in the figure the merge-join algorithm of figure 12.7 requires that each set ss of all tuples with the same value for the join attributes must fit in main memory this requirement can usually be met  even if the relation s is large if there are some join attribute values for which ss is larger than available memory  a block nested-loop join can be performed for such sets ss ,matching them with corresponding blocks of tuples in r with the same values for the join attributes if either of the input relations r and s is not sorted on the join attributes  they can be sorted first  and then the merge-join algorithm can be used the merge-join algorithm can also be easily extended from natural joins to the more general case of equi-joins 12.5.4.2 cost analysis once the relations are in sorted order  tuples with the same value on the join attributes are in consecutive order thereby  each tuple in the sorted order needs to be read only once  and  as a result  each block is also read only once since it makes only a single pass through both files  assuming all sets ss fit in memory  the merge-join method is efficient ; the number of block transfers is equal to the sum of the number of blocks in both files  br + bs  assuming that bb buffer blocks are allocated to each relation  the number of disk seeks required would be br /bb  + bs/bb  disk seeks since seeks are much more expensive than data transfer  it makes sense to allocate multiple buffer blocks to each relation  provided extra memory is available for example  with tt = 0.1 milliseconds per 4-kilobyte block  and ts = 4 milliseconds  the buffer size is 400 blocks  or 1.6 megabytes   so the seek time would be 4 milliseconds for every 40 milliseconds of transfer time  in other words  seek time would be just 10 percent of the transfer time thesumit67.blogspot.com 556 chapter 12 query processing if either of the input relations r and s is not sorted on the join attributes  they must be sorted first ; the cost of sorting must then be added to the above costs if some some sets ss do not fit in memory  the cost would increase slightly suppose the merge-join scheme is applied to our example of student  takes the join attribute here is id suppose that the relations are already sorted on the join attribute id in this case  the merge join takes a total of 400 + 100 = 500 block transfers if we assume that in the worst case only one buffer block is allocated to each input relation  that is  bb = 1   a total of 400 + 100 = 500 seeks would also be required ; in reality bb can be set much higher since we need to buffer blocks for only two relations  and the seek cost would be significantly less suppose the relations are not sorted  and the memory size is the worst case  only three blocks the cost is as follows  1 using the formulae thatwe developed in section 12.4,we can see that sorting relation takes requires log3-1  400/3   = 8 merge passes sorting of relation takes then takes 400 *  2 log3-1  400/3   + 1   or 6800  block transfers  with 400 more transfers to write out the result the number of seeks required is 2 * 400/3  + 400 *  2 * 8  1  or 6268 seeks for sorting  and 400 seeks for writing the output  for a total of 6668 seeks  since only one buffer block is available for each run 2 similarly  sorting relation student takes log3-1  100/3   = 6 merge passes and 100 *  2 log3-1  100/3   + 1   or 1300  block transfers  with 100 more transfers to write it out the number of seeks required for sorting student is 2 * 100/3  + 100 *  2 * 6 1  = 1164  and 100 seeks are required for writing the output  for a total of 1264 seeks 3 finally  merging the two relations takes 400 + 100 = 500 block transfers and 500 seeks thus  the total cost is 9100 block transfers plus 8932 seeks if the relations are not sorted  and the memory size is just 3 blocks with a memory size of 25 blocks  and the relations not sorted  the cost of sorting followed by merge join would be as follows  1 sorting the relation takes can be done with just one merge step  and takes a total of just 400 *  2 log24  400/25   + 1  = 1200 block transfers similarly  sorting student takes 300 block transfers writing the sorted output to disk requires 400 + 100 = 500 block transfers  and the merge step requires 500 block transfers to read the data back adding up these costs gives a total cost of 2500 block transfers 2 if we assume that only one buffer block is allocated for each run  the number of seeks required in this case is 2 * 400/25  + 400 + 400 = 832 seeks for sorting takes andwriting the sorted output to disk  and similarly 2 * 100/25  + 100 + 100 = 208 for student  plus 400 + 100 seeks for reading the sorted data in the merge-join step adding up these costs gives a total cost of 1640 seeks thesumit67.blogspot.com 12.5 join operation 557 the number of seeks can be significantly reduced by setting aside more buffer blocks for each run for example  if 5 buffer blocks are allocated for each run and for the output from merging the 4 runs of student  the cost is reduced to 2 * 100/25  + 100/5  + 100/5  = 48 seeks  from 208 seeks if the merge-join step sets aside 12 blocks each for buffering takes and student  the number of seeks for the merge-join step goes down to 400/12  + 100/12  = 43  from 500 the total number of seeks is then 251 thus  the total cost is 2500 block transfers plus 251 seeks if the relations are not sorted  and the memory size is 25 blocks 12.5.4.3 hybridmerge join it is possible to perform a variation of the merge-join operation on unsorted tuples  if secondary indices exist on both join attributes the algorithm scans the records through the indices  resulting in their being retrieved in sorted order this variation presents a significant drawback  however  since recordsmay be scattered throughout the file blocks hence  each tuple access could involve accessing a disk block  and that is costly to avoid this cost  we can use a hybrid merge-join technique that combines indices with merge join suppose that one of the relations is sorted ; the other is unsorted  but has a secondary b + -tree index on the join attributes the hybrid merge-join algorithm merges the sorted relation with the leaf entries of the secondary b + -tree index the result file contains tuples from the sorted relation and addresses for tuples of the unsorted relation the result file is then sorted on the addresses of tuples of the unsorted relation  allowing efficient retrieval of the corresponding tuples  in physical storage order  to complete the join extensions of the technique to handle two unsorted relations are left as an exercise for you 12.5.5 hash join like the merge-join algorithm  the hash-join algorithm can be used to implement natural joins and equi-joins in the hash-join algorithm  a hash function h is used to partition tuples of both relations the basic idea is to partition the tuples of each of the relations into sets that have the same hash value on the join attributes we assume that  ? h is a hash functionmapping joinattrs values to  0  1      nh  ,where joinattrs denotes the common attributes of r and s used in the natural join ? r0  r1      rnh denote partitions of r tuples  each initially empty each tuple tr ? r is put in partition ri  where i = h  tr  joinattrs    ? s0  s1    snh denote partitions of s tuples  each initially empty each tuple ts ? s is put in partition si  where i = h  ts  joinattrs    thesumit67.blogspot.com 558 chapter 12 query processing 0 1 2 3 4 0 1 2 3 r 4 s   partitions of r partitions of s figure 12.9 hash partitioning of relations the hash function h should have the ? goodness ? properties of randomness and uniformity that we discussed in chapter 11 figure 12.9 depicts the partitioning of the relations 12.5.5.1 basics the idea behind the hash-join algorithm is this  suppose that an r tuple and an s tuple satisfy the join condition ; then  they have the same value for the join attributes if that value is hashed to some value i  the r tuple has to be in ri and the s tuple in si  therefore  r tuples in ri need only to be compared with s tuples in si ; they do not need to be compared with s tuples in any other partition for example  if d is a tuple in student  c a tuple in takes  and h a hash function on the id attributes of the tuples  then d and c must be tested only if h  c  = h  d   if h  c  = h  d   then c and d must have different values for id.however  if h  c  = h  d   we must test c and d to seewhether the values in their join attributes are the same  since it is possible that c and d have different iids that have the same hash value figure 12.10 shows the details of the hash-join algorithm to compute the natural join of relations r and s as in the merge-join algorithm  tr  ts denotes the concatenation of the attributes of tuples tr and ts  followed by projecting out repeated attributes after the partitioning of the relations  the rest of the hash-join code performs a separate indexed nested-loop join on each of the partition pairs i  for i = 0      nh to do so  it first builds a hash index on each si  and then probes  that is  looks up si  with tuples fromri the relation s is the build input  and r is the probe input the hash index on si is built in memory  so there is no need to access the disk to retrieve the tuples the hash function used to build this hash index must be different from the hash function h used earlier  but is still applied to only the join thesumit67.blogspot.com 12.5 join operation 559 / * partition s * / for each tuple ts in s do begin i  = h  ts  joinattrs   ; hsi  = hsi ?  ts  ; end / * partition r * / for each tuple tr in r do begin i  = h  tr  joinattrs   ; hri  = hri ?  tr  ; end / * perform join on each partition * / for i  = 0 to nh do begin read hsi and build an in-memory hash index on it ; for each tuple tr in hri do begin probe the hash index on hsi to locate all tuples ts such that ts  joinattrs  = tr  joinattrs  ; for each matching tuple ts in hsi do begin add tr  ts to the result ; end end end figure 12.10 hash join attributes in the course of the indexed nested-loop join  the system uses this hash index to retrieve records that match records in the probe input the build and probe phases require only a single pass through both the build and probe inputs it is straightforward to extend the hash-join algorithm to compute general equi-joins the value nh must be chosen to be large enough such that  for each i  the tuples in the partition si of the build relation  along with the hash index on the partition  fit in memory it is not necessary for the partitions of the probe relation to fit in memory clearly  it is best to use the smaller input relation as the build relation if the size of the build relation is bs blocks  then  for each of the nh partitions to be of size less than or equal to m  nh must be at least bs/m   more precisely stated  we have to account for the extra space occupied by the hash index on the partition as well  so nh should be correspondingly larger for simplicity  we sometimes ignore the space requirement of the hash index in our analysis 12.5.5.2 recursive partitioning if the value of nh is greater than or equal to the number of blocks of memory  the relations can not be partitioned in one pass  since there will not be enough buffer blocks instead  partitioning has to be done in repeated passes in one pass  the input can be split into at most as many partitions as there are blocks available for use as output buffers each bucket generated by one pass is separately read in and thesumit67.blogspot.com 560 chapter 12 query processing partitioned again in the next pass  to create smaller partitions the hash function used in a pass is  of course  different from the one used in the previous pass the system repeats this splitting of the input until each partition of the build input fits in memory such partitioning is called recursive partitioning a relation does not need recursive partitioning if m > nh + 1  or equivalently m >  bs/m  + 1  which simplifies  approximately  to m > v bs  for example  consider a memory size of 12 megabytes  divided into 4-kilobyte blocks ; it would contain a total of 3k  3072  blocks we can use a memory of this size to partition relations of size up to 3k * 3k blocks  which is 36 gigabytes similarly  a relation of size 1 gigabyte requires just over v 256k blocks  or 2 megabytes  to avoid recursive partitioning 12.5.5.3 handling of overflows hash-table overflow occurs in partition i of the build relation s if the hash index on si is larger than main memory hash-table overflow can occur if there are many tuples in the build relation with the same values for the join attributes  or if the hash function does not have the properties of randomness and uniformity in either case  some of the partitions will have more tuples than the average  whereas others will have fewer ; partitioning is then said to be skewed we can handle a small amount of skew by increasing the number of partitions so that the expected size of each partition  including the hash index on the partition  is somewhat less than the size of memory the number of partitions is therefore increased by a small value  called the fudge factor  that is usually about 20 percent of the number of hash partitions computed as described in section 12.5.5 even if  by using a fudge factor  we are conservative on the sizes of the partitions  overflows can still occur hash-table overflows can be handled by either overflow resolution or overflow avoidance overflow resolution is performed during the build phase  if a hash-index overflow is detected overflow resolution proceeds in this way  if si  for any i  is found to be too large  it is further partitioned into smaller partitions by using a different hash function similarly  ri is also partitioned using the new hash function  and only tuples in the matching partitions need to be joined in contrast  overflow avoidance performs the partitioning carefully  so that overflows never occur during the build phase in overflow avoidance  the build relation s is initially partitioned into many small partitions  and then some partitions are combined in such a way that each combined partition fits in memory the probe relation r is partitioned in the same way as the combined partitions on s  but the sizes of ri do not matter if a large number of tuples in s have the same value for the join attributes  the resolution and avoidance techniques may fail on some partitions in that case  instead of creating an in-memory hash index and using a nested-loop join to join the partitions  we can use other join techniques  such as block nested-loop join  on those partitions thesumit67.blogspot.com 12.5 join operation 561 12.5.5.4 cost of hash join we now consider the cost of a hash join our analysis assumes that there is no hash-table overflow first  consider the case where recursive partitioning is not required ? the partitioning of the two relations r and s calls for a complete reading of both relations  and a subsequent writing back of them this operation requires 2  br + bs  block transfers  where br and bs denote the number of blocks containing records of relations r and s  respectively the build and probe phases read each of the partitions once  calling for further br + bs block transfers the number of blocks occupied by partitions could be slightly more than br + bs  as a result of partially filled blocks accessing such partially filled blocks can add an overhead of at most 2nh for each of the relations  since each of the nh partitions could have a partially filled block that has to be written and read back thus  a hash join is estimated to require  3  br + bs  + 4nh block transfers the overhead 4nh is usually quite small compared to br + bs  and can be ignored ? assuming bb blocks are allocated for the input buffer and each output buffer  partitioning requires a total of 2  br /bb  + bs/bb   seeks the build and probe phases require only one seek for each of the nh partitions of each relation  since each partition can be read sequentially the hash join thus requires 2  br /bb  + bs/bb   + 2nh seeks now consider the case where recursive partitioning is required each pass reduces the size of each of the partitions by an expected factor of m  1 ; and passes are repeated until each partition is of size at most mblocks the expected number of passes required for partitioning s is therefore logm-1  bs   1   ? since  in each pass  every block of s is read in and written out  the total block transfers for partitioning of s is 2bs logm-1  bs   1   the number of passes for partitioning of r is the same as the number of passes for partitioning of s  therefore the join is estimated to require  2  br + bs  logm-1  bs   1  + br + bs block transfers ? again assuming bb blocks are allocated for buffering each partition  and ignoring the relatively small number of seeks during the build and probe phase  hash join with recursive partitioning requires  2  br /bb  + bs/bb   logm-1  bs   1  disk seeks thesumit67.blogspot.com 562 chapter 12 query processing consider  for example  the natural join takes  student with a memory size of 20 blocks  the student relation can be partitioned into five partitions  each of size 20 blocks  which size will fit into memory only one pass is required for the partitioning the relation takes is similarly partitioned into five partitions  each of size 80 ignoring the cost of writing partially filled blocks  the cost is 3  100 + 400  = 1500 block transfers there is enough memory to allocate 3 buffers for the input and each of the 5 outputs during partitioning  leading to 2  100/3  + 400/3   = 336 seeks the hash join can be improved if the main-memory size is large when the entire build input can be kept in main memory  nh can be set to 0 ; then  the hashjoin algorithm executes quickly,without partitioning the relations into temporary files  regardless of the probe input ? s size the cost estimate goes down to br + bs block transfers and two seeks 12.5.5.5 hybrid hash join the hybrid hash-join algorithmperforms another optimization ; it is usefulwhen memory sizes are relatively large  but not all of the build relation fits in memory the partitioning phase of the hash-join algorithm needs a minimum of one block of memory as a buffer for each partition that is created  and one block of memory as an input buffer to reduce the impact of seeks  a larger number of blockswould be used as a buffer ; let bb denote the number of blocks used as a buffer for the input and for each partition hence  a total of  nh + 1  * bb blocks of memory are needed for partitioning the two relations if memory is larger than  nh + 1  * bb  we can use the rest ofmemory  m  nh + 1  * bb blocks  to buffer the first partition of the build input  that is  s0   so that it will not need to be written out and read back in further  the hash function is designed in such a way that the hash index on s0 fits in m  nh + 1  * bb blocks  in order that  at the end of partitioning of s  s0 is completely in memory and a hash index can be built on s0 when the system partitions r it again does not write tuples in r0 to disk ; instead  as it generates them  the system uses them to probe the memory-resident hash index on s0  and to generate output tuples of the join after they are used for probing  the tuples can be discarded  so the partition r0 does not occupy any memory space thus  a write and a read access have been saved for each block of both r0 and s0 the system writes out tuples in the other partitions as usual  and joins them later the savings of hybrid hash join can be significant if the build input is only slightly bigger than memory if the size of the build relation is bs  nh is approximately equal to bs/m thus  hybrid hash join is most useful if m > >  bs/m  * bb  or m > > v bs * bb  where the notation > > denotes much larger than for example  suppose the block size is 4 kilobytes  the build relation size is 5 gigabytes  and bb is 20 then  the hybrid hash-join algorithm is useful if the size of memory is significantly more than 20 megabytes ; memory sizes of gigabytes or more are common on computers today if we devote 1 gigabyte for the join algorithm  s0 would be nearly 1 gigabyte  and hybrid hash join would be nearly 20 percent cheaper than hash join thesumit67.blogspot.com 12.6 other operations 563 12.5.6 complex joins nested-loop and block nested-loop joins can be used regardless of the join conditions the other join techniques are more efficient than the nested-loop join and its variants  but can handle only simple join conditions  such as natural joins or equi-joins we can implement joins with complex join conditions  such as conjunctions and disjunctions  by using the efficient join techniques  if we apply the techniques developed in section 12.3.3 for handling complex selections consider the following join with a conjunctive condition  r   1 ?  2 ? ? ? ? ?  n s one or more of the join techniques described earlier may be applicable for joins on the individual conditions r   1 s  r   2 s  r   3 s  and so on we can compute the overall join by first computing the result of one of these simpler joins r   i s ; each pair of tuples in the intermediate result consists of one tuple fromr and one from s the result of the complete join consists of those tuples in the intermediate result that satisfy the remaining conditions   1 ? ? ? ? ?  i-1 ?  i + 1 ? ? ? ? ?  n these conditions can be tested as tuples in r   i s are being generated a join whose condition is disjunctive can be computed in this way consider  r   1 ?  2 ? ? ? ? ?  n s the join can be computed as the union of the records in individual joins r   i s   r   1 s  ?  r   2 s  ? ? ? ? ?  r   n s  section 12.6 describes algorithms for computing the union of relations 12.6 other operations other relational operations and extended relational operations ? such as duplicate elimination  projection  set operations  outer join  and aggregation ? can be implemented as outlined in sections 12.6.1 through 12.6.5 12.6.1 duplicate elimination we can implement duplicate elimination easily by sorting identical tuples will appear adjacent to each other as a result of sorting  and all but one copy can be removed.with external sort ? merge  duplicates foundwhile a run is being created can be removed before the run is written to disk  thereby reducing the number of block transfers the remaining duplicates can be eliminated during merging  and thesumit67.blogspot.com 564 chapter 12 query processing the final sorted run has no duplicates the worst-case cost estimate for duplicate elimination is the same as the worst-case cost estimate for sorting of the relation we can also implement duplicate elimination by hashing  as in the hash-join algorithm first  the relation is partitioned on the basis of a hash function on the whole tuple then  each partition is read in  and an in-memory hash index is constructed while constructing the hash index  a tuple is inserted only if it is not already present otherwise  the tuple is discarded after all tuples in the partition have been processed  the tuples in the hash index are written to the result the cost estimate is the same as that for the cost of processing  partitioning and reading each partition  of the build relation in a hash join because of the relatively high cost of duplicate elimination  sql requires an explicit request by the user to remove duplicates ; otherwise  the duplicates are retained 12.6.2 projection we can implement projection easily by performing projection on each tuple  which gives a relation that could have duplicate records  and then removing duplicate records duplicates can be eliminated by the methods described in section 12.6.1 if the attributes in the projection list include a key of the relation  no duplicates will exist ; hence  duplicate elimination is not required.generalized projection can be implemented in the same way as projection 12.6.3 set operations we can implement the union  intersection  and set-difference operations by first sorting both relations  and then scanning once through each of the sorted relations to produce the result in r ? s  when a concurrent scan of both relations reveals the same tuple in both files  only one of the tuples is retained the result of r n s will contain only those tuples that appear in both relations we implement set difference  r  s  similarly  by retaining tuples in r only if they are absent in s for all these operations  only one scan of the two sorted input relations is required  so the cost is br + bs block transfers if the relations are sorted in the same order assuming a worst case of one block buffer for each relation  a total of br + bs disk seeks would be required in addition to br + bs block transfers the number of seeks can be reduced by allocating extra buffer blocks if the relations are not sorted initially  the cost of sorting has to be included any sort order can be used in evaluation of set operations  provided that both inputs have that same sort order hashing provides another way to implement these set operations the first step in each case is to partition the two relations by the same hash function  and thereby create the partitions r0  r1      rnh and s0  s1      snh  depending on the operation  the system then takes these steps on each partition i = 0  1      nh  ? r ? s 1 build an in-memory hash index on ri  thesumit67.blogspot.com 12.6 other operations 565 2 add the tuples in si to the hash index only if they are not already present 3 add the tuples in the hash index to the result ? r n s 1 build an in-memory hash index on ri  2 for each tuple in si  probe the hash index and output the tuple to the result only if it is already present in the hash index ? r  s 1 build an in-memory hash index on ri  2 for each tuple in si  probe the hash index  and  if the tuple is present in the hash index  delete it from the hash index 3 add the tuples remaining in the hash index to the result 12.6.4 outer join recall the outer-join operations described in section 4.1.2 for example  the natural left outer join takes  student contains the join of takes and student  and  in addition  for each takes tuple t that has no matching tuple in student  that is  where id is not in student   the following tuple t1 is added to the result for all attributes in the schema of takes  tuple t1 has the same values as tuple t the remaining attributes  from the schema of student  of tuple t1 contain the value null we can implement the outer-join operations by using one of two strategies  1 compute the corresponding join  and then add further tuples to the join result to get the outer-join result consider the left outer-join operation and two relations  r  r  and s  s   to evaluate r   s  we first compute r   s  and save that result as temporary relation q1 next  we compute r   r  q1  to obtain those tuples in r that do not participate in the theta join.we can use any of the algorithms for computing the joins  projection  and set difference described earlier to compute the outer joins we pad each of these tuples with null values for attributes from s  and add it to q1 to get the result of the outer join the right outer-join operation r   s is equivalent to s   r  and can therefore be implemented in a symmetric fashion to the left outer join we can implement the full outer-join operation r   s by computing the join r  s  and then adding the extra tuples of both the left and right outer-join operations  as before 2 modify the join algorithms it is easy to extend the nested-loop join algorithms to compute the left outer join  tuples in the outer relation that do not match any tuple in the inner relation are written to the output after being padded with null values however  it is hard to extend the nested-loop join to compute the full outer join thesumit67.blogspot.com 566 chapter 12 query processing natural outer joins and outer joins with an equi-join condition can be computed by extensions of the merge-join and hash-join algorithms merge join can be extended to compute the full outer join as follows  when the merge of the two relations is being done  tuples in either relation that do not match any tuple in the other relation can be padded with nulls and written to the output similarly  we can extend merge join to compute the left and right outer joins by writing out nonmatching tuples  padded with nulls  from only one of the relations since the relations are sorted  it is easy to detect whether or not a tuple matches any tuples from the other relation for example  when a merge join of takes and student is done  the tuples are read in sorted order of id  and it is easy to check  for each tuple  whether there is a matching tuple in the other the cost estimates for implementing outer joins using the merge-join algorithm are the same as are those for the corresponding join the only difference lies in size of the result  and therefore in the block transfers for writing it out  which we did not count in our earlier cost estimates the extension of the hash-join algorithm to compute outer joins is left for you to do as an exercise  exercise 12.15   12.6.5 aggregation recall the aggregation function  operator   discussed in section 3.7 for example  the function select dept name  avg  salary  from instructor group by dept name ; computes the average salary in each university department the aggregation operation can be implemented in the same way as duplicate elimination.we use either sorting or hashing  just as we did for duplicate elimination  but based on the grouping attributes  branch name in the preceding example   however  instead of eliminating tuples with the same value for the grouping attribute  we gather them into groups  and apply the aggregation operations on each group to get the result the cost estimate for implementing the aggregation operation is the same as the cost of duplicate elimination  for aggregate functions such as min  max  sum  count  and avg instead of gathering all the tuples in a group and then applying the aggregation operations  we can implement the aggregation operations sum  min  max  count  and avg on the fly as the groups are being constructed for the case of sum  min  and max  when two tuples in the same group are found  the system replaces them by a single tuple containing the sum  min  or max  respectively  of the columns being aggregated for the count operation  it maintains a running count for each group for which a tuple has been found finally,we implement the thesumit67.blogspot.com 12.7 evaluation of expressions 567 avg operation by computing the sum and the count values on the fly  and finally dividing the sum by the count to get the average if all tuples of the result fit in memory  both the sort-based and the hash-based implementations do not need to write any tuples to disk as the tuples are read in  they can be inserted in a sorted tree structure or in a hash index when we use on-the-fly aggregation techniques  only one tuple needs to be stored for each of the groups hence  the sorted tree structure or hash index fits in memory  and the aggregation can be processed with just br block transfers  and 1 seek  instead of the 3br transfers  and a worst case of up to 2br seeks  that would be required otherwise 12.7 evaluation of expressions so far  we have studied how individual relational operations are carried out now we consider how to evaluate an expression containing multiple operations the obvious way to evaluate an expression is simply to evaluate one operation at a time  in an appropriate order the result of each evaluation is materialized in a temporary relation for subsequent use a disadvantage to this approach is the need to construct the temporary relations  which  unless they are small  must be written to disk an alternative approach is to evaluate several operations simultaneously in a pipeline  with the results of one operation passed on to the next  without the need to store a temporary relation in sections 12.7.1 and 12.7.2  we consider both the materialization approach and the pipelining approach we shall see that the costs of these approaches can differ substantially  but also that there are cases where only the materialization approach is feasible 12.7.1 materialization it is easiest to understand intuitively how to evaluate an expression by looking at a pictorial representation of the expression in an operator tree consider the expression   name   building = ? watson ?  department   instructor  in figure 12.11 if we apply the materialization approach  we start from the lowest-level operations in the expression  at the bottom of the tree   in our example  there is only one such operation  the selection operation on department the inputs to the lowest-level operations are relations in the database.we execute these operations by the algorithms that we studied earlier  and we store the results in temporary relations we can use these temporary relations to execute the operations at the next level up in the tree  where the inputs now are either temporary relations or relations stored in the database in our example  the inputs to the join are the instructor relation and the temporary relation created by the selection on department the join can now be evaluated  creating another temporary relation thesumit67.blogspot.com 568 chapter 12 query processing ? s name building = ? watson ? department instructor figure 12.11 pictorial representation of an expression by repeating the process,wewill eventually evaluate the operation at the root of the tree  giving the final result of the expression in our example  we get the final result by executing the projection operation at the root of the tree  using as input the temporary relation created by the join evaluation as just described is called materialized evaluation  since the results of each intermediate operation are created  materialized  and then are used for evaluation of the next-level operations the cost of a materialized evaluation is not simply the sum of the costs of the operations involved when we computed the cost estimates of algorithms  we ignored the cost of writing the result of the operation to disk to compute the cost of evaluating an expression as done here  we have to add the costs of all the operations  as well as the cost of writing the intermediate results to disk we assume that the records of the result accumulate in a buffer  and  when the buffer is full  they are written to disk the number of blocks written out  br  can be estimated as nr / fr  where nr is the estimated number of tuples in the result relation r  and fr is the blocking factor of the result relation  that is  the number of records of r that will fit in a block in addition to the transfer time  some disk seeks may be required  since the disk head may have moved between successive writes the number of seeks can be estimated as br /bb  where bb is the size of the output buffer  measured in blocks   double buffering  using two buffers  with one continuing execution of the algorithm while the other is being written out  allows the algorithm to execute more quickly by performing cpu activity in parallelwith i/o activity the number of seeks can be reduced by allocating extra blocks to the output buffer  and writing out multiple blocks at once 12.7.2 pipelining we can improve query-evaluation efficiency by reducing the number of temporary files that are produced we achieve this reduction by combining several relational operations into a pipeline of operations  in which the results of one opthesumit67 blogspot.com 12.7 evaluation of expressions 569 eration are passed along to the next operation in the pipeline evaluation as just described is called pipelined evaluation for example  consider the expression   a1,a2  r  s    if materialization were applied  evaluationwould involve creating a temporary relation to hold the result of the join  and then reading back in the result to perform the projection these operations can be combined  when the join operation generates a tuple of its result  it passes that tuple immediately to the project operation for processing by combining the join and the projection  we avoid creating the intermediate result  and instead create the final result directly creating a pipeline of operations can provide two benefits  1 it eliminates the cost of reading and writing temporary relations  reducing the cost of query evaluation 2 it can start generating query results quickly  if the root operator of a queryevaluation plan is combined in a pipeline with its inputs this can be quite useful if the results are displayed to a user as they are generated  since otherwise there may be a long delay before the user sees any query results 12.7.2.1 implementation of pipelining we can implement a pipeline by constructing a single  complex operation that combines the operations that constitute the pipeline although this approach may be feasible for some frequently occurring situations  it is desirable in general to reuse the code for individual operations in the construction of a pipeline in the example of figure 12.11  all three operations can be placed in a pipeline  which passes the results of the selection to the join as they are generated in turn  it passes the results of the join to the projection as they are generated the memory requirements are low  since results of an operation are not stored for long however  as a result of pipelining  the inputs to the operations are not available all at once for processing pipelines can be executed in either of two ways  1 in a demand-driven pipeline  the system makes repeated requests for tuples from the operation at the top of the pipeline each time that an operation receives a request for tuples  it computes the next tuple  or tuples  to be returned  and then returns that tuple if the inputs of the operation are not pipelined  the next tuple  s  to be returned can be computed from the input relations  while the system keeps track of what has been returned so far if it has some pipelined inputs  the operation also makes requests for tuples from its pipelined inputs using the tuples received from its pipelined inputs  the operation computes tuples for its output  and passes them up to its parent 2 in a producer-driven pipeline  operations do not wait for requests to produce tuples  but instead generate the tuples eagerly each operation in a producer-driven pipeline is modeled as a separate process or thread within thesumit67.blogspot.com 570 chapter 12 query processing the system that takes a stream of tuples from its pipelined inputs and generates a stream of tuples for its output we describe below how demand-driven and producer-driven pipelines can be implemented each operation in a demand-driven pipeline can be implemented as an iterator that provides the following functions  open    next    and close    after a call to open    each call to next   returns the next output tuple of the operation the implementation of the operation in turn calls open   and next   on its inputs  to get its input tuples when required the function close   tells an iterator that no more tuples are required the iterator maintains the state of its execution in between calls  so that successive next   requests receive successive result tuples for example  for an iterator implementing the select operation using linear search  the open   operation starts a file scan  and the iterator ? s state records the point towhich the file has been scanned when the next   function is called  the file scan continues from after the previous point ; when the next tuple satisfying the selection is found by scanning the file  the tuple is returned after storing the point where it was found in the iterator state a merge-join iterator ? s open   operation would open its inputs  and if they are not already sorted  it would also sort the inputs on calls to next    it would return the next pair of matching tuples the state information would consist of up to where each input had been scanned details of the implementation of iterators are left for you to complete in practice exercise 12.7 producer-driven pipelines  on the other hand  are implemented in a different manner for each pair of adjacent operations in a producer-driven pipeline  the system creates a buffer to hold tuples being passed from one operation to the next the processes or threads corresponding to different operations execute concurrently each operation at the bottom of a pipeline continually generates output tuples  and puts them in its output buffer  until the buffer is full an operation at any other level of a pipeline generates output tuples when it gets input tuples from lower down in the pipeline  until its output buffer is full once the operation uses a tuple from a pipelined input  it removes the tuple from its input buffer in either case  once the output buffer is full  the operation waits until its parent operation removes tuples from the buffer  so that the buffer has space for more tuples.at this point  the operation generates more tuples  until the buffer is full again the operation repeats this process until all the output tuples have been generated it is necessary for the system to switch between operations only when an output buffer is full  or an input buffer is empty and more input tuples are needed to generate any more output tuples in a parallel-processing system  operations in a pipeline may be run concurrently on distinct processors  see chapter 18   using producer-driven pipelining can be thought of as pushing data up an operation tree from below  whereas using demand-driven pipelining can be thought of as pulling data up an operation tree from the top whereas tuples are generated eagerly in producer-driven pipelining  they are generated lazily  on demand  in demand-driven pipelining demand-driven pipelining is used thesumit67.blogspot.com 12.7 evaluation of expressions 571 more commonly than producer-driven pipelining  because it is easier to implement however  producer-driven pipelining is very useful in parallel processing systems 12.7.2.2 evaluation algorithms for pipelining some operations  such as sorting  are inherently blocking operations  that is  they may not be able to output any results until all tuples from their inputs have been examined.5 other operations  such as join  are not inherently blocking  but specific evaluation algorithms may be blocking for example  the hash-join algorithm is a blocking operation  since it requires both its inputs to be fully retrieved and partitioned  before it outputs any tuples on the other hand  the indexed nested loops join algorithm can output result tuples as it gets tuples for the outer relation it is therefore said to be pipelined on its outer  left-hand side  relation  although it is blocking on its indexed  right-hand side  input  since the index must be fully constructed before the indexed nested-loop join algorithm can execute hybrid hash join can be viewed as partially pipelined on the probe relation  since it can output tuples from the first partition as tuples are received for the probe relation however  tuples that are not in the first partition will be output only after the entire pipelined input relation is received hybrid hash join thus provides pipelined evaluation on its probe input if the build input fits entirely in memory  or nearly pipelined evaluation if most of the build input fits in memory if both inputs are sorted on the join attribute  and the join condition is an equi-join  merge join can be used  with both its inputs pipelined however  in the more common case that the two inputs that we desire to pipeline into the join are not already sorted  another alternative is the doublepipelined join technique  shown in figure 12.12 the algorithm assumes that the input tuples for both input relations  r and s  are pipelined tuplesmade available for both relations are queued for processing in a single queue special queue entries  called endr and ends  which serve as end-of-file markers  are inserted in the queue after all tuples from r and s  respectively  have been generated for efficient evaluation  appropriate indices should be built on the relations r and s as tuples are added to r and s  the indices must be kept up to date when hash indices are used on r and s  the resultant algorithm is called the double-pipelined hash-join technique the double-pipelined join algorithm in figure 12.12 assumes that both inputs fit in memory in case the two inputs are larger than memory  it is still possible to use the double-pipelined join technique as usual until available memory is full when available memory becomes full  r and s tuples that have arrived up to that point can be treated as being in partition r0 and s0  respectively tuples for r and s that arrive subsequently are assigned to partitions r1 and s1  respectively  which 5blocking operations such as sorting may be able to output tuples early if the input is known to satisfy some special properties such as being sorted  or partially sorted  already however  in the absence of such information  blocking operations can not output tuples early thesumit67.blogspot.com 572 chapter 12 query processing doner  = false ; dones  = false ; r  = ? ; s  = ? ; result  = ? ; while not doner or not dones do begin if queue is empty  then wait until queue is not empty ; t  = top entry in queue ; if t = endr then doner  = true else if t = ends then dones  = true else if t is from input r then begin r  = r ?  t  ; result  = result ?   t   s  ; end else / * t is from input s * / begin s  = s ?  t  ; result  = result ?  r   t   ; end end figure 12.12 double-pipelined join algorithm are written to disk  and are not added to the in-memory index however  tuples assigned to r1 and s1 are used to probe s0 and r0  respectively  before they are written to disk thus  the join of r1 with s0  and s0 with r1  is also carried out in a pipelined fashion after r and s have been fully processed  the join of r1 tuples with s1 tuples must be carried out  to complete the join ; any of the join techniques we have seen earlier can be used to join r1 with s1 12.8 summary ? the first action that the system must perform on a query is to translate the query into its internal form,which  for relational database systems  is usually based on the relational algebra in the process of generating the internal form of the query  the parser checks the syntax of the user ? s query  verifies that the relation names appearing in the query are names of relations in the database  and so on if the query was expressed in terms of a view  the parser replaces all references to the view name with the relational-algebra expression to compute the view ? given a query  there are generally a variety of methods for computing the answer it is the responsibility of the query optimizer to transform the query thesumit67.blogspot.com review terms 573 as entered by the user into an equivalent query that can be computed more efficiently chapter 13 covers query optimization ? we can process simple selection operations by performing a linear scan  or by making use of indices we can handle complex selections by computing unions and intersections of the results of simple selections ? we can sort relations larger than memory by the external sort ? merge algorithm ? queries involving a natural joinmaybe processed in severalways  depending on the availability of indices and the form of physical storage for the relations ? if the join result is almost as large as the cartesian product of the two relations  a block nested-loop join strategy may be advantageous ? if indices are available  the indexed nested-loop join can be used ? if the relations are sorted  a merge join may be desirable it may be advantageous to sort a relation prior to join computation  so as to allow use of the merge-join strategy   ? the hash-join algorithm partitions the relations into several pieces  such that each piece of one of the relations fits in memory the partitioning is carried out with a hash function on the join attributes  so that corresponding pairs of partitions can be joined independently ? duplicate elimination  projection  set operations  union  intersection  and difference   and aggregation can be done by sorting or by hashing ? outer-join operations can be implemented by simple extensions of join algorithms ? hashing and sorting are dual  in the sense that any operation such as duplicate elimination  projection  aggregation  join  and outer join that can be implemented by hashing can also be implemented by sorting  and vice versa ; that is  any operation that can be implemented by sorting can also be implemented by hashing ? an expression can be evaluated by means of materialization  where the system computes the result of each subexpression and stores it on disk  and then uses it to compute the result of the parent expression ? pipelining helps to avoid writing the results of many subexpressions to disk  by using the results in the parent expression even as they are being generated review terms ? query processing ? evaluation primitive ? query-execution plan ? query-evaluation plan ? query-execution engine ? measures of query cost thesumit67.blogspot.com 574 chapter 12 query processing ? sequential i/o ? random i/o ? file scan ? linear search ? selections using indices ? access paths ? index scans ? conjunctive selection ? disjunctive selection ? composite index ? intersection of identifiers ? external sorting ? external sort ? merge ? runs ? n-way merge ? equi-join ? nested-loop join ? block nested-loop join ? indexed nested-loop join ? merge join ? sort-merge join ? hybrid merge join ? hash join ? build ? probe ? build input ? probe input ? recursive partitioning ? hash-table overflow ? skew ? fudge factor ? overflow resolution ? overflow avoidance ? hybrid hash join ? operator tree ? materialized evaluation ? double buffering ? pipelined evaluation ? demand-driven pipeline  lazy  pulling  ? producer-driven pipeline  eager  pushing  ? iterator ? double-pipelined join practice exercises 12.1 assume  for simplicity in this exercise  that only one tuple fits in a block and memory holds at most 3 blocks show the runs created on each pass of the sort-merge algorithm  when applied to sort the following tuples on the first attribute   kangaroo  17    wallaby  21    emu  1    wombat  13    platypus  3    lion  8    warthog  4    zebra  11    meerkat  6    hyena  9    hornbill  2    baboon  12   12.2 consider the bank database of figure 12.13  where the primary keys are underlined  and the following sql query  select t.branch name from branch t  branch s where t.assets > s.assets and s.branch city = ? brooklyn ? thesumit67.blogspot.com practice exercises 575 write an efficient relational-algebra expression that is equivalent to this query justify your choice 12.3 let relations r1  a  b,c  and r2  c  d  e  have the following properties  r1 has 20,000 tuples  r2 has 45,000 tuples  25 tuples of r1 fit on one block  and 30 tuples of r2 fit on one block estimate the number of block transfers and seeks required  using each of the following join strategies for r1  r2  a nested-loop join b block nested-loop join c merge join d hash join 12.4 the indexed nested-loop join algorithmdescribed in section 12.5.3 can be inefficient if the index is a secondary index  and there are multiple tuples with the same value for the join attributes.why is it inefficient ? describe a way  using sorting  to reduce the cost of retrieving tuples of the inner relation under what conditions would this algorithm be more efficient than hybrid merge join ? 12.5 let r and s be relations with no indices  and assume that the relations are not sorted assuming infinite memory  what is the lowest-cost way  in terms of i/o operations  to compute r  s ? what is the amount of memory required for this algorithm ? 12.6 consider the bank database of figure 12.13  where the primary keys are underlined suppose that a b + -tree index on branch city is available on relation branch  and that no other index is available list different ways to handle the following selections that involve negation  a  ?  branch city < ? brooklyn ?   branch  b  ?  branch city = ? brooklyn ?   branch  c  ?  branch city < ? brooklyn ? ? assets < 5000   branch  12.7 write pseudocode for an iterator that implements indexed nested-loop join  where the outer relation is pipelined your pseudocode must define branch  branch name  branch city  assets  customer  customer name  customer street  customer city  loan  loan number  branch name  amount  borrower  customer name  loan number  account  account number  branch name  balance  depositor  customer name  account number  figure 12.13 banking database thesumit67.blogspot.com 576 chapter 12 query processing the standard iterator functions open    next    and close    show what state information the iterator must maintain between calls 12.8 design sort-based and hash-based algorithms for computing the relational division operation  see practise exercises of chapter 6 for a definition of the division operation   12.9 what is the effect on the cost of merging runs if the number of buffer blocks per run is increased  while keeping overall memory available for buffering runs fixed ? exercises 12.10 suppose you need to sort a relation of 40 gigabytes,with 4 kilobyte blocks  using a memory size of 40 megabytes suppose the cost of a seek is 5 milliseconds  while the disk transfer rate is 40 megabytes per second a find the cost of sorting the relation  in seconds  with bb = 1 andwith bb = 100 b in each case  how many merge passes are required ? c suppose a flash storage device is used instead of a disk  and it has a seek time of 1 microsecond  and a transfer rate of 40 megabytes per second recompute the cost of sorting the relation  in seconds  with bb = 1 andwith bb = 100  in this setting 12.11 consider the following extended relational-algebra operators describe how to implement each operation using sorting  and using hashing a semijoin      r   s is defined as  r  r   s   where r is the set of attributes in the schema of r ; that it it selects those tuples ri in r for which there is a tuple s j in s such that ri and s j satisfy predicate   b anti-semijoin   ?    r  ?  s is defined as r  r  r   s  ; that it it selects those tuples ri in r for which there is no tuple s j in s such that ri and s j satisfy predicate   12.12 why is it not desirable to force users to make an explicit choice of a queryprocessing strategy ? are there cases in which it is desirable for users to be aware of the costs of competing query-processing strategies ? explain your answer 12.13 design a variant of the hybrid merge-join algorithm for the case where both relations are not physically sorted  but both have a sorted secondary index on the join attributes 12.14 estimate the number of block transfers and seeks required by your solution to exercise 12.13 for r1  r2  where r1 and r2 are as defined in practice exercise 12.3 thesumit67.blogspot.com bibliographical notes 577 12.15 the hash-join algorithm as described in section 12.5.5 computes the natural join of two relations describe how to extend the hash-join algorithm to compute the natural left outer join  the natural right outer join and the natural full outer join  hint  keep extra information with each tuple in the hash index  to detect whether any tuple in the probe relation matches the tuple in the hash index  try out your algorithm on the takes and student relations 12.16 pipelining is used to avoid writing intermediate results to disk suppose you need to sort relation r using sort ? merge and merge-join the result with an already sorted relation s a describe how the output of the sort of r can be pipelined to the merge join without being written back to disk b the same idea is applicable even if both inputs to the merge join are the outputs of sort ? merge operations however  the available memory has to be shared between the two merge operations  the merge-join algorithm itself needs very little memory   what is the effect of having to share memory on the cost of each sort ? merge operation ? 12.17 write pseudocode for an iterator that implements a version of the sort ? merge algorithm where the result of the final merge is pipelined to its consumers your pseudocode must define the standard iterator functions open    next    and close    show what state information the iterator must maintain between calls 12.18 suppose you have to compute agsum  c   r  aswell as a,bgsum  c   r   describe how to compute these together using a single sorting of r  bibliographical notes a query processor must parse statements in the query language  and must translate them into an internal form parsing of query languages differs little fromparsing of traditional programming languages most compiler texts cover the main parsing techniques  and present optimization from a programming-language point of view graefe and mckenna  1993b  presents an excellent survey of query-evaluation techniques knuth  1973  presents an excellent description of external sorting algorithms  including an optimization called replacement selection,which can create initial runs that are  on the average  twice the size of memory nyberg et al  1995  shows that due to poor processor-cache behavior  replacement selection performsworse than in-memory quicksort for run generation  negating the benefits of generating longer runs nyberg et al  1995  presents an efficient external sorting algorithm that takes processor cache effects into account query evaluation algorithms that thesumit67.blogspot.com 578 chapter 12 query processing take cache effects into account have been extensively studied ; see  for example  harizopoulos and ailamaki  2004   according to performance studies conducted in the mid-1970s  database systems of that period used only nested-loop join and merge join these studies  including blasgen and eswaran  1976   which was related to the development of system r  determined that either the nested-loop join or merge join nearly always provided the optimal join method hence  these two were the only join algorithms implemented in system r however  blasgen and eswaran  1976  did not include an analysis of hash-join algorithms today  hash joins are considered to be highly efficient and widely used hash-join algorithms were initially developed for parallel database systems hybrid hash join is described in shapiro  1986   zeller and gray  1990  and davison and graefe  1994  describe hash-join techniques that can adapt to the available memory  which is important in systems where multiple queries may be running at the same time graefe et al  1998  describes the use of hash joins and hash teams  which allow pipelining of hash joins by using the same partitioning for all hash joins in a pipeline sequence  in the microsoft sql server thesumit67.blogspot.com chapter13 query optimization query optimization is the process of selecting the most efficient query-evaluation plan from among the many strategies usually possible for processing a given query  especially if the query is complex we do not expect users to write their queries so that they can be processed efficiently rather  we expect the system to construct a query-evaluation plan that minimizes the cost of query evaluation this is where query optimization comes into play one aspect of optimization occurs at the relational-algebra level  where the system attempts to find an expression that is equivalent to the given expression  but more efficient to execute another aspect is selecting a detailed strategy for processing the query  such as choosing the algorithm to use for executing an operation  choosing the specific indices to use  and so on the difference in cost  in terms of evaluation time  between a good strategy and a bad strategy is often substantial  and may be several orders of magnitude hence  it is worthwhile for the system to spend a substantial amount of time on the selection of a good strategy for processing a query  even if the query is executed only once 13.1 overview consider the following relational-algebra expression  for the query ? find the names of all instructors in the music department together with the course title of all the courses that the instructors teach ?  name,title   dept name = ? music ?  instructor   teaches   course id ,title  course     note that the projection of course on  course id,title  is required since course shares an attribute dept name with instructor ; ifwe did not remove this attribute using the projection  the above expression using natural joins would return only courses from the music department  even if some music department instructors taught courses in other departments the above expression constructs a large intermediate relation  instructor  teaches   course id ,title  course   however  we are interested in only a few tuples 579 thesumit67.blogspot.com 580 chapter 13 query optimization name  title dept_name = music course_id  title instructor teaches instructor teaches course course ? ? ? ? name  title course_id  title s sdept_name = music  a  initial expression tree  b  transformed expression tree figure 13.1 equivalent expressions of this relation  those pertaining to instructors in the music department   and in only two of the ten attributes of this relation since we are concerned with only those tuples in the instructor relation that pertain to the music department  we do not need to consider those tuples that do not have dept name = ? music ?  by reducing the number of tuples of the instructor relation that we need to access  we reduce the size of the intermediate result our query is now represented by the relational-algebra expression   name,title    dept name = ? music ?  instructor     teaches   course id ,title  course    which is equivalent to our original algebra expression  but which generates smaller intermediate relations figure 13.1 depicts the initial and transformed expressions an evaluation plan defines exactly what algorithm should be used for each operation  and how the execution of the operations should be coordinated figure 13.2 illustrates one possible evaluation plan for the expression from figure 13.1  b   as we have seen  several different algorithms can be used for each relational operation  giving rise to alternative evaluation plans in the figure  hash join has been chosen for one of the join operations  while the other uses merge join  after sorting the relations on the join attribute  which is id where edges are marked as pipelined  the output of the producer is pipelined directly to the consumer  without being written out to disk given a relational-algebra expression  it is the job of the query optimizer to come up with a query-evaluation plan that computes the same result as the given expression  and is the least-costly way of generating the result  or  at least  is not much costlier than the least-costly way   to find the least-costly query-evaluation plan  the optimizer needs to generate alternative plans that produce the same result as the given expression  and to choose the least-costly one generation of query-evaluation plans involves three steps   1  generating expressions that are logically equivalent to the given exthesumit67 blogspot.com 13.1 overview 581 ? name  title instructor sdept_name = music sortid teaches  sort to remove duplicates   hash join   merge join  pipeline  use index 1  pipeline course ? course_id  title sortid pipeline pipeline figure 13.2 an evaluation plan pression   2  annotating the resultant expressions in alternative ways to generate alternative query-evaluation plans  and  3  estimating the cost of each evaluation plan  and choosing the one whose estimated cost is the least steps  1    2   and  3  are interleaved in the query optimizer ? some expressions are generated  and annotated to generate evaluation plans  then further expressions are generated and annotated  and so on as evaluation plans are generated  their costs are estimated by using statistical information about the relations  such as relation sizes and index depths to implement the first step  the query optimizer must generate expressions equivalent to a given expression it does so bymeans of equivalence rules that specify how to transform an expression into a logically equivalent one we describe these rules in section 13.2 in section 13.3 we describe how to estimate statistics of the results of each operation in a query plan using these statistics with the cost formulae in chapter 12 allows us to estimate the costs of individual operations the individual costs are combined to determine the estimated cost of evaluating a given relational-algebra expression  as outlined earlier in section 12.7 in section 13.4  we describe how to choose a query-evaluation plan we can choose one based on the estimated cost of the plans since the cost is an estimate  the selected plan is not necessarily the least-costly plan ; however  as long as the estimates are good  the plan is likely to be the least-costly one  or not much more costly than it finally  materialized views help to speed up processing of certain queries in section 13.5  we study how to ? maintain ? materialized views ? that is  to keep them up-to-date ? and how to perform query optimization with materialized views thesumit67.blogspot.com 582 chapter 13 query optimization viewing query evaluation plans most database systems provide a way to view the evaluation plan chosen to execute a given query it is usually best to use the guiprovided with the database systemto view evaluation plans however  if you use a command line interface  many databases support variations of a command ? explain < query > ?  which displays the execution plan chosen for the specified query < query >  the exact syntax varies with different databases  ? postgresql uses the syntax shown above ? oracle uses the syntax explain plan for however  the command stores the resultant plan in a table called plan table  instead of displaying it the query ? select * from table  dbms xplan.display  ; ? displays the stored plan ? db2 follows a similar approach to oracle  but requires the program db2exfmt to be executed to display the stored plan ? sql server requires the command set showplan text on to be executed before submitting the query ; then  when a query is submitted  instead of executing the query  the evaluation plan is displayed the estimated costs for the plan are also displayed along with the plan it is worth noting that the costs are usually not in any externally meaningful unit  such as seconds or i/o operations  but rather in units ofwhatever costmodel the optimizer uses some optimizers such as postgresql display two cost-estimate numbers ; the first indicates the estimated cost for outputting the first result  and the second indicates the estimated cost for outputting all results 13.2 transformation of relational expressions a query can be expressed in several different ways  with different costs of evaluation in this section  rather than take the relational expression as given  we consider alternative  equivalent expressions two relational-algebra expressions are said to be equivalent if  on every legal database instance  the two expressions generate the same set of tuples  recall that a legal database instance is one that satisfies all the integrity constraints specified in the database schema  note that the order of the tuples is irrelevant ; the two expressions may generate the tuples in different orders  but would be considered equivalent as long as the set of tuples is the same in sql  the inputs and outputs are multisets of tuples  and the multiset version of the relational algebra  described in the box in page 238  is used for evaluating sql queries two expressions in the multiset version of the relational algebra are said to be equivalent if on every legal database the two expressions generate the same multiset of tuples the discussion in this chapter is based on the relational thesumit67.blogspot.com 13.2 transformation of relational expressions 583 ? e1 e2 ? e2 e1 rule 5 e3 e1 e2 e2 e3 e1 rule 6.a rule 7.a if only has attributes from e1 e1 e2 e1 e2 s ? s ? ? figure 13.3 pictorial representation of equivalences algebra we leave extensions to the multiset version of the relational algebra to you as exercises 13.2.1 equivalence rules an equivalence rule says that expressions of two forms are equivalent we can replace an expression of the first form by an expression of the second form  or vice versa ? that is  we can replace an expression of the second form by an expression of the first form ? since the two expressions generate the same result on any valid database the optimizer uses equivalence rules to transform expressions into other logically equivalent expressions we now list a number of general equivalence rules on relational-algebra expressions some of the equivalences listed appear in figure 13.3.we use    1   2  and so on to denote predicates  l1  l2  l3  and so on to denote lists of attributes  and e  e1  e2  and so on to denote relational-algebra expressions a relation name r is simply a special case of a relational-algebra expression  and can be used wherever e appears 1 conjunctive selection operations can be deconstructed into a sequence of individual selections this transformation is referred to as a cascade of     1 ?  2  e  =   1    2  e   thesumit67.blogspot.com 584 chapter 13 query optimization 2 selection operations are commutative   1    2  e   =   2    1  e   3 only the final operations in a sequence of projection operations are needed ; the others can be omitted this transformation can also be referred to as a cascade of    l1   l2       ln  e        =  l1  e  4 selections can be combined with cartesian products and theta joins a    e1 ? e2  = e1   e2 this expression is just the definition of the theta join b   1  e1   2 e2  = e1   1 ?  2 e2 5 theta-join operations are commutative e1   e2 = e2   e1 actually  the order of attributes differs between the left-hand side and righthand side  so the equivalence does not hold if the order of attributes is taken into account a projection operation can be added to one of the sides of the equivalence to appropriately reorder attributes  but for simplicity we omit the projection and ignore the attribute order in most of our examples recall that the natural-join operator is simply a special case of the theta-join operator ; hence  natural joins are also commutative 6 a natural-join operations are associative  e1  e2   e3 = e1   e2  e3  b theta joins are associative in the following manner   e1   1 e2    2 ?  3 e3 = e1   1 ?  3  e2   2 e3  where  2 involves attributes from only e2 and e3 any of these conditions may be empty ; hence  it follows that the cartesian product  ?  operation is also associative the commutativity and associativity of join operations are important for join reordering in query optimization 7 the selection operation distributes over the theta-join operation under the following two conditions  a it distributes when all the attributes in selection condition  0 involve only the attributes of one of the expressions  say  e1  being joined   0  e1   e2  =    0  e1     e2 thesumit67.blogspot.com 13.2 transformation of relational expressions 585 b it distributes when selection condition  1 involves only the attributes of e1 and  2 involves only the attributes of e2   1 ?  2  e1   e2  =    1  e1        2  e2   8 the projection operation distributes over the theta-join operation under the following conditions a let l1 and l2 be attributes of e1 and e2  respectively suppose that the join condition  involves only attributes in l1 ? l2 then   l1 ? l2  e1   e2  =   l1  e1       l2  e2   b consider a join e1   e2 let l1 and l2 be sets of attributes from e1 and e2  respectively let l3 be attributes of e1 that are involved in join condition   but are not in l1 ? l2  and let l4 be attributes of e2 that are involved in join condition   but are not in l1 ? l2 then   l1 ? l2  e1   e2  =  l1 ? l2    l1 ? l3  e1       l2 ? l4  e2    9 the set operations union and intersection are commutative e1 ? e2 = e2 ? e1 e1 n e2 = e2 n e1 set difference is not commutative 10 set union and intersection are associative  e1 ? e2  ? e3 = e1 ?  e2 ? e3   e1 n e2  n e3 = e1 n  e2 n e3  11 the selection operation distributes over the union  intersection  and setdifference operations  p  e1  e2  =  p  e1    p  e2  similarly  the preceding equivalence  with  replaced with either ? or n  also holds further   p  e1  e2  =  p  e1   e2 the preceding equivalence  with  replaced by n  also holds  but does not hold if  is replaced by ?  12 the projection operation distributes over the union operation  l  e1 ? e2  =   l  e1   ?   l  e2   thesumit67.blogspot.com 586 chapter 13 query optimization this is only a partial list of equivalences more equivalences involving extended relational operators  such as the outer join and aggregation  are discussed in the exercises 13.2.2 examples of transformations we now illustrate the use of the equivalence rules.we use our university example with the relation schemas  instructor  id  name  dept name  salary  teaches  id  course id  sec id  semester  year  course  course id  title  dept name  credits  in our example in section 13.1  the expression   name,title   dept name = ? music ?  instructor   teaches   course id ,title  course     was transformed into the following expression   name,title    dept name = ? music ?  instructor     teaches   course id ,title  course    which is equivalent to our original algebra expression  but generates smaller intermediate relations we can carry out this transformation by using rule 7.a remember that the rule merely says that the two expressions are equivalent ; it does not say that one is better than the other multiple equivalence rules can be used  one after the other  on a query or on parts of the query as an illustration  suppose that we modify our original query to restrict attention to instructors who have taught a course in 2009 the new relational-algebra query is   name,title   dept name = ? music ? ? year = 2009  instructor   teaches   course id ,title  course     we can not apply the selection predicate directly to the instructor relation  since the predicate involves attributes of both the instructor and teaches relations however  we can first apply rule 6.a  associativity of natural join  to transform the join instructor   teaches   course id ,title  course   into  instructor  teaches    course id ,title  course    name,title   dept name = ? music ? ? year = 2009   instructor  teaches    course id ,title  course    then  using rule 7.a  we can rewrite our query as  thesumit67.blogspot.com 13.2 transformation of relational expressions 587  name,title    dept name = ? music ? ? year = 2009  instructor  teaches     course id ,title  course   let us examine the selection subexpression within this expression using rule 1  we can break the selection into two selections  to get the following subexpression   dept name = ? music ?   year = 2009  instructor  teaches   both of the preceding expressions select tuples with dept name = ? music ? and course id = 2009 however  the latter form of the expression provides a new opportunity to apply rule 7.a  ? perform selections early ?   resulting in the subexpression   dept name = ? music ?  instructor    year = 2009  teaches  figure 13.4 depicts the initial expression and the final expression after all these transformations we could equally well have used rule 7.b to get the final expression directly,without using rule 1 to break the selection into two selections in fact  rule 7.b can itself be derived from rules 1 and 7.a a set of equivalence rules is said to be minimal if no rule can be derived from any combination of the others the preceding example illustrates that the set of equivalence rules in section 13.2.1 is not minimal an expression equivalent to the original expression may be generated in different ways ; the number of different ways of generating an expression increases when we use a nonminimal set of equivalence rules query optimizers therefore use minimal sets of equivalence rules now consider the following form of our example query  name  title name  title course_id  title ? ? dept_name = music ? year = 2009 s instructor teaches course course_id  title ? ? sdept_name = music syear = 2009 instructor teaches course  a  initial expression tree  b  tree after multiple transformations figure 13.4 multiple transformations thesumit67.blogspot.com 588 chapter 13 query optimization  name,title    dept name = ? music ?  instructor   teaches    course id ,title  course   when we compute the subexpression    dept name = ? music ?  instructor   teaches  we obtain a relation whose schema is   id  name  dept name  salary  course id  sec id  semester  year  we can eliminate several attributes from the schema by pushing projections based on equivalence rules 8.a and 8.b the only attributes that we must retain are those that either appear in the result of the query or are needed to process subsequent operations.by eliminatingunneeded attributes,we reduce the number of columns of the intermediate result thus  we reduce the size of the intermediate result in our example  the only attributes we need from the join of instructor and teaches are name and course id therefore  we can modify the expression to   name,title    name,course id    dept name = ? music ?  instructor    teaches    course id ,title  course   the projection  name,course id reduces the size of the intermediate join results 13.2.3 join ordering agood ordering of join operations is important for reducing the size of temporary results ; hence  most query optimizers pay a lot of attention to the join order as mentioned in chapter 6 and in equivalence rule 6.a  the natural-join operation is associative thus  for all relations r1  r2  and r3   r1  r2   r3 = r1   r2  r3  although these expressions are equivalent  the costs of computing them may differ consider again the expression   name,title    dept name = ? music ?  instructor    teaches   course id ,title  course   we could choose to compute teaches   course id ,title  course  first  and then to join the result with   dept name = ? music ?  instructor  however  teaches   course id ,title  course  is likely to be a large relation  since it contains one tuple for every course taught in contrast   dept name = ? music ?  instructor   teaches thesumit67.blogspot.com 13.2 transformation of relational expressions 589 is probably a small relation to see that it is  we note that  since a university has a large number of departments  it is likely that only a small fraction of the university instructors are associated with the music department thus  the preceding expression results in one tuple for each course taught by an instructor in the music department therefore  the temporary relation that we must store is smaller than itwould have been hadwe computed teaches   course id ,title  course  first there are other options to consider for evaluating our query we do not care about the order in which attributes appear in a join  since it is easy to change the order before displaying the result thus  for all relations r1 and r2  r1  r2 = r2  r1 that is  natural join is commutative  equivalence rule 5   using the associativity and commutativity of the natural join  rules 5 and 6   consider the following relational-algebra expression   instructor   course id ,title  course    teaches note that there are no attributes in common between  course id ,title  course  and instructor  so the join is just a cartesian product if there are a tuples in instructor and b tuples in  course id ,title  course   this cartesian product generates a * b tuples  one for every possible pair of instructor tuple and course  without regard for whether the instructor taught the course   this cartesian product would produce a very large temporary relation however  if the user had entered the preceding expression  we could use the associativity and commutativity of the natural join to transform this expression to the more efficient expression   instructor  teaches    course id ,title  course  13.2.4 enumeration of equivalent expressions query optimizers use equivalence rules to systematically generate expressions equivalent to the given query expression conceptually  this can be done as outlined in figure 13.5 the process proceeds as follows given a query expression e  the set of equivalent expressions eq initially contains only e now  each expression in eq is matched with each equivalence rule if an expression  say ei  of any subexpression ei of ei  which could  as a special case  be ei itself  matches one side of an equivalence rule  the optimizer generates a new expression where ei is transformed to match the other side of the rule the resultant expression is added to eq this process continues until no more new expressions can be generated the preceding process is extremely costly both in space and in time  but optimizers can greatly reduce both the space and time cost  using two key ideas 1 if we generate an expression e  from an expression e1 by using an equivalence rule on subexpression ei  then e  and e1 have identical subexpressions thesumit67.blogspot.com 590 chapter 13 query optimization procedure genallequivalent  e  eq =  e  repeat match each expression ei in eq with each equivalence rule rj if any subexpression ei of ei matches one side of rj create a new expression e  which is identical to ei  except that ei is transformed to match the other side of rj add e  to eq if it is not already present in eq until no new expression can be added to eq figure 13.5 procedure to generate all equivalent expressions except for ei and its transformation even ei and its transformed version usually share many identical subexpressions expression-representation techniques that allow both expressions to point to shared subexpressions can reduce the space requirement significantly 2 it is not always necessary to generate every expression that can be generated with the equivalence rules if an optimizer takes cost estimates of evaluation into account  it may be able to avoid examining some of the expressions  as we shall see in section 13.4.we can reduce the time required for optimization by using techniques such as these we revisit these issues in section 13.4.2 13.3 estimating statistics of expression results the cost of an operation depends on the size and other statistics of its inputs given an expression such as a   b  c  to estimate the cost of joining a with  b  c   we need to have estimates of statistics such as the size of b  c in this section  we first list some statistics about database relations that are stored in database-system catalogs  and then show how to use the statistics to estimate statistics on the results of various relational operations one thing that will become clear later in this section is that the estimates are not very accurate  since they are based on assumptions that may not hold exactly a query-evaluation plan that has the lowest estimated execution cost may therefore not actually have the lowest actual execution cost however  realworld experience has shown that even if estimates are not precise  the plans with the lowest estimated costs usually have actual execution costs that are either the lowest actual execution costs  or are close to the lowest actual execution costs 13.3.1 catalog information the database-system catalog stores the following statistical information about database relations  thesumit67.blogspot.com 13.3 estimating statistics of expression results 591 ? nr  the number of tuples in the relation r ? br  the number of blocks containing tuples of relation r  ? lr  the size of a tuple of relation r in bytes ? fr  the blocking factor of relation r ? that is  the number of tuples of relation r that fit into one block ? v  a  r   the number of distinct values that appear in the relation r for attribute a this value is the same as the size of  a  r   if ais a key for relation r  v  a  r  is nr  the last statistic  v  a  r   can also be maintained for sets of attributes  if desired  instead of just for individual attributes thus  given a set of attributes  a  v  a  r  is the size of  a  r   if we assume that the tuples of relation r are stored together physically in a file  the following equation holds  br =  nr fr  statistics about indices  such as the heights of b + -tree indices and number of leaf pages in the indices  are also maintained in the catalog if we wish to maintain accurate statistics  then  every time a relation is modified  we must also update the statistics this update incurs a substantial amount of overhead therefore  most systems do not update the statistics on every modification instead  they update the statistics during periods of light system load as a result  the statistics used for choosing a query-processing strategy may not be completely accurate however  if not too many updates occur in the intervals between the updates of the statistics  the statistics will be sufficiently accurate to provide a good estimation of the relative costs of the different plans the statistical information noted here is simplified real-world optimizers often maintain further statistical information to improve the accuracy of their cost estimates of evaluation plans for instance  most databases store the distribution of values for each attribute as a histogram  in a histogram the values for the attribute are divided into a number of ranges  and with each range the histogram associates the number of tupleswhose attribute value lies in that range figure 13.6 shows an example of a histogram for an integer-valued attribute that takes values in the range 1 to 25 histograms used in database systems usually record the number of distinct values in each range  in addition to the number of tuples with attribute values in that range as an example of a histogram  the range of values for an attribute age of a relation person could be divided into 0 ? 9  10 ? 19      90 ? 99  assuming amaximum age of 99  .with each range we store a count of the number of person tuples whose age values lie in that range  and the number of distinct age values that lie in that thesumit67.blogspot.com 592 chapter 13 query optimization value frequency 50 40 30 20 10 1 ? 5 6 ? 10 11 ? 15 16 ? 20 21 ? 25 figure 13.6 example of histogram range.without such histogram information  an optimizer would have to assume that the distribution of values is uniform ; that is  each range has the same count a histogram takes up only a little space  so histograms on several different attributes can be stored in the system catalog there are several types of histograms used in database systems for example  an equi-width histogram divides the range of values into equal-sized ranges  whereas an equi-depth histogram adjusts the boundaries of the ranges such that each range has the same number of values 13.3.2 selection size estimation the size estimate of the result of a selection operation depends on the selection predicate.we first consider a single equality predicate  then a single comparison predicate  and finally combinations of predicates ?  a = a  r   if we assume uniform distribution of values  that is  each value appears with equal probability   the selection result can be estimated to have nr /v  a  r  tuples  assuming that the value a appears in attribute a of some record of r  the assumption that the value a in the selection appears in some record is generally true  and cost estimates often make it implicitly.however  it is often not realistic to assume that each value appears with equal probability the course id attribute in the takes relation is an example where the assumption is not valid it is reasonable to expect that a popular undergraduate course will have many more students than a smaller specialized graduate course therefore  certain course id values appear with greater probability than do others despite the fact that the uniform-distribution assumption is often not correct  it is a reasonable approximation of reality in many cases  and it helps us to keep our presentation relatively simple if a histogram is available on attribute a  we can locate the range that contains the value a  and modify the above-mentioned estimate nr /v  a  r  thesumit67.blogspot.com 13.3 estimating statistics of expression results 593 computing and maintaining statistics conceptually  statistics on relations can be thought of as materialized views  which should be automatically maintained when relations are modified unfortunately  keeping statistics up-to-date on every insert  delete or update to the database can be very expensive on the other hand  optimizers generally do not need exact statistics  an error of a few percent may result in a plan that is not quite optimal being chosen  but the alternative plan chosen is likely to have a cost which is within a few percent of the optimal cost thus  it is acceptable to have statistics that are approximate database systems reduce the cost of generating and maintaining statistics  as outlined below  by exploiting the fact that statistics can be approximate ? statistics are often computed from a sample of the underlying data  instead of examining the entire collection of data for example  a fairly accurate histogram can be computed from a sample of a few thousand tuples  even on a relation that has millions  or hundreds ofmillions of records however  the sample used must be a random sample ; a sample that is not random may have an excessive representation of one part of the relation  and can give misleading results for example  if we used a sample of instructors to compute a histogram on salaries  if the sample has an overrepresentation of lower-paid instructors the histogram would result in wrong estimates database systems today routinely use random sampling to create statistics see the bibliographic notes for references on sampling ? statistics are not maintained on every update to the database in fact  some database systems never update statistics automatically they rely on database administrators periodically running a command to update statistics oracle and postgresql provide an sql command called analyze that generates statistics on specified relations  or on all relations ibm db2 supports an equivalent command called runstats see the system manuals for details you should be aware that optimizers sometimes choose very bad plans due to incorrect statistics many database systems  such as ibm db2  oracle  and sql server  update statistics automatically at certain points of time for example  the system can keep approximate track of how many tuples there are in a relation and recompute statistics if this number changes significantly another approach is to compare estimated cardinalities of a relation scan with actual cardinalities when a query is executed  and if they differ significantly  initiate an update of statistics for that relation by using the frequency count for that range instead of nr  and the number of distinct values that occurs in that range instead of v  a  r   ?  a = v  r   consider a selection of the form  a = v  r   if the actual value used in the comparison  v  is available at the time of cost estimation  a more thesumit67.blogspot.com 594 chapter 13 query optimization accurate estimate can bemade the lowest and highest values  min  a  r  and max  a  r   for the attribute can be stored in the catalog assuming that values are uniformly distributed  we can estimate the number of records that will satisfy the condition a = v as 0 if v < min  a  r   as nr if v = max  a  r   and  nr ? v  min  a  r  max  a  r   min  a  r  otherwise if a histogram is available on attribute a  we can get a more accurate estimate ; we leave the details as an exercise for you in some cases  such as when the query is part of a stored procedure  the value v may not be available when the query is optimized in such cases  we assume that approximately one-half the records will satisfy the comparison condition that is,we assume the result has nr /2 tuples ; the estimate may be very inaccurate  but is the best we can do without any further information ? complex selections  ? conjunction  a conjunctive selection is a selection of the form    1 ?  2 ? ? ? ? ?  n  r  we can estimate the result size of such a selection  for each  i  we estimate the size of the selection   i  r   denoted by si  as described previously thus  the probability that a tuple in the relation satisfies selection condition  i is si/nr  the preceding probability is called the selectivity of the selection   i  r   assuming that the conditions are independent of each other  the probability that a tuple satisfies all the conditions is simply the product of all these probabilities thus  we estimate the number of tuples in the full selection as  nr * s1 * s2 * ? ? ? * sn nn r ? disjunction  a disjunctive selection is a selection of the form    1 ?  2 ? ? ? ? ?  n  r  a disjunctive condition is satisfied by the union of all records satisfying the individual  simple conditions  i  as before  let si/nr denote the probability that a tuple satisfies condition  i  the probability that the tuplewill satisfy the disjunction is then 1 minus the probability that it will satisfy none of the conditions  1   1  s1 nr  *  1  s2 nr  * ? ? ? *  1  sn nr  multiplying this value by nr gives us the estimated number of tuples that satisfy the selection thesumit67.blogspot.com 13.3 estimating statistics of expression results 595 ? negation  in the absence of nulls  the result of a selection  ?   r  is simply the tuples of r that are not in    r   we already know how to estimate the number of tuples in    r   the number of tuples in  ?   r  is therefore estimated to be n  r  minus the estimated number of tuples in    r   we can account for nulls by estimating the number of tuples for which the condition  would evaluate to unknown  and subtracting that number from the above estimate  ignoring nulls estimating that number would require extra statistics to be maintained in the catalog 13.3.3 join size estimation in this section  we see how to estimate the size of the result of a join the cartesian product r ? s contains nr * ns tuples each tuple of r ? s occupies lr + ls bytes  from which we can calculate the size of the cartesian product estimating the size of a natural join is somewhat more complicated than estimating the size of a selection or of a cartesian product let r  r  and s  s  be relations ? if r n s = ? ? that is  the relations have no attribute in common ? then r  s is the same as r ? s  and we can use our estimation technique for cartesian products ? if r n s is a key for r  then we know that a tuple of s will join with at most one tuple from r  therefore  the number of tuples in r  s is no greater than the number of tuples in s the case where r n s is a key for s is symmetric to the case just described if rn s forms a foreign key of s  referencing r  the number of tuples in r  s is exactly the same as the number of tuples in s ? the most difficult case is when r n s is a key for neither r nor s in this case  we assume  as we did for selections  that each value appears with equal probability consider a tuple t of r  and assume r n s =  a   we estimate that tuple t produces  ns v  a  s  tuples in r  s  since this number is the average number of tuples in s with a given value for the attributes a considering all the tuples in r  we estimate that there are  nr * ns v  a  s  tuples in r  s.observe that  ifwe reverse the roles of r and s in the preceding estimate  we obtain an estimate of  nr * ns v  a  r  thesumit67.blogspot.com 596 chapter 13 query optimization tuples in r  s these two estimates differ if v  a  r   = v  a  s   if this situation occurs  there are likely to be dangling tuples that do not participate in the join thus  the lower of the two estimates is probably the more accurate one the preceding estimate of join size may be too high if the v  a  r  values for attribute a in r have few values in common with the v  a  s  values for attribute a in s however  this situation is unlikely to happen in the real world  since dangling tuples either do not exist or constitute only a small fraction of the tuples  in most real-world relations more important  the preceding estimate depends on the assumption that each value appears with equal probability more sophisticated techniques for size estimation have to be used if this assumption does not hold for example  if we have histograms on the join attributes of both relations  and both histograms have the same ranges  then we can use the above estimation technique within each range  using the number of rows with values in the range instead of nr or ns  and the number of distinct values in that range  instead of v  a  r  or v  a  s   we then add up the size estimates obtained for each range to get the overall size estimate we leave the case where both relations have histograms on the join attribute  but the histograms have different ranges  as an exercise for you we can estimate the size of a theta join r   s by rewriting the join as    r ? s   and using the size estimates for cartesian products along with the size estimates for selections  which we saw in section 13.3.2 to illustrate all these ways of estimating join sizes  consider the expression  student  takes assume the following catalog information about the two relations  ? nstudent = 5000 ? fstudent = 50  which implies that bstudent = 5000/50 = 100 ? ntakes = 10000 ? ftakes = 25  which implies that btakes = 10000/25 = 400 ? v  id  takes  = 2500  which implies that only half the students have taken any course  this is unrealistic  but we use it to show that our size estimates are correct even in this case   and on average  each student who has taken a course has taken four courses the attribute id in takes is a foreign key on student  and null values do not occur in takes.id  since id is part of the primary key of takes ; thus  the sizeof student  takes is exactly ntakes  which is 10000 we now compute the size estimates for student  takes without using information about foreign keys since v  id  takes  = 2500 and v  id  student  = 5000  the two estimates we get are 5000 * 10000/2500 = 20000 and 5000 * 10000/5000 = 10000  and we choose the lower one in this case  the lower of these estimates is the same as thatwhich we computed earlier frominformation about foreign keys thesumit67.blogspot.com 13.3 estimating statistics of expression results 597 13.3.4 size estimation for other operations we outline belowhowto estimate the sizes of the results of other relational-algebra operations ? projection  the estimated size  number of records or number of tuples  of a projection of the form  a  r  is v  a  r   since projection eliminates duplicates ? aggregation  the size of agf  r  is simply v  a  r   since there is one tuple in agf  r  for each distinct value of a ? set operations  if the two inputs to a set operation are selections on the same relation  we can rewrite the set operation as disjunctions  conjunctions  or negations for example    1  r  ?   2  r  can be rewritten as   1 ?  2  r   similarly  we can rewrite intersections as conjunctions  and we can rewrite set difference by using negation  so long as the two relations participating in the set operations are selections on the same relation we can then use the estimates for selections involving conjunctions  disjunctions  and negation in section 13.3.2 if the inputs are not selections on the same relation  we estimate the sizes this way  the estimated size of r ? s is the sum of the sizes of r and s the estimated size of r n s is the minimum of the sizes of r and s the estimated size of r  s is the same size as r  all three estimates may be inaccurate  but provide upper bounds on the sizes ? outer join  the estimated size of r  s is the size of r  s plus the size of r ; that of r  s is symmetric  while that of r  s is the size of r  s plus the sizes of r and s all three estimates may be inaccurate  but provide upper bounds on the sizes 13.3.5 estimation of number of distinct values for selections  the number of distinct values of an attribute  or set of attributes  ain the result of a selection  v  a     r    can be estimated in these ways  ? if the selection condition  forces ato take on a specified value  e.g  a = 3   v  a     r   = 1 ? if  forces a to take on one of a specified set of values  e.g   a = 1 ? a = 3 ? a = 4    then v  a     r   is set to the number of specified values ? if the selection condition  is of the form a op v  where op is a comparison operator  v  a     r   is estimated to be v  a  r  * s  where s is the selectivity of the selection ? in all other cases of selections  we assume that the distribution of avalues is independent of the distribution of the values on which selection conditions are specified  and we use an approximate estimate of min  v  a  r   n    r    a more accurate estimate can be derived for this case using probability theory  but the above approximation works fairly well thesumit67.blogspot.com 598 chapter 13 query optimization for joins  the number of distinct values of an attribute  or set of attributes  a in the result of a join  v  a  r  s   can be estimated in these ways  ? if all attributes in aare from r  v  a  r  s  is estimated as min  v  a  r   nr  s   and similarly if all attributes in a are from s  v  a  r  s  is estimated to be min  v  a  s   nr  s   ? if acontains attributes a1 fromr and a2 froms  then v  a  r  s  is estimated as  min  v  a1  r  * v  a2  a1  s   v  a1  a2  r  * v  a2  s   nr  s  note that some attributes may be in a1 as well as in a2  and a1  a2 and a2-a1 denote  respectively  attributes in athat are only fromr and attributes in a that are only from s again  more accurate estimates can be derived by using probability theory  but the above approximations work fairly well the estimates of distinct values are straightforward for projections  they are the same in  a  r  as in r  the same holds for grouping attributes of aggregation for results of sum  count  and average  we can assume  for simplicity  that all aggregate values are distinct for min  a  and max  a   the number of distinct values can be estimated as min  v  a  r   v  g  r    where g denotes the grouping attributes.we omit details of estimating distinct values for other operations 13.4 choice of evaluation plans generation of expressions is only part of the query-optimization process  since each operation in the expression can be implemented with different algorithms an evaluation plan defines exactly what algorithm should be used for each operation  and how the execution of the operations should be coordinated given an evaluation plan  we can estimate its cost using statistics estimated by the techniques in section 13.3 coupled with cost estimates for various algorithms and evaluation methods described in chapter 12 a cost-based optimizer explores the space of all query-evaluation plans that are equivalent to the given query  and chooses the one with the least estimated cost we have seen how equivalence rules can be used to generate equivalent plans however  cost-based optimizationwith arbitrary equivalence rules is fairly complicated we first cover a simpler version of cost-based optimization  which involves only join-order and join algorithm selection  in section 13.4.1 later in section 13.4.2 we briefly sketch how a general-purpose optimizer based on equivalence rules can be built  without going into details exploring the space of all possible plans may be too expensive for complex queries most optimizers include heuristics to reduce the cost of query optimization  at the potential risk of not finding the optimal plan we study some such heuristics in section 13.4.3 thesumit67.blogspot.com 13.4 choice of evaluation plans 599 13.4.1 cost-based join order selection the most common type of query in sql consists of a join of a few relations  with join predicates and selections specified in the where clause in this section we consider the problem of choosing the optimal join order for such a query for a complex join query  the number of different query plans that are equivalent to the query can be large as an illustration  consider the expression  r1  r2  ? ? ?  rn where the joins are expressed without any ordering with n = 3  there are 12 different join orderings  r1   r2  r3  r1   r3  r2   r2  r3   r1  r3  r2   r1 r2   r1  r3  r2   r3  r1   r1  r3   r2  r3  r1   r2 r3   r1  r2  r3   r2  r1   r1  r2   r3  r2  r1   r3 in general  with n relations  there are  2  n  1   ! /  n  1  ! different join orders  we leave the computation of this expression for you to do in exercise 13.10  for joins involving small numbers of relations  this number is acceptable ; for example  with n = 5  the number is 1680 however  as n increases  this number rises quickly with n = 7  the number is 665,280 ; with n = 10  the number is greater than 17.6 billion ! luckily  it is not necessary to generate all the expressions equivalent to a given expression for example  suppose we want to find the best join order of the form   r1  r2  r3   r4  r5 which represents all join orders where r1  r2  andr3 are joinedfirst  in some order   and the result is joined  in some order  with r4 and r5 there are 12 different join orders for computing r1  r2  r3  and 12 orders for computing the join of this resultwith r4 and r5 thus  there appear to be 144 join orders to examine.however  once we have found the best join order for the subset of relations  r1  r2  r3   we can use that order for further joins with r4 and r5  and can ignore all costlier join orders of r1  r2  r3 thus  instead of 144 choices to examine,we need to examine only 12 + 12 choices using this idea  we can develop a dynamic-programming algorithm for finding optimal join orders dynamic-programming algorithms store results of computations and reuse them  a procedure that can reduce execution time greatly a recursive procedure implementing the dynamic-programming algorithm appears in figure 13.7 the procedure applies selections on individual relations at the earliest possible point  that is  when the relations are accessed it is easiest to understand the procedure assuming that all joins are natural joins  although the procedure works unchanged with any join condition.with arbitrary join conditions  the join of two subexpressions is understood to include all join conditions that relate attributes from the two subexpressions thesumit67.blogspot.com 600 chapter 13 query optimization procedure findbestplan  s  if  bestplan  s  .cost = 8  / * bestplan  s  already computed * / return bestplan  s  if  s contains only 1 relation  set bestplan  s   plan and bestplan  s  .cost based on best way of accessing s else for each non-empty subset s1 of s such that s1 = s p1 = findbestplan  s1  p2 = findbestplan  s  s1  a = best algorithm for joining results of p1 and p2 cost = p1.cost + p2.cost + cost of a if cost < bestplan  s  .cost bestplan  s  .cost = cost bestplan  s   plan = ? execute p1 plan ; execute p2 plan ; join results of p1 and p2 using a ? return bestplan  s  figure 13.7 dynamic-programming algorithm for join order optimization the procedure stores the evaluation plans it computes in an associative array bestplan  which is indexed by sets of relations each element of the associative array contains two components  the cost of the best plan of s  and the plan itself the value of bestplan  s  .cost is assumed to be initialized to8if bestplan  s  has not yet been computed the procedure first checks if the best plan for computing the join of the given set of relations s has been computed already  and stored in the associative array bestplan  ; if so  it returns the already computed plan if s contains only one relation  the best way of accessing s  taking selections on s  if any  into account  is recorded in bestplan this may involve using an index to identify tuples  and then fetching the tuples  often referred to as an index scan   or scanning the entire relation  often referred to as a relation scan  .1 if there is any selection condition on s  other than those ensured by an index scan  a selection operation is added to the plan  to ensure all selections on s are satisfied otherwise  if s contains more than one relation  the procedure tries every way of dividing s into two disjoint subsets for each division  the procedure recursively finds the best plans for each of the two subsets  and then computes the cost of the overall plan by using that division.2 the procedure picks the cheapest plan from among all the alternatives for dividing s into two sets the cheapest plan and its cost are stored in the array bestplan  and returned by the 1if an index contains all the attributes of a relation that are used in a query  it is possible to perform an index-only scan  which retrieves the required attribute values from the index  without fetching actual tuples 2note that an indexed nested loops join is considered for joining p1 and p2  with p2 as the inner relation  if p2 has only a single relation  say r  and an index is available on the join attributes of r plan p2 may contain an indexed access to r  based on selection conditions on r  to allow indexed nested loops join to be used  the index lookup using the selection condition on r would be dropped from p2 ; instead  the selection condition would be checked on tuples returned from the index on the join attributes of r  thesumit67.blogspot.com 13.4 choice of evaluation plans 601 procedure the time complexity of the procedure can be shown to be o  3n   see practice exercise 13.11   actually  the order in which tuples are generated by the join of a set of relations is also important for finding the best overall join order  since it can affect the cost of further joins  for instance  if merge join is used   a particular sort order of the tuples is said to be an interesting sort order if it could be useful for a later operation for instance  generating the result of r1  r2  r3 sorted on the attributes common with r4 or r5 may be useful  but generating it sorted on the attributes common to only r1 and r2 is not useful using merge join for computing r1  r2  r3 may be costlier than using some other join technique  but it may provide an output sorted in an interesting sort order hence  it is not sufficient to find the best join order for each subset of the set of n given relations instead  we have to find the best join order for each subset  for each interesting sort order of the join result for that subset the number of subsets of n relations is 2n the number of interesting sort orders is generally not large thus  about 2n join expressions need to be stored the dynamic-programming algorithm for finding the best join order can be easily extended to handle sort orders the cost of the extended algorithm depends on the number of interesting orders for each subset of relations ; since this number has been found to be small in practice  the cost remains at o  3n  .with n = 10  this number is around 59,000  which ismuch better than the 17.6 billion different join orders more important  the storage required is much less than before  since we need to store only one join order for each interesting sort order of each of 1024 subsets of r1      r10 although both numbers still increase rapidly with n  commonly occurring joins usually have less than 10 relations  and can be handled easily 13.4.2 cost-based optimization with equivalence rules the join order optimization techniquewe just saw handles the most common class of queries,which perform an inner join of a set of relations.however  clearly many queries use other features  such as aggregation  outer join  and nested queries  which are not addressed by join order selection many optimizers follow an approach based on using heuristic transformations to handle constructs other than joins  and applying the cost-based join order selection algorithm to subexpressions involving only joins and selections details of such heuristics are for the most part specific to individual optimizers  and we do not cover them however  heuristic transformations to handle nested queries are widely used  and are considered in more detail in section 13.4.4 in this section  however  we outline how to create a general-purpose costbased optimizer based on equivalence rules  which can handle a wide variety of query constructs the benefit of using equivalence rules is that it is easy to extend the optimizer with new rules to handle different query constructs for example  nested queries can be represented using extended relational-algebra constructs  and transformations of nested queries can be expressed as equivalence rules we have already thesumit67.blogspot.com 602 chapter 13 query optimization seen equivalence rules with aggregation operations  and equivalence rules can also be created for outer joins in section 13.2.4  we saw how an optimizer could systematically generate all expressions equivalent to the given query the procedure for generating equivalent expressions can be modified to generate all possible evaluation plans as follows  a new class of equivalence rules  called physical equivalence rules  is added that allows a logical operation  such as a join  to be transformed to a physical operation  such as a hash join  or a nested-loops join by adding such rules to the original set of equivalence rules  the procedure can generate all possible evaluation plans the cost estimation techniques we have seen earlier can then be used to choose the optimal  that is  the least-cost  plan however  the procedure shown in section 13.2.4 is very expensive  even if we do not consider generation of evaluation plans to make the approach work efficiently requires the following  1 a space-efficient representation of expressions that avoidsmaking multiple copies of the same subexpressions when equivalence rules are applied 2 efficient techniques for detecting duplicate derivations of the same expression 3 a form of dynamic programming based on memoization  which stores the optimal query evaluation plan for a subexpression when it is optimized for the first time ; subsequent requests to optimize the same subexpression are handled by returning the already memoized plan 4 techniques that avoid generating all possible equivalent plans  by keeping track of the cheapest plan generated for any subexpression up to any point of time  and pruning away any plan that is more expensive than the cheapest plan found so far for that subexpression the details are more complex than we wish to deal with here this approach was pioneered by the volcano research project  and the query optimizer of sql server is based on this approach see the bibliographical notes for references containing further information 13.4.3 heuristics in optimization adrawback of cost-based optimization is the cost of optimization itself although the cost of query optimization can be reduced by clever algorithms  the number of different evaluation plans for a query can be very large  and finding the optimal plan from this set requires a lot of computational effort hence  optimizers use heuristics to reduce the cost of optimization anexample of a heuristic rule is the following rule for transforming relationalalgebra queries  ? perform selection operations as early as possible thesumit67.blogspot.com 13.4 choice of evaluation plans 603 a heuristic optimizer would use this rule without finding out whether the cost is reduced by this transformation in the first transformation example in section 13.2  the selection operation was pushed into a join we say that the preceding rule is a heuristic because it usually  but not always  helps to reduce the cost for an example ofwhere it can result in an increase in cost  consider an expression    r  s   where the condition  refers to only attributes in s the selection can certainly be performed before the join however  if r is extremely small compared to s  and if there is an index on the join attributes of s  but no index on the attributes used by   then it is probably a bad idea to perform the selection early performing the selection early ? that is  directly on s ? would require doing a scan of all tuples in s it is probably cheaper  in this case  to compute the join by using the index  and then to reject tuples that fail the selection the projection operation  like the selection operation  reduces the size of relations thus  whenever we need to generate a temporary relation  it is advantageous to apply immediately any projections that are possible this advantage suggests a companion to the ? perform selections early ? heuristic  ? perform projections early it is usually better to perform selections earlier than projections  since selections have the potential to reduce the sizes of relations greatly  and selections enable the use of indices to access tuples an example similar to the one used for the selection heuristic should convince you that this heuristic does not always reduce the cost most practical query optimizers have further heuristics to reduce the cost of optimization for example  many query optimizers  such as the system r optimizer  3 do not consider all join orders  but rather restrict the search to particular kinds of join orders the system r optimizer considers only those join orders where the right operand of each join is one of the initial relations r1      rn such join orders are called left-deep join orders left-deep join orders are particularly convenient for pipelined evaluation  since the right operand is a stored relation  and thus only one input to each join is pipelined figure 13.8 illustrates the difference between left-deep join trees and non-leftdeep join trees the time it takes to consider all left-deep join orders is o  n !  ,which is much less than the time to consider all join orders with the use of dynamicprogramming optimizations  the system r optimizer can find the best join order in time o  n2n   contrast this cost with the o  3n  time required to find the best overall join order the system r optimizer uses heuristics to push selections and projections down the query tree a heuristic approach to reduce the cost of join-order selection  which was originally used in some versions of oracle  works roughly this way  for an n-way join  it considers n evaluation plans each plan uses a left-deep join order  starting 3system r was one of the first implementations of sql  and its optimizer pioneered the idea of cost-based join-order optimization thesumit67.blogspot.com 604 chapter 13 query optimization r3 r4 r5 r1 r2 r5 r4 r3 r1 r2  a  left-deep join tree  b  non-left-deep join tree figure 13.8 left-deep join trees with a different one of the n relations the heuristic constructs the join order for each of the n evaluation plans by repeatedly selecting the ? best ? relation to join next  on the basis of a ranking of the available access paths either nested-loop or sort-merge join is chosen for each of the joins  depending on the available access paths finally  the heuristic chooses one of the n evaluation plans in a heuristic manner  on the basis of minimizing the number of nested-loop joins that do not have an index available on the inner relation and on the number of sort-merge joins query-optimization approaches that apply heuristic plan choices for some parts of the query  with cost-based choice based on generation of alternative access plans on other parts of the query  have been adopted in several systems the approach used in system r and in its successor  the starburst project  is a hierarchical procedure based on the nested-block concept of sql the costbased optimization techniques described here are used for each block of the query separately the optimizers in several database products  such as ibm db2 and oracle  are based on the above approach  with extensions to handle other operations such as aggregation for compound sql queries  using the ?  n  or  operation   the optimizer processes each component separately  and combines the evaluation plans to form the overall evaluation plan most optimizers allow a cost budget to be specified for query optimization the search for the optimal plan is terminated when the optimization cost budget is exceeded  and the best plan found up to that point is returned the budget itself may be set dynamically ; for example  if a cheap plan is found for a query  the budget may be reduced  on the premise that there is no point spending a lot of time optimizing the query if the best plan found so far is already quite cheap on the other hand  if the best plan found so far is expensive  it makes sense to invest more time in optimization  which could result in a significant reduction in execution time to best exploit this idea  optimizers usually first apply cheap heuristics to find a plan  and then start full cost-based optimizationwith a budget based on the heuristically chosen plan thesumit67.blogspot.com 13.4 choice of evaluation plans 605 many applications execute the same query repeatedly  but with different values for the constants for example  a university application may repeatedly execute a query to find the courses for which a student has registered  but each time for a different student with a different value for the student id as aheuristic  many optimizers optimize a query once  withwhatever valueswere provided for the constants when the query was first submitted  and cache the query plan whenever the query is executed again  perhaps with new values for constants  the cached query plan is reused  using new values for the constants  of course   the optimal plan for the new constants may differ from the optimal plan for the initial values  but as a heuristic the cached plan is reused.4 caching and reuse of query plans is referred to as plan caching even with the use of heuristics  cost-based query optimization imposes a substantial overhead on query processing.however  the added cost of cost-based query optimization is usually more than offset by the saving at query-execution time  which is dominated by slow disk accesses the difference in execution time between a good plan and a bad one may be huge  making query optimization essential the achieved saving is magnified in those applications that run on a regular basis  where a query can be optimized once  and the selected query plan can be used each time the query is executed therefore  most commercial systems include relatively sophisticated optimizers the bibliographical notes give references to descriptions of the query optimizers of actual database systems 13.4.4 optimizing nested subqueries * * sql conceptually treats nested subqueries in the where clause as functions that take parameters and return either a single value or a set of values  possibly an empty set   the parameters are the variables from an outer level query that are used in the nested subquery  these variables are called correlation variables   for instance  suppose we have the following query  to find the names of all instructors who taught a course in 2007  select name from instructor where exists  select * from teaches where instructor.id = teaches.id and teaches.year = 2007  ; conceptually  the subquery can be viewed as a function that takes a parameter  here  instructor.id  and returns the set of all courses taught in 2007 by instructors  with the same id   4for the student registration query  the plan would almost certainly be the same for any student id but a query that took a range of student ids  and returned registration information for all student ids in that range  would probably have a different optimal plan if the range is very small than if the range is large thesumit67.blogspot.com 606 chapter 13 query optimization sql evaluates the overall query  conceptually  by computing the cartesian product of the relations in the outer from clause and then testing the predicates in the where clause for each tuple in the product in the preceding example  the predicate tests if the result of the subquery evaluation is empty this technique for evaluating a query with a nested subquery is called correlated evaluation correlated evaluation is not very efficient  since the subquery is separately evaluated for each tuple in the outer level query a large number of random disk i/o operations may result sql optimizers therefore attempt to transform nested subqueries into joins  where possible efficient join algorithms help avoid expensive randomi/o.where the transformation is not possible  the optimizer keeps the subqueries as separate expressions  optimizes them separately  and then evaluates them by correlated evaluation as an example of transforming a nested subquery into a join  the query in the preceding example can be rewritten as  select name from instructor  teaches where instructor.id = teaches.id and teaches.year = 2007 ;  to properly reflect sql semantics  the number of duplicate derivations should not change because of the rewriting ; the rewritten query can bemodified to ensure this property  as we shall see shortly  in the example  the nested subquery was very simple in general  it may not be possible to directlymove the nested subquery relations into the from clause of the outer query instead  we create a temporary relation that contains the results of the nested query without the selections using correlation variables from the outer query  and join the temporary table with the outer level query for instance  a query of the form  select    from l1 where p1 and exists  select * from l2 where p2  ; where p2 is a conjunction of simpler predicates  can be rewritten as  create table t1 as select distinct v from l2 where p1 2 ; select    from l1  t1 where p1 and p2 2 ; where p1 2 contains predicates in p2 without selections involving correlation variables  and p2 2 reintroduces the selections involving correlation variables  with thesumit67.blogspot.com 13.5 materialized views * * 607 relations referenced in the predicate appropriately renamed   here  v contains all attributes that are used in selections with correlation variables in the nested subquery in our example  the original query would have been transformed to  create table t1 as select distinct id from teaches where year = 2007 ; select name from instructor  t1 where t1.id = instructor.id ; the query we rewrote to illustrate creation of a temporary relation can be obtained by simplifying the above transformed query  assuming the number of duplicates of each tuple does not matter the process of replacing a nested query by a querywith a join  possibly with a temporary relation  is called decorrelation decorrelation is more complicated when the nested subquery uses aggregation  orwhen the result of the nested subquery is used to test for equality  orwhen the condition linking the nested subquery to the outer query is not exists  and so on we do not attempt to give algorithms for the general case  and instead refer you to relevant items in the bibliographical notes optimization of complex nested subqueries is a difficult task  as you can infer from the above discussion  and many optimizers do only a limited amount of decorrelation it is best to avoid using complex nested subqueries,where possible  since we can not be sure that the query optimizer will succeed in converting them to a form that can be evaluated efficiently 13.5 materialized views * * when a view is defined  normally the database stores only the query defining the view in contrast  a materialized view is a view whose contents are computed and stored materialized views constitute redundant data  in that their contents can be inferred from the view definition and the rest of the database contents however  it is much cheaper in many cases to read the contents of a materialized view than to compute the contents of the view by executing the query defining the view materialized views are important for improving performance in some applications consider this view  which gives the total salary in each department  create view department total salary  dept name  total salary  as select dept name  sum  salary  from instructor group by dept name ; thesumit67.blogspot.com 608 chapter 13 query optimization suppose the total salary amount at a department is required frequently computing the view requires reading every instructor tuple pertaining to a department  and summing up the salary amounts  which can be time-consuming in contrast  if the view definition of the total salary amount were materialized  the total salary amount could be found by looking up a single tuple in the materialized view.5 13.5.1 view maintenance a problem with materialized views is that they must be kept up-to-date when the data used in the view definition changes for instance  if the salary value of an instructor is updated  the materialized view will become inconsistent with the underlying data  and it must be updated the task of keeping a materialized view up-to-date with the underlying data is known as view maintenance views can be maintained by manually written code  that is  every piece of code that updates the salary value can be modified to also update the total salary amount for the corresponding department however  this approach is error prone  since it is easy to miss some places where the salary is updated  and the materialized view will then no longer match the underlying data another option for maintaining materialized views is to define triggers on insert  delete  and update of each relation in the view definition the triggers must modify the contents of the materialized view  to take into account the change that caused the trigger to fire a simplisticway of doing so is to completely recompute the materialized view on every update a better option is to modify only the affected parts of the materialized view  which is known as incremental view maintenance we describe howto perform incremental view maintenance in section 13.5.2 modern database systems provide more direct support for incremental view maintenance database-system programmers no longer need to define triggers for view maintenance instead  once a view is declared to be materialized  the database system computes the contents of the view and incrementally updates the contents when the underlying data change most database systems perform immediate view maintenance ; that is  incremental view maintenance is performed as soon as an update occurs  as part of the updating transaction some database systems also support deferred view maintenance  where view maintenance is deferred to a later time ; for example  updates may be collected throughout a day  and materialized views may be updated at night this approach reduces the overhead on update transactions.however  materialized views with deferred view maintenance may not be consistent with the underlying relations on which they are defined 5the difference may not be all that large for a medium-sized university  but in other settings the difference can be very large for example  if the materialized view computed total sales of each product  from a sales relation with tens of millions of tuples  the difference between computing the aggregate from the underlying data  and looking up the materialized view can be many orders of magnitude thesumit67.blogspot.com 13.5 materialized views * * 609 13.5.2 incremental view maintenance to understand how to maintain materialized views incrementally  we start off by considering individual operations  and then we see how to handle a complete expression the changes to a relation that can cause a materialized view to become outof date are inserts  deletes  and updates to simplify our description  we replace updates to a tuple by deletion of the tuple followed by insertion of the updated tuple thus  we need to consider only inserts and deletes the changes  inserts and deletes  to a relation or expression are referred to as its differential 13.5.2.1 join operation consider the materialized view v = r  s suppose we modify r by inserting a set of tuples denoted by ir  if the old value of r is denoted by r old  and the new value of r by r new  r new = r old ? ir  now  the old value of the view  vold  is given by r old  s  and the new value vnew is given by r new  s we can rewrite r new  s as  r old ? ir   s  which we can again rewrite as  r old  s  ?  ir  s   in other words  vnew = vold ?  ir  s  thus  to update the materialized view v  we simply need to add the tuples ir  s to the old contents of the materialized view inserts to s are handled in an exactly symmetric fashion now suppose r is modified by deleting a set of tuples denoted by dr using the same reasoning as above  we get  vnew = vold   dr  s  deletes on s are handled in an exactly symmetric fashion 13.5.2.2 selection and projection operations consider a view v =    r   if we modify r by inserting a set of tuples ir  the new value of v can be computed as  vnew = vold ?    ir  similarly  if r is modified by deleting a set of tuples dr  the new value of v can be computed as  vnew = vold     dr  projection is a more difficult operation with which to deal consider a materialized view v =  a  r   suppose the relation r is on the schema r =  a  b   and r contains two tuples  a  2  and  a  3   then   a  r  has a single tuple  a   if thesumit67.blogspot.com 610 chapter 13 query optimization we delete the tuple  a  2  from r  we can not delete the tuple  a  from  a  r   if we did so  the result would be an empty relation  whereas in reality  a  r  still has a single tuple  a   the reason is that the same tuple  a  is derived in two ways  and deleting one tuple from r removes only one of the ways of deriving  a  ; the other is still present this reason also gives us the intuition for solution  for each tuple in a projection such as  a  r   we will keep a count of how many times it was derived when a set of tuples dr is deleted from r  for each tuple t in dr we do the following  let t.a denote the projection of t on the attribute a we find  t.a  in the materialized view  and decrease the count stored with it by 1 if the count becomes 0   t.a  is deleted from the materialized view handling insertions is relatively straightforward when a set of tuples ir is inserted into r  for each tuple t in ir we do the following  if  t.a  is already present in the materialized view  we increase the count stored with it by 1 if not  we add  t.a  to the materialized view  with the count set to 1 13.5.2.3 aggregation operations aggregation operations proceed somewhat like projections the aggregate operations in sql are count  sum  avg  min  and max  ? count  consider a materialized view v = agcount  b   r   which computes the count of the attribute b  after grouping r by attribute a when a set of tuples ir is inserted into r  for each tuple t in ir we do the following  we look for the group t.a in the materialized view if it is not present  we add  t.a  1  to the materialized view if the group t.ais present  we add 1 to the count of the group when a set of tuples dr is deleted from r  for each tuple t in dr we do the following  we look for the group t.ain the materialized view  and subtract 1 from the count for the group if the count becomes 0  we delete the tuple for the group t.afrom the materialized view ? sum  consider a materialized view v = agsum  b   r   when a set of tuples ir is inserted into r  for each tuple t in ir we do the following  we look for the group t.a in the materialized view if it is not present  we add  t.a  t.b  to the materialized view ; in addition  we store a count of 1 associated with  t.a  t.b   just as we did for projection if the group t.a is present  we add the value of t.b to the aggregate value for the group  and add 1 to the count of the group when a set of tuples dr is deleted from r  for each tuple t in dr we do the following  we look for the group t.a in the materialized view  and subtract t.b from the aggregate value for the group.we also subtract 1 fromthe count for the group  and if the count becomes 0  we delete the tuple for the group t.afrom the materialized view without keeping the extra count value,we would not be able to distinguish a casewhere the sumfor a group is 0 fromthe case where the last tuple in a group is deleted thesumit67.blogspot.com 13.5 materialized views * * 611 ? avg  consider a materialized view v = agavg  b   r   directly updating the average on an insert or delete is not possible  since it depends not only on the old average and the tuple being inserted/deleted  but also on the number of tuples in the group instead  to handle the case of avg  we maintain the sum and count aggregate values as described earlier  and compute the average as the sum divided by the count ? min  max  consider a materialized view v = agmin  b   r    the case of max is exactly equivalent  handling insertions on r is straightforward maintaining the aggregate values min and max on deletions may be more expensive for example  if the tuple corresponding to the minimum value for a group is deleted from r  we have to look at the other tuples of r that are in the same group to find the new minimum value 13.5.2.4 other operations the set operation intersection is maintained as follows  given materialized view v = r n s  when a tuple is inserted in r we check if it is present in s  and if sowe add it to v if a tuple is deleted from r  we delete it from the intersection if it is present the other set operations  union and set difference  are handled in a similar fashion ; we leave details to you outer joins are handled in much the same way as joins  but with some extra work in the case of deletion from r we have to handle tuples in s that no longer match any tuple in r  in the case of insertion to r  we have to handle tuples in s that did not match any tuple in r  again we leave details to you 13.5.2.5 handling expressions so far we have seen how to update incrementally the result of a single operation to handle an entire expression  we can derive expressions for computing the incremental change to the result of each subexpression  starting fromthe smallest subexpressions for example  suppose we wish to incrementally update a materialized view e1  e2 when a set of tuples ir is inserted into relation r  let us assume r is used in e1 alone suppose the set of tuples to be inserted into e1 is given by expression d1 then the expression d1  e2 gives the set of tuples to be inserted into e1  e2 see the bibliographical notes for further details on incremental view maintenance with expressions 13.5.3 query optimization and materialized views query optimization can be performed by treating materialized views just like regular relations however  materialized views offer further opportunities for optimization  ? rewriting queries to use materialized views  thesumit67.blogspot.com 612 chapter 13 query optimization suppose a materialized view v = r  s is available  and a user submits a query r  s  t rewriting the query as v  t may provide a more efficient query plan than optimizing the query as submitted thus  it is the job of the query optimizer to recognize when a materialized view can be used to speed up a query ? replacing a use of a materialized view with the view definition  suppose a materialized view v = r  s is available  but without any index on it  and a user submits a query  a = 10  v   suppose also that s has an index on the common attribute b  and r has an index on attribute a the best plan for this query may be to replace v with r  s  which can lead to the query plan  a = 10  r   s ; the selection and join can be performed efficiently by using the indices on r.a and s.b  respectively in contrast  evaluating the selection directly on v may require a full scan of v  which may be more expensive the bibliographical notes give pointers to research showing how to efficiently perform query optimization with materialized views 13.5.4 materialized view and index selection another related optimization problem is that of materialized view selection  namely  ? what is the best set of views to materialize ? ? this decision must be made on the basis of the system workload  which is a sequence of queries and updates that reflects the typical load on the system one simple criterion would be to select a set of materialized views that minimizes the overall execution time of the workload of queries and updates  including the time taken to maintain the materialized views database administrators usually modify this criterion to take into account the importance of different queries and updates  fast response may be required for some queries and updates  but a slow response may be acceptable for others indices are just like materialized views  in that they too are derived data  can speed up queries  and may slow down updates thus  the problem of index selection is closely related to that of materialized view selection  although it is simpler we examine index and materialized view selection in more detail in sections 24.1.6 and 24.1.7 most database systems provide tools to help the database administrator with index and materialized view selection these tools examine the history of queries and updates  and suggest indices and views to bematerialized the microsoft sql server database tuning assistant  the ibm db2 design advisor  and the oracle sql tuning wizard are examples of such tools 13.6 advanced topics in query optimization * * there are a number of opportunities for optimizing queries  beyond those we have seen so far.we examine a few of these in this section thesumit67.blogspot.com 13.6 advanced topics in query optimization * * 613 13.6.1 top-k optimization many queries fetch results sorted on some attributes  and require only the top k results for some k sometimes the bound k is specified explicitly for example  some databases support a limit k clause which results in only the top k results being returned by the query other databases support alternative ways of specifying similar limits in other cases  the query may not specify such a limit  but the optimizer may allow a hint to be specified  indicating that only the top k results of the query are likely to be retrieved  even if the query generates more results when k is small  a query optimization plan that generates the entire set of results  then sorts and generates the top k  is very inefficient since it discards most of the intermediate results that it computes several techniques have been proposed to optimize such top-k queries one approach is to use pipelined plans that can generate the results in sorted order.another approach is to estimate what is the highest value on the sorted attributes that will appear in the top-k output  and introduce selection predicates that eliminate larger values if extra tuples beyond the top-k are generated they are discarded  and if too few tuples are generated then the selection condition is changed and the query is re-executed see the bibliographical notes for references to work on top-k optimization 13.6.2 join minimization when queries are generated through views  sometimes more relations are joined than are needed for computation of the query for example  a view v may include the join of instructor and department  but a use of the view v may use only attributes from instructor the join attribute dept name of instructor is a foreign key referencing department assuming that instructor.dept name has been declared not null  the join with department can be dropped  with no impact on the query for  under the above assumption  the join with department does not eliminate any tuples from instructor  nor does it result in extra copies of any instructor tuple dropping a relation from a join as above is an example of join minimization in fact  join minimization can be performed in other situations as well see the bibliographical notes for references on join minimization 13.6.3 optimization of updates update queries often involve subqueries in the set and where clauses  which must also be taken into account in optimizing the update updates that involve a selection on the updated column  e.g  give a 10 percent salary raise to all employees whose salary is = $ 100,000  must be handled carefully if the update is done while the selection is being evaluated by an index scan  an updated tuple may be reinserted in the index ahead of the scan and seen again by the scan ; the same employee tuple may then get incorrectly updated multiple times  an infinite number of times  in this case   a similar problemalso arises with updates involving subqueries whose result is affected by the update the problem of an update affecting the execution of a query associated with the update is known as the halloween problem  named so because it was first thesumit67.blogspot.com 614 chapter 13 query optimization recognized on ahalloween day  at ibm   the problem can be avoided by executing the queries defining the update first  creating a list of affected tuples  and updating the tuples and indices as the last step however  breaking up the execution plan in such a fashion increases the execution cost update plans can be optimized by checking if the halloween problem can occur  and if it can not occur  updates can be performedwhile the query is being processed  reducing the update overheads for example  the halloween problem can not occur if the update does not affect index attributes even if it does  if the updates decrease the value,while the index is scanned in increasing order  updated tuples will not be encountered again during the scan in such cases  the index can be updated even while the query is being executed  reducing the overall cost update queries that result in a large number of updates can also be optimized by collecting the updates as a batch  and then applying the batch of updates separately to each affected index when applying the batch of updates to an index  the batch is first sorted in the index order for that index ; such sorting can greatly reduce the amount of random i/o required for updating indices such optimizations of updates are implemented in most database systems see the bibliographical notes for references to such optimization 13.6.4 multiquery optimization and shared scans when a batch of queries are submitted together  a query optimizer can potentially exploit common subexpressions between the different queries  evaluating them once and reusing them where required complex queries may in fact have subexpressions repeated in different parts of the query  which can be similarly exploited  to reduce query evaluation cost such optimization is known as multiquery optimization common subexpression elimination optimizes subexpressions shared by different expressions in a program  by computing and storing the result  and reusing it wherever the subexpression occurs common subexpression elimination is a standardoptimization applied on arithmetic expressions byprogramminglanguage compilers exploiting common subexpressions among evaluation plans chosen for each of a batch of queries is just as useful in database query evaluation  and is implemented by some databases however  multiquery optimization can do even better in some cases  a query typically has more than one evaluation plan  and a judiciously chosen set of query evaluation plans for the queries may provide for a greater sharing and lesser cost than that afforded by choosing the lowest cost evaluation plan for each query more details on multiquery optimization may be found in references cited in the bibliographical notes sharing of relation scans between queries is another limited form of multiquery optimization that is implemented in some databases the shared-scan optimization works as follows  instead of reading the relation repeatedly from disk  once for each query that needs to scan a relation  data are read once from disk  and pipelined to each of the queries the shared-scan optimization is particularly useful when multiple queries perform a scan on a single large relation  typically a ? fact table ?   thesumit67.blogspot.com 13.7 summary 615 13.6.5 parametric query optimization plan caching  which we saw earlier in section 13.4.3  is used as a heuristic in many databases recall that with plan caching  if a query is invoked with some constants  the plan chosen by the optimizer is cached  and reused if the query is submitted again  even if the constants in the query are different for example  suppose a query takes a department name as a parameter  and retrieves all courses of the department.with plan caching  a plan chosen when the query is executed for the first time  say for the music department  is reused if the query is executed for any other department such reuse of plans by plan caching is reasonable if the optimal query plan is not significantly affected by the exact value of the constants in the query.however  if the plan is affected by the value of the constants  parametric query optimization is an alternative in parametric query optimization  a query is optimized without being provided specific values for its parameters  for example  dept name in the preceding example the optimizer then outputs several plans  each optimal for a different parameter value a plan would be output by the optimizer only if it is optimal for some possible value of the parameters the set of alternative plans output by the optimizer are stored when a query is submitted with specific values for its parameters  instead of performing a full optimization  the cheapest plan fromthe set of alternative plans computed earlier is used finding the cheapest such plan usually takes much less time than reoptimization see the bibliographical notes for references on parametric query optimization 13.7 summary ? given a query  there are generally a variety of methods for computing the answer it is the responsibility of the system to transform the query as entered by the user into an equivalent query that can be computed more efficiently the process of finding a good strategy for processing a query is called query optimization ? the evaluation of complex queries involves many accesses to disk since the transfer of data from disk is slow relative to the speed of main memory and the cpu of the computer system  it is worthwhile to allocate a considerable amount of processing to choose a method that minimizes disk accesses ? there are a number of equivalence rules that we can use to transform an expression into an equivalent one.we use these rules to generate systematically all expressions equivalent to the given query ? each relational-algebra expression represents a particular sequence of operations the first step in selecting a query-processing strategy is to find a relational-algebra expression that is equivalent to the given expression and is estimated to cost less to execute thesumit67.blogspot.com 616 chapter 13 query optimization ? the strategy that the database system chooses for evaluating an operation depends on the size of each relation and on the distribution of values within columns so that they can base the strategy choice on reliable information  database systems may store statistics for each relation r these statistics include  ? the number of tuples in the relation r ? the size of a record  tuple  of relation r in bytes ? the number of distinct values that appear in the relation r for a particular attribute ? most database systems use histograms to store the number of values for an attribute within each of several ranges of values histograms are often computed using sampling ? these statistics allow us to estimate the sizes of the results of various operations  as well as the cost of executing the operations statistical information about relations is particularly useful when several indices are available to assist in the processing of a query the presence of these structures has a significant influence on the choice of a query-processing strategy ? alternative evaluation plans for each expression can be generated by equivalence rules  and the cheapest plan across all expressions can be chosen several optimization techniques are available to reduce the number of alternative expressions and plans that need to be generated ? we use heuristics to reduce the number of plans considered  and thereby to reduce the cost of optimization heuristic rules for transforming relationalalgebra queries include ? perform selection operations as early as possible  ? ? perform projections early  ? and ? avoid cartesian products ? ? materialized views can be used to speed up query processing incremental view maintenance is needed to efficiently update materialized views when the underlying relations are modified the differential of an operation can be computed by means of algebraic expressions involving differentials of the inputs of the operation other issues related to materialized views include how to optimize queries by making use of available materialized views  and how to select views to be materialized ? a number of advanced optimization techniques have been proposed such as top-k optimization  join minimization  optimization of updates  multiquery optimization  and parametric query optimization review terms ? query optimization ? transformation of expressions ? equivalence of expressions thesumit67.blogspot.com practice exercises 617 ? equivalence rules ? join commutativity ? join associativity ? minimal set of equivalence rules ? enumeration of equivalent expressions ? statistics estimation ? catalog information ? size estimation ? selection ? selectivity ? join ? histograms ? distinct value estimation ? random sample ? choice of evaluation plans ? interaction of evaluation techniques ? cost-based optimization ? join-order optimization ? dynamic-programming algorithm ? left-deep join order ? interesting sort order ? heuristic optimization ? plan caching ? access-plan selection ? correlated evaluation ? decorrelation ? materialized views ? materialized view maintenance ? recomputation ? incremental maintenance ? insertion ? deletion ? updates ? query optimization with materialized views ? index selection ? materialized view selection ? top-k optimization ? join minimization ? halloween problem ? multiquery optimization practice exercises 13.1 show that the following equivalences hold explain how you can apply them to improve the efficiency of certain queries  a e1    e2  e3  =  e1   e2  e1   e3   b    agf  e   = agf     e    where  uses only attributes from a c    e1  e2  =    e1   e2  where  uses only attributes from e1 13.2 for each of the following pairs of expressions  give instances of relations that show the expressions are not equivalent a  a  r  s  and  a  r    a  s   b  b < 4  agmax  b  as b  r   and agmax  b  as b   b < 4  r    thesumit67.blogspot.com 618 chapter 13 query optimization c in the preceding expressions  if both occurrences of max were replaced by min would the expressions be equivalent ? d  r  s   t and r   s  t  in other words  the natural left outer join is not associative  hint  assume that the schemas of the three relations are r  a  b1   s  a  b2   and t  a  b3   respectively  e    e1  e2  and e1     e2   where  uses only attributes from e2 13.3 sql allows relations with duplicates  chapter 3   a define versions of the basic relational-algebra operations     ?      ?  and n that work on relations with duplicates  in a way consistent with sql b check which of the equivalence rules 1 through 7.b hold for the multiset version of the relational-algebra defined in part a 13.4 consider the relations r1  a  b,c   r2  c  d  e   and r3  e  f   with primary keys a  c  and e  respectively assume that r1 has 1000 tuples  r2 has 1500 tuples  and r3 has 750 tuples estimate the size of r1  r2  r3  and give an efficient strategy for computing the join 13.5 consider the relations r1  a  b,c   r2  c  d  e   and r3  e  f  of practice exercise 13.4 assume that there are no primary keys  except the entire schema let v  c  r1  be 900  v  c  r2  be 1100  v  e  r2  be 50  and v  e  r3  be 100 assume that r1 has 1000 tuples  r2 has 1500 tuples  and r3 has 750 tuples estimate the size of r1  r2  r3 and give an efficient strategy for computing the join 13.6 suppose that a b + -tree index on building is available on relation department  and that no other index is available.whatwould be the bestway to handle the following selections that involve negation ? a  ?  building < ? watson ?   department  b  ?  building = ? watson ?   department  c  ?  building < ? watson ? ? budget < 50000   department  13.7 consider the query  select * from r  s where upper  r.a  = upper  s.a  ; where ? upper ? is a function that returns its input argument with all lowercase letters replaced by the corresponding uppercase letters a find outwhat plan is generated for this query on the database system you use thesumit67.blogspot.com practice exercises 619 b some database systemswould use a  block  nested-loop join for this query  which can be very inefficient briefly explain how hash-join ormerge-join can be used for this query 13.8 give conditions under which the following expressions are equivalent a,bgagg  c   e1  e2  and  agagg  c   e1    e2 where agg denotes any aggregation operation how can the above conditions be relaxed if agg is one of min or max ? 13.9 consider the issue of interesting orders in optimization suppose you are given a query that computes the natural join of a set of relations s given a subset s1 of s  what are the interesting orders of s1 ? 13.10 show that  with n relations  there are  2  n  1   ! /  n  1  ! different join orders hint  a complete binary tree is one where every internal node has exactly two children use the fact that the number of different complete binary trees with n leaf nodes is  1 n  2  n  1   n  1   if youwish  you can derive the formula for the number of complete binary trees with n nodes from the formula for the number of binary trees with n nodes the number of binary trees with n nodes is  1 n + 1  2n n  this number is known as the catalan number  and its derivation can be found in any standard textbook on data structures or algorithms 13.11 show that the lowest-cost join order can be computed in time o  3n   assume that you can store and look up information about a set of relations  such as the optimal join order for the set  and the cost of that join order  in constant time  if you find this exercise difficult  at least show the looser time bound of o  22n    13.12 show that  if only left-deep join trees are considered  as in the system r optimizer  the time taken to find the most efficient join order is around n2n assume that there is only one interesting sort order 13.13 consider the bank database of figure 13.9  where the primary keys are underlined construct the following sql queries for this relational database a write a nested query on the relation account to find  for each branch with name starting with b  all accounts with the maximum balance at the branch thesumit67.blogspot.com 620 chapter 13 query optimization branch  branch name  branch city  assets  customer  customer name  customer street  customer city  loan  loan number  branch name  amount  borrower  customer name  loan number  account  account number  branch name  balance  depositor  customer name  account number  figure 13.9 banking database for exercise 13.13 b rewrite the preceding query  without using a nested subquery ; in other words  decorrelate the query c give a procedure  similar to that described in section 13.4.4  for decorrelating such queries 13.14 the set version of the semijoin operator  is defined as follows  r   s =  r  r   s  where r is the set of attributes in the schema of r  the multiset version of the semijoin operation returns the same set of tuples  but each tuple has exactly as many copies as it had in r  consider the nested query we saw in section 13.4.4which finds the names of all instructors who taught a course in 2007.write the query in relational algebra using the multiset semjoin operation  ensuring that the number of duplicates of each name is the same as in the sql query  the semijoin operation is widely used for decorrelation of nested queries  exercises 13.15 suppose that a b + -tree index on  dept name  building  is available on relation department what would be the best way to handle the following selection ?   building < ? watson ?  ?  budget < 55000  ?  dept name = ? music ?   department  13.16 show how to derive the following equivalences by a sequence of transformations using the equivalence rules in section 13.2.1 a   1 ?  2 ?  3  e  =   1    2    3  e    b   1 ?  2  e1   3 e2  =   1  e1   3    2  e2     where  2 involves only attributes from e2 thesumit67.blogspot.com exercises 621 13.17 consider the two expressions    e1  e2  and    e1  e2   a show using an example that the two expressions are not equivalent in general b give a simple condition on the predicate   which if satisfied will ensure that the two expressions are equivalent 13.18 a set of equivalence rules is said to be complete if  whenever two expressions are equivalent  one can be derived from the other by a sequence of uses of the equivalence rules is the set of equivalence rules that we considered in section 13.2.1 complete ? hint  consider the equivalence  3 = 5  r  =    13.19 explain how to use a histogram to estimate the size of a selection of the form  a = v  r   13.20 suppose two relations r and s have histograms on attributes r.aand s.a  respectively  butwith different ranges suggest how to use the histograms to estimate the size of r  s hint  split the ranges of each histogram further 13.21 consider the query select a  b from r where r.b < some  select b from s where s.a = r.a  show how to decorrelate the above query using the multiset version of the semijoin operation  defined in exercise 13.14 13.22 describe how to incrementally maintain the results of the following operations  on both insertions and deletions  a union and set difference b left outer join 13.23 give an example of an expression defining a materialized view and two situations  sets of statistics for the input relations and the differentials  such that incremental view maintenance is better than recomputation in one situation  and recomputation is better in the other situation 13.24 suppose you want to get answers to r  s sorted on an attribute of r  and want only the top k answers for some relatively small k give a good way of evaluating the query  a whenthe join ison a foreignkeyof r referencing s  where the foreign key attribute is declared to be not null b when the join is not on a foreign key thesumit67.blogspot.com 622 chapter 13 query optimization 13.25 consider a relation r  a  b,c   with an index on attribute a give an example of a query that can be answered by using the index only  without looking at the tuples in the relation  query plans that use only the index  without accessing the actual relation  are called index-only plans  13.26 suppose you have an update query u give a simple sufficient condition onu thatwill ensure that thehalloween problem can not occur  regardless of the execution plan chosen  or the indices that exist bibliographical notes the seminal work of selinger et al  1979  describes access-path selection in the system r optimizer  which was one of the earliest relational-query optimizers query processing in starburst  described in haas et al  1989   forms the basis for query optimization in ibm db2 graefe and mckenna  1993a  describe volcano  an equivalence-rule ? based query optimizer that  along with its successor cascades  graefe  1995    forms the basis of query optimization inmicrosoft sql server estimation of statistics of query results  such as result size  is addressed by ioannidis and poosala  1995   poosala et al  1996   and ganguly et al  1996   among others nonuniform distributions of values cause problems for estimation of query size and cost cost-estimation techniques that use histograms of value distributions have been proposed to tackle the problem ioannidis and christodoulakis  1993   ioannidis and poosala  1995   and poosala et al  1996  present results in this area the use of random sampling for constructing histograms is well known in statistics  but issues in histogram construction in the context of databases is discussed in chaudhuri et al  1998   klug  1982  was an early work on optimization of relational-algebra expressions with aggregate functions optimization of queries with aggregation is addressed by yan and larson  1995  and chaudhuri and shim  1994   optimization of queries containing outer joins is described in rosenthal and reiner  1984   galindo-legaria and rosenthal  1992   and galindo-legaria  1994   optimization of top-k queries is addressed in carey and kossmann  1998  and bruno et al  2002   optimization of nested subqueries is discussed in kim  1982   ganski and wong  1987   dayal  1987   seshadri et al  1996  and galindo-legaria and joshi  2001   blakeley et al  1986  describe techniques for maintenance of materialized views optimization ofmaterialized view maintenance plans is described byvista  1998  and mistry et al  2001   query optimization in the presence of materialized views is addressed by chaudhuri et al  1995   index selection and materialized view selection are addressed by ross et al  1996   and chaudhuri and narasayya  1997   optimization of top-k queries is addressed in carey and kossmann  1998  and bruno et al  2002   a collection of techniques for join minimization has thesumit67.blogspot.com bibliographical notes 623 been grouped under the name tableau optimization the notion of a tableau was introduced by aho et al  1979b  and aho et al  1979a   and was further extended by sagiv and yannakakis  1981   parametric query-optimization algorithms have been proposed by ioannidis et al  1992   ganguly  1998  and hulgeri and sudarshan  2003   sellis  1988  was an early work on multiquery optimization  while roy et al  2000  showed how to integrate multi-query optimization into a volcano-based query optimizer galindo-legaria et al  2004  describes query processing and optimization for database updates  including optimization of index maintenance  materialized viewmaintenance plans and integrity constraint checking  along with techniques to handle the halloween problem thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com part 4 transaction management the term transaction refers to a collection of operations that form a single logical unit of work for instance  transfer of money from one account to another is a transaction consisting of two updates  one to each account it is important that either all actions of a transaction be executed completely  or  in case of some failure  partial effects of each incomplete transaction be undone this property is called atomicity further  once a transaction is successfully executed  its effects must persist in the database ? a system failure should not result in the database forgetting about a transaction that successfully completed this property is called durability in a database system where multiple transactions are executing concurrently  if updates to shared data are not controlled there is potential for transactions to see inconsistent intermediate states created by updates of other transactions such a situation can result in erroneous updates to data stored in the database thus  database systems must provide mechanisms to isolate transactions from the effects of other concurrently executing transactions this property is called isolation chapter 14 describes the concept of a transaction in detail  including the properties of atomicity  durability  isolation  and other properties provided by the transaction abstraction in particular  the chapter makes precise the notion of isolation by means of a concept called serializability chapter 15 describes several concurrency-control techniques that help implement the isolation property chapter 16 describes the recovery management component of a database  which implements the atomicity and durability properties taken as a whole  the transaction-management component of a database system allows application developers to focus on the implementation of individual transactions  ignoring the issues of concurrency and fault tolerance 625 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter14 transactions often  a collection of several operations on the database appears to be a single unit from the point of view of the database user for example  a transfer of funds from a checking account to a savings account is a single operation from the customer ? s standpoint ; within the database system  however  it consists of several operations clearly  it is essential that all these operations occur  or that  in case of a failure  none occur it would be unacceptable if the checking account were debited but the savings account not credited collections of operations that form a single logical unit of work are called transactions a database system must ensure proper execution of transactions despite failures ? either the entire transaction executes  or none of it does furthermore  it must manage concurrent execution of transactions in a way that avoids the introduction of inconsistency in our funds-transfer example  a transaction computing the customer ? s total balance might see the checking-account balance before it is debited by the funds-transfer transaction  but see the savings balance after it is credited as a result  it would obtain an incorrect result this chapter introduces the basic concepts of transaction processing details on concurrent transaction processing and recovery from failures are in chapters 15 and 16  respectively further topics in transaction processing are discussed in chapter 26 14.1 transaction concept a transaction is a unit of program execution that accesses and possibly updates various data items usually  a transaction is initiated by a user program written in a high-level data-manipulation language  typically sql   or programming language  for example  c + +  or java   with embedded database accesses in jdbc or odbc a transaction is delimited by statements  or function calls  of the form begin transaction and end transaction the transaction consists of all operations executed between the begin transaction and end transaction this collection of steps must appear to the user as a single  indivisible unit since a transaction is indivisible  it either executes in its entirety or not at all thus  if a transaction begins to execute but fails forwhatever reason  any changes to the 627 thesumit67.blogspot.com 628 chapter 14 transactions database that the transaction may have made must be undone this requirement holds regardless of whether the transaction itself failed  for example  if it divided by zero   the operating system crashed  or the computer itself stopped operating as we shall see  ensuring that this requirement is met is difficult since some changes to the database may still be stored only in the main-memory variables of the transaction  while others may have been written to the database and stored on disk this ? all-or-none ? property is referred to as atomicity furthermore  since a transaction is a single unit  its actions can not appear to be separated by other database operations not part of the transaction while we wish to present this user-level impression of transactions  we know that reality is quite different even a single sql statement involves many separate accesses to the database  and a transaction may consist of several sql statements therefore  the database system must take special actions to ensure that transactions operate properly without interference from concurrently executing database statements this property is referred to as isolation even if the system ensures correct execution of a transaction  this serves little purpose if the system subsequently crashes and  as a result  the system ? forgets ? about the transaction thus  a transaction ? s actions must persist across crashes this property is referred to as durability because of the above three properties  transactions are an ideal way of structuring interaction with a database this leads us to impose a requirement on transactions themselves a transaction must preserve database consistency ? if a transaction is run atomically in isolation starting from a consistent database  the database must again be consistent at the end of the transaction this consistency requirement goes beyond the data integrity constraintswe have seen earlier  such as primary-key constraints  referential integrity  check constraints  and the like   rather  transactions are expected to go beyond that to ensure preservation of those application-dependent consistency constraints that are too complex to state using the sql constructs for data integrity how this is done is the responsibility of the programmerwho codes a transaction this property is referred to as consistency to restate the above more concisely  we require that the database system maintain the following properties of the transactions  ? atomicity either all operations of the transaction are reflected properly in the database  or none are ? consistency execution of a transaction in isolation  that is  with no other transaction executing concurrently  preserves the consistency of the database ? isolation even though multiple transactions may execute concurrently  the system guarantees that  for every pair of transactions ti and tj  it appears to ti that either tj finished execution before ti started or tj started execution after ti finished thus  each transaction is unaware of other transactions executing concurrently in the system ? durability after a transaction completes successfully  the changes it has made to the database persist  even if there are system failures thesumit67.blogspot.com 14.2 a simple transaction model 629 these properties are often called the acid properties ; the acronym is derived from the first letter of each of the four properties as we shall see later  ensuring the isolation property may have a significant adverse effect on system performance for this reason  some applications compromise on the isolation property we shall study these compromises after first studying the strict enforcement of the acid properties 14.2 a simple transaction model because sql is a powerful and complex language  we begin our study of transactionswith a simple database language that focuses on when data are moved from disk to main memory and from main memory to disk in doing this  we ignore sql insert and delete operations  and defer considering them until section 15.8 the only actual operations on the data are restricted in our simple language to arithmetic operations later we shall discuss transactions in a realistic  sql-based contextwith a richer set of operations the data items in our simplified model contain a single data value  a number in our examples   each data item is identified by a name  typically a single letter in our examples  that is  a  b  c  etc   we shall illustrate the transaction concept using a simple bank application consisting of several accounts and a set of transactions that access and update those accounts transactions access data using two operations  ? read  x   which transfers the data item x from the database to a variable  also called x  in a buffer in main memory belonging to the transaction that executed the read operation ? write  x   which transfers the value in the variable x in the main-memory buffer of the transaction that executed the write to the data item x in the database it is important to know if a change to a data item appears only in main memory or if it has been written to the database on disk in a real database system  the write operation does not necessarily result in the immediate update of the data on the disk ; the write operation may be temporarily stored elsewhere and executed on the disk later for now  however  we shall assume that the write operation updates the database immediately.we shall return to this subject in chapter 16 let ti be a transaction that transfers $ 50 from account a to account b this transaction can be defined as  ti  read  a  ; a  = a  50 ; write  a  ; read  b  ; b  = b + 50 ; write  b   thesumit67.blogspot.com 630 chapter 14 transactions let us now consider each of the acid properties  for ease of presentation  we consider them in an order different from the order a-c-i-d  ? consistency  the consistency requirement here is that the sum of a and b be unchanged by the execution of the transaction without the consistency requirement  money could be created or destroyed by the transaction ! it can be verified easily that  if the database is consistent before an execution of the transaction  the database remains consistent after the execution of the transaction ensuring consistency for an individual transaction is the responsibility of the application programmer who codes the transaction this task may be facilitated by automatic testing of integrity constraints  as we discussed in section 4.4 ? atomicity  suppose that  just before the execution of transaction ti  the values of accounts a and b are $ 1000 and $ 2000  respectively now suppose that  during the execution of transaction ti  a failure occurs that prevents ti from completing its execution successfully further  suppose that the failure happened after the write  a  operation but before the write  b  operation in this case  the values of accounts a and b reflected in the database are $ 950 and $ 2000 the system destroyed $ 50 as a result of this failure in particular  we note that the sum a + b is no longer preserved thus  because of the failure  the state of the system no longer reflects a real state of the world that the database is supposed to capture we term such a state an inconsistent state we must ensure that such inconsistencies are not visible in a database system note  however  that the system must at some point be in an inconsistent state even if transaction ti is executed to completion  there exists a point at which the value of account ais $ 950 and the value of account b is $ 2000  which is clearly an inconsistent state this state  however  is eventually replaced by the consistent state where the value of account a is $ 950  and the value of account b is $ 2050 thus  if the transaction never started orwas guaranteed to complete  such an inconsistent statewould not be visible except during the execution of the transaction that is the reason for the atomicity requirement  if the atomicity property is present  all actions of the transaction are reflected in the database  or none are the basic idea behind ensuring atomicity is this  the database system keeps track  on disk  of the old values of any data on which a transaction performs a write this information is written to a file called the log if the transaction does not complete its execution  the database system restores the old values from the log to make it appear as though the transaction never executed we discuss these ideas further in section 14.4 ensuring atomicity is the responsibility of the database system ; specifically  it is handled by a component of the database called the recovery system  which we describe in detail in chapter 16 ? durability  once the execution of the transaction completes successfully  and the user who initiated the transaction has been notified that the transfer of thesumit67.blogspot.com 14.2 a simple transaction model 631 funds has taken place  it must be the case that no system failure can result in a loss of data corresponding to this transfer of funds the durability property guarantees that  once a transaction completes successfully  all the updates that it carried out on the database persist  even if there is a system failure after the transaction completes execution we assume for now that a failure of the computer system may result in loss of data in main memory  but data written to disk are never lost protection against loss of data on disk is discussed in chapter 16 we can guarantee durability by ensuring that either  1 the updates carried out by the transaction have been written to disk before the transaction completes 2 information about the updates carried out by the transaction and written to disk is sufficient to enable the database to reconstruct the updates when the database system is restarted after the failure the recovery system of the database  described in chapter 16  is responsible for ensuring durability  in addition to ensuring atomicity ? isolation  even if the consistency and atomicity properties are ensured for each transaction  if several transactions are executed concurrently  their operations may interleave in some undesirable way  resulting in an inconsistent state for example  as we saw earlier  the database is temporarily inconsistent while the transaction to transfer funds from a to b is executing  with the deducted total written to aand the increased total yet to be written to b if a second concurrently running transaction reads a and b at this intermediate point and computes a + b  itwill observe an inconsistent value furthermore  if this second transaction then performs updates on a and b based on the inconsistent values that it read  the database may be left in an inconsistent state even after both transactions have completed a way to avoid the problem of concurrently executing transactions is to execute transactions serially ? that is  one after the other however  concurrent execution of transactions provides significant performance benefits  as we shall see in section 14.5 other solutions have therefore been developed ; they allow multiple transactions to execute concurrently we discuss the problems caused by concurrently executing transactions in section 14.5 the isolation property of a transaction ensures that the concurrent execution of transactions results in a system state that is equivalent to a state that could have been obtained had these transactions executed one at a time in some order.we shall discuss the principles of isolation further in section 14.6 ensuring the isolation property is the responsibility of a component of the database system called the concurrency-control system  which we discuss later  in chapter 15 thesumit67.blogspot.com 632 chapter 14 transactions 14.3 storage structure to understand how to ensure the atomicity and durability properties of a transaction  we must gain a better understanding of how the various data items in the database may be stored and accessed in chapter 10we saw that storage media can be distinguished by their relative speed  capacity  and resilience to failure  and classified as volatile storage or nonvolatile storage.we review these terms  and introduce another class of storage  called stable storage ? volatile storage information residing in volatile storage does not usually survive system crashes examples of such storage are main memory and cache memory access to volatile storage is extremely fast  both because of the speed of the memory access itself  and because it is possible to access any data item in volatile storage directly ? nonvolatile storage information residing in nonvolatile storage survives system crashes examples of nonvolatile storage include secondary storage devices such as magnetic disk and flash storage  used for online storage  and tertiary storage devices such as optical media  and magnetic tapes  used for archival storage at the current state of technology  nonvolatile storage is slower than volatile storage  particularly for random access both secondary and tertiary storage devices  however  are susceptible to failure which may result in loss of information ? stable storage information residing in stable storage is never lost  never should be taken with a grain of salt  since theoretically never can not be guaranteed ? for example  it is possible  although extremely unlikely  that a black hole may envelop the earth and permanently destroy all data !   although stable storage is theoretically impossible to obtain  it can be closely approximated by techniques that make data loss extremely unlikely to implement stable storage  we replicate the information in several nonvolatile storage media  usually disk  with independent failure modes updates must be done with care to ensure that a failure during an update to stable storage does not cause a loss of information section 16.2.1 discusses stable-storage implementation the distinctions among the various storage types can be less clear in practice than in our presentation for example  certain systems  for example some raid controllers  provide battery backup  so that some main memory can survive system crashes and power failures for a transaction to be durable  its changes need to be written to stable storage similarly  for a transaction to be atomic  log records need to be written to stable storage before any changes are made to the database on disk clearly  the degree to which a system ensures durability and atomicity depends on how stable its implementation of stable storage really is in some cases  a single copy on disk is considered sufficient  but applications whose data are highly valuable andwhose thesumit67.blogspot.com 14.4 transaction atomicity and durability 633 transactions are highly important require multiple copies  or  in other words  a closer approximation of the idealized concept of stable storage 14.4 transaction atomicity and durability as we noted earlier  a transaction may not always complete its execution successfully such a transaction is termed aborted if we are to ensure the atomicity property  an aborted transaction must have no effect on the state of the database thus  any changes that the aborted transaction made to the database must be undone once the changes caused by an aborted transaction have been undone  we say that the transaction has been rolled back it is part of the responsibility of the recovery scheme to manage transaction aborts this is done typically by maintaining a log each database modification made by a transaction is first recorded in the log.we record the identifier of the transaction performing the modification  the identifier of the data item being modified  and both the old value  prior to modification  and the new value  after modification  of the data item only then is the database itself modified maintaining a log provides the possibility of redoing a modification to ensure atomicity and durability as well as the possibility of undoing a modification to ensure atomicity in case of a failure during transaction execution details of log-based recovery are discussed in chapter 16 a transaction that completes its execution successfully is said to be committed acommitted transaction that has performed updates transforms the database into a new consistent state  which must persist even if there is a system failure once a transaction has committed  we can not undo its effects by aborting it the only way to undo the effects of a committed transaction is to execute a compensating transaction for instance  if a transaction added $ 20 to an account  the compensating transaction would subtract $ 20 from the account however  it is not always possible to create such a compensating transaction therefore  the responsibility of writing and executing a compensating transaction is left to the user  and is not handled by the database system chapter 26 includes a discussion of compensating transactions we need to be more precise about what we mean by successful completion of a transaction we therefore establish a simple abstract transaction model a transaction must be in one of the following states  ? active  the initial state ; the transaction stays in this statewhile it is executing ? partially committed  after the final statement has been executed ? failed  after the discovery that normal execution can no longer proceed ? aborted  after the transaction has been rolled back and the database has been restored to its state prior to the start of the transaction ? committed  after successful completion the state diagram corresponding to a transaction appears in figure 14.1 we say that a transaction has committed only if it has entered the committed state thesumit67.blogspot.com 634 chapter 14 transactions active failed partially commi  ed commi  ed aborted figure 14.1 state diagram of a transaction similarly  we say that a transaction has aborted only if it has entered the aborted state.atransaction is said to have terminated if it has either committed or aborted a transaction starts in the active state when it finishes its final statement  it enters the partially committed state at this point  the transaction has completed its execution  but it is still possible that it may have to be aborted  since the actual output may still be temporarily residing in main memory  and thus a hardware failure may preclude its successful completion the database system then writes out enough information to disk that  even in the event of a failure  the updates performed by the transaction can be re-created when the system restarts after the failure when the last of this information is written out  the transaction enters the committed state as mentioned earlier  we assume for now that failures do not result in loss of data on disk chapter 16 discusses techniques to deal with loss of data on disk a transaction enters the failed state after the system determines that the transaction can no longer proceed with its normal execution  for example  because of hardware or logical errors   such a transaction must be rolled back then  it enters the aborted state at this point  the system has two options  ? it can restart the transaction  but only if the transaction was aborted as a result of some hardware or software error that was not created through the internal logic of the transaction a restarted transaction is considered to be a new transaction ? it can kill the transaction it usually does so because of some internal logical error that can be corrected only by rewriting the application program  or because the input was bad  or because the desired data were not found in the database we must be cautious when dealing with observable external writes  such as writes to a user ? s screen  or sending email once such a write has occurred  it can not be erased  since it may have been seen external to the database system thesumit67.blogspot.com 14.5 transaction isolation 635 most systems allow suchwrites to take place only after the transaction has entered the committed state one way to implement such a scheme is for the database system to store any value associated with such external writes temporarily in a special relation in the database  and to perform the actual writes only after the transaction enters the committed state if the system should fail after the transaction has entered the committed state  but before it could complete the external writes  the database system will carry out the external writes  using the data in nonvolatile storage  when the system is restarted handling external writes can be more complicated in some situations for example  suppose the external action is that of dispensing cash at an automated teller machine  and the system fails just before the cash is actually dispensed  we assume that cash can be dispensed atomically   it makes no sense to dispense cash when the system is restarted  since the user may have left the machine in such a case a compensating transaction  such as depositing the cash back in the user ? s account  needs to be executed when the system is restarted as another example  consider a user making a booking over the web it is possible that the database system or the application server crashes just after the booking transaction commits it is also possible that the network connection to the user is lost just after the booking transaction commits in either case  even though the transaction has committed  the external write has not taken place to handle such situations  the application must be designed such that when the user connects to the web application again  she will be able to see whether her transaction had succeeded or not for certain applications  it may be desirable to allow active transactions to display data to users  particularly for long-duration transactions that run for minutes or hours unfortunately  we can not allow such output of observable data unless we are willing to compromise transaction atomicity in chapter 26  we discuss alternative transactionmodels that support long-duration  interactive transactions 14.5 transaction isolation transaction-processing systems usually allow multiple transactions to run concurrently allowing multiple transactions to update data concurrently causes several complications with consistency of the data  as we saw earlier ensuring consistency in spite of concurrent execution of transactions requires extra work ; it is far easier to insist that transactions run serially ? that is  one at a time  each starting only after the previous one has completed however  there are two good reasons for allowing concurrency  ? improved throughput and resource utilization a transaction consists of many steps some involve i/o activity ; others involve cpu activity the cpu and the disks in a computer system can operate in parallel therefore  i/o activity can be done in parallel with processing at the cpu the parallelism thesumit67.blogspot.com 636 chapter 14 transactions of the cpu and the i/o system can therefore be exploited to run multiple transactions in parallel while a read or write on behalf of one transaction is in progress on one disk  another transaction can be running in the cpu  while another disk may be executing a read or write on behalf of a third transaction all of this increases the throughput of the system ? that is  the number of transactions executed in a given amount of time correspondingly  the processor and disk utilization also increase ; in other words  the processor and disk spend less time idle  or not performing any useful work ? reduced waiting time there may be a mix of transactions running on a system  some short and some long if transactions run serially  a short transaction may have to wait for a preceding long transaction to complete  which can lead to unpredictable delays in running a transaction if the transactions are operating on different parts of the database  it is better to let them run concurrently  sharing the cpu cycles and disk accesses among them concurrent execution reduces the unpredictable delays in running transactions moreover  it also reduces the average response time  the average time for a transaction to be completed after it has been submitted the motivation for using concurrent execution in a database is essentially the same as the motivation for using multiprogramming in an operating system when several transactions run concurrently  the isolation property may be violated  resulting in database consistency being destroyed despite the correctness of each individual transaction in this section  we present the concept of schedules to help identify those executions that are guaranteed to ensure the isolation property and thus database consistency the database system must control the interaction among the concurrent transactions to prevent themfromdestroying the consistency of the database it does so through a variety of mechanisms called concurrency-control schemes we study concurrency-control schemes in chapter 15 ; for now  we focus on the concept of correct concurrent execution consider again the simplified banking system of section 14.1  which has several accounts  and a set of transactions that access and update those accounts let t1 and t2 be two transactions that transfer funds from one account to another transaction t1 transfers $ 50 from account a to account b it is defined as  t1  read  a  ; a  = a  50 ; write  a  ; read  b  ; b  = b + 50 ; write  b   transaction t2 transfers 10 percent of the balance from account a to account b it is defined as  thesumit67.blogspot.com 14.5 transaction isolation 637 trends in concurrency several current trends in the field of computing are giving rise to an increase in the amount of concurrency possible as database systems exploit this concurrency to increase overall system performance  there will necessarily be an increasing number of transactions run concurrently early computers had only one processor therefore  there was never any real concurrency in the computer the only concurrency was apparent concurrency created by the operating system as it shared the processor among several distinct tasks or processes modern computers are likely to have many processors these may be truly distinct processors all part of the one computer however even a single processor may be able to run more than one process at a time by having multiple cores the intel core duo processor is a well-known example of such a multicore processor for database systems to take advantage of multiple processors and multiple cores  two approaches are being taken one is to find parallelism within a single transaction or query another is to support a very large number of concurrent transactions many service providers now use large collections of computers rather than large mainframe computers to provide their services they are making this choice based on the lower cost of this approach a result of this is yet a further increase in the degree of concurrency that can be supported the bibliographic notes refer to texts that describe these advances in computer architecture and parallel computing chapter 18 describes algorithms for building parallel database systems  which exploitmultiple processors andmultiple cores t2  read  a  ; temp  = a * 0.1 ; a  = a  temp ; write  a  ; read  b  ; b  = b + temp ; write  b   suppose the current values of accounts a and b are $ 1000 and $ 2000  respectively suppose also that the two transactions are executed one at a time in the order t1 followed by t2 this execution sequence appears in figure 14.2 in the figure  the sequence of instruction steps is in chronological order from top to bottom  with instructions of t1 appearing in the left column and instructions of t2 appearing in the right column the final values of accounts a and b  after the execution in figure 14.2 takes place  are $ 855 and $ 2145  respectively thus  the total amount ofmoney in accounts a and b ? that is  the suma + b ? is preserved after the execution of both transactions thesumit67.blogspot.com 638 chapter 14 transactions t1 t2 read  a  a  = a  50 write  a  read  b  b  = b + 50 write  b  commit read  a  temp  = a * 0.1 a  = a  temp write  a  read  b  b  = b + temp write  b  commit figure 14.2 schedule 1 ? a serial schedule in which t1 is followed by t2 similarly  if the transactions are executed one at a time in the order t2 followed by t1  then the corresponding execution sequence is that of figure 14.3 again  as expected  the sum a + b is preserved  and the final values of accounts a and b are $ 850 and $ 2150  respectively t1 t2 read  a  temp  = a * 0.1 a  = a  temp write  a  read  b  b  = b + temp write  b  commit read  a  a  = a  50 write  a  read  b  b  = b + 50 write  b  commit figure 14.3 schedule 2 ? a serial schedule in which t2 is followed by t1 thesumit67.blogspot.com 14.5 transaction isolation 639 the execution sequences just described are called schedules they represent the chronological order in which instructions are executed in the system clearly  a schedule for a set of transactions must consist of all instructions of those transactions  and must preserve the order in which the instructions appear in each individual transaction for example  in transaction t1  the instruction write  a  must appear before the instruction read  b   in any valid schedule note that we include in our schedules the commit operation to indicate that the transaction has entered the committed state in the following discussion  we shall refer to the first execution sequence  t1 followed by t2  as schedule 1  and to the second execution sequence  t2 followed by t1  as schedule 2 these schedules are serial  each serial schedule consists of a sequence of instructions from various transactions  where the instructions belonging to one single transaction appear together in that schedule recalling a well-known formula from combinatorics  we note that  for a set of n transactions  there exist n factorial  n !  different valid serial schedules when the database system executes several transactions concurrently  the corresponding schedule no longer needs to be serial if two transactions are running concurrently  the operating system may execute one transaction for a little while  then perform a context switch  execute the second transaction for some time  and then switch back to the first transaction for some time  and so on with multiple transactions  the cpu time is shared among all the transactions several execution sequences are possible  since the various instructions from both transactions may now be interleaved in general  it is not possible to predict exactly how many instructions of a transaction will be executed before the cpu switches to another transaction.1 returning to our previous example  suppose that the two transactions are executed concurrently one possible schedule appears in figure 14.4 after this execution takes place  we arrive at the same state as the one in which the transactions are executed serially in the order t1 followed by t2 the suma + b is indeed preserved not all concurrent executions result in a correct state to illustrate  consider the schedule of figure 14.5 after the execution of this schedule  we arrive at a state where the final values of accounts a and b are $ 950 and $ 2100  respectively this final state is an inconsistent state  since we have gained $ 50 in the process of the concurrent execution indeed  the sum a + b is not preserved by the execution of the two transactions if control of concurrent execution is left entirely to the operating system  many possible schedules  including ones that leave the database in an inconsistent state  such as the one just described  are possible it is the job of the database system to ensure that any schedule that is executed will leave the database in a consistent state the concurrency-control component of the database system carries out this task 1the number of possible schedules for a set of n transactions is very large there are n ! different serial schedules considering all the possible ways that steps of transactions might be interleaved  the total number of possible schedules is much larger than n !  thesumit67.blogspot.com 640 chapter 14 transactions t1 t2 read  a  a  = a  50 write  a  read  a  temp  = a * 0.1 a  = a  temp write  a  read  b  b  = b + 50 write  b  commit read  b  b  = b + temp write  b  commit figure 14.4 schedule 3 ? a concurrent schedule equivalent to schedule 1 we can ensure consistency of the database under concurrent execution by making sure that any schedule that is executed has the same effect as a schedule that could have occurred without any concurrent execution that is  the schedule should  in some sense  be equivalent to a serial schedule such schedules are called serializable schedules t1 t2 read  a  a  = a  50 read  a  temp  = a * 0.1 a  = a  temp write  a  read  b  write  a  read  b  b  = b + 50 write  b  commit b  = b + temp write  b  commit figure 14.5 schedule 4 ? a concurrent schedule resulting in an inconsistent state thesumit67.blogspot.com 14.6 serializability 641 14.6 serializability before we can consider how the concurrency-control component of the database system can ensure serializability  we consider how to determine when a schedule is serializable certainly  serial schedules are serializable  but if steps of multiple transactions are interleaved  it is harder to determine whether a schedule is serializable since transactions are programs  it is difficult to determine exactly what operations a transaction performs and how operations of various transactions interact for this reason,we shall not consider the various types of operations that a transaction can perform on a data item  but instead consider only two operations  read and write we assume that  between a read  q  instruction and a write  q  instruction on a data item q  a transaction may perform an arbitrary sequence of operations on the copy of q that is residing in the local buffer of the transaction in this model  the only significant operations of a transaction  from a scheduling point of view  are its read and write instructions commit operations  though relevant  are not considered until section 14.7.we therefore may show only read and write instructions in schedules  as we do for schedule 3 in figure 14.6 in this section  we discuss different forms of schedule equivalence  but focus on a particular form called conflict serializability let us consider a schedule s in which there are two consecutive instructions  i and j  of transactions ti and tj  respectively  i  = j   if i and j refer to different data items  then we can swap i and j without affecting the results of any instruction in the schedule however  if i and j refer to the same data item q  then the order of the two steps may matter since we are dealing with only read and write instructions  there are four cases that we need to consider  1 i = read  q   j = read  q   the order of i and j does not matter  since the same value of q is read by ti and tj  regardless of the order 2 i = read  q   j = write  q   if i comes before j  then ti does not read the value of q that is written by tj in instruction j  if j comes before i  then ti reads the value of q that is written by tj thus  the order of i and j matters t1 t2 read  a  write  a  read  a  write  a  read  b  write  b  read  b  write  b  figure 14.6 schedule 3 ? showing only the read and write instructions thesumit67.blogspot.com 642 chapter 14 transactions t1 t2 read  a  write  a  read  a  read  b  write  a  write  b  read  b  write  b  figure 14.7 schedule 5 ? schedule 3 after swapping of a pair of instructions 3 i = write  q   j = read  q   the order of i and j matters for reasons similar to those of the previous case 4 i = write  q   j = write  q   since both instructions are write operations  the order of these instructions does not affect either ti or tj  however  the value obtained by the next read  q  instruction of s is affected  since the result of only the latter of the two write instructions is preserved in the database if there is no other write  q  instruction after i and j in s  then the order of i and j directly affects the final value of q in the database state that results from schedule s thus  only in the case where both i and j are read instructions does the relative order of their execution not matter we say that i and j conflict if they are operations by different transactions on the same data item  and at least one of these instructions is a write operation to illustrate the concept of conflicting instructions  we consider schedule 3in figure 14.6 the write  a  instruction of t1 conflicts with the read  a  instruction of t2 however  the write  a  instruction of t2 does not conflict with the read  b  instruction of t1  because the two instructions access different data items t1 t2 read  a  write  a  read  b  write  b  read  a  write  a  read  b  write  b  figure 14.8 schedule 6 ? a serial schedule that is equivalent to schedule 3 thesumit67.blogspot.com 14.6 serializability 643 t3 t4 read  q  write  q  write  q  figure 14.9 schedule 7 let i and j be consecutive instructions of a schedule s if i and j are instructions of different transactions and i and j do not conflict  then we can swap the order of i and j to produce a new schedule s   s is equivalent to s   since all instructions appear in the same order in both schedules except for i and j  whose order does not matter since the write  a  instruction of t2 in schedule 3 of figure 14.6 does not conflict with the read  b  instruction of t1  we can swap these instructions to generate an equivalent schedule  schedule 5  in figure 14.7 regardless of the initial system state  schedules 3 and 5 both produce the same final system state we continue to swap nonconflicting instructions  ? swap the read  b  instruction of t1 with the read  a  instruction of t2 ? swap the write  b  instruction of t1 with the write  a  instruction of t2 ? swap the write  b  instruction of t1 with the read  a  instruction of t2 the final result of these swaps  schedule 6 of figure 14.8  is a serial schedule note that schedule 6 is exactly the same as schedule 1  but it shows only the read and write instructions thus  we have shown that schedule 3 is equivalent to a serial schedule this equivalence implies that  regardless of the initial system state  schedule 3 will produce the same final state as will some serial schedule if a schedule s can be transformed into a schedule s  by a series of swaps of nonconflicting instructions  we say that s and s  are conflict equivalent.2 not all serial schedules are conflict equivalent to each other for example  schedules 1 and 2 are not conflict equivalent the concept of conflict equivalence leads to the concept of conflict serializability we say that a schedule s is conflict serializable if it is conflict equivalent to a serial schedule thus  schedule 3 is conflict serializable  since it is conflict equivalent to the serial schedule 1 finally  consider schedule 7 of figure 14.9 ; it consists of only the significant operations  that is  the read and write  of transactions t3 and t4 this schedule is not conflict serializable  since it is not equivalent to either the serial schedule < t3,t4 > or the serial schedule < t4,t3 >  2we use the term conflict equivalent to distinguish the way we have just defined equivalence from other definitions that we shall discuss later on in this section thesumit67.blogspot.com 644 chapter 14 transactions  a   b  t1 t2 t2 t1 figure 14.10 precedence graph for  a  schedule 1 and  b  schedule 2 we now present a simple and efficient method for determining conflict serializability of a schedule consider a schedule s we construct a directed graph  called a precedence graph  froms this graph consists of a pair g =  v  e   where v is a set of vertices and e is a set of edges the set of vertices consists of all the transactions participating in the schedule the set of edges consists of all edges ti ? tj for which one of three conditions holds  1 ti executes write  q  before tj executes read  q   2 ti executes read  q  before tj executes write  q   3 ti executes write  q  before tj executes write  q   if an edge ti ? tj exists in the precedence graph  then  in any serial schedule s  equivalent to s  ti must appear before tj  for example  the precedence graph for schedule 1 in figure 14.10a contains the single edge t1 ? t2  since all the instructions of t1 are executed before the first instruction of t2 is executed similarly  figure 14.10b shows the precedence graph for schedule 2 with the single edge t2 ? t1  since all the instructions of t2 are executed before the first instruction of t1 is executed the precedence graph for schedule 4 appears in figure 14.11 it contains the edge t1 ? t2  because t1 executes read  a  before t2 executes write  a   it also contains the edge t2 ? t1  because t2 executes read  b  before t1 executes write  b   if the precedence graph for s has a cycle  then schedule s is not conflict serializable if the graph contains no cycles  then the schedule s is conflict serializable a serializability order of the transactions can be obtained by finding a linear order consistent with the partial order of the precedence graph this process is called topological sorting there are  in general  several possible linear orders that t1 t2 figure 14.11 precedence graph for schedule 4 thesumit67.blogspot.com 14.6 serializability 645  b   c   a  tm tk tk tk tj ti tm tj ti tm ti tj figure 14.12 illustration of topological sorting can be obtained through a topological sort for example  the graph of figure 14.12a has the two acceptable linear orderings shown in figures 14.12b and 14.12c thus  to test for conflict serializability  we need to construct the precedence graph and to invoke a cycle-detection algorithm cycle-detection algorithms can be found in standard textbooks on algorithms cycle-detection algorithms  such as those based on depth-first search  require on the order of n2 operations  where n is the number of vertices in the graph  that is  the number of transactions   returning to our previous examples  note that the precedence graphs for schedules 1 and 2  figure 14.10  indeed do not contain cycles the precedence graph for schedule 4  figure 14.11   on the other hand  contains a cycle  indicating that this schedule is not conflict serializable it is possible to have two schedules that produce the same outcome  but that are not conflict equivalent for example  consider transaction t5  which transfers $ 10 from account b to account a let schedule 8 be as defined in figure 14.13 we claim that schedule 8 is not conflict equivalent to the serial schedule < t1,t5 >  since  in schedule 8  the write  b  instruction of t5 conflicts with the read  b  instruction of t1 this creates an edge t5 ? t1 in the precedence graph similarly  we see that the write  a  instruction of t1 conflicts with the read instruction of t5 thesumit67.blogspot.com 646 chapter 14 transactions t1 t5 read  a  a  = a  50 write  a  read  b  b  = b  10 write  b  read  b  b  = b + 50 write  b  read  a  a  = a + 10 write  a  figure 14.13 schedule 8 creating an edge t1 ? t5 this shows that the precedence graph has a cycle and that schedule 8 is not serializable however  the final values of accounts a and b after the execution of either schedule 8 or the serial schedule < t1,t5 > are the same ? $ 960 and $ 2040  respectively we can see fromthis example that there are less-stringent definitions of schedule equivalence than conflict equivalence for the system to determine that schedule 8 produces the same outcome as the serial schedule < t1,t5 >  it must analyze the computation performed by t1 and t5  rather than just the read and write operations in general  such analysis is hard to implement and is computationally expensive in our example  the final result is the same as that of a serial schedule because of the mathematical fact that addition and subtraction are commutative while this may be easy to see in our simple example  the general case is not so easy since a transaction may be expressed as a complex sql statement  a java program with jdbc calls  etc however  there are other definitions of schedule equivalence based purely on the read and write operations one such definition is view equivalence  a definition that leads to the concept of view serializability view serializability is not used in practice due to its high degree of computational complexity.3 we therefore defer discussion of view serializability to chapter 15  but  for completeness  note here that the example of schedule 8 is not view serializable 14.7 transaction isolation and atomicity so far  we have studied schedules while assuming implicitly that there are no transaction failures we now address the effect of transaction failures during concurrent execution 3testing for view serializability has been proven to be np-complete  which means that it is virtually certain that no efficient test for view serializability exists thesumit67.blogspot.com 14.7 transaction isolation and atomicity 647 t6 t7 read  a  write  a  read  a  commit read  b  figure 14.14 schedule 9  a nonrecoverable schedule if a transaction ti fails  for whatever reason  we need to undo the effect of this transaction to ensure the atomicity property of the transaction in a system that allows concurrent execution  the atomicity property requires that any transaction tj that is dependent on ti  that is  tj has read data written by ti  is also aborted to achieve this  we need to place restrictions on the type of schedules permitted in the system in the following two subsections  we address the issue of what schedules are acceptable from the viewpoint of recovery from transaction failure we describe in chapter 15 how to ensure that only such acceptable schedules are generated 14.7.1 recoverable schedules consider the partial schedule 9 in figure 14.14  in which t7 is a transaction that performs only one instruction  read  a   we call this a partial schedule because we have not included a commit or abort operation for t6 notice that t7 commits immediately after executing the read  a  instruction thus  t7 commits while t6 is still in the active state now suppose that t6 fails before it commits t7 has read the value of data item a written by t6 therefore  we say that t7 is dependent on t6 because of this  we must abort t7 to ensure atomicity however  t7 has already committed and can not be aborted thus  we have a situation where it is impossible to recover correctly from the failure of t6 schedule 9 is an example of a nonrecoverable schedule.arecoverable schedule is one where  for each pair of transactions ti and tj such that tj reads a data item previously written by ti  the commit operation of ti appears before the commit operation of tj  for the example of schedule 9 to be recoverable  t7 would have to delay committing until after t6 commits 14.7.2 cascadeless schedules even if a schedule is recoverable  to recover correctly from the failure of a transaction ti  we may have to roll back several transactions such situations occur if transactions have read data written by ti  as an illustration  consider the partial schedule of figure 14.15 transaction t8 writes a value of a that is read by transaction t9 transaction t9 writes a value of a that is read by transaction t10 suppose that  at this point  t8 fails t8 must be rolled back since t9 is dependent on t8  t9 must be rolled back since t10 is dependent on t9  t10 must be rolled back this thesumit67.blogspot.com 648 chapter 14 transactions t8 t9 t10 read  a  read  b  write  a  read  a  write  a  read  a  abort figure 14.15 schedule 10 phenomenon  in which a single transaction failure leads to a series of transaction rollbacks  is called cascading rollback cascading rollback is undesirable  since it leads to the undoing of a significant amount of work it is desirable to restrict the schedules to those where cascading rollbacks can not occur such schedules are called cascadeless schedules formally  a cascadeless schedule is one where  for each pair of transactions ti and tj such that tj reads a data item previously written by ti  the commit operation of ti appears before the read operation of tj  it is easy to verify that every cascadeless schedule is also recoverable 14.8 transaction isolation levels serializability is a useful concept because it allows programmers to ignore issues related to concurrency when they code transactions if every transaction has the property that it maintains database consistency if executed alone  then serializability ensures that concurrent executions maintain consistency however  the protocols required to ensure serializability may allow too little concurrency for certain applications in these cases  weaker levels of consistency are used the use of weaker levels of consistency places additional burdens on programmers for ensuring database correctness the sql standard also allows a transaction to specify that itmaybe executed in such a way that it becomes nonserializable with respect to other transactions for instance  a transaction may operate at the isolation level of read uncommitted  which permits the transaction to read a data item even if it was written by a transaction that has not been committed sql provides such features for the benefit of long transactions whose results do not need to be precise if these transactions were to execute in a serializable fashion  they could interfere with other transactions  causing the others ? execution to be delayed the isolation levels specified by the sql standard are as follows  ? serializable usually ensures serializable execution however  as we shall explain shortly  some database systems implement this isolation level in a manner that may  in certain cases  allow nonserializable executions thesumit67.blogspot.com 14.8 transaction isolation levels 649 ? repeatable read allows only committed data to be read and further requires that  between two reads of a data item by a transaction  no other transaction is allowed to update it however  the transaction may not be serializable with respect to other transactions for instance  when it is searching for data satisfying some conditions  a transaction may find some of the data inserted by a committed transaction  but may not find other data inserted by the same transaction ? read committed allows only committed data to be read  but does not require repeatable reads for instance  between two reads of a data item by the transaction  another transaction may have updated the data item and committed ? read uncommitted allows uncommitted data to be read it is the lowest isolation level allowed by sql all the isolation levels above additionally disallow dirty writes  that is  they disallowwrites to a data item that has already been written by another transaction that has not yet committed or aborted many database systems run  by default  at the read-committed isolation level in sql  it is possible to set the isolation level explicitly  rather than accepting the system ? s default setting for example  the statement ? set transaction isolation level serializable ; ? sets the isolation level to serializable ; any of the other isolation levels may be specified instead the above syntax is supported by oracle  postgresql and sql server ; db2 uses the syntax ? change isolation level  ? with its own abbreviations for isolation levels changing of the isolation level must be done as the first statement of a transaction further  automatic commit of individual statements must be turned off  if it is on by default ; api functions  such as the jdbc method connection setautocommit  false  which we saw in section 5.1.1.7  can be used to do so further  in jdbc the method connection.settransactionisolation  int level  can be used to set the isolation level ; see the jdbc manuals for details an application designermay decide to accept aweaker isolation level in order to improve system performance as we shall see in section 14.9 and chapter 15  ensuring serializability may force a transaction to wait for other transactions or  in some cases  to abort because the transaction can no longer be executed as part of a serializable execution while it may seem shortsighted to risk database consistency for performance  this trade-off makes sense if we can be sure that the inconsistency that may occur is not relevant to the application there are many means of implementing isolation levels as long as the implementation ensures serializability  the designer of a database application or a user of an application does not need to know the details of such implementations  except perhaps for dealing with performance issues unfortunately  even if the isolation level is set to serializable  some database systems actually implement a weaker level of isolation  which does not rule out every possible nonserializable execution ; we revisit this issue in section 14.9 if weaker levels of isolation are used  either explicitly or implicitly  the application designer has to be aware of some details of the implementation  to avoid or minimize the chance of inconsistency due to lack of serializability thesumit67.blogspot.com 650 chapter 14 transactions serializability in the real world serializable schedules are the ideal way to ensure consistency  but in our dayto day lives  we don ? t impose such stringent requirements a web site offering goods for sale may list an item as being in stock  yet by the time a user selects the item and goes through the checkout process  that item might no longer be available viewed from a database perspective  this would be a nonrepeatable read as another example  consider seat selection for air travel assume that a traveler has already booked an itinerary and now is selecting seats for each flight many airline web sites allow the user to step through the various flights and choose a seat  after which the user is asked to confirm the selection it could be that other travelers are selecting seats or changing their seat selections for the same flights at the same time the seat availability that the traveler was shown is thus actually changing  but the traveler is shown a snapshot of the seat availability as of when the traveler started the seat selection process even if two travelers are selecting seats at the same time  most likely they will select different seats  and if so there would be no real conflict however  the transactions are not serializable  since each traveler has read data that was subsequently updated by the other traveler  leading to a cycle in the precedence graph if two travelers performing seat selection concurrently actually selected the same seat  one of them would not be able to get the seat they selected ; however  the situation could be easily resolved by asking the traveler to perform the selection again  with updated seat availability information it is possible to enforce serializability by allowing only one traveler to do seat selection for a particular flight at a time however  doing so could cause significant delays as travelers would have to wait for their flight to become available for seat selection ; in particular a traveler who takes a long time to make a choice could cause serious problems for other travelers instead  any such transaction is typically broken up into a part that requires user interaction  and a part that runs exclusively on the database in the example above  the database transaction would check if the seats chosen by the user are still available  and if so update the seat selection in the database serializability is ensured only for the transactions that run on the database  without user interaction 14.9 implementation of isolation levels so far  we have seen what properties a schedule must have if it is to leave the database in a consistent state and allow transaction failures to be handled in a safe manner there are various concurrency-control policies that we can use to ensure that  even when multiple transactions are executed concurrently  only acceptable schedules are generated  regardless of how the operating system time-shares resources  such as cpu time  among the transactions thesumit67.blogspot.com 14.9 implementation of isolation levels 651 as a trivial example of a concurrency-control policy  consider this  a transaction acquires a lock on the entire database before it starts and releases the lock after it has committed while a transaction holds a lock  no other transaction is allowed to acquire the lock  and all must thereforewait for the lock to be released as a result of the locking policy  only one transaction can execute at a time therefore  only serial schedules are generated these are trivially serializable  and it is easy to verify that they are recoverable and cascadeless as well a concurrency-control policy such as this one leads to poor performance  since it forces transactions to wait for preceding transactions to finish before they can start in otherwords  it provides a poor degree of concurrency  indeed  no concurrency at all   as we saw in section 14.5  concurrent execution has substantial performance benefits the goal of concurrency-control policies is to provide a high degree of concurrency  while ensuring that all schedules that can be generated are conflict or view serializable  recoverable  and cascadeless here we provide an overview of how some of most important concurrencycontrol mechanisms work  and we defer the details to chapter 15 14.9.1 locking instead of locking the entire database  a transaction could  instead  lock only those data items that it accesses under such a policy  the transaction must hold locks long enough to ensure serializability  but for a period short enough not to harm performance excessively complicating matters are sql statements like those we saw in section 14.10  where the data items accessed depend on a where clause in chapter 15  we present the two-phase locking protocol  a simple  widely used technique that ensures serializability stated simply  two-phase locking requires a transaction to have two phases  one where it acquires locks but does not release any  and a second phase where the transaction releases locks but does not acquire any  in practice  locks are usually released only when the transaction completes its execution and has been either committed or aborted  further improvements to locking result if we have two kinds of locks  shared and exclusive shared locks are used for data that the transaction reads and exclusive locks are used for those it writes many transactions can hold shared locks on the same data item at the same time  but a transaction is allowed an exclusive lock on a data item only if no other transaction holds any lock  regardless of whether shared or exclusive  on the data item this use of two modes of locks along with two-phase locking allows concurrent reading of data while still ensuring serializability 14.9.2 timestamps another category of techniques for the implementation of isolation assigns each transaction a timestamp  typically when it begins for each data item  the system keeps two timestamps the read timestamp of a data item holds the largest  that is  the most recent  timestamp of those transactions that read the data item the write timestamp of a data item holds the timestamp of the transaction that thesumit67.blogspot.com 652 chapter 14 transactions wrote the current value of the data item timestamps are used to ensure that transactions access each data item in order of the transactions ? timestamps if their accesses conflict when this is not possible  offending transactions are aborted and restarted with a new timestamp 14.9.3 multiple versions and snapshot isolation by maintaining more than one version of a data item  it is possible to allow a transaction to read an old version of a data item rather than a newer version written by an uncommitted transaction or by a transaction that should come later in the serialization order there are a variety of multiversion concurrencycontrol techniques one in particular  called snapshot isolation  is widely used in practice in snapshot isolation  we can imagine that each transaction is given its own version  or snapshot  of the database when it begins.4 it reads data from this private version and is thus isolated fromthe updates made by other transactions if the transaction updates the database  that update appears only in its own version  not in the actual database itself information about these updates is saved so that the updates can be applied to the ? real ? database if the transaction commits when a transaction t enters the partially committed state  it then proceeds to the committed state only if no other concurrent transaction has modified data that t intends to update transactions that  as a result  can not commit abort instead snapshot isolation ensures that attempts to read data never need to wait  unlike locking   read-only transactions can not be aborted ; only those thatmodify data run a slight risk of aborting since each transaction reads its own version or snapshot of the database  reading data does not cause subsequent update attempts by other transactions to wait  unlike locking   since most transactions are read-only  and most others read more data than they update   this is often a major source of performance improvement as compared to locking the problem with snapshot isolation is that  paradoxically  it provides too much isolation consider two transactions t and t   in a serializable execution  either t sees all the updates made by t  or t  sees all the updates made by t  because one must follow the other in the serialization order under snapshot isolation  there are cases where neither transaction sees the updates of the other this is a situation that can not occur in a serializable execution in many  indeed  most  cases  the data accesses by the two transactions do not conflict and there is no problem however  if t reads some data item that t  updates and t  reads some data item that t updates  it is possible that both transactions fail to read the update made by the other the result  as we shall see in chapter 15  may be an inconsistent database state that  of course  could not be obtained in any serializable execution 4of course  in reality  the entire database is not copied multiple versions are kept only of those data items that are changed thesumit67.blogspot.com 14.10 transactions as sql statements 653 oracle  postgresql  and sql server offer the option of snapshot isolation oracle and postgresql implement the serializable isolation level using snapshot isolation as a result  their implementation of serializability can  in exceptional circumstances  result in a nonserializable execution being allowed sql server instead includes an additional isolation level beyond the standard ones  called snapshot  to offer the option of snapshot isolation 14.10 transactions as sql statements in section 4.3  we presented the sql syntax for specifying the beginning and end of transactions now that we have seen some of the issues in ensuring the acid properties for transactions  we are ready to consider how those properties are ensured when transactions are specified as a sequence of sql statements rather than the restricted model of simple reads and writes that we considered up to this point in our simplemodel  we assumed a set of data items exists.while our simple model allowed data-item values to be changed  it did not allow data items to be created or deleted in sql  however  insert statements create new data and delete statements delete data these two statements are  in effect  write operations  since they change the database  but their interactions with the actions of other transactions are different from what we saw in our simple model as an example  consider the following sql query on our university database that finds all instructors who earn more than $ 90,000 select id  name from instructor where salary > 90000 ; using our sample instructor relation  appendix a.3   we find that only einstein and brandt satisfy the condition now assume that around the same time we are running our query  another user inserts a new instructor named ? james ? whose salary is $ 100,000 insert into instructor values  ? 11111 ?  ? james ?  ? marketing ?  100000  ; the result of our query will be different depending on whether this insert comes before or after our query is run in a concurrent execution of these transactions  it is intuitively clear that they conflict  but this is a conflict not captured by our simple model this situation is referred to as the phantom phenomenon  because a conflict may exist on ? phantom ? data our simple model of transactions required that operations operate on a specific data item given as an argument to the operation in our simple model  we can look at the read and write steps to see which data items are referenced but in an sql statement  the specific data items  tuples  referenced may be determined by a where clause predicate so the same transaction  if run more than once  might thesumit67.blogspot.com 654 chapter 14 transactions reference different data items each time it is run if the values in the database change between runs one way of dealing with the above problem is to recognize that it is not sufficient for concurrency control to consider only the tuples that are accessed by a transaction ; the information used to find the tuples that are accessed by the transaction must also be considered for the purpose of concurrency control the information used to find tuples could be updated by an insertion or deletion  or in the case of an index  even by an update to a search-key attribute for example  if locking is used for concurrency control  the data structures that track the tuples in a relation  as well as index structures  must be appropriately locked however  such locking can lead to poor concurrency in some situations ; index-locking protocols which maximize concurrency  while ensuring serializability in spite of inserts  deletes  and predicates in queries  are discussed in section 15.8.3 let us consider again the query  select id  name from instructor where salary > 90000 ; and the following sql update  update instructor set salary = salary * 0.9 where name = ? wu ? ; we now face an interesting situation in determining whether our query conflicts with the update statement if our query reads the entire instructor relation  then it reads the tuple with wu ? s data and conflicts with the update however  if an index were available that allowed our query direct access to those tuples with salary > 90000  then our query would not have accessedwu ? s data at all because wu ? s salary is initially $ 90,000 in our example instructor relation  and reduces to $ 81,000 after the update however  using the above approach  it would appear that the existence of a conflict depends on a low-level query processing decision by the system that is unrelated to a user-level view of the meaning of the two sql statements ! an alternative approach to concurrency control treats an insert  delete or update as conflicting with a predicate on a relation  if it could affect the set of tuples selected by a predicate in our example query above  the predicate is ? salary > 90000 ?  and an update of wu ? s salary from $ 90,000 to a value greater than $ 90,000  or an update of einstein ? s salary from a value greater that $ 90,000 to a value less than or equal to $ 90,000  would conflict with this predicate locking based on this idea is called predicate locking ; however predicate locking is expensive  and not used in practice thesumit67.blogspot.com 14.11 summary 655 14.11 summary ? atransaction is a unit of program execution that accesses and possibly updates various data items understanding the concept of a transaction is critical for understanding and implementing updates of data in a database in such a way that concurrent executions and failures of various forms do not result in the database becoming inconsistent ? transactions are required to have the acid properties  atomicity  consistency  isolation  and durability ? atomicity ensures that either all the effects of a transaction are reflected in the database  or none are ; a failure can not leave the database in a state where a transaction is partially executed ? consistency ensures that  if the database is initially consistent  the execution of the transaction  by itself  leaves the database in a consistent state ? isolation ensures that concurrently executing transactions are isolated from one another  so that each has the impression that no other transaction is executing concurrently with it ? durability ensures that  once a transaction has been committed  that transaction ? s updates do not get lost  even if there is a system failure ? concurrent execution of transactions improves throughput of transactions and system utilization  and also reduces waiting time of transactions ? the various types of storage in a computer are volatile storage  nonvolatile storage  and stable storage data in volatile storage  such as in ram  are lost when the computer crashes data in nonvolatile storage  such as disk  are not lost when the computer crashes  but may occasionally be lost because of failures such as disk crashes data in stable storage are never lost ? stable storage that must be accessible online is approximated with mirrored disks  or other forms of raid,which provide redundant data storage offline  or archival  stable storage may consist of multiple tape copies of data stored in physically secure locations ? when several transactions execute concurrently on the database  the consistency of data may no longer be preserved it is therefore necessary for the system to control the interaction among the concurrent transactions ? since a transaction is a unit that preserves consistency  a serial execution of transactions guarantees that consistency is preserved ? a schedule captures the key actions of transactions that affect concurrent execution  such as read and write operations  while abstracting away internal details of the execution of the transaction thesumit67.blogspot.com 656 chapter 14 transactions ? we require that any schedule produced by concurrent processing of a set of transactionswill have an effect equivalent to a schedule producedwhen these transactions are run serially in some order ? a system that guarantees this property is said to ensure serializability ? there are several different notions of equivalence leading to the concepts of conflict serializability and view serializability ? serializability of schedules generated by concurrently executing transactions can be ensured through one of a variety of mechanisms called concurrencycontrol policies ? we can test a given schedule for conflict serializability by constructing a precedence graph for the schedule  and by searching for absence of cycles in the graph however  there are more efficient concurrency-control policies for ensuring serializability ? schedules must be recoverable  to make sure that if transaction a sees the effects of transaction b  and b then aborts  then a also gets aborted ? schedules should preferably be cascadeless  so that the abort of a transaction does not result in cascading aborts of other transactions cascadelessness is ensured by allowing transactions to only read committed data ? the concurrency-control ? management component of the database is responsible for handling the concurrency-control policies chapter 15 describes concurrency-control policies review terms ? transaction ? acid properties ? atomicity ? consistency ? isolation ? durability ? inconsistent state ? storage types ? volatile storage ? nonvolatile storage ? stable storage ? concurrency control system ? recovery system ? transaction state ? active ? partially committed ? failed ? aborted ? committed ? terminated ? transaction ? restart ? kill thesumit67.blogspot.com practice exercises 657 ? observable external writes ? concurrent executions ? serial execution ? schedules ? conflict of operations ? conflict equivalence ? conflict serializability ? serializability testing ? precedence graph ? serializability order ? recoverable schedules ? cascading rollback ? cascadeless schedules ? concurrency-control ? locking ? multiple versions ? snapshot isolation practice exercises 14.1 suppose that there is a database system that never fails is a recovery manager required for this system ? 14.2 consider a file system such as the one on your favorite operating system a what are the steps involved in creation and deletion of files  and in writing data to a file ? b explain how the issues of atomicity and durability are relevant to the creation and deletion of files and to writing data to files 14.3 database-system implementers have paid much more attention to the acid properties than have file-system implementers why might this be the case ? 14.4 justify the following statement  concurrent execution of transactions is more important when data must be fetched from  slow  disk or when transactions are long  and is less important when data are inmemory and transactions are very short 14.5 since every conflict-serializable schedule is view serializable  why do we emphasize conflict serializability rather than view serializability ? 14.6 consider the precedence graph of figure 14.16 is the corresponding schedule conflict serializable ? explain your answer 14.7 what is a cascadeless schedule ? why is cascadelessness of schedules desirable ? are there any circumstances under which it would be desirable to allow noncascadeless schedules ? explain your answer 14.8 the lost update anomaly is said to occur if a transaction tj reads a data item  then another transaction tk writes the data item  possibly based on a previous read   afterwhich tj writes the data item.the update performed by tk has been lost  since the update done by tj ignored the value written by tk  thesumit67.blogspot.com 658 chapter 14 transactions t1 t4 t5 t3 t2 figure 14.16 precedence graph for practice exercise 14.6 a give an example of a schedule showing the lost update anomaly b give an example schedule to show that the lost update anomaly is possible with the read committed isolation level c explain why the lost update anomaly is not possible with the repeatable read isolation level 14.9 consider a database for a bank where the database system uses snapshot isolation describe a particular scenario in which a nonserializable execution occurs that would present a problem for the bank 14.10 consider a database for an airline where the database system uses snapshot isolation describe a particular scenario in which a nonserializable execution occurs  but the airline may be willing to accept it in order to gain better overall performance 14.11 the definition of a schedule assumes that operations can be totally ordered by time consider a database system that runs on a system with multiple processors  where it is not always possible to establish an exact ordering between operations that executed on different processors however  operations on a data item can be totally ordered does the above situation cause any problemfor the definition of conflict serializability ? explain your answer exercises 14.12 list the acid properties explain the usefulness of each 14.13 during its execution  a transaction passes through several states  until it finally commits or aborts list all possible sequences of states through thesumit67.blogspot.com exercises 659 which a transaction may pass explain why each state transition may occur 14.14 explain the distinction between the terms serial schedule and serializable schedule 14.15 consider the following two transactions  t13  read  a  ; read  b  ; if a = 0 then b  = b + 1 ; write  b   t14  read  b  ; read  a  ; if b = 0 then a  = a + 1 ; write  a   let the consistency requirement be a = 0 ? b = 0  with a = b = 0 the initial values a show that every serial execution involving these two transactions preserves the consistency of the database b show a concurrent execution of t13 and t14 that produces a nonserializable schedule c is there a concurrent execution of t13 and t14 that produces a serializable schedule ? 14.16 give an example of a serializable schedule with two transactions such that the order in which the transactions commit is different from the serialization order 14.17 what is a recoverable schedule ? why is recoverability of schedules desirable ? are there any circumstances under which it would be desirable to allow nonrecoverable schedules ? explain your answer 14.18 why do database systems support concurrent execution of transactions  in spite of the extra programming effort needed to ensure that concurrent execution does not cause any problems ? 14.19 explain why the read-committed isolation level ensures that schedules are cascade-free 14.20 for each of the following isolation levels  give an example of a schedule that respects the specified level of isolation  but is not serializable  a read uncommitted b read committed c repeatable read thesumit67.blogspot.com 660 chapter 14 transactions 14.21 suppose that in addition to the operations read and write  we allow an operation pred read  r  p   which reads all tuples in relation r that satisfy predicate p a give an example of a schedule using the pred read operation that exhibits the phantom phenomenon  and is nonserializable as a result b give an example of a schedule where one transaction uses the pred read operation on relation r and another concurrent transactions deletes a tuple from r  but the schedule does not exhibit a phantom conflict  to do so  you have to give the schema of relation r  and show the attribute values of the deleted tuple  bibliographical notes gray and reuter  1993  provides detailed textbook coverage of transactionprocessing concepts  techniques and implementation details  including concurrency control and recovery issues bernstein and newcomer  1997  provides textbook coverage of various aspects of transaction processing the concept of serializability was formalized by eswaran et al  1976  in connection to work on concurrency control for system r references covering specific aspects of transaction processing  such as concurrency control and recovery  are cited in chapters 15  16  and 26 thesumit67.blogspot.com chapter15 concurrency control we saw in chapter 14 that one of the fundamental properties of a transaction is isolation when several transactions execute concurrently in the database  however  the isolation property may no longer be preserved to ensure that it is  the system must control the interaction among the concurrent transactions ; this control is achieved through one of a variety of mechanisms called concurrencycontrol schemes in chapter 26  we discuss concurrency-control schemes that admit nonserializable schedules in this chapter  we consider the management of concurrently executing transactions  and we ignore failures in chapter 16  we shall see how the system can recover from failures as we shall see  there are a variety of concurrency-control schemes no one scheme is clearly the best ; each one has advantages in practice  the most frequently used schemes are two-phase locking and snapshot isolation 15.1 lock-based protocols oneway to ensure isolation is to require that data items be accessed in a mutually exclusive manner ; that is  while one transaction is accessing a data item  no other transaction can modify that data item the most common method used to implement this requirement is to allow a transaction to access a data item only if it is currently holding a lock on that item we introduced the concept of locking in section 14.9 15.1.1 locks there are various modes in which a data item may be locked in this section  we restrict our attention to two modes  1 shared if a transaction ti has obtained a shared-mode lock  denoted by s  on item q  then ti can read  but can not write  q 2 exclusive if a transaction ti has obtained an exclusive-mode lock  denoted by x  on item q  then ti can both read and write q 661 thesumit67.blogspot.com 662 chapter 15 concurrency control s x s true false x false false figure 15.1 lock-compatibility matrix comp we require that every transaction request a lock in an appropriate mode on data item q  depending on the types of operations that it will perform on q the transaction makes the request to the concurrency-control manager the transaction can proceed with the operation only after the concurrency-control manager grants the lock to the transaction the use of these two lock modes allows multiple transactions to read a data item but limits write access to just one transaction at a time to state this more generally  given a set of lock modes  we can define a compatibility function on them as follows  let a and b represent arbitrary lock modes suppose that a transaction ti requests a lock of mode a on item q on which transaction tj  ti  = tj  currently holds a lock of mode b if transaction ti can be granted a lock on q immediately  in spite of the presence of the mode b lock  then we say mode a is compatible with mode b such a function can be represented conveniently by a matrix the compatibility relation between the two modes of locking discussed in this section appears in the matrix comp of figure 15.1 an element comp  a  b  of the matrix has the value true if and only if mode a is compatible with mode b note that sharedmode is compatible with shared mode  but notwith exclusive mode at any time  several shared-mode locks can be held simultaneously  by different transactions  on a particular data item a subsequent exclusive-mode lock request has to wait until the currently held shared-mode locks are released a transaction requests a shared lock on data item q by executing the lock s  q  instruction similarly  a transaction requests an exclusive lock through the lock-x  q  instruction a transaction can unlock a data item q by the unlock  q  instruction to access a data item  transaction ti must first lock that item if the data item is already locked by another transaction in an incompatible mode  the concurrencycontrol manager will not grant the lock until all incompatible locks held by other transactions have been released thus  ti is made to wait until all incompatible locks held by other transactions have been released transaction ti may unlock a data item that it had locked at some earlier point note that a transaction must hold a lock on a data item as long as it accesses that item moreover  it is not necessarily desirable for a transaction to unlock a data item immediately after its final access of that data item  since serializability may not be ensured as an illustration  consider again the banking example that we introduced in chapter 14 let a and b be two accounts that are accessed by transactions t1 thesumit67.blogspot.com 15.1 lock-based protocols 663 t1  lock-x  b  ; read  b  ; b  = b  50 ; write  b  ; unlock  b  ; lock-x  a  ; read  a  ; a  = a + 50 ; write  a  ; unlock  a   figure 15.2 transaction t1 and t2 transaction t1 transfers $ 50 from account b to account a  figure 15.2   transaction t2 displays the total amount of money in accounts a and b ? that is  the sum a + b  figure 15.3   suppose that the values of accounts a and b are $ 100 and $ 200  respectively if these two transactions are executed serially  either in the order t1  t2 or the order t2  t1  then transaction t2 will display the value $ 300 if  however  these transactions are executed concurrently  then schedule 1  in figure 15.4  is possible in this case  transaction t2 displays $ 250  which is incorrect the reason for this mistake is that the transaction t1 unlocked data item b too early  as a result of which t2 saw an inconsistent state the schedule shows the actions executed by the transactions  as well as the points at which the concurrency-control manager grants the locks the transaction making a lock request can not execute its next action until the concurrencycontrol manager grants the lock hence  the lock must be granted in the interval of time between the lock-request operation and the following action of the transaction exactly when within this interval the lock is granted is not important ; we can safely assume that the lock is granted just before the following action of the transaction we shall therefore drop the column depicting the actions of the concurrency-control manager from all schedules depicted in the rest of the chapter.we let you infer when locks are granted t2  lock-s  a  ; read  a  ; unlock  a  ; lock-s  b  ; read  b  ; unlock  b  ; display  a + b   figure 15.3 transaction t2 thesumit67.blogspot.com 664 chapter 15 concurrency control t1 t2 concurreny-control manager lock-x  b  grant-x  b  t1  read  b  b  = b  50 write  b  unlock  b  lock-s  a  grant-s  a  t2  read  a  unlock  a  lock-s  b  grant-s  b  t2  read  b  unlock  b  display  a + b  lock-x  a  grant-x  a  t1  read  a  a  = a  50 write  a  unlock  a  figure 15.4 schedule 1 suppose now that unlocking is delayed to the end of the transaction transaction t3 corresponds to t1 with unlocking delayed  figure 15.5   transaction t4 corresponds to t2 with unlocking delayed  figure 15.6   you should verify that the sequence of reads and writes in schedule 1  which lead to an incorrect total of $ 250 being displayed  is no longer possible with t3 t3  lock-x  b  ; read  b  ; b  = b  50 ; write  b  ; lock-x  a  ; read  a  ; a  = a + 50 ; write  a  ; unlock  b  ; unlock  a   figure 15.5 transaction t3  transaction t1 with unlocking delayed   thesumit67.blogspot.com 15.1 lock-based protocols 665 t4  lock-s  a  ; read  a  ; lock-s  b  ; read  b  ; display  a + b  ; unlock  a  ; unlock  b   figure 15.6 transaction t4  transaction t2 with unlocking delayed   and t4 other schedules are possible t4 will not print out an inconsistent result in any of them ; we shall see why later unfortunately  locking can lead to an undesirable situation consider the partial schedule of figure 15.7 for t3 and t4 since t3 is holding an exclusivemode lock on b and t4 is requesting a shared-mode lock on b  t4 is waiting for t3 to unlock b similarly  since t4 is holding a shared-mode lock on a and t3 is requesting an exclusive-mode lock on a  t3 is waiting for t4 to unlock a thus,we have arrived at a state where neither of these transactions can ever proceed with its normal execution this situation is called deadlock when deadlock occurs  the system must roll back one of the two transactions once a transaction has been rolled back  the data items that were locked by that transaction are unlocked these data items are then available to the other transaction  which can continue with its execution we shall return to the issue of deadlock handling in section 15.2 if we do not use locking  or if we unlock data items too soon after reading or writing them  we may get inconsistent states on the other hand  if we do not unlock a data item before requesting a lock on another data item  deadlocks may occur there are ways to avoid deadlock in some situations  as we shall see in section 15.1.5 however  in general  deadlocks are a necessary evil associated with locking  if we want to avoid inconsistent states deadlocks are definitely t3 t4 lock-x  b  read  b  b  = b  50 write  b  lock-s  a  read  a  lock-s  b  lock-x  a  figure 15.7 schedule 2 thesumit67.blogspot.com 666 chapter 15 concurrency control preferable to inconsistent states  since they can be handled by rolling back transactions  whereas inconsistent states may lead to real-world problems that can not be handled by the database system we shall require that each transaction in the system follow a set of rules  called a locking protocol  indicatingwhen a transactionmay lock and unlock each of the data items locking protocols restrict the number of possible schedules the set of all such schedules is a proper subset of all possible serializable schedules.we shall present several locking protocols that allow only conflict-serializable schedules  and thereby ensure isolation before doing so  we introduce some terminology let  t0  t1      tn  be a set of transactions participating in a schedule s we say that ti precedes tj in s  written ti ? tj  if there exists a data item q such that ti has held lock mode a on q  and tj has held lock mode b on q later  and comp  a,b  = false if ti ? tj  then that precedence implies that in any equivalent serial schedule  ti must appear before tj  observe that this graph is similar to the precedence graph that we used in section 14.6 to test for conflict serializability conflicts between instructions correspond to noncompatibility of lock modes we say that a schedule s is legal under a given locking protocol if s is a possible schedule for a set of transactions that follows the rules of the locking protocol.we say that a locking protocol ensures conflict serializability if and only if all legal schedules are conflict serializable ; in other words  for all legal schedules the associated ? relation is acyclic 15.1.2 granting of locks when a transaction requests a lock on a data item in a particular mode  and no other transaction has a lock on the same data item in a conflicting mode  the lock can be granted however  care must be taken to avoid the following scenario suppose a transaction t2 has a shared-mode lock on a data item  and another transaction t1 requests an exclusive-mode lock on the data item clearly  t1 has to wait for t2 to release the shared-mode lock meanwhile  a transaction t3 may request a shared-mode lock on the same data item the lock request is compatible with the lock granted to t2  so t3 may be granted the shared-mode lock at this point t2 may release the lock  but still t1 has to wait for t3 to finish but again  there may be a new transaction t4 that requests a shared-mode lock on the same data item  and is granted the lock before t3 releases it in fact  it is possible that there is a sequence of transactions that each requests a shared-mode lock on the data item  and each transaction releases the lock a short while after it is granted  but t1 never gets the exclusive-mode lock on the data item the transaction t1 may never make progress  and is said to be starved we can avoid starvation of transactions by granting locks in the following manner  when a transaction ti requests a lock on a data item q in a particular mode m  the concurrency-control manager grants the lock provided that  1 there is no other transaction holding a lock on q in a mode that conflicts with m thesumit67.blogspot.com 15.1 lock-based protocols 667 2 there is no other transaction that is waiting for a lock on q and that made its lock request before ti  thus  a lock request will never get blocked by a lock request that is made later 15.1.3 the two-phase locking protocol one protocol that ensures serializability is the two-phase locking protocol this protocol requires that each transaction issue lock and unlock requests in two phases  1 growing phase a transaction may obtain locks  but may not release any lock 2 shrinking phase a transaction may release locks  but may not obtain any new locks initially  a transaction is in the growing phase the transaction acquires locks as needed once the transaction releases a lock  it enters the shrinking phase  and it can issue no more lock requests for example  transactions t3 and t4 are two phase on the other hand  transactions t1 and t2 are not two phase.note that the unlock instructions do not need to appear at the end of the transaction for example  in the case of transaction t3  we could move the unlock  b  instruction to just after the lock-x  a  instruction  and still retain the two-phase locking property we can show that the two-phase locking protocol ensures conflict serializability consider any transaction the point in the schedule where the transaction has obtained its final lock  the end of its growing phase  is called the lock point of the transaction now  transactions can be ordered according to their lock points ? this ordering is  in fact  a serializability ordering for the transactions we leave the proof as an exercise for you to do  see practice exercise 15.1   two-phase locking does not ensure freedom from deadlock observe that transactions t3 and t4 are two phase  but  in schedule 2  figure 15.7   they are deadlocked recall from section 14.7.2 that  in addition to being serializable  schedules should be cascadeless cascading rollback may occur under two-phase locking as an illustration  consider the partial schedule of figure 15.8 each transaction observes the two-phase locking protocol  but the failure of t5 after the read  a  step of t7 leads to cascading rollback of t6 and t7 cascading rollbacks can be avoided by a modification of two-phase locking called the strict two-phase locking protocol this protocol requires not only that locking be two phase  but also that all exclusive-mode locks taken by a transaction be held until that transaction commits this requirement ensures that any data written by an uncommitted transaction are locked in exclusive mode until the transaction commits  preventing any other transaction from reading the data another variant of two-phase locking is the rigorous two-phase locking protocol  which requires that all locks be held until the transaction commits thesumit67.blogspot.com 668 chapter 15 concurrency control t5 t6 t7 lock-x  a  read  a  lock-s  b  read  b  write  a  unlock  a  lock-x  a  read  a  write  a  unlock  a  lock-s  a  read  a  figure 15.8 partial schedule under two-phase locking we can easily verify that  with rigorous two-phase locking  transactions can be serialized in the order in which they commit consider the following two transactions  forwhich we have shown only some of the significant read and write operations  t8  read  a1  ; read  a2  ;    read  an  ; write  a1   t9  read  a1  ; read  a2  ; display  a1 + a2   if we employ the two-phase locking protocol  then t8 must lock a1 in exclusive mode therefore  any concurrent execution of both transactions amounts to a serial execution notice  however  that t8 needs an exclusive lock on a1 only at the end of its execution  when it writes a1 thus  if t8 could initially lock a1 in shared mode  and then could later change the lock to exclusive mode  we could get more concurrency  since t8 and t9 could access a1 and a2 simultaneously this observation leads us to a refinement of the basic two-phase locking protocol  in which lock conversions are allowed we shall provide a mechanism for upgrading a shared lock to an exclusive lock  and downgrading an exclusive lock to a shared lock we denote conversion from shared to exclusive modes by upgrade  and from exclusive to shared by downgrade lock conversion can not be allowed arbitrarily rather  upgrading can take place in only the growing phase  whereas downgrading can take place in only the shrinking phase thesumit67.blogspot.com 15.1 lock-based protocols 669 t8 t9 lock-s  a1  lock-s  a1  lock-s  a2  lock-s  a2  lock-s  a3  lock-s  a4  unlock  a1  unlock  a2  lock-s  an  upgrade  a1  figure 15.9 incomplete schedule with a lock conversion returning to our example  transactions t8 and t9 can run concurrently under the refined two-phase locking protocol  as shown in the incomplete schedule of figure 15.9  where only some of the locking instructions are shown note that a transaction attempting to upgrade a lock on an item q may be forced to wait this enforced wait occurs if q is currently locked by another transaction in shared mode just like the basic two-phase locking protocol  two-phase locking with lock conversion generates only conflict-serializable schedules  and transactions can be serialized by their lock points further  if exclusive locks are held until the end of the transaction  the schedules are cascadeless for a set of transactions  there may be conflict-serializable schedules that can not be obtained through the two-phase locking protocol however  to obtain conflict-serializable schedules through non-two-phase locking protocols  we need either to have additional information about the transactions or to impose some structure or ordering on the set of data items in the database.we shall see examples when we consider other locking protocols later in this chapter strict two-phase locking and rigorous two-phase locking  with lock conversions  are used extensively in commercial database systems a simple but widely used scheme automatically generates the appropriate lock and unlock instructions for a transaction  on the basis of read and write requests from the transaction  ? when a transaction ti issues a read  q  operation  the system issues a lock s  q  instruction followed by the read  q  instruction ? when ti issues a write  q  operation  the system checks to see whether ti already holds a shared lock on q if it does  then the system issues an upgrade  q  instruction  followed by the write  q  instruction otherwise  the system issues a lock-x  q  instruction  followed by the write  q  instruction ? all locks obtained by a transaction are unlocked after that transaction commits or aborts thesumit67.blogspot.com 670 chapter 15 concurrency control 15.1.4 implementation of locking a lock manager can be implemented as a process that receives messages from transactions and sends messages in reply the lock-manager process replies to lock-request messages with lock-grant messages  or with messages requesting rollback of the transaction  in case of deadlocks   unlock messages require only an acknowledgment in response  but may result in a grant message to another waiting transaction the lock manager uses this data structure  for each data item that is currently locked  it maintains a linked list of records  one for each request  in the order in which the requests arrived it uses a hash table  indexed on the name of a data item  to find the linked list  if any  for a data item ; this table is called the lock table each record of the linked list for a data item notes which transaction made the request  and what lock mode it requested the record also notes if the request has currently been granted figure 15.10 shows an example of a lock table the table contains locks for five different data items  i4  i7  i23  i44  and i912 the lock table uses overflow chaining  so there is a linked list of data items for each entry in the lock table there is also a list of transactions that have been granted locks  or are waiting for locks  for each of the data items granted locks are the rectangles filled in a darker shade  while waiting requests are the rectangles filled in a lighter shade.we have omitted the lock mode to keep the figure simple it can be seen  for example  that t23 has been granted locks on i912 and i7  and is waiting for a lock on i4 although the figure does not show it  the lock table should also maintain an index on transaction identifiers  so that it is possible to determine efficiently the set of locks held by a given transaction the lock manager processes requests this way  ? when a lock request message arrives  it adds a record to the end of the linked list for the data item  if the linked list is present otherwise it creates a new linked list  containing only the record for the request it always grants a lock request on a data item that is not currently locked but if the transaction requests a lock on an item on which a lock is currently held  the lock manager grants the request only if it is compatiblewith the locks that are currently held  and all earlier requests have been granted already otherwise the request has to wait ? when the lock manager receives an unlock message from a transaction  it deletes the record for that data item in the linked list corresponding to that transaction it tests the record that follows  if any  as described in the previous paragraph  to see if that request can nowbe granted if it can  the lock manager grants that request  and processes the record following it  if any  similarly  and so on ? if a transaction aborts  the lock manager deletes any waiting request made by the transaction once the database system has taken appropriate actions to undo the transaction  see section 16.3   it releases all locks held by the aborted transaction thesumit67.blogspot.com 15.1 lock-based protocols 671 granted waiting t8 144 t1 t23 14 t23 17 123 t23 t1 t8 t2 1912 figure 15.10 lock table this algorithm guarantees freedom from starvation for lock requests  since a request can never be granted while a request received earlier is waiting to be granted we study how to detect and handle deadlocks later  in section 15.2.2 section 17.2.1 describes an alternative implementation ? one that uses shared memory instead of message passing for lock request/grant 15.1.5 graph-based protocols as noted in section 15.1.3  ifwe wish to develop protocols that are not two phase  we need additional information on how each transactionwill access the database there are various models that can give us the additional information  each differing in the amount of information provided the simplest model requires that we have prior knowledge about the order in which the database items will be accessed given such information  it is possible to construct locking protocols that are not two phase  but that  nevertheless  ensure conflict serializability to acquire such prior knowledge  we impose a partial ordering ? on the set d =  d1  d2      dh  of all data items if di ? dj  then any transaction accessing thesumit67.blogspot.com 672 chapter 15 concurrency control both di and dj must access di before accessing dj  this partial ordering may be the result of either the logical or the physical organization of the data  or it may be imposed solely for the purpose of concurrency control the partial ordering implies that the set d may now be viewed as a directed acyclic graph  called a database graph in this section  for the sake of simplicity  we will restrict our attention to only those graphs that are rooted trees we shall present a simple protocol  called the tree protocol,which is restricted to employ only exclusive locks references to other  more complex  graph-based locking protocols are in the bibliographical notes in the tree protocol  the only lock instruction allowed is lock-x each transaction ti can lock a data item at most once  and must observe the following rules  1 the first lock by ti may be on any data item 2 subsequently  a data item q can be locked by ti only if the parent of q is currently locked by ti  3 data items may be unlocked at any time 4 a data item that has been locked and unlocked by ti can not subsequently be relocked by ti  all schedules that are legal under the tree protocol are conflict serializable to illustrate this protocol  consider the database graph of figure 15.11 the following four transactions follow the tree protocol on this graph.we show only the lock and unlock instructions  a b c f e h i j d g figure 15.11 tree-structured database graph thesumit67.blogspot.com 15.1 lock-based protocols 673 t10  lock-x  b  ; lock-x  e  ; lock-x  d  ; unlock  b  ; unlock  e  ; lock-x  g  ; unlock  d  ; unlock  g   t11  lock-x  d  ; lock-x  h  ; unlock  d  ; unlock  h   t12  lock-x  b  ; lock-x  e  ; unlock  e  ; unlock  b   t13  lock-x  d  ; lock-x  h  ; unlock  d  ; unlock  h   one possible schedule in which these four transactions participated appears in figure 15.12 note that  during its execution  transaction t10 holds locks on two disjoint subtrees observe that the schedule of figure 15.12 is conflict serializable it can be shown not only that the tree protocol ensures conflict serializability  but also that this protocol ensures freedom from deadlock the tree protocol in figure 15.12 does not ensure recoverability and cascadelessness to ensure recoverability and cascadelessness  the protocol can be modified to not permit release of exclusive locks until the end of the transaction holding exclusive locks until the end of the transaction reduces concurrency here is an alternative that improves concurrency  but ensures only recoverability  for each data item with an uncommitted write  we record which transaction performed the last write to the data item whenever a transaction ti performs a read of an uncommitted data item  we record a commit dependency of ti on the t10 t11 t12 t13 lock-x  b  lock-x  d  lock-x  h  unlock  d  lock-x  e  lock-x  d  unlock  b  unlock  e  lock-x  b  lock-x  e  unlock  h  lock-x  g  unlock  d  lock-x  d  lock-x  h  unlock  d  unlock  h  unlock  e  unlock  b  unlock  g  figure 15.12 serializable schedule under the tree protocol thesumit67.blogspot.com 674 chapter 15 concurrency control transaction that performed the last write to the data item transaction ti is then not permitted to commit until the commit of all transactions on which it has a commit dependency if any of these transactions aborts  ti must also be aborted the tree-locking protocol has an advantage over the two-phase locking protocol in that  unlike two-phase locking  it is deadlock-free  so no rollbacks are required the tree-locking protocol has another advantage over the two-phase locking protocol in that unlocking may occur earlier earlier unlocking may lead to shorter waiting times  and to an increase in concurrency however  the protocol has the disadvantage that  in some cases  a transaction may have to lock data items that it does not access for example  a transaction that needs to access data items a and j in the database graph of figure 15.11 must lock not only a and j  but also data items b  d  and h this additional locking results in increased locking overhead  the possibility of additional waiting time  and a potential decrease in concurrency further,without prior knowledge ofwhat data items will need to be locked  transactions will have to lock the root of the tree  and that can reduce concurrency greatly for a set of transactions  there may be conflict-serializable schedules that can not be obtained through the tree protocol indeed  there are schedules possible under the two-phase locking protocol that are not possible under the tree protocol  and vice versa examples of such schedules are explored in the exercises 15.2 deadlock handling a system is in a deadlock state if there exists a set of transactions such that every transaction in the set is waiting for another transaction in the set more precisely  there exists a set of waiting transactions  t0  t1      tn  such that t0 is waiting for a data item that t1 holds  and t1 is waiting for a data item that t2 holds  and     and tn-1 is waiting for a data item that tn holds  and tn is waiting for a data item that t0 holds none of the transactions can make progress in such a situation the only remedy to this undesirable situation is for the system to invoke some drastic action  such as rolling back some of the transactions involved in the deadlock rollback of a transaction may be partial  that is  a transaction may be rolled back to the point where it obtained a lock whose release resolves the deadlock there are two principal methods for dealing with the deadlock problem we can use a deadlock prevention protocol to ensure that the system will never enter a deadlock state alternatively  we can allow the system to enter a deadlock state  and then try to recover by using a deadlock detection and deadlock recovery scheme aswe shall see  both methodsmay result in transaction rollback prevention is commonly used if the probability that the system would enter a deadlock state is relatively high ; otherwise  detection and recovery are more efficient note that a detection and recovery scheme requires overhead that includes not only the run-time cost ofmaintaining the necessary information and of executing the detection algorithm  but also the potential losses inherent in recovery from a deadlock thesumit67.blogspot.com 15.2 deadlock handling 675 15.2.1 deadlock prevention there are two approaches to deadlock prevention one approach ensures that no cyclic waits can occur by ordering the requests for locks  or requiring all locks to be acquired together the other approach is closer to deadlock recovery  and performs transaction rollback instead of waiting for a lock  whenever the wait could potentially result in a deadlock the simplest scheme under the first approach requires that each transaction locks all its data items before it begins execution moreover  either all are locked in one step or none are locked there are two main disadvantages to this protocol   1  it is often hard to predict  before the transaction begins  what data items need to be locked ;  2  data-item utilization may be very low  since many of the data items may be locked but unused for a long time another approach for preventing deadlocks is to impose an ordering of all data items  and to require that a transaction lock data items only in a sequence consistent with the ordering.we have seen one such scheme in the tree protocol  which uses a partial ordering of data items avariation of this approach is to use a total order of data items  in conjunction with two-phase locking once a transaction has locked a particular item  it can not request locks on items that precede that item in the ordering this scheme is easy to implement  as long as the set of data items accessed by a transaction is known when the transaction starts execution there is no need to change the underlying concurrency-control system if two-phase locking is used  all that is needed is to ensure that locks are requested in the right order the second approach for preventing deadlocks is to use preemption and transaction rollbacks in preemption  when a transaction tj requests a lock that transaction ti holds  the lock granted to ti may be preempted by rolling back of ti  and granting of the lock to tj  to control the preemption  we assign a unique timestamp  based on a counter or on the system clock  to each transaction when it begins the system uses these timestamps only to decide whether a transaction should wait or roll back locking is still used for concurrency control if a transaction is rolled back  it retains its old timestamp when restarted two different deadlock-prevention schemes using timestamps have been proposed  1 the wait ? die scheme is a nonpreemptive technique when transaction ti requests a data item currently held by tj  ti is allowed to wait only if it has a timestamp smaller than that of tj  that is  ti is older than tj   otherwise  ti is rolled back  dies   for example  suppose that transactions t14  t15  and t16 have timestamps 5  10  and 15  respectively if t14 requests a data item held by t15  then t14 will wait if t24 requests a data item held by t15  then t16 will be rolled back 2 the wound ? wait scheme is a preemptive technique it is a counterpart to the wait ? die scheme when transaction ti requests a data item currently held by tj  ti is allowed to wait only if it has a timestamp larger than that of tj  that is  ti is younger than tj   otherwise  tj is rolled back  tj is wounded by ti   thesumit67.blogspot.com 676 chapter 15 concurrency control returning to our example  with transactions t14  t15  and t16  if t14 requests a data itemheld by t15  then the data item will be preempted from t15  and t15 will be rolled back if t16 requests a data item held by t15  then t16 will wait the major problem with both of these schemes is that unnecessary rollbacks may occur another simple approach to deadlock prevention is based on lock timeouts in this approach  a transaction that has requested a lock waits for at most a specified amount of time if the lock has not been granted within that time  the transaction is said to time out  and it rolls itself back and restarts if there was in fact a deadlock  one or more transactions involved in the deadlock will time out and roll back  allowing the others to proceed this scheme falls somewhere between deadlock prevention  where a deadlock will never occur  and deadlock detection and recovery  which section 15.2.2 discusses the timeout scheme is particularly easy to implement  and works well if transactions are short and if longwaits are likely to be due to deadlocks.however  in general it is hard to decide how long a transaction must wait before timing out too long a wait results in unnecessary delays once a deadlock has occurred too short a wait results in transaction rollback even when there is no deadlock  leading to wasted resources starvation is also a possibility with this scheme hence  the timeout-based scheme has limited applicability 15.2.2 deadlock detection and recovery if a system does not employ some protocol that ensures deadlock freedom  then a detection and recovery scheme must be used an algorithm that examines the state of the system is invoked periodically to determine whether a deadlock has occurred if one has  then the system must attempt to recover from the deadlock to do so  the system must  ? maintain information about the current allocation of data items to transactions  as well as any outstanding data item requests ? provide an algorithm that uses this information to determine whether the system has entered a deadlock state ? recover from the deadlock when the detection algorithm determines that a deadlock exists in this section  we elaborate on these issues 15.2.2.1 deadlock detection deadlocks can be described precisely in terms of a directed graph called a waitfor graph this graph consists of a pair g =  v  e   where v is a set of vertices and e is a set of edges the set of vertices consists of all the transactions in the system each element in the set e of edges is an ordered pair ti ? tj if ti ? tj is in e  thesumit67.blogspot.com 15.2 deadlock handling 677 t18 t20 t17 t19 figure 15.13 wait-for graph with no cycle then there is a directed edge from transaction ti to tj  implying that transaction ti is waiting for transaction tj to release a data item that it needs when transaction ti requests a data item currently being held by transaction tj  then the edge ti ? tj is inserted in the wait-for graph this edge is removed only when transaction tj is no longer holding a data item needed by transaction ti  a deadlock exists in the system if and only if the wait-for graph contains a cycle each transaction involved in the cycle is said to be deadlocked to detect deadlocks  the system needs to maintain the wait-for graph  and periodically to invoke an algorithm that searches for a cycle in the graph to illustrate these concepts  consider thewait-for graph in figure 15.13  which depicts the following situation  ? transaction t17 is waiting for transactions t18 and t19 ? transaction t19 is waiting for transaction t18 ? transaction t18 is waiting for transaction t20 since the graph has no cycle  the system is not in a deadlock state suppose now that transaction t20 is requesting an item held by t19 the edge t20 ? t19 is added to the wait-for graph  resulting in the new system state in figure 15.14 this time  the graph contains the cycle  t18 ? t20 ? t19 ? t18 implying that transactions t18  t19  and t20 are all deadlocked consequently  the question arises  when should we invoke the detection algorithm ? the answer depends on two factors  1 how often does a deadlock occur ? 2 how many transactions will be affected by the deadlock ? if deadlocks occur frequently  then the detection algorithm should be invoked more frequently data items allocated to deadlocked transactions will be thesumit67.blogspot.com 678 chapter 15 concurrency control t18 t20 t17 t19 figure 15.14 wait-for graph with a cycle unavailable to other transactions until the deadlock can be broken in addition  the number of cycles in the graph may also grow in the worst case  we would invoke the detection algorithm every time a request for allocation could not be granted immediately 15.2.2.2 recovery from deadlock when a detection algorithm determines that a deadlock exists  the system must recover from the deadlock themost common solution is to roll back one or more transactions to break the deadlock three actions need to be taken  1 selection of a victim.given a set of deadlocked transactions,wemust determine which transaction  or transactions  to roll back to break the deadlock we should roll back those transactions that will incur the minimum cost unfortunately  the term minimum cost is not a precise one many factors may determine the cost of a rollback  including  a how long the transaction has computed  and how much longer the transaction will compute before it completes its designated task b how many data items the transaction has used c how many more data items the transaction needs for it to complete d how many transactions will be involved in the rollback 2 rollback oncewe have decided that a particular transaction must be rolled back  we must determine how far this transaction should be rolled back the simplest solution is a total rollback  abort the transaction and then restart it however  it is more effective to roll back the transaction only as far as necessary to break the deadlock such partial rollback requires the system to maintain additional information about the state of all the running transactions specifically  the sequence of lock requests/grants and updates performed by the transaction needs to be recorded the deadlock detection mechanism should decide which locks the selected transaction needs to release in order to break the deadlock the selected transaction must be rolled back to the point where it obtained the first of these locks  undoing all actions it took after that point the recoverymechanism must be capable thesumit67.blogspot.com 15.3 multiple granularity 679 of performing such partial rollbacks furthermore  the transactions must be capable of resuming execution after a partial rollback see the bibliographical notes for relevant references 3 starvation in a system where the selection of victims is based primarily on cost factors  it may happen that the same transaction is always picked as a victim as a result  this transaction never completes its designated task  thus there is starvation.we must ensure that a transaction can be picked as a victim only a  small  finite number of times the most common solution is to include the number of rollbacks in the cost factor 15.3 multiple granularity in the concurrency-control schemes described thus far  we have used each individual data item as the unit on which synchronization is performed there are circumstances  however  where it would be advantageous to group several data items  and to treat them as one individual synchronization unit for example  if a transaction ti needs to access the entire database  and a locking protocol is used  then ti must lock each item in the database clearly  executing these locks is time-consuming it would be better if ti could issue a single lock request to lock the entire database on the other hand  if transaction tj needs to access only a few data items  it should not be required to lock the entire database  since otherwise concurrency is lost what is needed is a mechanism to allow the system to define multiple levels of granularity this is done by allowing data items to be of various sizes and defining a hierarchy of data granularities  where the small granularities are nested within larger ones such a hierarchy can be represented graphically as a tree note that the tree that we describe here is significantly different from that used by the tree protocol  section 15.1.5   a nonleaf node of the multiple-granularity tree represents the data associated with its descendants in the tree protocol  each node is an independent data item as an illustration  consider the tree of figure 15.15  which consists of four levels of nodes the highest level represents the entire database below it are nodes of type area ; the database consists of exactly these areas each area in turn has nodes of type file as its children each area contains exactly those files that are its child nodes no file is in more than one area finally  each file has nodes of type record as before  the file consists of exactly those records that are its child nodes  and no record can be present in more than one file each node in the tree can be locked individually as we did in the twophase locking protocol  we shall use shared and exclusive lock modes when a transaction locks a node  in either shared or exclusive mode  the transaction also has implicitly locked all the descendants of that node in the same lock mode for example  if transaction ti gets an explicit lock on file fc of figure 15.15  in exclusive mode  then it has an implicit lock in exclusive mode on all the records belonging to that file it does not need to lock the individual records of fc explicitly thesumit67.blogspot.com 680 chapter 15 concurrency control ra1 ra2 ran rb1 rbk rc1 rcm fa fb fc a1 a2 db figure 15.15 granularity hierarchy suppose that transaction tj wishes to lock record rb6 of file fb since ti has locked fb explicitly  it follows that rb6 is also locked  implicitly   but  when tj issues a lock request for rb6  rb6 is not explicitly locked ! how does the system determine whether tj can lock rb6 ? tj must traverse the tree from the root to record rb6  if any node in that path is locked in an incompatible mode  then tj must be delayed suppose now that transaction tk wishes to lock the entire database to do so  it simply must lock the root of the hierarchy note  however  that tk should not succeed in locking the root node  since ti is currently holding a lock on part of the tree  specifically  on file fb   but how does the system determine if the root node can be locked ? one possibility is for it to search the entire tree this solution  however  defeats the whole purpose of the multiple-granularity locking scheme a more efficient way to gain this knowledge is to introduce a new class of lock modes  called intention lock modes if a node is locked in an intention mode  explicit locking is done at a lower level of the tree  that is  at a finer granularity   intention locks are put on all the ancestors of a node before that node is locked explicitly thus  a transaction does not need to search the entire tree to determine whether it can lock a node successfully.atransactionwishing to lock a node ? say  q ? must traverse a path in the tree from the root to q.while traversing the tree  the transaction locks the various nodes in an intention mode there is an intention mode associated with shared mode  and there is one with exclusive mode if a node is locked in intention-shared  is  mode  explicit locking is being done at a lower level of the tree  but with only shared-mode locks similarly  if a node is locked in intention-exclusive  ix  mode  then explicit locking is being done at a lower level  with exclusive-mode or shared-mode locks finally  if a node is locked in shared and intention-exclusive  six  mode  the subtree rooted by that node is locked explicitly in shared mode  and that explicit locking is being done at a lower level with exclusive-mode locks the compatibility function for these lock modes is in figure 15.16 thesumit67.blogspot.com 15.3 multiple granularity 681 is ix s six x is true true true true false ix true true false false false s true false true false false six true false false false false x false false false false false figure 15.16 compatibility matrix the multiple-granularity locking protocol uses these lock modes to ensure serializability it requires that a transaction ti that attempts to lock a node q must follow these rules  1 transaction ti must observe the lock-compatibility function of figure 15.16 2 transaction ti must lock the root of the tree first  and can lock it in anymode 3 transaction ti can lock a node q in s or is mode only if ti currently has the parent of q locked in either ix or is mode 4 transaction ti can lock a node q in x  six  or ix mode only if ti currently has the parent of q locked in either ix or six mode 5 transaction ti can lock a node only if ti has not previously unlocked any node  that is  ti is two phase   6 transaction ti can unlock a node q only if ti currently has none of the children of q locked observe that the multiple-granularity protocol requires that locks be acquired in top-down  root-to-leaf  order  whereas locks must be released in bottom-up  leafto root  order as an illustration of the protocol  consider the tree of figure 15.15 and these transactions  ? suppose that transaction t21 reads record ra2 in file fa  then  t21 needs to lock the database  area a1  and fa in is mode  and in that order   and finally to lock ra2 in s mode ? suppose that transaction t22 modifies record ra9 in file fa  then  t22 needs to lock the database  area a1  and file fa  and in that order  in ix mode  and finally to lock ra9 in x mode ? suppose that transaction t23 reads all the records in file fa  then  t23 needs to lock the database and area a1  and in that order  in is mode  and finally to lock fa in s mode thesumit67.blogspot.com 682 chapter 15 concurrency control ? suppose that transaction t24 reads the entire database it can do so after locking the database in s mode we note that transactions t21  t23  and t24 can access the database concurrently transaction t22 can execute concurrently with t21  but not with either t23 or t24 this protocol enhances concurrency and reduces lock overhead it is particularly useful in applications that include a mix of  ? short transactions that access only a few data items ? long transactions that produce reports from an entire file or set of files there is a similar locking protocol that is applicable to database systems in which data granularities are organized in the form of a directed acyclic graph see the bibliographical notes for additional references deadlock is possible in the multiple-granularity protocol  as it is in the two-phase locking protocol there are techniques to reduce deadlock frequency in the multiple-granularity protocol  and also to eliminate deadlock entirely these techniques are referenced in the bibliographical notes 15.4 timestamp-based protocols the locking protocols that we have described thus far determine the order between every pair of conflicting transactions at execution time by the first lock that both members of the pair request that involves incompatible modes another method for determining the serializability order is to select an ordering among transactions in advance the most common method for doing so is to use a timestamp-ordering scheme 15.4.1 timestamps with each transaction ti in the system  we associate a unique fixed timestamp  denoted by ts  ti   this timestamp is assigned by the database system before the transaction ti starts execution if a transaction ti has been assigned timestamp ts  ti   and a new transaction tj enters the system  then ts  ti  < ts  tj   there are two simple methods for implementing this scheme  1 use the value of thes ? ystem clock as the timestamp ; that is  a transaction ? s timestamp is equal to the value of the clock when the transaction enters the system 2 use a logical counter that is incremented after a new timestamp has been assigned ; that is  a transaction ? s timestamp is equal to the value of the counter when the transaction enters the system thesumit67.blogspot.com 15.4 timestamp-based protocols 683 the timestamps of the transactions determine the serializability order thus  if ts  ti  < ts  tj   then the system must ensure that the produced schedule is equivalent to a serial schedule in which transaction ti appears before transaction tj  to implement this scheme,we associatewith each data itemqtwo timestamp values  ? w-timestamp  q  denotes the largest timestamp of any transaction that executed write  q  successfully ? r-timestamp  q  denotes the largest timestamp of any transaction that executed read  q  successfully these timestamps are updated whenever a new read  q  or write  q  instruction is executed 15.4.2 the timestamp-ordering protocol the timestamp-ordering protocol ensures that any conflicting read and write operations are executed in timestamp order this protocol operates as follows  1 suppose that transaction ti issues read  q   a if ts  ti  < w-timestamp  q   then ti needs to read a value of q that was already overwritten hence  the read operation is rejected  and ti is rolled back b if ts  ti  = w-timestamp  q   then the read operation is executed  and r-timestamp  q  is set to the maximum of r-timestamp  q  and ts  ti   2 suppose that transaction ti issues write  q   a if ts  ti  < r-timestamp  q   then the value of q that ti is producing was needed previously  and the system assumed that that value would never be produced hence  the system rejects the write operation and rolls ti back b if ts  ti  < w-timestamp  q   then ti is attempting to write an obsolete value of q hence  the system rejects this write operation and rolls ti back c otherwise  the system executes the write operation and sets w-timestamp  q  to ts  ti   if a transaction ti is rolled back by the concurrency-control scheme as result of issuance of either a read or write operation  the system assigns it a new timestamp and restarts it to illustrate this protocol  we consider transactions t25 and t26 transaction t25 displays the contents of accounts a and b  thesumit67.blogspot.com 684 chapter 15 concurrency control t25  read  b  ; read  a  ; display  a + b   transaction t26 transfers $ 50 from account b to account a  and then displays the contents of both  t26  read  b  ; b  = b  50 ; write  b  ; read  a  ; a  = a + 50 ; write  a  ; display  a + b   in presenting schedules under the timestamp protocol  we shall assume that a transaction is assigned a timestamp immediately before its first instruction thus  in schedule 3 of figure 15.17  ts  t25  < ts  t26   and the schedule is possible under the timestamp protocol we note that the preceding execution can also be produced by the two-phase locking protocol there are  however  schedules that are possible under the twophase locking protocol  but are not possible under the timestamp protocol  and vice versa  see exercise 15.29   the timestamp-ordering protocol ensures conflict serializability this is because conflicting operations are processed in timestamp order the protocol ensures freedom from deadlock  since no transaction everwaits however  there is a possibility of starvation of long transactions if a sequence of conflicting short transactions causes repeated restarting of the long transaction if a transaction is suffering from repeated restarts  conflicting transactions need to be temporarily blocked to enable the transaction to finish t25 t26 read  b  read  b  b  = b  50 write  b  read  a  read  a  display  a + b  a  = a + 50 write  a  display  a + b  figure 15.17 schedule 3 thesumit67.blogspot.com 15.4 timestamp-based protocols 685 the protocol can generate schedules that are not recoverable however  it can be extended to make the schedules recoverable  in one of several ways  ? recoverability and cascadelessness can be ensured by performing all writes together at the end of the transaction the writes must be atomic in the following sense  while the writes are in progress  no transaction is permitted to access any of the data items that have been written ? recoverability and cascadelessness can also be guaranteed by using a limited form of locking  whereby reads of uncommitted items are postponed until the transaction that updated the item commits  see exercise 15.30   ? recoverability alone can be ensured by tracking uncommitted writes  and allowing a transaction ti to commit only after the commit of any transaction that wrote a value that ti read commit dependencies  outlined in section 15.1.5  can be used for this purpose 15.4.3 thomas ? write rule we now present a modification to the timestamp-ordering protocol that allows greater potential concurrency than does the protocol of section 15.4.2 let us consider schedule 4 of figure 15.18  and apply the timestamp-ordering protocol since t27 starts before t28  we shall assume that ts  t27  < ts  t28   the read  q  operation of t27 succeeds  as does the write  q  operation of t28 when t27 attempts its write  q  operation  we find that ts  t27  < w-timestamp  q   since wtimestamp  q  = ts  t28   thus  the write  q  by t27 is rejected and transaction t27 must be rolled back although the rollback of t27 is required by the timestamp-ordering protocol  it is unnecessary since t28 has already written q  the value that t27 is attempting to write is one that will never need to be read any transaction ti with ts  ti  < ts  t28  that attempts a read  q  will be rolled back  since ts  ti  < w-timestamp  q   any transaction tj with ts  tj  > ts  t28  must read the value of q written by t28  rather than the value that t27 is attempting to write this observation leads to amodified version of the timestamp-ordering protocol inwhich obsolete write operations can be ignored under certain circumstances the protocol rules for read operations remain unchanged the protocol rules for write operations  however  are slightly different from the timestamp-ordering protocol of section 15.4.2 t27 t28 read  q  write  q  write  q  figure 15.18 schedule 4 thesumit67.blogspot.com 686 chapter 15 concurrency control the modification to the timestamp-ordering protocol  called thomas ? write rule  is this  suppose that transaction ti issues write  q   1 if ts  ti  < r-timestamp  q   then the value of q that ti is producing was previously needed  and it had been assumed that the value would never be produced hence  the system rejects the write operation and rolls ti back 2 if ts  ti  < w-timestamp  q   then ti is attempting to write an obsolete value of q hence  this write operation can be ignored 3 otherwise  the system executes the write operation and setsw-timestamp  q  to ts  ti   the difference between these rules and those of section 15.4.2 lies in the second rule the timestamp-ordering protocol requires that ti be rolled back if ti issues write  q  and ts  ti  < w-timestamp  q   however  here  in those cases where ts  ti  = r-timestamp  q   we ignore the obsolete write by ignoring thewrite  thomas ? write rule allows schedules that are not conflict serializable but are nevertheless correct those non-conflict-serializable schedules allowed satisfy the definition of view serializable schedules  see example box   thomas ? write rule makes use of view serializability by  in effect  deleting obsolete write operations from the transactions that issue them this modification of transactions makes it possible to generate serializable schedules that would not be possible under the other protocols presented in this chapter for example  schedule 4 of figure 15.18 is not conflict serializable and  thus  is not possible under the two-phase locking protocol  the tree protocol  or the timestamp-ordering protocol under thomas ? write rule  the write  q  operation of t27 would be ignored the result is a schedule that is view equivalent to the serial schedule < t27  t28 >  15.5 validation-based protocols in cases where a majority of transactions are read-only transactions  the rate of conflicts among transactions may be low thus  many of these transactions  if executed without the supervision of a concurrency-control scheme  would nevertheless leave the system in a consistent state a concurrency-control scheme imposes overhead of code execution and possible delay of transactions it may be better to use an alternative scheme that imposes less overhead a difficulty in reducing the overhead is that we do not know in advance which transactions will be involved in a conflict to gain that knowledge  we need a scheme for monitoring the system the validation protocol requires that each transaction ti executes in two or three different phases in its lifetime  depending on whether it is a read-only or an update transaction the phases are  in order  thesumit67.blogspot.com 15.5 validation-based protocols 687 view serializability there is another form of equivalence that is less stringent than conflict equivalence  but that  like conflict equivalence  is based on only the read and write operations of transactions consider two schedules s and s   where the same set of transactions participates in both schedules the schedules s and s  are said to be view equivalent if three conditions are met  1 for each data item q  if transaction ti reads the initial value of q in schedule s  then transaction ti must  in schedule s   also read the initial value of q 2 for each data item q  if transaction ti executes read  q  in schedule s  andif that value was produced by a write  q  operation executed by transaction tj  then the read  q  operation of transaction ti must  in schedule s   also read the value of q that was produced by the same write  q  operation of transaction tj  3 for each data item q  the transaction  if any  that performs the final write  q  operation in schedule s must perform the final write  q  operation in schedule s   conditions 1 and 2 ensure that each transaction reads the same values in both schedules and  therefore  performs the same computation condition 3  coupled with conditions 1 and 2  ensures that both schedules result in the same final system state the concept of view equivalence leads to the concept of view serializability we say that a schedule s is view serializable if it is view equivalent to a serial schedule as an illustration  suppose that we augment schedule 4 with transaction t29  and obtain the following view serializable  schedule 5   t27 t28 t29 read  q  write  q  write  q  write  q  indeed  schedule 5 is view equivalent to the serial schedule < t27  t28  t29 >  since the one read  q  instruction reads the initial value of q in both schedules and t29 performs the final write of q in both schedules every conflict-serializable schedule is also view serializable  but there are view-serializable schedules that are not conflict serializable indeed  schedule 5 is not conflict serializable  since every pair of consecutive instructions conflicts  and  thus  no swapping of instructions is possible observe that  in schedule 5  transactions t28 and t29 perform write  q  operations without having performed a read  q  operation writes of this sort are called blind writes blind writes appear in any view-serializable schedule that is not conflict serializable thesumit67.blogspot.com 688 chapter 15 concurrency control 1 read phase during this phase  the system executes transaction ti it reads the values of the various data items and stores them in variables local to ti it performs all write operations on temporary local variables  without updates of the actual database 2 validation phase the validation test  described below  is applied to transaction ti  this determines whether ti is allowed to proceed to the write phase without causing a violation of serializability if a transaction fails the validation test  the system aborts the transaction 3 write phase if the validation test succeeds for transaction ti  the temporary local variables that hold the results of any write operations performed by ti are copied to the database read-only transactions omit this phase each transaction must go through the phases in the order shown.however  phases of concurrently executing transactions can be interleaved to perform the validation test  we need to know when the various phases of transactions took place we shall  therefore  associate three different timestamps with each transaction ti  1 start  ti   the time when ti started its execution 2 validation  ti   the time when ti finished its read phase and started its validation phase 3 finish  ti   the time when ti finished its write phase we determine the serializability order by the timestamp-ordering technique  using the value of the timestamp validation  ti   thus  the value ts  ti  = validation  ti  and  if ts  tj  < ts  tk   then any produced schedule must be equivalent to a serial schedule in which transaction tj appears before transaction tk the reason we have chosen validation  ti   rather than start  ti   as the timestamp of transaction ti is that we can expect faster response time provided that conflict rates among transactions are indeed low the validation test for transaction ti requires that  for all transactions tk with ts  tk  < ts  ti   one of the following two conditions must hold  1 finish  tk  < start  ti   since tk completes its execution before ti started  the serializability order is indeed maintained 2 the set of data itemswritten by tk does not intersectwith the set of data items read by ti  and tk completes its write phase before ti starts its validation phase  start  ti  < finish  tk  < validation  ti    this condition ensures that the writes of tk and ti do not overlap since the writes of tk do not affect the read of ti  and since ti can not affect the read of tk  the serializability order is indeed maintained thesumit67.blogspot.com 15.6 multiversion schemes 689 t25 t26 read  b  read  b  b  = b  50 read  a  a  = a + 50 read  a  < validate > display  a + b  < validate > write  b  write  a  figure 15.19 schedule 6  a schedule produced by using validation asan illustration  consider again transactions t25 and t26 suppose that ts  t25  < ts  t26   then  the validation phase succeeds in the schedule 6 in figure 15.19 note that thewrites to the actual variables are performed only after the validation phase of t26 thus  t25 reads the old values of b and a  and this schedule is serializable the validation scheme automatically guards against cascading rollbacks  since the actual writes take place only after the transaction issuing the write has committed however  there is a possibility of starvation of long transactions  due to a sequence of conflicting short transactions that cause repeated restarts of the long transaction to avoid starvation  conflicting transactions must be temporarily blocked  to enable the long transaction to finish this validation scheme is called the optimistic concurrency-control scheme since transactions execute optimistically  assuming they will be able to finish execution and validate at the end in contrast  locking and timestamp ordering are pessimistic in that they force a wait or a rollback whenever a conflict is detected  even though there is a chance that the schedule may be conflict serializable 15.6 multiversion schemes the concurrency-control schemes discussed thus far ensure serializability by either delaying an operation or aborting the transaction that issued the operation for example  a read operation may be delayed because the appropriate value has not been written yet ; or it may be rejected  that is  the issuing transaction must be aborted  because the value that itwas supposed to read has already been overwritten these difficulties could be avoided if old copies of each data item were kept in a system in multiversion concurrency-control schemes  each write  q  operation creates a new version of q when a transaction issues a read  q  operation  the thesumit67.blogspot.com 690 chapter 15 concurrency control concurrency-control manager selects one of the versions of q to be read the concurrency-control scheme must ensure that the version to be read is selected in a manner that ensures serializability it is also crucial  for performance reasons  that a transaction be able to determine easily and quickly which version of the data item should be read 15.6.1 multiversion timestamp ordering the timestamp-ordering protocol can be extended to a multiversion protocol with each transaction ti in the system  we associate a unique static timestamp  denoted by ts  ti   the database system assigns this timestamp before the transaction starts execution  as described in section 15.4 with each data itemq  a sequence of versions < q1  q2      qm > is associated each version qk contains three data fields  ? content is the value of version qk  ? w-timestamp  qk  is the timestamp of the transaction that created version qk  ? r-timestamp  qk  is the largest timestamp of any transaction that successfully read version qk  a transaction ? say  ti ? creates a new version qk of data item q by issuing a write  q  operation the content field of the version holds the value written by ti  the system initializes thew-timestamp and r-timestamp to ts  ti   it updates the r-timestamp value of qk whenever a transaction tj reads the content of qk  and r-timestamp  qk  < ts  tj   the multiversion timestamp-ordering scheme presented next ensures serializability the scheme operates as follows  suppose that transaction ti issues a read  q  or write  q  operation let qk denote the version of q whose write timestamp is the largest write timestamp less than or equal to ts  ti   1 if transaction ti issues a read  q   then the value returned is the content of version qk  2 if transaction ti issues write  q   and if ts  ti  < r-timestamp  qk   then the system rolls back transaction ti  on the other hand  if ts  ti  = wtimestamp  qk   the system overwrites the contents of qk ; otherwise  ifts  ti  > r-timestamp  qk    it creates a new version of q the justification for rule 1 is clear a transaction reads the most recent version that comes before it in time the second rule forces a transaction to abort if it is ? too late ? in doing a write more precisely  if ti attempts to write a version that some other transaction would have read  then we can not allow that write to succeed versions that are no longer needed are removed according to the following rule  suppose that there are two versions  qk and qj  of a data item  and that both thesumit67.blogspot.com 15.6 multiversion schemes 691 versions have aw-timestamp less than the timestamp of the oldest transaction in the system then  the older of the two versions qk and qj will not be used again  and can be deleted themultiversion timestamp-ordering scheme has the desirable property that a read request never fails and is never made to wait in typical database systems  where reading is a more frequent operation than is writing  this advantage may be of major practical significance the scheme  however  suffers fromtwo undesirable properties first  the reading of a data item also requires the updating of the r-timestamp field  resulting in two potential disk accesses  rather than one second  the conflicts between transactions are resolved through rollbacks  rather than throughwaits this alternative may be expensive section 15.6.2 describes an algorithmto alleviate this problem this multiversion timestamp-ordering scheme does not ensure recoverability and cascadelessness it can be extended in the same manner as the basic timestamp-ordering scheme  to make it recoverable and cascadeless 15.6.2 multiversion two-phase locking the multiversion two-phase locking protocol attempts to combine the advantages of multiversion concurrency control with the advantages of two-phase locking this protocol differentiates between read-only transactions and update transactions update transactions perform rigorous two-phase locking ; that is  they hold all locks up to the end of the transaction thus  they can be serialized according to their commit order each version of a data item has a single timestamp the timestamp in this case is not a real clock-based timestamp  but rather is a counter  which we will call the ts-counter  that is incremented during commit processing the database system assigns read-only transactions a timestamp by reading the current value of ts-counter before they start execution ; they follow the multiversion timestamp-ordering protocol for performing reads thus  when a read-only transaction ti issues a read  q   the value returned is the contents of the version whose timestamp is the largest timestamp less than or equal to ts  ti   when an update transaction reads an item  it gets a shared lock on the item  and reads the latest version of that item when an update transaction wants to write an item  it first gets an exclusive lock on the item  and then creates a new version of the data item the write is performed on the new version  and the timestamp of the new version is initially set to a value 8  a value greater than that of any possible timestamp when the update transaction ti completes its actions  it carries out commit processing  first  ti sets the timestamp on every version it has created to 1 more than the value of ts-counter ; then  ti increments ts-counter by 1 only one update transaction is allowed to perform commit processing at a time as a result  read-only transactions that start after ti increments ts-counter will see the values updated by ti  whereas those that start before ti increments ts-counter will see the value before the updates by ti  in either case  read-only thesumit67.blogspot.com 692 chapter 15 concurrency control transactions never need to wait for locks multiversion two-phase locking also ensures that schedules are recoverable and cascadeless versions are deleted in amanner like that ofmultiversion timestamp ordering suppose there are two versions  qk and qj  of a data item  and that both versions have a timestamp less than or equal to the timestamp of the oldest read-only transaction in the system then  the older of the two versions qk and qj will not be used again and can be deleted 15.7 snapshot isolation snapshot isolation is a particular type of concurrency-control scheme that has gained wide acceptance in commercial and open-source systems  including oracle  postgresql  andsql server.we introduced snapshot isolation in section 14.9.3 here  we take a more detailed look into how it works conceptually  snapshot isolation involves giving a transaction a ? snapshot ? of the database at the time when it begins its execution it then operates on that snapshot in complete isolation from concurrent transactions the data values in the snapshot consist only of values written by committed transactions this isolation is ideal for read-only transactions since they never wait and are never aborted by the concurrency manager transactions that update the database must  of course  interact with potentially conflicting concurrent update transactions before updates are actually placed in the database updates are kept in the transaction ? s private workspace until the transaction successfully commits  at which point the updates are written to the database when a transaction t is allowed to commit  the transition of t to the committed state and the writing of all of the updates made by t to the databasemust be done as an atomic action so that any snapshot created for another transaction either includes all updates by transaction t or none of them 15.7.1 validation steps for update transactions deciding whether or not to allow an update transaction to commit requires some care potentially  two transactions running concurrently might both update the same data item since these two transactions operate in isolation using their own private snapshots  neither transaction sees the update made by the other if both transactions are allowed to write to the database  the first update written will be overwritten by the second the result is a lost update clearly  this must be prevented there are two variants of snapshot isolation  both ofwhich prevent lost updates they are called first committerwins and first updaterwins both approaches are based on testing the transaction against concurrent transactions.atransaction is said to be concurrent with t if it was active or partially committed at any point from the start of t up to and including the timewhen this test is being performed under first committer wins  when a transaction t enters the partially committed state  the following actions are taken in an atomic action  thesumit67.blogspot.com 15.7 snapshot isolation 693 ? atest is made to see if any transaction that was concurrentwith t has already written an update to the database for some data item that t intends to write ? if some such transaction is found  then t aborts ? if no such transaction is found  then t commits and its updates are written to the database this approach is called ? first committer wins ? because if transactions conflict  the first one to be tested using the above rule succeeds in writing its updates  while the subsequent ones are forced to abort details of how to implement the above tests are addressed in exercise 15.19 under first updater wins the system uses a locking mechanism that applies only to updates  reads are unaffected by this  since they do not obtain locks   when a transaction ti attempts to update a data item  it requests a write lock on that data item if the lock is not held by a concurrent transaction  the following steps are taken after the lock is acquired  ? if the item has been updated by any concurrent transaction  then ti aborts ? otherwise ti may proceed with its execution including possibly committing if  however  some other concurrent transaction tj already holds a write lock on that data item  then ti can not proceed and the following rules are followed  ? ti waits until tj aborts or commits ? if tj aborts  then the lock is released and ti can obtain the lock after the lock is acquired  the check for an update by a concurrent transaction is performed as described earlier  ti aborts if a concurrent transaction had updated the data item  and proceeds with its execution otherwise ? if tj commits  then ti must abort locks are released when the transaction commits or aborts this approach is called ? first updater wins ? because if transactions conflict  the first one to obtain the lock is the one that is permitted to commit and perform its update those that attempt the update later abort unless the first updater subsequently aborts for some other reason  as an alternative to waiting to see if the first updater tj aborts  a subsequent updater ti can be aborted as soon as it finds that the write lock it wishes to obtain is held by tj   15.7.2 serializability issues snapshot isolation is attractive in practice because the overhead is low and no aborts occur unless two concurrent transactions update the same data item there is  however  one serious problemwith the snapshot isolation scheme as we have presented it  and as it is implemented in practice  snapshot isolation does not ensure serializability this is true even in oracle  which uses snapshot isolation thesumit67.blogspot.com 694 chapter 15 concurrency control as the implementation for the serializable isolation level ! next  we give examples of possible nonserializable executions under snapshot isolation and show how to deal with them 1 suppose that we have two concurrent transactions ti and tj and two data items a and b suppose that ti reads a and b  then updates b  while tj reads aand b  then updates a for simplicity  we assume there are no other concurrent transactions since ti and tj are concurrent  neither transaction sees the update by the other in its snapshot but  since they update different data items  both are allowed to commit regardless of whether the system uses the first-update-wins policy or the first-committer-wins policy however  the precedence graph has a cycle there is an edge in the precedence graph from ti to tj because ti reads the value of athat existed before tj writes a there is also an edge in the precedence graph from tj to ti because tj reads the value of b that existed before ti writes b since there is a cycle in the precedence graph  the result is a nonserializable schedule this situation  where each of a pair of transactions has read data that is written by the other  but there is no data written by both transactions  is referred to as write skew as a concrete example of write skew  consider a banking scenario suppose that the bank enforces the integrity constraint that the sum of the balances in the checking and the savings account of a customer must not be negative suppose the checking and savings balances for a customer are $ 100 and $ 200  respectively suppose that transaction t36 withdraws $ 200 from the checking account  after verifying the integrity constraint by reading both balances suppose that concurrently transaction t37 withdraws $ 200 from the savings account  again after verifying the integrity constraint since each of the transactions checks the integrity constraint on its own snapshot  if they run concurrently each will believe that the sum of the balances after the withdrawal is $ 100  and therefore its withdrawal does not violate the constraint since the two transactions update different data items  they do not have any update conflict  and under snapshot isolation both of them can commit unfortunately  in the final state after both t36 and t37 have committed  the sum of the balances is $ -100  violating the integrity constraint such a violation could never have occurred in any serial execution of t36 and t37 it is worth noting that integrity constraints that are enforced by the database  such as primary-key and foreign-key constraints  can not be checked on a snapshot ; otherwise it would be possible for two concurrent transactions to insert two tuples with the same primary key value  or for a transaction to insert a foreign key value that is concurrently deleted from the referenced table instead  the database system must check these constraints on the current state of the database  as part of validation at the time of commit 2 for the next example  we shall consider two concurrent update transactions that do not themselves present any problem as regards serializability unless thesumit67.blogspot.com 15.7 snapshot isolation 695 a read-only transaction happens to show up at just the right time to cause a problem suppose that we have two concurrent transactions ti and tj and two data items aand b suppose that ti reads b and then updates b,while tj reads a and b  then updates a running these two transactions concurrently causes no problem since ti accesses only data item b  there are no conflicts on data item a and therefore there is no cycle in the precedence graph the only edge in the precedence graph is the edge fromtj to ti because tj reads the value of b that existed before ti writes b however  let us suppose that ti commits while tj is still active suppose that  after ti commits but before tj commits  a new read-only transaction tk enters the system and tk reads both a and b its snapshot includes the update by ti because ti has already committed however  since tj has not committed  its update has not yet been written to the database and is not included in the snapshot seen by tk  consider the edges that are added to the precedence graph on account of tk  there is an edge in the precedence graph from ti to tk because ti writes the value of b that existed before tk reads b there is an edge in the precedence graph from tk to tj because tk reads the value of athat existed before tj writes a that leads to a cycle in the precedence graph  showing that the resulting schedule is nonserializable the above anomalies may not be as troublesome as they first appear recall that the reason for serializability is to ensure that  despite concurrent execution of transactions  database consistency is preserved since consistency is the goal  we can accept the potential for nonserializable executions if we are sure that those nonserializable executions that might occur will not lead to inconsistency the second example above is a problem only if the application that submits the read-only transaction  tk  cares about seeing updates to aand b out of order in that example  we did not specify the database consistency constraints that each transaction expects to hold if we are dealing with a financial database  it might be a very serious matter for tk to read updates out of proper serial order on the other hand  if aand b are enrollments in two sections of the same course  then tk may not demand perfect serialization and we may know from our applications that update rates are low enough that any inaccuracy in what tk reads is not significant the fact that the database must check integrity constraints at the time of commit  and not on a snapshot  also helps avoid inconsistencies in some situations some financial applications create consecutive sequence numbers  for example to number bills  by taking the maximum current bill number and adding 1 to the value to get a new bill number if two such transactions run concurrently  each would see the same set of bills in its snapshot  and each would create a new bill with the same number both transactions pass the validation tests for snapshot isolation  since they do not update any tuple in common however  the execution is not serializable ; the resultant database state can not be obtained by any serial thesumit67.blogspot.com 696 chapter 15 concurrency control execution of the two transactions creating two bills with the same number could have serious legal implications the above problem is an example of the phantom phenomenon  since the insert performed by each transaction conflicts with the read performed by the other transaction to find themaximum bill number  but the conflict is not detected by snapshot isolation.1 luckily  in most such applications the bill number would have been declared as a primary key  and the database system would detect the primary key violation outside the snapshot  and roll back one of the two transactions.2 an application developer can guard against certain snapshot anomalies by appending a for update clause to the sql select query as illustrated below  select * from instructor where id = 22222 for update ; adding the for update clause causes the system to treat data that are read as if they had been updated for purposes of concurrency control in our first example of write skew  if the for update clause is appended to the select queries that read the account balances  only one of the two concurrent transactions would be allowed to commit since it appears that both transactions have updated both the checking and savings balances in our second example of nonserializable execution  if the author of transaction tk wished to avoid this anomaly  the for update clause could be appended to the select query  even though there is in fact no update in our example  if tk used select for update  it would be treated as if it had updated a and b when it read them the result would be that either tk or tj would be aborted  and retried later as a new transaction this would lead to a serializable execution in this example  the queries in the other two transactions do not need the for update clause to be added ; unnecessary use of the for update clause can cause significant reduction in concurrency formal methods exist  see the bibliographical notes  to determine whether a given mix of transactions runs the risk of nonserializable execution under snapshot isolation  and to decide on what conflicts to introduce  using the for update clause  for example   to ensure serializability of course  such methods can work only if we know in advance what transactions are being executed in some applications  all transactions are from a predetermined set of transactions making this analysis possible however  if the application allows unrestricted  ad-hoc transactions  then no such analysis is possible 1the sql standard uses the term phantom problem to refer to non-repeatable predicate reads  leading some to claim that snapshot isolation avoids the phantomproblem ; however  such a claim is not valid under our definition of phantom conflict 2the problem of duplicate bill numbers actually occurred several times in a financial application in i.i.t bombay  where  for reasons too complex to discuss here  the bill number was not a primary key  and was detected by financial auditors thesumit67.blogspot.com 15.8 insert operations  delete operations  and predicate reads 697 of the three widely used systems that support snapshot isolation  sql server offers the option of a serializable isolation level that truly ensures serializability along with a snapshot isolation level that provides the performance advantages of snapshot isolation  along with the potential for the anomalies discussed above   in oracle and postgresql  the serializable isolation level offers only snapshot isolation 15.8 insert operations  delete operations  and predicate reads until now  we have restricted our attention to read and write operations this restriction limits transactions to data items already in the database some transactions require not only access to existing data items  but also the ability to create new data items others require the ability to delete data items to examine how such transactions affect concurrency control  we introduce these additional operations  ? delete  q  deletes data item q from the database ? insert  q  inserts a new data item q into the database and assigns q an initial value an attempt by a transaction ti to perform a read  q  operation after q has been deleted results in a logical error in ti  likewise  an attempt by a transaction ti to perform a read  q  operation before q has been inserted results in a logical error in ti  it is also a logical error to attempt to delete a nonexistent data item 15.8.1 deletion tounderstandhowthe presence of delete instructions affects concurrency control  we must decide when a delete instruction conflicts with another instruction let ii and i j be instructions of ti and tj  respectively  that appear in schedule s in consecutive order let ii = delete  q  .we consider several instructions i j  ? i j = read  q   ii and i j conflict if ii comes before i j  tj will have a logical error if i j comes before ii  tj can execute the read operation successfully ? i j = write  q   ii and i j conflict if ii comes before i j  tj will have a logical error if i j comes before ii  tj can execute the write operation successfully ? i j = delete  q   ii and i j conflict if ii comes before i j  ti will have a logical error if i j comes before ii  ti will have a logical error ? i j = insert  q   ii and i j conflict suppose that data item q did not exist prior to the execution of ii and i j  then  if ii comes before i j  a logical error results for ti if i j comes before ii  then no logical error results likewise  if q existed prior to the execution of ii and i j  then a logical error results if i j comes before ii  but not otherwise thesumit67.blogspot.com 698 chapter 15 concurrency control we can conclude the following  ? under the two-phase locking protocol  an exclusive lock is required on a data item before that item can be deleted ? under the timestamp-ordering protocol  a test similar to that for a write must be performed suppose that transaction ti issues delete  q   ? if ts  ti  < r-timestamp  q   then the value of q that ti was to delete has already been read by a transaction tj with ts  tj  > ts  ti   hence  the delete operation is rejected  and ti is rolled back ? if ts  ti  < w-timestamp  q   then a transaction tj with ts  tj  > ts  ti  has written q hence  this delete operation is rejected  and ti is rolled back ? otherwise  the delete is executed 15.8.2 insertion we have already seen that an insert  q  operation conflicts with a delete  q  operation similarly  insert  q  conflictswitharead  q  operation or a write  q  operation ; no read or write can be performed on a data item before it exists since an insert  q  assigns a value to data item q  an insert is treated similarly to a write for concurrency-control purposes  ? under the two-phase locking protocol  if ti performs an insert  q  operation  ti is given an exclusive lock on the newly created data item q ? under the timestamp-ordering protocol  if ti performs an insert  q  operation  the values r-timestamp  q  andw-timestamp  q  are set to ts  ti   15.8.3 predicate reads and the phantom phenomenon consider transaction t30 that executes the following sql query on the university database  select count  *  from instructor where dept name = ? physics ? ; transaction t30 requires access to all tuples of the instructor relation pertaining to the physics department let t31 be a transaction that executes the following sql insertion  insert into instructor values  11111  ? feynman ?  ? physics ?  94000  ; let s be a schedule involving t30 and t31 we expect there to be potential for a conflict for the following reasons  thesumit67.blogspot.com 15.8 insert operations  delete operations  and predicate reads 699 ? if t30 uses the tuple newly inserted by t31 in computing count  *   then t30 reads a value written by t31 thus  in a serial schedule equivalent to s  t31 must come before t30 ? if t30 does not use the tuple newly inserted by t31 in computing count  *   then in a serial schedule equivalent to s  t30 must come before t31 the second of these two cases is curious t30 and t31 do not access any tuple in common  yet they conflict with each other ! in effect  t30 and t31 conflict on a phantom tuple if concurrency control is performed at the tuple granularity  this conflict would go undetected as a result  the system could fail to prevent a nonserializable schedule this problem is called the phantom phenomenon in addition to the phantom problem  we also need to deal with the situation we saw in section 14.10  where a transaction used an index to find only tuples with dept name = ? physics ?  and as a result did not read any tuples with other department names if another transaction updates one of these tuples  changing its department name to physics  a problem equivalent to the phantom problem occurs both problems are rooted in predicate reads  and have a common solution to prevent the above problems  we allow transaction t30 to prevent other transactions from creating new tuples in the instructor relation with dept name = ? physics ?  and from updating the department name of an existing instructor tuple to physics to find all instructor tuples with dept name = ? physics ?  t30 must search either the whole instructor relation  or at least an index on the relation up to now  we have assumed implicitly that the only data items accessed by a transaction are tuples however  t30 is an example of a transaction that reads information about what tuples are in a relation  and t31 is an example of a transaction that updates that information clearly  it is not sufficient merely to lock the tuples that are accessed ; the information used to find the tuples that are accessed by the transaction must also be locked locking of information used to find tuples can be implemented by associating a data item with the relation ; the data item represents the information used to find the tuples in the relation transactions  such as t30  that read the information about what tuples are in a relation would then have to lock the data item corresponding to the relation in sharedmode transactions  such as t31  that update the information about what tuples are in a relation would have to lock the data item in exclusive mode thus  t30 and t31 would conflict on a real data item  rather than on a phantom similarly  transactions that use an index to retrieve tuples must lock the index itself do not confuse the locking of an entire relation  as in multiple-granularity locking  with the locking of the data item corresponding to the relation by locking the data item  a transaction only prevents other transactions fromupdating information about what tuples are in the relation locking is still required on tuples a transaction that directly accesses a tuple can be granted a lock on the tuples even thesumit67.blogspot.com 700 chapter 15 concurrency control when another transaction has an exclusive lock on the data item corresponding to the relation itself the major disadvantage of locking a data item corresponding to the relation  or locking an entire index  is the lowdegree of concurrency ? twotransactions that insert different tuples into a relation are prevented from executing concurrently a better solution is an index-locking technique that avoids locking the whole index any transaction that inserts a tuple into a relation must insert information into every index maintained on the relation we eliminate the phantom phenomenon by imposing a locking protocol for indices for simplicity we shall consider only b + -tree indices as we saw in chapter 11  every search-key value is associated with an index leaf node a query will usually use one or more indices to access a relation an insert must insert the new tuple in all indices on the relation in our example  we assume that there is an index on instructor for dept name then  t31 must modify the leaf containing the key ? physics ?  if t30 reads the same leaf node to locate all tuples pertaining to the physics department  then t30 and t31 conflict on that leaf node the index-locking protocol takes advantage of the availability of indices on a relation  by turning instances of the phantom phenomenon into conflicts on locks on index leaf nodes the protocol operates as follows  ? every relation must have at least one index ? a transaction ti can access tuples of a relation only after first finding them through one or more of the indices on the relation for the purpose of the index-locking protocol  a relation scan is treated as a scan through all the leaves of one of the indices ? a transaction ti that performs a lookup  whether a range lookup or a point lookup  must acquire a shared lock on all the index leaf nodes that it accesses ? a transaction ti may not insert  delete  or update a tuple ti in a relation r without updating all indices on r the transaction must obtain exclusive locks on all index leaf nodes that are affected by the insertion  deletion  or update for insertion and deletion  the leaf nodes affected are those that contain  after insertion  or contained  before deletion  the search-key value of the tuple for updates  the leaf nodes affected are those that  before the modification  contained the old value of the search key  and nodes that  after the modification  contain the new value of the search key ? locks are obtained on tuples as usual ? the rules of the two-phase locking protocol must be observed note that the index-locking protocol does not address concurrency control on internal nodes of an index ; techniques for concurrency control on indices  which minimize lock conflicts  are presented in section 15.10 locking an index leaf node prevents any update to the node  even if the update did not actually conflict with the predicate a variant called key-value thesumit67.blogspot.com 15.9 weak levels of consistency in practice 701 locking  which minimizes such false lock conflicts  is presented in section 15.10 as part of index concurrency control as noted in section 14.10  it would appear that the existence of a conflict between transactions depends on a low-level query-processing decision by the system that is unrelated to a user-level view of the meaning of the two transactions an alternative approach to concurrency control acquires shared locks on predicates in a query  such as the predicate ? salary > 90000 ? on the instructor relation inserts and deletes of the relation must then be checked to see if they satisfy the predicate ; if they do  there is a lock conflict  forcing the insert or delete to wait till the predicate lock is released for updates  both the initial value and the final value of the tuple must be checked against the predicate such conflicting inserts  deletes and updates affect the set of tuples selected by the predicate  and can not be allowed to execute concurrently with the query that acquired the  shared  predicate lock we call the above protocol predicate locking ; 3 predicate locking is not used in practice since it is more expensive to implement than the index-locking protocol  and does not give significant additional benefits variants of the predicate-locking technique can be used for eliminating the phantom phenomenon under the other concurrency-control protocols presented in this chapter however  many database systems  such as postgresql  as of version 8.1  and  to the best of our knowledge  oracle  as of version 10g  do not implement index locking or predicate locking  and are vulnerable to nonserializability due to phantom problems even if the isolation level is set to serializable 15.9 weak levels of consistency in practice in section 14.5  we discussed the isolation levels specified by the sql standard  serializable  repeatable read  read committed  and read uncommitted in this section  we first briefly outline some older terminology relating to consistency levels weaker than serializability and relate it to the sql standard levels we then discuss the issue of concurrency control for transactions that involve user interaction  an issue that we briefly discussed earlier in section 14.8 15.9.1 degree-two consistency the purpose of degree-two consistency is to avoid cascading aborts without necessarily ensuring serializability the locking protocol for degree-two consistency uses the same two lock modes that we used for the two-phase locking protocol  shared  s  and exclusive  x   a transaction must hold the appropriate lock mode when it accesses a data item  but two-phase behavior is not required in contrast to the situation in two-phase locking  s-locks may be released at any time  and locks may be acquired at any time exclusive locks  however  3the term predicate locking was used for a version of the protocol that used shared and exclusive locks on predicates  and was thus more complicated the version we present here  with only shared locks on predicates  is also referred to as precision locking thesumit67.blogspot.com 702 chapter 15 concurrency control t32 t33 lock-s  q  read  q  unlock  q  lock-x  q  read  q  write  q  unlock  q  lock-s  q  read  q  unlock  q  figure 15.20 nonserializable schedule with degree-two consistency can not be released until the transaction either commits or aborts serializability is not ensured by this protocol indeed  a transaction may read the same data item twice and obtain different results in figure 15.20  t32 reads the value of q before and after that value is written by t33 clearly  reads are not repeatable  but since exclusive locks are held until transaction commit  no transaction can read an uncommitted value thus  degreetwo consistency is one particular implementation of the read-committed isolation level 15.9.2 cursor stability cursor stability is a form of degree-two consistency designed for programs that iterate over tuples of a relation by using cursors instead of locking the entire relation  cursor stability ensures that  ? the tuple that is currently being processed by the iteration is locked in shared mode ? any modified tuples are locked in exclusive mode until the transaction commits these rules ensure that degree-two consistency is obtained two-phase locking is not required serializability is not guaranteed cursor stability is used in practice on heavily accessed relations as a means of increasing concurrency and improving system performance applications that use cursor stability must be coded in a way that ensures database consistency despite the possibility of nonserializable schedules thus  the use of cursor stability is limited to specialized situations with simple consistency constraints 15.9.3 concurrency control across user interactions concurrency-control protocols usually consider transactions that do not involve user interaction consider the airline seat selection example from section 14.8  thesumit67.blogspot.com 15.9 weak levels of consistency in practice 703 which involved user interaction suppose we treat all the steps from when the seat availability is initially shown to the user  till the seat selection is confirmed  as a single transaction if two-phase locking is used  the entire set of seats on a flight would be locked in shared mode till the user has completed the seat selection  and no other transaction would be able to update the seat allocation information in this period clearly such locking would be a very bad idea since a user may take a long time to make a selection  or even just abandon the transaction without explicitly cancelling it timestamp protocols or validation could be used instead  which avoid the problem of locking  but both these protocols would abort the transaction for a user a if any other user b has updated the seat allocation information  even if the seat selected by b does not conflict with the seat selected by user a snapshot isolation is a good option in this situation  since it would not abort the transaction of user aas long as b did not select the same seat as a however  snapshot isolation requires the database to remember information about updates performed by a transaction even after it has committed  as long as any other concurrent transaction is still active  which can be problematic for long duration transactions another option is to split a transaction that involves user interaction into two or more transactions  such that no transaction spans a user interaction if our seat selection transaction is split thus  the first transaction would read the seat availability  while the second transaction would complete the allocation of the selected seat if the second transaction is written carelessly  it could assign the selected seat to the user  without checking if the seat was meanwhile assigned to some other user  resulting in a lost-update problem to avoid the problem  as we outlined in section 14.8  the second transaction should perform the seat allocation only if the seat was not meanwhile assigned to some other user the above idea has been generalized in an alternative concurrency control scheme  which uses version numbers stored in tuples to avoid lost updates the schema of each relation is altered by adding an extra version number attribute  which is initialized to 0 when the tuple is created when a transaction reads  for the first time  a tuple that it intends to update  it remembers the version number of that tuple the read is performed as a stand-alone transaction on the database  and hence any locks that may be obtained are released immediately updates are done locally  and copied to the database as part of commit processing  using the following stepswhich are executed atomically  that is  as part of a single database transaction   ? for each updated tuple  the transaction checks if the current version number is the same as the version number of the tuple when it was first read by the transaction 1 if the version numbers match  the update is performed on the tuple in the database  and its version number is incremented by 1 2 if the version numbers do not match  the transaction is aborted  rolling back all the updates it performed thesumit67.blogspot.com 704 chapter 15 concurrency control if the version number check succeeds for all updated tuples  the transaction commits it is worth noting that a timestamp could be used instead of the version number  without impacting the scheme in any way observe the close similarity between the above scheme and snapshot isolation the version number check implements the first-committer-wins rule used in snapshot isolation  and can be used even if the transaction was active for a very long time however  unlike snapshot isolation  the reads performed by transaction may not correspond to a snapshot of the database ; and unlike the validation-based protocol  reads performed by the transaction are not validated we refer to the above scheme as optimistic concurrency control without read validation optimistic concurrency control without read validation provides a weak level of serializability  and does not ensure serializability a variant of this scheme uses version numbers to validate reads at the time of commit  in addition to validating writes  to ensure that the tuples read by the transaction were not updated subsequent to the initial read ; this scheme is equivalent to the optimistic concurrency-control scheme which we saw earlier the above scheme has been widely used by application developers to handle transactions that involve user interaction an attractive feature of the scheme is that it can be implemented easily on top of a database system the validation and update steps performed as part of commit processing are then executed as a single transaction in the database  using the concurrency-control scheme of the database to ensure atomicity for commit processing the above scheme is also used by the hibernate object-relational mapping system  section 9.4.2   and other objectrelational mapping systems  where it is referred to as optimistic concurrency control  even though reads are not validated by default   transactions that involve user interaction are called conversations in hibernate to differentiate them from regular transactions validation using version numbers is very useful for such transactions object-relational mapping systems also cache database tuples in the form of objects in memory  and execute transactions on the cached objects ; updates on the objects are converted into updates on the database when the transaction commits.datamay remain in cache for a long time  and if transactions update such cached data  there is a risk of lost updates hibernate and other object-relational mapping systems therefore perform the version number checks transparently as part of commit processing  hibernate allows programmers to bypass the cache and execute transactions directly on the database  if serializability is desired  15.10 concurrency in index structures * * it is possible to treat access to index structures like any other database structure  and to apply the concurrency-control techniques discussed earlier.however  since indices are accessed frequently  they would become a point of great lock contention  leading to a low degree of concurrency luckily  indices do not have to be treated like other database structures it is perfectly acceptable for a transaction to perform a lookup on an index twice  and to find that the structure of the index has changed in between  as long as the index lookup returns the correct set thesumit67.blogspot.com 15.10 concurrency in index structures * * 705 of tuples thus  it is acceptable to have nonserializable concurrent access to an index  as long as the accuracy of the index is maintained we outline two techniques for managing concurrent access to b + -trees the bibliographical notes reference other techniques for b + -trees  aswell as techniques for other index structures the techniques that we present for concurrency control on b + -trees are based on locking  but neither two-phase locking nor the tree protocol is employed the algorithms for lookup  insertion  and deletion are those used in chapter 11  with only minor modifications the first technique is called the crabbing protocol  ? when searching for a key value  the crabbing protocol first locks the root node in shared mode when traversing down the tree  it acquires a shared lock on the child node to be traversed further after acquiring the lock on the child node  it releases the lock on the parent node it repeats this process until it reaches a leaf node ? when inserting or deleting a key value  the crabbing protocol takes these actions  ? it follows the same protocol as for searching until it reaches the desired leaf node up to this point  it obtains  and releases  only shared locks ? it locks the leaf node in exclusive mode and inserts or deletes the key value ? if it needs to split a node or coalesce it with its siblings  or redistribute key values between siblings  the crabbing protocol locks the parent of the node in exclusive mode after performing these actions  it releases the locks on the node and siblings if the parent requires splitting  coalescing  or redistribution of key values  the protocol retains the lock on the parent  and splitting  coalescing  or redistribution propagates further in the same manner otherwise  it releases the lock on the parent the protocol gets its name from the way in which crabs advance by moving sideways  moving the legs on one side  then the legs on the other  and so on alternately the progress of locking while the protocol both goes down the tree and goes back up  in case of splits  coalescing  or redistribution  proceeds in a similar crab-like manner once a particular operation releases a lock on a node  other operations can access that node there is a possibility of deadlocks between search operations coming down the tree  and splits  coalescing  or redistribution propagating up the tree the system can easily handle such deadlocks by restarting the search operation from the root  after releasing the locks held by the operation the secondtechnique achieves evenmore concurrency  avoiding even holding the lock on one node while acquiring the lock on another node  by using a modified version of b + -trees called b-link trees ; b-link trees require that every thesumit67.blogspot.com 706 chapter 15 concurrency control node  including internal nodes  not just the leaves  maintain a pointer to its right sibling this pointer is required because a lookup that occurs while a node is being split may have to search not only that node but also that node ? s right sibling  if one exists   we shall illustrate this technique with an example later  but we first present the modified procedures of the b-link-tree locking protocol ? lookup each node of the b + -treemust be locked in sharedmode before it is accessed a lock on a nonleaf node is released before any lock on any other node in the b + -tree is requested if a split occurs concurrently with a lookup  the desired search-key valuemay no longer appearwithin the range of values represented by a node accessed during lookup in such a case  the search-key value is in the range represented by a sibling node  which the system locates by following the pointer to the right sibling however  the system locks leaf nodes following the two-phase locking protocol  as section 15.8.3 describes  to avoid the phantom phenomenon ? insertion and deletion the system follows the rules for lookup to locate the leaf node into which it will make the insertion or deletion it upgrades the shared-mode lock on this node to exclusive mode  and performs the insertion or deletion it locks leaf nodes affected by insertion or deletion following the two-phase locking protocol  as section 15.8.3 describes  to avoid the phantom phenomenon ? split if the transaction splits a node  it creates a new node according to the algorithm of section 11.3 and makes it the right sibling of the original node the right-sibling pointers of both the original node and the new node are set following this  the transaction releases the exclusive lock on the original node  provided it is an internal node ; leaf nodes are locked in two-phase manner   and then requests an exclusive lock on the parent  so that it can insert a pointer to the new node  there is no need to lock or unlock the new node  ? coalescence if a node has too few search-key values after a deletion  the node with which it will be coalescedmust be locked in exclusive mode once the transaction has coalesced these two nodes  it requests an exclusive lock on the parent so that the deleted node can be removed at this point  the transaction releases the locks on the coalesced nodes unless the parent node must be coalesced also  its lock is released observe this important fact  an insertion or deletion may lock a node  unlock it  and subsequently relock it furthermore  a lookup that runs concurrently with a split or coalescence operation may find that the desired search key has been moved to the right-sibling node by the split or coalescence operation as an illustration  consider the b-link tree in figure 15.21 assume that there are two concurrent operations on this b-link tree  1 insert ? chemistry ?  2 look up ? comp sci ? thesumit67.blogspot.com 15.10 concurrency in index structures * * 707 history elec eng biology comp sci elec eng finance history music music physics figure 15.21 b-link tree for department file with n = 3 let us assume that the insertion operation begins first it does a lookup on ? chemistry ?  and finds that the node into which ? chemistry ? should be inserted is full it therefore converts its shared lock on the node to exclusivemode  and creates a new node the original node now contains the search-key values ? biology ? and ? chemistry ?  the new node contains the search-key value ? comp sci ? now assume that a context switch occurs that results in control passing to the lookup operation this lookup operation accesses the root  and follows the pointer to the left child of the root it then accesses that node  and obtains a pointer to the left child this left-child node originally contained the search-key values ? biology ? and ? comp sci ?  since this node is currently locked by the insertion operation in exclusive mode  the lookup operation must wait note that  at this point  the lookup operation holds no locks at all ! the insertion operation now unlocks the leaf node and relocks its parent  this time in exclusive mode it completes the insertion  leaving the b-link tree as in figure 15.22 the lookup operation proceeds however  it is holding a pointer to an incorrect leaf node it therefore follows the right-sibling pointer to locate the next node if this node  too  turns out to be incorrect  the lookup follows that node ? s right-sibling pointer it can be shown that  if a lookup holds a pointer to an incorrect node  then  by following right-sibling pointers  the lookup must eventually reach the correct node lookup and insertion operations can not lead to deadlock coalescing of nodes during deletion can cause inconsistencies  since a lookup may have read a pointer to a deleted node from its parent  before the parent node was updated  and may history elec eng biology chemistry comp sci elec eng finance music music physics comp sci history figure 15.22 insertion of ? chemistry ? into the b-link tree of figure 15.21 thesumit67.blogspot.com 708 chapter 15 concurrency control then try to access the deleted node the lookup would then have to restart from the root leaving nodes uncoalesced avoids such inconsistencies this solution results in nodes that contain too few search-key values and that violate some properties of b + -trees in most databases  however  insertions are more frequent than deletions  so it is likely that nodes that have too few search-key values will gain additional values relatively quickly instead of locking index leaf nodes in a two-phase manner  some index concurrency-control schemes use key-value locking on individual key values  allowing other key values to be inserted or deleted fromthe same leaf key-value locking thus provides increased concurrency using key-value locking na ? ively  however  would allow the phantom phenomenon to occur ; to prevent the phantom phenomenon  the next-key locking technique is used in this technique  every index lookup must lock not only the keys found within the range  or the single key  in case of a point lookup  but also the next-key value ? that is  the key value just greater than the last key value that was within the range also  every insert must lock not only the value that is inserted  but also the next-key value thus  if a transaction attempts to insert a value that was within the range of the index lookup of another transaction  the two transactionswould conflict on the key value next to the inserted key value similarly  deletes must also lock the next-key value to the value being deleted  to ensure that conflictswith subsequent range lookups of other queries are detected 15.11 summary ? when several transactions execute concurrently in the database  the consistency of data may no longer be preserved it is necessary for the system to control the interaction among the concurrent transactions  and this control is achieved through one of a variety of mechanisms called concurrency-control schemes ? to ensure serializability,we can use various concurrency-control schemes all these schemes either delay an operation or abort the transaction that issued the operation the most common ones are locking protocols  timestampordering schemes  validation techniques  and multiversion schemes ? a locking protocol is a set of rules that state when a transaction may lock and unlock each of the data items in the database ? the two-phase locking protocol allows a transaction to lock a new data item only if that transaction has not yet unlocked any data item the protocol ensures serializability  but not deadlock freedom in the absence of information concerning the manner in which data items are accessed  the two-phase locking protocol is both necessary and sufficient for ensuring serializability ? the strict two-phase locking protocol permits release of exclusive locks only at the end of transaction  in order to ensure recoverability and cascadelessness thesumit67.blogspot.com 15.11 summary 709 of the resulting schedules the rigorous two-phase locking protocol releases all locks only at the end of the transaction ? graph-based locking protocols impose restrictions on the order in which items are accessed  and can thereby ensure serializability without requiring the use of two-phase locking  and can additionally ensure deadlock freedom ? various locking protocols do not guard against deadlocks one way to prevent deadlock is to use an ordering of data items  and to request locks in a sequence consistent with the ordering ? another way to prevent deadlock is to use preemption and transaction rollbacks to control the preemption  we assign a unique timestamp to each transaction the system uses these timestamps to decide whether a transaction should wait or roll back if a transaction is rolled back  it retains its old timestampwhen restarted thewound ? wait scheme is a preemptive scheme ? if deadlocks are not prevented  the system must deal with them by using a deadlock detection and recovery scheme to do so  the system constructs a wait-for graph a system is in a deadlock state if and only if the wait-for graph contains a cycle when the deadlock detection algorithm determines that a deadlock exists  the system must recover from the deadlock it does so by rolling back one or more transactions to break the deadlock ? there are circumstances where it would be advantageous to group several data items  and to treat them as one aggregate data item for purposes of working  resulting in multiple levels of granularity we allow data items of various sizes  and define a hierarchy of data items  where the small items are nestedwithin larger ones such a hierarchy can be represented graphically as a tree locks are acquired in root-to-leaf order ; they are released in leaf-to-root order the protocol ensures serializability  but not freedom from deadlock ? atimestamp-ordering scheme ensures serializability by selecting an ordering in advance between every pair of transactions a unique fixed timestamp is associated with each transaction in the system the timestamps of the transactions determine the serializability order thus  if the timestamp of transaction ti is smaller than the timestamp of transaction tj  then the scheme ensures that the produced schedule is equivalent to a serial schedule inwhich transaction ti appears before transaction tj  it does so by rolling back a transaction whenever such an order is violated ? a validation scheme is an appropriate concurrency-control method in cases where a majority of transactions are read-only transactions  and thus the rate of conflicts among these transactions is low a unique fixed timestamp is associated with each transaction in the system the serializability order is determined by the timestamp of the transaction a transaction in this scheme is never delayed it must  however  pass a validation test to complete if it does not pass the validation test  the system rolls it back to its initial state thesumit67.blogspot.com 710 chapter 15 concurrency control ? amultiversion concurrency-control scheme is based on the creation of a new version of a data item for each transaction that writes that item when a read operation is issued  the system selects one of the versions to be read the concurrency-control scheme ensures that the version to be read is selected in a manner that ensures serializability  by using timestamps a read operation always succeeds ? in multiversion timestamp ordering  a write operation may result in the rollback of the transaction ? in multiversion two-phase locking  write operations may result in a lock wait or  possibly  in deadlock ? snapshot isolation is a multiversion concurrency-control protocol based on validation  which  unlike multiversion two-phase locking  does not require transactions to be declared as read-only or update snapshot isolation does not guarantee serializability  but is nevertheless supported by many database systems ? a delete operation may be performed only if the transaction deleting the tuple has an exclusive lock on the tuple to be deleted a transaction that inserts a new tuple into the database is given an exclusive lock on the tuple ? insertions can lead to the phantom phenomenon  in which an insertion logically conflicts with a query even though the two transactions may access no tuple in common such conflict can not be detected if locking is done only on tuples accessed by the transactions locking is required on the data used to find the tuples in the relation the index-locking technique solves this problem by requiring locks on certain index nodes these locks ensure that all conflicting transactions conflict on a real data item  rather than on a phantom ? weak levels of consistency are used in some applicationswhere consistency of query results is not critical  and using serializability would result in queries adversely affecting transaction processing degree-two consistency is one such weaker level of consistency ; cursor stability is a special case of degreetwo consistency  and is widely used ? concurrency control is a challenging task for transactions that span user interactions applications often implement a scheme based on validation of writes using version numbers stored in tuples ; this scheme provides a weak level of serializability  and can be implemented at the application level without modifications to the database ? special concurrency-control techniques can be developed for special data structures often  special techniques are applied in b + -trees to allow greater concurrency these techniques allow nonserializable access to the b + -tree  but they ensure that the b + -tree structure is correct  and ensure that accesses to the database itself are serializable thesumit67.blogspot.com review terms 711 review terms ? concurrency control ? lock types ? shared-mode  s  lock ? exclusive-mode  x  lock ? lock ? compatibility ? request ? wait ? grant ? deadlock ? starvation ? locking protocol ? legal schedule ? two-phase locking protocol ? growing phase ? shrinking phase ? lock point ? strict two-phase locking ? rigorous two-phase locking ? lock conversion ? upgrade ? downgrade ? graph-based protocols ? tree protocol ? commit dependency ? deadlock handling ? prevention ? detection ? recovery ? deadlock prevention ? ordered locking ? preemption of locks ? wait ? die scheme ? wound ? wait scheme ? timeout-based schemes ? deadlock detection ? wait-for graph ? deadlock recovery ? total rollback ? partial rollback ? multiple granularity ? explicit locks ? implicit locks ? intention locks ? intention lock modes ? intention-shared  is  ? intention-exclusive  ix  ? shared and intentionexclusive  six  ? multiple-granularity locking protocol ? timestamp ? system clock ? logical counter ? w-timestamp  q  ? r-timestamp  q  ? timestamp-ordering protocol ? thomas ? write rule ? validation-based protocols ? read phase thesumit67.blogspot.com 712 chapter 15 concurrency control ? validation phase ? write phase ? validation test ? multiversion timestamp ordering ? multiversion two-phase locking ? read-only transactions ? update transactions ? snapshot isolation ? lost update ? first committer wins ? first updater wins ? write skew ? select for update ? insert and delete operations ? phantom phenomenon ? index-locking protocol ? predicate locking ? weak levels of consistency ? degree-two consistency ? cursor stability ? optimistic concurrency control without read validation ? conversations ? concurrency in indices ? crabbing ? b-link trees ? b-link-tree locking protocol ? next-key locking practice exercises 15.1 show that the two-phase locking protocol ensures conflict serializability  and that transactions can be serialized according to their lock points 15.2 consider the following two transactions  t34  read  a  ; read  b  ; if a = 0 then b  = b + 1 ; write  b   t35  read  b  ; read  a  ; if b = 0 then a  = a + 1 ; write  a   add lock and unlock instructions to transactions t31 and t32  so that they observe the two-phase locking protocol can the execution of these transactions result in a deadlock ? 15.3 what benefit does rigorous two-phase locking provide ? how does it compare with other forms of two-phase locking ? thesumit67.blogspot.com practice exercises 713 15.4 consider a database organized in the form of a rooted tree suppose that we insert a dummy vertex between each pair of vertices show that  if we follow the tree protocol on the new tree  we get better concurrency than if we follow the tree protocol on the original tree 15.5 show by example that there are schedules possible under the tree protocol that are not possible under the two-phase locking protocol  and vice versa 15.6 consider the following extension to the tree-locking protocol  which allows both shared and exclusive locks  ? a transaction can be either a read-only transaction  in which case it can request only shared locks  or an update transaction  in which case it can request only exclusive locks ? each transaction must follow the rules of the tree protocol read-only transactionsmaylock any data itemfirst,whereas update transactions must lock the root first show that the protocol ensures serializability and deadlock freedom 15.7 consider the following graph-based locking protocol  which allows only exclusive lock modes  and which operates on data graphs that are in the form of a rooted directed acyclic graph ? a transaction can lock any vertex first ? to lock any other vertex  the transaction must be holding a lock on the majority of the parents of that vertex show that the protocol ensures serializability and deadlock freedom 15.8 consider the following graph-based locking protocol  which allows only exclusive lock modes and which operates on data graphs that are in the form of a rooted directed acyclic graph ? a transaction can lock any vertex first ? to lock any other vertex  the transaction must have visited all the parents of that vertex and must be holding a lock on one of the parents of the vertex show that the protocol ensures serializability and deadlock freedom 15.9 locking is not done explicitly in persistent programming languages rather  objects  or the corresponding pages  must be locked when the objects are accessed most modern operating systems allow the user to set access protections  no access  read  write  on pages  and memory access that violate the access protections result in a protection violation  see the unix mprotect command  for example   describe how the accessprotection mechanism can be used for page-level locking in a persistent programming language thesumit67.blogspot.com 714 chapter 15 concurrency control s x i s true false false x false false false i false false true figure 15.23 lock-compatibility matrix 15.10 consider a database system that includes an atomic increment operation  in addition to the read and write operations let v be the value of data item x the operation increment  x  by c sets the value ofxtov + c in an atomic step the value ofxis not available to the transaction unless the latter executes a read  x   figure 15.23 shows a lock-compatibility matrix for three lock modes  share mode  exclusive mode  and incrementation mode a show that  if all transactions lock the data that they access in the corresponding mode  then two-phase locking ensures serializability b show that the inclusion of increment mode locks allows for increased concurrency  hint  consider check-clearing transactions in our bank example  15.11 in timestamp ordering,w-timestamp  q  denotes the largest timestamp of any transaction that executed write  q  successfully suppose that  instead  we defined it to be the timestamp of the most recent transaction to execute write  q  successfully.would this change in wording make any difference ? explain your answer 15.12 use of multiple-granularity locking may require more or fewer locks than an equivalent system with a single lock granularity provide examples of both situations  and compare the relative amount of concurrency allowed 15.13 consider the validation-based concurrency-control scheme of section 15.5 show that by choosing validation  ti   rather than start  ti   as the timestamp of transaction ti  we can expect better response time  provided that conflict rates among transactions are indeed low 15.14 for each of the following protocols  describe aspects of practical applications that would lead you to suggest using the protocol  and aspects that would suggest not using the protocol  ? two-phase locking ? two-phase locking with multiple-granularity locking thesumit67.blogspot.com practice exercises 715 ? the tree protocol ? timestamp ordering ? validation ? multiversion timestamp ordering ? multiversion two-phase locking 15.15 explain why the following technique for transaction execution may provide better performance than just using strict two-phase locking  first execute the transaction without acquiring any locks and without performing any writes to the database as in the validation-based techniques  but unlike the validation techniques do not perform either validation or writes on the database instead  rerun the transaction using strict twophase locking  hint  consider waits for disk i/o  15.16 consider the timestamp-ordering protocol  and two transactions  one that writes two data items p and q  and another that reads the same two data items give a schedule whereby the timestamp test for a write operation fails and causes the first transaction to be restarted  in turn causing a cascading abort of the other transaction show how this could result in starvation of both transactions  such a situation  where two or more processes carry out actions  but are unable to complete their task because of interaction with the other processes  is called a livelock  15.17 devise a timestamp-based protocol that avoids the phantom phenomenon 15.18 suppose that we use the tree protocol of section 15.1.5 to manage concurrent access to a b + -tree since a split may occur on an insert that affects the root  it appears that an insert operation can not release any locks until it has completed the entire operation under what circumstances is it possible to release a lock earlier ? 15.19 the snapshot isolation protocol uses a validation step which  before performing a write of a data item by transaction t  checks if a transaction concurrent with t has already written the data item a a straightforward implementation uses a start timestamp and a commit timestamp for each transaction  in addition to an update set  that is the set of data items updated by the transaction explain how to perform validation for the first-committer-wins scheme by using the transaction timestamps along with the update sets you may assume that validation and other commit processing steps are executed serially  that is for one transaction at a time  b explain how the validation step can be implemented as part of commit processing for the first-committer-wins scheme  using a modification of the above scheme  where instead of using update sets  each data item has a write timestamp associated with it again  you thesumit67.blogspot.com 716 chapter 15 concurrency control may assume that validation and other commit processing steps are executed serially c the first-updater-wins scheme can be implemented using timestamps as described above  except that validation is done immediately after acquiring an exclusive lock  instead of being done at commit time i explain how to assign write timestamps to data items to implement the first-updater-wins scheme ii show that as a result of locking  if the validation is repeated at commit time the result would not change iii explain why there is no need to perform validation and other commit processing steps serially in this case exercises 15.20 what benefit does strict two-phase locking provide ? what disadvantages result ? 15.21 most implementations of database systems use strict two-phase locking suggest three reasons for the popularity of this protocol 15.22 consider a variant of the tree protocol called the forest protocol the database is organized as a forest of rooted trees each transaction ti must follow the following rules  ? the first lock in each tree may be on any data item ? the second  and all subsequent  locks in a treemay be requested only if the parent of the requested node is currently locked ? data items may be unlocked at any time ? a data item may not be relocked by ti after it has been unlocked by ti  show that the forest protocol does not ensure serializability 15.23 underwhat conditions is it less expensive to avoid deadlock than to allow deadlocks to occur and then to detect them ? 15.24 if deadlock is avoided by deadlock-avoidance schemes  is starvation still possible ? explain your answer 15.25 in multiple-granularity locking  what is the difference between implicit and explicit locking ? 15.26 although six mode is useful in multiple-granularity locking  an exclusive and intention-shared  xis  mode is of no use.why is it useless ? thesumit67.blogspot.com exercises 717 15.27 the multiple-granularity protocol rules specify that a transaction ti can lock a node q in s or is mode only if ti currently has the parent of q locked in either ix or is mode given that six and s locks are stronger than ix or is locks  why does the protocol not allow locking a node in s or is mode if the parent is locked in either six or s mode ? 15.28 when a transaction is rolled back under timestamp ordering  it is assigned a new timestamp why can it not simply keep its old timestamp ? 15.29 show that there are schedules that are possible under the two-phase locking protocol  but are not possible under the timestamp protocol  and vice versa 15.30 under a modified version of the timestamp protocol  we require that a commit bit be tested to seewhether a read request mustwait explain how the commit bit can prevent cascading abort why is this test not necessary for write requests ? 15.31 as discussed in exercise 15.19  snapshot isolation can be implemented using a form of timestamp validation however  unlike the multiversion timestamp-ordering scheme  which guarantees serializability  snapshot isolation does not guarantee serializability explain what is the key difference between the protocols that results in this difference 15.32 outline the key similarities and differences between the timestamp based implementation of the first-committer-wins version of snapshot isolation  described in exercise 15.19  and the optimistic-concurrency-controlwithout read-validation scheme  described in section 15.9.3 15.33 explain the phantom phenomenon why may this phenomenon lead to an incorrect concurrent execution despite the use of the two-phase locking protocol ? 15.34 explain the reason for the use of degree-two consistency.what disadvantages does this approach have ? 15.35 give example schedules to show that with key-value locking  if any of lookup  insert  or delete do not lock the next-key value  the phantom phenomenon could go undetected 15.36 many transactions update a common item  e.g  the cash balance at a branch   and private items  e.g  individual account balances   explain how you can increase concurrency  and throughput  by ordering the operations of the transaction 15.37 consider the following locking protocol  all items are numbered  and once an item is unlocked  only higher-numbered items may be locked locks may be released at any time only x-locks are used show by an example that this protocol does not guarantee serializability thesumit67.blogspot.com 718 chapter 15 concurrency control bibliographical notes gray and reuter  1993  provides detailed textbook coverage of transactionprocessing concepts  including concurrency-control concepts and implementation details bernstein and newcomer  1997  provides textbook coverage of various aspects of transaction processing including concurrency control the two-phase locking protocol was introduced by eswaran et al  1976   the tree-locking protocol is from silberschatz and kedem  1980   other nontwo phase locking protocols that operate on more general graphs are described in yannakakis et al  1979   kedem and silberschatz  1983   and buckley and silberschatz  1985   korth  1983  explores various lock modes that can be obtained from the basic shared and exclusive lock modes practice exercise 15.4 is from buckley and silberschatz  1984   practice exercise 15.6 is from kedem and silberschatz  1983   practice exercise 15.7 is from kedem and silberschatz  1979   practice exercise 15.8 is from yannakakis et al  1979   practice exercise 15.10 is from korth  1983   the locking protocol for multiple-granularity data items is from gray et al  1975  .adetailed description is presented bygray et al  1976  .kedem and silberschatz  1983  formalizes multiple-granularity locking for an arbitrary collection of lock modes  allowing for more semantics than simply read and write   this approach includes a class of lock modes called update modes to dealwith lock conversion carey  1983  extends the multiple-granularity idea to timestamp-based concurrency control an extension of the protocol to ensure deadlock freedom is presented by korth  1982   the timestamp-based concurrency-control scheme is from reed  1983   a timestamp algorithm that does not require any rollback to ensure serializability is presented by buckley and silberschatz  1983   the validation concurrency-control scheme is from kung and robinson  1981   multiversion timestamp order was introduced in reed  1983   amultiversion tree-locking algorithm appears in silberschatz  1982   degree-two consistency was introduced in gray et al  1975   the levels of consistency ? or isolation ? offered in sql are explained and critiqued in berenson et al  1995   many commercial database systems use version-based approaches in combination with locking postgresql  oracle  and sql server all support forms of the snapshot isolation protocol mentioned in section 15.6.2 details can be found in chapters 27  28  and 30  respectively it should be noted that on postgresql  as of version 8.1.4  and oracle  as of version 10g   setting the isolation level to serializable results in the use of snapshot isolation  which does not guarantee serializability fekete et al  2005  describes how to ensure serializable executions under snapshot isolation  by rewriting certain transactions to introduce conflicts ; these conflicts ensure that the transactions can not run concurrently under snapshot isolation ; jorwekar et al  2007  describes an approach  that given a set of  parametrized  transactions running under snapshot isolation  can check if the transactions are vulnerability to nonserializability  thesumit67.blogspot.com bibliographical notes 719 concurrency in b + -trees was studied by bayer and schkolnick  1977  and johnson and shasha  1993   the techniques presented in section 15.10 are based on kung and lehman  1980  and lehman and yao  1981   the technique of keyvalue locking used in aries provides for very high concurrency on b + -tree access and is described in mohan  1990a  and mohan and narang  1992   ellis  1987  presents a concurrency-control technique for linear hashing thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter16 recovery system a computer system  like any other device  is subject to failure from a variety of causes  disk crash  power outage  software error  a fire in the machine room  even sabotage in any failure  information may be lost therefore  the database system must take actions in advance to ensure that the atomicity and durability properties of transactions  introduced in chapter 14  are preserved an integral part of a database system is a recovery scheme that can restore the database to the consistent state that existed before the failure the recovery scheme must also provide high availability ; that is  it must minimize the time for which the database is not usable after a failure 16.1 failure classification there are various types of failure that may occur in a system  each ofwhich needs to be dealt with in a different manner in this chapter  we shall consider only the following types of failure  ? transaction failure there are twotypes of errors thatmay cause a transaction to fail  ? logical error the transaction can no longer continue with its normal execution because of some internal condition  such as bad input  data not found  overflow  or resource limit exceeded ? system error the system has entered an undesirable state  for example  deadlock   as a result of which a transaction can not continue with its normal execution the transaction  however  can be reexecuted at a later time ? system crash there is a hardware malfunction  or a bug in the database software or the operating system  that causes the loss of the content of volatile storage  and brings transaction processing to a halt the content of nonvolatile storage remains intact  and is not corrupted 721 thesumit67.blogspot.com 722 chapter 16 recovery system the assumption that hardware errors and bugs in the software bring the system to a halt  but do not corrupt the nonvolatile storage contents  is known as the fail-stop assumption.well-designed systems have numerous internal checks  at the hardware and the software level  that bring the system to a halt when there is an error hence  the fail-stop assumption is a reasonable one ? disk failure a disk block loses its content as a result of either a head crash or failure during a data-transfer operation copies of the data on other disks  or archival backups on tertiary media  such as dvd or tapes  are used to recover from the failure to determine how the system should recover from failures  we need to identify the failure modes of those devices used for storing data next  we must consider how these failure modes affect the contents of the database we can then propose algorithms to ensure database consistency and transaction atomicity despite failures these algorithms  known as recovery algorithms  have two parts  1 actions taken during normal transaction processing to ensure that enough information exists to allow recovery from failures 2 actions taken after a failure to recover the database contents to a state that ensures database consistency  transaction atomicity  and durability 16.2 storage as we saw in chapter 10  the various data items in the database may be stored and accessed in a number of different storage media in section 14.3  we saw that storage media can be distinguished by their relative speed  capacity  and resilience to failure.we identified three categories of storage  ? volatile storage ? nonvolatile storage ? stable storage stable storage or  more accurately  an approximation thereof  plays a critical role in recovery algorithms 16.2.1 stable-storage implementation to implement stable storage  we need to replicate the needed information in several nonvolatile storagemedia  usually disk  with independent failure modes  and to update the information in a controlledmanner to ensure that failure during data transfer does not damage the needed information thesumit67.blogspot.com 16.2 storage 723 recall  from chapter 10  that raid systems guarantee that the failure of a single disk  even during data transfer  will not result in loss of data the simplest and fastest form of raid is the mirrored disk  which keeps two copies of each block  on separate disks other forms of raid offer lower costs  but at the expense of lower performance raid systems  however  can not guard against data loss due to disasters such as fires or flooding many systems store archival backups of tapes off site to guard against such disasters.however  since tapes can not be carried off site continually  updates since the most recent time that tapes were carried off site could be lost in such a disaster more secure systems keep a copy of each block of stable storage at a remote site  writing it out over a computer network  in addition to storing the block on a local disk system since the blocks are output to a remote system as and when they are output to local storage  once an output operation is complete  the output is not lost  even in the event of a disaster such as a fire or flood we study such remote backup systems in section 16.9 in the remainder of this section  we discuss how storage media can be protected from failure during data transfer block transfer between memory and disk storage can result in  ? successful completion the transferred information arrived safely at its destination ? partial failure a failure occurred in the midst of transfer  and the destination block has incorrect information ? total failure the failure occurred sufficiently early during the transfer that the destination block remains intact we require that  if a data-transfer failure occurs  the system detects it and invokes a recovery procedure to restore the block to a consistent state to do so  the system must maintain two physical blocks for each logical database block ; in the case of mirrored disks  both blocks are at the same location ; in the case of remote backup  one of the blocks is local  whereas the other is at a remote site an output operation is executed as follows  1 write the information onto the first physical block 2 when the firstwrite completes successfully,write the same information onto the second physical block 3 the output is completed only after the second write completes successfully if the system fails while blocks are being written  it is possible that the two copies of a block are inconsistentwith each other during recovery  for each block  the system would need to examine two copies of the blocks if both are the same and no detectable error exists  then no further actions are necessary  recall that errors in a disk block  such as a partial write to the block  are detected by storing a checksum with each block  if the system detects an error in one block  then it thesumit67.blogspot.com 724 chapter 16 recovery system replaces its content with the content of the other block if both blocks contain no detectable error  but they differ in content  then the system replaces the content of the first block with the value of the second this recovery procedure ensures that a write to stable storage either succeeds completely  that is  updates all copies  or results in no change the requirement of comparing every corresponding pair of blocks during recovery is expensive to meet.we can reduce the cost greatly by keeping track of block writes that are in progress  using a small amount of nonvolatile ram on recovery  only blocks for which writes were in progress need to be compared the protocols forwriting out a block to a remote site are similar to the protocols for writing blocks to a mirrored disk system  which we examined in chapter 10  and particularly in practice exercise 10.3 we can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage although a large number of copies reduces the probability of a failure to even lower than two copies do  it is usually reasonable to simulate stable storage with only two copies 16.2.2 data access as we saw in chapter 10  the database system resides permanently on nonvolatile storage  usually disks  and only parts of the database are in memory at any time.1 the database is partitioned into fixed-length storage units called blocks blocks are the units of data transfer to and from disk  and may contain several data items we shall assume that no data item spans two or more blocks this assumption is realistic for most data-processing applications  such as a bank or a university transactions input information from the disk to main memory  and then output the information back onto the disk the input and output operations are done in block units the blocks residing on the disk are referred to as physical blocks ; the blocks residing temporarily in main memory are referred to as buffer blocks the area of memory where blocks reside temporarily is called the disk buffer block movements between disk and main memory are initiated through the following two operations  1 input  b  transfers the physical block b to main memory 2 output  b  transfers the buffer block b to the disk  and replaces the appropriate physical block there figure 16.1 illustrates this scheme conceptually  each transaction ti has a private work area in which copies of data items accessed and updated by ti are kept the system creates this work area when the transaction is initiated ; the system removes it when the transaction 1there is a special category of database system  called main-memory database systems  where the entire database can be loaded into memory at once.we consider such systems in section 26.4 thesumit67.blogspot.com 16.2 storage 725 a b input  a  output  b  b main memory disk figure 16.1 block storage operations either commits or aborts each data item x kept in the work area of transaction ti is denoted by xi transaction ti interactswith the database system by transferring data to and fromits work area to the system buffer.we transfer data by these two operations  1 read  x  assigns the value of data item x to the local variable xi it executes this operation as follows  a if block bx on which x resides is not in main memory  it issues input  bx   b it assigns to xi the value of x from the buffer block 2 write  x  assigns the value of local variable xi to data item x in the buffer block it executes this operation as follows  a if block bx on which x resides is not in main memory  it issues input  bx   b it assigns the value of xi to x in buffer bx note that both operations may require the transfer of a block from disk to main memory they do not  however  specifically require the transfer of a block from main memory to disk a buffer block is eventually written out to the disk either because the buffer manager needs the memory space for other purposes or because the database system wishes to reflect the change to b on the disk.we shall say that the database system performs a force-output of buffer b if it issues an output  b   when a transaction needs to access a data item x for the first time  it must execute read  x   the system then performs all updates to x on xi at any point during its execution a transaction may execute write  x  to reflect the change to x in the database itself ; write  x  must certainly be done after the final write to x thesumit67.blogspot.com 726 chapter 16 recovery system the output  bx  operation for the buffer block bx on which x resides does not need to take effect immediately after write  x  is executed  since the block bx may contain other data items that are still being accessed thus  the actual output may take place later notice that  if the system crashes after the write  x  operationwas executed but before output  bx  was executed  the newvalue of x is neverwritten to disk and  thus  is lost as we shall see shortly  the database system executes extra actions to ensure that updates performed by committed transactions are not lost even if there is a system crash 16.3 recovery and atomicity consider again our simplified banking system and a transaction ti that transfers $ 50 from account a to account b  with initial values of a and b being $ 1000 and $ 2000  respectively suppose that a system crash has occurred during the execution of ti  after output  ba  has takenplace  but before output  bb  was executed,where ba and bb denote the buffer blocks on which aand b reside since the memory contents were lost  we do not know the fate of the transaction when the system restarts  the value of a would be $ 950  while that of b would be $ 2000  which is clearly inconsistent with the atomicity requirement for transaction ti  unfortunately  there is no way to find out by examining the database state what blocks had been output  and what had not  before the crash it is possible that the transaction completed  updating the database on stable storage from an initial state with the values of a and b being $ 1000 and $ 1950 ; it is also possible that the transaction did not affect the stable storage at all  and the values of a and b were $ 950 and $ 2000 initially ; or that the updated b was output but not the updated a ; or that the updated a was output but the updated b was not our goal is to perform either all or no database modifications made by ti  however  if ti performed multiple database modifications  several output operations may be required  and a failure may occur after some of these modifications have been made  but before all of them are made to achieve our goal of atomicity  we must first output to stable storage information describing the modifications  without modifying the database itself as we shall see  this information can help us ensure that all modifications performed by committed transactions are reflected in the database  perhaps during the course of recovery actions after a crash   this information can also help us ensure that no modifications made by an aborted transaction persist in the database 16.3.1 log records the most widely used structure for recording database modifications is the log the log is a sequence of log records  recording all the update activities in the database there are several types of log records an update log record describes a single database write it has these fields  thesumit67.blogspot.com 16.3 recovery and atomicity 727 shadow copies and shadow paging in the shadow-copy scheme  a transaction that wants to update the database first creates a complete copy of the database all updates are done on the new database copy  leaving the original copy  the shadow copy  untouched if at any point the transaction has to be aborted  the system merely deletes the new copy the old copy of the database has not been affected the current copy of the database is identified by a pointer  called db-pointer  which is stored on disk if the transaction partially commits  that is  executes its final statement  it is committed as follows  first  the operating system is asked to make sure that all pages of the new copy of the database have been written out to disk  unix systems use the fsync command for this purpose  after the operating system has written all the pages to disk  the database system updates the pointer dbpointer to point to the new copy of the database ; the new copy then becomes the current copy of the database the old copy of the database is then deleted the transaction is said to have been committed at the point where the updated db-pointer is written to disk the implementation actually depends on the write todb-pointer being atomic ; that is  either all its bytes are written or none of its bytes are written disk systems provide atomic updates to entire blocks  or at least to a disk sector in other words  the disk system guarantees that it will update db-pointer atomically  as long as we make sure that db-pointer lies entirely in a single sector  which we can ensure by storing db-pointer at the beginning of a block shadow copy schemes are commonly used by text editors  saving the file is equivalent to transaction commit  while quitting without saving the file is equivalent to transaction abort   shadow copying can be used for small databases  but copying a large database would be extremely expensive a variant of shadowcopying  called shadow-paging  reduces copying as follows  the scheme uses a page table containing pointers to all pages ; the page table itself and all updated pages are copied to a new location any page which is not updated by a transaction is not copied  but instead the new page table just stores a pointer to the original page.when a transaction commits  it atomically updates the pointer to the page table  which acts as db-pointer  to point to the new copy shadow paging unfortunately does not work well with concurrent transactions and is not widely used in databases ? transaction identifier  which is the unique identifier of the transaction that performed the write operation ? data-item identifier  which is the unique identifier of the data item written typically  it is the location on disk of the data item  consisting of the block identifier of the block on which the data item resides  and an offset within the block ? old value  which is the value of the data item prior to the write thesumit67.blogspot.com 728 chapter 16 recovery system ? new value  which is the value that the data item will have after the write we represent an update log record as < ti  xj  v1  v2 >  indicating that transaction ti has performed a write on data item xj  xj had value v1 before the write  and has value v2 after the write other special log records exist to record significant events during transaction processing  such as the start of a transaction and the commit or abort of a transaction among the types of log records are  ? < ti start >  transaction ti has started ? < ti commit >  transaction ti has committed ? < ti abort >  transaction ti has aborted we shall introduce several other types of log records later whenever a transaction performs a write  it is essential that the log record for that write be created and added to the log  before the database is modified once a log record exists  we can output the modification to the database if that is desirable also  we have the ability to undo a modification that has already been output to the database.we undo it by using the old-value field in log records for log records to be useful for recovery from system and disk failures  the log must reside in stable storage for now  we assume that every log record is written to the end of the log on stable storage as soon as it is created in section 16.5  we shall see when it is safe to relax this requirement so as to reduce the overhead imposed by logging observe that the log contains a complete record of all database activity as a result  the volume of data stored in the log may become unreasonably large in section 16.3.6  we shall show when it is safe to erase log information 16.3.2 database modification as we noted earlier  a transaction creates a log record prior to modifying the database the log records allow the system to undo changes made by a transaction in the event that the transaction must be aborted ; they allow the system also to redo changes made by a transaction if the transaction has committed but the system crashed before those changes could be stored in the database on disk in order for us to understand the role of these log records in recovery  we need to consider the steps a transaction takes in modifying a data item  1 the transaction performs some computations in its own private part of main memory 2 the transaction modifies the data block in the disk buffer in main memory holding the data item 3 the database system executes the output operation thatwrites the data block to disk thesumit67.blogspot.com 16.3 recovery and atomicity 729 we say a transaction modifies the database if it performs an update on a disk buffer  or on the disk itself ; updates to the private part of main memory do not count as database modifications if a transaction does not modify the database until it has committed  it is said to use the deferred-modification technique if database modifications occurwhile the transaction is still active  the transaction is said to use the immediate-modification technique deferred modification has the overhead that transactions need to make local copies of all updated data items ; further  if a transaction reads a data item that it has updated  it must read the value from its local copy the recovery algorithmswe describe in this chapter support immediatemodification as described  they work correctly even with deferred modification  but can be optimized to reduce overhead when used with deferred modification ; we leave details as an exercise a recovery algorithm must take into account a variety of factors  including  ? the possibility that a transaction may have committed although some of its database modifications exist only in the disk buffer in main memory and not in the database on disk ? the possibility that a transaction may have modified the database while in the active state and  as a result of a subsequent failure  may need to abort because all database modifications must be preceded by the creation of a log record  the system has available both the old value prior to the modification of the data item and the new value that is to be written for the data item this allows the system to perform undo and redo operations as appropriate ? undo using a log record sets the data item specified in the log record to the old value ? redo using a log record sets the data item specified in the log record to the new value 16.3.3 concurrency control and recovery if the concurrency control scheme allows a data item x that has been modified by a transaction t1 to be further modified by another transaction t2 before t1 commits  then undoing the effects of t1 by restoring the old value of x  before t1 updated x  would also undo the effects of t2 to avoid such situations  recovery algorithms usually require that if a data item has been modified by a transaction  no other transaction can modify the data item until the first transaction commits or aborts this requirement can be ensured by acquiring an exclusive lock on any updated data item and holding the lock until the transaction commits ; in other words  by using strict two-phase locking snapshot-isolation and validationthesumit67 blogspot.com 730 chapter 16 recovery system based concurrency-control techniques also acquire exclusive locks on data items at the time of validation  before modifying the data items  and hold the locks until the transaction is committed ; as a result the above requirement is satisfied even by these concurrency control protocols we discuss later  in section 16.7  how the above requirement can be relaxed in certain cases when either snapshot-isolation or validation is used for concurrency control  database updates of a transaction are  conceptually  deferred until the transaction is partially committed ; the deferred-modification technique is a natural fit with these concurrency control schemes however  it is worth noting that some implementations of snapshot isolation use immediate modification  but provide a logical snapshot on demand  when a transaction needs to read an item that a concurrent transaction has updated  a copy of the  already updated  item is made  and updates made by concurrent transactions are rolled back on the copy of the item similarly  immediate modification of the database is a natural fit with two-phase locking  but deferred modification can also be used with two-phase locking 16.3.4 transaction commit we say that a transaction has committed when its commit log record,which is the last log record of the transaction  has been output to stable storage ; at that point all earlier log records have already been output to stable storage thus  there is enough information in the log to ensure that even if there is a system crash  the updates of the transaction can be redone if a system crash occurs before a log record < ti commit > is output to stable storage  transaction ti will be rolled back thus  the output of the block containing the commit log record is the single atomic action that results in a transaction getting committed.2 with most log-based recovery techniques  including the ones we describe in this chapter  blocks containing the data items modified by a transaction do not have to be output to stable storage when the transaction commits  but can be output some time later.we discuss this issue further in section 16.5.2 16.3.5 using the log to redo and undo transactions we now provide an overview of how the log can be used to recover from a system crash  and to roll back transactions during normal operation however,we postpone details of the procedures for failure recovery and rollback to section 16.4 consider our simplified banking system let t0 be a transaction that transfers $ 50 from account a to account b  2the output of a block can be made atomic by techniques for dealing with data-transfer failure  as described earlier in section 16.2.1 thesumit67.blogspot.com 16.3 recovery and atomicity 731 < t0 start > < t0  a  1000  950 > < t0  b  2000  2050 > < t0 commit > < t1 start > < t1  c  700  600 > < t1 commit > figure 16.2 portion of the system log corresponding to t0 and t1 t0  read  a  ; a  = a  50 ; write  a  ; read  b  ; b  = b + 50 ; write  b   let t1 be a transaction that withdraws $ 100 from account c  t1  read  c  ; c  = c  100 ; write  c   the portion of the log containing the relevant information concerning these two transactions appears in figure 16.2 figure 16.3 shows one possible order in which the actual outputs took place in both the database system and the log as a result of the execution of t0 and t1.3 using the log  the system can handle any failure that does not result in the loss of information in nonvolatile storage the recovery scheme uses two recovery procedures both these procedures make use of the log to find the set of data items updated by each transaction ti  and their respective old and new values ? redo  ti  sets the value of all data items updated by transaction ti to the new values the order in which updates are carried out by redo is important ; when recovering from a system crash  if updates to a particular data item are applied in an order different from the order in which they were applied originally  the final state of that data item will have a wrong value most recovery algorithms  including the one we describe in section 16.4  do not perform redo of each transaction separately ; instead they perform a single scan of the log  during which redo actions are performed for each log record as it is encountered this approach ensures the order of updates is preserved  3notice that this order could not be obtained using the deferred-modification technique  because the database is modified by t0 before it commits  and likewise for t1 thesumit67.blogspot.com 732 chapter 16 recovery system log database a = 950 b = 2050 c = 600 < t0 start > < t0  a  1000  950 > < t0  b  2000  2050 > < t0 commit > < t1 start > < t1  c  700  600 > < t1 commit > figure 16.3 state of system log and database corresponding to t0 and t1 and is more efficient since the log needs to be read only once overall  instead of once per transaction ? undo  ti  restores the value of all data items updated by transaction ti to the old values in the recovery scheme that we describe in section 16.4  ? the undo operation not only restores the data items to their old value  but alsowrites log records to record the updates performed as part of the undo process these log records are special redo-only log records  since they do not need to contain the old-value of the updated data item as with the redo procedure  the order in which undo operations are performed is important ; again we postpone details to section 16.4 ? when the undo operation for transaction ti completes  it writes a < ti abort > log record  indicating that the undo has completed as we shall see in section 16.4  the undo  ti  procedure is executed only once for a transaction  if the transaction is rolled back during normal processing or if on recovering from a system crash  neither a commit nor an abort record is found for transaction ti  as a result  every transaction will eventually have either a commit or an abort record in the log after a system crash has occurred  the system consults the log to determine which transactions need to be redone  and which need to be undone so as to ensure atomicity ? transaction ti needs to be undone if the log contains the record < ti start >  but does not contain either the record < ti commit > or the record < ti abort >  ? transaction ti needs to be redone if the log contains the record < ti start > and either the record < ti commit > or the record < ti abort >  itmay seem strange to redo ti if the record < ti abort > is in the log to see why this works  note thesumit67.blogspot.com 16.3 recovery and atomicity 733 < t0 start > < t0  a  1000  950 > < t0  b  2000  2050 > < t0 start > < t0  a  1000  950 > < t0  b  2000  2050 > < t0 commit > < t1 start > < t1  c  700  600 > < t0 start > < t0  a  1000  950 > < t0  b  2000  2050 > < t0 commit > < t1 start > < t1  c  700  600 > < t1 commit >  a   b   c  figure 16.4 the same log  shown at three different times that if < ti abort > is in the log  so are the redo-only records written by the undo operation thus  the end resultwill be to undo ti ? s modifications in this case this slight redundancy simplifies the recovery algorithm and enables faster overall recovery time as an illustration  return to our banking example  with transaction t0 and t1 executed one after the other in the order t0 followed by t1 suppose that the system crashes before the completion of the transactions.we shall consider three cases the state of the logs for each of these cases appears in figure 16.4 first  let us assume that the crash occurs just after the log record for the step  write  b  of transaction t0 has been written to stable storage  figure 16.4a  .when the system comes back up  it finds the record < t0 start > in the log  but no corresponding < t0 commit > or < t0 abort > record thus  transaction t0 must be undone  so an undo  t0  is performed as a result  the values in accounts a and b  on the disk  are restored to $ 1000 and $ 2000  respectively next  let us assume that the crash comes just after the log record for the step  write  c  of transaction t1 has been written to stable storage  figure 16.4b   when the system comes back up  two recovery actions need to be taken the operation undo  t1  must be performed  since the record < t1 start > appears in the log  but there is no record < t1 commit > or < t1 abort >  the operation redo  t0  must be performed  since the log contains both the record < t0 start > and the record < t0 commit >  at the end of the entire recovery procedure  the values of accounts a  b  and c are $ 950  $ 2050  and $ 700  respectively finally  let us assume that the crash occurs just after the log record  < t1 commit > thesumit67.blogspot.com 734 chapter 16 recovery system has been written to stable storage  figure 16.4c   when the system comes back up  both t0 and t1 need to be redone  since the records < t0 start > and < t0 commit > appear in the log  as do the records < t1 start > and < t1 commit > .after the system performs the recovery procedures redo  t0  and redo  t1   the values in accounts a  b  and c are $ 950  $ 2050  and $ 600  respectively 16.3.6 checkpoints when a system crash occurs  we must consult the log to determine those transactions that need to be redone and those that need to be undone in principle  we need to search the entire log to determine this information there are two major difficulties with this approach  1 the search process is time-consuming 2 most of the transactions that  according to our algorithm  need to be redone have already written their updates into the database although redoing them will cause no harm  it will nevertheless cause recovery to take longer to reduce these types of overhead  we introduce checkpoints we describe below a simple checkpoint scheme that  a  does not permit any updates to be performed while the checkpoint operation is in progress  and  b  outputs all modified buffer blocks to disk when the checkpoint is performed we discuss later how to modify the checkpointing and recovery procedures to provide more flexibility by relaxing both these requirements a checkpoint is performed as follows  1 output onto stable storage all log records currently residing in main memory 2 output to the disk all modified buffer blocks 3 output onto stable storage a log record of the form < checkpoint l >  where l is a list of transactions active at the time of the checkpoint transactions are not allowed to perform any update actions  such as writing to a buffer block or writing a log record  while a checkpoint is in progress we discuss how this requirement can be enforced  later  in section 16.5.2 the presence of a < checkpoint l > record in the log allows the system to streamline its recovery procedure consider a transaction ti that completed prior to the checkpoint for such a transaction  the < ti commit > record  or < ti abort > record  appears in the log before the < checkpoint > record any database modifications made by ti must have been written to the database either prior to the checkpoint or as part of the checkpoint itself thus  at recovery time  there is no need to perform a redo operation on ti  after a system crash has occurred  the system examines the log to find the last < checkpoint l > record  this can be done by searching the log backward  from the end of the log  until the first < checkpoint l > record is found   thesumit67.blogspot.com 16.4 recovery algorithm 735 the redo or undo operations need to be applied only to transactions in l  and to all transactions that started execution after the < checkpoint l > record was written to the log let us denote this set of transactions as t ? for all transactions tk in t that have no < tk commit > record or < tk abort > record in the log  execute undo  tk   ? for all transactions tk in t such that either the record < tk commit > or the record < tk abort > appears in the log  execute redo  tk   note that we need only examine the part of the log starting with the last checkpoint log record to find the set of transactions t  and to find outwhether a commit or abort record occurs in the log for each transaction in t as an illustration  consider the set of transactions  t0  t1      t100   suppose that the most recent checkpoint took place during the execution of transaction t67 and t69  while t68 and all transactions with subscripts lower than 67 completed before the checkpoint thus  only transactions t67  t69      t100 need to be considered during the recovery scheme each of them needs to be redone if it has completed  that is  either committed or aborted  ; otherwise  it was incomplete  and needs to be undone consider the set of transactions l in a checkpoint log record for each transaction ti in l  log records of the transaction that occur prior to the checkpoint log record may be needed to undo the transaction  in case it does not commit however  all log records prior to the earliest of the < ti start > log records  among transactions ti in l  are not needed once the checkpoint has completed these log records can be erased whenever the database system needs to reclaim the space occupied by these records the requirement that transactions must not perform any updates to buffer blocks or to the log during checkpointing can be bothersome  since transaction processing has to halt while a checkpoint is in progress a fuzzy checkpoint is a checkpoint where transactions are allowed to perform updates even while buffer blocks are being written out section 16.5.4 describes fuzzy-checkpointing schemes later in section 16.8 we describe a checkpoint scheme that is not only fuzzy  but does not even require all modified buffer blocks to be output to disk at the time of the checkpoint 16.4 recovery algorithm until now  in discussing recovery  we have identified transactions that need to be redone and those that need to be undone  but we have not given a precise algorithm for performing these actions we are now ready to present the full recovery algorithm using log records for recovery from transaction failure and a combination of the most recent checkpoint and log records to recover from a system crash thesumit67.blogspot.com 736 chapter 16 recovery system the recovery algorithm described in this section requires that a data item that has been updated by an uncommitted transaction can not be modified by any other transaction  until the first transaction has either committed or aborted recall that this restriction was discussed earlier  in section 16.3.3 16.4.1 transaction rollback first consider transaction rollback during normal operation  that is  not during recovery froma system crash  .rollback of a transaction ti is performed as follows  1 the log is scanned backward  and for each log record of ti of the form < ti  xj  v1  v2 > that is found  a the value v1 is written to data item xj  and b aspecial redo-only log record < ti  xj  v1 > is written to the log  where v1 is the value being restored to data item xj during the rollback these log records are sometimes called compensation log records such records do not need undo information  since we never need to undo such an undo operation we shall explain later how they are used 2 once the log record < ti start > is found the backward scan is stopped  and a log record < ti abort > is written to the log observe that every update action performed by the transaction or on behalf of the transaction  including actions taken to restore data items to their old value  have now been recorded in the log in section 16.4.2 we shall see why this is a good idea 16.4.2 recovery after a system crash recovery actions  when the database system is restarted after a crash  take place in two phases  1 in the redo phase  the system replays updates of all transactions by scanning the log forward from the last checkpoint the log records that are replayed include log records for transactions that were rolled back before system crash  and those that had not committed when the system crash occurred this phase also determines all transactions that were incomplete at the time of the crash  and must therefore be rolled back such incomplete transactions would either have been active at the time of the checkpoint  and thus would appear in the transaction list in the checkpoint record  orwould have started later ; further  such incomplete transactions would have neither a < ti abort > nor a < ti commit > record in the log the specific steps taken while scanning the log are as follows  a the list of transactions to be rolled back  undo-list  is initially set to the list l in the < checkpoint l > log record thesumit67.blogspot.com 16.4 recovery algorithm 737 b whenever a normal log record of the form < ti  xj  v1  v2 >  or a redo-only log record of the form < ti  xj  v2 > is encountered  the operation is redone ; that is  the value v2 is written to data item xj  c whenever a log record of the form < ti start > is found  ti is added to undo-list d whenever a log record of the form < ti abort > or < ti commit > is found  ti is removed from undo-list at the end of the redo phase  undo-list contains the list of all transactions that are incomplete  that is  they neither committed nor completed rollback before the crash 2 in the undo phase  the system rolls back all transactions in the undo-list it performs rollback by scanning the log backward from the end a whenever it finds a log record belonging to a transaction in the undolist  it performs undo actions just as if the log record had been found during the rollback of a failed transaction b when the system finds a < ti start > log record for a transaction ti in undo-list  it writes a < ti abort > log record to the log  and removes ti from undo-list c the undo phase terminates once undo-list becomes empty  that is  the system has found < ti start > log records for all transactions that were initially in undo-list after the undo phase of recovery terminates  normal transaction processing can resume observe that the redo phase replays every log record since the most recent checkpoint record in other words  this phase of restart recovery repeats all the update actions that were executed after the checkpoint  and whose log records reached the stable log the actions include actions of incomplete transactions and the actions carried out to rollback failed transactions the actions are repeated in the same order in which they were originally carried out ; hence  this process is called repeating history although it may appear wasteful  repeating history even for failed transactions simplifies recovery schemes figure 16.5 shows an example of actions logged during normal operation  and actions performed during failure recovery in the log shown in the figure  transaction t1 had committed  and transaction t0 had been completely rolled back  before the system crashed observe how the value of data item b is restored during the rollback of t0 observe also the checkpoint record  with the list of active transactions containing t0 and t1 when recovering from a crash  in the redo phase  the system performs a redo of all operations after the last checkpoint record in this phase  the list undo-list initially contains t0 and t1 ; t1 is removed first when its commit log record is found  while t2 is added when its start log record is found transaction t0 is thesumit67.blogspot.com 738 chapter 16 recovery system figure 16.5 example of logged actions  and actions during recovery removed from undo-list when its abort log record is found  leaving only t2 in undo-list the undo phase scans the log backwards from the end  and when it finds a log record of t2 updating a  the old value of ais restored  and a redo-only log recordwritten to the log when the start record for t2 is found  an abort record is added for t2 since undo-list contains no more transactions  the undo phase terminates  completing recovery 16.5 buffer management in this section  we consider several subtle details that are essential to the implementation of a crash-recovery scheme that ensures data consistency and imposes a minimal amount of overhead on interactions with the database 16.5.1 log-record buffering so far  we have assumed that every log record is output to stable storage at the time it is created this assumption imposes a high overhead on system execution for several reasons  typically  output to stable storage is in units of blocks in most cases  a log record is much smaller than a block thus  the output of each log record translates to a much larger output at the physical level furthermore  as we saw in section 16.2.1  the output of a block to stable storage may involve several output operations at the physical level the cost of outputting a block to stable storage is sufficiently high that it is desirable to output multiple log records at once to do so  we write log records to a log buffer in main memory,where they stay temporarily until they are output to stable storage multiple log records can be gathered in the log buffer and output thesumit67.blogspot.com 16.5 buffer management 739 to stable storage in a single output operation the order of log records in the stable storage must be exactly the same as the order in which they were written to the log buffer as a result of log buffering  a log record may reside in only main memory  volatile storage  for a considerable time before it is output to stable storage since such log records are lost if the system crashes  we must impose additional requirements on the recovery techniques to ensure transaction atomicity  ? transaction ti enters the commit state after the < ti commit > log record has been output to stable storage ? before the < ti commit > log record can be output to stable storage  all log records pertaining to transaction ti must have been output to stable storage ? before a block of data in main memory can be output to the database  in nonvolatile storage   all log records pertaining to data in that block must have been output to stable storage this rule is called the write-ahead logging  wal  rule  strictly speaking  the wal rule requires only that the undo information in the log has been output to stable storage  and it permits the redo information to be written later the difference is relevant in systems where undo information and redo information are stored in separate log records  the three rules state situations in which certain log records must have been output to stable storage there is no problem resulting from the output of log records earlier than necessary thus  when the system finds it necessary to output a log record to stable storage  it outputs an entire block of log records  if there are enough log records in main memory to fill a block if there are insufficient log records to fill the block  all log records in main memory are combined into a partially full block and are output to stable storage writing the buffered log to disk is sometimes referred to as a log force 16.5.2 database buffering in section 16.2.2  we described the use of a two-level storage hierarchy the system stores the database in nonvolatile storage  disk   and brings blocks of data into main memory as needed since main memory is typically much smaller than the entire database  it may be necessary to overwrite a block b1 in main memory when another block b2 needs to be brought into memory if b1 has been modified  b1 must be output prior to the input of b2 as discussed in section 10.8.1 in chapter 10  this storage hierarchy is similar to the standard operating-system concept of virtual memory onemight expect that transactions would force-output all modified blocks to disk when they commit such a policy is called the force policy the alternative  the no-force policy  allows a transaction to commit even if it has modified some blocks that have not yet been written back to disk all the recovery algorithms described in this chapter work correctly even with the no-force policy the nothesumit67 blogspot.com 740 chapter 16 recovery system force policy allows faster commit of transactions ; moreover it allows multiple updates to accumulate on a block before it is output to stable storage  which can reduce the number of output operations greatly for frequently updated blocks as a result  the standard approach taken by most systems is the no-force policy similarly  one might expect that blocks modified by a transaction that is still active should not be written to disk this policy is called the no-steal policy the alternative  the steal policy  allows the system to write modified blocks to disk even if the transactions thatmade those modifications have not all committed as long as the write-ahead logging rule is followed  all the recovery algorithms we study in the chapterwork correctly evenwith the steal policy further  the no-steal policy does not work with transactions that perform a large number of updates  since the buffer may get filled with updated pages that can not be evicted to disk  and the transaction can not then proceed.as a result  the standard approach taken by most systems is the steal policy to illustrate the need for the write-ahead logging requirement  consider our banking example with transactions t0 and t1 suppose that the state of the log is  < t0 start > < t0  a  1000  950 > and that transaction t0 issues a read  b   assume that the block on which b resides is not in main memory  and that main memory is full suppose that the block on which a resides is chosen to be output to disk if the system outputs this block to disk and then a crash occurs  the values in the database for accounts a  b  and c are $ 950  $ 2000  and $ 700  respectively this database state is inconsistent however  because of the wal requirements  the log record  < t0  a  1000  950 > must be output to stable storage prior to output of the block on which a resides the system can use the log record during recovery to bring the database back to a consistent state when a block b1 is to be output to disk  all log records pertaining to data in b1 must be output to stable storage before b1 is output it is important that no writes to the block b1 be in progress while the block is being output  since such a write could violate the write-ahead logging rule.we can ensure that there are no writes in progress by using a special means of locking  ? before a transaction performs a write on a data item  it acquires an exclusive lock on the block in which the data item resides the lock is released immediately after the update has been performed ? the following sequence of actions is taken when a block is to be output  ? obtain an exclusive lock on the block  to ensure that no transaction is performing a write on the block thesumit67.blogspot.com 16.5 buffer management 741 ? output log records to stable storage until all log records pertaining to block b1 have been output ? output block b1 to disk ? release the lock once the block output has completed locks on buffer blocks are unrelated to locks used for concurrency-control of transactions  and releasing them in a non-two-phase manner does not have any implications on transaction serializability these locks  and other similar locks that are held for a short duration  are often referred to as latches locks on buffer blocks can also be used to ensure that buffer blocks are not updated  and log records are not generated,while a checkpoint is in progress this restriction may be enforced by acquiring exclusive locks on all buffer blocks  as well as an exclusive lock on the log  before the checkpoint operation is performed these locks can be released as soon as the checkpoint operation has completed database systems usually have a process that continually cycles through the buffer blocks  outputting modified buffer blocks back to disk the above locking protocol must of course be followed when the blocks are output as a result of continuous output of modified blocks  the number of dirty blocks in the buffer  that is  blocks that have been modified in the buffer but have not been subsequently output  is minimized thus  the number of blocks that have to be output during a checkpoint is minimized ; further  when a block needs to be evicted from the buffer it is likely that there will be a non-dirty block available for eviction  allowing the input to proceed immediately instead of waiting for an output to complete 16.5.3 operating system role in buffer management we can manage the database buffer by using one of two approaches  1 the database system reserves part of main memory to serve as a buffer that it  rather than the operating system  manages the database system manages data-block transfer in accordance with the requirements in section 16.5.2 this approach has the drawback of limiting flexibility in the use of main memory the buffer must be kept small enough that other applications have sufficient main memory available for their needs however  even when the other applications are not running  the database will not be able to make use of all the available memory likewise  non-database applications may not use that part of main memory reserved for the database buffer  even if some of the pages in the database buffer are not being used 2 the database system implements its buffer within the virtual memory provided by the operating system since the operating system knows about the memory requirements of all processes in the system  ideally it should be in charge of deciding what buffer blocks must be force-output to disk  and when but  to ensure thewrite-ahead logging requirements in section 16.5.1  the operating system should not write out the database buffer pages itself  thesumit67.blogspot.com 742 chapter 16 recovery system but instead should request the database system to force-output the buffer blocks the database system in turn would force-output the buffer blocks to the database  after writing relevant log records to stable storage unfortunately  almost all current-generation operating systems retain complete control of virtual memory the operating system reserves space on disk for storing virtual-memory pages that are not currently in main memory ; this space is called swap space if the operating system decides to output a block bx  that block is output to the swap space on disk  and there is no way for the database system to get control of the output of buffer blocks therefore  if the database buffer is in virtual memory  transfers between database files and the buffer in virtual memory must be managed by the database system  which enforces the write-ahead logging requirements that we discussed this approach may result in extra output of data to disk if a block bx is output by the operating system  that block is not output to the database instead  it is output to the swap space for the operating system ? s virtual memory when the database system needs to output bx  the operating system may need first to input bx from its swap space thus  instead of a single output of bx  theremay be two outputs of bx  one by the operating system  and one by the database system  and one extra input of bx although both approaches suffer fromsome drawbacks  one or the other must be chosen unless the operating system is designed to support the requirements of database logging 16.5.4 fuzzy checkpointing the checkpointing technique described in section 16.3.6 requires that all updates to the database be temporarily suspended while the checkpoint is in progress if the number of pages in the buffer is large  a checkpoint may take a long time to finish  which can result in an unacceptable interruption in processing of transactions to avoid such interruptions  the checkpointing technique can be modified to permit updates to start once the checkpoint record has been written  but before the modified buffer blocks are written to disk the checkpoint thus generated is a fuzzy checkpoint since pages are output to disk only after the checkpoint record has been written  it is possible that the system could crash before all pages are written thus  a checkpoint on disk may be incomplete one way to deal with incomplete checkpoints is this  the location in the log of the checkpoint record of the last completed checkpoint is stored in a fixed position  last-checkpoint  on disk the system does not update this information when it writes the checkpoint record instead  before it writes the checkpoint record  it creates a list of all modified buffer blocks the last-checkpoint information is updated only after all buffer blocks in the list of modified buffer blocks have been output to disk thesumit67.blogspot.com 16.6 failure with loss of nonvolatile storage 743 even with fuzzy checkpointing  a buffer block must not be updated while it is being output to disk  although other buffer blocks may be updated concurrently the write-ahead log protocol must be followed so that  undo  log records pertaining to a block are on stable storage before the block is output 16.6 failure with loss of nonvolatile storage until now  we have considered only the case where a failure results in the loss of information residing in volatile storage while the content of the nonvolatile storage remains intact although failures in which the content of nonvolatile storage is lost are rare  we nevertheless need to be prepared to deal with this type of failure in this section  we discuss only disk storage our discussions apply as well to other nonvolatile storage types the basic scheme is to dump the entire contents of the database to stable storage periodically ? say  once per day for example,we may dump the database to one ormoremagnetic tapes if a failure occurs that results in the loss of physical database blocks  the system uses the most recent dump in restoring the database to a previous consistent state once this restoration has been accomplished  the system uses the log to bring the database system to the most recent consistent state one approach to database dumping requires that no transaction may be active during the dump procedure  and uses a procedure similar to checkpointing  1 output all log records currently residing in main memory onto stable storage 2 output all buffer blocks onto the disk 3 copy the contents of the database to stable storage 4 output a log record < dump > onto the stable storage steps 1  2  and 4 correspond to the three steps used for checkpoints in section 16.3.6 to recover from the loss of nonvolatile storage  the system restores the database to disk by using the most recent dump then  it consults the log and redoes all the actions since the most recent dump occurred notice that no undo operations need to be executed in case of a partial failure of nonvolatile storage  such as the failure of a single block or a few blocks  only those blocks need to be restored  and redo actions performed only for those blocks a dump of the database contents is also referred to as an archival dump  since we can archive the dumps and use them later to examine old states of the database dumps of a database and checkpointing of buffers are similar most database systems also support an sql dump,which writes out sql ddl statements and sql insert statements to a file  which can then be reexecuted to thesumit67.blogspot.com 744 chapter 16 recovery system re-create the database such dumps are useful when migrating data to a different instance of the database  or to a different version of the database software  since the physical locations and layout may be different in the other database instance or database software version the simple dump procedure described here is costly for the following two reasons first  the entire database must be copied to stable storage  resulting in considerable data transfer second  since transaction processing is halted during the dump procedure  cpu cycles are wasted fuzzy dump schemes have been developed that allow transactions to be active while the dump is in progress they are similar to fuzzy-checkpointing schemes ; see the bibliographical notes for more details 16.7 early lock release and logical undo operations any index used in processing a transaction  such as a b + -tree  can be treated as normal data  but to increase concurrency  we can use the b + -tree concurrencycontrol algorithm described in section 15.10 to allow locks to be released early  in a non-two-phase manner as a result of early lock release  it is possible that a value in a b + -tree node is updated by one transaction t1  which inserts an entry  v1  r1   and subsequently by another transaction t2  which inserts an entry  v2  r2  in the same node,moving the entry  v1  r1  even before t1 completes execution.4 at this point  we can not undo transaction t1 by replacing the contents of the node with the old value prior to t1 performing its insert  since that would also undo the insert performed by t2 ; transaction t2 may still commit  or may have already committed   in this example  the only way to undo the effect of insertion of  v1  r1  is to execute a corresponding delete operation in the rest of this section  we see how to extend the recovery algorithm of section 16.4 to support early lock release 16.7.1 logical operations the insertion and deletion operations are examples of a class of operations that require logical undo operations since they release locks early ; we call such operations logical operations such early lock release is important not only for indices  but also for operations on other system data structures that are accessed and updated very frequently ; examples include data structures that track the blocks containing records of a relation  the free space in a block  and the free blocks in a database if locks were not released early after performing operations on such data structures  transactions would tend to run serially  affecting system performance the theory of conflict serializability has been extended to operations  based on what operations conflict with what other operations for example  two insert 4recall that an entry consists of a key value and a record identifier  or a key value and a record in the case of the leaf level of a b + -tree file organization thesumit67.blogspot.com 16.7 early lock release and logical undo operations 745 operations on a b + -tree do not conflict if they insert different key values  even if they both update overlapping areas of the same index page however  insert and delete operations conflict with other insert and delete operations  as well as with read operations  if they use the same key value see the bibliographical notes for references to more information on this topic operations acquire lower-level lockswhile they execute  but release themwhen they complete ; the corresponding transaction must however retain a higher-level lock in a two-phase manner to prevent concurrent transactions from executing conflicting actions for example  while an insert operation is being performed on a b + -tree page  a short-term lock is obtained on the page  allowing entries in the page to be shifted during the insert ; the short-term lock is released as soon as the page has been updated such early lock release allows a second insert to execute on the same page however  each transaction must obtain a lock on the key values being inserted or deleted  and retain it in a two-phase manner  to prevent a concurrent transaction from executing a conflicting read  insert or delete operation on the same key value once the lower-level lock is released  the operation can not be undone by using the old values of updated data items  and must instead be undone by executing a compensating operation ; such an operation is called a logical undo operation it is important that the lower-level locks acquired during an operation are sufficient to perform a subsequent logical undo of the operation  for reasons explained later in section 16.7.4 16.7.2 logical undo log records to allow logical undo of operations  before an operation is performed to modify an index  the transaction creates a log record < ti  oj  operation-begin >  where oj is a unique identifier for the operation instance.5 while the system is executing the operation  it creates update log records in the normal fashion for all updates performed by the operation thus  the usual old-value and new-value information is written out as usual for each update performed by the operation ; the old-value information is required in case the transaction needs to be rolled back before the operation completes when the operation finishes  it writes an operation-end log record of the form < ti  oj  operation-end  u >  where the u denotes undo information for example  if the operation inserted an entry in a b + -tree  the undo information u would indicate that a deletion operation is to be performed  and would identify the b + -tree and what entry to delete from the tree such logging of information about operations is called logical logging in contrast  logging of old-value and new-value information is called physical logging  and the corresponding log records are called physical log records note that in the above scheme  logical logging is used only for undo  not for redo ; redo operations are performed exclusively using physical log record this is because the state of the database after a system failure may reflect some updates 5the position in the log of the operation-begin log record can be used as the unique identifier thesumit67.blogspot.com 746 chapter 16 recovery system of an operation and not other operations  depending on what buffer blocks had been written to disk before the failure data structures such as b + -treeswould not be in a consistent state  and neither logical redo nor logical undo operations can be performed on an inconsistent data structure to perform logical redo or undo  the database state on disk must be operation consistent  that is  it should not have partial effects of any operation however  as we shall see  the physical redo processing in the redo phase of the recovery scheme  along with undo processing using physical log records ensures that the parts of the database accessed by a logical undo operation are in an operation consistent state  before the logical undo operation is performed an operation is said to be idempotent if executing it several times in a row gives the same result as executing it once operations such as inserting an entry into a b + -treemay not be idempotent  and the recovery algorithm must therefore make sure that an operation that has already been performed is not performed again on the other hand  a physical log record is idempotent  since the corresponding data item would have the same value regardless of whether the logged update is executed one or multiple times 16.7.3 transaction rollback with logical undo when rolling back a transaction ti  the log is scanned backwards  and log records corresponding to ti are processed as follows  1 physical log records encountered during the scan are handled as described earlier  except those that are skipped as described shortly incomplete logical operations are undone using the physical log records generated by the operation 2 completed logical operations  identified by operation-end records  are rolled back differently whenever the system finds a log record < ti  oj  operationend  u >  it takes special actions  a it rolls back the operation by using the undo information u in the log record it logs the updates performed during the rollback of the operation just like updates performed when the operation was first executed at the end of the operation rollback  instead of generating a log record < ti  oj  operation-end  u >  the database system generates a log record < ti  oj  operation-abort >  b as the backward scan of the log continues  the system skips all log records of transaction ti until it finds the log record < ti  oj  operationbegin >  after it finds the operation-begin log record  it processes log records of transaction ti in the normal manner again observe that the system logs physical undo information for the updates performed during rollback  instead of using a redo-only compensation log records this is because a crash may occurwhile a logical undo is in progress  thesumit67.blogspot.com 16.7 early lock release and logical undo operations 747 and on recovery the system has to complete the logical undo ; to do so  restart recovery will undo the partial effects of the earlier undo  using the physical undo information  and then perform the logical undo again observe also that skipping over physical log recordswhen the operationend log record is found during rollback ensures that the old values in the physical log record are not used for rollback  once the operation completes 3 if the system finds a record < ti  oj  operation-abort >  it skips all preceding records  including the operation-end record for oj  until it finds the record < ti  oj  operation-begin >  an operation-abort log record would be found only if a transaction that is being rolled back had been partially rolled back earlier recall that logical operations may not be idempotent  and hence a logical undo operation must not be performed multiple times these preceding log records must be skipped to prevent multiple rollback of the same operation  in case there had been a crash during an earlier rollback  and the transaction had already been partly rolled back 4 as before  when the < ti start > log record has been found  the transaction rollback is complete  and the system adds a record < ti abort > to the log if a failure occurs while a logical operation is in progress  the operation-end log record for the operation will not be found when the transaction is rolled back however  for every update performed by the operation  undo information ? in the form of the old value in the physical log records ? is available in the log the physical log records will be used to roll back the incomplete operation now suppose an operation undo was in progress when the system crash occurred  which could happen if a transaction was being rolled back when the crash occurred then the physical log records written during operation undo would be found  and the partial operation undo would itself be undone using these physical log records continuing in the backward scan of the log  the original operation ? s operation-end record would then be found  and the operation undo would be executed again rolling back the partial effects of the earlier undo operation using the physical log records brings the database to a consistent state  allowing the logical undo operation to be executed again figure 16.6 shows an example of a log generated by two transactions  which add or subtract a value from a data item early lock release on the data item c by transaction t0 after operation o1 completes allows transaction t1 to update the data item using o2  even before t0 completes  but necessitates logical undo the logical undo operation needs to add or subtract a value from the data item  instead of restoring an old value to the data item the annotations on the figure indicate that before an operation completes  rollback can perform physical undo ; after the operation completes and releases lower-level locks  the undo must be performed by subtracting or adding a value  instead of restoring the old value in the example in the figure  t0 rolls back operation o1 by adding 100 to c ; on the other hand  for data item b  which was not subject to early lock release  undo is performed physically observe that t1  thesumit67.blogspot.com 748 chapter 16 recovery system figure 16.6 transaction rollback with logical undo operations which had performed an update on c commits  and its update o2  which added 200 to c and was performed before the undo of o1  has persisted even though o1 has been undone figure 16.7 shows an example of recovery from a crash  with logical undo logging in this example  operation t1 was active and executing operation o4 at the time of checkpoint in the redo pass  the actions of o4 that are after the checkpoint log record are redone at the time of crash  operation o5 was being executed by t2  but the operation was not complete the undo-list contains t1 and t2 at the end of the redo pass during the undo pass  the undo of operation o5 is carried out using the old value in the physical log record  setting c to 400 ; this operation is logged using a redo-only log record the start record of t2 is encountered next  resulting in the addition of < t2 abort > to the log  and removal of t2 from undo-list the next log record encountered is the operation-end record of o4 ; logical undo is performed for this operation by adding 300 to c  which is logged physically  and an operation-abort log record is added for o4 the physical log records that were part of o4 are skipped until the operation-begin log record for o4 is encountered in this example  there are no other intervening log records  but in general log records from other transactions may be found before we reach the operation-begin log record ; such log records should of course not be skipped  unless they are part of a completed operation for the corresponding transaction and the algorithm skips those records   after the operation-begin log record is thesumit67.blogspot.com 16.7 early lock release and logical undo operations 749 was figure 16.7 failure recovery actions with logical undo operations found for o4  a physical log record is found for t1,which is rolled back physically finally the start log record for t1 is found ; this results in < t1 abort > being added to the log  and t1 being deleted from undo-list at this point undo-list is empty  and the undo phase is complete 16.7.4 concurrency issues in logical undo as mentioned earlier  it is important that the lower-level locks acquired during an operation are sufficient to perform a subsequent logical undo of the operation ; otherwise concurrent operations that execute during normal processing may cause problems in the undo-phase for example  suppose the logical undo of operation o1 of transaction t1 can conflict at the data item levelwith a concurrent operation o2 of transaction t2  and o1 completes while o2 does not assume also that neither transaction had committed when the system crashed the physical update log records of o2 may appear before and after the operation-end record for o1  and during recovery updates done during the logical undo of o1 may get fully or partially overwritten by old values during the physical undo of o2 this problemcannot occur if o1 had obtained all the lower-level locks required for the logical undo of o1  since then there can not be such a concurrent o2 thesumit67.blogspot.com 750 chapter 16 recovery system if both the original operation and its logical undo operation access a single page  such operations are called physiological operations  and are discussed in section 16.8   the locking requirement above is met easily.otherwise the details of the specific operation need to be considered when deciding on what lower-level locks need to be obtained for example  update operations on a b + -tree could obtain a short-term lock on the root  to ensure that operations execute serially see the bibliographical notes for references on b + -tree concurrency control and recovery exploiting logical undo logging see the bibliographical notes also for references to an alternative approach  called multi-level recovery  which relaxes this locking requirement 16.8 aries * * the state of the art in recovery methods is best illustrated by the aries recovery method the recovery technique that we described in section 16.4  along with the logical undo logging techniques described in section 16.7  is modeled after aries  but has been simplified significantly to bring out key concepts and make it easier to understand in contrast  aries uses a number of techniques to reduce the time taken for recovery  and to reduce the overhead of checkpointing in particular  aries is able to avoid redoing many logged operations that have already been applied and to reduce the amount of information logged the price paid is greater complexity ; the benefits are worth the price the major differences between aries and the recovery algorithm presented earlier are that aries  1 uses a log sequence number  lsn  to identify log records  and stores lsns in database pages to identifywhich operations have been applied to a database page 2 supports physiological redo operations  which are physical in that the affected page is physically identified  but can be logical within the page for instance  the deletion of a record from a pagemay result in many other records in the page being shifted  if a slotted page structure  section 10.5.2  is used with physical redo logging  all bytes of the page affected by the shifting of records must be logged.with physiological logging  the deletion operation can be logged  resulting in a much smaller log record redo of the deletion operation would delete the record and shift other records as required 3 uses a dirty page table to minimize unnecessary redos during recovery as mentioned earlier  dirty pages are those that have been updated inmemory  and the disk version is not up-to-date 4 uses a fuzzy-checkpointing scheme that records only information about dirty pages and associated information and does not even require writing thesumit67.blogspot.com 16.8 aries * * 751 of dirty pages to disk it flushes dirty pages in the background  continuously  instead of writing them during checkpoints in the rest of this section  we provide an overview of aries the bibliographical notes list references that provide a complete description of aries 16.8.1 data structures each log record in aries has a log sequence number  lsn  that uniquely identifies the record the number is conceptually just a logical identifier whose value is greater for log records that occur later in the log in practice  the lsn is generated in such a way that it can also be used to locate the log record on disk typically  aries splits a log into multiple log files  each of which has a file number when a log file grows to some limit  aries appends further log records to a new log file ; the new log file has a file number that is higher by 1 than the previous log file the lsn then consists of a file number and an offset within the file each page also maintains an identifier called the pagelsn whenever an update operation  whether physical or physiological  occurs on a page  the operation stores the lsn of its log record in the pagelsn field of the page during the redo phase of recovery  any log records with lsn less than or equal to the pagelsn of a page should not be executed on the page  since their actions are already reflected on the page in combination with a scheme for recording pagelsns as part of checkpointing  which we present later  aries can avoid even readingmany pages for which logged operations are already reflected on disk thereby  recovery time is reduced significantly the pagelsn is essential for ensuring idempotence in the presence of physiological redo operations  since reapplying a physiological redo that has already been applied to a page could cause incorrect changes to a page pages should not be flushed to disk while an update is in progress  since physiological operations can not be redone on the partially updated state of the page on disk therefore  aries uses latches on buffer pages to prevent them from being written to disk while they are being updated it releases the buffer page latch only after the update is completed  and the log record for the update has been written to the log each log record also contains the lsn of the previous log record of the same transaction this value  stored in the prevlsn field  permits log records of a transaction to be fetched backward  without reading the whole log there are special redo-only log records generated during transaction rollback  called compensation log records  clrs  in aries these serve the same purpose as the redo-only log records in our earlier recovery scheme in addition clrs serve the role of the operation-abort log records in our scheme the clrs have an extra field  called the undonextlsn  that records the lsn of the log that needs to be undone next  when the transaction is being rolled back this field serves the same purpose as the operation identifier in the operation-abort log record in our earlier recovery scheme  which helps to skip over log records that have already been rolled back thesumit67.blogspot.com 752 chapter 16 recovery system figure 16.8 data structures used in aries the dirtypagetable contains a list of pages that have been updated in the database buffer for each page  it stores the pagelsn and a field called the reclsn  which helps identify log records that have been applied already to the version of the page on disk when a page is inserted into the dirtypagetable  when it is first modified in the buffer pool   the value of reclsn is set to the current end of log whenever the page is flushed to disk  the page is removed from the dirtypagetable a checkpoint log record contains the dirtypagetable and a list of active transactions for each transaction  the checkpoint log record also notes lastlsn  the lsn of the last log record written by the transaction a fixed position on disk also notes the lsn of the last  complete  checkpoint log record figure 16.8 illustrates some of the data structures used in aries the log records shown in the figure are prefixed by their lsn ; these may not be explicitly stored  but inferred from the position in the log  in an actual implementation the data item identifier in a log record is shown in two parts  for example 4894.1 ; the first identifies the page  and the second part identifies a record within the page  we assume a slotted page record organization within a page   note that the log is shown with newest records on top  since older log records  which are on disk  are shown lower in the figure thesumit67.blogspot.com 16.8 aries * * 753 each page  whether in the buffer or on disk  has an associated pagelsn field you can verify that the lsn for the last log record that updated page 4894 is 7567 by comparing pagelsns for the pages in the buffer with the pagelsns for the corresponding pages in stable storage  you can observe that the dirtypagetable contains entries for all pages in the buffer that have been modified since they were fetched fromstable storage the reclsn entry in the dirtypagetable reflects the lsn at the end of the log when the page was added to dirtypagetable  and would be greater than or equal to the pagelsn for that page on stable storage 16.8.2 recovery algorithm aries recovers from a system crash in three passes ? analysis pass  this pass determineswhich transactions to undo,which pages were dirty at the time of the crash  and the lsn from which the redo pass should start ? redo pass  this pass starts from a position determined during analysis  and performs a redo  repeating history  to bring the database to a state it was in before the crash ? undo pass  this pass rolls back all transactions that were incomplete at the time of crash 16.8.2.1 analysis pass the analysis pass finds the last complete checkpoint log record  and reads in the dirtypagetable from this record it then sets redolsn to the minimum of the reclsns of the pages in the dirtypagetable if there are no dirty pages  it sets redolsn to the lsn of the checkpoint log record the redo pass starts its scan of the log from redolsn all the log records earlier than this point have already been applied to the database pages on disk the analysis pass initially sets the list of transactions to be undone  undo-list  to the list of transactions in the checkpoint log record the analysis pass also reads from the checkpoint log record the lsns of the last log record for each transaction in undo-list the analysis pass continues scanning forward fromthe checkpoint.whenever it finds a log record for a transaction not in the undo-list  it adds the transaction to undo-list.whenever it finds a transaction end log record  it deletes the transaction from undo-list all transactions left in undo-list at the end of analysis have to be rolled back later  in the undo pass the analysis pass also keeps track of the last record of each transaction in undo-list  which is used in the undo pass the analysis pass also updates dirtypagetable whenever it finds a log record for an update on a page if the page is not in dirtypagetable  the analysis pass adds it to dirtypagetable  and sets the reclsn of the page to the lsn of the log record thesumit67.blogspot.com 754 chapter 16 recovery system 16.8.2.2 redo pass the redo pass repeats history by replaying every action that is not already reflected inthepage ondisk.the redopass scans the logforwardfromredolsn.whenever it finds an update log record  it takes this action  1 if the page is not in dirtypagetable or the lsn of the update log record is less than the reclsn of the page in dirtypagetable  then the redo pass skips the log record 2 otherwise the redo pass fetches the page from disk  and if the pagelsn is less than the lsn of the log record  it redoes the log record note that if either of the tests is negative  then the effects of the log record have already appeared on the page ; otherwise the effects of the log record are not reflected on the page since aries allows non-idempotent physiological log records  a log record should not be redone if its effect is already reflected on the page if the first test is negative  it is not even necessary to fetch the page from disk to check its pagelsn 16.8.2.3 undo pass and transaction rollback the undo pass is relatively straightforward it performs a single backward scan of the log  undoing all transactions in undo-list the undo pass examines only log records of transactions in undo-list ; the last lsn recorded during the analysis pass is used to find the last log record for each transaction in undo-list whenever an update log record is found  it is used to perform an undo  whether for transaction rollback during normal processing  or during the restart undo pass   the undo pass generates a clr containing the undo action performed  which must be physiological   it sets the undonextlsn of the clr to the prevlsn value of the update log record if a clr is found  its undonextlsn value indicates the lsn of the next log record to be undone for that transaction ; later log records for that transaction have already been rolled back for log records other than clrs  the prevlsn field of the log record indicates the lsn of the next log record to be undone for that transaction the next log record to be processed at each stop in the undo pass is the maximum  across all transactions in undo-list  of next log record lsn figure 16.9 illustrates the recovery actions performed by aries  onanexample log we assume that the last completed checkpoint pointer on disk points to the checkpoint log record with lsn 7568 the prevlsn values in the log records are shown using arrows in the figure  while the undonextlsn value is shown using a dashed arrow for the one compensation log record  with lsn 7565  in the figure the analysis pass would start from lsn 7568  and when it is complete  redolsn would be 7564 thus  the redo pass must start at the log record with lsn 7564 note that this lsn is less than the lsn of the checkpoint log record  since the aries checkpointing algorithm does not flush modified pages to stable storage the dirtypagetable at the end of analysis would include pages 4894  7200 from thesumit67.blogspot.com 16.8 aries * * 755 figure 16.9 recovery actions in aries the checkpoint log record  and 2390 which is updated by the log record with lsn 7570 at the end of the analysis pass  the list of transactions to be undone consists of only t145 in this example the redo pass for the above example starts from lsn 7564 and performs redo of log records whose pages appear in dirtypagetable the undo pass needs to undo only transaction t145  and hence starts from its lastlsn value 7567  and continues backwards until the record < t145 start > is found at lsn 7563 16.8.3 other features among other key features that aries provides are  ? nested top actions  aries allows the logging of operations that should not be undone even if a transaction gets rolled back ; for example  if a transaction allocates a page to a relation  even if the transaction is rolled back the page allocation should not be undone since other transactions may have stored records in the page such operations that should not be undone are called nested top actions such operations can be modeled as operationswhose undo action does nothing in aries  such operations are implementedby creating a thesumit67.blogspot.com 756 chapter 16 recovery system dummy clr whose undonextlsn is set such that transaction rollback skips the log records generated by the operation ? recovery independence  some pages can be recovered independently from others  so that they can be used evenwhile other pages are being recovered if some pages of a disk fail  they can be recoveredwithout stopping transaction processing on other pages ? savepoints  transactions can record savepoints  and can be rolled back partially  up to a savepoint this can be quite useful for deadlock handling  since transactions can be rolled back up to a point that permits release of required locks  and then restarted from that point programmers can also use savepoints to undo a transaction partially  and then continue execution ; this approach can be useful to handle certain kinds of errors detected during the transaction execution ? fine-grained locking  the aries recovery algorithm can be used with index concurrency-control algorithms that permit tuple-level locking on indices  instead of page-level locking  which improves concurrency significantly ? recovery optimizations  the dirtypagetable can be used to prefetch pages during redo  instead of fetching a page only when the system finds a log record to be applied to the page out-of-order redo is also possible  redo can be postponed on a page being fetched from disk  and performed when the page is fetched meanwhile  other log records can continue to be processed in summary  the aries algorithm is a state-of-the-art recovery algorithm  incorporating a variety of optimizations designed to improve concurrency  reduce logging overhead  and reduce recovery time 16.9 remote backup systems traditional transaction-processing systems are centralized or client ? server systems such systems are vulnerable to environmental disasters such as fire  flooding  or earthquakes increasingly  there is a need for transaction-processing systems that can function in spite of system failures or environmental disasters such systems must provide high availability ; that is  the time for which the system is unusable must be extremely small we can achieve high availability by performing transaction processing at one site  called the primary site  and having a remote backup site where all the data from the primary site are replicated the remote backup site is sometimes also called the secondary site the remote site must be kept synchronized with the primary site  as updates are performed at the primary.we achieve synchronization by sending all log records from primary site to the remote backup site the remote backup site must be physically separated fromthe primary ? for example,we can locate it in a different state ? so that a disaster at the primary does not damage thesumit67.blogspot.com 16.9 remote backup systems 757 log records primary network backup figure 16.10 architecture of remote backup system the remote backup site figure 16.10 shows the architecture of a remote backup system when the primary site fails  the remote backup site takes over processing first  however  it performs recovery  using its  perhaps outdated  copy of the data from the primary  and the log records received from the primary in effect  the remote backup site is performing recovery actions that would have been performed at the primary site when the latter recovered standard recovery algorithms  with minor modifications  can be used for recovery at the remote backup site once recovery has been performed  the remote backup site starts processing transactions availability is greatly increased over a single-site system  since the system can recover even if all data at the primary site are lost several issues must be addressed in designing a remote backup system  ? detection of failure it is important for the remote backup system to detect when the primary has failed failure of communication lines can fool the remote backup into believing that the primary has failed to avoid this problem  we maintain several communication links with independent modes of failure between the primary and the remote backup for example  several independent network connections  including perhaps a modem connection over a telephone line  may be used these connections may be backed up via manual intervention by operators  who can communicate over the telephone system ? transfer of control when the primary fails  the backup site takes over processing and becomes the new primary when the original primary site recovers  it can either play the role of remote backup  or take over the role of primary site again in either case  the old primary must receive a log of updates carried out by the backup site while the old primary was down the simplest way of transferring control is for the old primary to receive redo logs from the old backup site  and to catch up with the updates by applying them locally the old primary can then act as a remote backup site if control must be transferred back  the old backup site can pretend to have failed  resulting in the old primary taking over thesumit67.blogspot.com 758 chapter 16 recovery system ? time to recover if the log at the remote backup grows large  recovery will take a long time the remote backup site can periodically process the redo log records that it has received and can perform a checkpoint  so that earlier parts of the log can be deleted the delay before the remote backup takes over can be significantly reduced as a result a hot-spare configuration can make takeover by the backup site almost instantaneous in this configuration  the remote backup site continually processes redo log records as they arrive  applying the updates locally as soon as the failure of the primary is detected  the backup site completes recovery by rolling back incomplete transactions ; it is then ready to process new transactions ? time to commit to ensure that the updates of a committed transaction are durable  a transaction must not be declared committed until its log records have reached the backup site this delay can result in a longer wait to commit a transaction  and some systems therefore permit lower degrees of durability the degrees of durability can be classified as follows  ? one-safe a transaction commits as soon as its commit log record is written to stable storage at the primary site the problem with this scheme is that the updates of a committed transaction may not have made it to the backup site  when the backup site takes over processing thus  the updates may appear to be lost.when the primary site recovers  the lost updates can not be merged in directly  since the updates may conflictwith later updates performed at the backup site thus  human intervention may be required to bring the database to a consistent state ? two-very-safe a transaction commits as soon as its commit log record is written to stable storage at the primary and the backup site the problem with this scheme is that transaction processing can not proceed if either the primary or the backup site is down thus  availability is actually less than in the single-site case  although the probability of data loss is much less ? two-safe this scheme is the same as two-very-safe if both primary and backup sites are active if only the primary is active  the transaction is allowed to commit as soon as its commit log record is written to stable storage at the primary site this scheme provides better availability than does two-very-safe,while avoiding the problem of lost transactions faced by the one-safe scheme it results in a slower commit than the one-safe scheme  but the benefits generally outweigh the cost several commercial shared-disk systems provide a level of fault tolerance that is intermediate between centralized and remote backup systems in these commercial systems  the failure of a cpu does not result in system failure instead  other cpus take over  and they carry out recovery recovery actions include rollback thesumit67.blogspot.com 16.10 summary 759 of transactions running on the failed cpu  and recovery of locks held by those transactions since data are on a shared disk  there is no need for transfer of log records however  we should safeguard the data from disk failure by using  for example  a raid disk organization an alternative way of achieving high availability is to use a distributed database  with data replicated at more than one site transactions are then required to update all replicas of any data item that they update we study distributed databases  including replication  in chapter 19 16.10 summary ? a computer system  like any other mechanical or electrical device  is subject to failure there are a variety of causes of such failure  including disk crash  power failure  and software errors in each of these cases  information concerning the database system is lost ? in addition to system failures  transactions may also fail for various reasons  such as violation of integrity constraints or deadlocks ? an integral part of a database system is a recovery scheme that is responsible for the detection of failures and for the restoration of the database to a state that existed before the occurrence of the failure ? the various types of storage in a computer are volatile storage  nonvolatile storage  and stable storage data in volatile storage  such as in ram  are lost when the computer crashes data in nonvolatile storage  such as disk  are not lost when the computer crashes  but may occasionally be lost because of failures such as disk crashes data in stable storage are never lost ? stable storage that must be accessible online is approximated with mirrored disks  or other forms of raid,which provide redundant data storage offline  or archival  stable storage may consist of multiple tape copies of data stored in a physically secure location ? in case of failure  the state of the database system may no longer be consistent ; that is  it may not reflect a state of the world that the database is supposed to capture to preserve consistency  we require that each transaction be atomic it is the responsibility of the recovery scheme to ensure the atomicity and durability property ? in log-based schemes  all updates are recorded on a log  which must be kept in stable storage a transaction is considered to have committed when its last log record  which is the commit log record for the transaction  has been output to stable storage ? log records contain old values and new values for all updated data items the new values are used in case the updates need to be redone after a system crash the old values are used to roll back the updates of the transaction if thesumit67.blogspot.com 760 chapter 16 recovery system the transaction aborts during normal operation  as well as to roll back the updates of the transaction in case the system crashed before the transaction committed ? in the deferred-modifications scheme  during the execution of a transaction  all the write operations are deferred until the transaction has been committed  at which time the system uses the information on the log associated with the transaction in executing the deferred writes.with deferred modification  log records do not need to contain old values of updated data items ? to reduce the overhead of searching the log and redoing transactions  we can use checkpointing techniques ? modern recovery algorithms are based on the concept of repeating history  whereby all actions taken during normal operation  since the last completed checkpoint  are replayed during the redo pass of recovery repeating history restores the system state to what it was at the time the last log record was output to stable storage before the system crashed undo is then performed from this state  by executing an undo pass that processes log records of incomplete transactions in reverse order ? undo of an incomplete transaction writes out special redo-only log records  and an abort log record after that  the transaction can be considered to have completed  and it will not be undone again ? transaction processing is based on a storage model in which main memory holds a log buffer  a database buffer  and a system buffer the system buffer holds pages of system object code and local work areas of transactions ? efficient implementation of a recovery scheme requires that the number of writes to the database and to stable storage be minimized log records may be kept in volatile log buffer initially  but must be written to stable storage when one of the following conditions occurs  ? before the < ti commit > log record may be output to stable storage  all log records pertaining to transaction ti must have been output to stable storage ? before a block of data in main memory is output to the database  in nonvolatile storage   all log records pertaining to data in that block must have been output to stable storage ? modern recovery techniques support high-concurrency locking techniques  such as those used for b + -tree concurrency control these techniques allow early release of lower-level locks obtained by operations such as inserts or deletes  which allows other such operations to be performed by other transactions after lower-level locks are released  physical undo is not possible  and instead logical undo  such as a deletion to undo an insertion  is required transactions retain higher-level locks that ensure that concurrent transacthesumit67 blogspot.com review terms 761 tions can not perform actions that could make logical undo of an operation impossible ? to recover from failures that result in the loss of nonvolatile storage  we must dump the entire contents of the database onto stable storage periodically ? say  once per day if a failure occurs that results in the loss of physical database blocks  we use the most recent dump in restoring the database to a previous consistent state once this restoration has been accomplished  we use the log to bring the database system to the most recent consistent state ? the aries recovery scheme is a state-of-the-art scheme that supports a number of features to provide greater concurrency  reduce logging overheads  and minimize recovery time it is also based on repeating history  and allows logical undo operations the scheme flushes pages on a continuous basis and does not need to flush all pages at the time of a checkpoint it uses log sequence numbers  lsns  to implement a variety of optimizations that reduce the time taken for recovery ? remote backup systems provide a high degree of availability  allowing transaction processing to continue even if the primary site is destroyed by a fire  flood  or earthquake data and log records from a primary site are continually backed up to a remote backup site if the primary site fails  the remote backup site takes over transaction processing  after executing certain recovery actions review terms ? recovery scheme ? failure classification ? transaction failure ? logical error ? system error ? system crash ? data-transfer failure ? fail-stop assumption ? disk failure ? storage types ? volatile storage ? nonvolatile storage ? stable storage ? blocks ? physical blocks ? buffer blocks ? disk buffer ? force-output ? log-based recovery ? log ? log records ? update log record ? deferred modification ? immediate modification ? uncommitted modifications ? checkpoints ? recovery algorithm thesumit67.blogspot.com 762 chapter 16 recovery system ? restart recovery ? transaction rollback ? physical undo ? physical logging ? transaction rollback ? checkpoints ? restart recovery ? redo phase ? undo phase ? repeating history ? buffer management ? log-record buffering ? write-ahead logging  wal  ? log force ? database buffering ? latches ? operating system and buffer management ? fuzzy checkpointing ? early lock release ? logical operations ? logical logging ? logical undo ? loss of nonvolatile storage ? archival dump ? fuzzy dump ? aries ? log sequence number  lsn  ? pagelsn ? physiological redo ? compensation log record  clr  ? dirtypagetable ? checkpoint log record ? analysis phase ? redo phase ? undo phase ? high availability ? remote backup systems ? primary site ? remote backup site ? secondary site ? detection of failure ? transfer of control ? time to recover ? hot-spare configuration ? time to commit ? one-safe ? two-very-safe ? two-safe practice exercises 16.1 explain why log records for transactions on the undo-list must be processed in reverse order,whereas redo is performed in a forward direction 16.2 explain the purpose of the checkpoint mechanism how often should checkpoints be performed ? how does the frequency of checkpoints affect  ? system performance when no failure occurs ? ? the time it takes to recover from a system crash ? ? the time it takes to recover from a media  disk  failure ? thesumit67.blogspot.com practice exercises 763 16.3 some database systems allow the administrator to choose between two forms of logging  normal logging  used to recover from system crashes  and archival logging  used to recover from media  disk  failure.when can a log record be deleted  in each of these cases  using the recovery algorithm of section 16.4 ? 16.4 describe how to modify the recovery algorithm of section 16.4 to implement savepoints  and to perform rollback to a savepoint  savepoints are described in section 16.8.3  16.5 suppose the deferred modification technique is used in a database a is the old-value part of an update log record required any more ? why or why not ? b if old values are not stored in update log records  transaction undo is clearly not feasible how would the redo-phase of recovery have to be modified as a result ? c deferred modification can be implemented by keeping updated data items in local memory of transactions  and reading data items that have not been updated directly from the database buffer suggest how to efficiently implement a data itemread  ensuring that a transaction sees its own updates d what problem would arise with the above technique  if transactions perform a large number of updates ? 16.6 the shadow-paging scheme requires the page table to be copied suppose the page table is represented as a b + -tree a suggest how to share as many nodes as possible between the new copy and the shadow-copy of the b + -tree  assuming that updates are made only to leaf entries  with no insertions and deletions b even with the above optimization  logging is much cheaper than a shadow-copy scheme  for transactions that perform small updates explain why 16.7 suppose we  incorrectly  modify the recovery algorithm of section 16.4 to not log actions taken during transaction rollback when recovering from a system crash  transactions that were rolled back earlier would then be included in undo-list  and rolled back again give an example to show how actions taken during the undo phase of recovery could result in an incorrect database state  hint  consider a data item updated by an aborted transaction  and then updated by a transaction that commits  16.8 disk space allocated to a file as a result of a transaction should not be released even if the transaction is rolled back explain why  and explain how aries ensures that such actions are not rolled back thesumit67.blogspot.com 764 chapter 16 recovery system 16.9 suppose a transaction deletes a record  and the free space generated thus is allocated to a record inserted by another transaction  even before the first transaction commits a what problem can occur if the first transaction needs to be rolled back ? b would this problem be an issue if page-level locking is used instead of tuple-level locking ? c suggest how to solve this problem while supporting tuple-level locking  by logging post-commit actions in special log records  and executing them after commit make sure your scheme ensures that such actions are performed exactly once 16.10 explain the reasons why recovery of interactive transactions is more difficult to deal with than is recovery of batch transactions is there a simple way to deal with this difficulty ?  hint  consider an automatic teller machine transaction in which cash is withdrawn  16.11 sometimes a transaction has to be undone after it has committed because it was erroneously executed  for example because of erroneous input by a bank teller a give an example to show that using the normal transaction undo mechanism to undo such a transaction could lead to an inconsistent state b one way to handle this situation is to bring the whole database to a state prior to the commit of the erroneous transaction  called point-in-time recovery   transactions that committed later have their effects rolled back with this scheme suggest a modification to the recovery algorithm of section 16.4 to implement point-in-time recovery using database dumps c later nonerroneous transactions can be re-executed logically  if the updates are available in the form of sql but can not be re-executed using their log records.why ? exercises 16.12 explain the difference between the three storage types ? volatile  nonvolatile  and stable ? in terms of i/o cost 16.13 stable storage can not be implemented a explain why it can not be b explain how database systems deal with this problem thesumit67.blogspot.com exercises 765 16.14 explain how the database may become inconsistent if some log records pertaining to a block are not output to stable storage before the block is output to disk 16.15 outline the drawbacks of the no-steal and force buffer management policies 16.16 physiological redo logging can reduce logging overheads significantly  especially with a slotted page record organization explain why 16.17 explain why logical undo logging is used widely  whereas logical redo logging  other than physiological redo logging  is rarely used 16.18 consider the log in figure 16.5 suppose there is a crash just before the < t0 abort > log record iswritten out explain what would happen during recovery 16.19 suppose there is a transaction that has been running for a very long time  but has performed very few updates a what effect would the transaction have on recovery time with the recovery algorithm of section 16.4  and with the aries recovery algorithm b whateffect would the transaction have ondeletion of old log records ? 16.20 consider the log in figure 16.6 suppose there is a crash during recovery  just after before the operation abort log record is written for operation o1 explain what would happen when the system recovers again 16.21 compare log-based recovery with the shadow-copy scheme in terms of their overheads  for the case when data is being added to newly allocated disk pages  in other words  there is no old value to be restored in case the transaction aborts   16.22 in the aries recovery algorithm  a if at the beginning of the analysis pass  a page is not in the checkpoint dirty page table  will we need to apply any redo records to it ? why ? b what is reclsn  and how is it used to minimize unnecessary redos ? 16.23 explain the difference between a system crash and a ? disaster ? 16.24 for each of the following requirements  identify the best choice of degree of durability in a remote backup system  a data loss must be avoided but some loss of availability may be tolerated b transaction commit must be accomplished quickly  even at the cost of loss of some committed transactions in a disaster c a high degree of availability and durability is required  but a longer running time for the transaction commit protocol is acceptable thesumit67.blogspot.com 766 chapter 16 recovery system 16.25 the oracle database system uses undo log records to provide a snapshot view of the database  under snapshot-isolation the snapshot view seen by transaction ti reflects updates of all transactions that had committed when ti started  and the updates of ti ; updates of all other transactions are not visible to ti  describe a scheme for buffer handling whereby transactions are given a snapshot view of pages in the buffer include details of how to use the log to generate the snapshot view you can assume that operations as well as their undo actions affect only one page bibliographical notes gray and reuter  1993  is an excellent textbook source of information about recovery  including interesting implementation and historical details bernstein and goodman  1981  is an early textbook source of information on concurrency control and recovery an overview of the recovery scheme of system r is presented by gray et al  1981   tutorial and survey papers on various recovery techniques for database systems include gray  1978   lindsay et al  1980   and verhofstad  1978   the concepts of fuzzy checkpointing and fuzzy dumps are described in lindsay et al  1980   a comprehensive presentation of the principles of recovery is offered by haerder and reuter  1983   the state-of-the-art in recovery methods is best illustrated by the aries recovery method  described inmohan et al  1992  and mohan  1990b   mohan and levine  1992  presents aries im  an extension of aries to optimize b + -tree concurrency control and recovery using logical undo logging aries and its variants are used in several database products  including ibm db2 and microsoft sql server recovery in oracle is described in lahiri et al  2001   specialized recovery techniques for index structures are described in mohan and levine  1992  and mohan  1993  ; mohan and narang  1994  describes recovery techniques for client ? server architectures  while mohan and narang  1992  describes recovery techniques for parallel-database architectures a generalized version of the theory of serializability  with short duration lower-level locks during operations  combined with longer duration higher-level locks  is described by weikum  1991   in section 16.7.3  we saw the requirement that an operation should acquire all lower-level locks that may be needed for the logical undo of the operation this requirement can be relaxed by performing all physical undo operations first  before perfoming any logical undo operations a generalized version of this idea  calledmulti-level recovery  presented inweikum et al  1990   allows multiple levels of logical operations  with level-by-level undo passes during recovery remote backup algorithms for disaster recovery are presented in king et al  1991  and polyzois and garcia-molina  1994   thesumit67.blogspot.com part 5 system architecture the architecture of a database system is greatly influenced by the underlying computer system on which the database system runs database systems can be centralized  where one server machine executes operations on the database database systems can also be designed to exploit parallel computer architectures distributed databases span multiple geographically separated machines chapter 17 first outlines the architectures of database systems running on server systems  which are used in centralized and client ? server architectures the various processes that together implement the functionality of a database are outlined here the chapter then outlines parallel computer architectures  and parallel database architectures designed for different types of parallel computers finally  the chapter outlines architectural issues in building a distributed database system chapter 18 describes how various actions of a database  in particular query processing  can be implemented to exploit parallel processing chapter 19 presents a number of issues that arise in a distributed database  and describes how to deal with each issue the issues include how to store data  how to ensure atomicity of transactions that execute at multiple sites  how to perform concurrency control  and how to provide high availability in the presence of failures.a cloud-based data storage systems  distributed query processing and directory systems are also described in this chapter 767 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter17 database-system architectures the architecture of a database system is greatly influenced by the underlying computer system on which it runs  in particular by such aspects of computer architecture as networking  parallelism  and distribution  ? networking of computers allows some tasks to be executed on a server system and some tasks to be executed on client systems this division of work has led to client ? server database systems ? parallel processing within a computer system allows database-system activities to be speeded up  allowing faster response to transactions  aswell asmore transactions per second queries can be processed in a way that exploits the parallelism offered by the underlying computer system the need for parallel query processing has led to parallel database systems ? distributing data across sites in an organization allows those data to reside where they are generated ormost needed  but still to be accessible fromother sites and from other departments keeping multiple copies of the database across different sites also allows large organizations to continue their database operations even when one site is affected by a natural disaster  such as flood  fire  or earthquake distributed database systems handle geographically or administratively distributed data spread across multiple database systems we study the architecture of database systems in this chapter  starting with the traditional centralized systems  and covering client ? server  parallel  and distributed database systems 17.1 centralized and client ? server architectures centralized database systems are those that run on a single computer system and do not interact with other computer systems such database systems span a range from single-user database systems running on personal computers to high-performance database systems running on high-end server systems client 769 thesumit67.blogspot.com 770 chapter 17 database-system architectures ? server systems  on the other hand  have functionality split between a server system and multiple client systems 17.1.1 centralized systems a modern  general-purpose computer system consists of one to a few processors and a number of device controllers that are connected through a common bus that provides access to shared memory  figure 17.1   the processors have local cache memories that store local copies of parts of the memory  to speed up access to data each processor may have several independent cores  each of which can execute a separate instruction stream each device controller is in charge of a specific type of device  for example  a disk drive  an audio device  or a video display   the processors and the device controllers can execute concurrently  competing for memory access cache memory reduces the contention for memory access  since it reduces the number of times that the processor needs to access the shared memory we distinguish two ways in which computers are used  as single-user systems and as multiuser systems personal computers and workstations fall into the first category a typical single-user system is a desktop unit used by a single person  usually with only one processor and one or two hard disks  and usually only one person using the machine at a time a typical multiuser system  on the other hand  has more disks and more memory and may have multiple processors it serves a large number of users who are connected to the system remotely database systems designed for use by single users usually do not provide many of the facilities that a multiuser database provides in particular  they may not support concurrency control  which is not required when only a single user can generate updates provisions for crash recovery in such systems are either absent or primitive ? for example  they may consist of simply making a backup of the database before any update some such systems do not support sql  and they provide a simpler query language  such as a variant of qbe in contrast  usb controller mouse keyboard printer monitor disks graphics adapter disk controller memory cpu on-line figure 17.1 a centralized computer system thesumit67.blogspot.com 17.1 centralized and client ? server architectures 771 database systems designed for multiuser systems support the full transactional features that we have studied earlier although most general-purpose computer systems in use today have multiple processors  they have coarse-granularity parallelism  with only a few processors  about two to four  typically   all sharing the main memory databases running on such machines usually do not attempt to partition a single query among the processors ; instead  they run each query on a single processor  allowing multiple queries to run concurrently thus  such systems support a higher throughput ; that is  they allow a greater number of transactions to run per second  although individual transactions do not run any faster databases designed for single-processor machines already provide multitasking  allowing multiple processes to run on the same processor in a time-shared manner  giving a view to the user of multiple processes running in parallel thus  coarse-granularity parallel machines logically appear to be identical to singleprocessor machines  and database systems designed for time-shared machines can be easily adapted to run on them in contrast  machines with fine-granularity parallelism have a large number of processors  and database systems running on such machines attempt to parallelize single tasks  queries  for example  submitted by users we study the architecture of parallel database systems in section 17.3 parallelism is emerging as a critical issue in the future design of database systems whereas today those computer systems with multicore processors have only a few cores  future processors will have large numbers of cores.1 as a result  parallel database systems  which once were specialized systems running on specially designed hardware  will become the norm 17.1.2 client ? server systems as personal computers became faster  more powerful  and cheaper  there was a shift away from the centralized system architecture personal computers supplanted terminals connected to centralized systems correspondingly  personal computers assumed the user-interface functionality that used to be handled directly by the centralized systems as a result  centralized systems today act as server systems that satisfy requests generated by client systems figure 17.2 shows the general structure of a client ? server system functionality provided by database systems can be broadly divided into two parts ? the front end and the back end the back end manages access structures  query evaluation and optimization  concurrency control  and recovery the front end of a database system consists of tools such as the sql user interface  forms interfaces  report generation tools  and data mining and analysis tools  see figure 17.3   the interface between the front end and the back end is through sql  or through an application program 1the reasons for this pertain to issues in computer architecture related to heat generation and power consumption rather than make processors significantly faster  computer architects are using advances in chip design to put more cores on a single chip  a trend likely to continue for some time thesumit67.blogspot.com 772 chapter 17 database-system architectures client client client client server network figure 17.2 general structure of a client ? server system standards such as odbc and jdbc,whichwe saw inchapter 3,were developed to interface clients with servers any client that uses the odbc or jdbc interface can connect to any server that provides the interface certain application programs  such as spreadsheets and statistical-analysis packages  use the client ? server interface directly to access data from a back-end server in effect  they provide front ends specialized for particular tasks systems that deal with large numbers of users adopt a three-tier architecture  which we saw earlier in figure 1.6  chapter 1   where the front end is a web browser that talks to an application server the application server  in effect  acts as a client to the database server some transaction-processing systems provide a transactional remote procedure call interface to connect clients with a server these calls appear like ordinary procedure calls to the programmer  but all the remote procedure calls from a client are enclosed in a single transaction at the server end thus  if the transaction aborts  the server can undo the effects of the individual remote procedure calls 17.2 server system architectures server systems can be broadly categorized as transaction servers and data servers sql user interface forms interface report generation tools data mining and analysis tools sql engine front end interface  sql api  back end figure 17.3 front-end and back-end functionality thesumit67.blogspot.com 17.2 server system architectures 773 ? transaction-server systems  also called query-server systems  provide an interface to which clients can send requests to perform an action  in response to which they execute the action and send back results to the client usually  client machines ship transactions to the server systems  where those transactions are executed  and results are shipped back to clients that are in charge of displaying the data requests may be specified by using sql  or through a specialized application program interface ? data-server systems allow clients to interact with the servers by making requests to read or update data  in units such as files or pages for example  file servers provide a file-system interface where clients can create  update  read  and delete files data servers for database systems offer much more functionality ; they support units of data ? such as pages  tuples  or objects ? that are smaller than a file they provide indexing facilities for data  and provide transaction facilities so that the data are never left in an inconsistent state if a client machine or process fails of these  the transaction-server architecture is by far the more widely used architecture we shall elaborate on the transaction-server and data-server architectures in sections 17.2.1 and 17.2.2 17.2.1 transaction servers atypical transaction-server system today consists of multiple processes accessing data in shared memory  as in figure 17.4 the processes that form part of the database system include  ? server processes  these are processes that receive user queries  transactions   execute them  and send the results back the queries may be submitted to the server processes froma user interface  or from a user process running embedded sql  or via jdbc  odbc  or similar protocols some database systems use a separate process for each user session  and a few use a single database process for all user sessions  but with multiple threads so that multiple queries can execute concurrently  a thread is like a process  but multiple threads execute as part of the same process  and all threads within a process run in the same virtual-memory space multiple threads within a process can execute concurrently  many database systems use a hybrid architecture  with multiple processes  each one running multiple threads ? lock manager process  this process implements lock manager functionality  which includes lock grant  lock release  and deadlock detection ? database writer process  there are one or more processes that output modified buffer blocks back to disk on a continuous basis ? log writer process  this process outputs log records from the log record buffer to stable storage server processes simply add log records to the log thesumit67.blogspot.com 774 chapter 17 database-system architectures lock manager lock table process log buffer shared memory database writer process log writer process checkpoint process process monitor process server process server process user process user process server process user process odbc jdbc log disks data disks query plan cache buffer pool figure 17.4 shared memory and process structure record buffer in shared memory  and if a log force is required  they request the log writer process to output log records ? checkpoint process  this process performs periodic checkpoints ? process monitor process  this process monitors other processes  and if any of them fails  it takes recovery actions for the process  such as aborting any transaction being executed by the failed process  and then restarting the process the sharedmemory contains all shared data  such as  ? buffer pool ? lock table ? log buffer  containing log records waiting to be output to the log on stable storage thesumit67.blogspot.com 17.2 server system architectures 775 ? cached query plans  which can be reused if the same query is submitted again all database processes can access the data in shared memory since multiple processes may read or perform updates on data structures in shared memory  there must be a mechanism to ensure that a data structure is modified by at most one process at a time  and no process is reading a data structure while it is being written by others such mutual exclusion can be implemented by means of operating system functions called semaphores alternative implementations  with less overhead  use special atomic instructions supported by the computer hardware ; one type of atomic instruction tests a memory location and sets it to 1 atomically further implementation details of mutual exclusion can be found in any standard operating system textbook the mutual exclusion mechanisms are also used to implement latches to avoid the overhead of message passing  in many database systems  server processes implement locking by directly updating the lock table  which is in shared memory   instead of sending lock request messages to a lock manager process the lock request procedure executes the actions that the lock manager process would take on getting a lock request the actions on lock request and release are like those in section 15.1.4  but with two significant differences  ? since multiple server processes may access shared memory  mutual exclusion must be ensured on the lock table ? if a lock can not be obtained immediately because of a lock conflict  the lock request code may monitor the lock table to check when the lock has been granted the lock release code updates the lock table to note which process has been granted the lock to avoid repeated checks on the lock table  operating system semaphores can be used by the lock request code to wait for a lock grant notification the lock release code must then use the semaphore mechanism to notify waiting transactions that their locks have been granted even if the system handles lock requests through shared memory  it still uses the lock manager process for deadlock detection 17.2.2 data servers data-server systems are used in local-area networks  where there is a high-speed connection between the clients and the server  the client machines are comparable in processing power to the server machine  and the tasks to be executed are computation intensive in such an environment  it makes sense to ship data to client machines  to perform all processing at the client machine  which may take a while   and then to ship the data back to the server machine note that this architecture requires full back-end functionality at the clients data-server architectures have been particularly popular in object-oriented database systems  chapter 22   thesumit67.blogspot.com 776 chapter 17 database-system architectures interesting issues arise in such an architecture  since the time cost of communication between the client and the server is high compared to that of a local memory reference  milliseconds  versus less than 100 nanoseconds   ? page shipping versus item shipping the unit of communication for data can be of coarse granularity  such as a page  or fine granularity  such as a tuple  or an object  in the context of object-oriented database systems   we use the term item to refer to both tuples and objects if the unit of communication is a single item  the overhead of message passing is high compared to the amount of data transmitted instead  when an item is requested  it makes sense also to send back other items that are likely to be used in the near future fetching items even before they are requested is called prefetching page shipping can be considered a form of prefetching if multiple items reside on a page  since all the items in the page are shipped when a process desires to access a single item in the page ? adaptive lock granularity locks are usually granted by the server for the data items that it ships to the client machines a disadvantage of page shipping is that client machines may be granted locks of too coarse a granularity ? a lock on a page implicitly locks all items contained in the page even if the client is not accessing some items in the page  it has implicitly acquired locks on all prefetched items other client machines that require locks on those items may be blocked unnecessarily techniques for lock de-escalation have been proposed where the server can request its clients to transfer back locks on prefetched items if the client machine does not need a prefetched item  it can transfer locks on the item back to the server  and the locks can then be allocated to other clients ? data caching data that are shipped to a client on behalf of a transaction can be cached at the client  even after the transaction completes  if sufficient storage space is available successive transactions at the same client may be able to make use of the cached data however  cache coherency is an issue  even if a transaction finds cached data  it must make sure that those data are up to date  since they may have been updated by a different client after they were cached thus  a message must still be exchanged with the server to check validity of the data  and to acquire a lock on the data ? lock caching if the use of data is mostly partitioned among the clients  with clients rarely requesting data that are also requested by other clients  locks can also be cached at the client machine suppose that a client finds a data item in the cache  and that it also finds the lock required for an access to the data item in the cache then  the access can proceed without any communication with the server however  the server must keep track of cached locks ; if a client requests a lock from the server  the server must call back all conflicting locks on the data item from any other client machines that have cached the locks the task becomes more complicated when machine failures are taken into account this technique differs from lock de-escalation in that lock caching takes place across transactions ; otherwise  the two techniques are similar thesumit67.blogspot.com 17.3 parallel systems 777 the bibliographical references provide more information about client ? server database systems 17.2.3 cloud-based servers servers are usually owned by the enterprise providing the service  but there is an increasing trend for service providers to rely at least in part upon servers that are owned by a ? third party ? that is neither the client nor the service provider one model for using third-party servers is to outsource the entire service to another company that hosts the service on its own computers using its own software this allows the service provider to ignore most details of technology and focus on the marketing of the service another model for using third-party servers is cloud computing  in which the service provider runs its own software  but runs it on computers provided by another company under this model  the third party does not provide any of the application software ; it provides only a collection of machines these machines are not ? real ? machines  but rather simulated by software that allows a single real computer to simulate several independent computers such simulated machines are called virtual machines the service provider runs its software  possibly including a database system  on these virtual machines a major advantage of cloud computing is that the service provider can add machines as needed to meet demand and release them at times of light load this can prove to be highly cost-effective in terms of both money and energy athird model uses a cloud computing service as a data server ; such cloud-based data storage systems are covered in detail in section 19.9 database applications using cloud-based storage may run on the same cloud  that is  the same set of machines   or on another cloud the bibliographical references provide more information about cloud-computing systems 17.3 parallel systems parallel systems improve processing and i/o speeds by using multiple processors and disks in parallel parallel machines are becoming increasingly common  making the study of parallel database systems correspondingly more important the driving force behind parallel database systems is the demands of applications that have to query extremely large databases  of the order of terabytes ? that is  1012 bytes  or that have to process an extremely large number of transactions per second  of the order of thousands of transactions per second   centralized and client ? server database systems are not powerful enough to handle such applications in parallel processing  many operations are performed simultaneously  as opposed to serial processing  in which the computational steps are performed sequentially acoarse-grain parallelmachine consists of a small number of powerful processors ; a massively parallel or fine-grain parallel machine uses thousands of smaller processors virtually all high-end machines today offer some degree of coarse-grain parallelism  at least two or four processors massively parallel comthesumit67 blogspot.com 778 chapter 17 database-system architectures puters can be distinguished from the coarse-grain parallelmachines by the much larger degree of parallelism that they support parallel computers with hundreds of processors and disks are available commercially there are two main measures of performance of a database system   1  throughput  the number of tasks that can be completed in a given time interval  and  2  response time  the amount of time it takes to complete a single task from the time it is submitted a system that processes a large number of small transactions can improve throughput by processing many transactions in parallel a system that processes large transactions can improve response time as well as throughput by performing subtasks of each transaction in parallel 17.3.1 speedup and scaleup two important issues in studying parallelism are speedup and scaleup running a given task in less time by increasing the degree of parallelism is called speedup handling larger tasks by increasing the degree of parallelism is called scaleup consider a database application running on a parallel system with a certain number of processors and disks now suppose that we increase the size of the system by increasing the number of processors  disks  and other components of the system the goal is to process the task in time inversely proportional to the number of processors and disks allocated suppose that the execution time of a task on the larger machine is tl  and that the execution time of the same task on the smaller machine is ts the speedup due to parallelism is defined as ts/tl the parallel system is said to demonstrate linear speedup if the speedup is n when the larger system has n times the resources  processors  disk  and so on  of the smaller system if the speedup is less than n  the system is said to demonstrate sublinear speedup figure 17.5 illustrates linear and sublinear speedup scaleup relates to the ability to process larger tasks in the same amount of time by providing more resources let q be a task  and let qn be a task that is n times bigger than q suppose that the execution time of task q on a given machine linear speedup sublinear speedup resources speed figure 17.5 speedup with increasing resources thesumit67.blogspot.com 17.3 parallel systems 779 linear scaleup sublinear scaleup problem size ts tl figure 17.6 scaleup with increasing problem size and resources ms is ts  and the execution time of task qn on a parallel machine ml  which is n times larger than ms  is tl  the scaleup is then defined as ts/tl  the parallel system ml is said to demonstrate linear scaleup on task q if tl = ts if tl > ts  the system is said to demonstrate sublinear scaleup figure 17.6 illustrates linear and sublinear scaleups  where the resources increase in proportion to problem size   there are two kinds of scaleup that are relevant in parallel database systems  depending on how the size of the task is measured  ? in batch scaleup  the size of the database increases  and the tasks are large jobs whose runtime depends on the size of the database.anexample of such a task is a scan of a relation whose size is proportional to the size of the database thus  the size of the database is the measure of the size of the problem batch scaleup also applies in scientific applications  such as executing a query at an n-times finer resolution or performing an n-times longer simulation ? in transaction scaleup  the rate at which transactions are submitted to the database increases and the size of the database increases proportionally to the transaction rate this kind of scaleup is what is relevant in transactionprocessing systems where the transactions are small updates ? for example  a deposit or withdrawal from an account ? and transaction rates grow as more accounts are created such transaction processing is especially well adapted for parallel execution  since transactions can run concurrently and independently on separate processors  and each transaction takes roughly the same amount of time  even if the database grows scaleup is usually the more important metric for measuring efficiency of parallel database systems the goal of parallelism in database systems is usually to make sure that the database system can continue to perform at an acceptable speed  even as the size of the database and the number of transactions increases increasing the capacity of the system by increasing the parallelism provides a smoother path for growth for an enterprise than does replacing a centralized thesumit67.blogspot.com 780 chapter 17 database-system architectures system with a faster machine  even assuming that such a machine exists   however  we must also look at absolute performance numbers when using scaleup measures ; a machine that scales up linearly may perform worse than a machine that scales less than linearly  simply because the latter machine is much faster to start off with anumber of factors work against efficient parallel operation and can diminish both speedup and scaleup ? start-up costs there is a start-up cost associated with initiating a single process in a parallel operation consisting of thousands of processes  the start-up time may overshadow the actual processing time  affecting speedup adversely ? interference since processes executing in a parallel system often access shared resources  a slowdown may result from the interference of each new process as it competes with existing processes for commonly held resources  such as a system bus  or shared disks  or even locks both speedup and scaleup are affected by this phenomenon ? skew by breaking down a single task into a number of parallel steps  we reduce the size of the average step nonetheless  the service time for the single slowest step will determine the service time for the task as a whole it is often difficult to divide a task into exactly equal-sized parts  and the way that the sizes are distributed is therefore skewed for example  if a task of size 100 is divided into 10 parts  and the division is skewed  there may be some tasks of size less than 10 and some tasks of size more than 10 ; if even one task happens to be of size 20  the speedup obtained by running the tasks in parallel is only five  instead of ten as we would have hoped 17.3.2 interconnection networks parallel systems consist of a set of components  processors  memory  and disks  that can communicate with each other via an interconnection network figure 17.7 shows three commonly used types of interconnection networks  ? bus.all the system components can send data on and receive data froma single communication bus this type of interconnection is shown in figure 17.7a the bus could be an ethernet or a parallel interconnect bus architectures work well for small numbers of processors however  they do not scale well with increasing parallelism  since the bus can handle communication from only one component at a time ? mesh the components are nodes in a grid  and each component connects to all its adjacent components in the grid in a two-dimensional mesh each node connects to four adjacent nodes,while in a three-dimensionalmesh each node connects to six adjacent nodes figure 17.7b shows a two-dimensional mesh thesumit67.blogspot.com 17.3 parallel systems 781 110 011 111 101 000 100  a  bus  b  mesh  c  hypercube 001 010 figure 17.7 interconnection networks nodes that are not directly connected can communicate with one another by routing messages via a sequence of intermediate nodes that are directly connected to one another the number of communication links grows as the number of components grows  and the communication capacity of a mesh therefore scales better with increasing parallelism ? hypercube the components are numbered in binary  and a component is connected to another if the binary representations of their numbers differ in exactly one bit thus  each of the n components is connected to log  n  other components figure 17.7c shows a hypercube with eight nodes in a hypercube interconnection  a message from a component can reach any other component by going through at most log  n  links in contrast  in a mesh architecture a component may be 2  v n  1  links away from some of the other components  or v n links away  if the mesh interconnection wraps around at the edges of the grid   thus communication delays in a hypercube are significantly lower than in a mesh 17.3.3 parallel database architectures there are several architectural models for parallel machines among the most prominent ones are those in figure 17.8  in the figure  m denotes memory  p denotes a processor  and disks are shown as cylinders   ? shared memory all the processors share a common memory  figure 17.8a   ? shared disk all the processors share a common set of disks  figure 17.8b   shared-disk systems are sometimes called clusters ? shared nothing the processors share neither a common memory nor common disk  figure 17.8c   ? hierarchical this model is a hybrid of the preceding three architectures  figure 17.8d   in sections 17.3.3.1 through 17.3.3.4  we elaborate on each of these models thesumit67.blogspot.com 782 chapter 17 database-system architectures p p m p p p m m m p p p p p p p p p p p p p p p  a  shared memory p p p p  c  shared nothing  d  hierarchical m p p  b  shared disk m p m p m p m m m p m m m figure 17.8 parallel database architectures techniques used to speed up transaction processing on data-server systems  such as data and lock caching and lock de-escalation  outlined in section 17.2.2  can also be used in shared-disk parallel databases as well as in shared-nothing parallel databases in fact  they are very important for efficient transaction processing in such systems 17.3.3.1 sharedmemory in a shared-memory architecture  the processors and disks have access to a common memory  typically via a bus or through an interconnection network the benefit of shared memory is extremely efficient communication between processors ? data in shared memory can be accessed by any processor without being moved with software a processor can send messages to other processors much faster by using memory writes  which usually take less than a microsecond  than by sending a message through a communication mechanism the downside of shared-memory machines is that the architecture is not scalable beyond 32 or 64 processors because the bus or the interconnection network becomes a bottleneck  since it is shared by all processors   adding more processors does not help after a point  since the processors will spend most of their time waiting for their turn on the bus to access memory shared-memory architectures usually have large memory caches at each processor  so that referencing of the shared memory is avoided whenever possible thesumit67.blogspot.com 17.3 parallel systems 783 however  at least some of the data will not be in the cache  and accesses will have to go to the shared memory moreover  the caches need to be kept coherent ; that is  if a processor performs a write to a memory location  the data in that memory location should be either updated at or removed from any processor where the data are cached maintaining cache coherency becomes an increasing overhead with increasing numbers of processors consequently  shared-memory machines are not capable of scaling up beyond a point ; current shared-memory machines can not support more than 64 processors 17.3.3.2 shared disk in the shared-disk model  all processors can access all disks directly via an interconnection network  but the processors have private memories there are two advantages of this architecture over a shared-memory architecture first  since each processor has its own memory  the memory bus is not a bottleneck second  it offers a cheap way to provide a degree of fault tolerance  if a processor  or its memory  fails  the other processors can take over its tasks  since the database is resident on disks that are accessible from all processors we can make the disk subsystem itself fault tolerant by using a raid architecture  as described in chapter 10 the shared-disk architecture has found acceptance in many applications the main problem with a shared-disk system is again scalability although the memory bus is no longer a bottleneck  the interconnection to the disk subsystem is now a bottleneck ; it is particularly so in a situation where the database makes a large number of accesses to disks compared to shared-memory systems  shared-disk systems can scale to a somewhat larger number of processors  but communication across processors is slower  up to a few milliseconds in the absence of special-purpose hardware for communication   since it has to go through a communication network 17.3.3.3 shared nothing in a shared-nothing system  each node of the machine consists of a processor  memory  and one or more disks the processors at one node may communicate with another processor at another node by a high-speed interconnection network a node functions as the server for the data on the disk or disks that the node owns since local disk references are serviced by local disks at each processor  the shared-nothing model overcomes the disadvantage of requiring all i/o to go through a single interconnection network ; only queries  accesses to nonlocal disks  and result relations pass through the network moreover  the interconnection networks for shared-nothing systems are usually designed to be scalable  so that their transmission capacity increases as more nodes are added consequently  shared-nothing architectures are more scalable and can easily support a large number of processors the main drawbacks of shared-nothing systems are the costs of communication and of nonlocal disk access  which are higher than in a shared-memory or shared-disk architecture since sending data involves software interaction at both ends thesumit67.blogspot.com 784 chapter 17 database-system architectures 17.3.3.4 hierarchical the hierarchical architecture combines the characteristics of shared-memory  shared-disk  and shared-nothing architectures at the top level  the system consists of nodes that are connected by an interconnection network and do not share disks or memory with one another thus  the top level is a shared-nothing architecture each node of the system could actually be a shared-memory system with a few processors alternatively  each node could be a shared-disk system  and each of the systems sharing a set of disks could be a shared-memory system thus  a system could be built as a hierarchy  with shared-memory architecture with a few processors at the base  and a shared-nothing architecture at the top  with possibly a shared-disk architecture in the middle figure 17.8d illustrates a hierarchical architecture with shared-memory nodes connected together in a shared-nothing architecture commercial parallel database systems today run on several of these architectures attempts to reduce the complexity ofprogramming such systems have yielded distributed virtual-memory architectures  where logically there is a single shared memory  but physically there are multiple disjoint memory systems ; the virtualmemory mapping hardware  coupled with system software  allows each processor to view the disjoint memories as a single virtual memory since access speeds differ  depending on whether the page is available locally or not  such an architecture is also referred to as a nonuniform memory architecture  numa   17.4 distributed systems in a distributed database system  the database is stored on several computers the computers in a distributed system communicate with one another through various communication media  such as high-speed private networks or the internet they do not share main memory or disks the computers in a distributed systemmayvary in size and function  ranging fromworkstations up to mainframe systems the computers in a distributed system are referred to by a number of different names  such as sites or nodes  depending on the context in which they are mentioned we mainly use the term site  to emphasize the physical distribution of these systems the general structure of a distributed system appears in figure 17.9 the main differences between shared-nothing parallel databases and distributed databases are that distributed databases are typically geographically separated  are separately administered  and have a slower interconnection another major difference is that  in a distributed database system  we differentiate between local and global transactions a local transaction is one that accesses data only from sites where the transaction was initiated a global transaction  on the other hand  is one that either accesses data in a site different from the one at which the transaction was initiated  or accesses data in several different sites thesumit67.blogspot.com 17.4 distributed systems 785 site a site c site b communication via network network figure 17.9 a distributed system there are several reasons for buildingdistributeddatabase systems  including sharing of data  autonomy  and availability ? sharing data themajor advantage inbuilding a distributeddatabase system is the provision of an environment where users at one site may be able to access the data residing at other sites for instance  in a distributeduniversity system  where each campus stores data related to that campus  it is possible for a user in one campus to access data in another campus without this capability  the transfer of student records fromone campus toanother campus would have to resort to some external mechanism that would couple existing systems ? autonomy the primary advantage of sharing data by means of data distribution is that each site is able to retain a degree of control over data that are stored locally in a centralized system  the database administrator of the central site controls the database in a distributed system  there is a global database administrator responsible for the entire system a part of these responsibilities is delegated to the local database administrator for each site depending on the design of the distributed database system  each administratormay have a different degree of local autonomy the possibility of local autonomy is often a major advantage of distributed databases ? availability if one site fails in a distributed system  the remaining sites may be able to continue operating in particular  if data items are replicated in several sites  a transaction needing a particular data itemmay find that item in any of several sites thus  the failure of a site does not necessarily imply the shutdown of the system thesumit67.blogspot.com 786 chapter 17 database-system architectures the failure of one site must be detected by the system  and appropriate action may be needed to recover from the failure the system must no longer use the services of the failed site finally  when the failed site recovers or is repaired  mechanisms must be available to integrate it smoothly back into the system although recovery fromfailure is more complex in distributed systems than in centralized systems  the ability of most of the system to continue to operate despite the failure of one site results in increased availability availability is crucial for database systems used for real-time applications loss of access to data by  for example  an airline may result in the loss of potential ticket buyers to competitors 17.4.1 an example of a distributed database consider a banking system consisting of four branches in four different cities each branch has its own computer  with a database of all the accounts maintained at that branch each such installation is thus a site there also exists one single site that maintains information about all the branches of the bank to illustrate the difference between the two types of transactions ? local and global ? at the sites  consider a transaction to add $ 50 to account number a-177 located at the valleyview branch if the transaction was initiated at the valleyview branch  then it is considered local ; otherwise  it is considered global.atransaction to transfer $ 50 from account a-177 to account a-305  which is located at the hillside branch  is a global transaction  since accounts in two different sites are accessed as a result of its execution in an ideal distributed database system  the sites would share a common global schema  although some relations may be stored only at some sites   all sites would run the same distributed database-management software  and the sites would be aware of each other ? s existence if a distributed database is built from scratch  it would indeed be possible to achieve the above goals however  in reality a distributed database has to be constructed by linking together multiple already-existing database systems  each with its own schema and possibly running different database-management software such systems are sometimes called multidatabase systems or heterogeneous distributed database systems we discuss these systems in section 19.8  wherewe show how to achieve a degree of global control despite the heterogeneity of the component systems 17.4.2 implementation issues atomicity of transactions is an important issue in building a distributed database system if a transaction runs across two sites  unless the system designers are careful  it may commit at one site and abort at another  leading to an inconsistent state transaction commit protocols ensure such a situation can not arise the two-phase commit protocol  2pc  is the most widely used of these protocols thesumit67.blogspot.com 17.4 distributed systems 787 the basic idea behind 2pc is for each site to execute the transaction until it enters the partially committed state  and then leave the commit decision to a single coordinator site ; the transaction is said to be in the ready state at a site at this point the coordinator decides to commit the transaction only if the transaction reaches the ready state at every site where it executed ; otherwise  for example  if the transaction aborts at any site   the coordinator decides to abort the transaction every site where the transaction executed must follow the decision of the coordinator if a site fails when a transaction is in ready state  when the site recovers from failure it should be in a position to either commit or abort the transaction  depending on the decision of the coordinator the 2pc protocol is described in detail in section 19.4.1 concurrency control is another issue in a distributed database since a transaction may access data items at several sites  transaction managers at several sites may need to coordinate to implement concurrency control if locking is used  locking can be performed locally at the sites containing accessed data items  but there is also a possibility of deadlock involving transactions originating at multiple sites therefore deadlock detection needs to be carried out across multiple sites failures are more common in distributed systems since not only may computers fail  but communication links may also fail replication of data items  which is the key to the continued functioning of distributed databaseswhen failures occur  further complicates concurrency control section 19.5 provides detailed coverage of concurrency control in distributed databases the standard transaction models  based on multiple actions carried out by a single program unit  are often inappropriate for carrying out tasks that cross the boundaries of databases that can not or will not cooperate to implement protocols such as 2pc alternative approaches  based on persistent messaging for communication  are generally used for such tasks ; persistent messaging is discussed in section 19.4.3 when the tasks to be carried out are complex  involving multiple databases and/or multiple interactions with humans  coordination of the tasks and ensuring transaction properties for the tasks become more complicated workflow management systems are systems designed to help with carrying out such tasks  and are described in section 26.2 in case an organization has to choose between a distributed architecture and a centralized architecture for implementing an application  the system architect must balance the advantages against the disadvantages of distribution of data we have already seen the advantages of using distributed databases the primary disadvantage of distributed database systems is the added complexity required to ensure proper coordination among the sites this increased complexity takes various forms  ? software-development cost it is more difficult to implement a distributed database system ; thus  it is more costly ? greater potential for bugs since the sites that constitute the distributed system operate in parallel  it is harder to ensure the correctness of algorithms  thesumit67.blogspot.com 788 chapter 17 database-system architectures especially operation during failures of part of the system  and recovery from failures the potential exists for extremely subtle bugs ? increased processing overhead the exchange of messages and the additional computation required to achieve intersite coordination are a form of overhead that does not arise in centralized systems there are several approaches to distributed database design  ranging from fully distributed designs to ones that include a large degree of centralization.we study them in chapter 19 17.5 network types distributed databases and client ? server systems are built around communication networks there are basically two types of networks  local-area networks and wide-area networks the main difference between the two is the way in which they are distributed geographically in local-area networks  processors are distributed over small geographical areas  such as a single building or a number of adjacent buildings in wide-area networks  on the other hand  a number of autonomous processors are distributed over a large geographical area  such as the united states or the entire world   these differences imply major variations in the speed and reliability of the communication network  and are reflected in the distributed operating-system design printer laptop file server workstation workstation workstation gateway application server figure 17.10 local-area network thesumit67.blogspot.com 17.5 network types 789 17.5.1 local-area networks local-area networks  lans   figure 17.10  emerged in the early 1970s as away for computers to communicate and to share data with one another people recognized that  for many enterprises  numerous small computers  each with its own selfcontained applications  are more economical than a single large system because each small computer is likely to need access to a full complement of peripheral devices  such as disks and printers   and because some form of data sharing is likely to occur in a single enterprise  it was a natural step to connect these small systems into a network lans are generally used in an office environment.all the sites in such systems are close to one another  so the communication links tend to have a higher speed and lower error rate than do their counterparts in wide-area networks the most common links in a local-area network are twisted pair  coaxial cable  fiber optics  and wireless connections communication speeds range from tens of megabits per second  for wireless local-area networks   to 1 gigabit per second for gigabit ethernet the most recent ethernet standard is 10-gigabit ethernet a storage-area network  san  is a special type of high-speed local-area network designed to connect large banks of storage devices  disks  to computers that use the data  see figure 17.11   thus storage-area networks help build large-scale shared-disk systems the motivation for using storage-area networks to connect multiple computers to large banks of storage devices is essentially the same as that for shared-disk databases  namely  ? scalability by adding more computers ? high availability  since data are still accessible even if a computer fails lan/wan storage array storage array data-processing center web content provider server client client client server tape library san figure 17.11 storage-area network thesumit67.blogspot.com 790 chapter 17 database-system architectures raid organizations are used in the storage devices to ensure high availability of thedata  permitting processing to continue even if individual disks fail storagearea networks are usually built with redundancy  such as multiple paths between nodes  so if a component such as a link or a connection to the network fails  the network continues to function 17.5.2 wide-area networks wide-area networks  wans  emerged in the late 1960s  mainly as an academic research project to provide efficient communication among sites  allowing hardware and software to be shared conveniently and economically by a wide community of users systems that allowed remote terminals to be connected to a central computer via telephone lines were developed in the early 1960s  but they were not true wans the first wan to be designed and developedwas the arpanet.work on the arpanet began in 1968 the arpanet has grown from a four-site experimental network to a worldwide network of networks  the internet  comprising hundreds of millions of computer systems typical links on the internet are fiber-optic lines and  sometimes  satellite channels data rates for wide-area links typically range from a few megabits per second to hundreds of gigabits per second the last link  to end user sites  has traditionally been the slowest link  using such technologies as digital subscriber line  dsl  technology  supporting a few megabits per second  or dial-up modem connections over land-based telephone lines  supporting up to 56 kilobits per second   today  the last link is typically a cable modem or fiber optic connection  each supporting tens of megabits per second   or a wireless connection supporting several megabits per second in addition to limits on data rates  communication in a wan must also contend with significant latency  a message may take up to a few hundred milliseconds to be delivered across the world  both due to speed of light delays  and due to queuing delays at a number of routers in the path of the message applications whose data and computing resources are distributed geographically have to be carefully designed to ensure latency does not affect system performance excessively wans can be classified into two types  ? in discontinuous connection wans  such as those based on mobile wireless connections  hosts are connected to the network only part of the time ? in continuous connection wans  such as the wired internet  hosts are connected to the network at all times networks that are not continuously connected typically do not allow transactions across sites  but may keep local copies of remote data  and refresh the copies periodically  every night  for instance   for applications where consistency is not critical  such as sharing of documents  groupware systems such as lotus notes allow updates of remote data to be made locally  and the updates are then propagated back to the remote site periodically there is a potential for conflicting updates at different sites  conflicts that have to be detected and resolved.a mechthesumit67 blogspot.com 17.6 summary 791 anism for detecting conflicting updates is described later  in section 25.5.4 ; the resolution mechanism for conflicting updates is  however  application dependent 17.6 summary ? centralized database systems run entirely on a single computer with the growth of personal computers and local-area networking  the database frontend functionality has moved increasingly to clients  with server systems providing the back-end functionality client ? server interface protocols have helped the growth of client ? server database systems ? servers can be either transaction servers or data servers  although the use of transaction servers greatly exceeds the use of data servers for providing database services ? transaction servers have multiple processes  possibly running on multiple processors so that these processes have access to common data  such as the database buffer  systems store such data in shared memory in addition to processes that handle queries  there are system processes that carry out tasks such as lock and log management and checkpointing ? data-server systems supply raw data to clients such systems strive to minimize communication between clients and servers by caching data and locks at the clients parallel database systems use similar optimizations ? parallel database systems consist of multiple processors and multiple disks connected by a fast interconnection network speedup measures how much we can increase processing speed by increasing parallelism for a single transaction scaleup measures how well we can handle an increased number of transactions by increasing parallelism interference  skew  and start-up costs act as barriers to getting ideal speedup and scaleup ? paralleldatabase architectures include the shared-memory  shared-disk  shared nothing  and hierarchical architectures these architectures have different trade-offs of scalability versus communication speed ? adistributed database system is a collection ofpartially independentdatabase systems that  ideally  share a common schema  and coordinate processing of transactions that access nonlocal data the systems communicate with one another through a communication network ? local-area networks connect nodes that are distributed over small geographical areas  such as a single building or a few adjacent buildings wide-area networks connect nodes spread over a large geographical area the internet is the most extensively used wide-area network today ? storage-area networks are a special type of local-area network designed to provide fast interconnection between large banks of storage devices and multiple computers thesumit67.blogspot.com 792 chapter 17 database-system architectures review terms ? centralized systems ? server systems ? coarse-granularity parallelism ? fine-granularity parallelism ? database process structure ? mutual exclusion ? thread ? server processes ? lock manager process ? database writer process ? log writer process ? checkpoint process ? process monitor process ? client ? server systems ? transaction server ? query server ? data server ? prefetching ? de-escalation ? data caching ? cache coherency ? lock caching ? call back ? parallel systems ? throughput ? response time ? speedup ? linear speedup ? sublinear speedup ? scaleup ? linear scaleup ? sublinear scaleup ? batch scaleup ? transaction scaleup ? start-up costs ? interference ? skew ? interconnection networks ? bus ? mesh ? hypercube ? parallel database architectures ? shared memory ? shared disk  clusters  ? shared nothing ? hierarchical ? fault tolerance ? distributed virtual memory ? nonuniform memory architecture  numa  ? distributed systems ? distributed database ? sites  nodes  ? local transaction ? global transaction ? local autonomy ? multidatabase systems ? network types ? local-area networks  lan  ? wide-area networks  wan  ? storage-area network  san  thesumit67.blogspot.com exercises 793 practice exercises 17.1 instead of storing shared structures in shared memory  an alternative architecture would be to store them in the local memory of a special process  and access the shared data by interprocess communication with the process what would be the drawback of such an architecture ? 17.2 in typical client ? server systems the server machine is much more powerful than the clients ; that is  its processor is faster  it may have multiple processors  and it has more memory and disk capacity consider instead a scenario where client and server machines have exactly the same power would it make sense to build a client ? server system in such a scenario ? why ? which scenariowould be better suited to a data-server architecture ? 17.3 consider a database system based on a client ? server architecture  with the server acting as a data server a what is the effect of the speed of the interconnection between the client and the server on the choice between tuple and page shipping ? b if page shipping is used  the cache of data at the client can be organized either as a tuple cache or a page cache the page cache stores data in units of a page  while the tuple cache stores data in units of tuples assume tuples are smaller than pages describe one benefit of a tuple cache over a page cache 17.4 suppose a transaction is written in c with embedded sql  and about 80 percent of the time is spent in the sql code  with the remaining 20 percent spent in c code how much speedup can one hope to attain if parallelism is used only for the sql code ? explain 17.5 some database operations such as joins can see a significant difference in speed when data  for example  one of the relations involved in a join  fits in memory as compared to the situation where the data does not fit in memory show how this fact can explain the phenomenon of superlinear speedup  where an application sees a speedup greater than the amount of resources allocated to it 17.6 parallel systems often have a network structurewhere sets of n processors connect to a single ethernet switch  and the ethernet switches themselves connect to another ethernet switch does this architecture correspond to a bus  mesh or hypercube architecture ? if not  how would you describe this interconnection architecture ? exercises 17.7 whyis it relatively easy to port a database froma single processor machine to a multiprocessor machine if individual queries need not be parallelized ? thesumit67.blogspot.com 794 chapter 17 database-system architectures 17.8 transaction-server architectures are popular for client ? server relational databases  where transactions are short on the other hand  data-server architectures are popular for client ? server object-oriented database systems  where transactions are expected to be relatively long give two reasons why data servers may be popular for object-oriented databases but not for relational databases 17.9 what is lock de-escalation  and under what conditions is it required ? why is it not required if the unit of data shipping is an item ? 17.10 suppose you were in charge of the database operations of a company whose main job is to process transactions suppose the company is growing rapidly each year  and has outgrown its current computer system when you are choosing a new parallel computer  what measure is most relevant ? speedup  batch scaleup  or transaction scaleup ? why ? 17.11 database systems are typically implemented as a set of processes  or threads  sharing a shared memory area a how is access to the shared memory area controlled ? b is two-phase locking appropriate for serializing access to the data structures in shared memory ? explain your answer 17.12 is it wise to allow a user process to access the shared memory area of a database system ? explain your answer 17.13 what are the factors that can work against linear scaleup in a transaction processing system ? which of the factors are likely to be the most important in each of the following architectures  shared memory  shared disk  and shared nothing ? 17.14 memory systems can be divided into multiple modules  each of which can be serving a separate request at a given time what impact would such amemory architecture have on the number of processors that can be supported in a shared-memory system ? 17.15 consider a bank that has a collection of sites  each running a database system suppose the only way the databases interact is by electronic transfer of money between themselves  using persistent messaging.would such a system qualify as a distributed database ? why ? bibliographical notes hennessy et al  2006  provides an excellent introduction to the area of computer architecture abadi  2009  provides an excellent introduction to cloud computing and the challenges of running database transactions in such an environment gray and reuter  1993  provides a textbook description of transaction processing  including the architecture of client ? server and distributed systems the thesumit67.blogspot.com bibliographical notes 795 bibliographical notes of chapter 5 provide references to more information on odbc  jdbc  and other database access apis dewitt and gray  1992  surveys parallel database systems  including their architecture and performance measures a survey of parallel computer architectures is presented by duncan  1990   dubois and thakkar  1992  is a collection of papers on scalable shared-memory architectures dec clusters running rdb were among the early commercial users of the shared-disk database architecture rdb is now owned by oracle  and is called oracle rdb the teradata database machine was among the earliest commercial systems to use the shared-nothing database architecture the grace and the gamma research prototypes also used shared-nothing architectures ozsu andvalduriez  1999  provides textbook coverage ofdistributed database systems further references pertaining to parallel and distributed database systems appear in the bibliographical notes of chapters 18 and 19  respectively comer  2009   halsall  2006   and thomas  1996  describe computer networking and the internet tanenbaum  2002   kurose and ross  2005   and peterson and davie  2007  provide general overviews of computer networks thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter18 parallel databases in this chapter  we discuss fundamental algorithms for parallel database systems that are based on the relational data model in particular  we focus on the placement of data on multiple disks and the parallel evaluation of relational operations  both of which have been instrumental in the success of parallel databases 18.1 introduction at one point over two decades ago  parallel database systems had been nearly written off  even by some of their staunchest advocates today  they are successfully marketed by practically every database-system vendor several trends fueled this transition  ? the transaction requirements of organizations have grown with increasing use of computers moreover  the growth of the world wide web has created many sites with millions of viewers  and the increasing amounts of data collected fromthese viewers has produced extremely large databases at many companies ? organizations are using these increasingly large volumes of data ? such as data about what items people buy  what web links users click on  and when people make telephone calls ? to plan their activities and pricing queries used for such purposes are called decision-support queries  and the data requirements for such queries may run into terabytes single-processor systems are not capable of handling such large volumes of data at the required rates ? the set-oriented nature of database queries naturally lends itself to parallelization a number of commercial and research systems have demonstrated the power and scalability of parallel query processing ? as microprocessors have become cheap  parallelmachines have become common and relatively inexpensive ? individual processors have themselves become parallelmachines using multicore architectures 797 thesumit67.blogspot.com 798 chapter 18 parallel databases as we discussed in chapter 17  parallelism is used to provide speedup,where queries are executed faster because more resources  such as processors and disks  are provided parallelism is also used to provide scaleup  where increasing workloads are handled without increased response time  via an increase in the degree of parallelism we outlined in chapter 17 the different architectures for parallel database systems  shared-memory  shared-disk  shared-nothing  and hierarchical architectures briefly  in shared-memory architectures  all processors share a common memory and disks ; in shared-disk architectures  processors have independent memories  but share disks ; in shared-nothing architectures  processors share neither memory nor disks ; and hierarchical architectures have nodes that share neithermemory nor disks with each other  but internally each node has a sharedmemory or a shared-disk architecture 18.2 i/o parallelism in its simplest form  i/o parallelism refers to reducing the time required to retrieve relations from disk by partitioning the relations over multiple disks the most common form of data partitioning in a parallel database environment is horizontal partitioning in horizontal partitioning  the tuples of a relation are divided  or declustered  among many disks  so that each tuple resides on one disk several partitioning strategies have been proposed 18.2.1 partitioning techniques we present three basic data-partitioning strategies assume that there are n disks  d0  d1      dn-1  across which the data are to be partitioned ? round-robin this strategy scans the relation in any order and sends the ith tuple to disk number di mod n  the round-robin scheme ensures an even distribution of tuples across disks ; that is  each disk has approximately the same number of tuples as the others ? hash partitioning this declustering strategy designates one or more attributes from the given relation ? s schema as the partitioning attributes a hash function is chosen whose range is  0  1      n  1   each tuple of the original relation is hashed on the partitioning attributes if the hash function returns i  then the tuple is placed on disk di .1 ? range partitioning this strategy distributes tuples by assigning contiguous attribute-value ranges to each disk it chooses a partitioning attribute  a  and a partitioning vector  v0  v1      vn-2   such that  if i < j  then vi < vj the relation is partitioned as follows  consider a tuple t such that t  a  = x if 1hash-function design is discussed in section 11.6.1 thesumit67.blogspot.com 18.2 i/o parallelism 799 x < v0  then t goes on disk d0 if x = vn-2  then t goes on disk dn-1 if vi = x < vi + 1  then t goes on disk di + 1 for example  range partitioning with three disks numbered 0  1  and 2 may assign tuples with values less than 5 to disk 0  values between 5 and 40 to disk 1  and values greater than 40 to disk 2 18.2.2 comparison of partitioning techniques once a relation has been partitioned among several disks  we can retrieve it in parallel  using all the disks similarly  when a relation is being partitioned  it can be written to multiple disks in parallel thus  the transfer rates for reading or writing an entire relation are much faster with i/o parallelism than without it however  reading an entire relation  or scanning a relation  is only one kind of access to data access to data can be classified as follows  1 scanning the entire relation 2 locating a tuple associatively  for example  employee name = ? campbell ?  ; these queries  called point queries  seek tuples that have a specified value for a specific attribute 3 locating all tuples for which the value of a given attribute lies within a specified range  for example  10000 < salary < 20000  ; these queries are called range queries the different partitioning techniques support these types of access at different levels of efficiency  ? round-robin the scheme is ideally suited for applications that wish to read the entire relation sequentially for each query with this scheme  both point queries and range queries are complicated to process  since each of the n disks must be used for the search ? hash partitioning this scheme is best suited for point queries based on the partitioning attribute for example  if a relation is partitioned on the telephone number attribute  then we can answer the query ? find the record of the employee with telephone number = 555-3333 ? by applying the partitioning hash function to 555-3333 and then searching that disk directing a query to a single disk saves the start-up cost of initiating a query on multiple disks  and leaves the other disks free to process other queries hash partitioning is also useful for sequential scans of the entire relation if the hash function is a good randomizing function  and the partitioning attributes form a key of the relation  then the number of tuples in each of the disks is approximately the same  without much variance hence  the time taken to scan the relation is approximately 1/n of the time required to scan the relation in a single disk system the scheme  however  is notwell suited for point queries on nonpartitioning attributes hash-based partitioning is also not well suited for answering range thesumit67.blogspot.com 800 chapter 18 parallel databases queries  since  typically  hash functions do not preserve proximity within a range therefore  all the disks need to be scanned for range queries to be answered ? range partitioning this scheme is well suited for point and range queries on the partitioning attribute for point queries  we can consult the partitioning vector to locate the diskwhere the tuple resides for range queries,we consult the partitioning vector to find the range of disks on which the tuples may reside in both cases  the search narrows to exactly those disks that might have any tuples of interest an advantage of this feature is that  if there are only a few tuples in the queried range  then the query is typically sent to one disk  as opposed to all the disks since other disks can be used to answer other queries  range partitioning results in higher throughput while maintaining good response time on the other hand  if there are many tuples in the queried range  as there are when the queried range is a larger fraction of the domain of the relation   many tuples have to be retrieved from a few disks  resulting in an i/o bottleneck  hot spot  at those disks in this example of execution skew  all processing occurs in one ? or only a few ? partitions in contrast  hash partitioning and round-robin partitioning would engage all the disks for such queries  giving a faster response time for approximately the same throughput the type of partitioning also affects other relational operations  such as joins  as we shall see in section 18.5 thus  the choice of partitioning technique also depends on the operations that need to be executed in general  hash partitioning or range partitioning are preferred to round-robin partitioning in a system with many disks  the number of disks across which to partition a relation can be chosen in this way  if a relation contains only a few tuples that will fit into a single disk block  then it is better to assign the relation to a single disk large relations are preferably partitioned across all the available disks if a relation consists of m disk blocks and there are n disks available in the system  then the relation should be allocated min  m  n  disks 18.2.3 handling of skew when a relation is partitioned  by a technique other than round-robin   theremay be a skew in the distribution of tuples  with a high percentage of tuples placed in some partitions and fewer tuples in other partitions the ways that skew may appear are classified as  ? attribute-value skew ? partition skew attribute-value skew refers to the fact that some values appear in the partitioning attributes of many tuples all the tuples with the same value for the thesumit67.blogspot.com 18.2 i/o parallelism 801 partitioning attribute end up in the same partition  resulting in skew partition skew refers to the fact that there may be load imbalance in the partitioning  even when there is no attribute skew attribute-value skew can result in skewed partitioning regardless of whether range partitioning or hash partitioning is used if the partition vector is not chosen carefully  range partitioning may result in partition skew partition skew is less likely with hash partitioning  if a good hash function is chosen as section 17.3.1 noted  even a small skew can result in a significant decrease in performance skew becomes an increasing problem with a higher degree of parallelism for example  if a relation of 1000 tuples is divided into 10 parts  and the division is skewed  then there may be some partitions of size less than 100 and some partitions of size more than 100 ; if even one partition happens to be of size 200  the speedup that we would obtain by accessing the partitions in parallel is only 5  instead of the 10 for which we would have hoped if the same relation has to be partitioned into 100 parts  a partition will have 10 tuples on an average if even one partition has 40 tuples  which is possible  given the large number of partitions  the speedup that we would obtain by accessing them in parallelwould be 25  rather than 100 thus,we see that the loss of speedup due to skew increases with parallelism a balanced range-partitioning vector can be constructed by sorting  the relation is first sorted on the partitioning attributes the relation is then scanned in sorted order after every 1/n of the relation has been read  the value of the partitioning attribute of the next tuple is added to the partition vector here  n denotes the number of partitions to be constructed in case there are many tuples with the same value for the partitioning attribute  the technique can still result in some skew the main disadvantage of this method is the extra i/o overhead incurred in doing the initial sort the i/o overhead for constructing balanced range-partition vectors can be reduced by constructing and storing a frequency table  or histogram  of the attribute values for each attribute of each relation figure 18.1 shows an example of a histogram for an integer-valued attribute that takes values in the range 1 to 25.a histogram takes up only a little space  so histograms on several different attributes can be stored in the system catalog it is straightforward to construct a balanced range-partitioning function given a histogram on the partitioning attributes if the histogram is not stored  it can be computed approximately by sampling the relation  using only tuples from a randomly chosen subset of the disk blocks of the relation another approach to minimizing the effect of skew  particularly with range partitioning  is to use virtual processors in the virtual processor approach  we pretend there are several times as many virtual processors as the number of real processors any of the partitioning techniques and query-evaluation techniques that we study later in this chapter can be used  but they map tuples and work to virtual processors instead of to real processors virtual processors  in turn  are mapped to real processors  usually by round-robin partitioning the idea is that even if one range had many more tuples than the others because of skew  these tuples would get split across multiple virtual processor thesumit67.blogspot.com 802 chapter 18 parallel databases value frequency 50 40 30 20 10 1 ? 5 6 ? 10 11 ? 15 16 ? 20 21 ? 25 figure 18.1 example of histogram ranges round-robin allocation of virtual processors to real processors would distribute the extra work among multiple real processors  so that one processor does not have to bear all the burden 18.3 interquery parallelism in interquery parallelism  different queries or transactions execute in parallel with one another transaction throughput can be increased by this form of parallelism however  the response times of individual transactions are no faster than they would be if the transactions were run in isolation thus  the primary use of interquery parallelism is to scale up a transaction-processing system to support a larger number of transactions per second interquery parallelism is the easiest form of parallelism to support in a database system ? particularly in a shared-memory parallel system database systems designed for single-processor systems can be used with few or no changes on a shared-memory parallel architecture  since even sequential database systems support concurrent processing transactions that would have operated in a timeshared concurrent manner on a sequential machine operate in parallel in the shared-memory parallel architecture supporting interquery parallelism is more complicated in a shared-disk or shared-nothing architecture processors have to perform some tasks  such as locking and logging  in a coordinated fashion  and that requires that they pass messages to each other a parallel database system must also ensure that two processors do not update the same data independently at the same time further  when a processor accesses or updates data  the database system must ensure that the processor has the latest version of the data in its buffer pool the problem of ensuring that the version is the latest is known as the cache-coherency problem thesumit67.blogspot.com 18.4 intraquery parallelism 803 various protocols are available to guarantee cache coherency ; often  cachecoherency protocols are integrated with concurrency-control protocols so that their overhead is reduced one such protocol for a shared-disk system is this  1 before any read or write access to a page  a transaction locks the page in shared or exclusivemode  as appropriate immediately after the transaction obtains either a shared or exclusive lock on a page  it also reads the most recent copy of the page from the shared disk 2 before a transaction releases an exclusive lock on a page  it flushes the page to the shared disk ; then  it releases the lock this protocol ensures that  when a transaction sets a shared or exclusive lock on a page  it gets the correct copy of the page more complex protocols avoid the repeated reading and writing to disk required by the preceding protocol such protocols do notwrite pages to diskwhen exclusive locks are released when a shared or exclusive lock is obtained  if the most recent version of a page is in the buffer pool of some processor  the page is obtained from there the protocols have to be designed to handle concurrent requests the shared-disk protocols can be extended to shared-nothing architectures by this scheme  each page has a home processor pi  and is stored on disk di  when other processors want to read or write the page  they send requests to the home processor pi of the page  since they can not directly communicate with the disk the other actions are the same as in the shared-disk protocols the oracle and oracle rdb systems are examples of shared-disk parallel database systems that support interquery parallelism 18.4 intraquery parallelism intraquery parallelism refers to the execution of a single query in parallel on multiple processors and disks using intraquery parallelism is important for speeding up long-running queries interquery parallelism does not help in this task  since each query is run sequentially to illustrate the parallel evaluation of a query  consider a query that requires a relation to be sorted suppose that the relation has been partitioned across multiple disks by range partitioning on some attribute  and the sort is requested on the partitioning attribute the sort operation can be implemented by sorting each partition in parallel  then concatenating the sorted partitions to get the final sorted relation thus,we can parallelize a query by parallelizing individual operations there is another source of parallelism in evaluating a query  the operator tree for a query can contain multiple operations.we can parallelize the evaluation of the operator tree by evaluating in parallel some of the operations that do not depend on one another further  as chapter 12 mentions  we may be able to pipeline the output of one operation to another operation the two operations can be executed in thesumit67.blogspot.com 804 chapter 18 parallel databases parallel on separate processors  one generating output that is consumed by the other  even as it is generated in summary  the execution of a single query can be parallelized in two different ways  ? intraoperation parallelism we can speed up processing of a query by parallelizing the execution of each individual operation  such as sort  select  project  and join we consider intraoperation parallelism in section 18.5 ? interoperation parallelism we can speed up processing of a query by executing in parallel the different operations in a query expression.we consider this form of parallelism in section 18.6 the two forms of parallelism are complementary  and can be used simultaneously on a query since the number of operations in a typical query is small  compared to the number of tuples processed by each operation  the first form of parallelism can scale better with increasing parallelism however  with the relatively small number of processors in typical parallel systems today  both forms of parallelism are important in the following discussion of parallelization of queries  we assume that the queries are read only the choice of algorithms for parallelizing query evaluation depends on the machine architecture rather than present algorithms for each architecture separately  we use a shared-nothing architecture model in our description thus  we explicitly describe when data have to be transferred from one processor to another we can simulate this model easily by using the other architectures  since transfer of data can be done via shared memory in a sharedmemory architecture  and via shared disks in a shared-disk architecture hence  algorithms for shared-nothing architectures can be used on the other architectures  too.we mention occasionally how the algorithms can be further optimized for shared-memory or shared-disk systems to simplify the presentation of the algorithms  assume that there are n processors  p0  p1      pn-1  and n disks d0  d1      dn-1  where disk di is associated with processor pi  a real system may have multiple disks per processor it is not hard to extend the algorithms to allow multiple disks per processor  we simply allow di to be a set of disks however  for simplicity  we assume here that di is a single disk 18.5 intraoperation parallelism since relational operations work on relations containing large sets of tuples  we can parallelize the operations by executing them in parallel on different subsets of the relations since the number of tuples in a relation can be large  the degree of parallelism is potentially enormous thus  intraoperation parallelism is natural in a database system.we shall study parallel versions of some common relational operations in sections 18.5.1 through 18.5.3 thesumit67.blogspot.com 18.5 intraoperation parallelism 805 18.5.1 parallel sort suppose that we wish to sort a relation that resides on n disks d0  d1      dn-1 if the relation has been range-partitioned on the attributes on which it is to be sorted  then  as noted in section 18.2.2  we can sort each partition separately  and can concatenate the results to get the full sorted relation since the tuples are partitioned on n disks  the time required for reading the entire relation is reduced by the parallel access if the relation has been partitioned in any other way  we can sort it in one of two ways  1 we can range-partition it on the sort attributes  and then sort each partition separately 2 we can use a parallel version of the external sort ? merge algorithm 18.5.1.1 range-partitioning sort range-partitioning sort works in two steps  first range partitioning the relation  then sorting each partition separately when we sort by range partitioning the relation  it is not necessary to range-partition the relation on the same set of processors or disks as those on which that relation is stored suppose that we choose processors p0  p1      pm  where m < n  to sort the relation there are two steps involved in this operation  1 redistribute the tuples in the relation  using a range-partition strategy  so that all tuples that lie within the ith range are sent to processor pi  which stores the relation temporarily on disk di  to implement range partitioning  in parallel every processor reads the tuples from its disk and sends the tuples to their destination processors each processor p0  p1      pm also receives tuples belonging to its partition  and stores them locally this step requires disk i/o and communication overhead 2 each of the processors sorts its partition of the relation locally  without interaction with the other processors each processor executes the same operation ? namely  sorting ? on a different data set  execution of the same operation in parallel on different sets of data is called data parallelism  the final merge operation is trivial  because the range partitioning in the first phase ensures that  for 1 = i < j = m  the key values in processor pi are all less than the key values in pj  we must do range partitioningwith a good range-partition vector  so that each partition will have approximately the same number of tuples virtual processor partitioning can also be used to reduce skew thesumit67.blogspot.com 806 chapter 18 parallel databases 18.5.1.2 parallel external sort ? merge parallel external sort ? merge is an alternative to range partitioning suppose that a relation has already been partitioned among disks d0  d1      dn-1  it does not matter how the relation has been partitioned   parallel external sort ? merge then works this way  1 each processor pi locally sorts the data on disk di  2 the system then merges the sorted runs on each processor to get the final sorted output the merging of the sorted runs in step 2 can be parallelized by this sequence of actions  1 the system range-partitions the sorted partitions at each processor pi  all by the same partition vector  across the processors p0  p1      pm-1 it sends the tuples in sorted order  so that each processor receives the tuples in sorted streams 2 each processor pi performs a merge on the streams as they are received  to get a single sorted run 3 the system concatenates the sorted runs on processors p0  p1      pm-1 to get the final result as described  this sequence of actions results in an interesting form of execution skew  since at first every processor sends all blocks of partition 0 to p0  then every processor sends all blocks of partition 1 to p1  and so on thus  while sending happens in parallel  receiving tuples becomes sequential  first only p0 receives tuples  then only p1 receives tuples  and so on to avoid this problem  each processor repeatedly sends a block of data to each partition in other words  each processor sends the first block of every partition  then sends the second block of every partition  and so on as a result  all processors receive data in parallel some machines  such as the teradata purpose-built platform family machines  use specialized hardware to performmerging the bynet interconnection network in the teradata machines can merge output from multiple processors to give a single sorted output 18.5.2 parallel join the join operation requires that the system test pairs of tuples to see whether they satisfy the join condition ; if they do  the system adds the pair to the join output parallel join algorithms attempt to split the pairs to be tested over several processors each processor then computes part of the join locally then  the system collects the results from each processor to produce the final result thesumit67.blogspot.com 18.5 intraoperation parallelism 807 18.5.2.1 partitioned join for certain kinds of joins  such as equi-joins and natural joins  it is possible to partition the two input relations across the processors and to compute the join locally at each processor suppose that we are using n processors and that the relations to be joined are r and s partitioned join then works thisway  the system partitions the relations r and s each into n partitions  denoted r0  r1      rn-1 and s0  s1      sn-1 the system sends partitions ri and si to processor pi  where their join is computed locally the partitioned join technique works correctly only if the join is an equi-join  for example  r  r.a = s.b s  and if we partition r and s by the same partitioning function on their join attributes the idea of partitioning is exactly the same as that behind the partitioning step of hash join in a partitioned join  however  there are two different ways of partitioning r and s  ? range partitioning on the join attributes ? hash partitioning on the join attributes in either case  the same partitioning function must be used for both relations for range partitioning  the same partition vector must be used for both relations for hash partitioning  the same hash function must be used on both relations figure 18.2 depicts the partitioning in a partitioned parallel join once the relations are partitioned  we can use any join technique locally at each processor pi to compute the join of ri and si  for example  hash join  merge join  or nested-loop join could be used thus,we can use partitioning to parallelize any join technique r0 p0 r1 p1 s r r2 p2 r3 p3 s0 s1 s2 s3      figure 18.2 partitioned parallel join thesumit67.blogspot.com 808 chapter 18 parallel databases if one or both of the relations r and s are already partitioned on the join attributes  by either hash partitioning or range partitioning   the work needed for partitioning is reduced greatly if the relations are not partitioned  or are partitioned on attributes other than the join attributes  then the tuples need to be repartitioned each processor pi reads in the tuples on disk di  computes for each tuple t the partition j to which t belongs  and sends tuple t to processor pj  processor pj stores the tuples on disk dj  we can optimize the join algorithm used locally at each processor to reduce i/o by buffering some of the tuples to memory  instead of writing them to disk we describe such optimizations in section 18.5.2.3 skew presents a special problem when range partitioning is used  since a partition vector that splits one relation of the join into equal-sized partitions may split the other relations into partitions of widely varying size the partition vector should be such that | ri | + | si |  that is  the sum of the sizes of ri and si  is roughly equal over all the i = 0  1      n  1 with a good hash function  hash partitioning is likely to have a smaller skew  except when there are many tuples with the same values for the join attributes 18.5.2.2 fragment-and-replicate join partitioning is not applicable to all types of joins for instance  if the join condition is an inequality  such as r  r.a < s.b s  it is possible that all tuples in r join with some tuple in s  and vice versa   thus  there may be no easy way of partitioning r and s so that tuples in partition ri join with only tuples in partition si  we can parallelize such joins by using a technique called fragment and replicate we first consider a special case of fragment and replicate ? asymmetric fragmentand replicate join ? which works as follows  1 the system partitions one of the relations ? say  r  any partitioning technique can be used on r  including round-robin partitioning 2 the system replicates the other relation  s  across all the processors 3 processor pi then locally computes the join of ri with all of s  using any join technique the asymmetric fragment-and-replicate scheme appears in figure 18.3a if r is already stored by partitioning  there is no need to partition it further in step 1 all that is required is to replicate s across all processors the general case of fragment-and-replicate join appears in figure 18.3b ; it works this way  the system partitions relation r into n partitions  r0  r1      rn-1  and partitions s into m partitions  s0  s1      sm-1 as before  any partitioning technique may be used on r and on s the values of m and n do not need to be equal  but they must be chosen so that there are at least m * n processors asymmetric fragment and replicate is simply a special case of general fragment and replicate  where m = 1 fragment and replicate reduces the sizes of the relations at each processor  compared to asymmetric fragment and replicate thesumit67.blogspot.com 18.5 intraoperation parallelism 809 r0 p0,0 s0 s1 s2 s s3 sm ? 1 r1 r r2 r3 rn ? 1 pn ? 1,m ? 1  r0 p0 r1 p1 r s r2 p2 r3 p3   p1,0 p2,0 p0,1 p1,1 p2,1 p0,2 p1,2 p0,3               a  asymmetric fragment and replicate  b  fragment and replicate figure 18.3 fragment-and-replicate schemes let the processors be p0,0  p0,1      p0,m-1  p1,0      pn-1,m-1 processor pi  j computes the join of ri with s j  each processor must get those tuples in the partitions on which it works to accomplish this  the system replicates ri to processors pi,0  pi,1      pi,m-1  which form a row in figure 18.3b   and replicates si to processors p0,i  p1,i      pn-1,i  which form a column in figure 18.3b   any join technique can be used at each processor pi  j  fragment and replicate works with any join condition  since every tuple in r can be tested with every tuple in s thus  it can be used where partitioning can not be fragment and replicate usually has a higher cost than partitioning when both relations are of roughly the same size  since at least one of the relations has to be replicated however  if one of the relations ? say  s ? is small  it may be cheaper to replicate s across all processors  rather than to repartition r and s on the join attributes in such a case  asymmetric fragment and replicate is preferable  even though partitioning could be used 18.5.2.3 partitioned parallel hash join the partitioned hash join of section 12.5.5 can be parallelized suppose that we have n processors  p0  p1      pn-1  and two relations r and s  such that the relations r and s are partitioned across multiple disks recall from section 12.5.5 thesumit67.blogspot.com 810 chapter 18 parallel databases that the smaller relation is chosen as the build relation if the size of s is less than that of r  the parallel hash-join algorithm proceeds this way  1 choose a hash function ? say  h1 ? that takes the join attribute value of each tuple in r and s and maps the tuple to one of the n processors let ri denote the tuples of relation r that are mapped to processor pi ; similarly  let si denote the tuples of relation s that are mapped to processor pi each processor pi reads the tuples of s that are on its disk di and sends each tuple to the appropriate processor on the basis of hash function h1 2 as the destination processor pi receives the tuples of si  it further partitions them by another hash function  h2  which the processor uses to compute the hash join locally the partitioning at this stage is exactly the same as in the partitioning phase of the sequential hash-join algorithm each processor pi executes this step independently from the other processors 3 once the tuples of s have been distributed  the system redistributes the larger relation r across the n processors by the hash function h1  in the sameway as before as it receives each tuple  the destination processor repartitions it by the function h2  just as the probe relation is partitioned in the sequential hash-join algorithm 4 each processor pi executes the build and probe phases of the hash-join algorithm on the local partitions ri and si of r and s to produce a partition of the final result of the hash join the hash join at each processor is independent of that at other processors  and receiving the tuples of ri and si is similar to reading them from disk therefore  any of the optimizations of the hash join described in chapter 12 can be applied as well to the parallel case in particular  we can use the hybrid hash-join algorithm to cache some of the incoming tuples in memory  and thus avoid the costs of writing them and of reading them back in 18.5.2.4 parallel nested-loop join to illustrate the use of fragment-and-replicate ? based parallelization  consider the case where the relation s is much smaller than relation r  suppose that relation r is stored by partitioning ; the attribute on which it is partitioned does not matter suppose too that there is an index on a join attribute of relation r at each of the partitions of relation r  we use asymmetric fragment and replicate  with relation s being replicated andwith the existing partitioning of relation r  each processor pj where a partition of relation s is stored reads the tuples of relation s stored in dj  and replicates the tuples to every other processor pi  at the end of this phase  relation s is replicated at all sites that store tuples of relation r  now  each processor pi performs an indexed nested-loop join of relation s with the ith partition of relation r  we can overlap the indexed nested-loop join thesumit67.blogspot.com 18.5 intraoperation parallelism 811 with the distribution of tuples of relation s  to reduce the costs of writing the tuples of relation s to disk  and of reading them back however  the replication of relation s must be synchronized with the join so that there is enough space in the in-memory buffers at each processor pi to hold the tuples of relation s that have been received but that have not yet been used in the join 18.5.3 other relational operations the evaluation of other relational operations also can be parallelized  ? selection let the selection be    r   consider first the case where  is of the form ai = v  where ai is an attribute and v is a value if the relation r is partitioned on ai  the selection proceeds at a single processor if  is of the form l = ai = u ? that is   is a range selection ? and the relation has been range-partitioned on ai  then the selection proceeds at each processor whose partition overlaps with the specified range of values in all other cases  the selection proceeds in parallel at all the processors ? duplicate elimination duplicates can be eliminated by sorting ; either of the parallel sort techniques can be used  optimized to eliminate duplicates as soon as they appear during sorting we can also parallelize duplicate elimination by partitioning the tuples  by either range or hash partitioning  and eliminating duplicates locally at each processor ? projection projection without duplicate elimination can be performed as tuples are read in from disk in parallel if duplicates are to be eliminated  either of the techniques just described can be used ? aggregation consider an aggregation operation we can parallelize the operation by partitioning the relation on the grouping attributes  and then computing the aggregate values locally at each processor either hash partitioning or range partitioning can be used if the relation is already partitioned on the grouping attributes  the first step can be skipped we can reduce the cost of transferring tuples during partitioning by partly computing aggregate values before partitioning  at least for the commonly used aggregate functions consider an aggregation operation on a relation r  using the sum aggregate function on attribute b  with grouping on attribute a the system can perform the operation at each processor pi on those r tuples stored on disk di  this computation results in tupleswith partial sums at each processor ; there is one tuple at pi for each value for attribute a present in r tuples stored on di  the system partitions the result of the local aggregation on the grouping attribute a  and performs the aggregation again  on tuples with the partial sums  at each processor pi to get the final result as a result of this optimization  fewer tuples need to be sent to other processors during partitioning this idea can be extended easily to the min and max aggregate functions extensions to the count and avg aggregate functions are left for you to do in exercise 18.12 thesumit67.blogspot.com 812 chapter 18 parallel databases the parallelization of other operations is covered in several of the exercises 18.5.4 cost of parallel evaluation of operations we achieve parallelism by partitioning the i/o among multiple disks  and partitioning the cpu work among multiple processors if such a split is achieved without any overhead  and if there is no skew in the splitting of work  a parallel operation using n processors will take 1/n times as long as the same operation on a single processor we already know how to estimate the cost of an operation such as a join or a selection the time cost of parallel processing would then be 1/n of the time cost of sequential processing of the operation we must also account for the following costs  ? start-up costs for initiating the operation at multiple processors ? skew in the distribution ofwork among the processors,with some processors getting a larger number of tuples than others ? contention for resources ? such as memory  disk  and the communication network ? resulting in delays ? cost of assembling the final result by transmitting partial results from each processor the time taken by a parallel operation can be estimated as  tpart + tasm + max  t0  t1      tn-1  where tpart is the time for partitioning the relations  tasm is the time for assembling the results  and ti is the time taken for the operation at processor pi  assuming that the tuples are distributed without any skew  the number of tuples sent to each processor can be estimated as 1/n of the total number of tuples ignoring contention  the cost ti of the operations at each processor pi can then be estimated by the techniques in chapter 12 the preceding estimate will be an optimistic estimate  since skew is common even though breaking down a single query into a number of parallel steps reduces the size of the average step  it is the time for processing the single slowest step that determines the time taken for processing the query as a whole a partitioned parallel evaluation  for instance  is only as fast as the slowest of the parallel executions thus  any skew in the distribution of the work across processors greatly affects performance the problem of skew in partitioning is closely related to the problem of partition overflow in sequential hash joins  chapter 12   we can use overflow resolution and avoidance techniques developed for hash joins to handle skew when hash partitioning is used we can use balanced range partitioning and virtual processor partitioning to minimize skew due to range partitioning  as in section 18.2.3 thesumit67.blogspot.com 18.6 interoperation parallelism 813 18.6 interoperation parallelism there are two forms of interoperation parallelism  pipelined parallelism and independent parallelism 18.6.1 pipelined parallelism as discussed in chapter 12  pipelining forms an important source of economy of computation for database query processing recall that  in pipelining  the output tuples of one operation  a  are consumed by a second operation  b  even before the first operation has produced the entire set of tuples in its output the major advantage of pipelined execution in a sequential evaluation is that we can carry out a sequence of such operations without writing any of the intermediate results to disk parallel systems use pipelining primarily for the same reason that sequential systems do however  pipelines are a source of parallelism as well  in the same way that instruction pipelines are a source of parallelism in hardware design it is possible to run operations aand b simultaneously on different processors  so that b consumes tuples in parallel with a producing them this form of parallelism is called pipelined parallelism consider a join of four relations  r1  r2  r3  r4 we can set up a pipeline that allows the three joins to be computed in parallel suppose processor p1 is assigned the computation of temp1 ? r1  r2  and p2 is assigned the computation of r3  temp1 as p1 computes tuples in r1  r2  it makes these tuples available to processor p2 thus  p2 has available to it some of the tuples in r1  r2 before p1 has finished its computation p2 can use those tuples that are available to begin computation of temp1  r3  even before r1  r2 is fully computed by p1 likewise  as p2 computes tuples in  r1  r2   r3  it makes these tuples available to p3  which computes the join of these tuples with r4 pipelined parallelism is useful with a small number of processors  but does not scale up well first  pipeline chains generally do not attain sufficient length to provide a high degree of parallelism second  it is not possible to pipeline relational operators that do not produce output until all inputs have been accessed  such as the set-difference operation third  onlymarginal speedup is obtained for the frequent cases in which one operator ? s execution cost ismuch higher than are those of the others all things considered  when the degree of parallelism is high  pipelining is a less important source of parallelism than partitioning the real reason for using pipelining is that pipelined executions can avoid writing intermediate results to disk thesumit67.blogspot.com 814 chapter 18 parallel databases 18.6.2 independent parallelism operations in a query expression that do not depend on one another can be executed in parallel this form of parallelism is called independent parallelism consider the join r1  r2  r3  r4 clearly  we can compute temp1 ? r1  r2 in parallel with temp2 ? r3  r4 when these two computations complete  we compute  temp1  temp2 to obtain further parallelism  we can pipeline the tuples in temp1 and temp2 into the computation of temp1  temp2  which is itself carried out by a pipelined join  section 12.7.2.2   like pipelined parallelism  independent parallelism does not provide a high degree of parallelism and is less useful in a highly parallel system  although it is useful with a lower degree of parallelism 18.7 query optimization query optimizers account in large measure for the success of relational technology recall that a query optimizer takes a query and finds the cheapest execution plan among the many possible execution plans that give the same answer query optimizers for parallel query evaluation are more complicated than query optimizers for sequential query evaluation first  the cost models are more complicated  since partitioning costs have to be accounted for  and issues such as skew and resource contention must be taken into account more important is the issue of how to parallelize a query suppose that we have somehow chosen an expression  from among those equivalent to the query  to be used for evaluating the query the expression can be represented by an operator tree  as in section 12.1 to evaluate an operator tree in a parallel system,we must make the following decisions  ? how to parallelize each operation  and how many processors to use for it ? what operations to pipeline across different processors  what operations to execute independently in parallel  and what operations to execute sequentially  one after the other these decisions constitute the task of scheduling the execution tree determining the resources of each kind ? such as processors  disks  and memory ? that should be allocated to each operation in the tree is another aspect of the optimization problem for instance  it may appear wise to use the maximum amount of parallelism available  but it is a good idea not to execute certain operations in parallel operations whose computational requirements are significantly smaller than the communication overhead should be clustered with one of their thesumit67.blogspot.com 18.8 design of parallel systems 815 neighbors otherwise  the advantage of parallelism is negated by the overhead of communication one concern is that long pipelines do not lend themselves to good resource utilization unless the operations are coarse grained  the final operation of the pipeline may wait for a long time to get inputs  while holding precious resources  such as memory hence  long pipelines should be avoided the number of parallel evaluation plans from which to choose ismuch larger than the number of sequential evaluation plans optimizing parallel queries by considering all alternatives is therefore much more expensive than optimizing sequential queries hence  we usually adopt heuristic approaches to reduce the number of parallel execution plans that we have to consider we describe two popular heuristics here the first heuristic is to consider only evaluation plans that parallelize every operation across all processors  and that do not use any pipelining this approach is used in the teradata systems finding the best such execution plan is like doing query optimization in a sequential system the main differences lie in how the partitioning is performed and what cost-estimation formula is used the second heuristic is to choose the most efficient sequential evaluation plan  and then to parallelize the operations in that evaluation plan the volcano parallel database popularized a model of parallelization called the exchange-operator model this model uses existing implementations of operations  operating on local copies of data  coupled with an exchange operation that moves data around between different processors exchange operators can be introduced into an evaluation plan to transform it into a parallel evaluation plan yet another dimension of optimization is the design of physical-storage organization to speed up queries the optimal physical organization differs for different queries the database administrator must choose a physical organization that appears to be good for the expected mix of database queries thus  the area of parallel query optimization is complex  and it is still an area of active research 18.8 design of parallel systems so far this chapter has concentrated on parallelization of data storage and of query processing since large-scale parallel database systems are used primarily for storing large volumes of data  and for processing decision-support queries on those data  these topics are the most important in a parallel database system parallel loading of data from external sources is an important requirement  if we are to handle large volumes of incoming data a large parallel database system must also address these availability issues  ? resilience to failure of some processors or disks ? online reorganization of data and schema changes we consider these issues here thesumit67.blogspot.com 816 chapter 18 parallel databases with a large number of processors and disks  the probability that at least one processor or disk will malfunction is significantly greater than in a singleprocessor system with one disk a poorly designed parallel system will stop functioning if any component  processor or disk  fails assuming that the probability of failure of a single processor or disk is small  the probability of failure of the system goes up linearly with the number of processors and disks if a single processor or disk would fail once every 5 years  a system with 100 processors would have a failure every 18 days therefore  large-scale parallel database systems  such as teradata  and ibm informix xps  are designed to operate even if a processor or disk fails data are replicated across at least two processors if a processor fails  the data that it stored can still be accessed from the other processors the system keeps track of failed processors and distributes the work among functioning processors requests for data stored at the failed site are automatically routed to the backup sites that store a replica of the data if all the data of a processor aare replicated at a single processor b  b will have to handle all the requests to aas well as those to itself  and that will result in b becoming a bottleneck therefore  the replicas of the data of a processor are partitioned across multiple other processors when we are dealing with large volumes of data  ranging in the terabytes   simple operations  such as creating indices  and changes to schema  such as adding a column to a relation  can take a long time ? perhaps hours or even days therefore  it is unacceptable for the database system to be unavailable while such operations are in progress most database systems allow such operations to be performed online  that is  while the system is executing other transactions consider  for instance  online index construction.asystem that supports this feature allows insertions  deletions  and updates on a relation even as an index is being built on the relation the index-building operation therefore can not lock the entire relation in shared mode  as it would have done otherwise instead  the process keeps track of updates that occur while it is active and incorporates the changes into the index being constructed  most database systems today support online index construction  since this feature is very important even for non-parallel database systems  in recent years  a number of companies have developed newparallel database products  including netezza  datallegro  which was acquired by microsoft   greenplum  and aster data each of these products runs on systems containing tens to thousands of nodes  with each node running an instance of an underlying database ; each product manages the partitioning of data  as well as parallel processing of queries  across the database instances netezza  greenplum and aster data use postgresql as the underlying database ; datallegro originally used ingres as the underlying database system  but moved to sql server subsequent to its acquisition by microsoft by building on top of an existing database system  these systems are able to leverage the data storage  query processing  and transaction management features of the underlying database  leaving them free to focus on data partitioning  including replication for fault tolerance   fast interprocessor communication  parallel query processing  and parallel-query optimization another benefit of using a public domain thesumit67.blogspot.com 18.9 parallelism on multicore processors 817 database such as postgresql is that the software cost per node is very low ; in contrast commercial databases have a significant per-processor cost it is also worth mentioning that netezza and datallegro actually sell data warehouse ? appliances ?  which include hardware and software  allowing customers to build parallel databases with minimal effort 18.9 parallelism on multicore processors parallelism has become commonplace on most computers today  even some of the smallest  due to current trends in computer architecture as a result  virtually all database systems today run on a parallel platform in this section  we shall explore briefly the reasons for this architectural trend and the effects this has on database system design and implementation 18.9.1 parallelism versus raw speed since the dawn of computers  processor speed has increased at an exponential rate  doubling every 18 to 24 months this increase results from an exponential growth in the number of transistors that could be fit within a unit area of a silicon chip  and is known popularly as moore ? s law  named after intel co-founder gordon moore technically  moore ? s law is not a law  but rather an observation and a prediction regarding technology trends until recently  the increase in the number of transistors and the decrease in their size led to ever-faster processors although technological progress continues to behave as predicted by moore ? s law  another factor has emerged to slow the growth in processor speed fast processors are power inefficient this is problematic in terms of energy consumption and cost  battery life for mobile computers  and heat dissipation  all the power used eventually turns into heat   as a result  modern processors typically are not one single processor but rather consist of several processors on one chip to maintain a distinction between on-chip multiprocessors and traditional processors  the term core is used for an on-chip processor thus we say that a machine has a multicore processor.2 18.9.2 cache memory and multithreading each core is capable of processing an independent streamofmachine instructions however  because processors are able to process data faster than it can be accessed from main memory  main memory can become a bottleneck that limits overall performance for this reason  computer designers include one or more levels of cache memory in a computer system cache memory is more costly than main memory on a per-byte basis  but offers a faster access time in multilevel cache designs  the levels are called l1  l2  and so on  with l1 being the fastest cache  and thus the most costly per byte and therefore the smallest   l2 the next fastest  2the use of the term core here is different from the use of that term in the early days of computing to refer to a main-memory technology based on magnetic cores thesumit67.blogspot.com 818 chapter 18 parallel databases and so on the result is an extension of the storage hierarchy that we discussed in chapter 10 to include the various levels of cache below main memory although the database system can control the transfer of data between disk and main memory  the computer hardware maintains control over the transfer of data among the various levels of cache and between cache and main memory despite this lack of direct control  the database system ? s performance can be affected by how cache is utilized if a core needs to access a data item that is not in cache  it must be fetched from main memory because main memory is so much slower than processors  a significant amount of potential processing speed may be lost while a core waits for data from main memory these waits are referred to as cache misses one way in which computer designers attempt to limit the impact of cache misses is via multithreading a thread is an execution stream that sharesmemory3 with other threads running on the same core if the thread currently executing on a core suffers a cache miss  or other type of wait   the core proceeds to execute another thread  thereby not wasting computing speed while waiting threads introduce yet another source of parallelism beyond the multiplicity of cores each new generation of processors supports more cores and more threads the sun ultrasparc t2 processor has 8 cores  each of which supports 8 threads  for a total of 64 threads on one processor chip the architecture trend of slower increase in raw speed accompanied by the growth in the number of cores has significant implications for database system design  as we shall see shortly 18.9.3 adapting database system design for modern architectures it would appear that database systems are an ideal application to take advantage of large numbers of cores and threads  since database systems support large numbers of concurrent transactions however  there are a variety of factors that make optimal use of modern processors challenging as we allow a higher degree of concurrency to take advantage of the parallelism of modern processors  we increase the amount of data that needs to be in cache this can result in more cache misses  perhaps so many that even a multithreaded core has to wait for data from memory concurrent transactions need some sort of concurrency control to ensure the acid properties that we discussed in chapter 14 when concurrent transactions access data in common  some sort of restrictions must be imposed on that concurrent access those restrictions  whether based on locks  timestamps  or validation  result in waiting or the loss of work due to transaction aborts to avoid excessive amounts of waiting or lost work  it is ideal that concurrent transactions conflict rarely  but attempting to ensure that can increase the amount of data needed in cache  resulting in more cache misses finally  there are components of a database system shared by all transactions in a system using locking  the lock table is shared by all transactions and access to 3technically  in operating-system terminology  its address space thesumit67.blogspot.com 18.10 summary 819 it can become a bottleneck similar problems exist for other forms of concurrency control similarly  the buffer manager  the logmanager  and the recoverymanager serve all transactions and are potential bottlenecks because having a large number of concurrent transactions may not take optimal advantage of modern processors  it is desirable to find ways to allow multiple cores to work on a single transaction this requires the database query processor to find effective ways to parallelize queries without creating excessive demands on cache this can be done by creating pipelines of database operations from queries and by finding ways to parallelize individual database operations the adaptation of database system design and database query processing to multicore and multithreaded systems remains an area of active research see the bibliographical notes for further details 18.10 summary ? parallel databases have gained significant commercial acceptance in the past 20 years ? in i/o parallelism  relations are partitioned among available disks so that they can be retrieved faster three commonly used partitioning techniques are round-robin partitioning  hash partitioning  and range partitioning ? skew is a major problem  especially with increasing degrees of parallelism balanced partitioning vectors  using histograms  and virtual processor partitioning are among the techniques used to reduce skew ? in interquery parallelism  we run different queries concurrently to increase throughput ? intraquery parallelism attempts to reduce the cost of running a query there are two types of intraquery parallelism  intraoperation parallelism and interoperation parallelism ? we use intraoperation parallelism to execute relational operations  such as sorts and joins  in parallel intraoperation parallelism is natural for relational operations  since they are set oriented ? there are two basic approaches to parallelizing a binary operation such as a join ? in partitioned parallelism  the relations are split into several parts  and tuples in ri are joined only with tuples fromsi  partitioned parallelism can be used only for natural and equi-joins ? in fragment and replicate  both relations are partitioned and each partition is replicated in asymmetric fragment and replicate  one of the relations is replicated while the other is partitioned unlike partitioned parallelism  fragment and replicate and asymmetric fragment and replicate can be used with any join condition thesumit67.blogspot.com 820 chapter 18 parallel databases both parallelization techniques can work in conjunction with any join technique ? in independent parallelism  different operations that do not depend on one another are executed in parallel ? in pipelined parallelism  processors send the results of one operation to another operation as those results are computed  without waiting for the entire operation to finish ? query optimization in parallel databases is significantly more complex than query optimization in sequential databases ? modern multicore processors are introducing new research problems in parallel databases review terms ? decision-support queries ? i/o parallelism ? horizontal partitioning ? partitioning techniques ? round-robin ? hash partitioning ? range partitioning ? partitioning attribute ? partitioning vector ? point query ? range query ? skew ? execution skew ? attribute-value skew ? partition skew ? handling of skew ? balanced range-partitioning vector ? histogram ? virtual processors ? interquery parallelism ? cache coherency ? intraquery parallelism ? intraoperation parallelism ? interoperation parallelism ? parallel sort ? range-partitioning sort ? parallel external sort ? merge ? data parallelism ? parallel join ? partitioned join ? fragment-and-replicate join ? asymmetric fragment-andreplicate join ? partitioned parallel hash join ? parallel nested-loop join ? parallel selection ? parallel duplicate elimination ? parallel projection ? parallel aggregation ? cost of parallel evaluation thesumit67.blogspot.com practice exercises 821 ? interoperation parallelism ? pipelined parallelism ? independent parallelism ? query optimization ? scheduling ? exchange-operator model ? design of parallel systems ? online index construction ? multicore processors practice exercises 18.1 in a range selection on a range-partitioned attribute  it is possible that only one disk may need to be accessed describe the benefits and drawbacks of this property 18.2 what form of parallelism  interquery  interoperation  or intraoperation  is likely to be the most important for each of the following tasks ? a increasing the throughput of a system with many small queries b increasing the throughput of a system with a few large queries,when the number of disks and processors is large 18.3 with pipelined parallelism  it is often a good idea to perform several operations in a pipeline on a single processor  evenwhen many processors are available a explain why b would the arguments you advanced in part a hold if the machine has a shared-memory architecture ? explain why or why not c would the arguments in part a hold with independent parallelism ?  that is  are there caseswhere  even if the operations are not pipelined and there are many processors available  it is still a good idea to perform several operations on the same processor ?  18.4 consider join processing using symmetric fragment and replicate with range partitioning how can you optimize the evaluation if the join condition is of the form | r.a s.b | = k  where k is a small constant ? here  | x | denotes the absolute value of x a join with such a join condition is called a band join 18.5 recall that histograms are used for constructing load-balanced range partitions a suppose you have a histogram where values are between 1 and 100  and are partitioned into 10 ranges  1 ? 10  11 ? 20      91 ? 100  with frequencies 15  5  20  10  10  5  5  20  5  and 5  respectively give a load-balanced range partitioning function to divide the values into 5 partitions thesumit67.blogspot.com 822 chapter 18 parallel databases b write an algorithm for computing a balanced range partition with p partitions  given a histogram of frequency distributions containing n ranges 18.6 large-scale parallel database systems store an extra copy of each data item on disks attached to a different processor  to avoid loss of data if one of the processors fails a instead of keeping the extra copy of data items from a processor at a single backup processor  it is a good idea to partition the copies of the data items of a processor across multiple processors explain why b explain how virtual-processor partitioning can be used to efficiently implement the partitioning of the copies as described above c what are the benefits and drawbacks of using raid storage instead of storing an extra copy of each data item ? 18.7 suppose we wish to index a large relation that is partitioned can the idea of partitioning  including virtual processor partitioning  be applied to indices ? explain your answer  considering the following two cases  assuming for simplicity that partitioning aswell as indexing are on single attributes   a where the index is on the partitioning attribute of the relation b where the index is on an attribute other than the partitioning attribute of the relation 18.8 suppose a well-balanced range-partitioning vector had been chosen for a relation  but the relation is subsequently updated  making the partitioning unbalanced even if virtual-processor partitioning is used  a particular virtual processormay end up with a very large number of tuples after the update  and repartitioning would then be required a suppose a virtual processor has a significant excess of tuples  say  twice the average   explain how repartitioning can be done by splitting the partition  thereby increasing the number of virtual processors b if  instead of round-robin allocation of virtual processors  virtual partitions can be allocated to processors in an arbitrary fashion  with a mapping table tracking the allocation if a particular node has excess load  compared to the others   explain how load can be balanced c assuming there are no updates  does query processing have to be stopped while repartitioning  or reallocation of virtual processors  is carried out ? explain your answer thesumit67.blogspot.com exercises 823 exercises 18.9 for each of the three partitioning techniques  namely round-robin  hash partitioning  and range partitioning  give an example of a query for which that partitioning technique would provide the fastest response 18.10 what factors could result in skew when a relation is partitioned on one of its attributes by  a hash partitioning ? b range partitioning ? in each case  what can be done to reduce the skew ? 18.11 give an example of a join that is not a simple equi-join for which partitioned parallelism can be used what attributes should be used for partitioning ? 18.12 describe a good way to parallelize each of the following  a the difference operation b aggregation by the count operation c aggregation by the count distinct operation d aggregation by the avg operation e left outer join  if the join condition involves only equality f left outer join  if the join condition involves comparisons other than equality g full outer join  if the join condition involves comparisons other than equality 18.13 describe the benefits and drawbacks of pipelined parallelism 18.14 suppose you wish to handle a workload consisting of a large number of small transactions by using shared-nothing parallelism a is intraquery parallelism required in such a situation ? if not  why  and what form of parallelism is appropriate ? b what form of skew would be of significance with such a workload ? c suppose most transactions accessed one account record  which includes an account type attribute  and an associated account type master record  which provides information about the account type how would you partition and/or replicate data to speed up transactions ? you may assume that the account type master relation is rarely updated thesumit67.blogspot.com 824 chapter 18 parallel databases 18.15 the attribute on which a relation is partitioned can have a significant impact on the cost of a query a given aworkload of sql queries on a single relation,what attributes would be candidates for partitioning ? b how would you choose between the alternative partitioning techniques  based on the workload ? c is it possible to partition a relation on more than one attribute ? explain your answer bibliographical notes in the late 1970s and early 1980s  as the relational model gained reasonably sound footing  people recognized that relational operators are highly parallelizable and have good dataflow properties several research projects including gamma  de witt  1990    xprs  stonebraker et al  1989   and volcano  graefe  1990   were launched to investigate the practicality of parallel execution of relational operators teradata was one of the first commercial parallel database systems  and continues to have a large market share the red brickwarehouse was another early parallel database system ; red brick was was acquired by informix  which was itself acquired by ibm more recent parallel database systems include netezza  datallegro  now part of microsoft   greenplum  and aster data locking in parallel databases is discussed in joshi  1991  and mohan and narang  1992   cache-coherency protocols for parallel database systems are discussed by dias et al  1989   mohan and narang  1992   and rahm  1993   carey et al  1991  discusses caching issues in a client ? server system graefe and mckenna  1993b  presents an excellent survey of query processing  including parallel processing of queries the exchange-operator model was advocated by graefe  1990  and graefe and mckenna  1993b   parallel sorting is discussed in dewitt et al  1992   parallel sorting on multicore and multithreaded processors is discussed in garcia and korth  2005  and chen et al  2007   parallel join algorithms are described by nakayama et al  1984   richardson et al  1987   kitsuregawa and ogawa  1990   and wilschut et al  1995   among other works skew handling in parallel joins is described by walton et al  1991   wolf  1991   and dewitt et al  1992   parallel query-optimization techniques are described by lu et al  1991  and ganguly et al  1992   the adaptation of database-system design and query-processing algorithms to multicore and multithreaded architectures is discussed in the proceedings of the international workshop on data management on modern hardware  damon   held annually since 2005 thesumit67.blogspot.com chapter19 distributed databases unlike parallel systems  in which the processors are tightly coupled and constitute a single database system  a distributed database system consists of loosely coupled sites that share no physical components furthermore  the database systems that run on each site may have a substantial degree of mutual independence we discussed the basic structure of distributed systems in chapter 17 each site may participate in the execution of transactions that access data at one site  or several sites themain difference between centralized and distributed database systems is that  in the former  the data reside in one single location  whereas in the latter  the data reside in several locations this distribution of data is the cause of many difficulties in transaction processing and query processing in this chapter  we address these difficulties we start by classifying distributed databases as homogeneous or heterogeneous  in section 19.1 we then address the question of how to store data in a distributed database in section 19.2 in section 19.3  we outline a model for transaction processing in a distributed database in section 19.4  we describe how to implement atomic transactions in a distributed database by using special commit protocols in section 19.5  we describe concurrency control in distributed databases in section 19.6  we outline how to provide high availability in a distributed database by exploiting replication  so the system can continue processing transactions even when there is a failure we address query processing in distributed databases in section 19.7 in section 19.8  we outline issues in handling heterogeneous databases in section 19.10  we describe directory systems  which can be viewed as a specialized form of distributed databases in this chapter  we illustrate all our examples using the bank database of figure 19.1 19.1 homogeneous and heterogeneous databases in a homogeneous distributed database system  all sites have identical databasemanagement system software  are aware of one another  and agree to cooperate in processing users ? requests in such a system  local sites surrender a portion of their autonomy in terms of their right to change schemas or database-management 825 thesumit67.blogspot.com 826 chapter 19 distributed databases branch  branch name  branch city  assets  account  account number  branch name  balance  depositor  customer name  account number  figure 19.1 banking database system software that software must also cooperatewith other sites in exchanging information about transactions  to make transaction processing possible across multiple sites in contrast  in a heterogeneous distributed database  different sitesmay use different schemas  and different database-management system software the sites may not be aware of one another  and they may provide only limited facilities for cooperation in transaction processing the differences in schemas are often a major problem for query processing  while the divergence in software becomes a hindrance for processing transactions that access multiple sites in this chapter  we concentrate on homogeneous distributed databases however  in section 19.8we brieflydiscuss issues in heterogeneousdistributed database systems 19.2 distributed data storage consider a relation r that is to be stored in the database there are two approaches to storing this relation in the distributed database  ? replication the system maintains several identical replicas  copies  of the relation  and stores each replica at a different site the alternative to replication is to store only one copy of relation r ? fragmentation the system partitions the relation into several fragments  and stores each fragment at a different site fragmentation and replication can be combined  a relation can be partitioned into several fragments and there may be several replicas of each fragment in the following subsections  we elaborate on each of these techniques 19.2.1 data replication if relation r is replicated  a copy of relation r is stored in two or more sites in the most extreme case  we have full replication  in which a copy is stored in every site in the system there are a number of advantages and disadvantages to replication ? availability if one of the sites containing relation r fails  then the relation r can be found in another site thus  the system can continue to process queries involving r  despite the failure of one site thesumit67.blogspot.com 19.2 distributed data storage 827 ? increased parallelism in the case where the majority of accesses to the relation r result in only the reading of the relation  then several sites can process queries involving r in parallel the more replicas of r there are  the greater the chance that the needed data will be found in the site where the transaction is executing hence  data replication minimizes movement of data between sites ? increased overhead on update the system must ensure that all replicas of a relation r are consistent ; otherwise  erroneous computations may result thus  whenever r is updated  the update must be propagated to all sites containing replicas the result is increased overhead for example  in a banking system  where account information is replicated in various sites  it is necessary to ensure that the balance in a particular account agrees in all sites in general  replication enhances the performance of read operations and increases the availability of data to read-only transactions however  update transactions incur greater overhead controlling concurrent updates by several transactions to replicated data is more complex than in centralized systems  which we studied in chapter 15 we can simplify the management of replicas of relation r by choosing one of them as the primary copy of r for example  in a banking system  an account can be associated with the site in which the account has been opened similarly  in an airline-reservation system  a flight can be associated with the site at which the flight originates.we shall examine the primary copy scheme and other options for distributed concurrency control in section 19.5 19.2.2 data fragmentation if relation r is fragmented  r is divided into a number of fragments r1  r2      rn these fragments contain sufficient information to allow reconstruction of the original relation r there are two different schemes for fragmenting a relation  horizontal fragmentation and vertical fragmentation horizontal fragmentation splits the relation by assigning each tuple of r to one or more fragments vertical fragmentation splits the relation by decomposing the scheme r of relation r in horizontal fragmentation  a relation r is partitioned into a number of subsets  r1  r2      rn each tuple of relation r must belong to at least one of the fragments  so that the original relation can be reconstructed  if needed as an illustration  the account relation can be divided into several different fragments  each of which consists of tuples of accounts belonging to a particular branch if the banking system has only two branches ? hillside and valleyview ? then there are two different fragments  account1 =  branch name = ? hillside ?  account  account2 =  branch name = ? valleyview ?  account  horizontal fragmentation is usually used to keep tuples at the sites where they are used the most  to minimize data transfer thesumit67.blogspot.com 828 chapter 19 distributed databases in general  a horizontal fragment can be defined as a selection on the global relation r that is,we use a predicate pi to construct fragment ri  ri =  pi  r  we reconstruct the relation r by taking the union of all fragments ; that is  r = r1 ? r2 ? ? ? ? ? rn in our example  the fragments are disjoint by changing the selection predicates used to construct the fragments  we can have a particular tuple of r appear in more than one of the ri  in its simplest form  vertical fragmentation is the same as decomposition  see chapter 8   vertical fragmentation of r  r  involves the definition of several subsets of attributes r1  r2      rn of the schema r so that  r = r1 ? r2 ? ? ? ? ? rn each fragment ri of r is defined by  ri =  ri  r  the fragmentation should be done in such a way that we can reconstruct relation r from the fragments by taking the natural join  r = r1  r2  r3  ? ? ?  rn one way of ensuring that the relation r can be reconstructed is to include the primary-key attributes of r in each ri .more generally  any superkey can be used it is often convenient to add a special attribute  called a tuple-id  to the schema r the tuple-id value of a tuple is a unique value that distinguishes the tuple from all other tuples the tuple-id attribute thus serves as a candidate key for the augmented schema  and is included in each ri  the physical or logical address for a tuple can be used as a tuple-id  since each tuple has a unique address to illustrate vertical fragmentation  consider a university database with a relation employee info that stores  for each employee  employee id  name  designation  and salary for privacy reasons  this relation may be fragmented into a relation employee private info containing employee id and salary  and another relation employee public info containing attributes employee id  name  and designation thesemay be stored at different sites  again  possibly for security reasons the twotypes of fragmentation can be applied to a single schema ; for instance  the fragments obtained by horizontally fragmenting a relation can be further partitioned vertically fragments can also be replicated in general  a fragment can be replicated  replicas of fragments can be fragmented further  and so on thesumit67.blogspot.com 19.2 distributed data storage 829 19.2.3 transparency the user of a distributed database system should not be required to know where the data are physically located nor how the data can be accessed at the specific local site this characteristic  called data transparency  can take several forms  ? fragmentation transparency users are not required to know how a relation has been fragmented ? replication transparency users view each data object as logically unique the distributed system may replicate an object to increase either system performance or data availability users do not have to be concerned with what data objects have been replicated  or where replicas have been placed ? location transparency users are not required to know the physical location of the data the distributed database system should be able to find any data as long as the data identifier is supplied by the user transaction data items ? such as relations  fragments  and replicas ? must have unique names this property is easy to ensure in a centralized database in a distributed database  however,we must take care to ensure that two sites do not use the same name for distinct data items one solution to this problem is to require all names to be registered in a central name server the name server helps to ensure that the same name does not get used for different data items we can also use the name server to locate a data item  given the name of the item this approach  however  suffers from two major disadvantages first  the name server may become a performance bottleneck when data items are located by their names  resulting in poor performance second  if the name server crashes  it may not be possible for any site in the distributed system to continue to run a more widely used alternative approach requires that each site prefix its own site identifier to any name that it generates this approach ensures that no two sites generate the same name  since each site has a unique identifier   furthermore  no central control is required this solution  however  fails to achieve location transparency  since site identifiers are attached to names thus  the account relation might be referred to as site17 account  or account @ site17  rather than as simply account many database systems use the internet address  ip address  of a site to identify it to overcome this problem  the database system can create a set of alternative names  or aliases  for data items a user may thus refer to data items by simple names that are translated by the system to complete names the mapping of aliases to the real names can be stored at each site with aliases  the user can be unaware of the physical location of a data item furthermore  the user will be unaffected if the database administrator decides to move a data item from one site to another users should not have to refer to a specific replica of a data item instead  the system should determine which replica to reference on a read request  and thesumit67.blogspot.com 830 chapter 19 distributed databases should update all replicas on a write request we can ensure that it does so by maintaining a catalog table  which the system uses to determine all replicas for the data item 19.3 distributed transactions access to the various data items in a distributed system is usually accomplished through transactions  which must preserve the acid properties  section 14.1   there are two types of transaction thatwe need to consider the local transactions are those that access and update data in only one local database ; the global transactions are those that access and update data in several local databases ensuring the acid properties of the local transactions can be done as described in chapters 14  15  and 16 however  for global transactions  this task is much more complicated  since several sites may be participating in execution the failure of one of these sites  or the failure of a communication link connecting these sites  may result in erroneous computations in this section  we study the system structure of a distributed database and its possible failure modes in section 19.4  we study protocols for ensuring atomic commit of global transactions  and in section 19.5 we study protocols for concurrency control in distributed databases in section 19.6  we study how a distributed database can continue functioning even in the presence of various types of failure 19.3.1 system structure each site has its own local transaction manager  whose function is to ensure the acid properties of those transactions that execute at that site the various transaction managers cooperate to execute global transactions to understand how such a manager can be implemented  consider an abstract model of a transaction system  in which each site contains two subsystems  ? the transaction manager manages the execution of those transactions  or subtransactions  that access data stored in a local site note that each such transaction may be either a local transaction  that is  a transaction that executes at only that site  or part of a global transaction  that is  a transaction that executes at several sites   ? the transaction coordinator coordinates the execution of the various transactions  both local and global  initiated at that site the overall system architecture appears in figure 19.2 the structure of a transaction manager is similar in many respects to the structure of a centralized system each transaction manager is responsible for  ? maintaining a log for recovery purposes thesumit67.blogspot.com 19.3 distributed transactions 831 tm1 tmn computer 1 computer n tc1 tcn transaction coordinator transaction manager figure 19.2 system architecture ? participating in an appropriate concurrency-control scheme to coordinate the concurrent execution of the transactions executing at that site as we shall see  we need to modify both the recovery and concurrency schemes to accommodate the distribution of transactions the transaction coordinator subsystem is not needed in the centralized environment  since a transaction accesses data at only a single site a transaction coordinator  as its name implies  is responsible for coordinating the execution of all the transactions initiated at that site for each such transaction  the coordinator is responsible for  ? starting the execution of the transaction ? breaking the transaction into a number of subtransactions and distributing these subtransactions to the appropriate sites for execution ? coordinating the termination of the transaction  which may result in the transaction being committed at all sites or aborted at all sites 19.3.2 system failure modes a distributed system may suffer from the same types of failure that a centralized system does  for example  software errors  hardware errors  or disk crashes   there are  however  additional types of failure with which we need to deal in a distributed environment the basic failure types are  ? failure of a site ? loss of messages thesumit67.blogspot.com 832 chapter 19 distributed databases ? failure of a communication link ? network partition the loss or corruption of messages is always a possibility in a distributed system the system uses transmission-control protocols  such as tcp/ip  tohandle such errors information about such protocolsmaybe found in standard textbooks on networking  see the bibliographical notes   however  if two sites a and b are not directly connected  messages from one to the other must be routed through a sequence of communication links if a communication link fails  messages that would have been transmitted across the link must be rerouted in some cases  it is possible to find another route through the network  so that the messages are able to reach their destination in other cases  a failure may result in there being no connection between some pairs of sites a system is partitioned if it has been split into two  or more  subsystems  called partitions  that lack any connection between them note that  under this definition  a partition may consist of a single node 19.4 commit protocols if we are to ensure atomicity  all the sites in which a transaction t executed must agree on the final outcome of the execution t must either commit at all sites  or it must abort at all sites to ensure this property  the transaction coordinator of t must execute a commit protocol among the simplest and mostwidely used commit protocols is the two-phase commit protocol  2pc   which is described in section 19.4.1 an alternative is the three-phase commit protocol  3pc   which avoids certain disadvantages of the 2pc protocol but adds to complexity and overhead section 19.4.2 briefly outlines the 3pc protocol 19.4.1 two-phase commit we first describe how the two-phase commit protocol  2pc  operates during normal operation  then describe how it handles failures and finally how it carries out recovery and concurrency control consider a transaction t initiated at site si  where the transaction coordinator is ci  19.4.1.1 the commit protocol when t completes its execution ? that is  when all the sites at which t has executed inform ci that t has completed ? ci starts the 2pc protocol ? phase 1 ci adds the record < prepare t > to the log  and forces the log onto stable storage it then sends a prepare t message to all sites at which t executed on receiving such a message  the transaction manager at that site thesumit67.blogspot.com 19.4 commit protocols 833 determines whether it is willing to commit its portion of t if the answer is no  it adds a record < no t > to the log  and then responds by sending an abort t message to ci  if the answer is yes  it adds a record < ready t > to the log  and forces the log  with all the log records corresponding to t  onto stable storage the transaction manager then replies with a ready t message to ci  ? phase 2 when ci receives responses to the prepare t message from all the sites  or when a prespecified interval of time has elapsed since the prepare t message was sent out  ci can determine whether the transaction t can be committed or aborted transaction t can be committed if ci received a ready t message from all the participating sites otherwise  transaction t must be aborted depending on the verdict  either a record < commit t > or a record < abort t > is added to the log and the log is forced onto stable storage at this point  the fate of the transaction has been sealed following this point  the coordinator sends either a commit t or an abort t message to all participating sites when a site receives that message  it records the message in the log a site at which t executed can unconditionally abort t at any time before it sends the message ready t to the coordinator once the message is sent  the transaction is said to be in the ready state at the site the ready t message is  in effect  a promise by a site to follow the coordinator ? s order to commit t or to abort t to make such a promise  the needed information must first be stored in stable storage otherwise  if the site crashes after sending ready t  it may be unable to make good on its promise further  locks acquired by the transaction must continue to be held until the transaction completes since unanimity is required to commit a transaction  the fate of t is sealed as soon as at least one site responds abort t since the coordinator site si is one of the sites at which t executed  the coordinator can decide unilaterally to abort t the final verdict regarding t is determined at the time that the coordinator writes that verdict  commit or abort  to the log and forces that verdict to stable storage in some implementations of the 2pc protocol  a site sends an acknowledge t message to the coordinator at the end of the second phase of the protocol when the coordinator receives the acknowledge t message from all the sites  it adds the record < complete t > to the log 19.4.1.2 handling of failures the 2pc protocol responds in different ways to various types of failures  ? failure of a participating site if the coordinator ci detects that a site has failed  it takes these actions  if the site fails before responding with a ready t message to ci  the coordinator assumes that it responded with an abort t message if the site fails after the coordinator has received the ready t message from the site  the coordinator executes the rest of the commit protocol in the normal fashion  ignoring the failure of the site when a participating site sk recovers froma failure  itmust examine its log to determine the fate of those transactions that were in the midst of execution thesumit67.blogspot.com 834 chapter 19 distributed databases when the failure occurred let t be one such transaction we consider each of the possible cases  ? the log contains a < commit t > record in this case  the site executes redo  t   ? the log contains an < abort t > record in this case  the site executes undo  t   ? the log contains a < ready t > record in this case  the site must consult ci to determine the fate of t if ci is up  it notifies sk regarding whether t committed or aborted in the former case  it executes redo  t  ; in the latter case  it executes undo  t   if ci is down  sk must try to find the fate of t from other sites it does so by sending a querystatus t message to all the sites in the system on receiving such a message  a site must consult its log to determine whether t has executed there  and if t has  whether t committed or aborted it then notifies sk about this outcome if no site has the appropriate information  that is  whether t committed or aborted   then sk can neither abort nor commit t the decision concerning t is postponed until sk can obtain the needed information thus  sk must periodically resend the querystatus message to the other sites it continues to do so until a site that contains the needed information recovers note that the site at which ci resides always has the needed information ? the log contains no control records  abort  commit  ready  concerning t thus  we know that sk failed before responding to the prepare t message from ci  since the failure of sk precludes the sending of such a response  by our algorithm ci must abort t hence  sk must execute undo  t   ? failure of the coordinator if the coordinator fails in the midst of the execution of the commit protocol for transaction t  then the participating sites must decide the fate of t we shall see that  in certain cases  the participating sites can not decide whether to commit or abort t  and therefore these sites must wait for the recovery of the failed coordinator ? if an active site contains a < commit t > record in its log  then t must be committed ? if an active site contains an < abort t > record in its log  then t must be aborted ? if some active site does not contain a < ready t > record in its log  then the failed coordinator ci can not have decided to commit t  because a site that does not have a < ready t > record in its log can not have sent a ready t message to ci  however  the coordinator may have decided to abort t  but not to commit t rather than wait for ci to recover  it is preferable to abort t ? if none of the preceding cases holds  then all active sites must have a < ready t > record in their logs  but no additional control records  such thesumit67.blogspot.com 19.4 commit protocols 835 as < abort t > or < commit t >   since the coordinator has failed  it is impossible to determine whether a decision has been made  and if one has  what that decision is  until the coordinator recovers thus  the active sitesmustwait for ci to recover since the fate of t remains in doubt  t may continue to hold system resources for example  if locking is used  t may hold locks on data at active sites such a situation is undesirable  because it may be hours or days before ci is again active during this time  other transactions may be forced to wait for t as a result  data items may be unavailable not only on the failed site  ci   but on active sites as well this situation is called the blocking problem  because t is blocked pending the recovery of site ci  ? network partition.when a network partitions  two possibilities exist  1 the coordinator and all its participants remain in one partition in this case  the failure has no effect on the commit protocol 2 the coordinator and its participants belong to several partitions from the viewpoint of the sites in one of the partitions  it appears that the sites in other partitions have failed sites that are not in the partition containing the coordinator simply execute the protocol to deal with failure of the coordinator the coordinator and the sites that are in the same partition as the coordinator follow the usual commit protocol  assuming that the sites in the other partitions have failed thus  the major disadvantage of the 2pc protocol is that coordinator failure may result in blocking  where a decision either to commit or to abort t may have to be postponed until ci recovers 19.4.1.3 recovery and concurrency control when a failed site restarts  we can perform recovery by using  for example  the recovery algorithm described in section 16.4 to deal with distributed commit protocols  the recovery procedure must treat in-doubt transactions specially ; indoubt transactions are transactions for which a < ready t > log record is found  but neither a < commit t > log record nor an < abort t > log record is found the recovering site must determine the commit ? abort status of such transactions by contacting other sites  as described in section 19.4.1.2 if recovery is done as just described  however  normal transaction processing at the site can not begin until all in-doubt transactions have been committed or rolled back finding the status of in-doubt transactions can be slow  sincemultiple sites may have to be contacted further  if the coordinator has failed  and no other site has information about the commit ? abort status of an incomplete transaction  recovery potentially could become blocked if 2pc is used as a result  the site performing restart recovery may remain unusable for a long period to circumvent this problem  recovery algorithms typically provide support for noting lock information in the log  we are assuming here that locking is used for concurrency control  instead of writing a < ready t > log record  the algorithm thesumit67.blogspot.com 836 chapter 19 distributed databases writes a < ready t  l > log record  where l is a list of all write locks held by the transaction t when the log record is written at recovery time  after performing local recovery actions  for every in-doubt transaction t  all the write locks noted in the < ready t  l > log record  read from the log  are reacquired after lock reacquisition is complete for all in-doubt transactions  transaction processing can start at the site  even before the commit ? abort status of the indoubt transactions is determined the commit or rollback of in-doubt transactions proceeds concurrentlywith the execution of new transactions thus  site recovery is faster  and never gets blocked note that new transactions that have a lock conflict with any write locks held by in-doubt transactions will be unable to make progress until the conflicting in-doubt transactions have been committed or rolled back 19.4.2 three-phase commit the three-phase commit  3pc  protocol is an extension of the two-phase commit protocol that avoids the blocking problem under certain assumptions in particular  it is assumed that no network partition occurs  and not more than k sites fail  where k is some predetermined number under these assumptions  the protocol avoids blocking by introducing an extra third phase where multiple sites are involved in the decision to commit instead of directly noting the commit decision in its persistent storage  the coordinator first ensures that at least k other sites know that it intended to commit the transaction if the coordinator fails  the remaining sites first select a new coordinator this new coordinator checks the status of the protocol from the remaining sites ; if the coordinator had decided to commit  at least one of the other k sites that it informed will be up and will ensure that the commit decision is respected the new coordinator restarts the third phase of the protocol if some site knew that the old coordinator intended to commit the transaction otherwise the new coordinator aborts the transaction while the 3pc protocol has the desirable property of not blocking unless k sites fail  it has the drawback that a partitioning of the network may appear to be the same as more than k sites failing  which would lead to blocking the protocol also has to be implemented carefully to ensure that network partitioning  or more than k sites failing  does not result in inconsistencies  where a transaction is committed in one partition and aborted in another because of its overhead  the 3pc protocol is not widely used see the bibliographical notes for references giving more details of the 3pc protocol 19.4.3 alternative models of transaction processing for many applications  the blocking problem of two-phase commit is not acceptable the problem here is the notion of a single transaction that works across multiple sites in this section  we describe how to use persistent messaging to avoid the problem of distributed commit  and then briefly outline the larger issue of workflows ; workflows are considered in more detail in section 26.2 to understand persistent messaging  consider how one might transfer funds between two different banks  each with its owncomputer one approach is to have thesumit67.blogspot.com 19.4 commit protocols 837 a transaction span the two sites and use two-phase commit to ensure atomicity however  the transaction may have to update the total bank balance  and blocking could have a serious impact on all other transactions at each bank  since almost all transactions at the bank would update the total bank balance in contrast  consider how funds transfer by a bank check occurs the bank first deducts the amount of the check from the available balance and prints out a check the check is then physically transferred to the other bank where it is deposited after verifying the check  the bank increases the local balance by the amount of the check the check constitutes amessage sent between the two banks so that funds are not lost or incorrectly increased  the check must not be lost  and must not be duplicated and deposited more than once.when the bank computers are connected by a network  persistent messages provide the same service as the check  but much faster  of course   persistent messages are messages that are guaranteed to be delivered to the recipient exactly once  neither less nor more   regardless of failures  if the transaction sending themessage commits  and are guaranteed to not be delivered if the transaction aborts database recovery techniques are used to implement persistent messaging on top of the normal network channels  as we shall see shortly in contrast  regular messages may be lost or may even be delivered multiple times in some situations error handling is more complicated with persistent messaging than with twophase commit for instance  if the account where the check is to be deposited has been closed  the check must be sent back to the originating account and credited back there both sitesmust therefore be providedwith error-handling code  along with code to handle the persistent messages in contrast  with two-phase commit  the error would be detected by the transaction  which would then never deduct the amount in the first place the types of exception conditions that may arise depend on the application  so it is not possible for the database system to handle exceptions automatically the application programs that send and receive persistent messages must include code to handle exception conditions and bring the system back to a consistent state for instance  it is not acceptable to just lose the money being transferred if the receiving account has been closed ; the money must be credited back to the originating account  and if that is not possible for some reason  humans must be alerted to resolve the situation manually there are many applications where the benefit of eliminating blocking is well worth the extra effort to implement systems that use persistent messages in fact  few organizations would agree to support two-phase commit for transactions originating outside the organization  since failures could result in blocking of access to local data persistent messaging therefore plays an important role in carrying out transactions that cross organizational boundaries workflows provide a general model of transaction processing involving multiple sites and possibly human processing of certain steps for instance  when a bank receives a loan application  there are many steps it must take  including contacting external credit-checking agencies  before approving or rejecting a loan application the steps  together  form a workflow we study workflows in more thesumit67.blogspot.com 838 chapter 19 distributed databases detail in section 26.2.we also note that persistentmessaging forms the underlying basis for workflows in a distributed environment we now consider the implementation of persistent messaging persistent messaging can be implemented on top of an unreliable messaging infrastructure  which may lose messages or deliver them multiple times  by these protocols  ? sending site protocol when a transaction wishes to send a persistent message  itwrites a record containing the message in a special relation messages to send  instead of directly sending out the message the message is also given a unique message identifier a message delivery process monitors the relation  and when a new message is found  it sends the message to its destination the usual database concurrency-control mechanisms ensure that the system process reads the message only after the transaction that wrote the message commits ; if the transaction aborts  the usual recovery mechanism would delete the message from the relation the message delivery process deletes a message from the relation only after it receives an acknowledgment from the destination site if it receives no acknowledgement from the destination site  after some time it sends the message again it repeats this until an acknowledgment is received in case of permanent failures  the system will decide  after some period of time  that the message is undeliverable exception handling code provided by the application is then invoked to deal with the failure writing the message to a relation and processing it only after the transaction commits ensures that the message will be delivered if and only if the transaction commits repeatedly sending it guarantees it will be delivered even if there are  temporary  system or network failures ? receiving site protocol when a site receives a persistent message  it runs a transaction that adds the message to a special received messages relation  provided it is not already present in the relation  the unique message identifier allows duplicates to be detected   after the transaction commits  or if the message was already present in the relation  the receiving site sends an acknowledgment back to the sending site note that sending the acknowledgment before the transaction commits is not safe  since a system failure may then result in loss of the message checking whether the message has been received earlier is essential to avoid multiple deliveries of the message in many messaging systems  it is possible for messages to get delayed arbitrarily  although such delays are very unlikely therefore  to be safe  the message must never be deleted from the received messages relation deleting it could result in a duplicate delivery not being detected but as a result  the received messages relation may grow indefinitely to deal with this problem  each message is given a timestamp  and if the timestamp of a received message is older than some cutoff  the message is discarded all messages recorded in the received messages relation that are older than the cutoff can be deleted thesumit67.blogspot.com 19.5 concurrency control in distributed databases 839 19.5 concurrency control in distributed databases we show here how some of the concurrency-control schemes discussed in chapter 15 can be modified so that they can be used in a distributed environment.we assume that each site participates in the execution of a commit protocol to ensure global transaction atomicity the protocols we describe in this section require updates to be done on all replicas of a data item if any site containing a replica of a data item has failed  updates to the data item can not be processed in section 19.6  we describe protocols that can continue transaction processing even if some sites or links have failed  thereby providing high availability 19.5.1 locking protocols the various locking protocols described in chapter 15 can be used in a distributed environment the only change that needs to be incorporated is in the way the lock manager deals with replicated data.we present several possible schemes that are applicable to an environment where data can be replicated in several sites as in chapter 15  we shall assume the existence of the shared and exclusive lock modes 19.5.1.1 single lock-manager approach in the single lock-manager approach  the system maintains a single lock manager that resides in a single chosen site ? say si  all lock and unlock requests are made at site si  when a transaction needs to lock a data item  it sends a lock request to si  the lock manager determines whether the lock can be granted immediately if the lock can be granted  the lockmanager sends a message to that effect to the site at which the lock request was initiated otherwise  the request is delayed until it can be granted  at which time a message is sent to the site at which the lock request was initiated the transaction can read the data item from any one of the sites at which a replica of the data item resides in the case of a write  all the sites where a replica of the data item resides must be involved in the writing the scheme has these advantages  ? simple implementation this scheme requires two messages for handling lock requests and one message for handling unlock requests ? simple deadlock handling since all lock and unlock requests are made at one site  the deadlock-handling algorithms discussed in chapter 15 can be applied directly the disadvantages of the scheme are  ? bottleneck the site si becomes a bottleneck  since all requests must be processed there ? vulnerability if the site si fails  the concurrency controller is lost either processing must stop  or a recovery scheme must be used so that a backup site can take over lock management from si  as described in section 19.6.5 thesumit67.blogspot.com 840 chapter 19 distributed databases 19.5.1.2 distributed lock manager a compromise between the advantages and disadvantages can be achieved through the distributed-lock-manager approach  in which the lock-manager function is distributed over several sites each site maintains a local lock manager whose function is to administer the lock and unlock requests for those data items that are stored in that site when a transaction wishes to lock a data item q that is not replicated and resides at site si  a message is sent to the lock manager at site si requestinga lock  ina particular lock mode   if data item q is locked in an incompatible mode  then the request is delayed until it can be granted once it has determined that the lock request can be granted  the lock manager sends a message back to the initiator indicating that it has granted the lock request we discuss several alternative ways of dealing with replication of data items in sections 19.5.1.3 to 19.5.1.6 the distributed-lock-manager scheme has the advantage of simple implementation  and reduces the degree to which the coordinator is a bottleneck it has a reasonably low overhead  requiring two message transfers for handling lock requests  and one message transfer for handling unlock requests however  deadlock handling is more complex  since the lock and unlock requests are no longer made at a single site  there may be intersite deadlocks even when there is no deadlock within a single site the deadlock-handling algorithms discussed in chapter 15 must be modified  as we shall discuss in section 19.5.4  to detect global deadlocks 19.5.1.3 primary copy when a system uses data replication  we can choose one of the replicas as the primary copy for each data itemq  theprimarycopyofqmust reside in precisely one site  which we call the primary site of q when a transaction needs to lock a data item q  it requests a lock at the primary site of q as before  the response to the request is delayed until it can be granted the primary copy enables concurrency control for replicated data to be handled like that for unreplicated data this similarity allows for a simple implementation however  if the primary site of q fails  q is inaccessible  even though other sites containing a replica may be accessible 19.5.1.4 majority protocol the majority protocol works this way  if data item q is replicated in n different sites  then a lock-request message must be sent to more than one-half of the n sites in which q is stored each lock manager determines whether the lock can be granted immediately  as far as it is concerned   as before  the response is delayed until the request can be granted the transaction does not operate on q until it has successfully obtained a lock on a majority of the replicas of q we assume for nowthatwrites are performed on all replicas  requiring all sites containing replicas to be available however  the major benefit of the majority thesumit67.blogspot.com 19.5 concurrency control in distributed databases 841 protocol is that it can be extended to deal with site failures  as we shall see in section 19.6.1 the protocol also deals with replicated data in a decentralized manner  thus avoiding the drawbacks of central control however  it suffers from these disadvantages  ? implementation the majority protocol is more complicated to implement than are the previous schemes it requires at least 2  n/2 + 1  messages for handling lock requests and at least  n/2 + 1  messages for handling unlock requests ? deadlock handling in addition to the problem of global deadlocks due to the use of a distributed-lock-manager approach  it is possible for a deadlock to occur even if only one data item is being locked.as an illustration  consider a system with four sites and full replication suppose that transactions t1 and t2 wish to lock data item q in exclusive mode transaction t1 may succeed in locking q at sites s1 and s3  while transaction t2 may succeed in locking q at sites s2 and s4 each then must wait to acquire the third lock ; hence  a deadlock has occurred luckily  we can avoid such deadlocks with relative ease  by requiring all sites to request locks on the replicas of a data item in the same predetermined order 19.5.1.5 biased protocol the biased protocol is another approach to handling replication the difference from the majority protocol is that requests for shared locks are given more favorable treatment than requests for exclusive locks ? shared locks.when a transactionneeds to lockdata itemq  it simply requests a lock on q from the lock manager at one site that contains a replica of q ? exclusive locks.when a transaction needs to lock data itemq  it requests a lock on q from the lock manager at all sites that contain a replica of q as before  the response to the request is delayed until it can be granted the biased scheme has the advantage of imposing less overhead on read operations than does the majority protocol this savings is especially significant in common cases in which the frequency of read is much greater than the frequency of write however  the additional overhead on writes is a disadvantage furthermore  the biased protocol shares the majority protocol ? s disadvantage of complexity in handling deadlock 19.5.1.6 quorum consensus protocol the quorum consensus protocol is a generalization of the majority protocol the quorum consensus protocol assigns each site a nonnegative weight it assigns read and write operations on an item x two integers  called read quorum qr and write quorum qw  that must satisfy the following condition  where s is the total weight of all sites at which x resides  thesumit67.blogspot.com 842 chapter 19 distributed databases qr + qw > s and 2 * qw > s to execute a read operation  enough replicas must be locked that their total weight is at least r  to execute a write operation  enough replicas must be locked so that their total weight is at least w a benefit of the quorum consensus approach is that it can permit the cost of either read or write locking to be selectively reduced by appropriately defining the read and write quorums for instance  with a small read quorum  reads need to obtain fewer locks  but the write quorum will be higher  hence writes need to obtain more locks also  if higher weights are given to some sites  for example  those less likely to fail   fewer sites need to be accessed for acquiring locks in fact  by setting weights and quorums appropriately  the quorum consensus protocol can simulate the majority protocol and the biased protocols like the majority protocol  quorum consensus can be extended to work even in the presence of site failures  as we shall see in section 19.6.1 19.5.2 timestamping the principal idea behind the timestamping scheme in section 15.4 is that each transaction is given a unique timestamp that the system uses in deciding the serialization order our first task  then  in generalizing the centralized scheme to a distributed scheme is to develop a scheme for generating unique timestamps then  the various protocols can operate directly to the nonreplicated environment there are two primary methods for generating unique timestamps  one centralized and one distributed in the centralized scheme  a single site distributes the timestamps the site can use a logical counter or its own local clock for this purpose in the distributed scheme  each site generates a unique local timestamp by using either a logical counter or the local clock we obtain the unique global timestamp by concatenating the unique local timestamp with the site identifier  which also must be unique  figure 19.3   the order of concatenation is important ! we use the site identifier in the least significant position to ensure that the global timestamps generated in one site are not always greater than those generated in another site compare this technique for generating unique timestamps with the one that we presented in section 19.2.3 for generating unique names site identifier global unique identifier local unique timestamp figure 19.3 generation of unique timestamps thesumit67.blogspot.com 19.5 concurrency control in distributed databases 843 we may still have a problem if one site generates local timestamps at a rate faster than that of the other sites in such a case  the fast site ? s logical counter will be larger than that of other sites therefore  all timestamps generated by the fast site will be larger than those generated by other sites what we need is a mechanism to ensure that local timestamps are generated fairly across the system we define within each site si a logical clock  lci   which generates the unique local timestamp the logical clock can be implemented as a counter that is incremented after a new local timestamp is generated to ensure that the various logical clocks are synchronized  we require that a site si advance its logical clock whenever a transaction ti with timestamp < x,y > visits that site and x is greater than the current value of lci in this case  site si advances its logical clock to the value x + 1 if the system clock is used to generate timestamps  then timestamps will be assigned fairly  provided that no site has a system clock that runs fast or slow since clocks may not be perfectly accurate  a technique similar to that for logical clocks must be used to ensure that no clock gets far ahead of or behind another clock 19.5.3 replication with weak degrees of consistency many commercial databases today support replication  which can take one of several forms with master ? slave replication  the database allows updates at a primary site  and automatically propagates updates to replicas at other sites transactions may read the replicas at other sites  but are not permitted to update them an important feature of such replication is that transactions do not obtain locks at remote sites to ensure that transactions running at the replica sites see a consistent  but perhaps outdated  view of the database  the replica should reflect a transaction-consistent snapshot of the data at the primary ; that is  the replica should reflect all updates of transactions up to some transaction in the serialization order  and should not reflect any updates of later transactions in the serialization order the database may be configured to propagate updates immediately after they occur at the primary  or to propagate updates only periodically master ? slave replication is particularly useful for distributing information  for instance from a central office to branch offices of an organization another use for this form of replication is in creating a copy of the database to run large queries  so that queries do not interfere with transactions updates should be propagated periodically ? every night  for example ? so that update propagation does not interfere with query processing the oracle database system supports a create snapshot statement  which can create a transaction-consistent snapshot copy of a relation  or set of relations  at a remote site it also supports snapshot refresh  which can be done either by recomputing the snapshot or by incrementally updating it oracle supports automatic refresh  either continuously or at periodic intervals thesumit67.blogspot.com 844 chapter 19 distributed databases with multimaster replication  also called update-anywhere replication  updates are permitted at any replica of a data item  and are automatically propagated to all replicas this model is the basic model used to manage replicas in distributed databases transactions update the local copy and the systemupdates other replicas transparently one way of updating replicas is to apply immediate update with two-phase commit  using one of the distributed concurrency-control techniques we have seen many database systems use the biased protocol  where writes have to lock and update all replicas and reads lock and read any one replica  as their currencycontrol technique many database systems provide an alternative form of updating  they update at one site  with lazy propagation of updates to other sites  instead of immediately applying updates to all replicas as part of the transaction performing the update schemes based on lazy propagation allow transaction processing  including updates  to proceed even if a site is disconnected from the network  thus improving availability  but  unfortunately  do so at the cost of consistency one of two approaches is usually followed when lazy propagation is used  ? updates at replicas are translated into updates at a primary site  which are then propagated lazily to all replicas this approach ensures that updates to an item are ordered serially  although serializability problems can occur  since transactions may read an old value of some other data item and use it to perform an update ? updates are performed at any replica and propagated to all other replicas this approach can cause even more problems  since the same data item may be updated concurrently at multiple sites some conflicts due to the lack of distributed concurrency control can be detected when updates are propagated to other sites  we shall see how in section 25.5.4   but resolving the conflict involves rolling back committed transactions  and durability of committed transactions is therefore not guaranteed further  human intervention may be required to deal with conflicts the above schemes should therefore be avoided or used with care 19.5.4 deadlock handling the deadlock-prevention and deadlock-detection algorithms inchapter 15 can be used in a distributed system  provided that modifications are made for example  we can use the tree protocol by defining a global tree among the system data items similarly  the timestamp-ordering approach could be directly applied to a distributed environment  as we saw in section 19.5.2 deadlock prevention may result in unnecessary waiting and rollback furthermore  certain deadlock-prevention techniques may require more sites to be involved in the execution of a transaction than would otherwise be the case if we allow deadlocks to occur and rely on deadlock detection  the main problem in a distributed system is deciding how to maintain the wait-for graph thesumit67.blogspot.com 19.5 concurrency control in distributed databases 845 t1 t2 t2 t4 t5 t3 t3 site s1 site s2 figure 19.4 local wait-for graphs common techniques for dealing with this issue require that each site keep a local wait-for graph the nodes of the graph correspond to all the transactions  local as well as nonlocal  that are currently either holding or requesting any of the items local to that site for example  figure 19.4 depicts a system consisting of two sites  each maintaining its local wait-for graph note that transactions t2 and t3 appear in both graphs  indicating that the transactions have requested items at both sites these local wait-for graphs are constructed in the usual manner for local transactions and data items when a transaction ti on site s1 needs a resource in site s2  it sends a request message to site s2 if the resource is held by transaction tj  the system inserts an edge ti ? tj in the local wait-for graph of site s2 clearly  if any local wait-for graph has a cycle  deadlock has occurred on the other hand  the fact that there are no cycles in any of the local wait-for graphs does not mean that there are no deadlocks to illustrate this problem  we consider the local wait-for graphs of figure 19.4 each wait-for graph is acyclic ; nevertheless  a deadlock exists in the system because the union of the local wait-for graphs contains a cycle this graph appears in figure 19.5 in the centralized deadlock detection approach  the system constructs and maintains a global wait-for graph  the union of all the local graphs  in a single site  the deadlock-detection coordinator since there is communication delay in the system  we must distinguish between two types of wait-for graphs the real graph describes the real but unknown state of the system at any instance in time  as would be seen by an omniscient observer the constructed graph is an t1 t4 t5 t2 t3 figure 19.5 global wait-for graph for figure 19.4 thesumit67.blogspot.com 846 chapter 19 distributed databases t1 t2 t2 t1 t3 s2 t1 t3 coordinator s1 figure 19.6 false cycles in the global wait-for graph approximation generated by the controller during the execution of the controller ? s algorithm obviously  the controller must generate the constructed graph in such a way that  whenever the detection algorithm is invoked  the reported results are correct correct means in this case that  if a deadlock exists  it is reported promptly  and if the system reports a deadlock  it is indeed in a deadlock state the global wait-for graph can be reconstructed or updated under these conditions  ? whenever a new edge is inserted in or removed from one of the localwait-for graphs ? periodically  when a number of changes have occurred in a local wait-for graph ? whenever the coordinator needs to invoke the cycle-detection algorithm when the coordinator invokes the deadlock-detection algorithm  it searches its global graph if it finds a cycle  it selects a victim to be rolled back the coordinator must notify all the sites that a particular transaction has been selected as victim the sites  in turn  roll back the victim transaction this scheme may produce unnecessary rollbacks if  ? false cycles exist in the global wait-for graph as an illustration  consider a snapshot of the system represented by the localwait-for graphs of figure 19.6 suppose that t2 releases the resource that it is holding in site s1  resulting in the deletion of the edge t1 ? t2 in s1 transaction t2 then requests a thesumit67.blogspot.com 19.6 availability 847 resource held by t3 at site s2  resulting in the addition of the edge t2 ? t3 in s2 if the insert t2 ? t3 message from s2 arrives before the remove t1 ? t2 message from s1  the coordinatormay discover the false cycle t1 ? t2 ? t3 after the insert  but before the remove   deadlock recovery may be initiated  although no deadlock has occurred note that the false-cycle situation could not occur under two-phase locking the likelihood of false cycles is usually sufficiently low that they do not cause a serious performance problem ? a deadlock has indeed occurred and a victim has been picked,while one of the transactions was aborted for reasons unrelated to the deadlock for example  suppose that site s1 in figure 19.4 decides to abort t2 at the same time  the coordinator has discovered a cycle  and has picked t3 as a victim both t2 and t3 are now rolled back  although only t2 needed to be rolled back deadlock detection can be done in a distributed manner  with several sites taking on parts of the task  instead of it being done at a single site however  such algorithms are more complicated and more expensive see the bibliographical notes for references to such algorithms 19.6 availability one of the goals in using distributed databases is high availability ; that is  the database must function almost all the time in particular  since failures are more likely in large distributed systems  a distributed database must continue functioning even when there are various types of failures the ability to continue functioning even during failures is referred to as robustness for a distributed system to be robust  it must detect failures  reconfigure the system so that computation may continue  and recover when a processor or a link is repaired the different types of failures are handled in different ways for example  message loss is handled by retransmission repeated retransmission of a message across a link  without receipt of an acknowledgment  is usually a symptom of a link failure the network usually attempts to find an alternative route for the message failure to find such a route is usually a symptom of network partition it is generally not possible  however  to differentiate clearly between site failure and network partition the system can usually detect that a failure has occurred  but it may not be able to identify the type of failure for example  suppose that site s1 is not able to communicate with s2 it could be that s2 has failed however  another possibility is that the link between s1 and s2 has failed  resulting in network partition the problem is partly addressed by using multiple links between sites  so that even if one link fails the sites will remain connected however  multiple link failure can still occur  so there are situations where we can not be sure whether a site failure or network partition has occurred thesumit67.blogspot.com 848 chapter 19 distributed databases suppose that site s1 has discovered that a failure has occurred it must then initiate a procedure that will allow the system to reconfigure  and to continue with the normal mode of operation ? if transactions were active at a failed/inaccessible site at the time of the failure  these transactions should be aborted it is desirable to abort such transactions promptly  since they may hold locks on data at sites that are still active ; waiting for the failed/inaccessible site to become accessible again may impede other transactions at sites that are operational however  in some cases  when data objects are replicated it may be possible to proceed with reads and updates even though some replicas are inaccessible in this case  when a failed site recovers  if it had replicas of any data object  it must obtain the current values of these data objects  and must ensure that it receives all future updates.we address this issue in section 19.6.1 ? if replicated data are stored at a failed/inaccessible site  the catalog should be updated so that queries do not reference the copy at the failed site when a site rejoins  care must be taken to ensure that data at the site are consistent  as we shall see in section 19.6.3 ? if a failed site is a central server for some subsystem  an election must be held to determine the new server  see section 19.6.5   examples of central servers include a name server  a concurrency coordinator  or a global deadlock detector since it is  in general  not possible to distinguish between network link failures and site failures  any reconfiguration scheme must be designed to work correctly in case of a partitioning of the network in particular  these situations must be avoided to ensure consistency  ? two or more central servers are elected in distinct partitions ? more than one partition updates a replicated data item although traditional database systems place a premium on consistency  there are many applications today that value availability more than consistency the design of replication protocols is different for such systems  and is discussed in section 19.6.6 19.6.1 majority-based approach the majority-based approach to distributedconcurrency control in section 19.5.1.4 can be modified to work in spite of failures in this approach  each data object stores with it a version number to detect when it was last written whenever a transaction writes an object it also updates the version number in this way  ? if data object a is replicated in n different sites  then a lock-request message must be sent to more than one-half of the n sites at which a is stored the thesumit67.blogspot.com 19.6 availability 849 transaction does not operate on a until it has successfully obtained a lock on a majority of the replicas of a ? read operations look at all replicas on which a lock has been obtained  and read the value from the replica that has the highest version number  optionally  they may also write this value back to replicas with lower version numbers  writes read all the replicas just like reads to find the highest version number  this step would normally have been performed earlier in the transaction by a read  and the result can be reused   the new version number is one more than the highest version number the write operation writes all the replicas on which it has obtained locks  and sets the version number at all the replicas to the new version number failures during a transaction  whether network partitions or site failures  can be tolerated as long as  1  the sites available at commit contain a majority of replicas of all the objects written to and  2  during reads  a majority of replicas are read to find the version numbers if these requirements are violated  the transaction must be aborted as long as the requirements are satisfied  the two-phase commit protocol can be used  as usual  on the sites that are available in this scheme  reintegration is trivial ; nothing needs to be done this is because writes would have updated a majority of the replicas  while reads will read a majority of the replicas and find at least one replica that has the latest version the version numbering technique usedwith the majority protocol can also be used to make the quorum consensus protocol work in the presence of failures.we leave the  straightforward  details to the reader however  the danger of failures preventing the system from processing transactions increases if some sites are given higher weights 19.6.2 read one  write all available approach as a special case of quorum consensus  we can employ the biased protocol by giving unit weights to all sites  setting the read quorum to 1  and setting the write quorum to n  all sites   in this special case  there is no need to use version numbers ; however  if even a single site containing a data item fails  no write to the item can proceed  since the write quorum will not be available this protocol is called the read one  write all protocol since all replicas must be written to allow work to proceed in the event of failures  we would like to be able to use a read one  write all available protocol in this approach  a read operation proceeds as in the read one  write all scheme ; any available replica can be read  and a read lock is obtained at that replica a write operation is shipped to all replicas ; and write locks are acquired on all the replicas if a site is down  the transaction manager proceeds without waiting for the site to recover while this approach appears very attractive  there are several complications in particular  temporary communication failure may cause a site to appear to be unavailable  resulting in a write not being performed  but when the link is restored  the site is not aware that it has to perform some reintegration actions to thesumit67.blogspot.com 850 chapter 19 distributed databases catch up onwrites it has lost further  if the network partitions  each partition may proceed to update the same data item  believing that sites in the other partitions are all dead the read one  write all available scheme can be used if there is never any network partitioning  but it can result in inconsistencies in the event of network partitions 19.6.3 site reintegration reintegration of a repaired site or link into the system requires care when a failed site recovers  it must initiate a procedure to update its system tables to reflect changesmade while it was down if the site had replicas of any data items  it must obtain the current values of these data items and ensure that it receives all future updates reintegration of a site is more complicated than it may seem to be at first glance  since there may be updates to the data items processed during the time that the site is recovering an easy solution is to halt the entire system temporarily while the failed site rejoins it in most applications  however  such a temporary halt is unacceptably disruptive techniques have been developed to allow failed sites to reintegrate while concurrent updates to data items proceed concurrently before a read or write lock is granted on any data item  the site must ensure that it has caught up on all updates to the data item if a failed link recovers  two or more partitions can be rejoined since a partitioning of the network limits the allowable operations by some or all sites  all sites should be informed promptly of the recovery of the link see the bibliographical notes for more information on recovery in distributed systems 19.6.4 comparison with remote backup remote backup systems  which we studied in section 16.9  and replication in distributed databases are two alternative approaches to providing high availability the main difference between the two schemes is that with remote backup systems  actions such as concurrency control and recovery are performed at a single site  and only data and log records are replicated at the other site in particular  remote backup systems help avoid two-phase commit  and its resultant overheads also  transactions need to contact only one site  the primary site   and thus avoid the overhead of running transaction code at multiple sites thus remote backup systems offer a lower-cost approach to high availability than replication on the other hand  replication can provide greater availability by having multiple replicas available and using the majority protocol 19.6.5 coordinator selection several of the algorithms that we have presented require the use of a coordinator if the coordinator fails because of a failure of the site atwhich it resides  the system can continue execution only by restarting a new coordinator on another site one thesumit67.blogspot.com 19.6 availability 851 way to continue execution is by maintaining a backup to the coordinator  which is ready to assume responsibility if the coordinator fails a backup coordinator is a site that  in addition to other tasks  maintains enough information locally to allow it to assume the role of coordinator with minimal disruption to the distributed system all messages directed to the coordinator are received by both the coordinator and its backup the backup coordinator executes the same algorithms and maintains the same internal state information  such as  for a concurrency coordinator  the lock table  as does the actual coordinator the only difference in function between the coordinator and its backup is that the backup does not take any action that affects other sites such actions are left to the actual coordinator in the event that the backup coordinator detects the failure of the actual coordinator  it assumes the role of coordinator since the backup has all the information available to it that the failed coordinator had  processing can continue without interruption the prime advantage to the backup approach is the ability to continue processing immediately if a backup were not ready to assume the coordinator ? s responsibility  a newly appointed coordinatorwould have to seek information from all sites in the system so that it could execute the coordination tasks frequently  the only source of some of the requisite information is the failed coordinator in this case  it may be necessary to abort several  or all  active transactions  and to restart them under the control of the new coordinator thus  the backup-coordinator approach avoids a substantial amount of delay while the distributed system recovers from a coordinator failure the disadvantage is the overhead of duplicate execution of the coordinator ? s tasks furthermore  a coordinator and its backup need to communicate regularly to ensure that their activities are synchronized in short  the backup-coordinator approach incurs overhead during normal processing to allow fast recovery from a coordinator failure in the absence of a designated backup coordinator  or in order to handle multiple failures  a new coordinator may be chosen dynamically by sites that are live election algorithms enable the sites to choose the site for the newcoordinator in a decentralizedmanner election algorithms require that a unique identification number be associated with each active site in the system the bully algorithm for election works as follows  to keep the notation and the discussion simple  assume that the identification number of site si is i and that the chosen coordinator will always be the active site with the largest identification number hence  when a coordinator fails  the algorithm must elect the active site that has the largest identification number the algorithm must send this number to each active site in the system in addition  the algorithm must provide a mechanism by which a site recovering from a crash can identify the current coordinator suppose that site si sends a request that is not answered by the coordinator within a prespecified time interval t in this situation  it is assumed that the coordinator has failed  and si tries to elect itself as the site for the new coordinator thesumit67.blogspot.com 852 chapter 19 distributed databases site si sends an election message to every site that has a higher identification number site si then waits  for a time interval t  for an answer from any one of these sites if it receives no response within time t  it assumes that all sites with numbers greater than i have failed  and it elects itself as the site for the newcoordinator and sends a message to inform all active sites with identification numbers lower than i that it is the site at which the new coordinator resides if si does receive an answer  it begins a time interval t   to receive a message informing it that a site with a higher identification number has been elected  some other site is electing itself coordinator  and should report the resultswithin time t    if si receives no message within t   then it assumes the site with a higher number has failed  and site si restarts the algorithm after a failed site recovers  it immediately begins execution of the same algorithm if there are no active sites with higher numbers  the recovered site forces all sites with lower numbers to let it become the coordinator site  even if there is a currently active coordinator with a lower number it is for this reason that the algorithm is termed the bully algorithm if the network partitions  the bully algorithm elects a separate coordinator in each partition ; to ensure that at most one coordinator is elected  winning sites should additionally verify that a majority of the sites are in their partition 19.6.6 trading off consistency for availability the protocols we have seen so far require a  weighted  majority of sites be in a partition for updates to proceed sites that are in a minority partition can not process updates ; if a network failure results in more than two partitions  no partition may have a majority of sites under such a situation  the system would be completely unavailable for updates  and depending on the read-quorum  may even become unavailable for reads the write-all-available protocol which we saw earlier provides availability  but not consistency ideally  we would like to have consistency and availability  even in the face of partitions unfortunately  this is not possible  a fact that is crystallized in the so-called cap theorem  which states that any distributed database can have at most two of the following three properties  ? consistency ? availability ? partition-tolerance the proof of the cap theorem uses the following definition of consistency  with replicated data  an execution of a set of operations  reads andwrites  on replicated data is said to be consistent if its result is the same as if the operations were executed on a single site  in some sequential order  and the sequential order is consistent with the ordering of operations issued by each process  transaction   thesumit67.blogspot.com 19.6 availability 853 the notion of consistency is similar to atomicity of transactions  but with each operation treated as a transaction  and is weaker than the atomicity property of transactions in any large-scale distributed system  partitions can not be prevented  and as a result either of availability or consistency has to be sacrificed the schemes we have seen earlier sacrifice availability for consistency in the face of partitions consider a web-based social-networking system that replicates its data on three servers  and a network partition occurs that prevents the servers from communicating with each other since none of the partitions has a majority  it would not be possible to execute updates on any of the partitions if one of these servers is in the same partition as a user  the user actually has access to data  but would be unable to update the data  since another user may be concurrently updating the same object in another partition  which could potentially lead to inconsistency inconsistency is not as great a risk in a social-networking system as in a banking database a designer of such a system may decide that a user who can access the system should be allowed to perform updates on whatever replicas are accessible  even at the risk of inconsistency in contrast to systems such as banking databases that require the acid properties  systems such as the social-networking system mentioned above are said to require the base properties  ? basically available ? soft state ? eventually consistent the primary requirement is availability  even at the cost of consistency updates should be allowed  even in the event of partitioning  following for example the write-all-available protocol  which is similar to multimaster replication described in section 19.5.3   soft state refers to the property that the state of the database may not be precisely defined  with each replica possibly having a somewhat different state due to partitioning of the network eventually consistent is the requirement that once a partitioning is resolved  eventually all replicas will become consistent with each other this last step requires that inconsistent copies of data items be identified ; if one is an earlier version of the other  the earlier version can be replaced by the later version it is possible  however  that the two copieswere the result of independent updates to acommonbase copy.aschemefor detecting such inconsistentupdates  called the version-vector scheme  is described in section 25.5.4 restoring consistency in the face of inconsistent updates requires that the updates be merged in some way that ismeaningful to the application this step can not be handled by the database ; instead the database detects and informs the application about the inconsistency  and the application then decides how to resolve the inconsistency thesumit67.blogspot.com 854 chapter 19 distributed databases 19.7 distributed query processing in chapter 13  we saw that there are a variety of methods for computing the answer to a query we examined several techniques for choosing a strategy for processing a query that minimize the amount of time that it takes to compute the answer for centralized systems  the primary criterion for measuring the cost of a particular strategy is the number of disk accesses in a distributed system  we must take into account several other matters  including  ? the cost of data transmission over the network ? the potential gain in performance from having several sites process parts of the query in parallel the relative cost of data transfer over the network and data transfer to and from disk varies widely depending on the type of network and on the speed of the disks thus  in general  we can not focus solely on disk costs or on network costs rather  we must find a good trade-off between the two 19.7.1 query transformation consider an extremely simple query  ? find all the tuples in the account relation ? although the query is simple ? indeed  trivial ? processing it is not trivial  since the account relation may be fragmented  replicated  or both  as we saw in section 19.2 if the account relation is replicated  we have a choice of replica to make if no replicas are fragmented  we choose the replica for which the transmission cost is lowest however  if a replica is fragmented  the choice is not so easy to make  since we need to compute several joins or unions to reconstruct the account relation in this case  the number of strategies for our simple example may be large query optimization by exhaustive enumeration of all alternative strategies may not be practical in such situations fragmentation transparency implies that a user may write a query such as   branch name = ? hillside ?  account  since account is defined as  account1 ? account2 the expression that results from the name translation scheme is   branch name = ? hillside ?  account1 ? account2  using the query-optimization techniques of chapter 13  we can simplify the preceding expression automatically the result is the expression  thesumit67.blogspot.com 19.7 distributed query processing 855  branch name = ? hillside ?  account1  ?  branch name = ? hillside ?  account2  which includes two subexpressions the first involves only account1  and thus can be evaluated at the hillside site the second involves only account2  and thus can be evaluated at the valleyview site there is a further optimization that can be made in evaluating   branch name = ? hillside ?  account1  since account1 has only tuples pertaining to the hillside branch  we can eliminate the selection operation in evaluating   branch name = ? hillside ?  account2  we can apply the definition of the account2 fragment to obtain   branch name = ? hillside ?   branch name = ? valleyview ?  account   this expression is the empty set  regardless of the contents of the account relation thus  our final strategy is for the hillside site to return account1 as the result of the query 19.7.2 simple join processing as we saw in chapter 13  a major decision in the selection of a query-processing strategy is choosing a join strategy consider the following relational-algebra expression  account  depositor  branch assume that the three relations are neither replicated nor fragmented  and that account is stored at site s1  depositor at s2  and branch at s3 let si denote the site at which the query was issued the system needs to produce the result at site si  among the possible strategies for processing this query are these  ? ship copies of all three relations to site si  using the techniques of chapter 13  choose a strategy for processing the entire query locally at site si  ? ship a copy of the account relation to site s2  and compute temp1 = account  depositor at s2 ship temp1 from s2 to s3  and compute temp2 = temp1  branch at s3 ship the result temp2 to si  ? devise strategies similar to the previous one  with the roles of s1  s2  s3 exchanged no one strategy is always the best one among the factors that must be considered are the volume of data being shipped  the cost of transmitting a block thesumit67.blogspot.com 856 chapter 19 distributed databases of data between a pair of sites  and the relative speed of processing at each site consider the first two strategies listed suppose indices present at s2 and s3 are useful for computing the join if we ship all three relations to si ,wewould need to either re-create these indices at si  or use a different  possibly more expensive  join strategy re-creation of indices entails extra processing overhead and extra disk accesses.with the second strategy a potentially large relation  account  depositor  must be shipped from s2 to s3 this relation repeats the name of a customer once for each account that the customer has thus  the second strategy may result in extra network transmission compared to the first strategy 19.7.3 semijoin strategy suppose that we wish to evaluate the expression r1  r2  where r1 and r2 are stored at sites s1 and s2  respectively let the schemas of r1 and r2 be r1 and r2 suppose that we wish to obtain the result at s1 if there are many tuples of r2 that do not join with any tuple of r1  then shipping r2 to s1 entails shipping tuples that fail to contribute to the result we want to remove such tuples before shipping data to s1  particularly if network costs are high a possible strategy to accomplish all this is  1 compute temp1 ?  r1 n r2  r1  at s1 2 ship temp1 from s1 to s2 3 compute temp2 ? r2  temp1 at s2 4 ship temp2 from s2 to s1 5 compute r1  temp2 at s1 the resulting relation is the same as r1  r2 before considering the efficiency of this strategy  let us verify that the strategy computes the correct answer in step 3  temp2 has the result of r2   r1 n r2  r1   in step 5  we compute  r1  r2   r1 n r2  r1  since join is associative and commutative  we can rewrite this expression as   r1   r1 n r2  r1    r2 since r1    r1 n r2   r1  = r1  the expression is  indeed  equal to r1  r2  the expression we are trying to evaluate this strategy is particularly advantageous when relatively few tuples of r2 contribute to the join this situation is likely to occur if r1 is the result of a relational-algebra expression involving selection in such a case  temp2 may have significantly fewer tuples than r2 the cost savings of the strategy result from having to ship only temp2  rather than all of r2  to s1 additional cost is incurred in shipping temp1 to s2 if a sufficiently small fraction of tuples in r2 contribute thesumit67.blogspot.com 19.8 heterogeneous distributed databases 857 to the join  the overhead of shipping temp1 will be dominated by the savings of shipping only a fraction of the tuples in r2 this strategy is called a semijoin strategy  after the semijoin operator of the relational algebra  denoted   the semijoin of r1 with r2  denoted r1  r2  is   r1  r1  r2  thus  r1  r2 selects those tuples of relation r1 that contributed to r1  r2 in step 3  temp2 = r2  r1 for joins of several relations  this strategy can be extended to a series of semijoin steps a substantial body of theory has been developed regarding the use of semijoins for query optimization some of this theory is referenced in the bibliographical notes 19.7.4 join strategies that exploit parallelism consider a join of four relations  r1  r2  r3  r4 where relation ri is stored at site si  assume that the result must be presented at site s1 there are many possible strategies for parallel evaluation  we studied the issue of parallel processing of queries in detail in chapter 18  in one such strategy  r1 is shipped to s2  and r1  r2 computed at s2 at the same time  r3 is shipped to s4  and r3  r4 computed at s4 site s2 can ship tuples of  r1  r2  to s1 as they are produced  rather than wait for the entire join to be computed similarly  s4 can ship tuples of  r3  r4  to s1 once tuples of  r1  r2  and  r3  r4  arrive at s1  the computation of  r1  r2    r3  r4  can begin  with the pipelined join technique of section 12.7.2.2 thus  computation of the final join result at s1 can be done in parallel with the computation of  r1  r2  at s2  and with the computation of  r3  r4  at s4 19.8 heterogeneous distributed databases many new database applications require data from a variety of preexisting databases located in a heterogeneous collection of hardware and software environments manipulation of information located in a heterogeneous distributed database requires an additional software layer on top of existing database systems this software layer is called a multidatabase system the local database systems may employ different logical models and data-definition and datamanipulation languages  and may differ in their concurrency-control and transaction management mechanisms a multidatabase system creates the illusion of logical database integration without requiring physical database integration full integration of heterogeneous systems into a homogeneous distributed database is often difficult or impossible  thesumit67.blogspot.com 858 chapter 19 distributed databases ? technical difficulties the investment in application programs based on existing database systems may be huge  and the cost of converting these applications may be prohibitive ? organizational difficulties even if integration is technically possible  it may not be politically possible  because the existing database systems belong to different corporations or organizations in such cases  it is important for a multidatabase system to allow the local database systems to retain a high degree of autonomy over the local database and transactions running against that data for these reasons  multidatabase systems offer significant advantages that outweigh their overhead in this section,we provide an overview of the challenges faced in constructing a multidatabase environment from the standpoint of data definition and query processing 19.8.1 unified view of data each local database management system may use a different data model for instance  some may employ the relational model  whereas others may employ older data models  such as the networkmodel  seeappendixd  or thehierarchical model  see appendix e   since the multidatabase system is supposed to provide the illusion of a single  integrated database system  a common data model must be used a commonly used choice is the relational model  with sql as the common query language indeed  there are several systems available today that allow sql queries to a nonrelational database-management system another difficulty is the provision of a common conceptual schema each local system provides its ownconceptual schema the multidatabase system must integrate these separate schemas into one common schema schema integration is a complicated task  mainly because of the semantic heterogeneity schema integration is not simply straightforward translation between datadefinition languages the same attribute names may appear in different local databases but with different meanings the data types used in one systemmay not be supported by other systems  and translation between types may not be simple even for identical data types  problemsmayarise fromthe physical representation of data  one system may use 8-bit ascii  another 16-bit unicode  and yet another ebcdic ; floating-point representations may differ ; integers may be represented in big-endian or little-endian form at the semantic level  an integer value for length may be inches in one system and millimeters in another  thus creating an awkward situation in which equality of integers is only an approximate notion  as is always the case for floating-point numbers   the same name may appear in different languages in different systems for example  a system based in the united states may refer to the city ? cologne  ? whereas one in germany refers to it as ? k ? oln ? all these seemingly minor distinctions must be properly recorded in the common global conceptual schema translation functions must be provided indices thesumit67.blogspot.com 19.8 heterogeneous distributed databases 859 must be annotated for system-dependent behavior  for example  the sort order of nonalphanumeric characters is not the same in ascii as in ebcdic   as we noted earlier  the alternative of converting each database to a common format may not be feasible without obsoleting existing application programs 19.8.2 query processing query processing in a heterogeneous database can be complicated some of the issues are  ? given a query on a global schema  the query may have to be translated into queries on local schemas at each of the sites where the query has to be executed the query results have to be translated back into the global schema the task is simplified by writing wrappers for each data source  which provide a view of the local data in the global schema.wrappers also translate queries on the global schema into queries on the local schema  and translate results back into the global schema.wrappers may be provided by individual sites  or may be written separately as part of the multidatabase system wrappers can even be used to provide a relational view of nonrelational data sources  such as web pages  possibly with forms interfaces   flat files  hierarchical and network databases  and directory systems ? some data sources may provide only limited query capabilities ; for instance  they may support selections  but not joins theymay even restrict the form of selections  allowing selections only on certain fields ; web data sources with form interfaces are an example of such data sources queries may therefore have to be broken up  to be partly performed at the data source and partly at the site issuing the query ? in general  more than one site may need to be accessed to answer a given query answers retrieved from the sites may have to be processed to remove duplicates suppose one site contains account tuples satisfying the selection balance < 100  while another contains account tuples satisfying balance > 50 a query on the entire account relation would require access to both sites and removal of duplicate answers resulting from tuples with balance between 50 and 100  which are replicated at both sites ? global query optimization in a heterogeneous database is difficult  since the query execution system may not know what the costs are of alternative query plans at different sites the usual solution is to rely on only local-level optimization  and just use heuristics at the global level mediator systems are systems that integrate multiple heterogeneous data sources  providing an integrated global view of the data and providing query facilities on the global view unlike full-fledged multidatabase systems,mediator systems do not bother about transaction processing  the terms mediator and multidatabase are often used in an interchangeable fashion  and systems that are called mediators may support limited forms of transactions  the term virtual thesumit67.blogspot.com 860 chapter 19 distributed databases database is used to refer to multidatabase/mediator systems  since they provide the appearance of a single database with a global schema  although data exist on multiple sites in local schemas 19.8.3 transaction management in multidatabases a multidatabase system supports two types of transactions  1 local transactions these transactions are executed by each local database system outside of the multidatabase system ? s control 2 global transactions these transactions are executed under the multidatabase system ? s control the multidatabase system is aware of the fact that local transactions may run at the local sites  but it is not aware of what specific transactions are being executed  or of what data they may access ensuring the local autonomy of each database system requires that no changes be made to its software a database system at one site thus is not able to communicate directly with one at any other site to synchronize the execution of a global transaction active at several sites since the multidatabase system has no control over the execution of local transactions  each local system must use a concurrency-control scheme  for example  two-phase locking or timestamping  to ensure that its schedule is serializable in addition  in case of locking  the local system must be able to guard against the possibility of local deadlocks the guarantee of local serializability is not sufficient to ensure global serializability as an illustration  consider two global transactions t1 and t2  each of which accesses and updates two data items  a and b  located at sites s1 and s2  respectively suppose that the local schedules are serializable it is still possible to have a situation where  at site s1  t2 follows t1  whereas  at s2  t1 follows t2  resulting in a nonserializable global schedule indeed  even if there is no concurrency among global transactions  that is  a global transaction is submitted only after the previous one commits or aborts   local serializability is not sufficient to ensure global serializability  see practice exercise 19.14   depending on the implementation of the local database systems  a global transaction may not be able to control the precise locking behavior of its local subtransactions thus  even if all local database systems follow two-phase locking  it may be possible only to ensure that each local transaction follows the rules of the protocol for example  one local database system may commit its subtransaction and release locks  while the subtransaction at another local system is still executing if the local systems permit control of locking behavior and all systems follow two-phase locking  then the multidatabase system can ensure that global transactions lock in a two-phase manner and the lock points of conflicting transactions would then define their global serialization order if different local systems follow different concurrency-control mechanisms  however  this straightforward sort of global control does not work thesumit67.blogspot.com 19.9 cloud-based databases 861 there are many protocols for ensuring consistency despite concurrent execution of global and local transactions in multidatabase systems some are based on imposing sufficient conditions to ensure global serializability others ensure only a form of consistency weaker than serializability  but achieve this consistency by less restrictive means section 26.6 describes approaches to consistency without serializability ; other approaches are cited in the bibliographical notes early multidatabase systems restricted global transactions to be read only they thus avoided the possibility of global transactions introducing inconsistency to the data  but were not sufficiently restrictive to ensure global serializability it is indeed possible to get such global schedules and to develop a scheme to ensure global serializability  and we ask you to do both in practice exercise 19.15 there are a number of general schemes to ensure global serializability in an environment where update as well as read-only transactions can execute several of these schemes are based on the idea of a ticket a special data item called a ticket is created in each local database system every global transaction that accesses data at a site must write the ticket at that site this requirement ensures that global transactions conflict directly at every site they visit furthermore  the global transaction manager can control the order in which global transactions are serialized  by controlling the order in which the tickets are accessed references to such schemes appear in the bibliographical notes if we want to ensure global serializability in an environment where no direct local conflicts are generated in each site  some assumptions must be made about the schedules allowed by the local database system for example  if the local schedules are such that the commit order and serialization order are always identical  we can ensure serializability by controlling only the order in which transactions commit a related problem in multidatabase systems is that of global atomic commit if all local systems follow the two-phase commit protocol  that protocol can be used to achieve global atomicity however  local systems not designed to be part of a distributed system may not be able to participate in such a protocol even if a local system is capable of supporting two-phase commit  the organization owning the system may be unwilling to permitwaiting in caseswhere blocking occurs in such cases  compromises may be made that allow for lack of atomicity in certain failure modes further discussion of these matters appears in the literature  see the bibliographical notes   19.9 cloud-based databases cloud computing is a relatively new concept in computing that emerged in the late 1990s and the 2000s  first under the name software as a service initial vendors of software services provided specific customizable applications that they hosted on their own machines the concept of cloud computing developed as vendors began to offer generic computers as a service on which clients could run software applications of their choosing a client can make arrangements with a cloud-computing vendor to obtain a certain number of machines of a thesumit67.blogspot.com 862 chapter 19 distributed databases certain capacity as well as a certain amount of data storage both the number of machines and the amount of storage can grow and shrink as needed in addition to providing computing services  many vendors also provide other services such as data storage services  map services  and other services that can be accessed using a web-service application programming interface many enterprises are finding the model of cloud computing and services beneficial it saves client enterprises the need to maintain a large system-support staff and allows new enterprises to begin operation without having to make a large  up-front capital investment in computing systems further  as the needs of the enterprise grow  more resources  computing and storage  can be added as required ; the cloud-computing vendor generally has very large clusters of computers  making it easy for the vendor to allocate resources on demand a variety of vendors offer cloud services they include traditional computing vendors as well as companies  such as amazon and google  that are seeking to leverage the large infrastructure they have in place for their core businesses web applications that need to store and retrieve data for very large numbers of users  ranging frommillions to hundreds of millions  have been a major driver of cloud-based databases the needs of these applications differ from those of traditional database applications  since they value availability and scalability over consistency several cloud-based data-storage systems have been developed in recent years to serve the needs of such applications.we discuss issues in building such data-storage systems on the cloud in section 19.9.1 in section 19.9.2  we consider issues in running traditional database systems on a cloud cloud-based databases have features of both homogeneous and heterogeneous systems although the data are owned by one organization  the client  and are part of one unified distributed database  the underlying computers are owned and operated by another organization  the service vendor   the computers are remote from the client ? s location  s  and are accessed over the internet as a result  some of the challenges of heterogeneous distributed systems remain  particularly as regards transaction processing however  many of the organizational and political challenges of heterogeneous systems are avoided finally  in section 19.9.3  we discuss several technical as well as nontechnical challenges that cloud databases face today 19.9.1 data storage systems on the cloud applications on the web have extremely high scalability requirements popular applications have hundreds of millions of users  and many applications have seen their load increasemanyfoldwithin a single year  or evenwithin a few months to handle the data management needs of such applications  data must be partitioned across thousands of processors a number of systems for data storage on the cloud have been developed and deployed over the past few years to address data management requirements of such applications ; these include bigtable from google  simple storage service  s3  from amazon  which provides a web interface to dynamo  which is a keyvalue storage system  cassandra  fromfacebook  which is similar to bigtable  and thesumit67.blogspot.com 19.9 cloud-based databases 863 sherpa/pnuts from yahoo !  the data storage component of the azure environment from microsoft  and several other systems in this section  we provide an overview of the architecture of such datastorage systems although some people refer to these systems as distributed database systems  they do not provide many of the features which are viewed as standard on database systems today  such as support for sql  or for transactions with the acid properties 19.9.1.1 data representation as an example of data management needs ofweb applications  consider the profile of a user,which needs to be accessible to a number of different applications that are run by an organization the profile contains a variety of attributes  and there are frequent additions to the attributes stored in the profile some attributes may contain complex data a simple relational representation is often not sufficient for such complex data some cloud-based data-storage systems support xml  described in chapter 23  for representing such complex data others support the javascript object notation  json  representation  which has found increasing acceptance for representing complex data the xml and json representations provide flexibility in the set of attributes that a record contains  as well as the types of these attributes yet others  such as bigtable  define their own data model for complex data including support for records with a very large number of optional columns.we revisit the bigtable data model later in this section further  many such web applications either do not need extensive query language support  or at least  can manage without such support the primary mode of data access is to store data with an associated key  and to retrieve data with that key in the above user profile example  the key for user-profile data would be the user ? s identifier there are applications that conceptually require joins  but implement the joins by a form of view materialization for example  in a social-networking application  each user should be shown new posts from all her friends unfortunately  finding the set of friends and then querying each one to find their posts may lead to a significant amount of delay when the data are distributed across a large number of machines an alternative is as follows  whenever a user makes a post  a message is sent to all friends of that user  and the data associated with each of the friends is updated with a summary of the new post when that user checks for updates  all required data are available in one place and can be retrieved quickly thus  cloud data-storage systems are  at their core  based on two primitive functions  put  key  value   used to store values with an associated key  and get  key   which retrieves the stored value associated with the specified key some systems such as bigtable additionally provide range queries on key values in bigtable  a record is not stored as a single value  but is instead split into component attributes that are stored separately thus  the key for an attribute value conceptually consists of  record-identifier  attribute-name   each attribute value is just a string as far as bigtable is concerned to fetch all attributes of a thesumit67.blogspot.com 864 chapter 19 distributed databases json javascript object notation  or json  is a textual representation of complex data types which is widely used for transmitting data between applications  as well as to store complex data json supports the primitive data types integer  real and string  aswell as arrays  and ? objects ? ,which are a collection of  attribute-name  value  pairs an example of a json object is   " id "  " 22222 "  " name "   " firstname  " albert "  " lastname  " einstein "   " deptname "  " physics "  " children "    " firstname "  " hans "  " lastname "  " einstein "    " firstname "  " eduard "  " lastname "  " einstein "    the above example illustrates objects  which contain  attribute-name  value  pairs  as well as arrays  delimited by square brackets json can be viewed as a simplified form of xml ; xml is covered in chapter 23 libraries have been developed to transformdata between the json representation and the object representation used in the javascript and php scripting languages  as well as other programming languages record  a range query  ormore precisely a prefix-match query consisting of just the record identifier  is used the get   function returns the attribute names alongwith the values for efficient retrieval of all attributes of a record  the storage system stores entries sorted by the key  so all attribute values of a particular record are clustered together in fact  the record identifier can itself be structured hierarchically  although to bigtable itself the record identifier is just a string for example  an application that stores pages retrieved from a web crawl could map a url of the form  www.cs.yale.edu/people/silberschatz.html to the record identifier  edu.yale.cs.www/people/silberschatz.html so that pages are clustered in a useful order.as another example  the record shown thesumit67.blogspot.com 19.9 cloud-based databases 865 in the json example  see example box on json  can be represented by a record with identifier ? 22222 ?  with multiple attribute names such as ? name.firstname ?  ? deptname ?  ? children  1  .firstname ? or ? children  2  .lastname ?  further  a single instance of bigtable can store data for multiple applications  with multiple tables per application  by simply prefixing the application name and table name to the record identifier data-storage systems typically allow multiple versions of data items to be stored versions are often identified by timestamp  butmay be alternatively identified by an integer value that is incremented whenever a new version of a data item is created lookups can specify the required version of a data item  or can pick the version with the highest version number in bigtable  for example  a key actually consists of three parts   record-identifier  attribute-name  timestamp   19.9.1.2 partitioning and retrieving data partitioning of data is  of course  the key to handling extremely large scale in data-storage systems unlike regular parallel databases  it is usually not possible to decide on a partitioning function ahead of time further  if load increases  more servers need to be added and each server should be able to take on parts of the load incrementally to solve both these problems  data-storage systems typically partition data into relatively small units  small on such systems may mean of the order of hundreds of megabytes   these partitions are often called tablets  reflecting the fact that each tablet is a fragment of a table the partitioning of data should be done on the search key  so that a request for a specific key value is directed to a single tablet ; otherwise each request would require processing at multiple sites  increasing the load on the system greatly two approaches are used  either range partitioning is used directly on the key  or a hash function is applied on the key  and range partitioning is applied on the result of the hash function the site to which a tablet is assigned acts as the master site for that tablet all updates are routed through this site  and updates are then propagated to replicas of the tablet lookups are also sent to the same site  so that reads are consistent with writes the partitioning of data into tablets is not fixed up front  but happens dynamically as data are inserted  if a tablet grows too big  it is broken into smaller parts further  even if a tablet is not large enough to merit being broken up  if the load  get/put operations  on that tablet are excessive  the tablet may be broken into smaller tablets  which can be distributed across two or more sites to share the load usually the number of tablets is much larger than the number of sites  for the same reason that virtual partitioning is used in parallel databases it is important to know which site in the overall system is responsible for a particular tablet this can be done by having a tablet controller site which tracks the partitioning function  to map a get   request to one or more tablets  and a mapping function from tablets to sites  to find which site were responsible for which tablet each request coming into the system must be routed to the correct site ; if a single tablet controller site is responsible for this task  it would soon thesumit67.blogspot.com 866 chapter 19 distributed databases routers requests requests requests tablets tablets servers tablets controlle master copy of partition table/ tablet mapping figure 19.7 architecture of a cloud data storage system get overloaded instead  the mapping information can be replicated on a set of router sites  which route requests to the site with the appropriate tablet protocols to update mapping information when a tablet is split or moved are designed in such a way that no locking is used ; a request may as a result end up at a wrong site the problemis handled by detecting that the site is no longer responsible for the key specified by the request  and rerouting the request based on up-to-date mapping information figure 19.7 depicts the architecture of a cloud data-storage system  based loosely on the pnuts architecture other systems provide similar functionality  although their architecture may vary for example  bigtable does not have separate routers ; the partitioning and tablet-server mapping information is stored in the google file system  and clients read the information from the file system  and decide where to send their requests 19.9.1.3 transactions and replication data-storage systems on the cloud typically do not fully support acid transactions the cost of two-phase commit is too high  and two-phase commit can lead to blocking in the event of failures  which is not acceptable to typical web applications this means that such systems typically do not even support a transactionally consistent secondary index  the secondary index would be partitioned on a different attribute from the key used for storing the data  and an insert or update would then need to update two sites  which would require two-phase commit at best  such systems support transactions on data within a single tablet  which is controlled by a a single master site sherpa/pnuts also provides a testthesumit67 blogspot.com 19.9 cloud-based databases 867 and-set function  which allows an update to a data item to be conditional on the current version of the data item being the same as a specified version number if the current version number of the data item is more recent than the specified version number  the update is not performed the test-and-set function can be used by applications to implement a limited form of validation-based concurrency control  with validation restricted to data items in a single tablet in a system with thousands of sites  at any time it is almost guaranteed that several of the sites will be down a data-storage system on the cloud must be able to continue normal processing even with many sites down such systems replicate data  such as tablets  to multiple machines in a cluster  so that a copy of the data is likely to be available even if some machines of a cluster are down  a cluster is a collection of machines in a data center  for example  the google file system  gfs   which is a distributed fault-tolerant file system  replicates all file system blocks at three or more nodes in a cluster normal operation can continue as long as at least one copy of the data is available  key system data  such as the mapping of files to nodes  is replicated at more nodes  a majority of which need to be available   in addition  replication is also used across geographically distributed clusters  for reasons that we shall see shortly since each tablet is controlled by a single master site  if the site fails the tablet should be reassigned to a different site that has a copy of the tablet  which becomes the new master site for the tablet updates to a tablet are logged  and the log is itself replicated.when a site fails  the tablets at the site are assigned to other sites ; the new master site of each tablet is responsible for performing recovery actions using the log to bring its copy of the tablet to an up-to-date consistent state  after which updates and lookups can be performed on the tablet in bigtable  as an example  mapping information is stored in an index structure  and the index as well as the actual tablet data are stored in the file system tablet data updates are not flushed immediately  but log data are the file system ensures that the file system data are replicated and will be available even in the face of failure of a few nodes in the cluster thus  when a tablet is reassigned  the new master site for the tablet has access to up-to-date log data yahoo ! ? s sherpa/pnuts system  on the other hand  explicitly replicates tablets to multiple nodes in a cluster  instead of using a distributed file system  and uses a reliable distributed-messaging system to implement a highly available log unfortunately  it is not uncommon for an entire data center to become unavailable for example  due to natural disasters or fires replication at a remote site is therefore essential for high availability for many web applications  round-trip delays across a long-distance network can affect performance significantly  a problem that is increasing with the use of ajax applications that require multiple rounds of communication between the browser and the application to deal with this problem  users are connectedwith application servers that are closest to them geographically  and data are replicated at multiple data centers so that one of the replicas is likely to be close to the application server however  the danger of partitioning of the network is increased as a result given that mostweb applications place a greater premium on availability than on consistency  data-storage systems on the cloud usually allow updates to proceed thesumit67.blogspot.com 868 chapter 19 distributed databases even in the event of a partitioning  and provide support for restoring consistency later  as discussed earlier in section 19.6.6 multimaster replication with lazy propagation of updates  which we saw in section 19.5.3  is typically used for processing updates lazy propagation implies that updates are not propagated to replicas as part of the updating transaction  although they are typically propagated as soon as possible  typically using a messaging infrastructure in addition to propagating updates to replicas of a data item  updates to secondary indices  or to certain kinds of materialized views  such as the updates fromfriends  in a social-networking applicationwe saw earlier in section 19.9.1.1   can be sent using the messaging infrastructure secondary indices are basically tables  partitioned just like regular tables  based on the index search key ; an update of a record in a table can be mapped to updates of one or more tablets in a secondary index on the table there is no transactional guarantee on the updates of such secondary indices or materialized views  and only a best-effort guarantee in terms of when the updates reach their destination 19.9.2 traditional databases on the cloud we now consider the issue of implementing a traditional distributed database system  supporting acid properties and queries  on a cloud the concept of computing utilities is an old one  envisioned back in the 1960s the first manifestation of the concept was in timesharing systems inwhich several users shared access to a single mainframe computer later  in the late 1960s  the concept of virtual machines was developed  inwhich a userwas given the illusion of having a private computer,while in reality a single computer simulated several virtual machines cloud computing makes extensive use of the virtual-machine concept to provide computing services virtual machines provide great flexibility since clients may choose their own software environment including not only the application software but also the operating system virtual machines of several clients can run on a single physical computer  if the computing needs of the clients are low on the other hand  an entire computer can be allocated to each virtual machine of a client whose virtual machines have a high load a client may request several virtual machines over which to run an application this makes it easy to add or subtract computing power as workloads grow and shrink simply by adding or releasing virtual machines having a set of virtual machines works well for applications that are easily parallelized database systems  as we have seen  fall into this category each virtual machine can run database system code locally and behave in a manner similar to a site in a homogeneous distributed database system 19.9.3 challenges with cloud-based databases cloud-based databases certainly have several important advantages compared to building a computing infrastructure from scratch  and are in fact essential for certain applications thesumit67.blogspot.com 19.9 cloud-based databases 869 however  cloud-based database systems also have several disadvantages that we shall now explore unlike purely computational applications in which parallel computations run largely independently  distributed database systems require frequent communication and coordination among sites for  ? access to data on another physicalmachine  either because the data are owned by another virtual machine or because the data are stored on a storage server separate from the computer hosting the virtual machine ? obtaining locks on remote data ? ensuring atomic transaction commit via two-phase commit in our earlier study of distributed databases  we assumed  implicitly  that the database administrator had control over the physical location of data in a cloud system  the physical location of data is under the control of the vendor  not the client as a result  the physical placement of data may be suboptimal in terms of communication cost  and this may result in a large number of remote lock requests and large transfers of data across virtual machines effective query optimization requires that the optimizer have accurate cost measures for operations lacking knowledge of the physical placement of data  the optimizer has to rely on estimates that may be highly inaccurate  resulting in poor execution strategies because remote accesses are relatively slow compared to local access  these issues can have a significant impact on performance the above issues are a particular challenge for implementing traditional database applications on the cloud  although less challenging for simple datastorage systems the next few challenges we discuss apply equally to both application scenarios the matter of replication further complicates cloud-based data management cloud systems replicate client data for availability indeed many contracts have clauses imposing penalties on the vendor if a certain level of availability is not maintained this replication is done by the vendor without specific knowledge of the application since replication is under control of the cloud and not under the control of the database system  care must be used when the database system accesses data so as to ensure that the latest versions of the data are read failure to take these issues properly into account can result in a loss of the atomicity or isolation properties in many current cloud database applications  the application itself may need to take some responsibility for consistency users of cloud computing must be willing to accept that their data are held by another organization this may present a variety of risks in terms of security and legal liability if the cloud vendor suffers a security breach  client data may be divulged  causing the client to face legal challenges from its customers yet the client has no direct control over cloud-vendor security these issues become more complex if the cloud vendor chooses to store data  or replicas of data  in a foreign country various legal jurisdictions differ in their privacy laws so  for example  if a german company ? s data are replicated on a server in new york  then the privacy laws of the united states rather than those of germany or the thesumit67.blogspot.com 870 chapter 19 distributed databases european union apply the cloud vendor might be required to release client data to the u.s government even though the client never knew that its data would wind up under u.s jurisdiction specific cloud vendors offer their clients varying degrees of control over how their data are distributed and replicated some vendors offer database services directly to their clients rather than require clients to contract for raw storage and virtual machines over which to run their own database systems the market for cloud services continues to evolve rapidly  but it is clear that a database administrator who is contracting for cloud services has to consider a wide variety of technical  economic  and legal issues in order to ensure the privacy and security of data  guarantees of the acid properties  or an acceptable approximation thereof   and adequate performance despite the likelihood of data being distributed over a wide geographic area the bibliographical notes provide some of the current thinking on these topics much new literature is likely to appear in the next few years  and many of the current issues in cloud databases are being addressed by the research community 19.10 directory systems consider an organization that wishes to make data about its employees available to a variety of people in the organization ; examples of the kinds of data include name  designation  employee-id  address  email address  phone number  fax number  and so on in the precomputerization days  organizations would create physical directories of employees and distribute themacross the organization even today  telephone companies create physical directories of customers in general  a directory is a listing of information about some class of objects such as persons directories can be used to find information about a specific object  or in the reverse direction to find objects that meet a certain requirement in the world of physical telephone directories  directories that satisfy lookups in the forward direction are called white pages  while directories that satisfy lookups in the reverse direction are called yellow pages in today ? s networked world  the need for directories is still present and  if anything  even more important however  directories today need to be available over a computer network  rather than in a physical  paper  form 19.10.1 directory access protocols directory information can be made available through web interfaces  as many organizations  and phone companies in particular  do such interfaces are good for humans however  programs too need to access directory information directories can be used for storing other types of information  much like file system directories for instance  web browsers can store personal bookmarks and other browser settings in a directory system a user can thus access the same settings from multiple locations  such as at home and at work  without having to share a file system thesumit67.blogspot.com 19.10 directory systems 871 several directory access protocols have been developed to provide a standardized way of accessing data in a directory the most widely used among them today is the lightweight directory access protocol  ldap   obviously all the types of data in our examples can be stored without much trouble in a database system  and accessed through protocols such as jdbc or odbc the question then is,why come up with a specialized protocol for accessing directory information ? there are at least two answers to the question ? first  directory access protocols are simplified protocols that cater to a limited type of access to data they evolved in parallel with the database access protocols ? second  and more important  directory systems provide a simple mechanism to name objects in a hierarchical fashion  similar to file system directory names  which can be used in a distributed directory system to specify what information is stored in each of the directory servers for example  a particular directory server may store information for bell laboratories employees in murray hill  while another may store information for bell laboratories employees in bangalore  giving both sites autonomy in controlling their local data the directory access protocol can be used to obtain data from both directories across a network more important  the directory system can be set up to automatically forward queries made at one site to the other site  without user intervention for these reasons  several organizations have directory systems to make organizational information available online through a directory access protocol information in an organizational directory can be used for a variety of purposes  such as to find addresses  phone numbers  or email addresses of people  to find which departments people are in  and to track department hierarchies directories are also used to authenticate users  applications can collect authentication information such as passwords from users and authenticate them using the directory as may be expected  several directory implementations find it beneficial to use relational databases to store data  instead of creating special-purpose storage systems 19.10.2 ldap  lightweight directory access protocol in general a directory system is implemented as one or more servers  which service multiple clients clients use the application programmer interface defined by the directory system to communicate with the directory servers directory access protocols also define a data model and access control the x.500 directory access protocol  defined by the international organization for standardization  iso   is a standard for accessing directory information however  the protocol is rather complex  and is notwidely used the lightweight directory access protocol  ldap  provides many of the x.500 features  but with less complexity  and is widely used in the rest of this section  we shall outline the data model and access protocol details of ldap thesumit67.blogspot.com 872 chapter 19 distributed databases 19.10.2.1 ldap data model in ldap  directories store entries  which are similar to objects each entry must have a distinguished name  dn   which uniquely identifies the entry a dn is in turn made up of a sequence of relative distinguished names  rdns   for example  an entry may have the following distinguished name  cn = silberschatz  ou = computer science  o = yale university  c = usa as you can see  the distinguished name in this example is a combination of a name and  organizational  address  starting with a person ? s name  then giving the organizational unit  ou   the organization  o   and country  c   the order of the components of a distinguished name reflects the normal postal address order  rather than the reverse order used in specifying path names for files the set of rdns for a dn is defined by the schema of the directory system entries can also have attributes ldap provides binary  string  and time types  and additionally the types tel for telephone numbers  and postaladdress for addresses  lines separated by a ? $ ? character  .unlike those in the relational model  attributes are multivalued by default  so it is possible to store multiple telephone numbers or addresses for an entry ldap allows the definition of object classes with attribute names and types inheritance can be used in defining object classes moreover  entries can be specified to be of one or more object classes it is not necessary that there be a single most-specific object class to which an entry belongs entries are organized into a directory information tree  dit   according to their distinguished names entries at the leaf level of the tree usually represent specific objects entries that are internal nodes represent objects such as organizational units  organizations  or countries the children of a node have a dn containing all the rdns of the parent  and one or more additional rdns for instance  an internal node may have a dn c = usa  and all entries below it have the value usa for the rdn c the entire distinguished name need not be stored in an entry the systemcan generate the distinguished name of an entry by traversing up the dit from the entry  collecting the rdn = value components to create the full distinguished name entriesmay have more than one distinguished name ? for example  an entry for a person in more than one organization to deal with such cases  the leaf level of a dit can be an alias  which points to an entry in another branch of the tree 19.10.2.2 data manipulation unlike sql  ldap does not define either a data-definition language or a datamanipulation language however  ldap defines a network protocol for carrying out data definition and manipulation users of ldap can either use an application programming interface or use tools provided by various vendors to perform data definition and manipulation ldap also defines a file format called ldap data interchange format  ldif  that can be used for storing and exchanging information thesumit67.blogspot.com 19.10 directory systems 873 the querying mechanism in ldap is very simple  consisting of just selections and projections  without any join a query must specify the following  ? a base ? that is  a node within a dit ? by giving its distinguished name  the path from the root to the node   ? a search condition  which can be a boolean combination of conditions on individual attributes equality  matching by wild-card characters  and approximate equality  the exact definition of approximate equality is system dependent  are supported ? a scope  which can be just the base  the base and its children  or the entire subtree beneath the base ? attributes to return ? limits on number of results and resource consumption the query can also specify whether to automatically dereference aliases ; if alias dereferences are turned off  alias entries can be returned as answers one way of querying an ldap data source is by using ldap urls examples of ldap urls are  ldap   //codex.cs.yale.edu/o = yale university,c = usa ldap   //codex.cs.yale.edu/o = yale university,c = usa ? ? sub ? cn = silberschatz the first url returns all attributes of all entries at the server with organization being yale university  and country being usa the second url executes a search query  selection  cn = silberschatz on the subtree of the node with distinguished name o = yale university  c = usa the question marks in the url separate different fields the first field is the distinguished name  here o = yale university,c = usa the second field  the list of attributes to return  is left empty  meaning return all attributes the third attribute  sub  indicates that the entire subtree is to be searched the last parameter is the search condition a second way of querying an ldap directory is by using an application programming interface figure 19.8 shows a piece of c code used to connect to an ldap server and run a query against the server the code first opens a connection to an ldap server by ldap open and ldap bind it then executes a query by ldap search s the arguments to ldap search s are the ldap connection handle  the dn of the base from which the search should be done  the scope of the search  the search condition  the list of attributes to be returned  and an attribute called attrsonly  which  if set to 1  would result in only the schema of the result being returned  without any actual tuples the last argument is an output argument that returns the result of the search as an ldapmessage structure the first for loop iterates over and prints each entry in the result note that an entry may have multiple attributes  and the second for loop prints each attribute since attributes in ldap may be multivalued  the third for loop prints each value of an attribute the calls ldap msgfree and ldap value free free memory that is thesumit67.blogspot.com 874 chapter 19 distributed databases # include < stdio.h > # include < ldap.h > main    ldap * ld ; ldapmessage * res  * entry ; char * dn  * attr  * attrlist   =  ? telephonenumber ?  null  ; berelement * ptr ; int vals  i ; ld = ldap open  ? codex.cs.yale.edu ?  ldap port  ; ldap simple bind  ld  ? avi ?  ? avi-passwd ?  ; ldap search s  ld  ? o = yale university  c = usa ?  ldap scope subtree  ? cn = silberschatz ?  attrlist  / * attrsonly * / 0  &res  ; printf  ? found % d entries ?  ldap count entries  ld  res   ; for  entry = ldap first entry  ld  res  ; entry ! = null ; entry = ldap next entry  ld  entry    dn = ldap get dn  ld  entry  ; printf  ? dn  % s ?  dn  ; ldap memfree  dn  ; for  attr = ldap first attribute  ld  entry  &ptr  ; attr ! null ; attr = ldap next attribute  ld  entry  ptr    printf  ? % s  ?  attr  ; vals = ldap get values  ld  entry  attr  ; for  i = 0 ; vals  i  ! = null ; i + +  printf  ? % s  ?  vals  i   ; ldap value free  vals  ;   ldap msgfree  res  ; ldap unbind  ld  ;  figure 19.8 example of ldap code in c allocated by the ldap libraries figure 19.8 does not show code for handling error conditions the ldap api also contains functions to create  update  and delete entries  as well as other operations on the dit each function call behaves like a separate transaction ; ldap does not support atomicity of multiple updates 19.10.2.3 distributed directory trees information about an organization may be split into multiple dits  each of which stores information about some entries the suffix of a dit is a sequence of thesumit67.blogspot.com 19.11 summary 875 rdn = value pairs that identify what information the dit stores ; the pairs are concatenated to the rest of the distinguished name generated by traversing from the entry to the root for instance  the suffix of a dit may be o = lucent  c = usa  while anothermay have the suffix o = lucent  c = india the ditsmay be organizationally and geographically separated a node in a dit may contain a referral to another node in another dit ; for instance  the organizational unit bell labs under o = lucent  c = usa may have its own dit  in which case the dit for o = lucent  c = usa would have a node ou = bell labs representing a referral to the dit for bell labs referrals are the key component that help organize a distributed collection of directories into an integrated system when a server gets a query on a dit  it may return a referral to the client  which then issues a query on the referenced dit access to the referenced dit is transparent  proceeding without the user ? s knowledge alternatively  the server itself may issue the query to the referred dit and return the results along with locally computed results the hierarchical naming mechanism used by ldap helps break up control of information across parts of an organization the referral facility then helps integrate all the directories in an organization into a single virtual directory although it is not an ldap requirement  organizations often choose to break up information either by geography  for instance  an organization may maintain a directory for each site where the organization has a large presence  or by organizational structure  for instance  each organizational unit  such as department  maintains its own directory   many ldap implementations support master ? slave and multimaster replication of dits  although replication is not part of the current ldap version 3 standard.work on standardizing replication in ldap is in progress 19.11 summary ? a distributed database system consists of a collection of sites  each of which maintains a local database system each site is able to process local transactions  those transactions that access data in only that single site in addition  a site may participate in the execution of global transactions  those transactions that access data in several sites the execution of global transactions requires communication among the sites ? distributed databases may be homogeneous  where all sites have a common schema and database system code  or heterogeneous,where the schemas and system codes may differ ? there are several issues involved in storing a relation in the distributed database  including replication and fragmentation it is essential that the system minimize the degree to which a user needs to be aware of how a relation is stored ? a distributed system may suffer fromthe same types of failure that can afflict a centralized system there are  however  additional failures with which we thesumit67.blogspot.com 876 chapter 19 distributed databases need to deal in a distributed environment  including the failure of a site  the failure of a link  loss of a message  and network partition each of these problems needs to be considered in the design of a distributed recovery scheme ? to ensure atomicity  all the sites inwhich a transaction t executed must agree on the final outcome of the execution t either commits at all sites or aborts at all sites to ensure this property  the transaction coordinator of t must execute a commit protocol the most widely used commit protocol is the two-phase commit protocol ? the two-phase commit protocol may lead to blocking  the situation in which the fate of a transaction can not be determined until a failed site  the coordinator  recovers we can use the three-phase commit protocol to reduce the probability of blocking ? persistent messaging provides an alternative model for handling distributed transactions the model breaks a single transaction into parts that are executed at different databases persistent messages  which are guaranteed to be delivered exactly once  regardless of failures   are sent to remote sites to request actions to be taken there while persistent messaging avoids the blocking problem  application developers have to write code to handle various types of failures ? the various concurrency-control schemes used in a centralized system can be modified for use in a distributed environment ? in the case of locking protocols  the only change that needs to be incorporated is in the way that the lock manager is implemented there are a variety of different approaches here one or more central coordinators may be used if  instead  a distributed-lock-manager approach is taken  replicated data must be treated specially ? protocols for handling replicated data include the primary copy  majority  biased  and quorum consensus protocols these have different trade-offs in terms of cost and ability to work in the presence of failures ? in the case of timestamping and validation schemes  the only needed change is to develop a mechanism for generating unique global timestamps ? many database systems support lazy replication  where updates are propagated to replicas outside the scope of the transaction that performed the update such facilities must be used with great care  since they may result in nonserializable executions ? deadlock detection in a distributed-lock-manager environment requires cooperation between multiple sites  since there may be global deadlocks even when there are no local deadlocks thesumit67.blogspot.com 19.11 summary 877 ? to provide high availability  a distributed database must detect failures  reconfigure itself so that computation may continue  and recover when a processor or a link is repaired the task is greatly complicated by the fact that it is hard to distinguish between network partitions and site failures themajority protocol can be extended by using version numbers to permit transaction processing to proceed even in the presence of failures while the protocol has a significant overhead  it works regardless of the type of failure less-expensive protocols are available to deal with site failures  but they assume network partitioning does not occur ? some of the distributed algorithms require the use of a coordinator to provide high availability  the system must maintain a backup copy that is ready to assume responsibility if the coordinator fails another approach is to choose the new coordinator after the coordinator has failed the algorithms that determinewhich site should act as a coordinator are called election algorithms ? queries on a distributed database may need to access multiple sites several optimization techniques are available to identify the best set of sites to access queries can be rewritten automatically in terms of fragments of relations and then choices can be made among the replicas of each fragment semijoin techniques may be employed to reduce data transfer involved in joining relations  or fragments or relicas thereof  across distinct sites ? heterogeneous distributed databases allow sites to have their own schemas and database system code.amultidatabase system provides an environment in which new database applications can access data from a variety of preexisting databases located in various heterogeneous hardware and software environments the local database systems may employ different logical models and data-definition and data-manipulation languages  and may differ in their concurrency-control and transaction-management mechanisms a multidatabase system creates the illusion of logical database integration  without requiring physical database integration ? a large number of data-storage systems on the cloud have been built in recent years  in response to data storage needs of extremely large-scale web applications these data-storage systems allow scalability to thousands of nodes  with geographic distribution  and high availability however  they do not support the usual acid properties  and they achieve availability during partitions at the cost of consistency of replicas current data-storage systems also do not support sql  and provide only a simple put   /get   interface while cloud computing is attractive even for traditional databases  there are several challenges due to lack of control on data placement and geographic replication ? directory systems can be viewed as a specialized form of database  where information is organized in a hierarchical fashion similar to the way files are organized in a file system directories are accessed by standardized directory access protocols such as ldap directories can be distributed across multiple thesumit67.blogspot.com 878 chapter 19 distributed databases sites to provide autonomy to individual sites directories can contain referrals to other directories  which help build an integrated view whereby a query is sent to a single directory  and it is transparently executed at all relevant directories review terms ? homogeneous distributed database ? heterogeneous distributed database ? data replication ? primary copy ? data fragmentation ? horizontal fragmentation ? vertical fragmentation ? data transparency ? fragmentation transparency ? replication transparency ? location transparency ? name server ? aliases ? distributed transactions ? local transactions ? global transactions ? transaction manager ? transaction coordinator ? system failure modes ? network partition ? commit protocols ? two-phase commit protocol  2pc  ? ready state ? in-doubt transactions ? blocking problem ? three-phase commit protocol  3pc  ? persistent messaging ? concurrency control ? single lock manager ? distributed lock manager ? protocols for replicas ? primary copy ? majority protocol ? biased protocol ? quorum consensus protocol ? timestamping ? master ? slave replication ? multimaster  update-anywhere  replication ? transaction-consistent snapshot ? lazy propagation ? deadlock handling ? local wait-for graph ? global wait-for graph ? false cycles ? availability ? robustness ? majority-based approach ? read one  write all ? read one  write all available ? site reintegration ? coordinator selection thesumit67.blogspot.com practice exercises 879 ? backup coordinator ? election algorithms ? bully algorithm ? distributed query processing ? semijoin strategy ? multidatabase system ? autonomy ? mediators ? local transactions ? global transactions ? ensuring global serializability ? ticket ? cloud computing ? cloud data storage ? tablet ? directory systems ? ldap  lightweight directory access protocol ? distinguished name  dn  ? relative distinguished names  rdns  ? directory information tree  dit  ? distributed directory trees ? dit suffix ? referral practice exercises 19.1 howmight a distributed database designed for a local-area network differ from one designed for a wide-area network ? 19.2 to build a highly available distributed system  you must know what kinds of failures can occur a list possible types of failure in a distributed system b which items in your list from part a are also applicable to a centralized system ? 19.3 consider a failure that occurs during 2pc for a transaction for each possible failure that you listed in practice exercise 19.2a  explain how 2pc ensures transaction atomicity despite the failure 19.4 consider a distributed system with two sites  a and b can site a distinguish among the following ? ? b goes down ? the link between a and b goes down ? b is extremely overloaded and response time is 100 times longer than normal what implications does your answer have for recovery in distributed systems ? thesumit67.blogspot.com 880 chapter 19 distributed databases 19.5 the persistent messaging scheme described in this chapter depends on timestamps combined with discarding of received messages if they are too old suggest an alternative scheme based on sequence numbers instead of timestamps 19.6 give an example where the read one  write all available approach leads to an erroneous state 19.7 explain the difference between data replication in a distributed system and the maintenance of a remote backup site 19.8 give an examplewhere lazy replication can lead to an inconsistent database state even when updates get an exclusive lock on the primary  master  copy 19.9 consider the following deadlock-detection algorithm when transaction ti  at site s1  requests a resource from tj  at site s3  a request message with timestamp n is sent the edge  ti  tj  n  is inserted in the local wait-for graph of s1 the edge  ti  tj  n  is inserted in the local wait-for graph of s3 only if tj has received the request message and can not immediately grant the requested resource a request from ti to tj in the same site is handled in the usualmanner ; no timestamps are associated with the edge  ti  tj   a central coordinator invokes the detection algorithm by sending an initiating message to each site in the system on receiving this message  a site sends its local wait-for graph to the coordinator note that such a graph contains all the local information that the site has about the state of the real graph the wait-for graph reflects an instantaneous state of the site  but it is not synchronized with respect to any other site when the controller has received a reply from each site  it constructs a graph as follows  ? the graph contains a vertex for every transaction in the system ? the graph has an edge  ti  tj  if and only if  ? there is an edge  ti  tj  in one of the wait-for graphs ? an edge  ti  tj  n   for some n  appears in more than one wait-for graph show that  if there is a cycle in the constructed graph  then the system is in a deadlock state  and that  if there is no cycle in the constructed graph  then the system was not in a deadlock state when the execution of the algorithm began 19.10 consider a relation that is fragmented horizontally by plant number  employee  name  address  salary  plant number  thesumit67.blogspot.com practice exercises 881 assume that each fragment has two replicas  one stored at the new york site and one stored locally at the plant site describe a good processing strategy for the following queries entered at the san jose site a find all employees at the boca plant b find the average salary of all employees c find the highest-paid employee at each of the following sites  toronto  edmonton  vancouver  montreal d find the lowest-paid employee in the company 19.11 compute r  s for the relations of figure 19.9 19.12 give an example of an application ideally suited for the cloud and another that would be hard to implement successfully in the cloud explain your answer 19.13 given that the ldap functionality can be implemented on top of a database system  what is the need for the ldap standard ? 19.14 consider a multidatabase system in which it is guaranteed that at most one global transaction is active at any time  and every local site ensures local serializability a suggest ways in which the multidatabase system can ensure that there is at most one active global transaction at any time b show by example that it is possible for a nonserializable global schedule to result despite the assumptions 19.15 consider a multidatabase system in which every local site ensures local serializability  and all global transactions are read only a show by example that nonserializable executions may result in such a system b show how you could use a ticket scheme to ensure global serializability r a b c s c d e 1 2 3 3 4 5 4 5 6 3 6 8 1 2 4 2 3 2 5 3 2 1 4 1 8 9 7 1 2 3 figure 19.9 relations for practice exercise 19.11 thesumit67.blogspot.com 882 chapter 19 distributed databases exercises 19.16 discuss the relative advantages of centralized and distributed databases 19.17 explain how the following differ  fragmentation transparency  replication transparency  and location transparency 19.18 when is it useful to have replication or fragmentation of data ? explain your answer 19.19 explain the notions of transparency and autonomy.why are these notions desirable from a human-factors standpoint ? 19.20 if we apply a distributed version of the multiple-granularity protocol of chapter 15 to a distributed database  the site responsible for the root of the dag may become a bottleneck suppose we modify that protocol as follows  ? only intention-mode locks are allowed on the root ? all transactions are given all possible intention-mode locks on the root automatically show that these modifications alleviate this problem without allowing any nonserializable schedules 19.21 study and summarize the facilities that the database system you are using provides for dealingwith inconsistent states that can be reachedwith lazy propagation of updates 19.22 discuss the advantages and disadvantages of the two methods that we presented in section 19.5.2 for generating globally unique timestamps 19.23 consider the relations  employee  name  address  salary  plant number  machine  machine number  type  plant number  assume that the employee relation is fragmented horizontally by plant number  and that each fragment is stored locally at its corresponding plant site assume that the machine relation is stored in its entirety at the armonk site describe a good strategy for processing each of the following queries a find all employees at the plant that contains machine number 1130 b find all employees at plants that contain machines whose type is ? milling machine ? c find all machines at the almaden plant d find employee  machine thesumit67.blogspot.com bibliographical notes 883 19.24 for each of the strategies of exercise 19.23  state how your choice of a strategy depends on  a the site at which the querywas entered b the site at which the result is desired 19.25 is the expression ri  r j necessarily equal to r j  ri ? under what conditions does ri  r j = r j  ri hold ? 19.26 if a cloud data-storage service is used to store two relations r and s and we need to join r and s  why might it be useful to maintain the join as a materialized view ? in your answer  be sure to distinguish among various meanings of ? useful ?  overall throughput  efficient use of space  and response time to user queries 19.27 why do cloud-computing services support traditional database systems best by using a virtual machine instead of running directly on the service provider ? s actual machine ? 19.28 describe how ldap can be used to provide multiple hierarchical views of data  without replicating the base-level data bibliographical notes textbook discussions of distributed databases are offered by ozsu and valduriez  1999   breitbart et al  1999b  presents an overview of distributed databases the implementation of the transaction concept in a distributed database is presented by gray  1981  and traiger et al  1982   the 2pc protocolwas developed by lampson and sturgis  1976   the three-phase commit protocol is from skeen  1981   mohan and lindsay  1983  discusses two modified versions of 2pc  called presume commit and presume abort  that reduce the overhead of 2pc by defining default assumptions regarding the fate of transactions the bully algorithm in section 19.6.5 is fromgarcia-molina  1982   distributed clock synchronization is discussed in lamport  1978   distributed concurrency control is covered by bernstein and goodman  1981   the transaction manager of r * is described in mohan et al  1986   validation techniques for distributed concurrency-control schemes are described by schlageter  1981  and bassiouni  1988   the problem of concurrent updates to replicated data was revisited in the context of data warehouses by gray et al  1996   anderson et al  1998  discusses issues concerning lazy replication and consistency breitbart et al  1999a  describes lazy update protocols for handling replication the user manuals of various database systems provide details of how they handle replication and consistency huang and garcia-molina  2001  addresses exactly-once semantics in a replicated messaging system knapp  1987  surveys the distributed deadlock-detection literature practice exercise 19.9 is from stuart et al  1984   thesumit67.blogspot.com 884 chapter 19 distributed databases distributed query processing is discussed in epstein et al  1978  and hevner and yao  1979   daniels et al  1982  discusses the approach to distributed query processing taken by r *  dynamic query optimization in multidatabases is addressed by ozcan et al  1997   adali et al  1996  and papakonstantinou et al  1996  describe queryoptimization issues in mediator systems transaction processing in multidatabase systems is discussed in mehrotra et al  2001   the ticket scheme is presented in georgakopoulos et al  1994   2lsr is introduced in mehrotra et al  1991   a collection of papers on data management on cloud systems is in ooi and s parthasarathy  2009   the implementation of google ? s bigtable is described in chang et al  2008   while cooper et al  2008  describe yahoo ! ? s pnuts system experience in building a database using amazon ? s s3 cloud-based storage is described in brantner et al  2008   an approach to making transactions work correctly in cloud systems is discussed in lomet et al  2009   the cap theorem was conjectured by brewer  2000   and was formalized and proved by gilbert and lynch  2002   howes et al  1999  provides textbook coverage of ldap thesumit67.blogspot.com part 6 data warehousing  data mining  and information retrieval database queries are often designed to extract specific information  such as the balance of an account or the sum of a customer ? s account balances however  queries designed to help formulate a corporate strategy usually requires access to large amounts of data originating at multiple sources a data warehouse is a repository of data gathered from multiple sources and stored under a common  unified database schema data stored in warehouse are analyzed by a variety of complex aggregations and statistical analyses  often exploiting sql constructs for data analysiswhich we saw inchapter 5 furthermore  knowledge-discovery techniques may be used to attempt to discover rules and patterns fromthe data for example  a retailermay discover that certain products tend to be purchased together  and may use that information to develop marketing strategies this process of knowledge discovery from data is called data mining chapter 20 addresses the issues of data warehousing and data mining in ourdiscussions so far,we have focused on relatively simple  well-structured data however  there is an enormous amount of unstructured textual data on the internet  on intranets within organizations  and on the computers of individual users users wish to find relevant information from this vast body of mostly textual information  using simple query mechanisms such as keyword queries the field of information retrieval deals with querying of such unstructured data  and pays particular attention to the ranking of query results although this area of research is several decades old  it has undergone tremendous growth with the development of theworldwideweb chapter 21 provides an introduction to the field of information retrieval 885 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter20 data warehousing and mining businesses have begun to exploit the burgeoning data online to make better decisions about their activities  such as what items to stock and how best to target customers to increase sales there are two aspects to exploiting such data the first aspect is to gather data from multiple sources into a central repository  called a datawarehouse issues involved inwarehousing include techniques for dealing with dirty data  that is  data with some errors  and with techniques for efficient storage and indexing of large volumes of data the second aspect is to analyze the gathered data to find information or knowledge that can be the basis for business decisions some kinds of data analysis can be done by using sql constructs for online analytical processing  olap   which we saw in section 5.6  chapter 5   and by using tools and graphical interfaces for olap another approach to getting knowledge from data is to use data mining  which aims at detecting various types of patterns in large volumes of data data mining supplements various types of statistical techniques with similar goals 20.1 decision-support systems database applications can be broadly classified into transaction-processing and decision-support systems.transaction-processing systems are systems that record information about transactions  such as product sales information for companies  or course registration and grade information for universities transactionprocessing systems are widely used today  and organizations have accumulated a vast amount of information generated by these systems decision-support systems aim to get high-level information out of the detailed information stored in transaction-processing systems  and to use the high-level information to make a variety of decisions decision-support systems help managers to decide what products to stock in a shop  what products to manufacture in a factory  or which of the applicants should be admitted to a university for example  company databases often contain enormous quantities of information about customers and transactions the size of the information storage required may range up to hundreds of gigabytes  or even terabytes  for large 887 thesumit67.blogspot.com 888 chapter 20 data warehousing and mining retail chains transaction information for a retailer may include the name or identifier  such as credit-card number  of the customer  the items purchased  the price paid  and the dates on which the purchases were made information about the items purchased may include the name of the item  the manufacturer  the model number  the color  and the size customer information may include credit history  annual income  residence  age  and even educational background such large databases can be treasure troves of information for making business decisions  such as what items to stock and what discounts to offer for instance  a retail company may notice a sudden spurt in purchases of flannel shirts in the pacific northwest  may realize that there is a trend  and may start stocking a larger number of such shirts in shops in that area as another example  a car company may find  on querying its database  that most of its small sports cars are bought by young women whose annual incomes are above $ 50,000 the company may then target its marketing to attract more such women to buy its small sports cars  and may avoidwasting money trying to attract other categories of people to buy those cars in both cases  the company has identified patterns in customer behavior and has used the patterns to make business decisions the storage and retrieval of data for decision support raises several issues  ? although many decision-support queries can be written in sql  others either can not be expressed in sql or can not be expressed easily in sql several sql extensions have therefore been proposed to make data analysis easier in section 5.6  we covered sql extensions for data analysis and techniques for online analytical processing  olap   ? database query languages are not suited to the performance of detailed statistical analyses of data there are several packages  such as sas and s + +  that help in statistical analysis such packages have been interfaced with databases  to allow large volumes of data to be stored in the database and retrieved efficiently for analysis the field of statistical analysis is a large discipline on its own ; see the references in the bibliographical notes for more information ? large companies have diverse sources ofdata that they need to use for making business decisions the sources may store the data under different schemas for performance reasons  as well as for reasons of organization control   the data sources usually will not permit other parts of the company to retrieve data on demand to execute queries efficiently on such diverse data  companies have built data warehouses data warehouses gather data from multiple sources under a unified schema  at a single site thus  they provide the user a single uniform interface to data.we study issues in building and maintaining a data warehouse in section 20.2 ? knowledge-discovery techniques attempt to discover automatically statistical rules and patterns fromdata the field of data mining combines knowledgediscovery techniques invented by artificial intelligence researchers and stathesumit67 blogspot.com 20.2 data warehousing 889 tistical analysts  with efficient implementation techniques that enable them to be used on extremely large databases section 20.3 discusses data mining the area of decision support can be broadly viewed as covering all the above areas  although some people use the term in a narrower sense that excludes statistical analysis and data mining 20.2 data warehousing large companies have presences in many places  each of which may generate a large volume of data for instance  large retail chains have hundreds or thousands of stores  whereas insurance companies may have data from thousands of local branches further  large organizations have a complex internal organization structure  and therefore different data may be present in different locations  or on different operational systems  or under different schemas for instance  manufacturing-problem data and customer-complaint data may be stored on different database systems organizations often purchase data fromexternal sources  such as mailing lists that are used for product promotions  or credit scores of customers that are provided by credit bureaus  to decide on credit-worthiness of customers.1 corporate decision makers require access to information from multiple such sources setting up queries on individual sources is both cumbersome and inefficient moreover  the sources of datamay store only current data,whereas decision makers may need access to past data as well ; for instance  information about how purchase patterns have changed in the past year could be of great importance data warehouses provide a solution to these problems a data warehouse is a repository  or archive  of information gathered from multiple sources  stored under a unified schema  at a single site once gathered  the data are stored for a long time  permitting access to historical data thus  data warehouses provide the user a single consolidated interface to data  making decision-support queries easier to write moreover  by accessing information for decision support from a data warehouse  the decision maker ensures that online transaction-processing systems are not affected by the decision-support workload 20.2.1 components of a data warehouse figure 20.1 shows the architecture of a typical data warehouse  and illustrates the gathering of data  the storage of data  and the querying and data analysis support among the issues to be addressed in building a warehouse are the following  1credit bureaus are companies that gather information about consumers from multiple sources and compute a creditworthiness score for each consumer thesumit67.blogspot.com 890 chapter 20 data warehousing and mining data loaders dbms data warehouse query and analysis tools data source n data source 2 data source 1  figure 20.1 data-warehouse architecture ? when and how to gather data in a source-driven architecture for gathering data  the data sources transmit new information  either continually  as transaction processing takes place   or periodically  nightly  for example   in a destination-driven architecture  the data warehouse periodically sends requests for new data to the sources unless updates at the sources are replicated at the warehouse via twophase commit  thewarehousewill never be quite up-to-datewith the sources two-phase commit is usually far too expensive to be an option  so data warehouses typically have slightly out-of-date data that  however  is usually not a problem for decision-support systems ? what schema to use data sources that have been constructed independently are likely to have different schemas in fact  they may even use different data models part of the task of a warehouse is to perform schema integration  and to convert data to the integrated schema before they are stored as a result  the data stored in the warehouse are not just a copy of the data at the sources instead  they can be thought of as a materialized view of the data at the sources ? data transformation and cleansing.the taskof correcting andpreprocessing data is called data cleansing data sources often deliver data with numerous minor inconsistencies  which can be corrected for example  names are often misspelled  and addressesmay have street  area  or city namesmisspelled  or postal codes entered incorrectly these can be corrected to a reasonable extent by consulting a database of street names and postal codes in each city the approximate matching of data required for this task is referred to as fuzzy lookup address lists collected from multiple sources may have duplicates that need to be eliminated in a merge ? purge operation  this operation is also referred to as deduplication   records for multiple individuals in a house thesumit67.blogspot.com 20.2 data warehousing 891 may be grouped together so only one mailing is sent to each house ; this operation is called householding data may be transformed in ways other than cleansing  such as changing the units of measure  or converting the data to a different schema by joining data from multiple source relations data warehouses typically have graphical tools to support data transformation such tools allow transformation to be specified as boxes  and edges can be created between boxes to indicate the flow of data conditional boxes can route data to an appropriate next step in transformation see figure 30.7 for an example of a transformation specified using the graphical tool provided by microsoft sql server ? how to propagate updates updates on relations at the data sources must be propagated to the data warehouse if the relations at the data warehouse are exactly the same as those at the data source  the propagation is straightforward if they are not  the problem of propagating updates is basically the view-maintenance problem  which was discussed in section 13.5 ? what data to summarize the rawdata generated by a transaction-processing system may be too large to store online however  we can answer many queries by maintaining just summary data obtained by aggregation on a relation  rather than maintaining the entire relation for example  instead of storing data about every sale of clothing  we can store total sales of clothing by item name and category suppose that a relation r has been replaced by a summary relation s users may still be permitted to pose queries as though the relation r were available online if the query requires only summary data  it may be possible to transform it into an equivalent one using s instead ; see section 13.5 the different steps involved in getting data into a data warehouse are called extract  transform  and load or etl tasks ; extraction refers to getting data from the sources  while load refers to loading the data into the data warehouse 20.2.2 warehouse schemas data warehouses typically have schemas that are designed for data analysis  using tools such as olap tools thus  the data are usually multidimensional data  with dimension attributes and measure attributes tables containing multidimensional data are called fact tables and are usually very large a table recording sales information for a retail store  with one tuple for each item that is sold  is a typical example of a fact table the dimensions of the sales table would include what the item is  usually an item identifier such as that used in bar codes   the date when the item is sold  which location  store  the item was sold from  which customer bought the item  and so on the measure attributes may include the number of items sold and the price of the items to minimize storage requirements  dimension attributes are usually short identifiers that are foreign keys into other tables called dimension tables for instance  a fact table sales would have attributes item id  store id  customer id  and thesumit67.blogspot.com 892 chapter 20 data warehousing and mining figure 20.2 star schema for a data warehouse date  and measure attributes number and price the attribute store id is a foreign key into a dimension table store  which has other attributes such as store location  city  state  country   the item id attribute of the sales table would be a foreign key into a dimension table item info  which would contain information such as the name of the item  the category to which the item belongs  and other item details such as color and size the customer id attribute would be a foreign key into a customer table containing attributes such as name and address of the customer we can also view the date attribute as a foreign key into a date info table giving the month  quarter  and year of each date the resultant schema appears in figure 20.2 such a schema  with a fact table  multiple dimension tables  and foreign keys from the fact table to the dimension tables  is called a star schema more complex data-warehouse designs may have multiple levels of dimension tables ; for instance  the item info table may have an attribute manufacturer id that is a foreign key into another table giving details of the manufacturer such schemas are called snowflake schemas complex datawarehouse designs may also have more than one fact table 20.2.3 column-oriented storage databases traditionally store all attributes of a tuple together  and tuples are stored sequentially in a file such a storage layout is referred to as row-oriented storage in contrast  in column-oriented storage  each attribute of a relation is stored in a separate file  with values from successive tuples stored at successive positions in the file assuming fixed-size data types  the value of attribute a of the ith tuple of a relation can be found by accessing the file corresponding to attribute a  and reading the value at offset  i -1  times the size  in bytes  of values in attribute a thesumit67.blogspot.com 20.3 data mining 893 column-oriented storage has at least two major benefits over row-oriented storage  1 when a query needs to access only a few attributes of a relation with a large number of attributes  the remaining attributes need not be fetched from disk into memory in contrast  in row-oriented storage  not only are irrelevant attributes fetched into memory  but they may also get prefetched into processor cache  wasting cache space and memory bandwidth  if they are stored adjacent to attributes used in the query 2 storing values of the same type together increases the effectiveness of compression ; compression can greatly reduce both the disk storage cost and the time to retrieve data from disk on the other hand  column-oriented storage has the drawback that storing or fetching a single tuple requires multiple i/o operations as a result of the above trade-offs  column-oriented storage is not widely used for transaction-processing applications however  column-oriented storage is gaining increasing acceptance for data-warehousing applications  where accesses are rarely to individual tuples  but rather require scanning and aggregating multiple tuples sybase iq was one of the early products to use column-oriented storage  but there are now several research projects and companies that have developed databases based on column-oriented storage systems these systems have been able to demonstrate significant performance gains for many data-warehousing applications see the bibliographical notes for references on how column-oriented stores are implemented  and queries optimized and processed on such stores 20.3 data mining the term data mining refers loosely to the process of semiautomatically analyzing large databases to find useful patterns like knowledge discovery in artificial intelligence  also called machine learning  or statistical analysis  data mining attempts to discover rules and patterns from data however  data mining differs from machine learning and statistics in that it deals with large volumes of data  stored primarily on disk that is  data mining deals with ? knowledge discovery in databases ? some types of knowledge discovered from a database can be represented by a set of rules the following is an example of a rule  stated informally  ? young women with annual incomes greater than $ 50,000 are the most likely people to buy small sports cars ? of course such rules are not universally true  and have degrees of ? support ? and ? confidence  ? as we shall see other types of knowledge are represented by equations relating different variables to each other  or by other mechanisms for predicting outcomes when the values of some variables are known thesumit67.blogspot.com 894 chapter 20 data warehousing and mining there are a variety of possible types of patterns that may be useful  and different techniques are used to find different types of patterns.we shall study a few examples of patterns and see how they may be automatically derived from a database usually there is a manual component to data mining  consisting of preprocessing data to a form acceptable to the algorithms and postprocessing of discovered patterns to find novel ones that could be useful there may also be more than one type of pattern that can be discovered from a given database  and manual interaction may be needed to pick useful types of patterns for this reason  data mining is really a semiautomatic process in real life however  in our description we concentrate on the automatic aspect of mining the discovered knowledge has numerous applications the most widely used applications are those that require some sort of prediction for instance,when a person applies for a credit card  the credit-card company wants to predict if the person is a good credit risk the prediction is to be based on known attributes of the person  such as age  income  debts  and past debt-repayment history rules for making the prediction are derived from the same attributes of past and current credit-card holders  along with their observed behavior  such as whether they defaulted on their credit-card dues other types of prediction include predicting which customersmayswitch over to a competitor  these customersmaybe offered special discounts to tempt them not to switch   predicting which people are likely to respond to promotional mail  ? junk mail ?   or predicting what types of phone calling card usage are likely to be fraudulent another class of applications looks for associations  for instance  books that tend to be bought together if a customer buys a book  an online bookstore may suggest other associated books if a person buys a camera  the system may suggest accessories that tend to be bought along with cameras a good salesperson is aware of such patterns and exploits them to make additional sales the challenge is to automate the process other types of associations may lead to discovery of causation for instance  discovery of unexpected associations between a newly introduced medicine and cardiac problems led to the finding that the medicine may cause cardiac problems in some people the medicine was then withdrawn from the market associations are an example of descriptive patterns clusters are another example of such patterns for example  over a century ago a cluster of typhoid cases was found around a well  which led to the discovery that the water in the well was contaminated and was spreading typhoid detection of clusters of disease remains important even today 20.4 classification as mentioned in section 20.3  prediction is one of the most important types of data mining we describe classification  study techniques for building one type of classifiers  called decision-tree classifiers  and then study other prediction techniques thesumit67.blogspot.com 20.4 classification 895 abstractly  the classification problem is this  given that items belong to one of several classes  and given past instances  called training instances  of items along with the classes to which they belong  the problem is to predict the class to which a new item belongs the class of the new instance is not known  so other attributes of the instance must be used to predict the class classification can be done by finding rules that partition the given data into disjoint groups for instance  suppose that a credit-card companywants to decide whether or not to give a credit card to an applicant the company has a variety of information about the person  such as her age  educational background  annual income  and current debts  that it can use for making a decision some of this information could be relevant to the credit-worthiness of the applicant  whereas some may not be to make the decision  the company assigns a credit-worthiness level of excellent  good  average  or bad to each of a sample set of current customers according to each customer ? s payment history then  the company attempts to find rules that classify its current customers into excellent  good  average  or bad  on the basis of the information about the person  other than the actual payment history  which is unavailable for new customers   let us consider just two attributes  education level  highest degree earned  and income the rules may be of the following form  ? person p  p.degree = masters and p.income > 75  000 ? p.credit = excellent ? person p  p.degree = bachelors or  p.income = 25  000 and p.income = 75  000  ? p.credit = good similar ruleswould also be present for the other credit-worthiness levels  average and bad   the process of building a classifier starts froma sample of data  called a training set for each tuple in the training set  the class to which the tuple belongs is already known for instance  the training set for a credit-card application may be the existing customers  with their credit-worthiness determined from their payment history the actual data  or population  may consist of all people  including those who are not existing customers there are several ways of building a classifier  as we shall see 20.4.1 decision-tree classifiers the decision-tree classifier is a widely used technique for classification as the name suggests  decision-tree classifiers use a tree ; each leaf node has an associated class  and each internal node has a predicate  or more generally  a function  associated with it figure 20.3 shows an example of a decision tree to classify a new instance  we start at the root and traverse the tree to reach a leaf ; at an internal node we evaluate the predicate  or function  on the data instance  to find which child to go to the process continues until we reach a leaf node for example  if the degree level of a person is masters  and the person ? s thesumit67.blogspot.com 896 chapter 20 data warehousing and mining degree income income income income none bachelors masters doctorate bad average good bad average good excellent < 50k > 100k < 25k > = 25k < 50k > = 50k < 25k > 75k 50 to 100k 25 to 75k figure 20.3 classification tree income is 40k  starting from the root we follow the edge labeled ? masters  ? and from there the edge labeled ? 25k to 75k  ? to reach a leaf the class at the leaf is ? good  ? so we predict that the credit risk of that person is good 20.4.1.1 building decision-tree classifiers the question then is how to build a decision-tree classifier  given a set of training instances themost common way of doing so is to use a greedy algorithm  which works recursively  starting at the root and building the tree downward initially there is only one node  the root  and all training instances are associated with that node at each node  if all  or ? almost all ? training instances associated with the node belong to the same class  then the node becomes a leaf node associated with that class otherwise  a partitioning attribute and partitioning conditions must be selected to create child nodes the data associated with each child node is the set of training instances that satisfy the partitioning condition for that child node in our example  the attribute degree is chosen  and four children  one for each value of degree  are created the conditions for the four children nodes are degree = none  degree = bachelors  degree = masters  and degree = doctorate  respectively the data associated with each child consist of training instances satisfying the condition associated with that child at the node corresponding to masters  the attribute income is chosen  with the range of values partitioned into intervals 0 to 25k  25k to 50k  50k to 75k  and over 75k the data associated with each node consist of training instances with the degree attribute being masters and the income attribute being in each of these ranges  respectively as an optimization  since the thesumit67.blogspot.com 20.4 classification 897 class for the range 25k to 50k and the range 50k to 75k is the same under the node degree = masters  the two ranges have been merged into a single range 25k to 75k 20.4.1.2 best splits intuitively  by choosing a sequence of partitioning attributes,we start with the set of all training instances  which is ? impure ? in the sense that it contains instances from many classes  and ends up with leaves which are ? pure ? in the sense that at each leaf all training instances belong to only one class we shall see shortly how to measure purity quantitatively to judge the benefit of picking a particular attribute and condition for partitioning of the data at a node  we measure the purity of the data at the children resulting from partitioning by that attribute the attribute and condition that result in the maximum purity are chosen the purity of a set s of training instances can be measured quantitatively in several ways suppose there are k classes  and of the instances in s the fraction of instances in class i is pi  one measure of purity  the gini measure  is defined as  gini  s  = 1   k i-1 p2 i when all instances are in a single class  the gini value is 0  while it reaches its maximum  of 1  1/k  if each class has the same number of instances another measure of purity is the entropy measure  which is defined as  entropy  s  =   k i-1 pi log2 pi the entropy value is 0 if all instances are in a single class  and reaches itsmaximum when each class has the same number of instances the entropy measure derives from information theory when a set s is split into multiple sets si  i = 1  2      r  we can measure the purity of the resultant set of sets as  purity  s1  s2      sr  =  r i = 1 | si | | s | purity  si  that is  the purity is the weighted average of the purity of the sets si the above formula can be used with both the gini measure and the entropy measure of purity the information gain due to a particular split of s into si  i = 1  2      r is then  information gain  s   s1  s2      sr   = purity  s   purity  s1  s2      sr  thesumit67.blogspot.com 898 chapter 20 data warehousing and mining splits into fewer sets are preferable to splits into many sets  since they lead to simpler and more meaningful decision trees the number of elements in each of the sets si may also be taken into account ; otherwise  whether a set si has 0 elements or 1 element wouldmake a big difference in the number of sets  although the split is the same for almost all the elements the information content of a particular split can be defined in terms of entropy as  information content  s   s1  s2      sr   =   r i-1 | si | | s | log2 | si | | s | all of this leads to a definition  the best split for an attribute is the one that gives the maximum information gain ratio  defined as  information gain  s   s1  s2      sr   information content  s   s1  s2      sr   20.4.1.3 finding best splits how do we find the best split for an attribute ? how to split an attribute depends on the type of the attribute attributes can be either continuous valued  that is  the values can be ordered in a fashion meaningful to classification  such as age or income  or they can be categorical ; that is  they have no meaningful order  such as department names or country names we do not expect the sort order of department names or country names to have any significance to classification usually attributes that are numbers  integers/reals  are treated as continuous valued while character string attributes are treated as categorical  but this may be controlled by the user of the system in our example  we have treated the attribute degree as categorical  and the attribute income as continuous valued we first consider how to find best splits for continuous-valued attributes for simplicity  we shall consider only binary splits of continuous-valued attributes  that is  splits that result in two children the case of multiway splits is more complicated ; see the bibliographical notes for references on the subject to find the best binary split of a continuous-valued attribute  we first sort the attribute values in the training instances we then compute the information gain obtained by splitting at each value for example  if the training instances have values 1  10  15  and 25 for an attribute  the split points considered are 1  10  and 15 ; in each case values less than or equal to the split point form one partition and the rest of the values form the other partition the best binary split for the attribute is the split that gives the maximum information gain for a categorical attribute  we can have a multiway split  with a child for each value of the attribute this works fine for categorical attributes with only a few distinct values  such as degree or gender however  if the attribute has many distinct values  such as department names in a large company  creating a child for each value is not a good idea in such cases  we would try to combine multiple values into each child  to create a smaller number of children see the bibliographical notes for references on how to do so thesumit67.blogspot.com 20.4 classification 899 procedure growtree  s  partition  s  ; procedure partition  s  if  purity  s  >  p or | s | <  s  then return ; for each attribute a evaluate splits on attribute a ; use best split found  across all attributes  to partition s into s1  s2      sr ; for i = 1  2      r partition  si  ; figure 20.4 recursive construction of a decision tree 20.4.1.4 decision-tree construction algorithm the main idea of decision-tree construction is to evaluate different attributes and different partitioning conditions  and pick the attribute and partitioning condition that results in the maximum information-gain ratio the same procedure works recursively on each of the sets resulting from the split  thereby recursively constructing a decision tree if the data can be perfectly classified  the recursion stops when the purity of a set is 0 however  often data are noisy  or a set may be so small that partitioning it further may not be justified statistically in this case  the recursion stops when the purity of a set is ? sufficiently high  ? and the class of the resulting leaf is defined as the class of the majority of the elements of the set in general  different branches of the tree could grow to different levels figure 20.4 shows pseudocode for a recursive tree-construction procedure  which takes a set of training instances s as parameter the recursion stops when the set is sufficiently pure or the set s is too small for further partitioning to be statistically significant the parameters  p and  s define cutoffs for purity and size ; the system may give them default values  which may be overridden by users there are a wide variety of decision-tree construction algorithms  and we outline the distinguishing features of a few of them see the bibliographical notes for details with very large data sets  partitioning may be expensive  since it involves repeated copying several algorithms have therefore been developed to minimize the i/o and computation cost when the training data are larger than available memory several of the algorithms also prune subtrees of the generated decision tree to reduce overfitting  a subtree is overfitted if it has been so highly tuned to the specifics of the training data that it makes many classification errors on other data a subtree is pruned by replacing it with a leaf node there are different pruning heuristics ; one heuristic uses part of the training data to build the tree and another part of the training data to test it the heuristic prunes a subtree if it finds that misclassification on the test instances would be reduced if the subtree were replaced by a leaf node thesumit67.blogspot.com 900 chapter 20 data warehousing and mining we can generate classification rules from a decision tree  if we so desire for each leaf we generate a rule as follows  the left-hand side is the conjunction of all the split conditions on the path to the leaf  and the class is the class of themajority of the training instances at the leaf an example of such a classification rule is  degree = master s and income > 75000 ? excellent 20.4.2 other types of classifiers there are several types of classifiers other than decision-tree classifiers two types that have been quite useful are neural-net classifiers  bayesian classifiers  and support vector machine classifiers neural-net classifiers use the training data to train artificial neural nets there is a large body of literature on neural nets  and we do not consider them further here bayesian classifiers find the distribution of attribute values for each class in the training data ; when given a new instance d  they use the distribution information to estimate  for each class c j  the probability that instance d belongs to class c j  denoted by p  c j | d   in a manner outlined here the classwithmaximum probability becomes the predicted class for instance d to find the probability p  c j | d  of instance d being in class c j  bayesian classifiers use bayes ? theorem  which says  p  c j | d  = p  d | c j  p  c j  p  d  where p  d | c j  is the probability of generating instance d given class c j  p  c j  is the probability of occurrence of class c j  and p  d  is the probability of instance d occurring of these  p  d  can be ignored since it is the same for all classes p  c j  is simply the fraction of training instances that belong to class c j  for example  let us consider a special case where only one attribute  income  is used for classification  and suppose we need to classify a person whose income is 76000.we assume that income values are broken up into buckets  and assume that the bucket containing 76000 contains values in the range  75000  80000   suppose among instances of class excellent  the probability of income being in  75000  80000  is 0.1  while among instances of class good  the probability of income being in  75000  80000  is 0.05 suppose also that overall 0.1 fraction of people are classified as excellent  and 0.3 are classified as good then  p  d | c j  p  c j  for class excellent is .01  while for class good  it is 0.015 the person would therefore be classified in class good in general  multiple attributes need to be considered for classification then  finding p  d | c j  exactly is difficult  since it requires the distribution of instances of c j  across all combinations of values for the attributes used for classification the number of such combinations  for example of income buckets  with degree values and other attributes  can be very large.with a limited training set used to find the distribution  most combinations would not have even a single training set matching them  leading to incorrect classification decisions to avoid this thesumit67.blogspot.com 20.4 classification 901 problem  as well as to simplify the task of classification  naive bayesian classifiers assume attributes have independent distributions  and thereby estimate  p  d | c j  = p  d1 | c j  * p  d2 | c j  * ? ? ? * p  dn | c j  that is  the probability of the instance d occurring is the product of the probability of occurrence of each of the attribute values di of d  given the class is c j  the probabilities p  di | c j  derive from the distribution of values for each attribute i  for each class c j  this distribution is computed from the training instances that belong to each class c j ; the distribution is usually approximated by a histogram for instance  we may divide the range of values of attribute i into equal intervals  and store the fraction of instances of class c j that fall in each interval given a value di for attribute i  the value of p  di | c j  is simply the fraction of instances belonging to class c j that fall in the interval to which di belongs a significant benefit of bayesian classifiers is that they can classify instances with unknown and null attribute values ? unknown or null attributes are just omitted from the probability computation in contrast  decision-tree classifiers can not meaningfully handle situations where an instance to be classified has a null value for a partitioning attribute used to traverse further down the decision tree the support vector machine  svm  is a type of classifier that has been found to give very accurate classification across a range of applications.we provide some basic intuition about support vector machine classifiers here ; see the references in the bibliographical notes for further information support vector machine classifiers can best be understood geometrically in the simplest case  consider a set of points in a two-dimensional plane  some belonging to class a  and some belonging to class b.we are given a training set of points whose class  aor b  is known  and we need to build a classifier of points  using these training points this situation is illustrated in figure 20.5  where the points in class aare denoted by x marks  while those in class b are denoted by o marks suppose we can draw a line on the plane  such that all points in class a lie to one side and all points in line b lie to the other then  the line can be used to classify new points  whose class we don ? t already know but there may be many possible such lines that can separate points in class a from points in class b a few such lines are shown in figure 20.5 the support vector machine classifier chooses the line whose distance from the nearest point in either class  from the points in the training data set  is maximum this line  called the maximum margin line  is then used to classify other points into class a or b  depending on which side of the line they lie on in figure 20.5  the maximum margin line is shown in bold  while the other lines are shown as dashed lines the above intuition can be generalized to more than two dimensions  allowing multiple attributes to be used for classification ; in this case  the classifier finds a dividing plane  not a line further  by first transforming the input points using certain functions  called kernel functions  support vector machine classifiers can find nonlinear curves separating the sets of points this is important for cases thesumit67.blogspot.com 902 chapter 20 data warehousing and mining figure 20.5 example of a support vector machine classifier where the points are not separable by a line or plane in the presence of noise  some points of one class may lie in the midst of points of the other class in such cases  there may not be any line or meaningful curve that separates the points in the two classes ; then  the line or curve that most accurately divides the points into the two classes is chosen although the basic formulation of support vector machines is for binary classifiers  i.e  those with only two classes  they can be used for classification into multiple classes as follows  if there are n classes  we build n classifiers  with classifier i performing a binary classification  classifying a point either as in class i or not in class i given a point  each classifier i also outputs a value indicating how related a given point is to class i we then apply all n classifiers on a given point  and choose the class for which the relatedness value is the highest 20.4.3 regression regression deals with the prediction of a value  rather than a class given values for a set of variables  x1  x2      xn  we wish to predict the value of a variable y for instance  we could treat the level of education as a number and income as another number  and  on the basis of these two variables  we wish to predict the likelihood of default  which could be a percentage chance of defaulting  or the amount involved in the default one way is to infer coefficients a0  a1  a2      an such that  y = a0 + a1 * x1 + a2 * x2 + ? ? ? + an * xn finding such a linear polynomial is called linear regression in general  we wish to find a curve  defined by a polynomial or other formula  that fits the data ; the process is also called curve fitting thesumit67.blogspot.com 20.4 classification 903 the fit may be only approximate  because of noise in the data or because the relationship is not exactly a polynomial  so regression aims to find coefficients that give the best possible fit there are standard techniques in statistics for finding regression coefficients we do not discuss these techniques here  but the bibliographical notes provide references 20.4.4 validating a classifier it is important to validate a classifier  that is  to measure its classification error rate  before deciding to use it for an application consider an example of a classification problemwhere a classifier has to predict  based on some inputs  the exact inputs are not relevant here   whether a person is suffering from a particular disease x or not a positive prediction says that the person has the disease  and a negative prediction says the person does not have the disease  the terminology of positive/negative prediction can be used for any binary classification problem  not just disease classification  a set of test caseswhere the outcome is already known  in our example  cases where it is already known whether or not the person actually has the disease  is used to measure the quality  that is  the error rate  of the classifier a true positive is a case where the prediction was positive  and the person actually had the disease,while a false positive is a case where the predictionwas positive  but the person did not have the disease true negative and false negative are defined similarly for the case where the prediction was negative given a set of test cases  let t pos  f pos  t neg and f neg denote the number of true positives  false positives  true negatives and false negatives generated let pos and neg denote the actual number of positives and negatives  it is easy to see that pos = t pos + f neg  and neg = f pos + t neg   the quality of classification can be measured in several different ways  1 accuracy  defined as  t pos + t neg  /  pos + neg   that is  the fraction of the time when the classifier gives the correct classification 2 recall  also known as sensitivity  defined as t pos/pos  that is  how many of the actual positive cases are classified as positive 3 precision  defined as t pos/  t pos + f pos   that is  how often the positive prediction is correct 4 specificity  defined as t neg/neg which of these measures should be used for a specific application depends on the needs of that application for example  a high recall is important for a screening test  which is to be followed up by a more precise test  so that patients with the disease are not missed out in contrast a researcher who wants to find a few actual patients of the disease for further follow up  but is not interested in finding all patients  may value high precision over recall different classifiers may be appropriate for each of these applications this issue is explored further in exercise 20.5 thesumit67.blogspot.com 904 chapter 20 data warehousing and mining a set of test cases where the outcome is already known can be used either to train or to measure the quality the classifier it is a bad idea to use exactly the same set of test cases to train as well as to measure the quality of the classifier  since the classifier has already seen the correct classification of the test cases during training ; this can lead to artificially high measures of quality the quality of a classifiermust therefore be measured on test cases that have not been seen during training therefore  a subset of the available test cases is used for training and a disjoint subset is used for validation in cross validation  the available test cases are divided into k parts numbered 1 to k  fromwhich k different test sets are created as follows  test set i uses the ith part for validation  after training the classifier using the other k-1parts.the results  t pos  f pos  etc  fromall k test sets are added up before computing the quality measures cross validation provides much more accurate measures than merely partitioning the data into a single training and a single test set 20.5 association rules retail shops are often interested in associations between different items that people buy examples of such associations are  ? someone who buys bread is quite likely also to buy milk ? a person who bought the book database system concepts is quite likely also to buy the book operating system concepts association information can be used in several ways when a customer buys a particular book  an online shop may suggest associated books a grocery shop may decide to place bread close to milk  since they are often bought together  to help shoppers finish their task faster or  the shop may place them at opposite ends of a row  and place other associated items in between to tempt people to buy those items as well  as the shoppers walk from one end of the row to the other a shop that offers discounts on one associated item may not offer a discount on the other  since the customer will probably buy the other anyway an example of an association rule is  bread ? milk in the context of grocery-store purchases  the rule says that customers who buy bread also tend to buy milk with a high probability an association rule must have an associated population  the population consists of a set of instances in the grocery-store example  the population may consist of all grocery-store purchases ; each purchase is an instance in the case of a bookstore  the population may consist of all people who made purchases  regardless of when they made a purchase each customer is an instance in the bookstore example  the analyst has thesumit67.blogspot.com 20.5 association rules 905 decided that when a purchase is made is not significant  whereas for the grocerystore example  the analyst may have decided to concentrate on single purchases  ignoring multiple visits by the same customer rules have an associated support  as well as an associated confidence these are defined in the context of the population  ? support is a measure of what fraction of the population satisfies both the antecedent and the consequent of the rule for instance  suppose only 0.001 percent of all purchases include milk and screwdrivers the support for the rule  milk ? screwdrivers is low the rule may not even be statistically significant ? perhaps there was only a single purchase that included both milk and screwdrivers businesses are usually not interested in rules that have low support  since they involve few customers  and are not worth bothering about on the other hand  if 50 percent of all purchases involve milk and bread  then support for rules involving bread and milk  and no other item  is relatively high  and such rules may be worth attention exactly what minimum degree of support is considered desirable depends on the application ? confidence is a measure of how often the consequent is true when the antecedent is true for instance  the rule  bread ? milk has a confidence of 80 percent if 80 percent of the purchases that include bread also include milk a rule with a low confidence is not meaningful in business applications  rules usually have confidences significantly less than 100 percent  whereas in other domains  such as in physics  rules may have high confidences note that the confidence of bread ? milk may be very different from the confidence of milk ? bread  although both have the same support to discover association rules of the form  i1  i2      in ? i0 we first find sets of items with sufficient support  called large itemsets in our example  we find sets of items that are included in a sufficiently large number of instances we shall see shortly how to compute large itemsets for each large itemset  we then output all rules with sufficient confidence that involve all and only the elements of the set for each large itemset s  we output a rule s  s ? s for every subset s ? s  provided s  s ? s has sufficient confidence ; the confidence of the rule is given by support of s divided by support of s thesumit67.blogspot.com 906 chapter 20 data warehousing and mining we now consider how to generate all large itemsets if the number of possible sets of items is small  a single pass over the data suffices to detect the level of support for all the sets a count  initialized to 0  is maintained for each set of items when a purchase record is fetched  the count is incremented for each set of items such that all items in the set are contained in the purchase for instance  if a purchase included items a  b  and c  counts would be incremented for  a    b    c    a  b    b  c    a  c   and  a  b  c   those sets with a sufficiently high count at the end of the pass correspond to items that have a high degree of association the number of sets grows exponentially,making the procedure just described infeasible if the number of items is large luckily  almost all the sets would normally have very low support ; optimizations have been developed to eliminate most such sets from consideration these techniques use multiple passes on the database  considering only some sets in each pass in the a priori technique for generating large itemsets  only sets with single items are considered in the first pass in the second pass  sets with two items are considered  and so on at the end of a pass  all sets with sufficient support are output as large itemsets sets found to have too little support at the end of a pass are eliminated once a set is eliminated  none of its supersets needs to be considered in other words  in pass i we need to count only supports for sets of size i such that all subsets of the set have been found to have sufficiently high support ; it suffices to test all subsets of size i  1 to ensure this property at the end of some pass i  we would find that no set of size i has sufficient support  so we do not need to consider any set of size i + 1 computation then terminates 20.6 other types of associations using plain association rules has several shortcomings one of the major shortcomings is that many associations are not very interesting  since they can be predicted for instance  if many people buy cereal and many people buy bread  we can predict that a fairly large number of people would buy both  even if there is no connection between the two purchases in fact  even if buying cereal has a mild negative influence on buying bread  that is  customers who buy cereal tend to purchase bread less often than the average customer   the association between cereal and bread may still have a high support what would be more interesting is if there is a deviation from the expected co-occurrence of the two in statistical terms  we look for correlations between items ; correlations can be positive  in that the co-occurrence is higher than would have been expected  or negative  in that the items co-occur less frequently than predicted thus  if purchase of bread is not correlated with cereal  it would not be reported  even if there was a strong association between the two there are standard measures of correlation  widely used in the area of statistics see a standard textbook on statistics for more information about correlations another important class of data-mining applications is sequence associations  or sequence correlations   time-series data  such as stock prices on a sequence thesumit67.blogspot.com 20.7 clustering 907 of days  form an example of sequence data stock-market analysts want to find associations among stock-market price sequences an example of such an association is the following rule  ? whenever bond rates go up  the stock prices go down within 2 days ? discovering such associations between sequences can help us to make intelligent investment decisions see the bibliographical notes for references to research on this topic deviations from temporal patterns are often interesting for instance  if a company has been growing at a steady rate each year  a deviation from the usual growth rate is surprising if sales of winter clothes go down in summer  it is not surprising  since we can predict it from past years ; a deviation that we could not have predicted from past experiencewould be considered interesting.mining techniques can find deviations fromwhat onewould have expected on the basis of past temporal or sequential patterns see the bibliographical notes for references to research on this topic 20.7 clustering intuitively  clustering refers to the problem of finding clusters of points in the given data the problem of clustering can be formalized from distance metrics in several ways one way is to phrase it as the problem of grouping points into k sets  for a given k  so that the average distance of points from the centroid of their assigned cluster is minimized.2 another way is to group points so that the average distance between every pair of points in each cluster is minimized there are other definitions too ; see the bibliographical notes for details but the intuition behind all these definitions is to group similar points together in a single set another type of clustering appears in classification systems in biology  such classification systems do not attempt to predict classes ; rather they attempt to cluster related items together  for instance  leopards and humans are clustered under the class mammalia  while crocodiles and snakes are clustered under reptilia both mammalia and reptilia come under the common class chordata the clustering of mammalia has further subclusters  such as carnivora and primates we thus have hierarchical clustering given characteristics of different species  biologists have created a complex hierarchical clustering scheme grouping related species together at different levels of the hierarchy hierarchical clustering is also useful in other domains ? for clustering documents  for example internet directory systems  such as the yahoo ! directory  cluster related documents in a hierarchical fashion  see section 21.9   hierarchical clustering algorithms can be classified as agglomerative clustering algorithms  which start by building small clusters and then create higher levels  or divisive 2the centroid of a set of points is defined as a point whose coordinate on each dimension is the average of the coordinates of all the points of that set on that dimension for example in two dimensions  the centroid of a set of points   x1  y1    x2  y2        xn  yn   is given by   n i = 1 xi n   n i = 1 yi n   thesumit67.blogspot.com 908 chapter 20 data warehousing and mining clustering algorithms  which first create higher levels of the hierarchical clustering  then refine each resulting cluster into lower-level clusters the statistics community has studied clustering extensively database research has provided scalable clustering algorithms that can cluster very large data sets  that may not fit in memory   the birch clustering algorithm is one such algorithm intuitively  data points are inserted into a multidimensional tree structure  based on r-trees  described in section 25.3.5.3   and guided to appropriate leaf nodes on the basis of nearness to representative points in the internal nodes of the tree nearby points are thus clustered together in leaf nodes  and summarized if there are more points than fit in memory the result of this first phase of clustering is to create a partially clustered data set that fits in memory standard clustering techniques can then be executed on the in-memory data to get the final clustering see the bibliographical notes for references to the birch algorithm  and other techniques for clustering  including algorithms for hierarchical clustering an interesting application of clustering is to predict what new movies  or books or music  a person is likely to be interested in  on the basis of  1 the person ? s past preferences in movies 2 other people with similar past preferences 3 the preferences of such people for new movies one approach to this problem is as follows  to find people with similar past preferences we create clusters of people based on their preferences for movies the accuracy of clustering can be improved by previously clustering movies by their similarity  so even if people have not seen the samemovies  if they have seen similar movies they would be clustered together we can repeat the clustering  alternately clustering people  then movies  then people  and so on until we reach an equilibrium given a new user  we find a cluster of users most similar to that user  on the basis of the user ? s preferences for movies already seen we then predict movies in movie clusters that are popular with that user ? s cluster as likely to be interesting to the new user in fact  this problem is an instance of collaborative filtering  where users collaborate in the task of filtering information to find information of interest 20.8 other forms of data mining text mining applies data-mining techniques to textual documents for instance  there are tools that form clusters on pages that a user has visited ; this helps users when they browse the history of their browsing to find pages they have visited earlier the distance between pages can be based  for instance  on common words in the pages  see section 21.2.2   another application is to classify pages into a web directory automatically  according to their similarity with other pages  see section 21.9   thesumit67.blogspot.com 20.9 summary 909 data-visualization systems help users to examine large volumes of data  and to detect patterns visually visual displays of data ? such as maps  charts  and other graphical representations ? allow data to be presented compactly to users a single graphical screen can encode as much information as a far larger number of text screens for example  if the user wants to find out whether production problems at plants are correlated to the locations of the plants  the problem locations can be encoded in a special color ? say  red ? on a map the user can then quickly discover locationswhere problems are occurring the user may then form hypotheses about why problems are occurring in those locations  and may verify the hypotheses quantitatively against the database as another example  information about values can be encoded as a color  and can be displayed with as little as one pixel of screen area to detect associations between pairs of items  we can use a two-dimensional pixel matrix  with each row and each column representing an item the percentage of transactions that buy both items can be encoded by the color intensity of the pixel itemswith high association will show up as bright pixels in the screen ? easy to detect against the darker background data-visualization systems do not automatically detect patterns  but they provide system support for users to detect patterns since humans are very good at detecting visual patterns  data visualization is an important component of data mining 20.9 summary ? decision-support systems analyze online data collected by transaction-processing systems  to help people make business decisions since most organizations are extensively computerized today  a very large body of information is available for decision support decision-support systems come in various forms  including olap systems and data-mining systems ? data warehouses help gather and archive important operational data.warehouses are used for decision support and analysis on historical data  for instance  to predict trends data cleansing from input data sources is often a major task in data warehousing.warehouse schemas tend to be multidimensional  involving one or a few very large fact tables and several much smaller dimension tables ? column-oriented storage systems provide good performance for many data warehousing applications ? data mining is the process of semiautomatically analyzing large databases to find useful patterns there are a number of applications of data mining  such as prediction of values based on past examples  finding of associations between purchases  and automatic clustering of people and movies ? classification deals with predicting the class of test instances by using attributes of the test instances  based on attributes of training instances  and thesumit67.blogspot.com 910 chapter 20 data warehousing and mining the actual class of training instances there are several types of classifiers  such as  ? decision-tree classifiers  which perform classification by constructing a tree based on training instances with leaves having class labels the tree is traversed for each test instance to find a leaf  and the class of the leaf is the predicted class several techniques are available to construct decision trees  most of them based on greedy heuristics ? bayesian classifiers are simpler to construct than decision-tree classifiers  and they work better in the case of missing/null attribute values ? the support vector machine is another widely used classification technique ? association rules identify items that co-occur frequently  for instance  items that tend to be bought by the same customer correlations look for deviations from expected levels of association ? other types of data mining include clustering  text mining  and data visualization review terms ? decision-support systems ? statistical analysis ? data warehousing ? gathering data ? source-driven architecture ? destination-driven architecture ? data cleansing merge ? purge householding ? extract  transform  load  etl  ? warehouse schemas ? fact table ? dimension tables ? star schema ? column-oriented storage ? data mining ? prediction ? associations ? classification ? training data ? test data ? decision-tree classifiers ? partitioning attribute ? partitioning condition ? purity gini measure entropy measure ? information gain ? information content ? information gain ratio ? continuous-valued attribute thesumit67.blogspot.com practice exercises 911 ? categorical attribute ? binary split ? multiway split ? overfitting ? bayesian classifiers ? bayes ? theorem ? naive bayesian classifiers ? support vector machine  svm  ? regression ? linear regression ? curve fitting ? validation ? accuracy ? recall ? precision ? specificity ? cross validation ? association rules ? population ? support ? confidence ? large itemsets ? other types of associations ? clustering ? hierarchical clustering ? agglomerative clustering ? divisive clustering ? text mining ? data visualization practice exercises 20.1 describe benefits and drawbacks of a source-driven architecture for gathering of data at a data warehouse  as compared to a destination-driven architecture 20.2 why is column-oriented storage potentially advantageous in a database system that supports a data warehouse ? 20.3 suppose that there are two classification rules  one that says that people with salaries between $ 10,000 and $ 20,000 have a credit rating of good  and another that says that people with salaries between $ 20,000 and $ 30,000 have a credit rating of good under what conditions can the rules be replaced  without any loss of information  by a single rule that says people with salaries between $ 10,000 and $ 30,000 have a credit rating of good ? 20.4 consider the schemadepicted in figure 20.2 give an sql query to summarize sales numbers and price by store and date  along with the hierarchies on store and date 20.5 consider a classification problem where the classifier predicts whether a person has a particular disease suppose that 95 % of the people tested do thesumit67.blogspot.com 912 chapter 20 data warehousing and mining not suffer from the disease  that is  pos corresponds to 5 % and neg to 95 % of the test cases  consider the following classifiers  ? classifierc1 which always predicts negative  a rather useless classifier of course   ? classifier c2 which predicts positive in 80 % of the cases where the person actually has the disease  but also predicts positive in 5 % of the cases where the person does not have the disease ? classifier c3 which predicts positive in 95 % of the cases where the person actually has the disease  but also predicts positive in 20 % of the cases where the person does not have the disease given the above classifiers  answer the following questions a for each of the above classifiers  compute the accuracy  precision  recall and specificity b if you intend to use the results of classification to perform further screening for the disease  how would you choose between the classifiers c on the other hand  if you intend to use the result of classification to start medication  where the medication could have harmful effects if given to someone who does not have the disease  how would you choose between the classifiers ? exercises 20.6 draw a diagram that shows how the classroom relation of our university example as shown in appendix a would be stored under a columnoriented storage structure 20.7 explain why the nested-loops join algorithm  see section 12.5.1  would work poorly on database stored in a column-oriented manner describe an alternative algorithm that would work better and explain why your solution is better 20.8 construct a decision-tree classifier with binary splits at each node  using tuples in relation r  a  b,c  shown below as training data ; attribute c denotes the class show the final tree  and with each node show the best split for each attribute along with its information gain value  1  2  a    2  1  a    2  5  b    3  3  b    3  6  b    4  5  b    5  5  c    6  3  b    6  7  c  20.9 suppose half of all the transactions in a clothes shop purchase jeans  and one third of all transactions in the shop purchase t-shirts suppose also thesumit67.blogspot.com tools 913 that half of the transactions that purchase jeans also purchase t-shirts write down all the  nontrivial  association rules you can deduce from the above information  giving support and confidence of each rule 20.10 consider the problem of finding large itemsets a describe how to find the support for a given collection of itemsets by using a single scan of the data assume that the itemsets and associated information  such as counts  will fit in memory b suppose an itemset has support less than j show that no superset of this itemset can have support greater than or equal to j 20.11 create a small example of a set of transactions showing that although many transactions contain two items  that is  the itemset containing the two items has a high support  purchase of one of the items may have a negative correlation with purchase of the other 20.12 the organization of parts  chapters  sections  and subsections in a book is related to clustering explain why  and to what form of clustering 20.13 suggest how predictive mining techniques can be used by a sports team  using your favorite sport as an example tools a variety of tools are available for each of the applications we have studied in this chapter most database vendors provide olap tools as part of their database systems  or as add-on applications these include olap tools from microsoft corp  sap  ibm and oracle the mondrian olap server is a public-domain olap server many companies also provide analysis tools for specific applications  such as customer relationship management major database vendors also offer data warehousing products coupled with their database systems these provide support functionality for data modeling  cleansing  loading  and querying the web site www.dwinfocenter.org provides information on data-warehousing products there is also a wide variety of general-purpose data-mining tools  including data-mining suites fromthe sas institute  ibm intelligent miner  and oracle there are also several open-source data-mining tools  such as the widely used weka  and rapidminer the open-source business intelligence suite pentaho has several components including an etl tool  the mondrian olap server  and data-mining tools based onweka a good deal of expertise is required to apply general-purpose mining tools for specific applications as a result  a large number of mining tools have been developed to address specialized applications the web site www.kdnuggets.com provides an extensive directory of mining software  solutions  publications  and so on thesumit67.blogspot.com 914 chapter 20 data warehousing and mining bibliographical notes definitions of statistical functions can be found in standard statistics textbooks such as bulmer  1979  and ross  1999   poe  1995  and mattison  1996  provide textbook coverage of data warehousing zhuge et al  1995  describes view maintenance in a data-warehousing environment chaudhuri et al  2003  describes techniques for fuzzy matching for data cleaning  while sarawagi et al  2002  describes a system for deduplication using active learning techniques abadi et al  2008  presents a comparison of column-oriented and roworiented storage  including issues related to query processing and optimization witten and frank  1999  and han and kamber  2000  provide textbook coverage of data mining mitchell  1997  is a classic textbook on machine learning  and covers classification techniques in detail fayyad et al  1995  presents an extensive collection of articles on knowledge discovery and data mining kohavi and provost  2001  presents a collection of articles on applications of data mining to electronic commerce agrawal et al  1993b  provides an early overview of data mining in databases algorithms for computing classifiers with large training sets are described by agrawal et al  1992  and shafer et al  1996  ; the decision-tree construction algorithm described in this chapter is based on the sprint algorithm of shafer et al  1996   cortes and vapnik  1995  introduced several key results on support vector machines  while cristianini and shawe-taylor  2000  provides textbook coverage of support vector machines agrawal et al  1993a  introduced the notion of association rules  and an efficient algorithm for association rule mining was presented by agrawal and srikant  1994   algorithms for mining of different forms of association rules are described by srikant and agrawal  1996a  and srikant and agrawal  1996b   chakrabarti et al  1998  describes techniques for mining surprising temporal patterns techniques for integrating data cubes with data mining are described by sarawagi  2000   clustering has long been studied in the area of statistics  and jain and dubes  1988  provides textbook coverage of clustering ngandhan  1994  describes spatial clustering techniques clustering techniques for large datasets are described by zhang et al  1996   breese et al  1998  provides an empirical analysis of different algorithms for collaborative filtering techniques for collaborative filtering of news articles are described by konstan et al  1997   chakrabarti  2002  and manning et al  2008  provide textbook description of information retrieval  including extensive coverage of data-mining tasks related to textual and hypertext data  such as classification and clustering chakrabarti  2000  provides a survey of hypertext mining techniques such as hypertext classification and clustering thesumit67.blogspot.com chapter21 information retrieval textual data is unstructured  unlike the rigidly structured data in relational databases the term information retrieval generally refers to the querying of unstructured textual data information-retrieval systems have much in common with database systems  in particular  the storage and retrieval of data on secondary storage however  the emphasis in the field of information systems is different fromthat in database systems  concentrating on issues such as querying based on keywords ; the relevance of documents to the query ; and the analysis  classification  and indexing of documents web search engines today go beyond the paradigm of retrieving documents  and address broader issues such as what information to display in response to a keyword query  to satisfy the information needs of a user 21.1 overview the field of information retrieval has developed in parallel with the field of databases in the traditional model used in the field of information retrieval  information is organized into documents  and it is assumed that there is a large number of documents data contained in documents are unstructured  without any associated schema the process of information retrieval consists of locating relevant documents  on the basis of user input  such as keywords or example documents theweb provides a convenient way to get to  and to interact with  information sources across the internet however  a persistent problem facing the web is the explosion of stored information  with little guidance to help the user to locate what is interesting information retrieval has played a critical role in making the web a productive and useful tool  especially for researchers traditional examples of information-retrieval systems are online library catalogs and online document-management systems such as those that store newspaper articles the data in such systems are organized as a collection of documents ; a newspaper article and a catalog entry  in a library catalog  are examples of documents in the context of the web  usually each html page is considered to be a document 915 thesumit67.blogspot.com 916 chapter 21 information retrieval a user of such a system may want to retrieve a particular document or a particular class of documents the intended documents are typically described by a set of keywords ? for example  the keywords ? database system ? may be used to locate books on database systems  and the keywords ? stock ? and ? scandal ? may be used to locate articles about stock-market scandals documents have associated with them a set of keywords  and documentswhose keywords contain those supplied by the user are retrieved keyword-based information retrieval can be used not only for retrieving textual data  but also for retrieving other types of data  such as video and audio data  that have descriptive keywords associated with them for instance  a video movie may have associated with it keywords such as its title  director  actors  and genre  while an image or video clip may have tags  which are keywords describing the image or video clip  associated with it there are several differences between this model and the models used in traditional database systems ? database systems dealwith several operations that are not addressed in information retrieval systems for instance  database systems deal with updates and with the associated transactional requirements of concurrency control and durability these matters are viewed as less important in information systems similarly  database systems deal with structured information organized with relatively complex data models  such as the relational model or object-oriented data models   whereas information-retrieval systems traditionally have used a much simpler model  where the information in the database is organized simply as a collection of unstructured documents ? information-retrieval systems deal with several issues that have not been addressed adequately in database systems for instance  the field of information retrieval has dealt with the issue of querying collections of unstructured documents  focusing on issues such as keyword queries  and of ranking of documents on estimated degree of relevance of the documents to the query in addition to simple keyword queries that are just sets ofwords  informationretrieval systems typically allow query expressions formed using keywords and the logical connectives and  or  and not for example  a user could ask for all documents that contain the keywords ? motorcycle and maintenance  ? or documents that contain the keywords ? computer or microprocessor  ? or even documents that contain the keyword ? computer but not database ? a query containing keywords without any of the above connectives is assumed to have ands implicitly connecting the keywords in full text retrieval  all the words in each document are considered to be keywords for unstructured documents  full text retrieval is essential since there may be no information about what words in the document are keywords we shall use the word term to refer to the words in a document  since all words are keywords thesumit67.blogspot.com 21.2 relevance ranking using terms 917 in its simplest form  an information-retrieval system locates and returns all documents that contain all the keywords in the query  if the query has no connectives ; connectives are handled as you would expect more-sophisticated systems estimate relevance of documents to a query so that the documents can be shown in order of estimated relevance they use information about term occurrences  as well as hyperlink information  to estimate relevance information-retrieval systems  as exemplified by web search engines  have today evolved beyond just retrieving documents based on a ranking scheme today  search engines aim to satisfy a user ? s information needs  by judging what topic a query is about  and displaying not onlyweb pages judged as relevant  but also displaying other kinds of information about the topic for example  given a query term ? cricket ?  a search engine may display scores from ongoing or recent cricketmatches  rather than just display top-ranked documents related to cricket as another example  in response to a query ? new york ?  a search engine may show a map of new york  and images of new york  in addition to web pages related to new york 21.2 relevance ranking using terms the set of all documents that satisfy a query expression may be very large ; in particular  there are billions of documents on theweb  and most keyword queries on a web search engine find hundreds of thousands of documents containing the keywords full text retrieval makes this problem worse  each document may contain many terms  and even terms that are mentioned only in passing are treated equivalentlywith documentswhere the term is indeed relevant irrelevant documents may be retrieved as a result information-retrieval systems therefore estimate relevance of documents to a query  and return only highly ranked documents as answers relevance ranking is not an exact science  but there are some well-accepted approaches 21.2.1 ranking using tf-idf the first question to address is  given a particular term t  how relevant is a particular document d to the term one approach is to use the the number of occurrences of the term in the document as a measure of its relevance  on the assumption that relevant terms are likely to be mentioned many times in a document just counting the number of occurrences of a term is usually not a good indicator  first  the number of occurrences depends on the length of the document  and second  a document containing 10 occurrences of a term may not be 10 times as relevant as a document containing one occurrence one way of measuring tf  d  t   the relevance of a document d to a term t  is  tf  d  t  = log  1 + n  d  t  n  d   thesumit67.blogspot.com 918 chapter 21 information retrieval where n  d  denotes the number of terms in the document and n  d  t  denotes the number of occurrences of term t in the document d observe that this metric takes the length of the document into account the relevance grows with more occurrences of a term in the document  although it is not directly proportional to the number of occurrences many systems refine the above metric by using other information for instance  if the term occurs in the title  or the author list  or the abstract  the document would be considered more relevant to the term similarly  if the first occurrence of a term is late in the document  the document may be considered less relevant than if the first occurrence is early in the document the above notions can be formalized by extensions of the formula we have shown for tf  d  t   in the information retrieval community  the relevance of a document to a term is referred to as term frequency  tf   regardless of the exact formula used aquery q may contain multiple keywords the relevance of a document to a query with two or more keywords is estimated by combining the relevance measures of the document to each keyword.asimpleway of combining the measures is to add them up however  not all terms used as keywords are equal suppose a query uses two terms  one of which occurs frequently  such as ? database ?  and another that is less frequent  such as ? silberschatz ?  a document containing ? silberschatz ? but not ? database ? should be ranked higher than a document containing the term ? database ? but not ? silberschatz ?  to fix the above problem  weights are assigned to terms using the inverse document frequency  idf   defined as  idf  t  = 1 n  t  where n  t  denotes the number of documents  among those indexed by the system  that contain the term t the relevance of a document d to a set of terms q is then defined as  r  d  q  =  t ? q tf  d  t  * idf  t  this measure can be further refined if the user is permitted to specify weights w  t  for terms in the query  inwhich case the user-specifiedweights are also taken into account by multiplying tf  t  by w  t  in the above formula the above approach of using term frequency and inverse document frequency as a measure of the relevance of a document is called the tf ? idf approach almost all text documents  in english  contain words such as ? and  ? ? or  ? ? a  ? and so on  and hence these words are useless for querying purposes since their inverse document frequency is extremely low information-retrieval systems define a set of words  called stop words  containing 100 or so of themost common words  and ignore these words when indexing a document such words are not used as keywords  and are discarded if present in the keywords supplied by the user thesumit67.blogspot.com 21.2 relevance ranking using terms 919 another factor taken into account when a query contains multiple terms is the proximity of the terms in the document if the terms occur close to each other in the document  the document would be ranked higher than if they occur far apart the formula for r  d  q  can be modified to take proximity of the terms into account given a query q  the job of an information-retrieval system is to return documents in descending order of their relevance to q since there may be a very large number of documents that are relevant  information-retrieval systems typically return only the first few documents with the highest degree of estimated relevance  and permit users to interactively request further documents 21.2.2 similarity-based retrieval certain information-retrieval systems permit similarity-based retrieval here  the user can give the system document a  and ask the system to retrieve documents that are ? similar ? to a the similarity of a document to another may be defined  for example  on the basis of common terms one approach is to find k terms in awith highest values of tf  a  t  * idf  t   and to use these k terms as a query to find relevance of other documents the terms in the query are themselves weighted by tf  a  t  * idf  t   more generally  the similarity of documents is defined by the cosine similarity metric let the terms occurring in either of the two documents be t1  t2      tn let r  d  t  = tf  d  t  * idf  t   then the cosine similarity metric between documents d and e is defined as   n i = 1 r  d  ti  r  e  ti    n i = 1 r  d  ti  2   n i = 1 r  e  ti  2 you can easily verify that the cosine similarity metric of a document with itself is 1  while that between two documents that do not share any terms is 0 the name ? cosine similarity ? comes from the fact that the above formula computes the cosine of the angle between two vectors  one representing each document  defined as follows  let there be n words overall across all the documents being considered an n-dimensional space is defined  with each word as one of the dimensions a document d is represented by a point in this space  with the value of the ith coordinate of the point being r  d  ti   the vector for document d connects the origin  all coordinates = 0  to the point representing the document the model of documents as points and vectors in an n-dimensional space is called the vector space model if the set of documents similar to a query document a is large  the system may present the user a few of the similar documents  allow the user to choose the most relevant few  and start a new search based on similarity to aand to the chosen documents the resultant set of documents is likely to be what the user intended to find this idea is called relevance feedback relevance feedback can also be used to help users find relevant documents from a large set of documents matching the given query keywords in such a thesumit67.blogspot.com 920 chapter 21 information retrieval situation  usersmay be allowed to identify one or a fewof the returned documents as relevant ; the system then uses the identified documents to find other similar ones the resultant set of documents is likely to be what the user intended to find an alternative to the relevance feedback approach is to require users to modify the query by adding more keywords ; relevance feedback can be easier to use  in addition to giving a better final set of documents as the answer in order to show the user a representative set of documentswhen the number of documents is very large  a search system may cluster the documents  based on their cosine similarity then  a few documents from each cluster may be shown  so that more than one cluster is represented in the set of answers clustering was described earlier in section 20.7  and several techniques have been developed to cluster sets of documents see the bibliographical notes for references to more information on clustering as a special case of similarity  there are often multiple copies of a document on the web ; this could happen  for example  if a web site mirrors the contents of another web site in this case  it makes no sense to return multiple copies of a highly ranked document as separate answers ; duplicates should be detected  and only one copy should be returned as an answer 21.3 relevance using hyperlinks early web-search engines ranked documents by using only tf ? idf based relevance measures like those described in section 21.2 however  these techniques had some limitations when used on very large collections of documents  such as the set of all web pages in particular  many web pages have all the keywords specified in a typical search engine query ; further  some of the pages that users want as answers often have just a few occurrences of the query terms  and would not get a very high tf ? idf score however  researchers soon realized that web pages have very important information that plain text documents do not have  namely hyperlinks these can be exploited to get better relevance ranking ; in particular  the relevance ranking of a page is influenced greatly by hyperlinks that point to the page in this section  we study how hyperlinks are used for ranking of web pages 21.3.1 popularity ranking the basic idea of popularity ranking  also called prestige ranking  is to find pages that are popular  and to rank themhigher than other pages that contain the specified keywords since most searches are intended to find information from popular pages  ranking such pages higher is generally a good idea for instance  the term ? google ? may occur in vast numbers of pages  but the page google.com is the most popular among the pages that contain the term ? google ? the page google.com should therefore be ranked as the most relevant answer to a query consisting of the term ? google ?  thesumit67.blogspot.com 21.3 relevance using hyperlinks 921 traditional measures of relevance of a page such as the tf ? idfbasedmeasures that we saw in section 21.2  can be combined with the popularity of the page to get an overall measure of the relevance of the page to the query pages with the highest overall relevance value are returned as the top answers to a query this raises the question of how to define and how to find the popularity of a page one way would be to find how many times a page is accessed and use the number as a measure of the site ? s popularity however  getting such information is impossible without the cooperation of the site  and while a few sites may be persuaded to reveal this information  it is difficult to get it for all sites further  sites may lie about their access frequency  in order to get ranked higher a very effective alternative is to use hyperlinks to a page as a measure of its popularity many people have bookmark files that contain links to sites that they use frequently sites that appear in a large number of bookmark files can be inferred to be very popular sites bookmark files are usually stored privately and not accessible on the web however  many users do maintain web pages with links to their favorite web pages many web sites also have links to other related sites  which can also be used to infer the popularity of the linked sites a web search engine can fetch web pages  by a process called crawling  which we describe in section 21.7   and analyze them to find links between the pages a first solution to estimating the popularity of a page is to use the number of pages that link to the page as a measure of its popularity however  this by itself has the drawback that many sites have a number of useful pages  yet external links often point only to the root page of the site the root page in turn has links to other pages in the site these other pages would then be wrongly inferred to be not very popular  and would have a low ranking in answering queries one alternative is to associate popularity with sites  rather than with pages all pages at a site then get the popularity of the site  and pages other than the root page of a popular site would also benefit from the site ? s popularity however  the question of what constitutes a site then arises in general the internet address prefix of a page urlwould constitute the site corresponding to the page.however  there are many sites that host a large number of mostly unrelated pages  such as home page servers in universities and web portals such as groups.yahoo.com or groups.google.com for such sites  the popularity of one part of the site does not imply popularity of another part of the site a simpler alternative is to allow transfer of prestige from popular pages to pages to which they link under this scheme  in contrast to the one-person onevote principles of democracy  a link from a popular page x to a page y is treated as conferring more prestige to page y than a link from a not-so-popular page z.1 this notion of popularity is in fact circular  since the popularity of a page is defined by the popularity of other pages  and there may be cycles of links between pages however  the popularity of pages can be defined by a system of simultaneous linear equations  which can be solved by matrix manipulation 1this is similar in some sense to giving extra weight to endorsements of products by celebrities  such as film stars   so its significance is open to question  although it is effective and widely used in practice thesumit67.blogspot.com 922 chapter 21 information retrieval techniques the linear equations can be defined in such a way that they have a unique and well-defined solution it is interesting to note that the basic idea underlying popularity ranking is actually quite old  and first appeared in a theory of social networking developed by sociologists in the 1950s in the social-networking context  the goal was to define the prestige of people for example  the president of the united states has high prestige since a large number of people know him if someone is known by multiple prestigious people  then she also has high prestige  even if she is not known by as large a number of people the use of a set of linear equations to define the popularity measure also dates back to this work 21.3.2 pagerank the web search engine google introduced pagerank  which is a measure of popularity of a page based on the popularity of pages that link to the page using the pagerank popularity measure to rank answers to a query gave results so much better than previously used ranking techniques that google became the most widely used search engine  in a rather short period of time pagerank can be understood intuitively using a random walk model suppose a person browsing the web performs a random walk  traversal  on web pages as follows  the first step starts at a random web page  and in each step  the random walker does one of the following with a probability  the walker jumps to a randomly chosenweb page  and with a probability of 1  the walker randomly chooses one of the outlinks from the currentweb page and follows the link the pagerank of a page is then the probability that the random walker is visiting the page at any given point in time note that pages that are pointed to from many web pages are more likely to be visited  and thus will have a higher pagerank similarly  pages pointed to by web pages with a high pagerank will also have a higher probability of being visited  and thus will have a higher pagerank pagerank can be defined by a set of linear equations  as follows  first  web pages are given integer identifiers the jump probability matrix t is defined with t  i  j  set to the probability that a random walker who is following a link out of page i follows the link to page j assuming that each link from i has an equal probability of being followed t  i  j  = 1/ni  where ni is the number of links out of page i most entries of t are 0 and it is best represented as an adjacency list then the pagerank p  j  for each page j can be defined as  p  j  =  /n +  1    *  n i = 1  t  i  j  * p  i   where  is a constant between 0 and 1  and n the number of pages ;  represents the probability of a step in the random walk being a jump the set of equations generated as above are usually solved by an an iterative technique  starting with each p  i  set to 1/n each step of the iteration computes new values for each p  i  using the p values from the previous iteration iteration thesumit67.blogspot.com 21.3 relevance using hyperlinks 923 stops when the maximum change in any p  i  value in an iteration goes below some cutoff value 21.3.3 other measures of popularity basic measures of popularity such as pagerank play an important role in ranking of query answers  but are by nomeans the only factor the tf ? idf scores of a page are used to judge its relevance to the query keywords  and must be combined with the popularity ranking other factors must also be taken into account  to handle limitations of pagerank and related popularity measures information about how often a site is visited would be a useful measure of popularity  but as mentioned earlier it is hard to obtain in general however  search engines do track what fraction of times users click on a page  when it is returned as an answer this fraction can be used as a measure of the site ? s popularity to measure the click fraction  instead of providing a direct link to the page  the search engine provides an indirect link through the search engine ? s site  which records the page click  and transparently redirects the browser to the original link.2 one drawback of the pagerank algorithm is that it assigns a measure of popularity that does not take query keywords into account for example  the page google.com is likely to have a very high pagerank because many sites contain a link to it suppose it contains aword mentioned in passing  such as ? stanford ?  the advanced search page at google did in fact contain this word at one point several years ago   a search on the keyword stanford would then return google.com as the highest-ranked answer  ahead of a more relevant answer such as the stanford universityweb page one widely used solution to this problem is to use keywords in the anchor text of links to a page to judge what topics the page is highly relevant to the anchor text of a link consists of the text that appears within the html a href tag for example  the anchor text of the link  < a href = " http  //stanford.edu " > stanford university < /a > is ? stanford university ?  if many links to the page stanford.edu have the word stanford in their anchor text  the page can be judged to be very relevant to the keyword stanford text near the anchor text may also be taken into account ; for example  aweb site may contain the text ? stanford ? s home page is here ?  butmay have used only the word ? here ? as anchor text in the link to the stanfordweb site popularity based on anchor text is combined with other measures of popularity  and with tf ? idf measures  to get an overall ranking for query answers  as discussed in section 21.3.5 as an implementation trick  the words in the anchor 2sometimes this indirection is hidden from the user for example when you point the mouse at a link  such as dbbook com  in a google query result  the link appears to point directly to the site however  at least as of mid 2009  when you actually click on the link  javascript code associated with the page actually rewrites the link to go indirectly through google ? s site if you use the back button of the browser to go back to the query result page  and point to the link again  the change in the linked url becomes visible thesumit67.blogspot.com 924 chapter 21 information retrieval text are often treated as part of the page  with a term frequency based on the the popularity of the pages where the anchor text appears then  tf ? idf ranking automatically takes anchor text into account an alternative approach to taking keywords into account when defining popularity is to compute a measure of popularity using only pages that contain the query keywords  instead of computing popularity using all availableweb pages this approach is more expensive  since the computation of popularity ranking has to be done dynamically when a query is received,whereas pagerank is computed statically once  and reused for all queries web search engines handling billions of queries per day can not afford to spend so much time answering a query as a result  although this approach can give better answers  it is not very widely used the hits algorithm was based on the above idea of first finding pages that contain the query keywords  and then computing a popularitymeasure using just this set of related pages in addition it introduced a notion of hubs and authorities a hub is a page that stores links to many related pages ; it may not in itself contain actual information on a topic  but points to pages that contain actual information in contrast  an authority is a page that contains actual information on a topic  although it may not store links to many related pages each page then gets a prestige value as a hub  hub-prestige   and another prestige value as an authority  authority-prestige   the definitions of prestige  as before  are cyclic and are defined by a set of simultaneous linear equations a page gets higher hub-prestige if it points to many pages with high authority-prestige  while a page gets higher authority-prestige if it is pointed to by many pages with high hub-prestige given a query  pages with highest authority-prestige are ranked higher than other pages see the bibliographical notes for references giving further details 21.3.4 search engine spamming search engine spamming refers to the practice of creating web pages  or sets of web pages  designed to get a high relevance rank for some queries  even though the sites are not actually popular sites for example  a travel site may want to be ranked high for queries with the keyword ? travel ?  it can get high tf ? idf scores by repeating the word ? travel ? many times in its page.3 even a site unrelated to travel  such as a pornographic site  could do the same thing  and would get highly ranked for a query on the word travel in fact  this sort of spamming of tf ? idf was common in the early days of web search  and there was a constant battle between such sites and search engines that tried to detect spamming and deny them a high ranking popularity ranking schemes such as pagerank make the job of search engine spamming more difficult  since just repeatingwords to get a high tf ? idf scorewas no longer sufficient.however  even these techniques can be spammed  by creating a collection of web pages that point to each other  increasing their popularity 3repeated words in a web page may confuse users ; spammers can tackle this problem by delivering different pages to search engines than to other users  for the same url  or by making the repeated words invisible  for example  by formatting the words in small white font on a white background thesumit67.blogspot.com 21.4 synonyms  homonyms  and ontologies 925 rank techniques such as using sites instead of pages as the unit of ranking  with appropriately normalized jump probabilities  have been proposed to avoid some spamming techniques  but are not fully effective against other spamming techniques the war between search engine spammers and the search engines continues even today the hubs and authorities approach of the hits algorithm is more susceptible to spamming.aspammer can create aweb page containing links to good authorities on a topic  and gains a high hub score as a result in addition the spammer ? sweb page includes links to pages that they wish to popularize  which may not have any relevance to the topic because these linked pages are pointed to by a page with high hub score  they get a high but undeserved authority score 21.3.5 combining tf-idf and popularity ranking measures we have seen two broad kinds of features used in ranking  namely tf ? idf and popularity scores such as pagerank tf ? idf itself reflects a combination of several factors including raw term frequency and inverse document frequency  occurrence of a term in anchor text linking to the page  and a variety of other factors such as occurrence of the term in the title  occurrence of the term early in the document  and larger font size for the term  among other factors how to combine the scores of a page on each these factors  to generate an overall page score  is a major problem that must be addressed by any information retrieval system in the early days of search engines  humans created functions to combine scores into an overall score but today  search engines use machinelearning techniques to decide how to combine scores typically  a score combining formula is fixed  but the formula takes as parameters weights for different scoring factors by using a training set of query results ranked by humans  a machinelearning algorithm can come up with an assignment of weights for each scoring factor that results in the best ranking performance across multiple queries we note that most search engines do not reveal how they compute relevance rankings ; they believe that revealing their ranking techniques would allow competitors to catch up  and would make the job of search engine spamming easier  resulting in poorer quality of results 21.4 synonyms  homonyms  and ontologies consider the problem of locating documents about motorcycle maintenance  using the query ? motorcycle maintenance ?  suppose that the keywords for each document are the words in the title and the names of the authors the document titled motorcycle repair would not be retrieved  since the word ? maintenance ? does not occur in its title we can solve that problem by making use of synonyms each word can have a set of synonyms defined  and the occurrence of aword can be replaced by the or of all its synonyms  including the word itself   thus  the query ? motorcycle and repair ? can be replaced by ? motorcycle and  repair or maintenance   ? this query would find the desired document thesumit67.blogspot.com 926 chapter 21 information retrieval keyword-based queries also suffer fromthe opposite problem  of homonyms  that is single words with multiple meanings for instance  the word ? object ? has different meanings as a noun and as a verb the word ? table ? may refer to a dinner table  or to a table in a relational database in fact  a danger even with using synonyms to extend queries is that the synonyms may themselves have different meanings for example  ? allowance ? is a synonym for one meaning of the word ? maintenance ?  but has a different meaning than what the user intended in the query ? motorcycle maintenance ?  documents that use the synonyms with an alternative intended meaning would be retrieved the user is then left wondering why the system thought that a particular retrieved document  for example  using theword ? allowance ?  is relevant  if it contains neither the keywords the user specified  nor words whose intended meaning in the document is synonymous with specified keywords ! it is therefore a bad idea to use synonyms to extend a query without first verifying the synonyms with the user a better approach to the above problem is for the system to understand what concept each word in a document represents  and similarly to understand what concepts a user is looking for  and to return documents that address the concepts that the user is interested in a system that supports concept-based querying has to analyze each document to disambiguate each word in the document  and replace it with the concept that it represents ; disambiguation is usually done by looking at other surroundingwords in the document for example  if a document contains words such as database or query  the word table probably should be replaced by the concept ? table  data ? whereas if the document contains words such as furniture  chair  or wood near the word table  the word table should be replaced by the concept ? table  furniture ? disambiguation based on nearby words is usually harder for user queries  since queries contain very few words  so concept-based query systems would offer several alternative concepts to the user  who picks one or more before the search continues concept-based querying has several advantages ; for example  a query in one language can retrieve documents in other languages  so long as they relate to the same concept automated translation mechanisms can be used subsequently if the user does not understand the language in which the document is written however  the overhead of processing documents to disambiguate words is very high when billions of documents are being handled internet search engines therefore generally did not support concept-based querying initially  but interest in concept-based approaches is growing rapidly however  concept-based querying systems have been built and used for other large collections of documents querying based on concepts can be extended further by exploiting concept hierarchies for example  suppose a person issues a query ? flying animals ? ; a document containing information about ? flying mammals ? is certainly relevant  since amammalis an animal.however  the two concepts are not the same  and just matching concepts would not allow the document to be returned as an answer concept-based querying systems can support retrieval of documents based on concept hierarchies thesumit67.blogspot.com 21.5 indexing of documents 927 ontologies are hierarchical structures that reflect relationships between concepts the most common relationship is the is-a relationship ; for example  a leopard is-a mammal  and a mammal is-a animal other relationships  such as part-of are also possible ; for example  an airplane wing is part-of an airplane thewordnet system defines a large variety of concepts with associated words  called a synset in wordnet terminology   the words associated with a synset are synonyms for the concept ; a word may of course be a synonym for several different concepts in addition to synonyms  wordnet defines homonyms and other relationships in particular  the is-a and part-of relationships that it defines connect concepts  and in effect define an ontology the cyc project is another effort to create an ontology in addition to language-wide ontologies  ontologies have been defined for specific areas to deal with terminology relevant to those areas for example  ontologies have been created to standardize terms used in businesses ; this is an important step in building a standard infrastructure for handling order processing and other interorganization flow of data as another example  consider a medical insurance company that needs to get reports from hospitals containing diagnosis and treatment information an ontology that standardizes the terms helps hospital staff to understand the reports unambiguously this can greatly help in analysis of the reports  for example to track how many cases of a particular disease occurred in a particular time frame it is also possible to build ontologies that link multiple languages for example  wordnets have been built for different languages  and common concepts between languages can be linked to each other such a system can be used for translation of text in the context of information retrieval  a multilingual ontology can be used to implement a concept-based search across documents in multiple languages the largest effort in using ontologies for concept-based queries is the semantic web the semantic web is led by the world wide web consortium and consists of a collection of tools  standards  and languages that permit data on the web to be connected based on their semantics  or meaning instead of being a centralized repository  the semantic web is designed to permit the same kind of decentralized  distributed growth that has made theworldwideweb so successful key to this is the capability to integrate multiple  distributed ontologies as a result  anyone with access to the internet can add to the semanticweb 21.5 indexing of documents an effective index structure is important for efficient processing of queries in an information-retrieval system documents that contain a specified keyword can be located efficiently by using an inverted index that maps each keyword ki to a list si of  identifiers of  the documents that contain ki  for example  if documents d1  d9 and d21 contain the term ? silberschatz ?  the inverted list for the keyword silberschatz would be ? d1 ; d9 ; d21 ?  to support relevance ranking based on proximity of keywords  such an index may provide not just identifiers of thesumit67.blogspot.com 928 chapter 21 information retrieval documents  but also a list of locations within the document where the keyword appears for example  if ? silberschatz ? appeared at position 21 in d1  positions 1 and 19 in d2  and positions 4  29 and 46 in d3  the inverted list with positions would be ? d1/21 ; d9/1  19 ; d21/4  29  46 ?  the inverted lists may also include with each document the term frequency of the term such indices must be stored on disk  and each list si can span multiple disk pages to minimize the number of i/o operations to retrieve each list si  the system would attempt to keep each list si in a set of consecutive disk pages  so the entire list can be retrievedwith just one disk seek.a b + -tree index can be used to map each keyword ki to its associated inverted list si  the and operation finds documents that contain all of a specified set of keywords k1  k2      kn.we implement the and operation by first retrieving the sets of document identifiers s1  s2      sn of all documents that contain the respective keywords the intersection  s1 n s2 n ? ? ? n sn  of the sets gives the document identifiers of the desired set of documents the or operation gives the set of all documents that contain at least one of the keywords k1  k2      kn we implement the or operation by computing the union  s1 ? s2 ? ? ? ? ? sn  of the sets the not operation finds documents that do not contain a specified keyword ki  given a set of document identifiers s  we can eliminate documents that contain the specified keyword ki by taking the difference s  si  where si is the set of identifiers of documents that contain the keyword ki  given a set of keywords in a query  many information-retrieval systems do not insist that the retrieved documents contain all the keywords  unless an and operation is used explicitly   in this case  all documents containing at least one of the words are retrieved  as in the or operation   but are ranked by their relevance measure to use term frequency for ranking  the index structure should additionally maintain the number of times terms occur in each document to reduce this effort  they may use a compressed representationwith only a few bits that approximates the term frequency the index should also store the document frequency of each term  that is  the number of documents in which the term appears   if the popularity ranking is independent of the index term  as is the case for page rank   the list si can be sorted on the popularity ranking  and secondarily  for documents with the same popularity ranking  on document-id   then  a simple merge can be used to compute and and or operations for the case of the and operation  if we ignore the tf ? idf contribution to the relevance score  and merely require that the document should contain the given keywords  merging can stop once k answers have been obtained  if the user requires only the top k answers in general  the results with highest final score  after including tf ? idf scores  are likely to have high popularity scores  and would appear near the front of the lists techniques have been developed to estimate the best possible scores of remaining results  and these can be used to recognize that answers not yet seen can not be part of the top k answers processing of the lists can then terminate early however  sorting on popularity score is not fully effective in avoiding long inverted list scans  since it ignores the contribution of the tf ? idf scores an alternative in such cases is to break up the inverted list for each term into two thesumit67.blogspot.com 21.6 measuring retrieval effectiveness 929 parts the first part contains documents that have a high tf ? idf score for that term  for example  documents where the term occurs in the document title  or in anchor text referencing the document   the second part contains all documents each part of the list can be sorted in order of  popularity  document-id   given a query  merging the first parts of the list for each term is likely to give several answers with an overall high score if sufficient high-scoring answers are not found using the first parts of the lists  the second parts of the lists are used to find all remaining answers if a document scores high on tf ? idf  it is likely to be found when merging the first parts of the lists see the bibliographical notes for related references 21.6 measuring retrieval effectiveness each keyword may be contained in a large number of documents ; hence  a compact representation is critical to keep space usage of the index low thus  the sets of documents for a keyword are maintained in a compressed form so that storage space is saved  the index is sometimes stored such that the retrieval is approximate ; a few relevant documents may not be retrieved  called a false drop or false negative   or a few irrelevant documents may be retrieved  called a false positive   a good index structure will not have any false drops  but may permit a few false positives ; the system can filter them away later by looking at the keywords that they actually contain inweb indexing  false positives are not desirable either  since the actual document may not be quickly accessible for filtering two metrics are used to measure how well an information-retrieval system is able to answer queries the first  precision  measures what percentage of the retrieved documents are actually relevant to the query the second  recall  measures what percentage of the documents relevant to the query were retrieved ideally both should be 100 percent precision and recall are also important measures for understanding how well a particular document-ranking strategy performs ranking strategies can result in false negatives and false positives  but in a more subtle sense ? false negatives may occur when documents are ranked  as a result of relevant documents receiving a low ranking if the system fetched all documents down to those with very low ranking there would be very few false negatives however  humans would rarely look beyond the first few tens of returned documents  and may thus miss relevant documents because they are not ranked high exactly what is a false negative depends on how many documents are examined therefore instead of having a single number as the measure of recall  we can measure the recall as a function of the number of documents fetched ? false positives may occur because irrelevant documents get higher rankings than relevant documents this too depends on how many documents are examined one option is to measure precision as a function of number of documents fetched thesumit67.blogspot.com 930 chapter 21 information retrieval a better and more intuitive alternative for measuring precision is to measure it as a function of recall with this combined measure  both precision and recall can be computed as a function of number of documents  if required for instance  we can say that with a recall of 50 percent the precision was 75 percent  whereas at a recall of 75 percent the precision dropped to 60 percent in general  we can draw a graph relating precision to recall these measures can be computed for individual queries  then averaged out across a suite of queries in a query benchmark yet another problem with measuring precision and recall lies in how to define which documents are really relevant and which are not in fact  it requires understanding of natural language  and understanding of the intent of the query  to decide if a document is relevant or not researchers therefore have created collections of documents and queries  and have manually tagged documents as relevant or irrelevant to the queries different ranking systems can be run on these collections to measure their average precision and recall across multiple queries 21.7 crawling and indexing the web web crawlers are programs that locate and gather information on theweb they recursively follow hyperlinks present in known documents to find other documents crawlers start from an initial set of urls  which may be createdmanually each of the pages identified by these urls are fetched from the web the web crawler then locates all url links in these pages  and adds them to the set of urls to be crawled  if they have not already been fetched  or added to the to-be-crawled set this process is again repeated by fetching all pages in the to-be-crawled set  and processing the links in these pages in the same fashion by repeating the process  all pages that are reachable by any sequence of links from the initial set of urls would be eventually fetched since the number of documents on the web is very large  it is not possible to crawl the whole web in a short period of time ; and in fact  all search engines cover only some portions of the web  not all of it  and their crawlers may take weeks or months to perform a single crawl of all the pages they cover there are usually many processes  running on multiple machines  involved in crawling a database stores a set of links  or sites  to be crawled ; it assigns links from this set to each crawler process new links found during a crawl are added to the database  and may be crawled later if they are not crawled immediately pages have to be refetched  that is  links recrawled  periodically to obtain updated information  and to discard sites that no longer exist  so that the information in the search index is kept reasonably up-to-date see the references in the bibliography for a number of practical details in performing a web crawl  such as infinite sequences of links created by dynamically generated pages  called a spider trap   prioritization of page fetches  and ensuring that web sites are not flooded by a burst of requests from a crawler pages fetched during a crawl are handed over to a prestige computation and indexing system  which may be running on a different machine the prestige thesumit67.blogspot.com 21.8 information retrieval  beyond ranking of pages 931 computation and indexing systems themselves run on multiple machines in parallel pages can be discarded after they are used for prestige computation and added to the index ; however  they are usually cached by the search engine  to give search engine users fast access to a cached copy of a page  even if the original web site containing the page is not accessible it is not a good idea to add pages to the same index that is being used for queries  since doing so would require concurrency control on the index  and would affect query and update performance instead  one copy of the index is used to answer querieswhile another copy is updatedwith newly crawled pages at periodic intervals the copies switch over,with the old one being updated while the new copy is being used for queries to support very high query rates  the indices may be kept in main memory  and there are multiple machines ; the system selectively routes queries to the machines to balance the load among them popular search engines often have tens of thousands of machines carrying out the various tasks of crawling  indexing  and answering user queries web crawlers depend on all relevant pages being reachable through hyperlinks however  many sites containing large collections of data may not make all the data available as hyperlinked pages instead  they provide search interfaces  where users can enter terms  or select menu options  and get results as an example  a database of flight information is usually made available using such a search interface  without any hyperlinks to the pages containing flight information as a result  the information inside such sites is not accessible to a normalweb crawler the information in such sites is often referred to as deepweb information deep web crawlers extract some such information by guessing what terms would make sense to enter  or what menu options to choose  in such search interfaces by entering each possible term/option and executing the search interface  they are able to extract pages with data that they would not have been able to find otherwise the pages extracted by a deep web crawl may be indexed just like regular web pages the google search engine  for example  includes results from deepweb crawls 21.8 information retrieval  beyond ranking of pages information-retrieval systemswere originally designed to find textual documents related to a query  and later extended to finding pages on theweb that are related to a query people use search engines for many different tasks  from simple tasks such as locating a web site that they want to use  to a broader goal of finding information on a topic of interest web search engines have become extremely good at the task of locating web sites that a user wants to visit the task of providing information on a topic of interest is much harder  and we study some approaches in this section there is also an increasing need for systems that try to understand documents  to a limited extent   and answer questions based on the  limited  understanding one approach is to create structured information from unstructured documents thesumit67.blogspot.com 932 chapter 21 information retrieval and to answer questions based on the structured information another approach applies natural language techniques to find documents related to a question  phrased in natural language  and return relevant segments of the documents as an answer to the question 21.8.1 diversity of query results today  search engines do not just return a ranked list of web pages relevant to a query they also return image and video results relevant to a query further  there are a variety of sites providing dynamically changing content such as sports scores  or stock market tickers to get current information from such sites  users would have to first click on the query result instead  search engines have created ? gadgets  ? which take data from a particular domain  such as sports updates  stock prices  or weather conditions  and format them in a nice graphical manner  to be displayed as results for a query search engines have to rank the set of gadgets available in terms of relevance to a query  and display the most relevant gadgets  along with web pages  images  videos  and other types of results thus a query result has a diverse set of result types search terms are often ambiguous for example  a query ? eclipse ? may be referring to a solar or lunar eclipse  or to the integrated development environment  ide  called eclipse if all the highly ranked pages for the term ? eclipse ? are about the ide  a user looking for information about solar or lunar eclipses may be very dissatisfied search engines therefore attempt to provide a set of results that are diverse in terms of their topics  to minimize the chance that a user would be dissatisfied to do so  at indexing time the search engine must disambiguate the sense in which a word is used in a page ; for example  it must decide whether the use of the word ? eclipse ? in a page refers to the ide or the astronomical phenomenon then  given a query  the search engine attempts to provide results that are relevant to the most common senses in which the query words are used the results obtained from aweb page need to be summarized as a snippet in a query result traditionally  search engines provided a few words surrounding the query keywords as a snippet that helps indicate what the page contains however  there are many domains where the snippet can be generated in a much more meaningful manner for example  if a user queries about a restaurant  a search engine can generate a snippet containing the restaurant ? s rating  a phone number  and a link to a map  in addition to providing a link to the restaurant ? s home page such specialized snippets are often generated for results retrieved from a database  for example  a database of restaurants 21.8.2 information extraction information-extraction systems convert information from textual form to a more structured form for example  a real-estate advertisement may describe attributes of a home in textual form  such as ? two-bedroom three-bath house in queens  $ 1 million ?  from which an information extraction system may extract attributes such as number of bedrooms  number of bathrooms  cost and neighborhood the original advertisement could have used various terms such as 2br  or two br  thesumit67.blogspot.com 21.8 information retrieval  beyond ranking of pages 933 or two bed  to denote two bedrooms the extracted information can be used to structure the data in a standard way thus  a user could specify that he is interested in two-bedroom houses  and a search system would be able to return all relevant houses based on the structured data  regardless of the terms used in the advertisement an organization that maintains a database of company information may use an information-extraction system to extract information automatically from newspaper articles ; the information extractedwould relate to changes in attributes of interest  such as resignations  dismissals  or appointments of company officers as another example  search engines designed for finding scholarly research articles  such as citeseer andgoogle scholar  crawl theweb to retrieve documents that are likely to be research articles they examine some features of each retrieved document  such as the presence of words such as ? bibliography ?  ? references ?  and ? abstract ?  to judge if a document is in fact a scholarly research article they then extract the title  list of authors  and the citations at the end of the article  by using information extraction techniques the extracted citation information can be used to link each article to articles that it cites  or to articles that cite it ; such citation links between articles can be very useful for a researcher several systems have been built for information extraction for specialized applications they use linguistic techniques  page structure  and user-defined rules for specific domains such as real estate advertisements or scholarly publications for limited domains  such as a specificweb site  it is possible for a human to specify patterns that can be used to extract information for example  on a particular web site  a pattern such as ? price  < number > $ ?  where < number > indicates any number  may match locations where the price is specified such patterns can be created manually for a limited number of web sites however  on the web scale with millions of web sites  manual creation of such patterns is not feasible machine-learning techniques  which can learn such patterns given a set of training examples  are widely used to automate the process of information extraction information extraction usually has errors in some fraction of the extracted information ; typically this is because some page had information in a format that syntactically matched a pattern  but did not actually specify a value  such as the price   information extraction using simple patterns  which separately match parts of a page  is relatively error prone machine-learning techniques can perform much more sophisticated analysis  based on interactions between patterns  to minimize errors in the information extracted  while maximizing the amount of information extracted see the references in the bibliographical notes for more information 21.8.3 question answering information retrieval systems focus on finding documents relevant to a given query however  the answer to a query may lie in just one part of a document  or in small parts of several documents question answering systems attempt to provide direct answers to questions posed by users for example  a question of the thesumit67.blogspot.com 934 chapter 21 information retrieval form ? who killed lincoln ? ? may best be answered by a line that says ? abraham lincoln was shot by john wilkes booth in 1865 ? note that the answer does not actually contain the words ? killed ? or ? who ?  but the system infers that ? who ? can be answered by a name  and ? killed ? is related to ? shot ?  question answering systems targeted at information on the web typically generate one or more keyword queries from a submitted question  execute the keyword queries against web search engines  and parse returned documents to find segments of the documents that answer the question a number of linguistic techniques and heuristics are used to generate keyword queries  and to find relevant segments from the document an issue in answering questions is that different documents may indicate different answers to a question for example  if the question is ? how tall is a giraffe ? ? different documents may give different numbers as an answer these answers form a distribution of values  and a question answering system may choose the average  or median value of the distribution as the answer to be returned ; to reflect the fact that the answer is not expected to be precise  the system may return the average along with the standard deviation  for example  average of 16 feet  with a standard deviation of 2 feet   or a range based on the average and the standard deviation  for example  between 14 and 18 feet   current-generation question answering systems are limited in power  since they do not really understand either the question or the documents used to answer the question however  they are useful for a number of simple question answering tasks 21.8.4 querying structured data structured data are primarily represented in either relational or xml form several systems have been built to support keyword querying on relational and xml data  see chapter 23   a common theme between these systems lies in finding nodes  tuples or xml elements  containing the specified keywords  and finding connecting paths  or common ancestors  in the case of xml data  between them for example  a query ? zhang katz ? on a university database may find the name ? zhang ? occurring in a student tuple  and the name ? katz ? in an instructor tuple  and a path through the advisor relation connecting the two tuples other paths  such as student ? zhang ? taking a course taught by ? katz ? may also be found in response to this query such queries may be used for ad hoc browsing and querying of data  when the user does not know the exact schema and does not wish to take the effort to write an sql query defining what she is searching for indeed it is unreasonable to expect lay users to write queries in a structured query language  whereas keyword querying is quite natural since queries are not fully defined  they may have many different types of answers  which must be ranked a number of techniques have been proposed to rank answers in such a setting  based on the lengths of connecting paths  and on techniques for assigning directions and weights to edges techniques have also been proposed for assigning popularity ranks to tuples and xml elements  based thesumit67.blogspot.com 21.9 directories and categories 935 on links such as foreign key and idref links see the bibliographical notes for more information on keyword searching of relational and xml data 21.9 directories and categories a typical library user may use a catalog to locate a book for which she is looking when she retrieves the book from the shelf  however  she is likely to browse through other books that are located nearby libraries organize books in such a way that related books are kept close together hence  a book that is physically near the desired book may be of interest as well  making it worthwhile for users to browse through such books to keep related books close together  libraries use a classification hierarchy books on science are classified together within this set of books  there is a finer classification  with computer-science books organized together,mathematics books organized together  and so on since there is a relation between mathematics and computer science  relevant sets of books are stored close to each other physically at yet another level in the classification hierarchy  computer-science books are broken down into subareas  such as operating systems  languages  and algorithms figure 21.1 illustrates a classification hierarchy that may be used by a library because books can be kept at only one place  each book in a library is classified into exactly one spot in the classification hierarchy in an information-retrieval system  there is no need to store related documents close together however  such systems need to organize documents logically so as to permit browsing thus  such a system could use a classification hierarchy similar books algorithms graph algorithms math science engineering fiction computer science figure 21.1 a classification hierarchy for a library system thesumit67.blogspot.com 936 chapter 21 information retrieval books algorithms graph algorithms math science engineering fiction computer science figure 21.2 a classification dag for a library information-retrieval system to one that libraries use  and  when it displays a particular document  it can also display a brief description of documents that are close in the hierarchy in an information-retrieval system  there is no need to keep a document in a single spot in the hierarchy a document that talks of mathematics for computer scientists could be classified under mathematics as well as under computer science all that is stored at each spot is an identifier of the document  that is  a pointer to the document   and it is easy to fetch the contents of the document by using the identifier as a result of this flexibility  not only can a document be classified under two locations  but also a subarea in the classification hierarchy can itself occur under two areas the class of ? graph algorithm ? documents can appear both under mathematics and under computer science thus  the classification hierarchy is now a directed acyclic graph  dag   as shown in figure 21.2 a graph-algorithm document may appear in a single location in the dag  but can be reached via multiple paths a directory is simply a classification dag structure each leaf of the directory stores links to documents on the topic represented by the leaf internal nodesmay also contain links  for example  to documents that can not be classified under any of the child nodes to find information on a topic  a user would start at the root of the directory and follow paths down the dag until reaching a node representing the desired topic while browsing down the directory  the user can find not only documents on the topic he is interested in  but also find related documents and related classes in the classification hierarchy the user may learn new information by browsing through documents  or subclasses  within the related classes thesumit67.blogspot.com 21.10 summary 937 organizing the enormous amount of information available on theweb into a directory structure is a daunting task ? the first problem is determining what exactly the directory hierarchy should be ? the second problem is  given a document  deciding which nodes of the directory are categories relevant to the document to tackle the first problem  portals such as yahoo ! have teams of ? internet librarians ? who come up with the classification hierarchy and continually refine it the second problem can also be tackled manually by librarians  or web site maintainers may be responsible for deciding where their sites should lie in the hierarchy there are also techniques for deciding automatically the location of documents based on computing their similarity to documents that have already been classified wikipedia  the online encyclopedia  addresses the classification problem in the reverse direction each page in wikipedia has a list of categories to which it belongs for example  as of 2009  the wikipedia page on giraffes had several categories including ? mammals of africa ?  in turn  the ? mammals of africa ? category itself belongs to the category ? mammals by geography ?  which in turn belongs to the category ? mammals ?  which in turn has a category ? vertebrates ?  and so on the category structure is useful to browse other instances of the same category  for example  to find other mammals of africa  or other mammals conversely  a query that looks for mammals can use the category information to infer that a giraffe is a mammal the wikipedia category structure is not a tree  but is almost a dag ; it is not actually a dag since it has a few instances of loops  which probably reflect categorization errors 21.10 summary ? information-retrieval systems are used to store and query textual data such as documents they use a simpler data model than do database systems  but provide more powerful querying capabilities within the restricted model ? queries attempt to locate documents that are of interest by specifying  for example  sets of keywords the query that a user has in mind usually can not be stated precisely ; hence  information-retrieval systems order answers on the basis of potential relevance ? relevance ranking makes use of several types of information  such as  ? term frequency  how important each term is to each document ? inverse document frequency ? popularity ranking thesumit67.blogspot.com 938 chapter 21 information retrieval ? similarity of documents is used to retrieve documents similar to an example document the cosine metric is used to define similarity  and is based on the vector space model ? pagerank and hub/authority rank are two ways to assign prestige to pages on the basis of links to the page the pagerank measure can be understood intuitively using a random-walk model anchor text information is also used to compute a per-keyword notion of popularity information-retrieval systems need to combine scores on multiple factors such as tf ? idf and pagerank  to get an overall score for a page ? search engine spamming attempts to get  an undeserved  high ranking for a page ? synonyms and homonyms complicate the task of information retrieval concept based querying aims at finding documents containing specified concepts  regardless of the exact words  or language  in which the concept is specified ontologies are used to relate concepts using relationships such as is-a or part-of ? inverted indices are used to answer keyword queries ? precision and recall are two measures of the effectiveness of an information retrieval system ? web search engines crawl the web to find pages  analyze them to compute prestige measures  and index them ? techniques have been developed to extract structured information from textual data  to perform keyword querying on structured data  and to give direct answers to simple questions posed in natural language ? directory structures and categories are used to classify documentswith other similar documents review terms ? information-retrieval systems ? keyword search ? full text retrieval ? term ? relevance ranking ? term frequency ? inverse document frequency ? relevance ? proximity ? similarity-based retrieval ? vector space model ? cosine similarity metric ? relevance feedback ? stop words ? relevance using hyperlinks ? popularity/prestige thesumit67.blogspot.com practice exercises 939 ? transfer of prestige ? pagerank ? random walk model ? anchor-text ? based relevance ? hub/authority ranking ? search engine spamming ? synonyms ? homonyms ? concepts ? concept-based querying ? ontologies ? semanticweb ? inverted index ? false drop ? false negative ? false positive ? precision ? recall ? web crawlers ? deepweb ? query result diversity ? information extraction ? question answering ? querying structured data ? directories ? classification hierarchy ? categories practice exercises 21.1 compute the relevance  using appropriate definitions of term frequency and inverse document frequency  of each of the practice exercises in this chapter to the query ? sql relation ?  21.2 suppose you want to find documents that contain at least k of a given set of n keywords suppose also you have a keyword index that gives you a  sorted  list of identifiers of documents that contain a specified keyword give an efficient algorithm to find the desired set of documents 21.3 suggest how to implement the iterative technique for computing page rank given that the t matrix  even in adjacency list representation  does not fit in memory 21.4 suggest how a document containing a word  such as ? leopard ?  can be indexed such that it is efficiently retrieved by queries using a more general concept  such as ? carnivore ? or ? mammal ?   you can assume that the concept hierarchy is not very deep  so each concept has only a few generalizations  a concept can  however  have a large number of specializations   you can also assume that you are provided with a function that returns the concept for each word in a document also suggest how a query using a specialized concept can retrieve documents using a more general concept 21.5 suppose inverted lists are maintained in blocks  with each block noting the largest popularity rank and tf ? idf scores of documents in the remaining thesumit67.blogspot.com 940 chapter 21 information retrieval blocks in the list suggest how merging of inverted lists can stop early if the user wants only the top k answers exercises 21.6 using a simple definition of term frequency as the number of occurrences of the term in a document  give the tf ? idf scores of each term in the set of documents consisting of this and the next exercise 21.7 create a small example of four small documents  each with a pagerank  and create inverted lists for the documents sorted by the pagerank you do not need to compute pagerank  just assume some values for each page 21.8 suppose you wish to perform keyword querying on a set of tuples in a database  where each tuple has only a few attributes  each containing only a few words does the concept of term frequency make sense in this context ? and that of inverse document frequency ? explain your answer also suggest how you can define the similarity of two tuples using tf ? idf concepts 21.9 web sites that want to get some publicity can join aweb ring  where they create links to other sites in the ring  in exchange for other sites in the ring creating links to their site what is the effect of such rings on popularity ranking techniques such as pagerank ? 21.10 the google search engine provides a feature whereby web sites can display advertisements supplied by google the advertisements supplied are based on the contents of the page suggest how google might choose which advertisements to supply for a page  given the page contents 21.11 one way to create a keyword-specific version of pagerank is to modify the random jump such that a jump is only possible to pages containing the keyword thus pages that do not contain the keyword but are close  in terms of links  to pages that contain the keyword also get a nonzero rank for that keyword a give equations defining such a keyword-specific version of page rank b give a formula for computing the relevance of a page to a query containing multiple keywords 21.12 the idea of popularity ranking using hyperlinks can be extended to relational and xml data  using foreign key and idref edges in place of hyperlinks suggest how such a ranking scheme may be of value in the following applications  a abibliographic database that has links fromarticles to authors of the articles and links from each article to every article that it references thesumit67.blogspot.com bibliographical notes 941 b a sales database that has links from each sales record to the items that were sold also suggest why prestige ranking can give less than meaningful results in a movie database that records which actor has acted in which movies 21.13 what is the difference between a false positive and a false drop ? if it is essential that no relevant information be missed by an information retrieval query  is it acceptable to have either false positives or false drops ? why ? tools google  www.google.com  is currently the most popular search engine  but there are a number of other search engines  such as microsoft bing  www.bing.com  and yahoo ! search  search.yahoo.com   the site searchenginewatch.com provides a variety of information about search engines yahoo !  dir.yahoo.com  and theopen directory project  dmoz.org  provide classification hierarchies forweb sites bibliographical notes manning et al  2008   chakrabarti  2002   grossman and frieder  2004   witten et al  1999   and baeza-yates and ribeiro-neto  1999  provide textbook descriptions of information retrieval in particular  chakrabarti  2002  and manning et al  2008  provide detailed coverage of web crawling  ranking techniques  and mining techniques related to information retrieval such as text classification and clustering brin and page  1998  describes the anatomy of the google search engine  including the pagerank technique  while a hubs and authorities-based ranking technique called hits is described by kleinberg  1999   bharat and henzinger  1998  presents a refinement of the hits ranking technique these techniques  as well as other popularity-based ranking techniques  and techniques to avoid search engine spamming  are described in detail in chakrabarti  2002   chakrabarti et al  1999  addresses focused crawling of the web to find pages related to a specific topic chakrabarti  1999  provides a survey of web resource discovery indexing of documents is covered in detail by witten et al  1999   jones and willet  1997  is a collection of articles on information retrieval salton  1989  is an early textbook on information-retrieval systems a number of practical issues in ranking and indexing of web pages  as done in an early version of the google search engine  are discussed in brin and page  1998   unfortunately  there are no publicly available details of how exactly ranking is done currently by any of the leading search engines the citeseer system  citeseer.ist.psu.edu  maintains a very large database of research articles  with citation links between the publications  and uses citations to thesumit67.blogspot.com 942 chapter 21 information retrieval rank publications it includes a technique for adjusting the citation ranking based on the age of a publication  to compensate for the fact that citations to a publication increase as time passes ; without the adjustment  older documents tend to get a higher ranking than they truly deserve google scholar  scholar.google.com  provides a similar searchable database of research articles incorporating citations between articles it is worth noting that these systems use information extraction techniques to infer the title and list of authors of an article  as well as the citations at the end of the article they then create citation links between articles based on  approximate  matching of the article title and author list with the citation text information extraction and question answering have had a fairly long history in the artificial intelligence community jackson and moulinier  2002  provides textbook coverage of natural language processing techniques with an emphasis on information extraction soderland  1999  describes information extraction using the whisk system  while appelt and israel  1999  provides a tutorial on information extraction the annual text retrieval conference  trec  has a number of tracks  each of which defines a problem and infrastructure to test the quality of solutions to the problem details on trec may be found at trec.nist.gov more information about wordnet can be found at wordnet.princeton.edu and globalwordnet.org the goal of the cyc system is to provide a formal representation of large amounts of human knowledge its knowledge base contains a large number of terms  and assertions about each term cyc also includes a support for natural language understanding and disambiguation information about the cyc system may be found at cyc.com and opencyc.org the evolution of web search toward concepts and semantics rather than keywords is discussed in dalvi et al  2009   the annual international semantic web conference  iswc  is one of themajor conferences where new developments in the semanticweb are presented details may be found at semanticweb.org agrawal et al  2002   bhalotia et al  2002  and hristidis and papakonstantinou  2002  cover keyword querying of relational data keyword querying of xml data is addressed by florescu et al  2000  and guo et al  2003   among others thesumit67.blogspot.com part 7 specialty databases several application areas for database systems are limited by the restrictions of the relational data model as a result  researchers have developed several data models based on an object-oriented approach  to deal with these application domains the object-relational model  described in chapter 22  combines features of the relational and object-oriented models this model provides the rich type system of object-oriented languages  combined with relations as the basis for storage of data it applies inheritance to relations  not just to types the object-relational data model provides a smooth migration path from relational databases  which is attractive to relational database vendors as a result  starting with sql  1999  the sql standard includes a number of object-oriented features in its type system  while continuing to use the relational model as the underlying model the term object-oriented database is used to describe a database system that supports direct access to data fromobject-oriented programming languages  without requiring a relational query language as the database interface chapter 22 also provides a brief overview of object-oriented databases the xml language was initially designed as a way of adding markup information to text documents  but has become important because of its applications in data exchange xml provides a way to represent data that have nested structure  and furthermore allows a great deal of flexibility in structuring of data  which is important for certain kinds of nontraditional data chapter 23 describes the xml language  and then presents different ways of expressing queries on data represented in xml  including the xquery xml query language  and sql/xml  an extension of sql which allows the creation of nested xml output 943 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter22 object-based databases traditional database applications consist of data-processing tasks  such as banking and payroll management  with relatively simple data types that are well suited to the relational data model as database systems were applied to a wider range of applications  such as computer-aided design and geographical information systems  limitations imposed by the relational model emerged as an obstacle the solution was the introduction of object-based databases  which allow one to deal with complex data types 22.1 overview the first obstacle faced by programmers using the relational data model was the limited type system supported by the relational model complex application domains require correspondingly complex data types  such as nested record structures,multivalued attributes  and inheritance,which are supported by traditional programming languages such features are in fact supported in the e-r and extended e-r notations  but had to be translated to simpler sql data types the object-relational data model extends the relational data model by providing a richer type system including complex data types and object orientation relational query languages  in particular sql  need to be correspondingly extended to deal with the richer type system such extensions attempt to preserve the relational foundations ? in particular  the declarative access to data ? while extending the modeling power object-relational database systems  that is  database systems based on the object-relationmodel  provide a convenient migration path for users of relational databases who wish to use object-oriented features the second obstacle was the difficulty in accessing database data from programs written in programming languages such as c + + or java merely extending the type system supported by the database was not enough to solve this problem completely differences between the type system of the database and the type system of the programming language make data storage and retrieval more complicated  and need to be minimized having to express database access using a language  sql  that is different from the programming language again makes the job of the programmer harder it is desirable  for many applications  to have 945 thesumit67.blogspot.com 946 chapter 22 object-based databases programming language constructs or extensions that permit direct access to data in the database  without having to go through an intermediate language such as sql in this chapter,we first explain the motivation for the development of complex data types we then study object-relational database systems  specifically using features that were introduced in sql  1999 and sql  2003 note that most database products support only a subset of the sql features described here and for supported features  the syntax often differs slightly from the standard this is the result of commercial systems introducing object-relational features to the market before the standards were finalized refer to the user manual of the database system you use to find out what features it supports we then address the issue of supporting persistence for data that is in the native type system of an object-oriented programming language two approaches are used in practice  1 build an object-oriented database system  that is  a database system that natively supports an object-oriented type system  and allows direct access to data from an object-oriented programming language using the native type system of the language 2 automatically convert data fromthe native type system of the programming language to a relational representation  and vice versa data conversion is specified using an object-relational mapping we provide a brief introduction to both these approaches finally  we outline situations in which the object-relational approach is better than the object-oriented approach  and vice versa  and mention criteria for choosing between them 22.2 complex data types traditional database applications have conceptually simple data types the basic data items are records that are fairly small and whose fields are atomic ? that is  they are not further structured  and first normal form holds  see chapter 8   further  there are only a few record types in recent years  demand has grown for ways to deal with more complex data types consider  for example  addresses while an entire address could be viewed as an atomic data item of type string  this view would hide details such as the street address  city  state  and postal code  which could be of interest to queries on the other hand  if an address were represented by breaking it into the components  street address  city  state  and postal code   writing queries would bemore complicated since theywould have to mention each field.abetter alternative is to allow structured data types that allow a type address with subparts street address  city  state  and postal code as another example  consider multivalued attributes fromthe e-rmodel such attributes are natural  for example  for representing phone numbers  since people thesumit67.blogspot.com 22.2 complex data types 947 title author_array publisher keyword_set  name  branch  compilers  smith  jones   mcgraw-hill  newyork   parsing  analysis  networks  jones  frick   oxford  london   internet  web  figure 22.1 non-1nf books relation  books may have more than one phone the alternative of normalization by creating a new relation is expensive and artificial for this example with complex type systems we can represent e-r model concepts  such as composite attributes  multivalued attributes  generalization  and specialization directly  without a complex translation to the relational model in chapter 8  we defined first normal form  1nf   which requires that all attributes have atomic domains recall that a domain is atomic if elements of the domain are considered to be indivisible units the assumption of 1nf is a natural one in the database application examples we have considered however  not all applications are best modeled by 1nf relations for example  rather than view a database as a set of records  users of certain applications view it as a set of objects  or entities   these objects may require several records for their representation a simple  easy-to-use interface requires a one-to-one correspondence between the user ? s intuitive notion of an object and the database system ? s notion of a data item consider  for example  a library application  and suppose we wish to store the following information for each book  ? book title ? list of authors ? publisher ? set of keywords we can see that  if we define a relation for the preceding information  several domains will be nonatomic ? authors a book may have a list of authors  which we can represent as an array nevertheless  we may want to find all books of which jones was one of the authors thus  we are interested in a subpart of the domain element ? authors ? ? keywords if we store a set of keywords for a book  we expect to be able to retrieve all books whose keywords include one or more specified keywords thus  we view the domain of the set of keywords as nonatomic ? publisher unlike keywords and authors  publisher does not have a set-valued domain however  we may view publisher as consisting of the subfields name and branch this view makes the domain of publisher nonatomic figure 22.1 shows an example relation  books thesumit67.blogspot.com 948 chapter 22 object-based databases title author position compilers smith compilers jones networks jones networks frick 1212 authors title keyword compilers parsing compilers analysis networks internet networks web keywords title pub_name pub_branch compilers mcgraw-hill new york networks oxford london books4 figure 22.2 4nf version of the relation books for simplicity  we assume that the title of a book uniquely identifies the book.1 we can then represent the same information using the following schema  where the primary key attributes are underlined  ? authors  title  author  position  ? keywords  title  keyword  ? books4  title  pub name  pub branch  the above schema satisfies 4nf figure 22.2 shows the normalized representation of the data from figure 22.1 although our example book database can be adequately expressed without using nested relations  the use of nested relations leads to an easier-to-understand model the typical user or programmer of an information-retrieval system thinks of the database in terms of books having sets of authors  as the non-1nf design models the 4nf design requires queries to join multiple relations  whereas the non-1nf design makes many types of queries easier on the other hand  it may be better to use a first normal form representation in other situations for instance  consider the takes relationship in our university example the relationship ismany-to-many between student and section.we could 1this assumption does not hold in the real world books are usually identified by a 10-digit isbn number that uniquely identifies each published book thesumit67.blogspot.com 22.3 structured types and inheritance in sql 949 conceivably store a set of sections with each student  or a set of students with each section  or both if we store both  we would have data redundancy  the relationship of a particular student to a particular section would be stored twice   the ability to use complex data types such as sets and arrays can be useful in many applications but should be used with care 22.3 structured types and inheritance in sql before sql  1999  the sql type system consisted of a fairly simple set of predefined types sql  1999 added an extensive type system to sql  allowing structured types and type inheritance 22.3.1 structured types structured types allow composite attributes of e-r designs to be represented directly for instance  we can define the following structured type to represent a composite attribute name with component attribute firstname and lastname  create type name as  firstname varchar  20   lastname varchar  20   final ; similarly  the following structured type can be used to represent a composite attribute address  create type address as  street varchar  20   city varchar  20   zipcode varchar  9   not final ; such types are called user-defined types in sql2 the above definition corresponds to the e-r diagram in figure 7.4 the final and not final specifications are related to subtyping  which we describe later  in section 22.3.2.3 we can now use these types to create composite attributes in a relation  by simply declaring an attribute to be of one of these types for example  we could create a table person as follows  2to illustrate our earlier note about commercial implementations defining their syntax before the standards were developed  we point out that oracle requires the keyword object following as 3the final specification for name indicates that we can not create subtypes for name  whereas the not final specification for address indicates that we can create subtypes of address thesumit67.blogspot.com 950 chapter 22 object-based databases create table person  name name  address address  dateofbirth date  ; the components of a composite attribute can be accessed using a ? dot ? notation ; for instance name.firstname returns the firstname component of the name attribute an access to attribute name would return a value of the structured type name we can also create a tablewhose rows are of a user-defined type for example  we could define a type persontype and create the table person as follows  4 create type persontype as  name name  address address  dateofbirth date  not final create table person of persontype ; an alternativeway of defining composite attributes in sql is to use unnamed row types for instance  the relation representing person information could have been created using row types as follows  create table person r  name row  firstname varchar  20   lastname varchar  20    address row  street varchar  20   city varchar  20   zipcode varchar  9    dateofbirth date  ; this definition is equivalent to the preceding table definition  except that the attributes name and address have unnamed types  and the rows of the table also have an unnamed type the following query illustrates how to access component attributes of a composite attribute the query finds the last name and city of each person select name.lastname  address.city from person ; a structured type can have methods defined on it we declare methods as part of the type definition of a structured type  4most actual systems  being case insensitive  would not permit name to be used both as an attribute name and a data type thesumit67.blogspot.com 22.3 structured types and inheritance in sql 951 create type persontype as  name name  address address  dateofbirth date  not final method ageondate  ondate date  returns interval year ; we create the method body separately  create instance method ageondate  ondate date  returns interval year for persontype begin return ondate  self.dateofbirth ; end note that the for clause indicates which type this method is for  while the keyword instance indicates that thismethod executes on an instance of the person type the variable self refers to the person instance on which the method is invoked the body of the method can contain procedural statements  which we saw earlier in section 5.2 methods can update the attributes of the instance on which they are executed methods can be invoked on instances of a type if we had created a table person of type persontype  we could invoke the method ageondate   as illustrated below  to find the age of each person select name.lastname  ageondate  current date  from person ; in sql  1999  constructor functions are used to create values of structured types.afunctionwith the same name as a structured type is a constructor function for the structured type for instance  we could declare a constructor for the type name like this  create function name  firstname varchar  20   lastname varchar  20   returns name begin set self.firstname = firstname ; set self.lastname = lastname ; end we can then use new name  ? john ?  ? smith ?  to create a value of the type name we can construct a row value by listing its attributes within parentheses for instance  if we declare an attribute name as a row type with components firstname thesumit67.blogspot.com 952 chapter 22 object-based databases and lastname we can construct this value for it   ? ted ?  ? codd ?  without using a constructor by default every structured type has a constructor with no arguments  which sets the attributes to their default values.any other constructors have to be created explicitly there can be more than one constructor for the same structured type ; although they have the same name  they must be distinguishable by the number of arguments and types of their arguments the following statement illustrates how we can create a new tuple in the person relation we assume that a constructor has been defined for address  just like the constructor we defined for name insert into person values  new name  ? john ?  ? smith ?   new address  ? 20 main st ?  ? new york ?  ? 11001 ?   date ? 1960-8-22 ?  ; 22.3.2 type inheritance suppose that we have the following type definition for people  create type person  name varchar  20   address varchar  20   ; we maywant to store extra information in the database about people who are students  and about peoplewho are teachers since students and teachers are also people  we can use inheritance to define the student and teacher types in sql  create type student under person  degree varchar  20   department varchar  20   ; create type teacher under person  salary integer  department varchar  20   ; both student and teacher inherit the attributes of person ? namely  name and address student and teacher are said to be subtypes of person  and person is a supertype of student  as well as of teacher methods of a structured type are inherited by its subtypes  just as attributes are however  a subtype can redefine the effect of a method by declaring the method again  using overriding method in place of method in the method declaration thesumit67.blogspot.com 22.3 structured types and inheritance in sql 953 the sql standard requires an extra field at the end of the type definition  whose value is either final or not final the keyword final says that subtypes may not be created from the given type  while not final says that subtypes may be created now suppose that we want to store information about teaching assistants  who are simultaneously students and teachers  perhaps even in different departments we can do this if the type system supports multiple inheritance  where a type is declared as a subtype of multiple types note that the sql standard does not support multiple inheritance  although future versions of the sql standard may support it  so we discuss the concept here for instance  if our type system supports multiple inheritance  we can define a type for teaching assistant as follows  create type teachingassistant under student  teacher ; teachingassistant inherits all the attributes of student and teacher there is a problem  however  since the attributes name  address  and department are present in student  as well as in teacher the attributes name and address are actually inherited froma common source  person so there is no conflict caused by inheriting them from student as well as teacher however  the attribute department is defined separately in student and teacher in fact  a teaching assistant may be a student of one department and a teacher in another department to avoid a conflict between the two occurrences of department  we can rename themby using an as clause  as in this definition of the type teachingassistant  create type teachingassistant under student with  department as student dept   teacher with  department as teacher dept  ; in sql  as in most other languages  a value of a structured type must have exactly one most-specific type that is  each value must be associated with one specific type  called its most-specific type,when it is created bymeans of inheritance  it is also associated with each of the supertypes of its most-specific type for example  suppose that an entity has the type person  as well as the type student then  the most-specific type of the entity is student  since student is a subtype of person however  an entity can not have the type student as well as the type teacher unless it has a type  such as teachingassistant  that is a subtype of teacher  as well as of student  which is not possible in sql since multiple inheritance is not supported by sql   thesumit67.blogspot.com 954 chapter 22 object-based databases 22.4 table inheritance subtables in sql correspond to the e-r notion of specialization/generalization for instance  suppose we define the people table as follows  create table people of person ; we can then define tables students and teachers as subtables of people  as follows  create table students of student under people ; create table teachers of teacher under people ; the typesof the subtables  student and teacher in the above example  are subtypes of the type of the parent table  person in the above example   as a result  every attribute present in the table people is also present in the subtables students and teachers further  when we declare students and teachers as subtables of people  every tuple present in students or teachers becomes implicitly present in people thus  if a query uses the table people  it will find not only tuples directly inserted into that table  but also tuples inserted into its subtables  namely students and teachers however  only those attributes that are present in people can be accessed by that query sql permits us to find tuples that are in people but not in its subtables by using ? only people ? in place of people in a query the only keyword can also be used in delete and update statements without the only keyword  a delete statement on a supertable  such as people  also deletes tuples that were originally inserted in subtables  such as students  ; for example  a statement  delete from people where p ; would delete all tuples from the table people  as well as its subtables students and teachers  that satisfy p if the only keyword is added to the above statement  tuples that were inserted in subtables are not affected  even if they satisfy the where clause conditions subsequent queries on the supertable would continue to find these tuples conceptually  multiple inheritance is possiblewith tables  just as it is possible with types for example  we can create a table of type teachingassistant  create table teaching assistants of teachingassistant under students  teachers ; thesumit67.blogspot.com 22.4 table inheritance 955 as a result of the declaration  every tuple present in the teaching assistants table is also implicitly present in the teachers and in the students table  and in turn in the people table we note  however  that multiple inheritance of tables is not supported by sql there are some consistency requirements for subtables before we state the constraints  we need a definition  we say that tuples in a subtable and parent table correspond if they have the same values for all inherited attributes thus  corresponding tuples represent the same entity the consistency requirements for subtables are  1 each tuple of the supertable can correspond to at most one tuple in each of its immediate subtables 2 sql has an additional constraint that all the tuples corresponding to each other must be derived from one tuple  inserted into one table   for example  without the first condition  we could have two tuples in students  or teachers  that correspond to the same person the second condition rules out a tuple in people corresponding to both a tuple in students and a tuple in teachers  unless all these tuples are implicitly present because a tuple was inserted in a table teaching assistants  which is a subtable of both teachers and students since sql does not support multiple inheritance  the second condition actually prevents a person from being both a teacher and a student even if multiple inheritancewere supported  the same problemwould arise if the subtable teaching assistants were absent obviously it would be useful to model a situation where a person can be a teacher and a student  even if a common subtable teaching assistants is not present thus  it can be useful to remove the second consistency constraint doing so would allow an object to have multiple types  without requiring it to have a most-specific type for example  suppose we again have the type person  with subtypes student and teacher  and the corresponding table people  with subtables teachers and students we can then have a tuple in teachers and a tuple in students corresponding to the same tuple in people there is no need to have a type teachingassistant that is a subtype of both student and teacher we need not create a type teachingassistant unless we wish to store extra attributes or redefine methods in a manner specific to people who are both students and teachers we note  however  that sql unfortunately prohibits such a situation  because of consistency requirement 2 since sql also does not support multiple inheritance  we can not use inheritance to model a situation where a person can be both a student and a teacher as a result  sql subtables can not be used to represent overlapping specializations from the e-r model we can of course create separate tables to represent the overlapping specializations/ generalizations without using inheritance the process was described earlier  in section 7.8.6.1 in the above example,wewould create tables people  students  andteachers,withthe students and teachers tables containing the primary-key thesumit67.blogspot.com 956 chapter 22 object-based databases attribute of person and other attributes specific to student and teacher  respectively the people table would contain information about all persons  including students and teachers we would then have to add appropriate referential-integrity constraints to ensure that students and teachers are also represented in the people table in other words  we can create our own improved implementation of the subtable mechanism using existing features of sql  with some extra effort in defining the table  as well as some extra effort at query time to specify joins to access required attributes we note that sql defines a privilege called under  which is required in order to create a subtype or subtable under another type or table the motivation for this privilege is similar to that for the references privilege 22.5 array and multiset types in sql sql supports two collection types  arrays and multisets ; array types were added in sql  1999  while multiset types were added in sql  2003 recall that a multiset is an unordered collection  where an element may occur multiple times multisets are like sets  except that a set allows each element to occur at most once suppose we wish to record information about books  including a set of keywords for each book suppose also that we wished to store the names of authors of a book as an array ; unlike elements in a multiset  the elements of an array are ordered  so we can distinguish the first author from the second author  and so on the following example illustrates how these array and multiset-valued attributes can be defined in sql  create type publisher as  name varchar  20   branch varchar  20   ; create type book as  title varchar  20   author array varchar  20  array  10   pub date date  publisher publisher  keyword set varchar  20  multiset  ; create table books of book ; the first statement defines a type called publisher with two components  a name and a branch the second statement defines a structured type book that contains a title  an author array  which is an array of up to 10 author names  a publication date  a publisher  of type publisher   and a multiset of keywords finally  a table books containing tuples of type book is created thesumit67.blogspot.com 22.5 array and multiset types in sql 957 note that we used an array  instead of a multiset  to store the names of authors  since the ordering of authors generally has some significance  whereas we believe that the ordering of keywords associated with a book is not significant in general  multivalued attributes from an e-r schema can be mapped to multiset-valued attributes in sql ; if ordering is important  sql arrays can be used instead of multisets 22.5.1 creating and accessing collection values an array of values can be created in sql  1999 in this way  array  ? silberschatz ?  ? korth ?  ? sudarshan ?  similarly  a multiset of keywords can be constructed as follows  multiset  ? computer ?  ? database ?  ? sql ?  thus,we can create a tuple of the type defined by the books relation as   ? compilers ?  array  ? smith ?  ? jones ?   new publisher  ? mcgraw-hill ?  ? new york ?   multiset  ? parsing ?  ? analysis ?   here we have created a value for the attribute publisher by invoking a constructor function for publisher with appropriate arguments note that this constructor for publisher must be created explicitly  and is not present by default ; it can be declared just like the constructor for name  which we saw earlier in section 22.3 if we want to insert the preceding tuple into the relation books  we could execute the statement  insert into books values  ? compilers ?  array  ? smith ?  ? jones ?   new publisher  ? mcgraw-hill ?  ? new york ?   multiset  ? parsing ?  ? analysis ?   ; we can access or update elements of an array by specifying the array index  for example author array  1   22.5.2 querying collection-valued attributes we now consider how to handle collection-valued attributes in queries an expression evaluating to a collection can appear anywhere that a relation name may appear  such as in a from clause  as the following paragraphs illustrate we use the table books that we defined earlier if we want to find all books that have the word ? database ? as one of their keywords  we can use this query  thesumit67.blogspot.com 958 chapter 22 object-based databases select title from books where ? database ? in  unnest  keyword set   ; note that we have used unnest  keyword set  in a positionwhere sql without nested relations would have required a select-from-where subexpression if we know that a particular book has three authors  we could write  select author array  1   author array  2   author array  3  from books where title = ? database system concepts ? ; now  suppose that we want a relation containing pairs of the form ? title  author name ? for each book and each author of the book.we can use this query  select b.title  a.author from books as b  unnest  b.author array  as a  author  ; since the author array attribute of books is a collection-valued field  unnest  b.author array  can be used in a from clause  where a relation is expected note that the tuple variable b is visible to this expression since it is defined earlier in the from clause when unnesting an array  the previous query loses information about the ordering of elements in the array the unnest with ordinality clause can be used to get this information  as illustrated by the following query this query can be used to generate the authors relation,whichwesaw earlier  fromthe books relation select title  a.author  a.position from books as b  unnest  b.author array  with ordinality as a  author  position  ; the with ordinality clause generates an extra attribute which records the position of the element in the array a similar query  but without the with ordinality clause  can be used to generate the keyword relation 22.5.3 nesting and unnesting the transformation of a nested relation into a form with fewer  or no  relationvalued attributes is called unnesting the books relation has two attributes  author array and keyword set  that are collections  and two attributes  title and publisher  that are not suppose that we want to convert the relation into a single flat relation  with no nested relations or structured types as attributes.we can use the following query to carry out the task  thesumit67.blogspot.com 22.5 array and multiset types in sql 959 title author pub_name pub_branch keyword compilers smith mcgraw-hill new york parsing compilers jones mcgraw-hill new york parsing compilers smith mcgraw-hill new york analysis compilers jones mcgraw-hill new york analysis networks jones oxford london internet networks frick oxford london internet networks jones oxford london web networks frick oxford london web figure 22.3 flat books  result of unnesting attributes author array and keyword set of relation books select title  a.author  publisher.name as pub name  publisher.branch as pub branch  k.keyword from books as b  unnest  b.author array  as a  author   unnest  b.keyword set  as k  keyword  ; the variable b in the from clause is declared to range over books the variable a is declared to range over the authors in author array for the book b  and k is declared to range over the keywords in the keyword set of the book b figure 22.1 shows an instance books relation  and figure 22.3 shows the relation  which we call flat books  that is the result of the preceding query note that the relation flat books is in 1nf  since all its attributes are atomic valued the reverse process of transforming a 1nf relation into a nested relation is called nesting nesting can be carried out by an extension of grouping in sql in the normal use of grouping in sql  a temporary multiset relation is  logically  created for each group  and an aggregate function is applied on the temporary relation to get a single  atomic  value the collect function returns the multiset of values  so instead of creating a single value  we can create a nested relation suppose that we are given the 1nf relation flat books  as in figure 22.3 the following query nests the relation on the attribute keyword  select title  author  publisher  pub name  pub branch  as publisher  collect  keyword  as keyword set from flat books group by title  author  publisher ; the result of the query on the flat books relation from figure 22.3 appears in figure 22.4 if we want to nest the author attribute also into a multiset  we can use the query  thesumit67.blogspot.com 960 chapter 22 object-based databases title author publisher keyword_set  pub_name,pub_branch  compilers smith  mcgraw-hill  new york   parsing  analysis  compilers jones  mcgraw-hill  new york   parsing  analysis  networks jones  oxford  london   internet  web  networks frick  oxford  london   internet  web  figure 22.4 a partially nested version of the flat books relation select title  collect  author  as author set  publisher  pub name  pub branch  as publisher  collect  keyword  as keyword set from flat books group by title  publisher ; another approach to creating nested relations is to use subqueries in the select clause an advantage of the subquery approach is that an order by clause can be used in the subquery to generate results in the order desired for the creation of an array the following query illustrates this approach ; the keywords array and multiset specify that an array and multiset  respectively  are to be created from the results of the subqueries select title  array  select author from authors as a where a.title = b.title order by a.position  as author array  publisher  pub name  pub branch  as publisher  multiset  select keyword from keywords as k where k.title = b.title  as keyword set  from books4 as b ; the system executes the nested subqueries in the select clause for each tuple generated by the from and where clauses of the outer query observe that the attribute b.title from the outer query is used in the nested queries  to ensure that only the correct sets of authors and keywords are generated for each title sql  2003 provides a variety of operators on multisets  including a function set  m  that returns a duplicate-free version of a multiset m  an intersection aggregate operation,which returns the intersection of all the multisets in a group  a fusion aggregate operation  which returns the union of all multisets in a group  and a submultiset predicate  which checks if a multiset is contained in another multiset thesumit67.blogspot.com 22.6 object-identity and reference types in sql 961 the sql standard does not provide any way to update multiset attributes except by assigning a new value for example  to delete a value v from a multiset attribute a  we would have to set it to  a except all multiset  v    22.6 object-identity and reference types in sql object-oriented languages provide the ability to refer to objects an attribute of a type can be a reference to an object of a specified type for example  in sql we can define a type department with a field name and a field head that is a reference to the type person  and a table departments of type department  as follows  create type department  name varchar  20   head ref  person  scope people  ; create table departments of department ; here  the reference is restricted to tuples of the table people the restriction of the scope of a reference to tuples of a table is mandatory in sql  and it makes references behave like foreign keys we can omit the declaration scope people fromthe typedeclarationand instead make an addition to the create table statement  create table departments of department  head with options scope people  ; the referenced table must have an attribute that stores the identifier of the tuple we declare this attribute  called the self-referential attribute  by adding a ref is clause to the create table statement  create table people of person ref is person id system generated ; here  person id is an attribute name  not a keyword  and the create table statement specifies that the identifier is generated automatically by the database in order to initialize a reference attribute  we need to get the identifier of the tuple that is to be referenced.we can get the identifier value of a tuple by means of a query thus  to create a tuple with the reference value  we may first create the tuple with a null reference and then set the reference separately  thesumit67.blogspot.com 962 chapter 22 object-based databases insert into departments values  ? cs ?  null  ; update departments set head =  select p.person id from people as p where name = ? john ?  where name = ? cs ? ; an alternative to system-generated identifiers is to allow users to generate identifiers the type of the self-referential attribute must be specified as part of the type definition of the referenced table  and the table definition must specify that the reference is user generated  create type person  name varchar  20   address varchar  20   ref using varchar  20  ; create table people of person ref is person id user generated ; when inserting a tuple in people  we must then provide a value for the identifier  insert into people  person id  name  address  values  ? 01284567 ?  ? john ?  ? 23 coyote run ?  ; no other tuple for people or its supertables or subtables can have the same identifier.we can then use the identifier value when inserting a tuple into departments  without the need for a separate query to retrieve the identifier  insert into departments values  ? cs ?  ? 01284567 ?  ; it is even possible to use an existing primary-key value as the identifier  by including the ref from clause in the type definition  create type person  name varchar  20  primary key  address varchar  20   ref from  name  ; create table people of person ref is person id derived ; thesumit67.blogspot.com 22.7 implementing o-r features 963 note that the table definition must specify that the reference is derived  and must still specify a self-referential attribute name when inserting a tuple for departments  we can then use  insert into departments values  ? cs ?  ? john ?  ; references are dereferenced in sql  1999 by the  > symbol consider the departments table defined earlier we can use this query to find the names and addresses of the heads of all departments  select head > name  head > address from departments ; an expression such as ? head > name ? is called a path expression since head is a reference to a tuple in the people table  the attribute name in the preceding query is the name attribute of the tuple from the people table references can be used to hide join operations ; in the preceding example  without the references  the head field of department would be declared a foreign key of the table people to find the name and address of the head of a department  we would require an explicit join of the relations departments and people the use of references simplifies the query considerably we can use the operation deref to return the tuple pointed to by a reference  and then access its attributes  as shown below  select deref  head  .name from departments ; 22.7 implementing o-r features object-relational database systems are basically extensions of existing relational database systems changes are clearly required at many levels of the database system however  to minimize changes to the storage-system code  relation storage  indices  etc   the complex data types supported by object-relational systems can be translated to the simpler type system of relational databases to understand how to do this translation  we need look only at how some features of the e-r model are translated into relations for instance  multivalued attributes in the e-r model correspond to multiset-valued attributes in the objectrelational model composite attributes roughly correspond to structured types isa hierarchies in the e-r model correspond to table inheritance in the objectrelational model the techniques for converting e-r model features to tables  which we saw in section 7.6  can be used  with some extensions  to translate object-relational data to relational data at the storage level thesumit67.blogspot.com 964 chapter 22 object-based databases subtables can be stored in an efficient manner  without replication of all inherited fields  in one of two ways  ? each table stores the primary key  which may be inherited from a parent table  and the attributes that are defined locally inherited attributes  other than the primary key  do not need to be stored  and can be derived by means of a join with the supertable  based on the primary key ? each table stores all inherited and locally defined attributes.when a tuple is inserted  it is stored only in the table inwhich it is inserted  and its presence is inferred in each of the supertables access to all attributes of a tuple is faster  since a join is not required however  in case the type system allows an entity to be represented in two subtables without being present in a common subtable of both  this representation can result in replication of information further  it is hard to translate foreign keys referring to a supertable into constraints on the subtables ; to implement such foreign keys efficiently  the supertable has to be defined as a view  and the database system would have to support foreign keys on views implementations may choose to represent array and multiset types directly  or may choose to use a normalized representation internally normalized representations tend to take up more space and require an extra join/grouping cost to collect data in an array or multiset however  normalized representationsmay be easier to implement the odbc and jdbc application program interfaces have been extended to retrieve and store structured types jdbc provides a method getobject   that is similar to getstring   but returns a java struct object  from which the components of the structured type can be extracted it is also possible to associate a java class with an sql structured type  and jdbc will then convert between the types see the odbc or jdbc reference manuals for details 22.8 persistent programming languages database languages differ from traditional programming languages in that they directly manipulate data that are persistent ? that is  data that continue to exist even after the program that created it has terminated.a relation in a database and tuples in a relation are examples of persistent data in contrast  the only persistent data that traditional programming languages directly manipulate are files access to a database is only one component of any real-world application while a data-manipulation language like sql is quite effective for accessing data  a programming language is required for implementing other components of the application such as user interfaces or communication with other computers the traditional way of interfacing database languages to programming languages is by embedding sql within the programming language thesumit67.blogspot.com 22.8 persistent programming languages 965 a persistent programming language is a programming language extended with constructs to handle persistent data persistent programming languages can be distinguished from languages with embedded sql in at least two ways  1 with an embedded language  the type system of the host language usually differs from the type system of the data-manipulation language the programmer is responsible for any type conversions between the host language and sql having the programmer carry out this task has several drawbacks  ? the code to convert between objects and tuples operates outside the object-oriented type system  and hence has a higher chance of having undetected errors ? conversion between the object-oriented format and the relational format of tuples in the database takes a substantial amount of code the format translation code  along with the code for loading and unloading data from a database  can form a significant percentage of the total code required for an application in contrast  in a persistent programming language  the query language is fully integrated with the host language  and both share the same type system objects can be created and stored in the database without any explicit type or format changes ; any format changes required are carried out transparently 2 the programmer using an embedded query language is responsible for writing explicit code to fetch data from the database into memory if any updates are performed  the programmer must write code explicitly to store the updated data back in the database in contrast  in a persistent programming language  the programmer can manipulate persistent data without writing code explicitly to fetch it into memory or store it back to disk in this section  we describe how object-oriented programming languages  such as c + + and java  can be extended to make them persistent programming languages these language features allow programmers to manipulate data directly from the programming language  without having to go through a datamanipulation language such as sql they thereby provide tighter integration of the programming languageswith the database than  for example  embedded sql there are certain drawbacks to persistent programming languages  however  that we must keep in mind when deciding whether to use them since the programming language is usually a powerful one  it is relatively easy to make programming errors that damage the database the complexity of the language makes automatic high-level optimization  such as to reduce disk i/o  harder support for declarative querying is important for many applications  but persistent programming languages currently do not support declarative querying well in this section  we describe a number of conceptual issues that must be addressed when adding persistence to an existing programming language.we first thesumit67.blogspot.com 966 chapter 22 object-based databases address language-independent issues  and in subsequent sections we discuss issues that are specific to the c + + language and to the java language however  we do not cover details of language extensions ; although several standards have been proposed  none has met universal acceptance see the references in the bibliographical notes to learn more about specific language extensions and further details of implementations 22.8.1 persistence of objects object-oriented programming languages already have a concept of objects  a type system to define object types  and constructs to create objects however  these objects are transient ? they vanish when the program terminates  just as variables in a java or c program vanish when the program terminates if we wish to turn such a language into a database programming language  the first step is to provide a way tomake objects persistent several approaches have been proposed ? persistence by class the simplest  but least convenient  way is to declare that a class is persistent all objects of the class are then persistent objects by default objects of nonpersistent classes are all transient this approach is not flexible  since it is often useful to have both transient and persistent objects in a single class.manyobject-oriented database systems interpret declaring a class to be persistent as saying that objects in the class potentially can be made persistent  rather than that all objects in the class are persistent such classes might more appropriately be called ? persistable ? classes ? persistence by creation in this approach  new syntax is introduced to create persistent objects  by extending the syntax for creating transient objects thus  an object is either persistent or transient  depending on how it was created several object-oriented database systems follow this approach ? persistence by marking a variant of the preceding approach is to mark objects as persistent after they are created all objects are created as transient objects  but  if an object is to persist beyond the execution of the program  it must be marked explicitly as persistent before the program terminates this approach  unlike the previous one  postpones the decision on persistence or transience until after the object is created ? persistence by reachability one or more objects are explicitly declared as  root  persistent objects.all other objects are persistent if  and only if  they are reachable from the root object through a sequence of one or more references thus  all objects referenced by  that is  whose object identifiers are stored in  the root persistent objects are persistent but also  all objects referenced from these objects are persistent  and objects to which they refer are in turn persistent  and so on a benefit of this scheme is that it is easy to make entire data structures persistent by merely declaring the root of such structures as persistent howthesumit67 blogspot.com 22.8 persistent programming languages 967 ever  the database system has the burden of following chains of references to detect which objects are persistent  and that can be expensive 22.8.2 object identity and pointers in an object-oriented programming language that has not been extended to handle persistence  when an object is created  the system returns a transient object identifier transient object identifiers are valid only when the program that created them is executing ; after that program terminates  the objects are deleted  and the identifier is meaningless when a persistent object is created  it is assigned a persistent object identifier the notion of object identity has an interesting relationship to pointers in programming languages a simple way to achieve built-in identity is through pointers to physical locations in storage in particular  in many object-oriented languages such as c + +  a transient object identifier is actually an in-memory pointer however  the association of an object with a physical location in storage may change over time there are several degrees of permanence of identity  ? intraprocedure identity persists only during the execution of a single procedure examples of intraprogram identity are local variables within procedures ? intraprogram identity persists only during the execution of a single program or query examples of intraprogram identity are global variables in programming languages main-memory or virtual-memory pointers offer only intraprogram identity ? interprogram identity persists from one program execution to another pointers to file-system data on disk offer interprogram identity  but they may change if the way data is stored in the file system is changed ? persistent identity persists not only among program executions  but also among structural reorganizations of the data it is the persistent form of identity that is required for object-oriented systems in persistent extensions of languages such as c + +  object identifiers for persistent objects are implemented as ? persistent pointers ? a persistent pointer is a type of pointer that  unlike in-memory pointers  remains valid even after the end of a program execution  and across some forms of data reorganization a programmer may use a persistent pointer in the same ways that she may use an in-memory pointer in a programming language conceptually  we may think of a persistent pointer as a pointer to an object in the database 22.8.3 storage and access of persistent objects what does it mean to store an object in a database ? clearly  the data part of an object has to be stored individually for each object logically  the code that thesumit67.blogspot.com 968 chapter 22 object-based databases implements methods of a class should be stored in the database as part of the database schema  along with the type definitions of the classes however  many implementations simply store the code in files outside the database  to avoid having to integrate system software such as compilers with the database system there are several ways to find objects in the database one way is to give names to objects  just as we give names to files this approach works for a relatively small number of objects  but does not scale to millions of objects a second way is to expose object identifiers or persistent pointers to the objects  which can be stored externally unlike names  these pointers do not have to be mnemonic  and they can even be physical pointers into a database a third way is to store collections of objects  and to allow programs to iterate over the collections to find required objects collections of objects can themselves be modeled as objects of a collection type collection types include sets  multisets  that is  sets with possibly many occurrences of a value   lists  and so on a special case of a collection is a class extent,which is the collection of all objects belonging to the class if a class extent is present for a class  then  whenever an object of the class is created  that object is inserted in the class extent automatically  and  whenever an object is deleted  that object is removed from the class extent class extents allow classes to be treated like relations in that we can examine all objects in the class  just as we can examine all tuples in a relation most object-oriented database systems support all three ways of accessing persistent objects they give identifiers to all objects they usually give names only to class extents and other collection objects  and perhaps to other selected objects  but not to most objects they usually maintain class extents for all classes that can have persistent objects  but  in many of the implementations  the class extents contain only persistent objects of the class 22.8.4 persistent c + + systems there are several object-oriented databases based on persistent extensions to c + +  see the bibliographical notes   there are differences among them in terms of the system architecture  yet they have many common features in terms of the programming language several of the object-oriented features of the c + + language provide support for persistence without changing the language itself for example  we can declare a class called persistent object with attributes and methods to support persistence ; any other class that should be persistent can be made a subclass of this class  and thereby inherit the support for persistence the c + + language  like some othermodern programming languages  also lets us redefine standard function names and operators ? such as +    the pointer dereference operator  >  and so on ? according to the types of the operands on which they are applied this ability is called overloading ; it is used to redefine operators to behave in the required manner when they are operating on persistent objects providing persistence support via class libraries has the benefit of making only minimal changes to c + + necessary ; moreover  it is relatively easy to implement however  it has the drawback that the programmer has to spend much more thesumit67.blogspot.com 22.8 persistent programming languages 969 time to write a program that handles persistent objects  and it is not easy for the programmer to specify integrity constraints on the schema or to provide support for declarative querying some persistent c + + implementations support extensions to the c + + syntax to make these tasks easier the following aspects need to be addressedwhen adding persistence support to c + +  and other languages   ? persistent pointers  a newdata type has to be defined to represent persistent pointers for example  the odmg c + + standard defines a template class d ref < t > to represent persistent pointers to a class t the dereference operator on this class is redefined to fetch the object from disk  if not already present in memory   and it returns an in-memory pointer to the buffer where the object has been fetched thus if p is a persistent pointer to a class t  one can use standard syntax such as p > a or p > f  v  to access attribute a of class t or invoke method f of class t the objectstore database system uses a different approach to persistent pointers it uses normal pointer types to store persistent pointers this poses two problems   1  in-memory pointer sizes may be only 4 bytes  which is too small to use with databases larger than 4 gigabytes  and  2  when an object is moved on disk  in-memory pointers to its old physical location are meaningless objectstore uses a technique called ? hardware swizzling ? to address both problems ; it prefetches objects from the database into memory  and replaces persistent pointerswith in-memory pointers  andwhen data are stored back on disk  in-memory pointers are replaced by persistent pointers when on disk  the value stored in the in-memory pointer field is not the actual persistent pointer ; instead  the value is looked up in a table that contains the full persistent pointer value ? creation of persistent objects  the c + + new operator is used to create persistent objects by defining an ? overloaded ? version of the operator that takes extra arguments specifying that it should be created in the database thus instead of new t    one would call new  db  t   to create a persistent object  where db identifies the database ? class extents  class extents are created and maintained automatically for each class the odmg c + + standard requires the name of the class to be passed as an additional parameter to the new operation this also allows multiple extents to be maintained for a class  by passing different names ? relationships  relationships between classes are often represented by storing pointers from each object to the objects to which it is related objects related to multiple objects of a given class store a set of pointers thus if a pair of objects is in a relationship  each should store a pointer to the other persistent c + + systems provide a way to specify such integrity constraints and to enforce them by automatically creating and deleting pointers  for example  if a pointer is created from an object a to an object b  a pointer to a is added automatically to object b thesumit67.blogspot.com 970 chapter 22 object-based databases ? iterator interface  since programs need to iterate over class members  an interface is required to iterate over members of a class extent the iterator interface also allows selections to be specified  so that only objects satisfying the selection predicate need to be fetched ? transactions  persistent c + + systems provide support for starting a transaction  and for committing it or rolling it back ? updates  one of the goals of providing persistence support in a programming language is to allow transparent persistence that is  a function that operates on an object should not need to know that the object is persistent ; the same functions can thus be used on objects regardless ofwhether they are persistent or not however  one resultant problem is that it is difficult to detect when an object has been updated some persistent extensions to c + + require the programmer to specify explicitly that an object has been modified by calling a function mark modified    in addition to increasing programmer effort  this approach increases the chance that programming errors can result in a corrupt database if a programmer omits a call to mark modified    it is possible that one update made by a transaction may never be propagated to the database  while another update made by the same transaction is propagated  violating atomicity of transactions other systems  such as objectstore  use memory-protection support provided by the operating system/hardware to detect writes to a block of memory and mark the block as a dirty block that should be written later to disk ? query language  iterators provide support for simple selection queries to support more complex queries  persistent c + + systems define a query language a large number of object-oriented database systems based on c + + were developed in the late 1980s and early 1990s.however  the market for such databases turned out to be much smaller than anticipated  since most application requirements are more than met by using sql through interfaces such as odbc or jdbc as a result  most of the object-oriented database systems developed in that period do not exist any longer in the 1990s  the object data management group  odmg  defined standards for adding persistence to c + + and java however  the group wound up its activities around 2002 objectstore and versant are among the original object-oriented database systems that are still in existence although object-oriented database systems did not find the commercial success that they had hoped for  the motivation for adding persistence to programming language remains there are several applications with high performance requirements that run on object-oriented database systems ; using sql would impose too high a performance overhead for many such systems with objectrelational database systems now providing support for complex data types  including references  it is easier to store programming language objects in an sql thesumit67.blogspot.com 22.8 persistent programming languages 971 database a new generation of object-oriented database systems using objectrelational databases as a backend may yet emerge 22.8.5 persistent java systems the java language has seen an enormous growth in usage in recent years.demand for support for persistence of data in java programs has grown correspondingly initial attempts at creating a standard for persistence in java were led by the odmg consortium ; the consortium wound up its efforts later  but transferred its design to the java database objects  jdo  effort  which is coordinated by sun microsystems the jdo model for object persistence in java programs differs from the model for persistence support in c + + programs among its features are  ? persistence by reachability  objects are not explicitly created in a database explicitly registering an object as persistent  using the makepersistent   method of the persistencemanager class  makes the object persistent in addition  any object reachable from a persistent object becomes persistent ? byte code enhancement  instead of declaring a class to be persistent in the java code  classes whose objects may be made persistent are specified in a configuration file  with suffix .jdo   an implementation-specific enhancer program is executed that reads the configuration file and carries out two tasks first  it may create structures in a database to store objects of the class second  it modifies the byte code  generated by compiling the java program  to handle tasks related to persistence below are some examples of such modifications  ? any code that accesses an object could be changed to check first if the object is in memory  and if not  take steps to bring it into memory ? any code that modifies an object is modified to record additionally that the object has been modified  and perhaps to save a pre-updated value used in case the update needs to be undone  that is  if the transaction is rolled back   other modifications to the byte code may also be carried out such byte code modification is possible since the byte code is standard across all platforms  and includes much more information than compiled object code ? database mapping  jdo does not define how data are stored in the back-end database for example  a common scenario is to store objects in a relational database the enhancer program may create an appropriate schema in the database to store class objects how exactly it does this is implementation dependent and not defined by jdo some attributes could be mapped to relational attributes  while others may be stored in a serialized form  treated as a binary object by the database jdo implementations may allow existing relational data to be viewed as objects by defining an appropriate mapping thesumit67.blogspot.com 972 chapter 22 object-based databases ? class extents  class extents are created and maintained automatically for each class declared to be persistent all objects made persistent are added automatically to the class extent corresponding to their class jdo programs may access a class extent  and iterate over selected members the iterator interface provided by java can be used to create iterators on class extents  and to step through the members of the class extent jdo also allows selections to be specified when an iterator is created on a class extent  and only objects satisfying the selection are fetched ? single reference type  there is no difference in type between a reference to a transient object and a reference to a persistent object one approach to achieving such a unification of pointer types would be to load the entire database into memory  replacing all persistent pointers with in-memory pointers after updates were done  the process would be reversed  storing updated objects back on disk such an approach would be very inefficient for large databases we now describe an alternative approach that allows persistent objects to be fetched automatically into memory when required  while allowing all references contained in in-memory objects to be in-memory references.when an object a is fetched  a hollow object is created for each object bi that it references  and the in-memory copy of ahas references to the corresponding hollow object for each bi  of course the system has to ensure that if an object bi was fetched already  the reference points to the already fetched object instead of creating a new hollow object similarly  if an object bi has not been fetched  but is referenced by another object fetched earlier  it would already have a hollow object created for it ; the reference to the existing hollow object is reused  instead of creating a new hollow object thus  for every object oi that has been fetched  every reference from oi is either to an already fetched object or to a hollow object the hollow objects form a fringe surrounding fetched objects whenever the program actually accesses a hollow object o  the enhanced byte code detects this and fetches the object from the database when this object is fetched  the same process of creating hollow objects is carried out for all objects referenced by o after this the access to the object is allowed to proceed.5 an in-memory index structure mapping persistent pointers to in-memory references is required to implement this scheme in writing objects back to disk  this index would be used to replace in-memory references with persistent pointers in the copy written to disk 5the technique using hollow objects described above is closely related to the hardware swizzling technique  mentioned earlier in section 22.8.4  .hardware swizzling is used by some persistent c + + implementations to provide a single pointer type for persistent and in-memory pointers hardware swizzling uses virtual-memory protection techniques provided by the operating system to detect accesses to pages  and fetches the pages from the database when required in contrast  the java version modifies byte code to check for hollow objects  instead of using memory protection  and fetches objects when required  instead of fetching whole pages from the database thesumit67.blogspot.com 22.10 object-oriented versus object-relational 973 22.9 object-relational mapping so far we have seen two approaches to integrating object-oriented data models and programming languages with database systems object-relational mapping systems provide a third approach to integration of object-oriented programming languages and databases object-relational mapping systems are built on top of a traditional relational database  and allow a programmer to define a mapping between tuples in database relations and objects in the programming language unlike in persistent programming languages  objects are transient  and there is no permanent object identity an object  or a set of objects  can be retrieved based on a selection condition on its attributes ; relevant data are retrieved from the underlying database based on the selection conditions  and one ormore objects are created from the retrieved data  based on the prespecified mapping between objects and relations the program can optionally update such objects  create new objects  or specify that an object is to be deleted  and then issue a save command ; the mapping from objects to relations is then used to correspondingly update  insert or delete tuples in the database object-relational mapping systems in general  and in particular the widely used hibernate system which provides an object-relational mapping to java  are described in more detail in section 9.4.2 the primary goal of object-relational mapping systems is to ease the job of programmers who build applications  by providing them an object-model  while retaining the benefits of using a robust relational database underneath as an added benefit  when operating on objects cached in memory  object-relational systems can provide significant performance gains over direct access to the underlying database object-relational mapping systems also provide query languages that allow programmers to write queries directly on the object model ; such queries are translated into sql queries on the underlying relational database  and result objects created from the sql query results on the negative side  object-relational mapping systems can suffer from significant overheads for bulk database updates  and may provide only limited querying capabilities however  it is possible to directly update the database  bypassing the object-relational mapping system  and to write complex queries directly in sql the benefits or object-relational models exceed the drawbacks for many applications  and object-relational mapping systems have seen widespread adoption in recent years 22.10 object-oriented versus object-relational we have now studied object-relational databases  which are object-oriented databases built on top of the relation model  as well as object-oriented databases  which are built around persistent programming languages  and object-relational thesumit67.blogspot.com 974 chapter 22 object-based databases mapping systems  which build an object layer on top of a traditional relational database each of these approaches targets a different market the declarative nature and limited power  compared to a programming language  of the sql language provides good protection of data fromprogramming errors  and makes high-level optimizations  such as reducing i/o  relatively easy  we covered optimization of relational expressions in chapter 13  object-relational systems aim at making data modeling and querying easier by using complex data types typical applications include storage and querying of complex data  including multimedia data a declarative language such as sql  however  imposes a significant performance penalty for certain kinds of applications that run primarily in main memory  and that perform a large number of accesses to the database persistent programming languages target such applications that have high performance requirements they provide low-overhead access to persistent data and eliminate the need for data translation if the data are to be manipulated by a programming language however  they are more susceptible to data corruption by programming errors  and they usually do not have a powerful querying capability typical applications include cad databases object-relational mapping systems allow programmers to build applications using an object model,while using a traditional database system to store the data thus  they combine the robustness of widely used relational database systems  with the power of object models for writing applications however  they suffer from overheads of data conversion between the object model and the relational model used to store data we can summarize the strengths of the various kinds of database systems in this way  ? relational systems  simple data types  powerful query languages  high protection ? persistent programming language ? based oodbs  complex data types  integration with programming language  high performance ? object-relational systems  complex data types  powerful query languages  high protection ? object-relational mapping systems  complex data types integrated with programming languages  designed as a layer on top of a relational database system these descriptions hold in general  but keep in mind that some database systems blur the boundaries for example  object-oriented database systems built around a persistent programming language can be implemented on top of a relational or object-relational database system such systems may provide lower performance than object-oriented database systems built directly on a storage system  but provide some of the stronger protection guarantees of relational systems thesumit67.blogspot.com review terms 975 22.11 summary ? the object-relational data model extends the relational data model by providing a richer type system including collection types and object orientation ? collection types include nested relations  sets  multisets  and arrays  and the object-relational model permits attributes of a table to be collections ? object orientation provides inheritance with subtypes and subtables  as well as object  tuple  references ? the sql standard includes extensions of the sql data-definition and query language to deal with new data types and with object orientation these include support for collection-valued attributes  inheritance  and tuple references such extensions attempt to preserve the relational foundations ? in particular  the declarative access to data ? while extending the modeling power ? object-relational database systems  that is  database systems based on the object-relation model  provide a convenient migration path for users of relational databases who wish to use object-oriented features ? persistent extensions to c + + and java integrate persistence seamlessly and orthogonally with existing programming language constructs and so are easy to use ? the odmg standard defines classes and other constructs for creating and accessing persistent objects from c + +  while the jdo standard provides equivalent functionality for java ? object-relational mapping systems provide an object view of data that is stored in a relational database objects are transient  and there is no notion of persistent object identity objects are created on-demand from relational data  and updates to objects are implemented by updating the relational data object-relational mapping systems have been widely adopted  unlike the more limited adoption of persistent programming languages ? we discussed differences between persistent programming languages and object-relational systems  andwe mention criteria for choosing between them review terms ? nested relations ? nested relational model ? complex types ? collection types ? large object types ? sets ? arrays ? multisets ? structured types ? methods thesumit67.blogspot.com 976 chapter 22 object-based databases ? row types ? constructors ? inheritance ? single inheritance ? multiple inheritance ? type inheritance ? most-specific type ? table inheritance ? subtable ? overlapping subtables ? reference types ? scope of a reference ? self-referential attribute ? path expressions ? nesting and unnesting ? sql functions and procedures ? persistent programming languages ? persistence by ? class ? creation ? marking ? reachability ? odmg c + + binding ? objectstore ? jdo ? persistence by reachability ? roots ? hollow objects ? object-relational mapping practice exercises 22.1 a car-rental company maintains a database for all vehicles in its current fleet for all vehicles  it includes the vehicle identification number  license number  manufacturer  model  date of purchase  and color special data are included for certain types of vehicles  ? trucks  cargo capacity ? sports cars  horsepower  renter age requirement ? vans  number of passengers ? off-road vehicles  ground clearance  drivetrain  four or two-wheel drive   construct an sql schema definition for this database use inheritance where appropriate 22.2 consider a database schema with a relation emp whose attributes are as shown below  with types specified for multivalued attributes emp =  ename  childrenset multiset  children   skillset multiset  skills   children =  name  birthday  skills =  type  examset setof  exams   exams =  year  city  thesumit67.blogspot.com practice exercises 977 a define the above schema in sql  with appropriate types for each attribute b using the above schema  write the following queries in sql i find the names of all employees who have a child born on or after january 1  2000 ii find those employees who took an examination for the skill type ? typing ? in the city ? dayton ?  iii list all skill types in the relation emp 22.3 consider the e-r diagram in figure 22.5  which contains composite  multivalued  and derived attributes a give an sql schema definition corresponding to the e-r diagram b give constructors for each of the structured types defined above 22.4 consider the relational schema shown in figure 22.6 a give a schemadefinition in sqlcorresponding to the relational schema  but using references to express foreign-key relationships b write each of the queries given in exercise 6.13 on the above schema  using sql instructor id name first_name middle_inital last_name address street street_number street_name apt_number city state zip  phone_number  date_of_birth age   figure 22.5 e-r diagram with composite  multivalued  and derived attributes thesumit67.blogspot.com 978 chapter 22 object-based databases employee  person name  street  city  works  person name  company name  salary  company  company name  city  manages  person name  manager name  figure 22.6 relational database for practice exercise 22.4 22.5 suppose that you have been hired as a consultant to choose a database system for your client ? s application for each of the following applications  state what type of database system  relational  persistent programming language ? based oodb  object relational ; do not specify a commercial product  you would recommend justify your recommendation a a computer-aided design system for a manufacturer of airplanes b a system to track contributions made to candidates for public office c an information system to support the making of movies 22.6 how does the concept of an object in the object-orientedmodel differ from the concept of an entity in the entity-relationship model ? exercises 22.7 redesign the database of practice exercise 22.2 into first normal form and fourth normal form list any functional or multivalued dependencies that you assume also list all referential-integrity constraints that should be present in the first and fourth normal form schemas 22.8 consider the schema from practice exercise 22.2 a give sql ddl statements to create a relation empa which has the same information as emp  butwheremultiset-valued attributes childrenset  skillsset and examsset are replaced by array-valued attributes childrenarray  skillsarray and examsarray b write a query to convert data from the schema of emp to that of empa  with the array of children sorted by birthday  the array of skills by the skill type and the array of exams by the year c write an sql statement to update the emp relation by adding a child jeb  with a birthdate of february 5  2001  to the employee named george d write an sql statement to perform the same update as above but on the empa relation make sure that the array of children remains sorted by year thesumit67.blogspot.com exercises 979 person id name address student instructor rank secretary hours_per_week employee salary tot_credits figure 22.7 specialization and generalization 22.9 consider the schemas for the table people  and the tables students and teachers,which were created under people  in section 22.4 give a relational schema in third normal form that represents the same information recall the constraints on subtables  and give all constraints that must be imposed on the relational schema so that every database instance of the relational schema can also be represented by an instance of the schema with inheritance 22.10 explain the distinction between a type x and a reference type ref  x   under what circumstances would you choose to use a reference type ? 22.11 consider the e-r diagram in figure 22.7  which contains specializations  using subtypes and subtables a give an sql schema definition of the e-r diagram b give an sql query to find the names of all people who are not secretaries c give an sql query to print the names of people who are neither employees nor students d can you create a person who is an employee and a student with the schema you created ? explain how  or explain why it is not possible 22.12 suppose a jdo database had an object a,which references object b  which in turn references objectc.assume all objects are on disk initially suppose a program first dereferences a  then dereferences b by following the reference from a  and then finally dereferences c show the objects that thesumit67.blogspot.com 980 chapter 22 object-based databases are represented in memory after each dereference  along with their state  hollow or filled  and values in their reference fields   tools there are considerable differences between database products in their support for object-relational features oracle probably has the most extensive support among the major database vendors the informix database system provides support for many object-relational features both oracle and informix provided objectrelational features before the sql  1999 standard was finalized  and have some features that are not part of sql  1999 information about objectstore and versant  including download of trial versions  may be obtained from their respective web sites  objectstore.com and versant com   the apache db project  db.apache.org  provides an object-relational mapping tool for java that supports both an odmg java and jdo apis a reference implementation of jdo may be obtained from sun.com ; use a search engine to get the full url bibliographical notes several object-oriented extensions to sql have been proposed postgres  stonebraker and rowe  1986  and stonebraker  1986   was an early implementation of an object-relational system other early object-relational systems include the sql extensions of o2  bancilhon et al  1989   and unisql  unisql  1991    sql  1999 was the product of an extensive  and long-delayed  standardization effort  which originally started off as adding object-oriented features to sql and ended up adding many more features  such as procedural constructs  which we saw earlier support for multiset types was added as part of sql  2003 melton  2002  concentrates on the object-relational features of sql  1999 eisenberg et al  2004  provides an overview of sql  2003  including its support for multisets a number of object-oriented database systems were developed in the late 1980s and early 1990s among the notable commercial ones were objectstore  lamb et al  1991    o2  lecluse et al  1988    and versant the object database standard odmg is described in detail in cattell  2000   jdo is described by roos  2002   tyagi et al  2003   and jordan and russell  2003   thesumit67.blogspot.com chapter23 xml the extensible markup language  xml  was not designed for database applications in fact  like the hyper-text markup language  html  onwhich theworld wide web is based  xml has its roots in document management  and is derived from a language for structuring large documents known as the standard generalized markup language  sgml   however  unlike sgml and html  xml is designed to represent data it is particularly useful as a data format when an application must communicate with another application  or integrate information from several other applications when xml is used in these contexts  many database issues arise  including how to organize  manipulate  and query the xml data in this chapter  we introduce xml and discuss both the management of xml data with database techniques and the exchange of data formatted as xml documents 23.1 motivation to understand xml  it is important tounderstandits roots as a document markup language the term markup refers to anything in a document that is not intended to be part of the printed output for example  a writer creating text that will eventually be typeset in a magazine may want to make notes about how the typesetting should be done it would be important to type these notes in a way so that they could be distinguished from the actual content  so that a note like ? set this word in large size  bold font ? or ? insert a line break here ? does not end up printed in the magazine such notes convey extra information about the text in electronic document processing  a markup language is a formal description of what part of the document is content  what part ismarkup  and what the markup means just as database systems evolved from physical file processing to provide a separate logical view  markup languages evolved from specifying instructions for how to print parts of the document to specifying the function of the content for instance  with functional markup  text representing section headings  for this section  the word ? motivation ?  would be marked up as being a section heading  instead of being marked up as text to be printed in large size  bold font from the viewpoint of typesetting  such functional markup allows the document to be 981 thesumit67.blogspot.com 982 chapter 23 xml formatted differently in different situations it also helps different parts of a large document  or different pages in a large web site  to be formatted in a uniform manner more importantly  functional markup also helps recordwhat each part of the text represents semantically  and correspondingly helps automate extraction of key parts of documents for the family of markup languages that includes html  sgml  and xml  the markup takes the form of tags enclosed in angle brackets  < >  tags are used in pairs  with < tag > and < /tag > delimiting the beginning and the end of the portion of the document to which the tag refers for example  the title of a document might be marked up as follows  < title > database system concepts < /title > unlike html  xml does not prescribe the set of tags allowed  and the set may be chosen as needed by each application this feature is the key to xml ? s major role in data representation and exchange  whereas html is used primarily for document formatting < university > < department > < dept name > comp sci < /dept name > < building > taylor < /building > < budget > 100000 < /budget > < /department > < department > < dept name > biology < /dept name > < building > watson < /building > < budget > 90000 < /budget > < /department > < course > < course id > cs-101 < /course id > < title > intro to computer science < /title > < dept name > comp sci < /dept name > < credits > 4 < /credits > < /course > < course > < course id > bio-301 < /course id > < title > genetics < /title > < dept name > biology < /dept name > < credits > 4 < /credits > < /course > continued in figure 23.2 figure 23.1 xml representation of  part of  university information thesumit67.blogspot.com 23.1 motivation 983 < instructor > < iid > 10101 < /iid > < name > srinivasan < /name > < dept name > comp sci < /dept name > < salary > 65000 < /salary > < /instructor > < instructor > < iid > 83821 < /iid > < name > brandt < /name > < dept name > comp sci < /dept name > < salary > 92000 < /salary > < /instructor > < instructor > < iid > 76766 < /iid > < name > crick < /name > < dept name > biology < /dept name > < salary > 72000 < /salary > < /instructor > < teaches > < iid > 10101 < /iid > < course id > cs-101 < /course id > < /teaches > < teaches > < iid > 83821 < /iid > < course id > cs-101 < /course id > < /teaches > < teaches > < iid > 76766 < /iid > < course id > bio-301 < /course id > < /teaches > < /university > figure 23.2 continuation of figure 23.1 for example  in our running university application  department  course and instructor information can be represented as part of an xml document as in figures 23.1 and 23.2 observe the use of tags such as department  course  instructor  and teaches to keep the example short  we use a simplified version of the university schema that ignores section information for courses we have also used the tag iid to denote the identifier of the instructor  for reasons we shall see later these tags provide context for each value and allow the semantics of the value to be identified for this example  the xml data representation does not provide any significant benefit over the traditional relational data representation ; however  we use this example as our running example because of its simplicity thesumit67.blogspot.com 984 chapter 23 xml < purchase order > < identifier > p-101 < /identifier > < purchaser > < name > cray z coyote < /name > < address > mesa flats  route 66  arizona 12345  usa < /address > < /purchaser > < supplier > < name > acme supplies < /name > < address > 1 broadway  new york  ny  usa < /address > < /supplier > < itemlist > < item > < identifier > rs1 < /identifier > < description > atom powered rocket sled < /description > < quantity > 2 < /quantity > < price > 199.95 < /price > < /item > < item > < identifier > sg2 < /identifier > < description > superb glue < /description > < quantity > 1 < /quantity > < unit-of-measure > liter < /unit-of-measure > < price > 29.95 < /price > < /item > < /itemlist > < total cost > 429.85 < /total cost > < payment terms > cash-on-delivery < /payment terms > < shipping mode > 1-second-delivery < /shipping mode > < /purchaseorder > figure 23.3 xml representation of a purchase order figure 23.3  which shows how information about a purchase order can be represented in xml  illustrates a more realistic use of xml purchase orders are typically generated by one organization and sent to another traditionally they were printed on paper by the purchaser and sent to the supplier ; the datawould be manually re-entered into a computer system by the supplier this slow process can be greatly sped up by sending the information electronically between the purchaser and supplier the nested representation allows all information in a purchase order to be represented naturally in a single document  real purchase orders have considerably more information than that depicted in this simplified example  xml provides a standardway of tagging the data ; the twoorganizations must of course agree on what tags appear in the purchase order  and what they mean thesumit67.blogspot.com 23.1 motivation 985 compared to storage of data in a relational database  the xml representation may be inefficient  since tag names are repeated throughout the document however  in spite of this disadvantage  an xml representation has significant advantageswhen it is used to exchange data between organizations  and for storing complex structured information in files  ? first  the presence of the tags makes the message self-documenting ; that is  a schema need not be consulted to understand the meaning of the text we can readily read the fragment above  for example ? second  the format of the document is not rigid for example  if some sender adds additional information  such as a tag last accessed noting the last date on which an account was accessed  the recipient of the xml data may simply ignore the tag as another example  in figure 23.3  the item with identifier sg2 has a tag called unit-of-measure specified  which the first item does not the tag is required for items that are ordered by weight or volume  and may be omitted for items that are simply ordered by number the ability to recognize and ignore unexpected tags allows the format of the data to evolve over time  without invalidating existing applications similarly  the ability to have multiple occurrences of the same tag makes it easy to represent multivalued attributes ? third  xml allows nested structures the purchase order shown in figure 23.3 illustrates the benefits of having a nested structure each purchase order has a purchaser and a list of items as two of its nested structures each item in turn has an item identifier  description and a price nested within it  while the purchaser has a name and address nested within it such information would have been split into multiple relations in a relational schema item information would have been stored in one relation  purchaser information in a second relation  purchase orders in a third  and the relationship between purchase orders  purchasers  and itemswould have been stored in a fourth relation the relational representation helps to avoid redundancy ; for example  item descriptions would be stored only once for each item identifier in a normalized relational schema in the xml purchase order  however  the descriptions may be repeated in multiple purchase orders that order the same item however  gathering all information related to a purchase order into a single nested structure  even at the cost of redundancy  is attractive when information has to be exchanged with external parties ? finally  since the xml format is widely accepted  a wide variety of tools are available to assist in its processing  including programming language apis to create and to read xml data  browser software  and database tools we describe several applications for xml data later  in section 23.7 just as sql is the dominant language for querying relational data  xml has become the dominant format for data exchange thesumit67.blogspot.com 986 chapter 23 xml 23.2 structure of xml data the fundamental construct in an xml document is the element an element is simply a pair ofmatching start and end-tags and all the text that appears between them xml documents must have a single root element that encompasses all other elements in the document in the example in figure 23.1  the < university > element forms the root element further  elements in an xmldocument must nest properly for instance  < course >    < title >    < /title >   < /course > is properly nested  whereas  < course >    < title >    < /course >  < /title > is not properly nested while proper nesting is an intuitive property,we may define it more formally text is said to appear in the context of an element if it appears between the starttag and end-tag of that element tags are properly nested if every start-tag has a unique matching end-tag that is in the context of the same parent element note that text may be mixed with the subelements of an element  as in figure 23.4 as with several other features of xml  this freedom makesmore sense in a document-processing context than in a data-processing context  and is not particularly useful for representing more-structured data such as database content in xml the ability to nest elements within other elements provides an alternative way to represent information figure 23.5 shows a representation of part of the university information from figure 23.1  but with course elements nested within department elements the nested representation makes it easy to find all courses offered by a department similarly  identifiers of courses taught by an instructor are nested within the instructor elements if an instructor teaches more than one course  there would be multiple course id elements within the correspond    < course > this course is being offered for the first time in 2009 < course id > bio-399 < /course id > < title > computational biology < /title > < dept name > biology < /dept name > < credits > 3 < /credits > < /course >    figure 23.4 mixture of text with subelements thesumit67.blogspot.com 23.2 structure of xml data 987 < university-1 > < department > < dept name > comp sci < /dept name > < building > taylor < /building > < budget > 100000 < /budget > < course > < course id > cs-101 < /course id > < title > intro to computer science < /title > < credits > 4 < /credits > < /course > < course > < course id > cs-347 < /course id > < title > database system concepts < /title > < credits > 3 < /credits > < /course > < /department > < department > < dept name > biology < /dept name > < building > watson < /building > < budget > 90000 < /budget > < course > < course id > bio-301 < /course id > < title > genetics < /title > < credits > 4 < /credits > < /course > < /department > < instructor > < iid > 10101 < /iid > < name > srinivasan < /name > < dept name > comp sci < /dept name > < salary > 65000 < /salary > < course id > cs-101 < /coursr id > < /instructor > < /university-1 > figure 23.5 nested xml representation of university information ing instructor element details of instructors brandt and crick are omitted from figure 23.5 for lack of space  but are similar in structure to that for srinivasan although nested representations are natural in xml  they may lead to redundant storage of data for example  suppose details of courses taught by an instructor are stored nestedwithin the instructor element as shown in figure 23.6 if a course is taught by more than one instructor  course information such as title  department  and credits would be stored redundantly with every instructor associated with the course thesumit67.blogspot.com 988 chapter 23 xml < university-2 > < instructor > < id > 10101 < /id > < name > srinivasan < /name > < dept name > comp sci < /dept name > < salary > 65000 < /salary > < teaches > < course > < course id > cs-101 < /course id > < title > intro to computer science < /title > < dept name > comp sci < /dept name > < credits > 4 < /credits > < /course > < /teaches > < /instructor > < instructor > < id > 83821 < /id > < name > brandt < /name > < dept name > comp sci < /dept name > < salary > 92000 < /salary > < teaches > < course > < course id > cs-101 < /course id > < title > intro to computer science < /title > < dept name > comp sci < /dept name > < credits > 4 < /credits > < /course > < /teaches > < /instructor > < /university-2 > figure 23.6 redundancy in nested xml representation nested representations are widely used in xml data interchange applications to avoid joins for instance  a purchase orderwould store the full address of sender and receiver redundantly on multiple purchase orders  whereas a normalized representationmay require a join of purchase order recordswith a company address relation to get address information in addition to elements  xml specifies the notion of an attribute for instance  the course identifier of a course can be represented as an attribute  as shown in figure 23.7 the attributes of an element appear as name = value pairs before the closing ? > ? of a tag attributes are strings and do not contain markup furthermore  attributes can appear only once in a given tag  unlike subelements  which may be repeated thesumit67.blogspot.com 23.2 structure of xml data 989    < course course id = ? cs-101 ? > < title > intro to computer science < /title > < dept name > comp sci < /dept name > < credits > 4 < /credits > < /course >    figure 23.7 use of attributes note that in a document construction context  the distinction between subelement and attribute is important ? an attribute is implicitly text that does not appear in the printed or displayed document however  in database and data exchange applications of xml  this distinction is less relevant  and the choice of representing data as an attribute or a subelement is frequently arbitrary in general  it is advisable to use attributes only to represent identifiers  and to store all other data as subelements one final syntactic note is that an element of the form < element > < /element > that contains no subelements or text can be abbreviated as < element/ > ; abbreviated elements may  however  contain attributes since xml documents are designed to be exchanged between applications  a namespace mechanism has been introduced to allow organizations to specify globally unique names to be used as element tags in documents the idea of a namespace is to prepend each tag or attribute with a universal resource identifier  for example  a web address   thus  for example  if yale university wanted to ensure that xml documents it created would not duplicate tags used by any business partner ? s xml documents  it could prepend a unique identifier with a colon to each tag name the university may use aweb url such as  http  //www.yale.edu as a unique identifier using long unique identifiers in every tag would be rather inconvenient  so the namespace standard provides awayto define an abbreviation for identifiers in figure 23.8  the root element  university  has an attribute xmlns  yale  which declares that yale is defined as an abbreviation for the url given above the abbreviation can then be used in various element tags  as illustrated in the figure a document can have more than one namespace  declared as part of the root element different elements can then be associated with different namespaces a default namespace can be defined by using the attribute xmlns instead of xmlns  yale in the root element elements without an explicit namespace prefix would then belong to the default namespace sometimes we need to store values containing tags without having the tags interpreted as xml tags so that we can do so  xml allows this construct  < !  cdata  < course > ? ? ? < /course >   > thesumit67.blogspot.com 990 chapter 23 xml < university xmlns  yale = ? http  //www.yale.edu ? >    < yale  course > < yale  course id > cs-101 < /yale  course id > < yale  title > intro to computer science < /yale  title > < yale  dept name > comp sci < /yale  dept name > < yale  credits > 4 < /yale  credits > < /yale  course >    < /university > figure 23.8 unique tag names can be assigned by using namespaces because it is enclosed within cdata  the text < course > is treated as normal text data  not as a tag the term cdata stands for character data 23.3 xml document schema databases have schemas  which are used to constrain what information can be stored in the database and to constrain the data types of the stored information in contrast  by default  xml documents can be created without any associated schema  an element may then have any subelement or attribute while such freedom may occasionally be acceptable given the self-describing nature of the data format  it is not generally useful when xml documents must be processed automatically as part of an application  or even when large amounts of related data are to be formatted in xml here,we describe the first schema-definition language included as part of the xml standard  the document type definition  as well as its more recently defined replacement  xml schema another xml schema-definition language called relax ng is also in use  but we do not cover it here ; for more information on relax ng see the references in the bibliographical notes section 23.3.1 document type definition the document type definition  dtd  is an optional part of anxml document the main purpose of a dtd is much like that of a schema  to constrain and type the information present in the document however  the dtd does not in fact constrain types in the sense of basic types like integer or string instead  it constrains only the appearance of subelements and attributes within an element the dtd is primarily a list of rules for what pattern of subelements may appear within an element figure 23.9 shows a part of an example dtd for a university information document ; the xml document in figure 23.1 conforms to this dtd each declaration is in the form of a regular expression for the subelements of an element thus  in the dtd in figure 23.9  a university element consists of one or more course  department  or instructor elements ; the | operator specifies ? or ? thesumit67.blogspot.com 23.3 xml document schema 991 < ! doctype university  < ! element university   department | course | instructor | teaches  +  > < ! element department  dept name  building  budget  > < ! element course  course id  title  dept name  credits  > < ! element instructor  iid  name  dept name  salary  > < ! element teaches  iid  course id  > < ! element dept name  # pcdata  > < ! element building  # pcdata  > < ! element budget  # pcdata  > < ! element course id  # pcdata  > < ! element title  # pcdata  > < ! element credits  # pcdata  > < ! element iid  # pcdata  > < ! element name  # pcdata  > < ! element salary  # pcdata  >  > figure 23.9 example of a dtd while the + operator specifies ? one or more ? although not shown here  the * operator is used to specify ? zero or more  ? while the ? operator is used to specify an optional element  that is  ? zero or one ?   the course element contains subelements course id  title  dept name  and credits  in that order   similarly  department and instructor have the attributes of their relational schema defined as subelements in the dtd finally  the elements course id  title  dept name  credits  building  budget  iid  name  and salary are all declared to be of type # pcdata the keyword # pcdata indicates text data ; it derives its name  historically  from ? parsed character data ? two other special type declarations are empty  which says that the element has no contents  and any  which says that there is no constraint on the subelements of the element ; that is  any elements  even those not mentioned in the dtd  can occur as subelements of the element the absence of a declaration for an element is equivalent to explicitly declaring the type as any the allowable attributes for each element are also declared in the dtd unlike subelements  no order is imposed on attributes attributes may be specified to be of type cdata  id  idref  or idrefs ; the type cdata simply says that the attribute contains character data  while the other three are not so simple ; they are explained in more detail shortly for instance  the following line from a dtd specifies that element course has an attribute of type course id  and a value must be present for this attribute  < ! attlist course course id cdata # required > attributes must have a type declaration and a default declaration the default declaration can consist of a default value for the attribute or # required  meaning thesumit67.blogspot.com 992 chapter 23 xml < ! doctype university-3  < ! element university   department | course | instructor  +  > < ! element department  building  budget  > < ! attlist department dept name id # required > < ! element course  title  credits  > < ! attlist course course id id # required dept name idref # required instructors idrefs # implied > < ! element instructor  name  salary  > < ! attlist instructor iid id # required > dept name idref # required > ? ? ? declarations for title  credits  building  budget  name and salary ? ? ?  > figure 23.10 dtd with id and idrefs attribute types that a value must be specified for the attribute in each element  or # implied  meaning that no default value has been provided  and the document may omit this attribute if an attribute has a default value  for every element that does not specify a value for the attribute  the default value is filled in automatically when the xml document is read an attribute of type id provides a unique identifier for the element ; a value that occurs in an id attribute of an element must not occur in any other element in the same document at most one attribute of an element is permitted to be of type id  we renamed the attribute id of the instructor relation to iid in the xml representation  in order to avoid confusion with the type id  an attribute of type idref is a reference to an element ; the attribute must contain a value that appears in the id attribute of some element in the document the type idrefs allows a list of references  separated by spaces figure 23.10 shows an example dtdin which identifiers of course  department and instructor are represented by id attributes  andrelationships between themare represented by idref and idrefs attributes the course elements use course id as their identifier attribute ; to do so  course id has been made an attribute of course instead of a subelement additionally  each course element also contains an idref of the department corresponding to the course  and an idrefs attribute instructors identifying the instructors who teach the course the department elements have an identifier attribute called dept name the instructor elements have an identifier attribute called iid  and an idref attribute dept name identifying the department to which the instructor belongs figure 23.11 shows an example xml document based on the dtd in figure 23.10 thesumit67.blogspot.com 23.3 xml document schema 993 < university-3 > < department dept name = ? comp sci ? > < building > taylor < /building > < budget > 100000 < /budget > < /department > < department dept name = ? biology ? > < building > watson < /building > < budget > 90000 < /budget > < /department > < course course id = ? cs-101 ? dept name = ? comp sci ? instructors = ? 10101 83821 ? > < title > intro to computer science < /title > < credits > 4 < /credits > < /course > < course course id = ? bio-301 ? dept name = ? biology ? instructors = ? 76766 ? > < title > genetics < /title > < credits > 4 < /credits > < /course > < instructor iid = ? 10101 ? dept name = ? comp sci ? > < name > srinivasan < /name > < salary > 65000 < /salary > < /instructor > < instructor iid = ? 83821 ? dept name = ? comp sci ? > < name > brandt < /name > < salary > 72000 < /salary > < /instructor > < instructor iid = ? 76766 ? dept name = ? biology ? > < name > crick < /name > < salary > 72000 < /salary > < /instructor > < /university-3 > figure 23.11 xml data with id and idref attributes the id and idref attributes serve the same role as reference mechanisms in object-oriented and object-relational databases  permitting the construction of complex data relationships document type definitions are strongly connected to the document formatting heritage of xml because of this  they are unsuitable in many ways for serving as the type structure of xml for data-processing applications nevertheless  a number of data exchange formats have been defined in terms of dtds  since they were part of the original standard here are some of the limitations of dtds as a schema mechanism  thesumit67.blogspot.com 994 chapter 23 xml ? individual text elements and attributes can not be typed further for instance  the element balance can not be constrained to be a positive number the lack of such constraints is problematic for data processing and exchange applications  which must then contain code to verify the types of elements and attributes ? it is difficult to use the dtd mechanism to specify unordered sets of subelements order is seldom important for data exchange  unlike document layout  where it is crucial   while the combination of alternation  the | operation  and the * or the + operation as in figure 23.9 permits the specification of unordered collections of tags  it is much more difficult to specify that each tag may only appear once ? there is a lack of typing in ids and idrefss thus  there is no way to specify the type of element to which an idref or idrefs attribute should refer as a result  the dtd in figure 23.10 does not prevent the ? dept name ? attribute of a course element from referring to other courses  even though this makes no sense 23.3.2 xml schema an effort to redress the deficiencies of the dtd mechanism resulted in the development of a more sophisticated schema language  xml schema we provide a brief overview of xml schema  and then we list some areas in which it improves dtds xmlschema defines a number of built-in types such as string  integer  decimal date  and boolean in addition  it allows user-defined types ; these may be simple types with added restrictions  or complex types constructed using constructors such as complextype and sequence figures 23.12 and 23.13 show how the dtd in figure 23.9 can be represented by xmlschema ; we describe below xmlschema features illustrated by the figures the first thing to note is that schemadefinitions in xmlschema are themselves specified in xml syntax  using a variety of tags defined by xml schema to avoid conflicts with user-defined tags,weprefix the xmlschema tag with the namespace prefix ? xs  ? ; this prefix is associated with the xml schema namespace by the xmlns  xs specification in the root element  < xs  schema xmlns  xs = ? http  //www.w3.org/2001/xmlschema ? > note that any namespace prefix could be used in place of xs ; thuswe couldreplace all occurrences of ? xs  ? in the schema definition with ? xsd  ? without changing the meaning of the schema definition all types defined by xml schema must be prefixed by this namespace prefix the first element is the root element university  whose type is specified to be universitytype  which is declared later the example then defines the types of elements department  course  instructor  and teaches note that each of these thesumit67.blogspot.com 23.3 xml document schema 995 < xs  schema xmlns  xs = ? http  //www.w3.org/2001/xmlschema ? > < xs  element name = ? university ? type = ? universitytype ? / > < xs  element name = ? department ? > < xs  complextype > < xs  sequence > < xs  element name = ? dept name ? type = ? xs  string ? / > < xs  element name = ? building ? type = ? xs  string ? / > < xs  element name = ? budget ? type = ? xs  decimal ? / > < /xs  sequence > < /xs  complextype > < /xs  element > < xs  element name = ? course ? > < xs  element name = ? course id ? type = ? xs  string ? / > < xs  element name = ? title ? type = ? xs  string ? / > < xs  element name = ? dept name ? type = ? xs  string ? / > < xs  element name = ? credits ? type = ? xs  decimal ? / > < /xs  element > < xs  element name = ? instructor ? > < xs  complextype > < xs  sequence > < xs  element name = ? iid ? type = ? xs  string ? / > < xs  element name = ? name ? type = ? xs  string ? / > < xs  element name = ? dept name ? type = ? xs  string ? / > < xs  element name = ? salary ? type = ? xs  decimal ? / > < /xs  sequence > < /xs  complextype > < /xs  element > continued in figure 23.13 figure 23.12 xml schema version of dtd from figure 23.9 is specified by an element with tag xs  element  whose body contains the type definition the type of department is defined to be a complex type  which is further specified to consist of a sequence of elements dept name  building  and budget any type that has either attributes or nested subelements must be specified to be a complex type alternatively  the type of an element can be specified to be a predefined type by the attribute type ; observe howthe xmlschema types xs  string and xs  decimal are used to constrain the types of data elements such as dept name and credits finally the example defines the type universitytype as containing zero or more occurrences of each of department  course  instructor  and teaches note the use of ref to specify the occurrence of an element defined earlier xml schema can define theminimum andmaximum number of occurrences of subelements by thesumit67.blogspot.com 996 chapter 23 xml < xs  element name = ? teaches ? > < xs  complextype > < xs  sequence > < xs  element name = ? iid ? type = ? xs  string ? / > < xs  element name = ? course id ? type = ? xs  string ? / > < /xs  sequence > < /xs  complextype > < /xs  element > < xs  complextype name = ? universitytype ? > < xs  sequence > < xs  element ref = ? department ? minoccurs = ? 0 ? maxoccurs = ? unbounded ? / > < xs  element ref = ? course ? minoccurs = ? 0 ? maxoccurs = ? unbounded ? / > < xs  element ref = ? instructor ? minoccurs = ? 0 ? maxoccurs = ? unbounded ? / > < xs  element ref = ? teaches ? minoccurs = ? 0 ? maxoccurs = ? unbounded ? / > < /xs  sequence > < /xs  complextype > < /xs  schema > figure 23.13 continuation of figure 23.12 using minoccurs and maxoccurs the default for both minimum and maximum occurrences is 1  so these have to be specified explicitly to allow zero or more department  course  instructor  and teaches elements attributes are specified using the xs  attribute tag for example,we could have defined dept name as an attribute by adding  < xs  attribute name = ? dept name ? / > within the declaration of the department element adding the attribute use = ? required ? to the above attribute specification declares that the attribute must be specified  whereas the default value of use is optional attribute specifications would appear directly under the enclosing complextype specification  even if elements are nested within a sequence specification we can use the xs  complextype element to create named complex types ; the syntax is the same as that used for the xs  complextype element in figure 23.12  except thatweadd an attribute name = typename to the xs  complextype element  where typename is the name we wish to give to the type.we can then use the named type to specify the type of an element using the type attribute  just as we used xs  decimal and xs  string in our example in addition to defining types  a relational schema also allows the specification of constraints xml schema allows the specification of keys and key references  thesumit67.blogspot.com 23.3 xml document schema 997 corresponding to the primary-key and foreign-key definition in sql in sql  a primary-key constraint or unique constraint ensures that the attribute values do not recur within the relation in the context of xml  we need to specify a scope within which values are unique and form a key the selector is a path expression that defines the scope for the constraint  and field declarations specify the elements or attributes that form the key.1 to specify that dept name forms a key for department elements under the root university element  we add the following constraint specification to the schema definition  < xs  key name = ? deptkey ? > < xs  selector xpath = ? /university/department ? / > < xs  field xpath = ? dept name ? / > < /xs  key > correspondingly a foreign-key constraint from course to department may be defined as follows  < xs  name = ? coursedeptfkey ? refer = ? deptkey ? > < xs  selector xpath = ? /university/course ? / > < xs  field xpath = ? dept name ? / > < /xs  keyref > note that the refer attribute specifies the name of the key declaration that is being referenced  while the field specification identifies the referring attributes xml schema offers several benefits over dtds  and is widely used today among the benefits that we have seen in the examples above are these  ? it allows the text that appears in elements to be constrained to specific types  such as numeric types in specific formats or complex types such as sequences of elements of other types ? it allows user-defined types to be created ? it allows uniqueness and foreign-key constraints ? it is integrated with namespaces to allow different parts of a document to conform to different schemas in addition to the features we have seen  xml schema supports several other features that dtds do not  such as these  ? it allows types to be restricted to create specialized types  for instance by specifying minimum and maximum values ? it allows complex types to be extended by using a form of inheritance 1we use simple path expressions here that are in a familiar syntax xml has a rich syntax for path expressions  called xpath  which we explore in section 23.4.2 thesumit67.blogspot.com 998 chapter 23 xml our description of xml schema is just an overview ; to learn more about xml schema  see the references in the bibliographical notes 23.4 querying and transformation given the increasing number of applications that use xml to exchange  mediate  and store data  tools for effective management of xml data are becoming increasingly important in particular  tools for querying and transformation of xml data are essential to extract information from large bodies of xml data  and to convert data between different representations  schemas  in xml just as the output of a relational query is a relation  the output of an xml query can be an xml document as a result  querying and transformation can be combined into a single tool in this section  we describe the xpath and xquery languages  ? xpath is a language for path expressions and is actually a building block for xquery ? xquery is the standard language for querying xml data it is modeled after sql but is significantly different  since it has to deal with nested xml data xquery also incorporates xpath expressions the xslt language is another language designed for transforming xml however  it is used primarily in document-formatting applications  rather in datamanagement applications  so we do not discuss it in this book the tools section at the end of this chapter provides references to software that can be used to execute queries written in xpath and xquery 23.4.1 tree model of xml a tree model of xml data is used in all these languages an xml document is modeled as a tree  with nodes corresponding to elements and attributes element nodes can have child nodes  which can be subelements or attributes of the element correspondingly  each node  whether attribute or element   other than the root element  has a parent node  which is an element the order of elements and attributes in the xml document is modeled by the ordering of children of nodes of the tree the terms parent  child  ancestor  descendant  and siblings are used in the tree model of xml data the text content of an element can be modeled as a text-node child of the element elements containing text broken up by intervening subelements can have multiple text-node children for instance  an element containing ? this is a < bold > wonderful < /bold > book ? would have a subelement child corresponding to the element bold and two text node children corresponding to ? this is a ? and ? book ? since such structures are not commonly used in data representation  we shall assume that elements do not contain both text and subelements thesumit67.blogspot.com 23.4 querying and transformation 999 23.4.2 xpath xpath addresses parts of an xml document by means of path expressions the language can be viewed as an extension of the simple path expressions in objectoriented and object-relational databases  see section 22.6   the current version of the xpath standard is xpath 2.0  and our description is based on this version a path expression in xpath is a sequence of location steps separated by ? / ?  instead of the ?  ? operator that separates location steps in sql   the result of a path expression is a set of nodes for instance  on the document in figure 23.11  the xpath expression  /university-3/instructor/name returns these elements  < name > srinivasan < /name > < name > brandt < /name > the expression  /university-3/instructor/name/text   returns the same names  but without the enclosing tags path expressions are evaluated from left to right like a directory hierarchy  the initial ? / ? indicates the root of the document note that this is an abstract root ? above ? < university-3 > that is the document tag as a path expression is evaluated  the result of the path at any point consists of an ordered set of nodes from the document initially  the ? current ? set of elements contains only one node  the abstract root.when the next step in a path expression is an element name  such as instructor  the result of the step consists of the nodes corresponding to elements of the specified name that are children of elements in the current element set these nodes then become the current element set for the next step of the path expression evaluation thus  the expression  /university-3 returns a single node corresponding to the  < university-3 > tag  while  /university-3/instructor returns the two nodes corresponding to the  thesumit67.blogspot.com 1000 chapter 23 xml instructor elements that are children of the  university-3 node the result of a path expression is then the set of nodes after the last step of path expression evaluation the nodes returned by each step appear in the same order as their appearance in the document since multiple children can have the same name  the number of nodes in the node set can increase or decrease with each step attribute values may also be accessed  using the ? @ ? symbol for instance  /university-3/course/ @ course id returns a set of all values of course id attributes of course elements by default  idref links are not followed ; we shall see how to deal with idrefs later xpath supports a number of other features  ? selection predicates may follow any step in a path  and are contained in square brackets for example  /university-3/course  credits > = 4  returns course elementswith a credits value greater than or equal to 4  while  /university-3/course  credits > = 4  / @ course id returns the course identifiers of those courses we can test the existence of a subelement by listing it without any comparison operation ; for instance  if we removed just ? > = 4 ? from the above  the expression would return course identifiers of all courses that have a credits subelement  regardless of its value ? xpath provides several functions that can be used as part of predicates  including testing the position of the current node in the sibling order and the aggregate function count    which counts the number of nodes matched by the expression to which it is applied for example  on the xml representation in figure 23.6  the path expression  /university-2/instructor  count  ./teaches/course  > 2  returns instructors who teach more than two courses boolean connectives and and or can be used in predicates  while the function not      can be used for negation ? the function id  ? foo ?  returns the node  if any  with an attribute of type id and value ? foo ?  the function id can even be applied on sets of references  thesumit67.blogspot.com 23.4 querying and transformation 1001 or even strings containing multiple references separated by blanks  such as idrefs for instance  the path  /university-3/course/id  @ dept name  returns all department elements referred to from the dept name attribute of course elements  while  /university-3/course/id  @ instructors  returns the instructor elements referred to in the instuctors attribute of course elements ? the | operator allows expression results to be unioned for example  given data using the schemafromfigure 23.11,wecould find the union of computer science and biology courses using the expression  /university-3/course  @ dept name = ? comp sci ?  | /university-3/course  @ dept name = ? biology ?  however  the | operator can not be nested inside other operators it is also worth noting that the nodes in the union are returned in the order in which they appear in the document ? an xpath expression can skip multiple levels of nodes by using ? // ?  for instance  the expression /university-3//name finds all name elements anywhere under the /university-3 element  regardless of the elements in which they are contained  and regardless of how many levels of enclosing elements are present between the university-3 and name elements this example illustrates the ability to find required data without full knowledge of the schema ? a step in the path need not just select from the children of the nodes in the current node set in fact  this is just one of several directions along which a step in the path may proceed  such as parents  siblings  ancestors  and descendants we omit details  but note that ? // ?  described above  is a short form for specifying ? all descendants  ? while ?  ? specifies the parent ? the built-in function doc  name  returns the root of a named document ; the name could be a file name or a url the root returned by the function can then be used in a path expression to access the contents of the document thus  a path expression can be applied on a specified document  instead of being applied on the current default document for example  if the university data in our university example is contained in a file ? university.xml ?  the following path expression would return all departments at the university  doc  ? university.xml ?  /university/department thesumit67.blogspot.com 1002 chapter 23 xml the function collection  name  is similar to doc  but returns a collection of documents identified by name the function collection can be used  for example  to open an xml database  which can be viewed as a collection of documents ; the following element in the xpath expression would select the appropriate document  s  from the collection in most of our examples,weassume that the expressions are evaluated in the context of a database  which implicitly provides a collection of ? documents ? on which xpath expressions are evaluated in such cases  we do not need to use the functions doc and collection 23.4.3 xquery the world wide web consortium  w3c  has developed xquery as the standard query language for xml our discussion is based on xquery 1.0  which was released as a w3c recommendation on 23 january 2007 23.4.3.1 flwor expressions xquery queries are modeled after sql queries  but differ significantly from sql they are organized into five sections  for  let  where  order by  and return they are referred to as ? flwor ?  pronounced ? flower ?  expressions  with the letters in flwor denoting the five sections a simple flwor expression that returns course identifiers of courses with greater than 3 credits  shown below  is based on the xml document of figure 23.11  which uses id and idrefs  for $ x in /university-3/course let $ courseid  = $ x/ @ course id where $ x/credits > 3 return < course id >  $ courseid  < /course id > the for clause is like the from clause of sql  and specifies variables that range over the results of xpath expressions when more than one variable is specified  the results include the cartesian product of the possible values the variables can take  just as the sql from clause does the let clause simply allows the results of xpath expressions to be assigned to variable names for simplicity of representation the where clause  like the sql where clause  performs additional tests on the joined tuples from the for clause the order by clause  like the sql order by clause  allows sorting of the output finally  the return clause allows the construction of results in xml a flwor query need not contain all the clauses ; for example a query may contain just the for and return clauses  and omit the let  where  and order by clauses the preceding xquery query did not contain an order by clause in fact  since this query is simple  we can easily do away with the let clause  and the variable $ courseid in the return clause could be replaced with $ x/ @ course id note further that  since the for clause uses xpath expressions  selections may thesumit67.blogspot.com 23.4 querying and transformation 1003 occur within the xpath expression thus  an equivalent query may have only for and return clauses  for $ x in /university-3/course  credits > 3  return < course id >  $ x/ @ course id  < /course id > however  the let clause helps simplify complex queries note also that variables assigned by let clauses may contain sequences with multiple elements or values  if the path expression on the right-hand side returns a sequence of multiple elements or values observe the use of curly brackets  ?   ?  in the return clause when xquery finds an element such as < course id > starting an expression  it treats its contents as regular xml text  except for portions enclosed within curly brackets  which are evaluated as expressions thus  if we omitted the curly brackets in the above return clause  the resultwould contain several copies of the string ? $ x/ @ course id ? each enclosed in a course id tag the contents within the curly brackets are  however  treated as expressions to be evaluated note that this convention applies even if the curly brackets appear within quotes thus  we could modify the above query to return an element with tag course  with the course identifier as an attribute  by replacing the return clause with the following  return < course course id = ?  $ x/ @ course id  ? / > xquery provides anotherway of constructing elements using the element and attribute constructors for example  if the return clause in the previous query is replaced by the following return clause  the query would return course elements with course id and dept name as attributes and title and credits as subelements return element course  attribute course id  $ x/ @ course id   attribute dept name  $ x/dept name   element title  $ x/title   element credits  $ x/credits   note that  as before  the curly brackets are required to treat a string as an expression to be evaluated 23.4.3.2 joins joins are specified inxquerymuchas they are insql the joinof course  instructor  and teaches elements in figure 23.1 can be written in xquery this way  thesumit67.blogspot.com 1004 chapter 23 xml for $ c in /university/course  $ i in /university/instructor  $ t in /university/teaches where $ c/course id = $ t/course id and $ t/iid = $ i/iid return < course instructor >  $ c $ i  < /course instructor > the same query can be expressed with the selections specified as xpath selections  for $ c in /university/course  $ i in /university/instructor  $ t in /university/teaches  $ c/course id = $ t/course id and $ t/iid = $ i/iid  return < course instructor >  $ c $ i  < /course instructor > path expressions in xquery are the same as path expressions in xpath2.0 path expressions may return a single value or element  or a sequence of values or elements in the absence of schema information  it may not be possible to infer whether a path expression returns a single value or a sequence of values such path expressions may participate in comparison operations such as =  <  and > =  xquery has an interesting definition of comparison operations on sequences for example  the expression $ x/credits > 3 would have the usual interpretation if the result of $ x/credits is a single value  but if the result is a sequence containing multiple values  the expression evaluates to true if at least one of the values is greater than 3 similarly  the expression $ x/credits = $ y/credits evaluates to true if any one of the values returned by the first expression is equal to any one of the values returned by the second expression if this behavior is not appropriate  the operators eq  ne  lt  gt  le  ge can be used instead these raise an error if either of their inputs is a sequence with multiple values 23.4.3.3 nested queries xquery flwor expressions can be nested in the return clause  in order to generate element nestings that do not appear in the source document for instance  the xml structure shown in figure 23.5  with course elements nested within department elements  can be generated from the structure in figure 23.1 by the query shown in figure 23.14 the query also introduces the syntax $ d/ *  which refers to all the children of the node  or sequence of nodes  bound to the variable $ d similarly  $ d/text   gives the text content of an element  without the tags xquery provides a variety of aggregate functions such as sum   and count   that can be applied on sequences of elements or values the function distinctvalues   applied on a sequence returns a sequence without duplication the sequence  collection  of values returned by a path expression may have some values repeated because they are repeated in the document  although an xpath expresthesumit67 blogspot.com 23.4 querying and transformation 1005 < university-1 >  for $ d in /university/department return < department >  $ d/ *   for $ c in /university/course  dept name = $ d/dept name  return $ c  < /department >   for $ i in /university/instructor return < instructor >  $ i/ *   for $ c in /university/teaches  iid = $ i/iid  return $ c/course id  < /instructor >  < /university-1 > figure 23.14 creating nested structures in xquery sion result can contain at most one occurrence of each node in the document xquery supports many other functions ; see the references in the bibliographical notes for more information these functions are actually common to xpath 2.0 and xquery  and can be used in any xpath path expression to avoid namespace conflicts  functions are associated with a namespace  http  //www.w3.org/2005/xpath-functions which has a default namespace prefix of fn thus  these functions can be referred to unambiguously as fn  sum or fn  count while xquery does not provide a group by construct  aggregate queries can be written by using the aggregate functions on path or flwor expressions nested within the return clause for example  the following query on the university xml schema finds the total salary of all instructors in each department  for $ d in /university/department return < department-total-salary > < dept name >  $ d/dept name  < /dept name > < total salary >  fn  sum  for $ i in /university/instructor  dept name = $ d/dept name  return $ i/salary   < /total salary > < /department-total-salary > thesumit67.blogspot.com 1006 chapter 23 xml 23.4.3.4 sorting of results results can be sorted in xquery by using the order by clause for instance  this query outputs all instructor elements sorted by the name subelement  for $ i in /university/instructor order by $ i/name return < instructor >  $ i/ *  < /instructor > to sort in descending order  we can use order by $ i/name descending sorting can be done at multiple levels of nesting for instance  we can get a nested representation of university information with departments sorted in department name order  with courses sorted by course identifiers  as follows  < university-1 >  for $ d in /university/department order by $ d/dept name return < department >  $ d/ *   for $ c in /university/course  dept name = $ d/dept name  order by $ c/course id return < course >  $ c/ *  < /course >  < /department >  < /university-1 > 23.4.3.5 functions and types xquery provides a variety of built-in functions  such as numeric functions and string matching and manipulation functions in addition  xquery supports userdefined functions the following user-defined function takes as input an instructor identifier  and returns a list of all courses offered by the department to which the instructor belongs  declare function local  dept courses  $ iid as xs  string  as element  course  *  for $ i in /university/instructor  iid = $ iid   $ c in /university/courses  dept name = $ i/dept name  return $ c  the namespace prefix xs  used in the above example is predefined by xquery to be associated with the xml schema namespace  while the namespace local  is predefined to be associated with xquery local functions the type specifications for functionarguments and return values are optional  and may be omitted xquery uses the type system of xml schema the type element allows elements with any tag  while element  course  allows elements thesumit67.blogspot.com 23.4 querying and transformation 1007 with the tag course types can be suffixed with a * to indicate a sequence of values of that type ; for example  the definition of function dept courses specifies the return value as a sequence of course elements the following query  which illustrates function invocation  prints out the department courses for the instructor  s  named srinivasan  for $ i in /university/instructor  name = ? srinivasan ?   returnlocal  inst dept courses  $ i/iid  xquery performs type conversion automatically whenever required for example  if a numeric value represented by a string is compared to a numeric type  type conversion from string to the numeric type is done automatically when an element is passed to a function that expects a string value  type conversion to a string is done by concatenating all the text values contained  nested  within the element thus  the function contains  a,b   which checks if string a contains string b  can be used with its first argument set to an element  in which case it checks if the element a contains the string b nested anywhere inside it xquery also provides functions to convert between types for instance  number  x  converts a string to a number 23.4.3.6 other features xquery offers a variety of other features  such as if-then-else constructs that can be used within return clauses  and existential and universal quantification that can be used in predicates in where clauses for example  existential quantification can be expressed in the where clause by using  some $ e in path satisfies p where path is a path expression and p is a predicate that can use $ e universal quantification can be expressed by using every in place of some for example  to find departments where every instructor has a salary greater than $ 50,000  we can use the following query  for $ d in /university/department where every $ i in /university/instructor  dept name = $ d/dept name  satisfies $ i/salary > 50000 return $ d note  however  that if a department has no instructor  it will trivially satisfy the above condition an extra clause  and fn  exists  /university/instructor  dept name = $ d/dept name   thesumit67.blogspot.com 1008 chapter 23 xml can be used to ensure that there is at least one instructor in the department the built-in function exists   used in the clause returns true if its input argument is nonempty the xqj standard provides an api to submit xquery queries to an xml database system and to retrieve the xml results its functionality is similar to the jdbc api 23.5 application program interfaces to xml with the wide acceptance of xml as a data representation and exchange format  software tools are widely available for manipulation of xml data there are two standard models for programmatic manipulation of xml  each available for use with a number of popular programming languages both these apis canbeusedto parse an xml document and create an in-memory representation of the document they are used for applications that deal with individual xml documents note  however  that they are not suitable for querying large collections of xml data ; declarative querying mechanisms such as xpath and xquery are better suited to this task one of the standard apis for manipulating xml is based on the document object model  dom   which treats xml content as a tree  with each element represented by a node  called a domnode programs may access parts of the document in a navigational fashion  beginning with the root dom libraries are available for most common programming languages and are even present in web browsers  where they may be used to manipulate the document displayed to the user we outline here some of the interfaces and methods in the java api for dom  to give a flavor of dom ? the java dom api provides an interface called node  and interfaces element and attribute  which inherit from the node interface ? the node interface providesmethods such as getparentnode    getfirstchild    and getnextsibling    to navigate the dom tree  starting with the root node ? subelements of an element can be accessed by name  using getelementsby tagname  name  ,which returns a list of all child elements with a specified tag name ; individual members of the list can be accessed by the method item  i   which returns the ith element in the list ? attribute values of an element can be accessed by name  using the method getattribute  name   ? the text value of an element is modeled as a text node  which is a child of the element node ; an element node with no subelements has only one such child node the method getdata   on the text node returns the text contents dom also provides a variety of functions for updating the document by adding and deleting attribute and element children of a node  setting node values  and so on thesumit67.blogspot.com 23.6 storage of xml data 1009 many more details are required for writing an actual dom program ; see the bibliographical notes for references to further information domcan be used to access xml data stored in databases  and an xml database can be built with dom as its primary interface for accessing and modifying data however  the dom interface does not support any form of declarative querying the second commonly used programming interface  the simple api for xml  sax  is an event model  designed to provide a common interface between parsers and applications this api is built on the notion of event handlers  which consist of user-specified functions associated with parsing events parsing events correspond to the recognition of parts of a document ; for example  an event is generated when the start-tag is found for an element  and another event is generated when the end-tag is found the pieces of a document are always encountered in order from start to finish the sax application developer creates handler functions for each event  and registers them when a document is read in by the sax parser  as each event occurs  the handler function is called with parameters describing the event  such as element tag or text contents   the handler functions then carry out their task for example  to construct a tree representing the xml data  the handler functions for an attribute or element start event could add a node  or nodes  to a partially constructed tree the start and end-tag event handlers would also have to keep track of the current node in the tree to which new nodes must be attached ; the element start event would set the new element as the node that is the point where further child nodes must be attached the corresponding element end event would set the parent of the node as the current node where further child nodes must be attached sax generally requires more programming effort than dom  but it helps avoid the overhead of creating a dom tree in situations where the application needs to create its own data representation if dom were used for such applications  there would be unnecessary space and time overhead for constructing the dom tree 23.6 storage of xml data many applications require storage of xml data one way to store xml data is to store it as documents in a file system  while a second is to build a special-purpose database to store xml data another approach is to convert the xml data to a relational representation and store it in a relational database several alternatives for storing xml data are briefly outlined in this section 23.6.1 nonrelational data stores there are several alternatives for storing xml data in nonrelational data-storage systems  ? store in flat files since xml is primarily a file format  a natural storage mechanism is simply a flat file this approach has many of the drawbacks  outlined thesumit67.blogspot.com 1010 chapter 23 xml in chapter 1  of using file systems as the basis for database applications in particular  it lacks data isolation  atomicity  concurrent access  and security however  the wide availability of xml tools that work on file data makes it relatively easy to access and query xml data stored in files thus  this storage format may be sufficient for some applications ? create an xml database xml databases are databases that use xml as their basic data model early xml databases implemented the document object model on a c + + -based object-oriented database this allows much of the object-oriented database infrastructure to be reused  while providing a standard xml interface the addition of xquery or other xml query languages provides declarative querying other implementations have built the entire xml storage and querying infrastructure on top of a storage manager that provides transactional support although several databases designed specifically to store xml data have been built  building a full-featured database system from ground up is a very complex task such a database must support not only xml data storage and querying but also other database features such as transactions  security  support for data access from clients  and a variety of administration facilities it makes sense to instead use an existing database system to provide these facilities and implement xml data storage and querying either on top of the relational abstraction  or as a layer parallel to the relational abstraction.we study these approaches in section 23.6.2 23.6.2 relational databases since relational databases arewidely used in existing applications  there is a great benefit to be had in storing xml data in relational databases  so that the data can be accessed from existing applications converting xml data to relational form is usually straightforward if the data were generated froma relational schema in the first place and xml is used merely as a data exchange format for relational data however  there are many applications where the xml data are not generated from a relational schema  and translating the data to relational form for storage may not be straightforward in particular  nested elements and elements that recur  corresponding to set-valued attributes  complicate storage of xml data in relational format several alternative approaches are available  which we describe below 23.6.2.1 store as string small xml documents can be stored as string  clob  values in tuples in a relational database large xml documentswith the top-level element havingmany children can be handled by storing each child element as a string in a separate tuple for instance  the xml data in figure 23.1 could be stored as a set of tuples in a relation elements  data   with the attribute data of each tuple storing one xml element  department  course  instructor  or teaches  in string form thesumit67.blogspot.com 23.6 storage of xml data 1011 while the above representation is easy to use  the database system does not know the schema of the stored elements as a result  it is not possible to query the data directly in fact  it is not even possible to implement simple selections such as finding all department elements  or finding the department element with department name ? comp sci ?  without scanning all tuples of the relation and examining the string contents a partial solution to this problem is to store different types of elements in different relations  and also store the values of some critical elements as attributes of the relation to enable indexing for instance  in our example  the relationswould be department elements  course elements  instructor elements  and teaches elements  each with an attribute data each relation may have extra attributes to store the values of some subelements  such as dept name  course id  or name thus  a query that requires department elements with a specified department name can be answered efficiently with this representation such an approach depends on type information about xml data  such as the dtd of the data some database systems  such as oracle  support function indices  which can help avoid replication of attributes between the xml string and relation attributes unlike normal indices  which are on attribute values  function indices can be built on the result of applying user-defined functions on tuples for instance  a function index can be built on a user-defined function that returns the value of the dept name subelement of the xml string in a tuple the index can then be used in the same way as an index on a dept name attribute the above approaches have the drawback that a large part of the xml information is stored within strings it is possible to store all the information in relations in one of several ways that we examine next 23.6.2.2 tree representation arbitrary xml data can be modeled as a tree and stored using a relation  nodes  id  parent id  type  label  value  each element and attribute in the xml data is given a unique identifier a tuple inserted in the nodes relation for each element and attribute with its identifier  id   the identifier of its parent node  parent id   the type of the node  attribute or element   the name of the element or attribute  label   and the text value of the element or attribute  value   if order information of elements and attributes must be preserved  an extra attribute position can be added to the nodes relation to indicate the relative position of the child among the children of the parent as an exercise  you can represent the xml data of figure 23.1 by using this technique this representation has the advantage that all xml information can be represented directly in relational form  and many xml queries can be translated into relational queries and executed inside the database system however  it has the drawback that each element gets broken up intomany pieces  and a large number of joins are required to reassemble subelements into an element thesumit67.blogspot.com 1012 chapter 23 xml 23.6.2.3 map to relations in this approach  xml elements whose schema is known are mapped to relations and attributes elements whose schema is unknown are stored as strings or as a tree a relation is created for each element type  including subelements  whose schema is known and whose type is a complex type  that is  contains attributes or subelements   the root element of the document can be ignored in this step if it does not have any attributes the attributes of the relation are defined as follows  ? all attributes of these elements are stored as string-valued attributes of the relation ? if a subelement of the element is a simple type  that is  can not have attributes or subelements   an attribute is added to the relation to represent the subelement the type of the relation attribute defaults to a string value  but if the subelement had an xml schema type  a corresponding sql type may be used for example  when applied to the element department in the schema  dtd or xml schema  of the data in figure 23.1  the subelements dept name  building and budget of the element department all becomeattributes of a relation department applying this procedure to the remaining elements  we get back the original relational schema that we have used in earlier chapters ? otherwise  a relation is created corresponding to the subelement  using the same rules recursively on its subelements   further  ? an identifier attribute is added to the relations representing the element  the identifier attribute is added only once even if an element has several subelements  ? an attribute parent id is added to the relation representing the subelement  storing the identifier of its parent element ? if ordering is to be preserved  an attribute position is added to the relation representing the subelement for example  if we apply the above procedure to the schema corresponding to the data in figure 23.5  we get the following relations  department  id  dept name  building  budget  course  parent id  course id  dept name  title  credits  variants of this approach are possible for example  the relations corresponding to subelements that can occur at most once can be ? flattened ? into the parent relation by moving all their attributes into the parent relation the bibliographical notes provide references to different approaches to represent xml data as relations thesumit67.blogspot.com 23.6 storage of xml data 1013 23.6.2.4 publishing and shredding xml data when xml is used to exchange data between business applications  the data most often originates in relational databases data in relational databases must be published  that is  converted to xml form  for export to other applications incoming data must be shredded  that is  converted back from xml to normalized relation form and stored in a relational database while application code can perform the publishing and shredding operations  the operations are so common that the conversions should be done automatically  without writing application code  where possible database vendors have spent a lot of effort to xml-enable their database products an xml-enabled database supports an automatic mechanism for publishing relational data as xml the mapping used for publishing data may be simple or complex a simple relation to xml mapping might create an xml element for every row of a table  and make each column in that row a subelement of the xml element the xml schema in figure 23.1 can be created from a relational representation of university information  using such a mapping such a mapping is straightforward to generate automatically such an xml view of relational data can be treated as a virtual xml document  and xml queries can be executed against the virtual xml document a more complicated mapping would allow nested structures to be created extensions of sql with nested queries in the select clause have been developed to allow easy creation of nested xml output we outline these extensions in section 23.6.3 mappings also have to be defined to shred xml data into a relational representation for xml data created from a relational representation  the mapping required to shred the data is a straightforward inverse of the mapping used to publish the data for the general case  a mapping can be generated as outlined in section 23.6.2.3 23.6.2.5 native storage within a relational database some relational databases support native storage of xml such systems store xml data as strings or inmore efficient binary representations,without converting the data to relational form a new data type xml is introduced to represent xml data  although the clob and blob data types may provide the underlying storage mechanism xml query languages such as xpath and xquery are supported to query xml data a relation with an attribute of type xml can be used to store a collection of xml documents ; each document is stored as a value of type xml in a separate tuple special-purpose indices are created to index the xml data several database systems provide native support for xml data they provide an xml data type and allow xquery queries to be embedded within sql queries anxquery query can be executed on a singlexml document andcan beembedded within an sql query to allow it to execute on each of a collection of documents  with each document stored in a separate tuple for example  see section 30.11 for more details on native xml support in microsoft sql server 2005 thesumit67.blogspot.com 1014 chapter 23 xml < university > < department > < row > < dept name > comp sci < /dept name > < building > taylor < /building > < budget > 100000 < /budget > < /row > < row > < dept name > biology < /dept name > < building > watson < /building > < budget > 90000 < /budget > < /row > < /department > < course > < row > < course id > cs-101 < /course id > < title > intro to computer science < /title > < dept name > comp sci < /dept name > < credits > 4 < /credits > < /row > < row > < course id > bio-301 < /course id > < title > genetics < /title > < dept name > biology < /dept name > < credits > 4 < /credits > < /row > < course > < /university > figure 23.15 sql/xml representation of  part of  university information 23.6.3 sql/xml while xml is used widely for data interchange  structured data is still widely stored in relational databases there is often a need to convert relational data to xml representation the sql/xml standard  developed to meet this need  defines a standard extension of sql  allowing the creation of nested xml output the standard has several parts  including a standard way of mapping sql types to xmlschematypes  and a standardway tomaprelational schemas to xml schemas  as well as sql query language extensions for example  the sql/xml representation of the department relation would have an xml schema with outermost element department  with each tuplemapped to an xml element row  and each relation attribute mapped to an xml element of the same name  with some conventions to resolve incompatibilities with special characters in names   an entire sql schema  with multiple relations  can also be mapped to xml in a similar fashion figure 23.15 shows the sql/xml representathesumit67 blogspot.com 23.6 storage of xml data 1015 tion of  part of  the university data from 23.1  containing the relations department and course sql/xml adds several operators and aggregate operations to sql to allow the construction of xml output directly from the extended sql the xmlelement function can be used to create xml elements  while xmlattributes can be used to create attributes  as illustrated by the following query select xmlelement  name ? course ?  xmlattributes  course id as course id  dept name as dept name   xmlelement  name ? title ?  title   xmlelement  name ? credits ?  credits   from course the above query creates an xml element for each course  with the course identifier and department name represented as attributes  and title and credits as subelements the result would look like the course elements shown in figure 23.11  but without the instructor attribute the xmlattributes operator creates the xml attribute name using the sql attribute name  which can be changed using an as clause as shown the xmlforest operator simplifies the construction of xml structures its syntax and behavior are similar to those of xmlattributes  except that it creates a forest  collection  of subelements  instead of a list of attributes it takes multiple arguments  creating an element for each argument  with the attribute ? s sql name used as the xml element name the xmlconcat operator can be used to concatenate elements created by subexpressions into a forest when the sql value used to construct an attribute is null  the attribute is omitted null values are omitted when the body of an element is constructed sql/xml also provides an aggregate function xmlagg that creates a forest  collection  of xml elements from the collection of values on which it is applied the following query creates an element for each department with a course  containing as subelements all the courses in that department since the query has a clause group by dept name  the aggregate function is applied on all courses in each department  creating a sequence of course id elements select xmlelement  name ? department ?  dept name  xmlagg  xmlforest  course id  order by course id   from course group by dept name sql/xml allows the sequence created by xmlagg to be ordered  as illustrated in the preceding query see the bibliographical notes for references to more information on sql/xml thesumit67.blogspot.com 1016 chapter 23 xml 23.7 xml applications we now outline several applications of xml for storing and communicating  exchanging  data and for accessing web services  information resources   23.7.1 storing data with complex structure many applications need to store data that are structured  but are not easily modeled as relations consider  for example  user preferences that must be stored by an application such as a browser there are usually a large number of fields  such as home page  security settings  language settings  and display settings  that must be recorded some of the fields are multivalued  for example  a list of trusted sites  or maybe ordered lists  for example  a list of bookmarks applications traditionally used some type of textual representation to store such data today  a majority of such applications prefer to store such configuration information in xml format the ad hoc textual representations used earlier require effort to design and effort to create parsers that can read the file and convert the data into a form that a program can use the xml representation avoids both these steps xml-based representations are now widely used for storing documents  spreadsheet data and other data that are part of office application packages the open document format  odf   supported by the open office software suite as well as other office suites  and the office open xml  ooxml  format  supported by the microsoft office suite  are document representation standards based on xml they are the twomostwidely used formats for editable document representation xml is also used to represent data with complex structure that must be exchanged between different parts of an application for example  a database system may represent a query execution plan  a relational-algebra expression with extra information on how to execute operations  by using xml this allows one part of the system to generate the query execution plan and another part to display it  without using a shared data structure for example  the data may be generated at a server system and sent to a client system where the data are displayed 23.7.2 standardized data exchange formats xml-based standards for representation of data have been developed for a variety of specialized applications  ranging from business applications such as banking and shipping to scientific applications such as chemistry and molecular biology some examples  ? the chemical industry needs information about chemicals  such as their molecular structure  and a variety of important properties  such as boiling and melting points  calorific values  and solubility in various solvents chemml is a standard for representing such information ? in shipping  carriers of goods and customs and tax officials need shipment records containing detailed information about the goods being shipped  from thesumit67.blogspot.com 23.7 xml applications 1017 whom and to where they were sent  to whom and to where they are being shipped  the monetary value of the goods  and so on ? an online marketplace in which business can buy and sell goods  a so-called business-to-business  b2b  market  requires information such as product catalogs  including detailed product descriptions and price information  product inventories  quotes for a proposed sale  and purchase orders for example  the rosettanet standards for e-business applications define xml schemas and semantics for representing data as well as standards for message exchange using normalized relational schemas to model such complex data requirements would result in a large number of relations that do not correspond directly to the objects that are being modeled the relations would often have large numbers of attributes ; explicit representation of attribute/element names along with values in xml helps avoid confusion between attributes nested element representations help reduce the number of relations that must be represented  as well as the number of joins required to get required information  at the possible cost of redundancy for instance  in our university example  listing departments with course elements nested within department elements  as in figure 23.5  results in a format that is more natural for some applications ? in particular  for humans to read ? than is the normalized representation in figure 23.1 23.7.3 web services applications often require data from outside of the organization  or from another department in the same organization that uses a different database in many such situations  the outside organization or department is not willing to allow direct access to its database using sql  but is willing to provide limited forms of information through predefined interfaces when the information is to be used directly by a human  organizations provide web-based forms  where users can input values and get back desired information in html form however  there are many applications where such information needs to be accessed by software programs  rather than by end users providing the results of a query in xml form is a clear requirement in addition  it makes sense to specify the input values to the query also in xml format in effect  the provider of the information defines procedureswhose input and output are both in xml format the http protocol is used to communicate the input and output information  since it iswidely used and can go through firewalls that institutions use to keep out unwanted traffic from the internet the simple object access protocol  soap  defines a standard for invoking procedures  using xml for representing the procedure input and output soap defines a standard xml schema for representing the name of the procedure  and result status indicators such as failure/error indicators the procedure parameters and results are application-dependent xml data embedded within the soap xml headers typically  http is used as the transport protocol for soap  but a messagebased protocol  such as email over the smtp protocol  may also be used the thesumit67.blogspot.com 1018 chapter 23 xml soap standard is widely used today for example  amazon and google provide soap-based procedures to carry out search and other activities these procedures can be invoked by other applications that provide higher-level services to users the soap standard is independent of the underlying programming language  and it is possible for a site running one language  such as c #  to invoke a service that runs on a different language  such as java a site providing such a collection of soap procedures is called a web service several standards have been defined to support web services the web services description language  wsdl  is a language used to describe a web service ? s capabilities wsdl provides facilities that interface definitions  or function definitions  provide in a traditional programming language  specifyingwhat functions are available and their input and output types in addition wsdl allows specification of the url and network port number to be used to invoke the web service there is also a standard called universal description  discovery  and integration  uddi  that defines how a directory of available web services may be created and how a program may search in the directory to find a web service satisfying its requirements the following example illustrates the value of web services an airline may define aweb service providing a set of procedures that can be invoked by a travel web site ; thesemay include procedures to find flight schedules and pricing information  as well as to make flight bookings the travelweb site may interact with multiple web services  provided by different airlines  hotels  and other companies  to provide travel information to a customer and tomake travel bookings by supporting web services  the individual companies allow a useful service to be constructed on top  integrating the individual services users can interact with a singleweb site to make their travel bookings  without having to contact multiple separateweb sites to invoke a web service  a client must prepare an appropriate soap xml message and send it to the service ; when it gets the result encoded in xml  the client must then extract information from the xml result there are standard apis in languages such as java and c # to create and extract information from soap messages see the bibliographical notes for references to more information on web services 23.7.4 data mediation comparison shopping is an example of a mediation application  in which data about items  inventory  pricing  and shipping costs are extracted froma variety of web sites offering a particular item for sale the resulting aggregated information is significantly more valuable than the individual information offered by a single site a personal financial manager is a similar application in the context of banking consider a consumer with a variety of accounts to manage  such as bank accounts  credit-card accounts  and retirement accounts suppose that these accounts may be held at different institutions providing centralized management thesumit67.blogspot.com 23.8 summary 1019 for all accounts of a customer is a major challenge xml-based mediation addresses the problem by extracting an xml representation of account information from the respective web sites of the financial institutions where the individual holds accounts this information may be extracted easily if the institution exports it in a standard xml format  for example  as aweb service for those that do not  wrapper software is used to generate xml data from html web pages returned by the web site wrapper applications need constant maintenance  since they depend on formatting details of web pages  which change often nevertheless  the value provided by mediation often justifies the effort required to develop and maintain wrappers once the basic tools are available to extract information from each source  a mediator application is used to combine the extracted information under a single schema this may require further transformation of the xml data from each site  since different sites may structure the same information differently they may also use different names for the same information  for instance  acct number and account id   or may even use the same name for different information the mediator must decide on a single schema that represents all required information  andmust provide code to transform data between different representations such issues are discussed in more detail in section 19.8  in the context of distributed databases xml query languages such as xslt and xquery play an important role in the task of transformation between different xml representations 23.8 summary ? like the hyper-text markup language  html  on which the web is based  the extensiblemarkup language  xml  is derived from the standard generalized markup language  sgml   xml was originally intended for providing functional markup for web documents  but has now become the de facto standard data format for data exchange between applications ? xml documents contain elements with matching starting and ending tags indicating the beginning and end of an element elements may have subelements nested within them  to any level of nesting elements may also have attributes the choice between representing information as attributes and subelements is often arbitrary in the context of data representation ? elements may have an attribute of type id that stores a unique identifier for the element elements may also store references to other elements by using attributes of type idref.attributes of type idrefs can store a list of references ? documents optionally may have their schema specified by a document type declaration  dtd   the dtd of a document specifies what elements may occur  how they may be nested  and what attributes each element may have although dtds are widely used  they have several limitations for instance  they do not provide a type system ? xml schema is now the standard mechanism for specifying the schema of an xml document it provides a large set of basic types  as well as constructs for thesumit67.blogspot.com 1020 chapter 23 xml creating complex types and specifying integrity constraints  including key constraints and foreign-key  keyref  constraints ? xml data can be represented as tree structures  with nodes corresponding to elements and attributes nesting of elements is reflected by the parent-child structure of the tree representation ? path expressions can be used to traverse the xml tree structure and locate data xpath is a standard language for path expressions  and allows required elements to be specified by a file-system-like path  and additionally allows selections and other features xpath also forms part of other xml query languages ? the xquery language is the standard language for querying xml data it has a structure not unlike sql  with for  let  where  order by  and return clauses however  it supports many extensions to deal with the tree nature of xml and to allow for the transformation of xml documents into other documents with a significantly different structure xpath path expressions form a part of xquery xquery supports nested queries and user-defined functions ? the dom and sax apis are widely used for programmatic access to xml data these apis are available from a variety of programming languages ? xml data can be stored in any of several different ways xml data may also be stored in file systems  or in xml databases  which use xml as their internal representation ? xml data can be stored as strings in a relational database alternatively  relations can represent xml data as trees as another alternative  xml data can be mapped to relations in the same way that e-r schemas are mapped to relational schemas native storage of xml in relational databases is facilitated by adding an xml data type to sql ? xml is used in a variety of applications  such as storing complex data  exchange of data between organizations in a standardized form  data mediation  and web services web services provide a remote-procedure call interface  with xml as the mechanism for encoding parameters as well as results review terms ? extensible markup language  xml  ? hyper-text markup language  html  ? standard generalized markup language ? markup language ? tags ? self-documenting ? element ? root element ? nested elements ? attribute thesumit67.blogspot.com practice exercises 1021 ? namespace ? default namespace ? schema definition ? document type definition  dtd  ? id ? idref and idrefs ? xml schema ? simple and complex types ? sequence type ? key and keyref ? occurrence constraints ? tree model of xml data ? nodes ? querying and transformation ? path expressions ? xpath ? xquery ? flwor expressions  for  let  where  order by  return ? joins ? nested flwor expression ? sorting ? xml api ? document object model  dom  ? simple api for xml  sax  ? storage of xml data ? in nonrelational data stores ? in relational databases  store as string  tree representation  map to relations  publish and shred  xml-enabled database  native storage  sql/xml ? xml applications ? storing complex data ? exchange of data ? data mediation ? soap ? web services practice exercises 23.1 give an alternative representation of university information containing the samedata as in figure 23.1  butusing attributes instead of subelements also give the dtd or xml schema for this representation 23.2 give the dtd or xml schema for an xml representation of the following nested-relational schema  emp =  ename  childrenset setof  children   skillsset setof  skills   children =  name  birthday  birthday =  day  month  year  skills =  type  examsset setof  exams   exams =  year  city  thesumit67.blogspot.com 1022 chapter 23 xml < ! doctype bibliography  < ! element book  title  author +  year  publisher  place ?  > < ! element article  title  author +  journal  year  number  volume  pages ?  > < ! element author  last name  first name  > < ! element title  # pcdata  > ? ? ? similar pcdata declarations for year  publisher  place  journal  year  number  volume  pages  last name and first name  > figure 23.16 dtd for bibliographical data 23.3 write a query in xpath on the schema of practice exercise 23.2 to list all skill types in emp 23.4 write a query in xquery on the xml representation in figure 23.11 to find the total salary of all instructors in each department 23.5 write a query in xquery on the xml representation in figure 23.1 to compute the left outer join of department elementswith course elements  hint  use universal quantification  23.6 write queries in xquery to output department elements with associated course elements nested within the department elements  given the university information representation using id and idrefs in figure 23.11 23.7 give a relational schema to represent bibliographical information specified according to the dtd fragment in figure 23.16 the relational schema must keep track of the order of author elements you can assume that only books and articles appear as top-level elements in xml documents 23.8 show the tree representation of the xml data in figure 23.1  and the representation of the tree using nodes and child relations described in section 23.6.2 23.9 consider the following recursive dtd  < ! doctype parts  < ! element part  name  subpartinfo *  > < ! element subpartinfo  part  quantity  > < ! element name  # pcdata  > < ! element quantity  # pcdata  >  > a give a small example of data corresponding to this dtd b show how to map this dtd to a relational schema you can assume that part names are unique ; that is  wherever a part appears  its subpart structure will be the same c create a schema in xml schema corresponding to this dtd thesumit67.blogspot.com exercises 1023 exercises 23.10 show  by giving a dtd  how to represent the non-1nf books relation from section 22.2  using xml 23.11 write the following queries in xquery  assuming the schema frompractice exercise 23.2 a find the names of all employeeswhohave a childwhohas a birthday in march b find those employees who took an examination for the skill type ? typing ? in the city ? dayton ?  c list all skill types in emp 23.12 consider the xml data shown in figure 23.3 suppose we wish to find purchase orders that ordered two or more copies of the partwith identifier 123 consider the following attempt to solve this problem  for $ p in purchaseorder where $ p/part/id = 123 and $ p/part/quantity > = 2 return $ p explainwhy the querymay return some purchase orders that order less than two copies of part 123 give a correct version of the above query 23.13 give a query in xquery to flip the nesting of data from exercise 23.10 that is  at the outermost level of nesting the output must have elements corresponding to authors  and each such elementmust have nestedwithin it items corresponding to all the books written by the author 23.14 give the dtd for an xml representation of the information in figure 7.29 create a separate element type to represent each relationship  but use id and idref to implement primary and foreign keys 23.15 give an xml schema representation of the dtd from exercise 23.14 23.16 write queries in xquery on the bibliography dtd fragment in figure 23.16  to do the following  a find all authors who have authored a book and an article in the same year b display books and articles sorted by year c display books with more than one author d find all books that contain the word ? database ? in their title and the word ? hank ? in an author ? s name  whether first or last   thesumit67.blogspot.com 1024 chapter 23 xml 23.17 give a relational mapping of the xml purchase order schema illustrated in figure 23.3  using the approach described in section 23.6.2.3 suggest how to remove redundancy in the relational schema  if item identifiers functionally determine the description and purchase and supplier names functionally determine the purchase and supplier address  respectively 23.18 write queries in sql/xml to convert university data from the relational schemawe have used in earlier chapters to the university-1 and university-2 xml schemas 23.19 as in exercise 23.18  write queries to convert university data to the university-1 and university-2 xml schemas  but this time bywriting xquery queries on the default sql/xml database to xml mapping 23.20 one way to shred an xml document is to use xquery to convert the schema to an sql/xml mapping of the corresponding relational schema  and then use the sql/xml mapping in the backward direction to populate the relation as an illustration  give an xquery query to convert data from the university-1 xml schema to the sql/xml schema shown in figure 23.15 23.21 consider the example xml schema from section 23.3.2  and write xquery queries to carry out the following tasks  a check if the key constraint shown in section 23.3.2 holds b check if the keyref constraint shown in section 23.3.2 holds 23.22 consider practice exercise 23.7  and suppose that authors could also appear as top-level elements what change would have to be done to the relational schema ? tools a number of tools to deal with xml are available in the public domain the w3c web site www.w3.org has pages describing the various xml-related standards  as well as pointers to software tools such as language implementations.an extensive list of xquery implementations is available at www.w3.org/xml/query saxon d  saxon.sourceforge.net  and galax  http  //www.galaxquery.org/  are useful as learning tools  although not designed to handle large databases exist  exist-db.org  is an open source xml database  supporting a variety of features several commercial databases  including ibm db2  oracle  and microsoft sql server support xml storage  publishing using various sql extensions  and querying using xpath and xquery bibliographical notes the world wide web consortium  w3c  acts as the standards body for webrelated standards  including basic xml and all the xml-related languages such as thesumit67.blogspot.com bibliographical notes 1025 xpath  xslt  and xquery a large number of technical reports defining the xmlrelated standards are available at www.w3.org this site also contains tutorials and pointers to software implementing the various standards the xquery language derives from an xml query language called quilt ; quilt itself included features from earlier languages such as xpath  discussed in section 23.4.2  and two other xml query languages  xql and xml-ql quilt is described in chamberlin et al  2000   deutsch et al  1999  describes the xml-ql language the w3c issued a candidate recommendation for an extension of xquery in mid-2009 that includes updates katz et al  2004  provides detailed textbook coverage of xquery the xquery specification may be found at www.w3.org/tr/xquery specifications of xquery extensions  including the xquery update facility and the xquery scripting extension are also available at this site integration of keyword querying into xml is outlined by florescu et al  2000  and amer-yahia et al  2004   funderburk et al  2002a   florescu and kossmann  1999   kanne and moerkotte  2000   and shanmugasundaram et al  1999  describe storage of xml data eisenberg and melton  2004a  provides an overview of sql/xml  while funderburk et al  2002b  provides overviews of sql/xml and xquery see chapters 28 through 30 for more information on xml support in commercial databases eisenberg and melton  2004b  provides an overview of the xqj api for xquery  while the standard definition may be found online at http  //www.w3.org/tr/xquery xml indexing  query processing and optimization  indexing of xml data  and query processing and optimization of xml queries  has been an area of great interest in the past few years a large number of papers have been published in this area one of the challenges in indexing is that queries may specify a selection on a path  such as /a/b//c  d = ? cse ?  ; the index must support efficient retrieval of nodes that satisfy the path specification and the value selection.work on indexing of xml data includes pal et al  2004  and kaushik et al  2004   if data is shredded and stored in relations  evaluating a path expression maps to computation of a join several techniques have been proposed for efficiently computing such joins  in particular when the path expression specifies any descendant  //   several techniques for numbering of nodes in xml data have been proposed that can be used to efficiently check if a node is a descendant of another ; see  for example  o ? neil et al  2004   work on optimization of xml queries includes mchugh and widom  1999   wu et al  2003  and krishnaprasad et al  2004   thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com part 8 advanced topics chapter 24 covers a number of advanced topics in application development  starting with performance tuning to improve application speed it then discusses standard benchmarks that are used as measures of commercial database-system performance issues in application development  such as application testing and application migration are discussed next the chapter concludeswith an overview of the standardization process and existing database-language standards chapter 25 describes spatial and temporal data types  and multimedia data  and the issues in storing such data in databases database issues related to mobile computing systems are also described in this chapter finally  chapter 26 describes several advanced transaction-processing techniques  including transaction-processing monitors  transactional workflows  and transaction processing issues in electronic commerce the chapter then discusses main-memory database systems and real-time transaction systems  and concludes with a discussion of long-duration transactions 1027 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter24 advanced application development there are a number of tasks in application development.we saw earlier in chapters 7 to 9 how to design and build an application one of the aspects of application design is the performance one expects out of the application in fact  it is common to find that once an application has been built  it runs slower than the designers wanted  or handles fewer transactions per second than they required an application that takes an excessive amount of time to perform requested actions can cause user dissatisfaction at best and be completely unusable at worst applications can be made to run significantly faster by performance tuning  which consists of finding and eliminating bottlenecks and adding appropriate hardware such as memory or disks there are many things an application developer can do to tune the application  and there are things that a database-system administrator can do to speed up processing for an application benchmarks are standardized sets of tasks that help to characterize the performance of database systems they are useful to get a rough idea of the hardware and software requirements of an application  even before the application is built applications must be tested as they are being developed testing requires generation of database states and test inputs  and verifying that the outputs match the expected outputs we discuss issues in application testing legacy systems are application systems that are outdated and usually based on older-generation technology.however  they are often at the core of organizations  and run missioncritical applications.we outline issues in interfacing with and issues in migrating away from legacy systems  replacing them with more modern systems standards are very important for application development  especially in the age of the internet  since applications need to communicate with each other to perform useful tasks a variety of standards have been proposed that affect database-application development 24.1 performance tuning tuning the performance of a system involves adjusting various parameters and design choices to improve its performance for a specific application various 1029 thesumit67.blogspot.com 1030 chapter 24 advanced application development aspects of a database-system design ? ranging from high-level aspects such as the schema and transaction design to database parameters such as buffer sizes  down to hardware issues such as number of disks ? affect the performance of an application each of these aspects can be adjusted so that performance is improved 24.1.1 improving set orientation when sql queries are executed from an application program  it is often the case that a query is executed frequently  butwith different values for a parameter each call has an overhead of communication with the server  in addition to processing overheads at the server for example  consider a program that steps through each department  invoking an embedded sql query to find the total salary of all instructors in the department  select sum  salary  from instructor where dept name = ? if the instructor relation does not have a clustered index on dept name  each such query will result in a scan of the relation even if there is such an index  a random i/o operation will be required for each dept name value instead  we can use a single sql query to find total salary expenses of each department  select dept name  sum  salary  from instructor group by dept name ; this query can be evaluated with a single scan of the instructor relation  avoiding random i/o for each department the results can be fetched to the client side using a single round of communication  and the client program can then step through the results to find the aggregate for each department combining multiple sql queries into a single sql query as above can reduce execution costs greatly in many cases ? for example  if the instructor relation is very large and has a large number of departments the jdbc api also provides a feature called batch update that allows a number of inserts to be performed using a single communication with the database figure 24.1 illustrates the use of this feature the code shown in the figure requires only one round of communication with the database  when the executebatch   method is executed  in contrast to similar code without the batch update feature that we saw earlier in figure 5.2 in the absence of batch update  as many rounds of communication with the database are required as there are instructors to be inserted the batch update feature also enables the database to process a batch of thesumit67.blogspot.com 24.1 performance tuning 1031 preparedstatement pstmt = conn.preparestatement  " insert into instructor values  ?  ?  ?  ?  "  ; pstmt.setstring  1  " 88877 "  ; pstmt.setstring  2  " perry "  ; pstmt.setint  3  " finance "  ; pstmt.setint  4  125000  ; pstmt.addbatch   ; pstmt.setstring  1  " 88878 "  ; pstmt.setstring  2  " thierry "  ; pstmt.setint  3  " physics "  ; pstmt.setint  4  100000  ; pstmt.addbatch   ; pstmt.executebatch   ; figure 24.1 batch update in jdbc inserts at once  which can potentially be done much more efficiently than a series of single record inserts another technique used widely in client ? server systems to reduce the cost of communication and sql compilation is to use stored procedures  where queries are stored at the server in the form of procedures  which may be precompiled clients can invoke these stored procedures  rather than communicate a series of queries another aspect of improving set orientation lies in rewriting queries with nested subqueries in the past  optimizers on many database systems were not particularly good  so how a query waswrittenwould have a big influence on how it was executed  and therefore on the performance today ? s advanced optimizers can transform even badly written queries and execute them efficiently  so the need for tuning individual queries is less important than it used to be however  complex queries containing nested subqueries are not optimized very well by many optimizers we saw techniques for nested subquery decorrelation in section 13.4.4 if a subquery is not decorrelated  it gets executed repeatedly  potentially resulting in a great deal of random i/o in contrast  decorrelation allows efficient set-oriented operations such as joins to be used  minimizing random i/o most database query optimizers incorporate some forms of decorrelation  but some can handle only very simple nested subqueries the execution plan chosen by the optimizer can be found as described earlier in chapter 13 if the optimizer has not succeeded in decorrelating a nested subquery  the query can be decorrelated by rewriting it manually 24.1.2 tuning of bulk loads and updates when loading a large volume of data into a database  called a bulk load operation   performance is usually very poor if the inserts are carried out a separate sql insert statements one reason is the overhead of parsing each sql query ; a more important reason is that performing integrity constraint checks and index thesumit67.blogspot.com 1032 chapter 24 advanced application development updates separately for each inserted tuple results in a large number of randomi/o operations if the inserts were done as a large batch  integrity-constraint checking and index update can be done in a much more set-oriented fashion  reducing overheads greatly ; performance improvements of an order-of-magnitude or more are not uncommon to support bulk load operations  most database systems provide a bulk import utility  and a corresponding bulk export utility the bulk-import utility reads data froma file  and performs integrity constraint checking as well as index maintenance in a very efficient manner common input and output file format supported by such bulk import/export utilities include text files with characters such as commas or tabs separating attribute values  with each record in a line of its own  such file formats are referred to as comma-separated values or tab-separated values formats   database specific binary formats  as well as xml formats are also supported by bulk import/export utilities the names of the bulk import/export utilities differ by database in postgresql  the utilities are called pg dump and pg restore  postgresql also provides an sql command copy which provides similar functionality   the bulk import/export utility in oracle is called sql * loader  the utility in db2 is called load  and the utility in sql server is called bcp  sql server also provides an sql command called bulk insert   we now consider the case of tuning of bulk updates suppose we have a relation funds received  dept name  amount  that stores funds received  say  by electronic funds transfer  for each of a set of departments suppose now that we want to add the amounts to the balances of the corresponding department budgets in order to use the sql update statement to carry out this task  we have to perform a look up on the funds received relation for each tuple in the department relation we can use subqueries in the update clause to carry out this task  as follows  we assume for simplicity that the relation funds received contains at most one tuple for each department update department set budget = budget +  select amount from funds received where funds received.dept name = department.dept name  where exists  select * from funds received where funds received.dept name = department.dept name  ; note that the condition in the where clause of the update ensures that only accounts with corresponding tuples in funds received are updated  while the subquery within the set clause computes the amount to be added to each such department there are many applications that require updates such as that illustrated above typically  there is a table,which we shall call the master table  andupdates to the master table are received as a batch now the master table has to be thesumit67.blogspot.com 24.1 performance tuning 1033 correspondingly updated sql  2003 provides a special construct  called the merge construct  to simplify the task of performing such merging of information for example  the above update can be expressed using merge as follows  merge into department as a using  select * from funds received  as f on  a.dept name = f.dept name  when matched then update set budget = budget + f.amount ; when a record from the subquery in the using clause matches a record in the department relation  the when matched clause is executed  which can execute an update on the relation ; in this case  the matching record in the department relation is updated as shown the merge statement can also have a when not matched then clause  which permits insertion of new records into the relation in the above example  when there is no matching department for a funds received tuple  the insertion action could create a new department record  with a null building  using the following clause  when not matched then insert values  f.dept name  null  f.budget  although not very meaningful in this example,1 the when not matched then clause can be quite useful in other cases for example  suppose the local relation is a copy of a master relation  and we receive updated as well as newly inserted records fromthemaster relation the merge statement can update matched records  these would be updated old records  and insert records that are not matched  these would be new records   not all sql implementations support the merge statement currently ; see the respective system manuals for further details 24.1.3 location of bottlenecks the performance of most systems  at least before they are tuned  is usually limited primarily by the performance of one or a few components  called bottlenecks for instance  a program may spend 80 percent of its time in a small loop deep in the code  and the remaining 20 percent of the time on the rest of the code ; the small loop then is a bottleneck improving the performance of a component that is not a bottleneck does little to improve the overall speed of the system ; in the example  improving the speed of the rest of the code can not lead to more than a 1a better action here would have been to insert these records into an error relation  but that can not be done with the merge statement thesumit67.blogspot.com 1034 chapter 24 advanced application development concurrency-control manager disk manager cpu manager transaction manager transaction monitor transaction source buffer manager lock grant lock request page page reply request page reply page request ? ? figure 24.2 queues in a database system 20 percent improvement overall  whereas improving the speed of the bottleneck loop could result in an improvement of nearly 80 percent overall  in the best case hence  when tuning a system  we must first try to discover what the bottlenecks are and then eliminate them by improving the performance of system components causing the bottlenecks.when one bottleneck is removed  itmay turn out that another component becomes the bottleneck in a well-balanced system  no single component is the bottleneck if the system contains bottlenecks  components that are not part of the bottleneck are underutilized  and could perhaps have been replaced by cheaper components with lower performance for simple programs  the time spent in each region of the code determines the overall execution time however  database systems are much more complex  and can be modeled as queueing systems a transaction requests various services from the database system  starting from entry into a server process  disk reads during execution  cpu cycles  and locks for concurrency control each of these services has a queue associated with it  and small transactions may spend most of their time waiting in queues ? especially in disk i/o queues ? instead of executing code figure 24.2 illustrates some of the queues in a database system as a result of the numerous queues in the database  bottlenecks in a database system typically show up in the form of long queues for a particular service  or  equivalently  in high utilizations for a particular service if requests are spaced exactly uniformly  and the time to service a request is less than or equal to the thesumit67.blogspot.com 24.1 performance tuning 1035 time before the next request arrives  then each request will find the resource idle and can therefore start execution immediately without waiting unfortunately  the arrival of requests in a database system is never so uniform and is instead random if a resource  such as a disk  has a low utilization  then,when a request ismade  the resource is likely to be idle  in which case the waiting time for the request will be 0 assuming uniformly randomly distributed arrivals  the length of the queue  and correspondingly the waiting time  go up exponentially with utilization ; as utilization approaches 100 percent  the queue length increases sharply  resulting in excessively long waiting times the utilization of a resource should be kept low enough that queue length is short as a rule of the thumb  utilizations of around 70 percent are considered to be good  and utilizations above 90 percent are considered excessive  since theywill result in significant delays to learn more about the theory of queueing systems  generally referred to as queueing theory  you can consult the references cited in the bibliographical notes 24.1.4 tunable parameters database administrators can tune a database system at three levels the lowest level is at the hardware level options for tuning systems at this level include adding disks or using a raid system if disk i/o is a bottleneck  adding more memory if the disk buffer size is a bottleneck  or moving to a faster processor if cpu use is a bottleneck the second level consists of the database-system parameters  such as buffer size and checkpointing intervals the exact set of database-system parameters that can be tuned depends on the specific database system most database-system manuals provide information on what database-system parameters can be adjusted  and how you should choose values for the parameters well-designed database systems perform as much tuning as possible automatically  freeing the user or database administrator from the burden for instance  in many database systems the buffer size is fixed but tunable if the system automatically adjusts the buffer size by observing indicators such as page-fault rates  then the database administrator will not have to worry about tuning the buffer size the third level is the highest level it includes the schema and transactions the administrator can tune the design of the schema  the indices that are created  and the transactions that are executed  to improve performance tuning at this level is comparatively system independent the three levels of tuning interact with one another ; we must consider them together when tuning a system for example  tuning at a higher level may result in the hardware bottleneck changing from the disk system to the cpu  or vice versa 24.1.5 tuning of hardware even in a well-designed transaction processing system  each transaction usually has to do at least a few i/o operations  if the data required by the transaction thesumit67.blogspot.com 1036 chapter 24 advanced application development are on disk an important factor in tuning a transaction processing system is to make sure that the disk subsystem can handle the rate at which i/o operations are required for instance  consider a disk that supports an access time of about 10 milliseconds  and average transfer rate of 25 to 100 megabytes per second  a fairly typical disk today   such a disk would support a little under 100 random-access i/o operations of 4 kilobytes each per second if each transaction requires just 2 i/o operations  a single disk would support at most 50 transactions per second the onlyway to support more transactions per second is to increase the number of disks if the system needs to support n transactions per second  each performing 2 i/o operations  data must be striped  or otherwise partitioned  across at least n/50 disks  ignoring skew   notice here that the limiting factor is not the capacity of the disk  but the speed at which random data can be accessed  limited in turn by the speed at which the disk arm can move   the number of i/o operations per transaction can be reduced by storing more data in memory if all data are in memory  there will be no disk i/o except for writes keeping frequently used data in memory reduces the number of disk i/os  and is worth the extra cost of memory keeping very infrequently used data inmemory would be a waste  since memory is much more expensive than disk the question is  for a given amount of money available for spending on disks or memory  what is the best way to spend the money to achieve the maximum number of transactions per second a reduction of 1 i/o per second saves   price per disk drive  /  access per second per disk  thus  if a particular page is accessed n times per second  the saving due to keeping it in memory is n times the above value storing a page in memory costs   price per megabyte of memory  /  pages per megabyte of memory  thus  the break-even point is  n * price per disk drive access per second per disk = price per megabyte of memory pages per megabyte of memory we can rearrange the equation and substitute current values for each of the above parameters to get a value for n ; if a page is accessed more frequently than this  it is worth buying enough memory to store it current disk technology and memory and disk prices  which we assume to be about $ 50 per disk  and $ 0.020 per megabyte  give a value of n around 1/6400 times per second  or equivalently  once in nearly 2 hours  for pages that are randomly accessed ; with disk and memory cost and speeds as of some years ago  the corresponding value was in 5 minutes this reasoning is captured by the rule of thumb that was originally called the 5-minute rule  if a page is used more frequently than once in 5 minutes  it should be cached in memory in other words  some years ago  the rule suggested buying thesumit67.blogspot.com 24.1 performance tuning 1037 enough memory to cache all pages that are accessed at least once in 5 minutes on average today  it is worth buying enough memory to cache all pages that are accessed at least once in 2 hours on average for data that are accessed less frequently  buy enough disks to support the rate of i/o required for the data for data that are sequentially accessed  significantly more pages can be read per second assuming 1 megabyte of data is read at a time  some years agowe had the 1-minute rule,which said that sequentially accessed data should be cached in memoryif they are used at least once in 1minute the corresponding number,with current memory and disk costs from our earlier example  is around 30 seconds surprisingly  this figure has not changed all that much over the years  since disk transfer rates have increased greatly  even though the price of a megabyte of memory has reduced greatly compared to the price of a disk clearly the amount of data read per i/o operation greatly affects the time above ; in fact the 5-minute rule still holds if about 100 kilobytes of data are read or written per i/o operation the 5-minute rule of thumb and its variants take only the number of i/o operations into account  and do not consider factors such as response time some applications need to keep even infrequently used data in memory  to support response times that are less than or comparable to disk-access time with the wide availability of flash memory  and ? solid-state disks ? based on flash memory  system designers can now choose to store frequently used data in flash storage  instead of storing it on disk alternatively  in the flash-as-buffer approach  flash storage is used as a persistent buffer  with each block having a permanent location on disk  but stored in flash instead of being written to disk as long as it is frequently used when flash storage is full  a block that is not frequently used is evicted  and flushed back to disk if it was updated after being read from disk the flash-as-buffer approach requires changes in the database system itself even if a database system does not support flash as a buffer  a database administrator can control the mapping of relations or indices to disks  and allocate frequently used relations/indices to flash storage the tablespace feature  supported by most database systems  can be used to control the mapping  by creating a tablespace on flash storage and assigning desired relations and indices to that tablespace controlling the mapping at a finer level of granularity than a relation  however  requires changes to the database-system code the ? 5-minute ? rule has been extended to the case where data can be stored on flash  in addition to main memory and disk see the bibliographical notes for references to more information another aspect of tuning is whether to use raid 1 or raid 5 the answer depends on how frequently the data are updated  since raid 5 is much slower than raid 1 on random writes  raid 5 requires 2 reads and 2 writes to execute a single random write request if an application performs r random reads and w random writes per second to support a particular throughput rate  a raid 5 implementation would require r + 4w i/o operations per second whereas a raid 1 implementation would require r + 2w i/o operations per second we can then calculate the number of disks required to support the required i/o operations per thesumit67.blogspot.com 1038 chapter 24 advanced application development second by dividing the result of the calculation by 100 i/o operations per second  for current-generation disks   for many applications  r and w are large enough that the  r + w  /100 disks can easily hold two copies of all the data for such applications  if raid 1 is used  the required number of disks is actually less than the required number of disks if raid 5 is used ! thus raid 5 is useful onlywhen the data storage requirements are very large  but the update rates  and particularly random update rates  are small  that is  for very large and very ? cold ? data 24.1.6 tuning of the schema within the constraints of the chosen normal form  it is possible to partition relations vertically for example  consider the course relation  with the schema  course  course id  title  dept name  credits  for which course id is a key.within the constraints of the normal forms  bcnf and third normal forms   we can partition the course relation into two relations  course credit  course id  credits  course title dept  course id  title  dept name  the two representations are logically equivalent  since course id is a key  but they have different performance characteristics if most accesses to course information look at only the course id and credits  then they can be run against the course credit relation  and access is likely to be somewhat faster  since the title and dept name attributes are not fetched for the same reason  more tuples of course credit will fit in the buffer than corresponding tuples of course  again leading to faster performance this effect would be particularly marked if the title and dept name attributes were large hence  a schema consisting of course credit and course title dept would be preferable to a schema consisting of the course relation in this case on the other hand  if most accesses to course information require both dept name and credits  using the course relation would be preferable  since the cost of the join of course credit and course title dept would be avoided also  the storage overhead would be lower  since therewould be only one relation  and the attribute course id would not be replicated the column store approach to storing data is based on vertical partitioning  but takes it to the limit by storing each attribute  column  of the relation in a separate file column stores have been shown to perform well for several datawarehouse applications another trick to improve performance is to store a denormalized relation  suchas a joinof instructor and department,where the information about dept name  building  and budget is repeated for every instructor more effort has to be expended to make sure the relation is consistent whenever an update is carried out however  a query that fetches the names of the instructors and the associated buildings will be speeded up  since the join of instructor and department will have been thesumit67.blogspot.com 24.1 performance tuning 1039 precomputed if such a query is executed frequently  and has to be performed as efficiently as possible  the denormalized relation could be beneficial materialized views can provide the benefits that denormalized relations provide  at the cost of some extra storage ; we describe performance tuning of materialized views in section 24.1.8 a major advantage to materialized views over denormalized relations is that maintaining consistency of redundant data becomes the job of the database system  not the programmer thus  materialized views are preferable  whenever they are supported by the database system another approach to speed up the computation of the join without materializing it  is to cluster records that would match in the join on the same disk page we saw such clustered file organizations in section 10.6.2 24.1.7 tuning of indices we can tune the indices in a database system to improve performance if queries are the bottleneck  we can often speed them up by creating appropriate indices on relations if updates are the bottleneck  there may be too many indices  which have to be updatedwhen the relations are updated removing indicesmay speed up certain updates the choice of the type of index also is important some database systems support different kinds of indices  such as hash indices and b-tree indices if range queries are common  b-tree indices are preferable to hash indices whether to make an index a clustered index is another tunable parameter only one index on a relation can be made clustered  by storing the relation sorted on the index attributes generally  the index that benefits the greatest number of queries and updates should be made clustered to help identify what indices to create  and which index  if any  on each relation should be clustered  most commercial database systems provide tuning wizards ; these are described in more detail in section 24.1.9 these tools use the past history of queries and updates  called the workload  to estimate the effects of various indices on the execution time of the queries and updates in the workload recommendations on what indices to create are based on these estimates 24.1.8 using materialized views maintaining materialized views can greatly speed up certain types of queries  in particular aggregate queries recall the example fromsection 13.5where the total salary for each department  obtained by summing the salary of each instructor in the department  is required frequently as we saw in that section  creating a materialized view storing the total salary for each department can greatly speed up such queries materialized views should be used with care  however  since there is not only space overhead for storing them but  more important  there is also time overhead for maintaining materialized views in the case of immediate view maintenance  if the updates of a transaction affect the materialized view  the materialized view must be updated as part of the same transaction the transaction may therefore run slower in the case of deferred view maintenance  the materialized view is thesumit67.blogspot.com 1040 chapter 24 advanced application development updated later ; until it is updated  the materialized view may be inconsistent with the database relations for instance  the materialized view may be brought upto date when a query uses the view  or periodically using deferred maintenance reduces the burden on update transactions the database administrator is responsible for the selection of materialized views and for view-maintenance policies the database administrator can make the selection manually by examining the types of queries in the workload  and finding out which queries need to run faster and which updates/queries may be executed more slowly from the examination  the database administrator may choose an appropriate set of materialized views for instance  the administrator mayfind that a certain aggregate is used frequently  and choose to materialize it  or may find that a particular join is computed frequently  and choose to materialize it however  manual choice is tedious for even moderately large sets of query types  andmaking a good choice may be difficult  since it requires understanding the costs of different alternatives ; only the query optimizer can estimate the costs with reasonable accuracy  without actually executing the query thus a good set of views may be found only by trial and error ? that is  by materializing one or more views  running the workload  and measuring the time taken to run the queries in the workload the administrator repeats the process until a set of views is found that gives acceptable performance a better alternative is to provide support for selecting materialized views within the database system itself  integrated with the query optimizer this approach is described in more detail in section 24.1.9 24.1.9 automated tuning of physical design most commercial database systems today provide tools to help the database administratorwith index and materialized view selection  and other tasks related to physical database design such as how to partition data in a parallel database system these tools examine the workload  the history of queries and updates  and suggest indices and views to be materialized the database administrator may specify the importance of speeding up different queries  which the tool takes into account when selecting views to materialize often tuning must be done before the application is fully developed  and the actual database contents may be small on the development database  but expected to be much larger on a production database thus  some tuning tools also allow the database administrator to specify information about the expected size of the database and related statistics microsoft ? s database tuning assistant  for example  allows the user to ask ? what if ? questions  whereby the user can pick a view  and the optimizer then estimates the effect of materializing the view on the total cost of theworkload and on the individual costs of different types of queries and updates in the workload the automatic selection of indices and materialized views is usually implemented by enumerating different alternatives and using the query optimizer to estimate the costs and benefits of selecting each alternative by using the workthesumit67 blogspot.com 24.1 performance tuning 1041 load since the number of design alternatives may be extremely large  as also the workload  the selection techniques must be designed carefully the first step is to generate a workload this is usually done by recording all the queries and updates that are executed during some time period next  the selection tools perform workload compression  that is  create a representation of the workload using a small number of updates and queries for example  updates of the same form can be represented by a single update with a weight corresponding to how many times the update occurred queries of the same form can be similarly replaced by a representative with appropriate weight after this  queries that are very infrequent and do not have a high cost may be discarded from consideration the most expensive queries may be chosen to be addressed first such workload compression is essential for large workloads with the help of the optimizer  the tool would come up with a set of indices andmaterialized views that could help the queries and updates in the compressed workload different combinations of these indices and materialized views can be tried out to find the best combination.however  an exhaustive approachwould be totally impractical  since the number of potential indices and materialized views is already large  and each subset of these is a potential design alternative  leading to an exponential number of alternatives heuristics are used to reduce the space of alternatives  that is  to reduce the number of combinations considered greedy heuristics for index and materialized view selection operate as follows  they estimate the benefits ofmaterializing different indices or views  using the optimizer ? s cost estimation functionality as a subroutine   they then choose the index or view that gives either the maximum benefit or the maximum benefit per unit space  that is  benefit divided by the space required to store the index or view   the cost of maintaining the index or view must be taken into account when computing the benefit once the heuristic has selected an index or view  the benefits of other indices or views may have changed  so the heuristic recomputes these  and chooses the next best index or view for materialization the process continues until either the available disk space for storing indices or materialized views is exhausted  or the cost of maintaining the remaining candidates is more than the benefit to queries that could use the indices or views real-world index and materialized-view selection tools usually incorporate some elements of greedy selection  but use other techniques to get better results they also support other aspects of physical database design  such as decidinghow to partition a relation in a parallel database  or what physical storage mechanism to use for a relation 24.1.10 tuning of concurrent transactions concurrent execution of different types of transactions can sometimes lead to poor performance because of contention on locks we first consider the case of read-write contention  which is more common  and then consider the case of write-write contention as an example of read-write contention  consider the following situation on a banking database during the day  numerous small update transactions are thesumit67.blogspot.com 1042 chapter 24 advanced application development executed almost continuously suppose that a large query that computes statistics on branches is run at the same time if the query performs a scan on a relation  it may block out all updates on the relation while it runs  and that can have a disastrous effect on the performance of the system several database systems ? oracle  postgresql  and microsoft sql server  for example ? support snapshot isolation  whereby queries are executed on a snapshot of the data  and updates can go on concurrently  snapshot isolation is described in detail in section 15.7  snapshot isolation should be used  if available  for large queries  to avoid lock contention in the above situation in sql server  executing the statement set transaction isolation level snapshot at the beginning of a transaction results in snapshot isolation being used for that transaction in oracle and postgresql  using the keyword serializable in place of the keyword snapshot in the above command has the same effect  since both these systems actually use snapshot isolation when the isolation level is set to serializable if snapshot isolation is not available  an alternative option is to execute large queries at times when updates are few or nonexistent however  for databases supportingweb sites  there may be no such quiet period for updates another alternative is to use weaker levels of consistency  such as the read committed isolation level,whereby evaluation of the query has a minimal impact on concurrent updates  but the query results are not guaranteed to be consistent the application semantics determine whether approximate  inconsistent  answers are acceptable we now consider the case of write-write contention data items that are updated very frequently can result in poor performance with locking  with many transactions waiting for locks on those data items such data items are called update hot spots update hot spots can cause problems even with snapshot isolation  causing frequent transaction aborts due to write validation failures a commonly occurring situation that results in an update hot spot is as follows  transactions need to assign unique identifiers to data items being inserted into the database  and to do so they read and increment a sequence counter stored in a tuple in the database if inserts are frequent  and the sequence counter is locked in a two-phase manner  the tuple containing the sequence counter becomes a hot spot onewaytoimprove concurrencyis torelease the lockonthe sequence counter immediately after it is read and incremented ; however  after doing so  even if the transaction aborts  the update to the sequence counter should not be rolled back to understand why  suppose t1 increments the sequence counter  and then t2 increments the sequence counter before t1 commits ; if t1 then aborts  rolling back its update  either by restoring the counter to the original value  or by decrementing the counter  will result in the sequence value used by t2 getting reused by a subsequent transaction thesumit67.blogspot.com 24.1 performance tuning 1043 most databases provide a special construct for creating sequence counters that implement early  non-two-phase  lock release  coupled with special case treatment of undo logging so that updates to the counter are not rolled back if the transaction aborts the sql standard allows a sequence counter to be created using the command  create sequence counter1 ; in the above command  counter1 is the name of the sequence ; multiple sequences can be created with different names the syntax to get a value from the sequence is not standardized ; in oracle  counter1.nextval would return the next value of the sequence  after incrementing it  while the function call nextval  ? counter1 ?  would have the same effect in postgresql  and db2 uses the syntax nextval for counter1 the sql standardprovides an alternative tousing an explicit sequence counter  which is useful when the goal is to give unique identifiers to tuples inserted into a relation to do so  the keyword identity can be added to the declaration of an integer attribute of a relation  usually this attribute would also be declared as the primary key   if the value for that attribute is left unspecified in an insert statement for that relation  a unique new value is created automatically for each newly inserted tuple a non-two-phase locked sequence counter is used internally to implement the identity declaration  with the counter incremented each time a tuple is inserted several databases including db2 and sql server support the identity declaration  although the syntax varies postgresql supports a data type called serial  which provides the same effect ; the postgresql type serial is implemented by transparently creating a non-two-phase locked sequence it is worth noting that since the acquisition of a sequence number by a transaction can not be rolled back if the transaction aborts  for reasons discussed earlier   transaction aborts may result in gaps in the sequence numbers in tuple inserted in the database for example  there may be tuples with identifier value 1001 and 1003  but no tuple with value 1002  if the transaction that acquired the sequence number 1002 did not commit such gaps are not acceptable in some applications ; for example  some financial applications require that there be no gaps in bill or receipt numbers database provided sequences and automatically incremented attributes should not be used for such applications  since they can result in gaps.a sequence counter stored in normal tuple  which is locked in a two-phase manner  would not be susceptible to such gaps since a transaction abort would restore the sequence counter value  and the next transaction would get the same sequence number  avoiding a gap long update transactions can cause performance problems with system logs  and can increase the time taken to recover from system crashes if a transaction performs many updates  the system log may become full even before the transaction completes  in which case the transaction will have to be rolled back if an update transaction runs for a long time  even with few updates   it may block deletion of old parts of the log  if the logging system is not well designed again  this blocking could lead to the log getting filled up thesumit67.blogspot.com 1044 chapter 24 advanced application development to avoid such problems  many database systems impose strict limits on the number of updates that a single transaction can carry out even if the system does not impose such limits  it is often helpful to break up a large update transaction into a set of smaller update transactionswhere possible for example  a transaction that gives a raise to every employee in a large corporation could be split up into a series of small transactions  each of which updates a small range of employeeids such transactions are called minibatch transactions however  minibatch transactions must be used with care first  if there are concurrent updates on the set of employees  the result of the set of smaller transactions may not be equivalent to that of the single large transaction second  if there is a failure  the salaries of some of the employees would have been increased by committed transactions  but salaries of other employees would not to avoid this problem  as soon as the system recovers from failure  we must execute the transactions remaining in the batch long transactions  whether read-only or update  can also result in the lock table becoming full if a single query scans a large relation  the query optimizer would ensure that a relation lock is obtained instead of acquiring a large number of tuple locks however  if a transaction executes a large number of small queries or updates  it may acquire a large number of locks  resulting in the lock table becoming full to avoid this problem  some databases provide for automatic lock escalation ; with this technique  if a transaction has acquired a large number of tuple locks  tuple locks are upgraded to page locks  or even full relation locks recall thatwith multiple-granularity locking  section 15.3   once a coarser level lock is obtained  there is no need to record finer-level locks  so tuple lock entries can be removed from the lock table  freeing up space on databases that do not support lock escalation  it is possible for the transaction to explicitly acquire a relation lock  thereby avoiding the acquisition of tuple locks 24.1.11 performance simulation to test the performance of a database system even before it is installed  we can create a performance-simulation model of the database system each service shown in figure 24.2  such as the cpu  each disk  the buffer  and the concurrency control  is modeled in the simulation instead of modeling details of a service  the simulation model may capture only some aspects of each service  such as the service time ? that is  the time taken to finish processing a request once processing has begun thus  the simulation can model a disk access from just the average disk-access time since requests for a service generally have to wait their turn  each service has an associated queue in the simulation model a transaction consists of a series of requests the requests are queued up as they arrive  and are serviced according to the policy for that service  such as first come  first served the models for services such as cpu and the disks conceptually operate in parallel  to account for the fact that these subsystems operate in parallel in a real system thesumit67.blogspot.com 24.2 performance benchmarks 1045 once the simulation model for transaction processing is built  the system administrator can run a number of experiments on it the administrator can use experiments with simulated transactions arriving at different rates to find how the system would behave under various load conditions the administrator could run other experiments that vary the service times for each service to find out how sensitive the performance is to each of them system parameters  too  can be varied  so that performance tuning can be done on the simulation model 24.2 performance benchmarks as database servers become more standardized  the differentiating factor among the products of different vendors is those products ? performance performance benchmarks are suites of tasks that are used to quantify the performance of software systems 24.2.1 suites of tasks since most software systems  such as databases  are complex  there is a good deal of variation in their implementation by different vendors as a result  there is a significant amount of variation in their performance on different tasks one system may be the most efficient on a particular task ; another may be the most efficient on a different task hence  a single task is usually insufficient to quantify the performance of the system instead  the performance of a system is measured by suites of standardized tasks  called performance benchmarks combining the performance numbers from multiple tasks must be done with care suppose that we have two tasks  t1 and t2  and that we measure the throughput of a system as the number of transactions of each type that run in a given amount of time ? say  1 second suppose that system a runs t1 at 99 transactions per second and t2 at 1 transaction per second similarly  let system b run both t1 and t2 at 50 transactions per second suppose also that a workload has an equal mixture of the two types of transactions if we took the average of the two pairs of numbers  that is  99 and 1  versus 50 and 50   it might appear that the two systems have equal performance however  it is wrong to take the averages in this fashion ? if we ran 50 transactions of each type  system awould take about 50.5 seconds to finish  whereas system b would finish in just 2 seconds ! the example shows that a simple measure of performance is misleading if there is more than one type of transaction the right way to average out the numbers is to take the time to completion for the workload  rather than the average throughput for each transaction type we can then compute system performance accurately in transactions per second for a specified workload thus  system a takes 50.5/100  which is 0.505 seconds per transaction  whereas system b takes 0.02 seconds per transaction  on average in terms of throughput  system a runs at an average of 1.98 transactions per second  whereas system b runs at 50 transactions per second assuming that transactions of all the types are equally thesumit67.blogspot.com 1046 chapter 24 advanced application development likely  the correct way to average out the throughputs on different transaction types is to take the harmonic mean of the throughputs the harmonic mean of n throughputs t1      tn is defined as  n 1 t1 + 1 t2 + ? ? ? + 1 tn for our example  the harmonic mean for the throughputs in system a is 1.98 for system b  it is 50 thus  system b is approximately 25 times faster than system a on a workload consisting of an equal mixture of the two example types of transactions 24.2.2 database-application classes online transaction processing  oltp  and decision support  including online analytical processing  olap   are two broad classes of applications handled by database systems these two classes of tasks have different requirements high concurrency and clever techniques to speed up commit processing are required for supporting a high rate of update transactions on the other hand  good queryevaluation algorithms and query optimization are required for decision support the architecture of some database systems has been tuned to transaction processing ; that of others  such as the teradata series of parallel database systems  has been tuned to decision support other vendors try to strike a balance between the two tasks applications usually have a mixture of transaction-processing and decisionsupport requirements hence  which database system is best for an application depends on what mix of the two requirements the application has suppose that we have throughput numbers for the two classes of applications separately  and the application at hand has a mix of transactions in the two classes we must be careful even about taking the harmonic mean of the throughput numbers  because of interference between the transactions for example  a longrunning decision-support transactionmay acquire a number of locks  which may prevent all progress of update transactions the harmonic mean of throughputs should be used only if the transactions do not interfere with one another 24.2.3 the tpc benchmarks the transaction processing performance council  tpc  has defined a series of benchmark standards for database systems the tpc benchmarks are defined in great detail they define the set of relations and the sizes of the tuples they define the number of tuples in the relations not as a fixed number  but rather as a multiple of the number of claimed transactions per second  to reflect that a larger rate of transaction execution is likely to be correlated with a larger number of accounts the performance metric is throughput  expressed as transactions per second  tps   when its performance is measured  the system must provide a response time within certain bounds  so that a high throughput can not be obtained at the cost of very long response times thesumit67.blogspot.com 24.2 performance benchmarks 1047 further  for business applications  cost is of great importance hence  the tpc benchmark also measures performance in terms of price per tps a large system may have a high number of transactions per second  but may be expensive  that is  have a high price per tps   moreover  a company can not claim tpc benchmark numbers for its systems without an external audit that ensures that the system faithfully follows the definition of the benchmark  including full support for the acid properties of transactions the first in the series was the tpc-a benchmark  which was defined in 1989 this benchmark simulates a typical bank application by a single type of transaction that models cash withdrawal and deposit at a bank teller the transaction updates several relations ? such as the bank balance  the teller ? s balance  and the customer ? s balance ? and adds a record to an audit trail relation the benchmark also incorporates communication with terminals  to model the end-to-end performance of the system realistically the tpc-b benchmark was designed to test the core performance of the database system  along with the operating system on which the system runs it removes the parts of the tpc-a benchmark that deal with users  communication  and terminals  to focus on the back-end database server neither tpc-a nor tpc-b is in use today the tpc-c benchmark was designed to model a more complex system than the tpc-a benchmark the tpc-c benchmark concentrates on the main activities in an order-entry environment  such as entering and delivering orders  recording payments  checking status of orders  and monitoring levels of stock the tpc-c benchmark is still widely used for benchmarking online transaction processing  oltp  systems the more recent tpc-e benchmark is also aimed at oltp systems  but is based on a model of a brokerage firm  with customerswho interact with the firm and generate transactions the firm in turn interacts with financial markets to execute transactions the tpc-d benchmark was designed to test the performance of database systems on decision-support queries decision-support systems are becoming increasingly important today the tpc-a  tpc-b  and tpc-c benchmarks measure performance on transaction-processing workloads  and should not be used as a measure of performance on decision-support queries the d in tpc-d stands for decision support the tpc-d benchmark schema models a sales/distribution application  with parts  suppliers  customers  and orders  along with some auxiliary information the sizes of the relations are defined as a ratio  and database size is the total size of all the relations  expressed in gigabytes tpc-d at scale factor 1 represents the tpc-d benchmark on a 1-gigabyte database  while scale factor 10 represents a 10-gigabyte database the benchmark workload consists of a set of 17 sql queries modeling common tasks executed on decision-support systems some of the queries make use of complex sql features  such as aggregation and nested queries the benchmark ? s users soon realized that the various tpc-d queries could be significantly speeded up by using materialized views and other redundant information there are applications  such as periodic reporting tasks  where the queries are known in advance and materialized view can be carefully selected thesumit67.blogspot.com 1048 chapter 24 advanced application development to speed up the queries it is necessary  however  to account for the overhead of maintaining materialized views the tpc-h benchmark  where h represents ad hoc  is a refinement of the tpc-d benchmark the schema is the same  but there are 22 queries  of which 16 are from tpc-d in addition  there are two updates  a set of inserts  and a set of deletes tpc-h prohibits materialized views and other redundant information  and permits indices only on primary and foreign keys this benchmark models ad hoc queryingwhere the queries are not known beforehand  so it is not possible to create appropriatematerialized views ahead of time a variant  tpc-r  where r stands for ? reporting ?   which is no longer in use  allowed the use of materialized views and other redundant information tpc-h measures performance in thisway  the power test runs the queries and updates one at a time sequentially  and 3600 seconds divided by the geometric mean of the execution times of the queries  in seconds  gives ameasure of queries per hour the throughput test runs multiple streams in parallel  with each stream executing all 22 queries there is also a parallel update stream here the total time for the entire run is used to compute the number of queries per hour the composite query per hour metric  which is the overall metric  is then obtained as the square root of the product of the power and throughput metrics a composite price/performance metric is defined by dividing the system price by the composite metric the tpc-w web commerce benchmark is an end-to-end benchmark that models web sites having static content  primarily images  and dynamic content generated from a database caching of dynamic content is specifically permitted  since it is very useful for speeding up web sites the benchmark models an electronic bookstore  and like other tpc benchmarks  provides for different scale factors the primary performance metrics areweb interactions per second  wips  and price per wips however  the tpc-w benchmark is no longer in use 24.3 other issues in application development in this section  we discuss two issues in application development  testing of applications  and migration of applications 24.3.1 testing applications testing of programs involves designing a test suite  that is  a collection of test cases testing is not a one-time process  since programs evolve continuously  and bugs may appear as an unintended consequence of a change in the program ; such a bug is referred to as program regression thus  after every change to a program  the program must be tested again it is usually infeasible to have a human perform tests after every change to a program instead  expected test outputs are stored with each test case in a test suite regression testing involves running the program on each test case in a test suite  and checking that the program generates the expected test output thesumit67.blogspot.com 24.3 other issues in application development 1049 in the context of database applications  a test case consists of two parts  a database state  and an input to a specific interface of the application sql queries can have subtle bugs that can be difficult to catch for example  a querymay execute r  s  when it should have actually performed r  s the difference between these two queries would be found only if the test database had an r tuple with no matching s tuple thus  it is important to create test databases that can catch commonly occurring errors such errors are referred to as mutations  since they are usually small changes to a query  or program   a test case that produces different outputs on an intended query and a mutant of the query is said to kill the mutant a test suite should have test cases that kill  most  commonly occurring mutants if a test case performs an update on the database  to check that it executed properly one must verify that the contents of the database match the expected contents thus  the expected output consists not only of data displayed on the user ? s screen  but also  updates to  the database state since the database state can be rather large  multiple test cases would share a common database state testing is complicated by the fact that if a test case performs an update on the database  the results of other test cases run subsequently on the same database may not match the expected results the other test caseswould then be erroneously reported as having failed to avoid this problem  whenever a test case performs an update  the database state must be restored to its original state after running the test testing can also be used to ensure that an application meets performance requirements to carry out such performance testing  the test database must be of the same size as the real database would be in some cases  there is already existing data on which performance testing can be carried out in other cases  a test database of the required size must be generated ; there are several tools available for generating such test databases these tools ensure that the generated data satisfies constraints such as primary and foreign key constraints they may additionally generate data that looks meaningful  for example  by populating a name attribute using meaningful names instead of random strings some tools also allow data distributions to be specified ; for example  a university database may require a distribution with most students in the range of 18 to 25 years  and most faculty in the range of 25 to 65 years even if there is an existing database  organizations usually do not want to reveal sensitive data to an external organization that may be carrying out the performance tests in such a situation  a copy of the real database may be made  and the values in the copy may be modified in such a way that any sensitive data  such as credit-card numbers  social-security numbers  or dates of birth  are obfuscated obfuscation is done in most cases by replacing a real value with a randomly generated value  taking care to also update all references to that value  in case the value is a primary key   on the other hand  if the application execution depends on the value  such as the date of birth in an application that performs different actions based on the date of birth  obfuscation may make small random changes in the value instead of replacing it completely thesumit67.blogspot.com 1050 chapter 24 advanced application development 24.3.2 application migration legacy systems are older-generation application systems that are in use by an organization  but that the organization wishes to replace with a different application for example  many organizations developed applications in house  but may decide to replace themwith a commercial product in some cases  a legacy system may use old technology that is incompatible with current-generation standards and systems some legacy systems in operation today are several decades old and are based on technologies such as databases that use the network or hierarchical data models  or use cobol and file systemswithout a database such systems may still contain valuable data  and may support critical applications replacing legacy applications with new applications is often costly in terms of both time and money  since they are often very large  consisting of millions of lines of code developed by teams of programmers  often over several decades they contain large amounts of data that must be ported to the new application  which may use a completely different schema switchover from an old to a new application involves retraining large numbers of staff switchover must usually be done without any disruption  with data entered in the old system available through the new system as well many organizations attempt to avoid replacing legacy systems  and instead try to interoperate them with newer systems one approach used to interoperate between relational databases and legacy databases is to build a layer  called a wrapper  on top of the legacy systems that can make the legacy system appear to be a relational database the wrapper may provide support for odbc or other interconnection standards such as ole-db,which can be used to query and update the legacy system the wrapper is responsible for converting relational queries and updates into queries and updates on the legacy system when an organization decides to replace a legacy system with a new system  it may follow a process called reverse engineering  which consists of going over the code of the legacy system to come up with schema designs in the required data model  such as an e-r model or an object-oriented data model   reverse engineering also examines the code to find out what procedures and processes were implemented  in order to get a high-level model of the system reverse engineering is needed because legacy systems usually do not have high-level documentation of their schema and overall system design when coming up with the design of a new system  developers review the design  so that it can be improved rather than just reimplemented as is extensive coding is required to support all the functionality  such as user interface and reporting systems  that was provided by the legacy system the overall process is called re-engineering when a new system has been built and tested  the system must be populated with data from the legacy system  and all further activities must be carried out on the new system.however  abruptly transitioning to a new system,which is called the big-bang approach  carries several risks first  users may not be familiar with the interfaces of the new system second  there may be bugs or performance problems in the new system that were not discovered when it was tested such problems may lead to great losses for companies  since their ability to carry out thesumit67.blogspot.com 24.4 standardization 1051 critical transactions such as sales and purchasesmay be severely affected in some extreme cases the new system has even been abandoned  and the legacy system reused  after an attempted switchover failed an alternative approach  called the chicken-little approach  incrementally replaces the functionality of the legacy system for example  the new user interfaces may be used with the old system in the back end  or vice versa another option is to use the new system only for some functionality that can be decoupled from the legacy system in either case  the legacy and new systems coexist for some time there is therefore a need for developing and using wrappers on the legacy system to provide required functionality to interoperate with the new system this approach therefore has a higher development cost associated with it 24.4 standardization standards define the interface of a software system ; for example  standards define the syntax and semantics of a programming language  or the functions in an application-program interface  or even a data model  such as the object-oriented database standards   today  database systems are complex  and are often made up of multiple independently created parts that need to interact for example  client programs may be created independently of back-end systems  but the two must be able to interact with each other a company that has multiple heterogeneous database systems may need to exchange data between the databases given such a scenario  standards play an important role formal standards are those developed by a standards organization or by industry groups  through a public process dominant products sometimes become de facto standards  in that they become generally accepted as standards without any formal process of recognition some formal standards  like many aspects of the sql-92 and sql  1999 standards  are anticipatory standards that lead the marketplace ; they define features that vendors then implement in products in other cases  the standards  or parts of the standards  are reactionary standards  in that they attempt to standardize features that some vendors have already implemented  and that may even have become de facto standards sql-89 was in many ways reactionary  since it standardized features  such as integrity checking  that were already present in the ibm saa sql standard and in other databases formal standards committees are typically composed of representatives of the vendors and of members from user groups and standards organizations such as the international organization for standardization  iso  or the american national standards institute  ansi   or professional bodies  such as the institute of electrical and electronics engineers  ieee   formal standards committees meet periodically  and members present proposals for features to be added to or modified in the standard after a  usually extended  period of discussion  modifications to the proposal  and public review,members vote onwhether to accept or reject a feature some time after a standard has been defined and implemented  its shortcomings become clear and new requirements become apparent the process of updating thesumit67.blogspot.com 1052 chapter 24 advanced application development the standard then begins  and a new version of the standard is usually released after a few years this cycle usually repeats every few years  until eventually  perhaps many years later  the standard becomes technologically irrelevant  or loses its user base the dbtg codasyl standard for network databases  formulated by the database task group  was one of the early formal standards for databases ibm database products formerly established de facto standards  since ibm commanded muchof the database market.with the growth of relational databases came a number of new entrants in the database business ; hence  the need for formal standards arose in recent years  microsoft has created a number of specifications that also have become de facto standards a notable example is odbc  which is now used in non-microsoft environments jdbc  whose specification was created by sun microsystems  is another widely used de facto standard this section gives a very high-level overview of different standards  concentrating on the goals of the standard the bibliographical notes at the end of the chapter provide references to detailed descriptions of the standards mentioned in this section 24.4.1 sql standards since sql is the most widely used query language  much work has been done on standardizing it ansi and iso  with the various database vendors  have played a leading role in this work the sql-86 standard was the initial version the ibm systems application architecture  saa  standard for sql was released in 1987 as people identified the need for more features  updated versions of the formal sql standard were developed  called sql-89 and sql-92 the sql  1999 version of the sql standard added a variety of features to sql we have seen many of these features in earlier chapters the sql  2003 version of the sql standard is a minor extension of the sql  1999 standard some features such as the sql  1999 olap features  section 5.6.3  were specified as an amendment to the earlier version of the sql  1999 standard  instead of waiting for the release of sql  2003 the sql  2003 standard was broken into several parts  ? part 1  sql/framework provides an overview of the standard ? part 2  sql/foundation defines the basics of the standard  types  schemas  tables  views  query and update statements  expressions  security model  predicates  assignment rules  transaction management  and so on ? part 3  sql/cli  call level interface  defines application program interfaces to sql ? part 4  sql/psm  persistent stored modules  defines extensions to sql to make it procedural ? part 9  sql/med  management of external data  defines standards or interfacing an sql system to external sources by writing wrappers  system thesumit67.blogspot.com 24.4 standardization 1053 designers can treat external data sources  such as files or data in nonrelational databases  as if they were ? foreign ? tables ? part 10  sql/olb  object language bindings  defines standards for embedding sql in java ? part 11  sql/schemata  information and definition schema  defines a standard catalog interface ? part 13  sql/jrt  java routines and types  defines standards for accessing routines and types in java ? part 14  sql/xml defines xml-related specifications the missing numbers cover features such as temporal data  distributed transaction processing  and multimedia data  for which there is as yet no agreement on the standards the latest versions of the sql standard are sql  2006  which added several features related to xml  and sql  2008  which introduces a number of extensions to the sql language 24.4.2 database connectivity standards the odbc standard is a widely used standard for communication between client applications and database systems odbc is based on the sql call level interface  cli  standards developed by the x/open industry consortium and the sql access group  but it has several extensions the odbc api defines a cli  an sql syntax definition  and rules about permissible sequences of cli calls the standard also defines conformance levels for the cli and the sql syntax for example  the core level of the cli has commands to connect to a database  to prepare and execute sql statements  to get back results or status values  and to manage transactions the next level of conformance  level 1  requires support for catalog information retrieval and some other features over and above the core-level cli ; level 2 requires further features  such as ability to send and retrieve arrays of parameter values and to retrieve more detailed catalog information odbc allows a client to connect simultaneously to multiple data sources and to switch among them  but transactions on each are independent ; odbc does not support two-phase commit a distributed system provides a more general environment than a client ? server system the x/open consortium has also developed the x/open xa standards for interoperation of databases these standards define transactionmanagement primitives  such as transaction begin  commit  abort  and prepareto commit  that compliant databases should provide ; a transaction manager can invoke these primitives to implement distributed transactions by two-phase commit the xa standards are independent of the data model and of the specific interfaces between clients and databases to exchange data thus  we can use the xa protocols to implement a distributed transaction system in which a single transacthesumit67 blogspot.com 1054 chapter 24 advanced application development tion can access relational as well as object-oriented databases  yet the transaction manager ensures global consistency via two-phase commit there are many data sources that are not relational databases  and in fact may not be databases at all examples are flat files and email stores microsoft ? s ole-db is a c + + api with goals similar to odbc  but for nondatabase data sources that may provide only limited querying and update facilities just like odbc  ole-db provides constructs for connecting to a data source  starting a session  executing commands  and getting back results in the form of a rowset  which is a set of result rows however  ole-db differs fromodbc in severalways to support data sources with limited feature support  features in ole-db are divided into a number of interfaces  and a data source may implement only a subset of the interfaces an ole-db program can negotiate with a data source to find what interfaces are supported in odbc commands are always in sql in ole-db  commands may be in any language supported by the data source ; while some sources may support sql  or a limited subset of sql  other sources may provide only simple capabilities such as accessing data in a flat file  without any query capability another major difference of ole-db from odbc is that a rowset is an object that can be shared by multiple applications through shared memory a rowset object can be updated by one application  and other applications sharing that object will get notified about the change the active data objects  ado  api  also created by microsoft  provides an easy-to-use interface to the ole-db functionality  which can be called from scripting languages  such as vbscript and jscript the newer ado.net api is designed for applications written in the .net languages such as c # and visual basic.net in addition to providing simplified interfaces  itprovides an abstraction called the dataset that permits disconnected data access 24.4.3 object database standards standards in the area of object-oriented databases have so far been driven primarily by oodb vendors the object database management group  odmg  was a group formed by oodb vendors to standardize the data model and language interfaces to oodbs the c + + language interface specified by odmg was briefly outlined in chapter 22 odmg is no longer active jdo is a standard for adding persistence to java the object management group  omg  is a consortium of companies  formed with the objective of developing a standard architecture for distributed software applications based on the object-oriented model.omgbrought out the object management architecture  oma  reference model the object request broker  orb  is a component of the oma architecture that provides message dispatch to distributed objects transparently  so the physical location of the object is not important the commonobjectrequest broker architecture  corba  provides a detailed specification of the orb  and includes aninterface description language  idl   which is used to define the data types used for data interchange the idl helps to supthesumit67 blogspot.com 24.4 standardization 1055 port data conversion when data are shipped between systems with different data representations microsoft introduced the entity data model  which incorporates ideas from the entity-relationship and object-oriented data models  and an approach to integrating querying with the programming language  called language integrated querying or linq these are likely to become de facto standards 24.4.4 xml-based standards a wide variety of standards based on xml  see chapter 23  have been defined for a wide variety of applications many of these standards are related to e-commerce they include standards promulgated by nonprofit consortia and corporate-backed efforts to create de facto standards rosettanet  which falls into the former category  is an industry consortium that uses xml-based standards to facilitate supply-chain management in the computer and information technology industries supply-chain management refers to the purchases of material and services that an organization needs to function in contrast  customer-relationship management refers to the front end of a company ? s interaction  dealing with customers supply-chain management requires standardization of a variety of things such as  ? global company identifier  rosettanet specifies a systemfor uniquely identifying companies  using a 9-digit identifier called data universal numbering system  duns   ? global product identifier  rosettanet specifies a 14-digit global trade item number  gtin  for identifying products and services ? global class identifier  this is a 10-digit hierarchical code for classifying products and services called the united nations/standard product and services code  un/spsc   ? interfaces between trading partners  rosettanet partner interface processes  pips  define business processes between partners pips are system-to-system xml-based dialogs  they define the formats and semantics of business documents involved in a process and the steps involved in completing a transaction examples of steps could include getting product and service information  purchase orders  order invoicing  payment  order status requests  inventory management  post-sales support including service warranty  and so on exchange of design  configuration  process  and quality information is also possible to coordinate manufacturing activities across organizations participants in electronic marketplaces may store data in a variety of database systems these systems may use different data models  data formats  and data types furthermore  there may be semantic differences  metric versus english measure  distinct monetary currencies  and so forth  in the data standards for electronic marketplaces include methods for wrapping each of these heterogethesumit67 blogspot.com 1056 chapter 24 advanced application development neous systems with an xml schema these xml wrappers form the basis of a unified view of data across all of the participants in the marketplace simple object access protocol  soap  is a remote procedure call standard that uses xml to encode data  both parameters and results   and uses http as the transport protocol ; that is  a procedure call becomes an http request soap is backed by the world wide web consortium  w3c  and has gained wide acceptance in industry soap can be used in a variety of applications for instance  in business-to-business e-commerce  applications running at one site can access data from and execute actions at other sites through soap soap and web services were described in more detail in section 23.7.3 24.5 summary ? tuning of thedatabase-system parameters  aswell as the higher-level database design ? such as the schema  indices  and transactions ? is important for good performance queries can be tuned to improve set-orientation  while bulk-loading utilities can greatly speed up data import into a database tuning is best done by identifying bottlenecks and eliminating them database systems usually have a variety of tunable parameters  such as buffer sizes  memory size  and number of disks the set of indices and materialized views can be appropriately chosen to minimize overall cost transactions can be tuned to minimize lock contention ; snapshot isolation  and sequence numbering facilities supporting early lock release are useful tools for reducing read-write and write-write contention ? performance benchmarks play an important role in comparisons of database systems  especially as systems become more standards compliant the tpc benchmark suites are widely used  and the different tpc benchmarks are useful for comparing the performance of databases under different workloads ? applications need to be tested extensively as they are developed  and before they are deployed testing is used to catch errors  as well as to ensure that performance goals are met ? legacy systems are systems based on older-generation technologies such as nonrelational databases or even directly on file systems interfacing legacy systems with new-generation systems is often important when they run mission-critical systems migrating from legacy systems to new-generation systems must be done carefully to avoid disruptions  which can be very expensive ? standards are important because of the complexity of database systems and their need for interoperation formal standards exist for sql de facto standards  such as odbc and jdbc  and standards adopted by industry groups  such as corba  have played an important role in the growth of client ? server database systems thesumit67.blogspot.com practice exercises 1057 review terms ? performance tuning ? set-orientation ? batch update  jdbc  ? bulk load ? bulk update ? merge statement ? bottlenecks ? queueing systems ? tunable parameters ? tuning of hardware ? five-minute rule ? one-minute rule ? tuning of the schema ? tuning of indices ? materialized views ? immediate view maintenance ? deferred view maintenance ? tuning of transactions ? lock contention ? sequences ? minibatch transactions ? performance simulation ? performance benchmarks ? service time ? time to completion ? database-application classes ? the tpc benchmarks ? tpc-a ? tpc-b ? tpc-c ? tpc-d ? tpc-e ? tpc-h ? web interactions per second ? regression testing ? killing mutants ? legacy systems ? reverse engineering ? re-engineering ? standardization ? formal standards ? de facto standards ? anticipatory standards ? reactionary standards ? database connectivity standards ? odbc ? ole-db ? x/open xa standards ? object database standards ? odmg ? corba ? xml-based standards practice exercises 24.1 many applications need to generate sequence numbers for each transaction a if a sequence counter is locked in two-phase manner  it can become a concurrency bottleneck explain why this may be the case thesumit67.blogspot.com 1058 chapter 24 advanced application development b many database systems support built-in sequence counters that are not locked in two-phase manner ; when a transaction requests a sequence number  the counter is locked  incremented and unlocked i explain how such counters can improve concurrency ii explainwhy there may be gaps in the sequence numbers belonging to the final set of committed transactions 24.2 suppose you are given a relation r  a  b  c   a give an example of a situation under which the performance of equality selection queries on attribute a can be greatly affected by how r is clustered b suppose you also had range selection queries on attribute b canyou cluster r in such a way that the equality selection queries on r.a and the range selection queries on r.b can both be answered efficiently ? explain your answer c if clustering as above is not possible  suggest how both types of queries can be executed efficiently by choosing appropriate indices  assuming your database supports index-only plans  that is  if all information required for a query is available in an index  the database can generate a plan that uses the index but does not access the relation   24.3 suppose that a database application does not appear to have a single bottleneck ; that is  cpu and disk utilization are both high  and all database queues are roughly balanced does that mean the application can not be tuned further ? explain your answer 24.4 suppose a system runs three types of transactions transactions of type a run at the rate of 50 per second  transactions of type b run at 100 per second  and transactions of type c run at 200 per second suppose the mix of transactions has 25 percent of type a  25 percent of type b  and 50 percent of type c a what is the average transaction throughput of the system  assuming there is no interference between the transactions ? b what factors may result in interference between the transactions of different types  leading to the calculated throughput being incorrect ? 24.5 list some benefits and drawbacks of an anticipatory standard compared to a reactionary standard exercises 24.6 find out all performance information your favorite database system provides look for at least the following  what queries are currently executing thesumit67.blogspot.com bibliographical notes 1059 or executed recently  what resources each of them consumed  cpu and i/o   what fraction of page requests resulted in buffer misses  for each query  if available   and what locks have a high degree of contention you may also be able to get information about cpu and i/o utilization from the operating system 24.7 a what are the three broad levels at which a database system can be tuned to improve performance ? b give two examples of how tuning can be done for each of the levels 24.8 when carrying out performance tuning  should you try to tune your hardware  by adding disks or memory  first  or should you try to tune your transactions  by adding indices or materialized views  first ? explain your answer 24.9 suppose that your application has transactions that each access and update a single tuple in a very large relation stored in a b + -tree file organization assume that all internal nodes of the b + -tree are inmemory  but only a very small fraction of the leaf pages can fit in memory explain how to calculate the minimum number of disks required to support a workload of 1000 transactions per second also calculate the required number of disks  using values for disk parameters given in section 10.2 24.10 what is the motivation for splitting a long transaction into a series of small ones ? what problems could arise as a result  and how can these problems be averted ? 24.11 suppose the price of memory falls by half  and the speed of disk access  number of accesses per second  doubles  while all other factors remain the same.what would be the effect of this change on the 5-minute and 1-minute rules ? 24.12 list at least four features of the tpc benchmarks that help make them realistic and dependable measures 24.13 why was the tpc-d benchmark replaced by the tpc-h and tpc-r benchmarks ? 24.14 explain what application characteristics would help you decide which of tpc-c  tpc-h  or tpc-r best models the application bibliographical notes the classic text on queueing theory is kleinrock  1975   an early proposal for a database-system benchmark  the wisconsin benchmark  was made by bitton et al  1983   the tpc-a  -b  and -c benchmarks are described in gray  1991   an online version of all the tpc benchmark descriptions  as well as benchmark results  is available on theworldwideweb at the url thesumit67.blogspot.com 1060 chapter 24 advanced application development www.tpc.org ; the site also contains up-to-date information about new benchmark proposals the oo1 benchmark for oodbs is described in cattell and skeen  1992  ; the oo7 benchmark is described in carey et al  1993   shasha and bonnet  2002  provides detailed coverage of database tuning o ? neil and o ? neil  2000  provides a very good textbook coverage of performance measurement and tuning the 5-minute and 1-minute rules are described in gray and graefe  1997   and more recently extended to consider combinations of main memory  flash  and disk  in graefe  2008   index selection and materialized view selection are addressed by ross et al  1996   chaudhuri and narasayya  1997   agrawal et al  2000   and mistry et al  2001   zilio et al  2004   dageville et al  2004   and agrawal et al  2004  describe tuning support in ibm db2  oracle and microsoft sql server information about odbc  ole-db  ado  and ado.net can be found on the web site www.microsoft.com/data and in a number of books on the subject that can be found through www.amazon.com acm sigmod record  which is published quarterly  has a regular section on standards in databases awealth of information on xml-based standards and tools is available online on the web site www.w3c.org information about rosettanet can be found on the web at www.rosettanet.org business process re-engineering is covered by cook  1996   umar  1997  covers re-engineering and issues in dealing with legacy systems thesumit67.blogspot.com chapter25 spatial and temporal data and mobility for most of the history of databases  the types of data stored in databases were relatively simple  and this was reflected in the rather limited support for data types in earlier versions of sql over time  however  there developed increasing need for handling more complex data types in databases  such as temporal data  spatial data  and multimedia data another major trend has created its own set of issues  the growth of mobile computers  starting with laptop computers and pocket organizers and extending in more recent years to mobile phones with built-in computers and a variety of wearable computers that are increasingly used in commercial applications in this chapter  we study several data types and other database issues dealing with these applications 25.1 motivation before we address each of the topics in detail  we summarize the motivation for  and some important issues in dealing with  each of these types of data ? temporal data most database systems model the current state of the world  for instance  current customers  current students  and courses currently being offered in many applications  it is very important to store and retrieve information about past states historical information can be incorporated manually into a schema design however  the task is greatly simplified by database support for temporal data  which we study in section 25.2 ? spatial data spatial data include geographic data  such as maps and associated information  and computer-aided-design data  such as integratedcircuit designs or building designs applications of spatial data initially stored data as files in a file system  as did early-generation business applications but as the complexity and volume of the data  and the number of users  have grown  ad hoc approaches to storing and retrieving data in a file 1061 thesumit67.blogspot.com 1062 chapter 25 spatial and temporal data and mobility system have proved insufficient for the needs of many applications that use spatial data spatial-data applications require facilities offered by a database system ? in particular  the ability to store and query large amounts of data efficiently some applications may also require other database features  such as atomic updates to parts of the stored data  durability  and concurrency control in section 25.3  we study the extensions needed in traditional database systems to support spatial data ? multimedia data in section 25.4  we study the features required in database systems that store multimedia data such as image  video  and audio data the main distinguishing feature of video and audio data is that the display of the data requires retrieval at a steady  predetermined rate ; hence  such data are called continuous-media data ? mobile databases in section 25.5,we study the database requirements of mobile computing systems  such as laptop and netbook computers and high-end cell phones that are connected to base stations via wireless digital communication networks such computers may need to be able to operate while disconnected from the network  unlike the distributed database systems discussed in chapter 19 they also have limited storage capacity  and thus require special techniques for memory management 25.2 time in databases a database models the state of some aspect of the real world outside itself typically  databases model only one state ? the current state ? of the real world  and do not store information about past states  except perhaps as audit trails when the state of the real world changes  the database gets updated  and information about the old state gets lost however  in many applications  it is important to store and retrieve information about past states for example  a patient database must store information about the medical history of a patient a factory monitoring system may store information about current and past readings of sensors in the factory  for analysis databases that store information about states of the real world across time are called temporal databases when considering the issue of time in database systems  we must distinguish between time as measured by the system and time as observed in the real world the valid time for a fact is the set of time intervals during which the fact is true in the real world the transaction time for a fact is the time interval during which the fact is current within the database system this latter time is based on the transaction serialization order and is generated automatically by the system note that valid-time intervals  being a real-world concept  can not be generated automatically and must be provided to the system a temporal relation is one where each tuple has an associated time when it is true ; the time may be either valid time or transaction time of course  both valid time and transaction time can be stored  in which case the relation is said thesumit67.blogspot.com 25.2 time in databases 1063 id name dept name salary from to 10101 srinivasan comp sci 61000 2007/1/1 2007/12/31 10101 srinivasan comp sci 65000 2008/1/1 2008/12/31 12121 wu finance 82000 2005/1/1 2006/12/31 12121 wu finance 87000 2007/1/1 2007/12/31 12121 wu finance 90000 2008/1/1 2008/12/31 98345 kim elec eng 80000 2005/1/1 2008/12/31 figure 25.1 a temporal instructor relation to be a bitemporal relation figure 25.1 shows an example of a temporal relation to simplify the representation  each tuple has only one time interval associated with it ; thus  a tuple is represented once for every disjoint time interval in which it is true intervals are shown here as a pair of attributes from and to ; an actual implementation would have a structured type  perhaps called interval  that contains both fields note that some of the tuples have a ? * ? in the to time column ; these asterisks indicate that the tuple is true until the value in the to time column is changed ; thus  the tuple is true at the current time although times are shown in textual form  they are stored internally in a more compact form  such as the number of seconds since some fixed time on a fixed date  such as 12  00 a.m  january 1  1900  that can be translated back to the normal textual form 25.2.1 time specification in sql the sql standard defines the types date  time  and timestamp as we saw in chapter 4 the type date contains four digits for the year  1 ? 9999   two digits for the month  1 ? 12   and two digits for the date  1 ? 31   the type time contains two digits for the hour  two digits for the minute  and two digits for the second  plus optional fractional digits the seconds field can go beyond 60  to allow for leap seconds that are added during some years to correct for small variations in the speed of rotation of earth the type timestamp contains the fields of date and time  with six fractional digits for the seconds field since different places in the world have different local times  there is often a need for specifying the time zone alongwith the time the universal coordinated time  utc  is a standard reference point for specifying time  with local times defined as offsets from utc  the standard abbreviation is utc  rather than uct  since it is an abbreviation of ? universal coordinated time ? written in french as universel temps coordonn ? e  sql also supports two types  time with time zone  and timestamp with time zone  which specify the time as a local time plus the offset of the local time from utc for instance  the time could be expressed in terms of u.s eastern standard time  with an offset of -6  00  since u.s eastern standard time is 6 hours behind utc sql supports a type called interval  which allows us to refer to a period of time such as ? 1 day ? or ? 2 days and 5 hours  ? without specifying a particular time when this period starts this notion differs from the notion of interval we used thesumit67.blogspot.com 1064 chapter 25 spatial and temporal data and mobility previously  which refers to an interval of time with specific starting and ending times.1 25.2.2 temporal query languages adatabase relationwithout temporal information is sometimes called a snapshot relation  since it reflects the state in a snapshot of the realworld thus  a snapshot of a temporal relation at a point in time t is the set of tuples in the relation that are true at time t  with the time-interval attributes projected out the snapshot operation on a temporal relation gives the snapshot of the relation at a specified time  or the current time  if the time is not specified   a temporal selection is a selection that involves the time attributes ; a temporal projection is a projection where the tuples in the projection inherit their times from the tuples in the original relation a temporal join is a join  with the time of a tuple in the result being the intersection of the times of the tuples from which it is derived if the times do not intersect  the tuple is removed from the result the predicates precedes  overlaps  andcontains can be applied on intervals ; their meanings should be clear the intersect operation can be applied on two intervals  to give a single  possibly empty  interval however  the union of two intervals may or may not be a single interval functional dependencies must be used with care in a temporal relation  as we saw in section 8.9 although the instructor id may functionally determine the salary at any given point in time  obviously the salary can change over time a temporal functional dependency x  ? y holds on a relation schema r if  for all legal instances r of r  all snapshots of r satisfy the functional dependency x ? y several proposals have been made for extending sql to improve its support of temporal data  but at least until sql  2008  sql has not provided any special support for temporal data beyond the time-related data types and operations 25.3 spatial and geographic data spatial data support in databases is important for efficiently storing  indexing  and querying of data on the basis of spatial locations for example  suppose that we want to store a set of polygons in a database and to query the database to find all polygons that intersect a given polygon we can not use standard index structures  such as b-trees or hash indices  to answer such a query efficiently efficient processing of the above query would require special-purpose index structures  such as r-trees  which we study later  for the task two types of spatial data are particularly important  ? computer-aided-design  cad  data,which includes spatial information about how objects ? such as buildings  cars  or aircraft ? are constructed other im 1many temporal database researchers feel this type should have been called span since it does not specify an exact start or end time  only the time span between the two thesumit67.blogspot.com 25.3 spatial and geographic data 1065 portant examples of computer-aided-design databases are integrated-circuit and electronic-device layouts ? geographic data such as road maps  land-usage maps  topographic elevation maps  political maps showing boundaries  land-ownership maps  and so on geographic information systems are special-purpose databases tailored for storing geographic data support for geographic data has been added to many database systems  such as the ibm db2 spatial extender  the informix spatial datablade  and oracle spatial 25.3.1 representation of geometric information figure 25.2 illustrates how various geometric constructs can be represented in a database  in a normalized fashion.we stress here that geometric information can be represented in several different ways  only some of which we describe a line segment can be represented by the coordinates of its endpoints for example  in a map database  the two coordinates of a pointwould be its latitude and longitude a polyline  also called a linestring  consists of a connected sequence of line segments and can be represented by a list containing the coordinates of the endpoints of the segments  in sequence.we can approximately represent an arbitrary curve by polylines  by partitioning the curve into a sequence of segments this representation is useful for two-dimensional features such as roads ; here  the width of the road is small enough relative to the size of the full map that it can be considered to be a line some systems also support circular arcs as primitives  allowing curves to be represented as sequences of arcs we can represent a polygon by listing its vertices in order  as in figure 25.2.2 the list of vertices specifies the boundary of a polygonal region in an alternative representation  a polygon can be divided into a set of triangles  as shown in figure 25.2 this process is called triangulation  and any polygon can be triangulated the complex polygon can be given an identifier  and each of the triangles into which it is divided carries the identifier of the polygon circles and ellipses can be represented by corresponding types  or can be approximated by polygons list-based representations of polylines or polygons are often convenient for query processing such non-first-normal-form representations are used when supported by the underlying database so that we can use fixed-size tuples  in first normal form  for representing polylines,we can give the polyline or curve an identifier  and can represent each segment as a separate tuple that also carries with it the identifier of the polyline or curve similarly  the triangulated representation of polygons allows a first normal form relational representation of polygons the representation of points and line segments in three-dimensional space is similar to their representation in two-dimensional space  the only difference being that points have an extra z component similarly  the representation of planar figures ? such as triangles  rectangles  and other polygons ? does not change 2some references use the term closed polygon to refer to what we call polygons  and refer to polylines as open polygons thesumit67.blogspot.com 1066 chapter 25 spatial and temporal data and mobility 1 2 1 1 3 3 5 4 2 2 1 3 5 4 2 line segment triangle polygon polygon   x1,y1    x2,y2     x1,y1    x2,y2    x3,y3     x1,y1    x2,y2    x3,y3    x4,y4    x5,y5     x1,y1    x2,y2    x3,y3   id1    x1,y1    x3,y3    x4,y4   id1    x1,y1    x4,y4    x5,y5   id1  object representation figure 25.2 representation of geometric constructs much when we move to three dimensions tetrahedrons and cuboids can be represented in the same way as triangles and rectangles we can represent arbitrary polyhedra by dividing them into tetrahedrons  just as we triangulate polygons we can also represent them by listing their faces  each ofwhich is itself a polygon  along with an indication of which side of the face is inside the polyhedron 25.3.2 design databases computer-aided-design  cad  systems traditionally stored data in memory during editing or other processing  and wrote the data back to a file at the end of a session of editing the drawbacks of such a scheme include the cost  programming complexity  as well as time cost  of transforming data from one form to another  and the need to read in an entire file even if only parts of it are required for large designs  such as the design of a large-scale integrated circuit or the design of an entire airplane  it may be impossible to hold the complete design in thesumit67.blogspot.com 25.3 spatial and geographic data 1067 memory designers of object-oriented databases were motivated in large part by the database requirements of cad systems object-oriented databases represent components of the design as objects  and the connections between the objects indicate how the design is structured the objects stored in a design database are generally geometric objects simple two-dimensional geometric objects include points  lines  triangles  rectangles  and  in general  polygons complex two-dimensional objects can be formed from simple objects by means of union  intersection  and difference operations similarly  complex three-dimensional objects may be formed fromsimpler objects such as spheres  cylinders  and cuboids  by union  intersection  and difference operations  as in figure 25.3 three-dimensional surfaces may also be represented by wireframe models,which essentially model the surface as a set of simpler objects  such as line segments  triangles  and rectangles design databases also store nonspatial information about objects  such as the material from which the objects are constructed we can usually model such information by standard data-modeling techniques we concern ourselves here with only the spatial aspects various spatial operations must be performed on a design for instance  the designer may want to retrieve that part of the design that corresponds to a particular region of interest spatial-index structures  discussed in section 25.3.5  are useful for such tasks spatial-index structures are multidimensional  dealing with two and three-dimensional data  rather than dealing with just the simple one-dimensional ordering provided by the b + -trees spatial-integrity constraints  such as ? two pipes should not be in the same location  ? are important in design databases to prevent interference errors such errors often occur if the design is performedmanually  and are detected onlywhen a prototype is being constructed as a result  these errors can be expensive to fix database support for spatial-integrity constraints helps people to avoid design  a  difference of cylinders  b  union of cylinders figure 25.3 complex three-dimensional objects thesumit67.blogspot.com 1068 chapter 25 spatial and temporal data and mobility errors  thereby keeping the design consistent implementing such integrity checks again depends on the availability of efficient multidimensional index structures 25.3.3 geographic data geographic data are spatial in nature  but differ from design data in certainways maps and satellite images are typical examples of geographic data maps may provide not only location information ? about boundaries  rivers  and roads  for example ? but also much more detailed information associated with locations  such as elevation  soil type  land usage  and annual rainfall 25.3.3.1 applications of geographic data geographic databases have a variety of uses  including online map services ; vehicle-navigation systems ; distribution-network information for public-service utilities such as telephone  electric-power  and water-supply systems ; and landusage information for ecologists and planners web-based road map services form a very widely used application of map data at the simplest level  these systems can be used to generate online road maps of a desired region an important benefit of online maps is that it is easy to scale the maps to the desired size ? that is  to zoom in and out to locate relevant features road map services also store information about roads and services  such as the layout of roads  speed limits on roads  road conditions  connections between roads  and one-way restrictions.with this additional information about roads  the maps can be used for getting directions to go from one place to another and for automatic trip planning users can query online information about services to locate  for example  hotels  gas stations  or restaurants with desired offerings and price ranges in recent years  several web-based map services have defined apis that allow programmers to create customized maps that include data from the map service along with data from other sources such customized maps can be used to display  for example  houses available for sale or rent  or shops and restaurants  in a particular area vehicle-navigation systems are systems that are mounted in automobiles and provide roadmaps and trip-planning services they include a global positioning system  gps  unit  which uses information broadcast from gps satellites to find the current location with an accuracy of tens of meters with such a system  a driver can never3 get lost ? the gps unit finds the location in terms of latitude  longitude  and elevation and the navigation system can query the geographic database to find where and on which road the vehicle is currently located geographic databases for public-utility information have become very important as the network of buried cables and pipes has grown without detailed maps  work carried out by one utility may damage the cables of another utility  resulting in large-scale disruption of service geographic databases  coupledwith accurate location-finding systems  help avoid such problems 3well  hardly ever ! thesumit67.blogspot.com 25.3 spatial and geographic data 1069 25.3.3.2 representation of geographic data geographic data can be categorized into two types  ? raster data such data consist of bit maps or pixel maps  in two or more dimensions.atypical example of a two-dimensional raster image is a satellite image of an area in addition to the actual image  the data includes the location of the image  specified for example by the latitude and longitude of its corners  and the resolution  specified either by the total number of pixels  or  more commonly in the context of geographic data  by the area covered by each pixel raster data is often represented as tiles  each covering a fixed sized area a larger area can be displayed by displaying all the tiles that overlap with the area to allow the display of data at different zoom levels  a separate set of tiles is created for each zoom level once the zoom level is set by the user interface  for example aweb browser   tiles at that zoom level,which overlap the area being displayed  are retrieved and displayed raster data can be three-dimensional ? for example  the temperature at different altitudes at different regions  again measured with the help of a satellite time could form another dimension ? for example  the surface temperature measurements at different points in time ? vector data vector data are constructed from basic geometric objects  such as points  line segments  polylines  triangles  and other polygons in two dimensions  and cylinders  spheres  cuboids  and other polyhedrons in three dimensions in the context of geographic data  points are usually represented by latitude and longitude  and where the height is relevant  additionally by elevation map data are often represented in vector format roads are often represented as polylines geographic features  such as large lakes  or even political features such as states and countries  are represented as complex polygons some features  such as rivers  may be represented either as complex curves or as complex polygons  depending on whether their width is relevant geographic information related to regions  such as annual rainfall  can be represented as an array ? that is  in raster form for space efficiency  the array can be stored in a compressed form in section 25.3.5.2  we study an alternative representation of such arrays by a data structure called a quadtree as another alternative  we can represent region information in vector form  using polygons,where each polygon is a region within which the array value is the same the vector representation is more compact than the raster representation in some applications it is also more accurate for some tasks  such as depicting roads  where dividing the region into pixels  which may be fairly large  leads to a loss of precision in location information however  the vector representation is unsuitable for applications where the data are intrinsically raster based  such as satellite images thesumit67.blogspot.com 1070 chapter 25 spatial and temporal data and mobility topographical information  that is information about the elevation  height  of each point on a surface  can be represented in raster form alternatively it can be represented in vector form by dividing the surface into polygons covering regions of  approximately  equal elevation,with a single elevation value associated with each polygon as another alternative  the surface can be triangulated  that is  divided into triangles   with each triangle represented by the latitude  longitude  and elevation of each of its corners the latter representation  called the triangulated irregular network  tin  representation  is a compact representation which is particularly useful for generating three-dimensional views of an area geographic information systems usually contain both raster and vector data  and can merge the two kinds of data when displaying results to users for example  maps applications usually contain both satellite images and vector data about roads  building and other landmarks a map display usually overlays different kinds of information ; for example  road information can be overlaid on a background satellite image  to create a hybrid display in fact  a map typically consists of multiple layers,which are displayed in bottom-to-top order ; data from higher layers appears on top of data from lower layers it is also interesting to note that even information that is actually stored in vector form may be converted to raster form before it is sent to a user interface such as a web browser one reason is that even web browsers that do not support scripting languages  required to interpret and display vector data  can then display map data ; a second reason may be to prevent end users from extracting and using the vector data map services such as google maps and yahoo ! maps provide apis that allow users to create specialized map displays  containing application specific data overlaid on top of standard map data for example  aweb site may showa map of an area with information about restaurants overlaid on the map the overlays can be constructed dynamically  displaying only restaurants with a specific cuisine  for example  or allowing users to change the zoom level  or pan the display the maps apis for a specific language  typically javascript or flash  are built on top of a web service that provides the underlying map data 25.3.4 spatial queries there are a number of types of queries that involve spatial locations ? nearness queries request objects that lie near a specified location a query to find all restaurants that lie within a given distance of a given point is an example of a nearness query the nearest-neighbor query requests the object that is nearest to a specified point for example  we may want to find the nearest gasoline station note that this query does not have to specify a limit on the distance  and hence we can ask it even if we have no idea how far the nearest gasoline station lies ? region queries deal with spatial regions such a query can ask for objects that lie partially or fully inside a specified region a query to find all retail shops within the geographic boundaries of a given town is an example thesumit67.blogspot.com 25.3 spatial and geographic data 1071 ? queries may also request intersections and unions of regions for example  given region information  such as annual rainfall and population density  a query may request all regions with a low annual rainfall as well as a high population density queries that compute intersections of regions can be thought of as computing the spatial join of two spatial relations ? for example  one representing rainfall and the other representing population density ? with the location playing the role of join attribute in general  given two relations  each containing spatial objects  the spatial join of the two relations generates either pairs of objects that intersect  or the intersection regions of such pairs several join algorithms efficiently compute spatial joins on vector data although nested-loop join and indexed nested-loop join  with spatial indices  can be used  hash joins and sort ? merge joins can not be used on spatial data researchers have proposed join techniques based on coordinated traversal of spatial index structures on the two relations see the bibliographical notes for more information in general  queries on spatial data may have a combination of spatial and nonspatial requirements for instance,wemay want to find the nearest restaurant that has vegetarian selections and that charges less than $ 10 for a meal since spatial data are inherently graphical  we usually query them by using a graphical query language results of such queries are also displayed graphically  rather than in tables the user can invoke various operations on the interface  such as choosing an area to be viewed  for example  by pointing and clicking on suburbs west of manhattan   zooming in and out  choosing what to display on the basis of selection conditions  for example  houses with more than three bedrooms   overlay of multiple maps  for example  houses with more than three bedrooms overlaid on a map showing areas with low crime rates   and so on the graphical interface constitutes the front end extensions of sql have been proposed to permit relational databases to store and retrieve spatial information efficiently  and also to allow queries to mix spatial and nonspatial conditions extensions include allowing abstract data types  such as lines  polygons  and bit maps  and allowing spatial conditions  such as contains or overlaps 25.3.5 indexing of spatial data indices are required for efficient access to spatial data traditional index structures  such as hash indices and b-trees  are not suitable  since they deal only with one-dimensional data  whereas spatial data are typically of two or more dimensions 25.3.5.1 k-d trees to understand how to index spatial data consisting of two or more dimensions  we consider first the indexing of points in one-dimensional data tree structures  such as binary trees and b-trees  operate by successively dividing space into smaller parts for instance  each internal node of a binary tree partitions a onethesumit67 blogspot.com 1072 chapter 25 spatial and temporal data and mobility 3 1 3 2 3 3 2 figure 25.4 division of space by a k-d tree dimensional interval in two points that lie in the left partition go into the left subtree ; points that lie in the right partitiongo into the right subtree ina balanced binary tree  the partition is chosen so that approximately one-half of the points stored in the subtree fall in each partition similarly  each level of a b-tree splits a one-dimensional interval into multiple parts we can use that intuition to create tree structures for two-dimensional space  as well as in higher-dimensional spaces a tree structure called a k-d tree was one of the early structures used for indexing inmultipledimensions each level of a k-d tree partitions the space into two the partitioning is done along one dimension at the node at the top level of the tree  along another dimension in nodes at the next level  and so on  cycling through the dimensions the partitioning proceeds in such a way that  at each node  approximately one-half of the points stored in the subtree fall on one side and one-half fall on the other partitioning stops when a node has less than a given maximum number of points figure 25.4 shows a set of points in two-dimensional space  and a k-d tree representation of the set of points each line corresponds to a node in the tree  and the maximum number of points in a leaf node has been set at 1 each line in the figure  other than the outside box  corresponds to a node in the k-d tree the numbering of the lines in the figure indicates the level of the tree atwhich the corresponding node appears the k-d-b tree extends the k-d tree to allow multiple child nodes for each internal node  just as a b-tree extends a binary tree  to reduce the height of the tree k-d-b trees are better suited for secondary storage than k-d trees 25.3.5.2 quadtrees analternative representation for two-dimensional data is a quadtree.anexample of the division of space by a quadtree appears in figure 25.5 the set of points thesumit67.blogspot.com 25.3 spatial and geographic data 1073 figure 25.5 division of space by a quadtree is the same as that in figure 25.4 each node of a quadtree is associated with a rectangular region of space the top node is associated with the entire target space each nonleaf node in a quadtree divides its region into four equal-sized quadrants  and correspondingly each such node has four child nodes corresponding to the four quadrants leaf nodes have between zero and some fixed maximum number of points correspondingly  if the region corresponding to a node has more than the maximum number of points  child nodes are created for that node in the example in figure 25.5  the maximum number of points in a leaf node is set to 1 this type of quadtree is called a pr quadtree  to indicate it stores points  and that the division of space is divided based on regions  rather than on the actual set of points stored.we can use region quadtrees to store array  raster  information a node in a region quadtree is a leaf node if all the array values in the region that it covers are the same otherwise  it is subdivided further into four children of equal area  and is therefore an internal node each node in the region quadtree corresponds to a subarray of values the subarrays corresponding to leaves either contain just a single array element or have multiple array elements  all of which have the same value indexing of line segments and polygons presents new problems there are extensions of k-d trees and quadtrees for this task however  a line segment or polygon may cross a partitioning line if it does  it has to be split and represented in each of the subtrees in which its pieces occur multiple occurrences of a line segment or polygon can result in inefficiencies in storage  as well as inefficiencies in querying 25.3.5.3 r-trees a storage structure called an r-tree is useful for indexing of objects such as points  line segments  rectangles  and other polygons an r-tree is a balanced thesumit67.blogspot.com 1074 chapter 25 spatial and temporal data and mobility tree structure with the indexed objects stored in leaf nodes  much like a b + -tree however  instead of a range of values  a rectangular bounding box is associated with each tree node the bounding box of a leaf node is the smallest rectangle parallel to the axes that contains all objects stored in the leaf node the bounding box of internal nodes is  similarly  the smallest rectangle parallel to the axes that contains the bounding boxes of its child nodes the bounding box of an object  such as a polygon  is defined  similarly  as the smallest rectangle parallel to the axes that contains the object each internal node stores the bounding boxes of the child nodes along with the pointers to the child nodes each leaf node stores the indexed objects  and may optionally store the bounding boxes of the objects ; the bounding boxes help speed up checks for overlaps of the rectanglewith the indexed objects ? if a query rectangle does not overlap with the bounding box of an object  it can not overlap with the object  either  if the indexed objects are rectangles  there is of course no need to store bounding boxes  since they are identical to the rectangles  figure 25.6 shows an example of a set of rectangles  drawn with a solid line  and the bounding boxes  drawn with a dashed line  of the nodes of an r-tree for the set of rectangles note that the bounding boxes are shown with extra space inside them  to make them stand out pictorially in reality  the boxes would be smaller and fit tightly on the objects that they contain ; that is  each side of a bounding box b would touch at least one of the objects or bounding boxes that are contained in b the r-tree itself is at the right side of figure 25.6 the figure refers to the coordinates of bounding box i as bbi in the figure we shall now see how to implement search  insert  and delete operations on an r-tree bb1 bb2 bb a b c e f h i a b c i e f h 1 2 3 d g d g 3 figure 25.6 an r-tree thesumit67.blogspot.com 25.3 spatial and geographic data 1075 ? search as the figure shows  the bounding boxes associated with sibling nodes may overlap ; in b + -trees  k-d trees  and quadtrees  in contrast  the ranges do not overlap a search for objects containing a point therefore has to follow all child nodes whose associated bounding boxes contain the point ; as a result  multiple paths may have to be searched similarly  a query to find all objects that intersect a given object has to go down every node where the associated rectangle intersects the given object ? insert when we insert an object into an r-tree  we select a leaf node to hold the object ideally we should pick a leaf node that has space to hold a new entry  and whose bounding box contains the bounding box of the object however  such a node may not exist ; even if it did  finding the node may be very expensive  since it is not possible to find it by a single traversal down from the root at each internal node we may find multiple children whose bounding boxes contain the bounding box of the object  and each of these children needs to be explored therefore  as a heuristic  in a traversal fromthe root  if any of the child nodes has a bounding box containing the bounding box of the object  the r-tree algorithm chooses one of themarbitrarily if none of the children satisfy this condition  the algorithm chooses a child node whose bounding box has the maximum overlap with the bounding box of the object for continuing the traversal once the leaf node has been reached  if the node is already full  the algorithm performs node splitting  and propagates splitting upward if required  in a manner very similar to b + -tree insertion just aswith b + -tree insertion  the rtree insertion algorithm ensures that the tree remains balanced additionally  it ensures that the bounding boxes of leaf nodes  as well as internal nodes  remain consistent ; that is  bounding boxes of leaves contain all the bounding boxes of the objects stored at the leaf  while the bounding boxes for internal nodes contain all the bounding boxes of the children nodes the main difference of the insertion procedure from the b + -tree insertion procedure lies in how the node is split in a b + -tree  it is possible to find a value such that half the entries are less than the midpoint and half are greater than the value this property does not generalize beyond one dimension ; that is  for more than one dimension  it is not always possible to split the entries into two sets so that their bounding boxes do not overlap instead  as a heuristic  the set of entries s can be split into two disjoint sets s1 and s2 so that the bounding boxes of s1 and s2 have the minimum total area ; another heuristic would be to split the entries into two sets s1 and s2 in such a way that s1 and s2 have minimum overlap the two nodes resulting from the split would contain the entries in s1 and s2  respectively the cost of finding splits with minimum total area or overlap can itself be large  so cheaper heuristics  such as the quadratic split heuristic  are used  the heuristic gets is name from the fact that it takes time quadratic in the number of entries  the quadratic split heuristic works this way  first  it picks a pair of entries a and b from s such that putting them in the same node would result in a bounding box with the maximum wasted space ; that is  the area of the thesumit67.blogspot.com 1076 chapter 25 spatial and temporal data and mobility minimum bounding box of a and b minus the sum of the areas of a and b is the largest the heuristic places the entries a and b in sets s1 and s2  respectively it then iteratively adds the remaining entries  one entry per iteration  to one of the two sets s1 or s2 at each iteration  for each remaining entry e  let ie,1 denote the increase in the size of the bounding box of s1 if e is added to s1 and let ie,2 denote the corresponding increase for s2 in each iteration  the heuristic chooses one of the entries with the maximum difference of ie,1 and ie,2 and adds it to s1 if ie,1 is less than ie,2  and to s2 otherwise that is  an entry with ? maximum preference ? for one of s1 or s2 is chosen at each iteration the iteration stops when all entries have been assigned  or when one of the sets s1 or s2 has enough entries that all remaining entries have to be added to the other set so the nodes constructed from s1 and s2 both have the required minimum occupancy the heuristic then adds all unassigned entries to the set with fewer entries ? deletion deletion can be performed like a b + -tree deletion  borrowing entries from sibling nodes  or merging sibling nodes if a node becomes underfull an alternative approach redistributes all the entries of underfull nodes to sibling nodes  with the aim of improving the clustering of entries in the r-tree see the bibliographical references for more details on insertion and deletion operations on r-trees  as well as on variants of r-trees  called r * -trees or r + -trees the storage efficiency of r-trees is better than that of k-d trees or quadtrees  since an object is stored only once  and we can ensure easily that each node is at least half full however  querying may be slower  since multiple paths have to be searched spatial joins are simpler with quadtrees than with r-trees  since all quadtrees on a region are partitioned in the same manner however  because of their better storage efficiency  and their similarity to b-trees  r-trees and their variants have proved popular in database systems that support spatial data 25.4 multimedia databases multimedia data  such as images  audio  and video ? an increasingly popular form of data ? are today almost always stored outside the database  in file systems this kind of storage is not a problem when the number of multimedia objects is relatively small  since features provided by databases are usually not important however  database features become importantwhen the number of multimedia objects stored is large issues such as transactional updates  querying facilities  and indexing then become important multimedia objects often have descriptive attributes  such as those indicating when they were created  who created them  and to what category they belong one approach to building a database for such multimedia objects is to use databases for storing the descriptive attributes and for keeping track of the files in which the multimedia objects are stored thesumit67.blogspot.com 25.4 multimedia databases 1077 however  storing multimedia outside the database makes it harder to provide database functionality  such as indexing on the basis of actual multimedia data content it can also lead to inconsistencies  such as a file that is noted in the database  but whose contents are missing  or vice versa it is therefore desirable to store the data themselves in the database several issues must be addressed if multimedia data are to be stored in a database ? the database must support large objects  since multimedia data such as videos can occupy up to a few gigabytes of storage many database systems do not support objects larger than a few gigabytes larger objects could be split into smaller pieces and stored in the database alternatively  the multimedia object may be stored in a file system  but the database may contain a pointer to the object ; the pointer would typically be a file name the sql/med standard  med stands for management of external data  allows external data  such as files  to be treated as if they are part of the database with sql/med  the object would appear to be part of the database  but can be stored externally.we discuss multimedia data formats in section 25.4.1 ? the retrieval of some types of data  such as audio and video  has the requirement that data delivery must proceed at a guaranteed steady rate such data are sometimes called isochronous data  or continuous-media data for example  if audio data are not supplied in time  there will be gaps in the sound if the data are supplied too fast  system buffers may overflow  resulting in loss of data.we discuss continuous-media data in section 25.4.2 ? similarity-based retrieval is needed in many multimedia database applications for example  in a database that stores fingerprint images  a query fingerprint image is provided  and fingerprints in the database that are similar to the query fingerprint must be retrieved index structures such as b +  trees and r-trees can not be used for this purpose ; special index structures need to be created.we discuss similarity-based retrieval in section 25.4.3 25.4.1 multimedia data formats because of the large number of bytes required to represent multimedia data  it is essential that multimedia data be stored and transmitted in compressed form for image data  the most widely used format is jpeg  named after the standards body that created it  the joint picture experts group we can store video data by encoding each frame of video in jpeg format  but such an encoding is wasteful  since successive frames of a video are often nearly the same the moving picture experts group has developed the mpeg series of standards for encoding video and audio data ; these encodings exploit commonalities among a sequence of frames to achieve a greater degree of compression the mpeg-1 standard stores a minute of 30-frame-per-second video and audio in approximately 12.5 megabytes  compared to approximately 75 megabytes for video in only jpeg   however  mpeg-1 encoding introduces some loss of video quality  to a level roughly comparable thesumit67.blogspot.com 1078 chapter 25 spatial and temporal data and mobility to that of vhs videotape the mpeg-2 standard is designed for digital broadcast systems and digital video disks  dvds  ; it introduces only a negligible loss of video quality mpeg-2 compresses 1 minute of video and audio to approximately 17 megabytes mpeg-4 provides techniques for further compression of video,with variable bandwidth to support delivery of video data over networks with a wide range of bandwidths several competing standards are used for audio encoding  including mp3  which stands for mpeg-1 layer 3  realaudio  windows media audio  and other formats high-definition videowith audio is encoded in several variants of mpeg-4 that include mpeg-4 avc and avchd 25.4.2 continuous-media data the most important types of continuous-media data are video and audio data  for example  a database of movies   continuous-media systems are characterized by their real-time information-delivery requirements  ? data must be delivered sufficiently fast that no gaps in the audio or video result ? data must be delivered at a rate that does not cause overflow of system buffers ? synchronization among distinct data streamsmust be maintained this need arises  for example  when the video of a person speaking must show lips moving synchronously with the audio of the person speaking to supply data predictably at the right time to a large number of consumers of the data  the fetching of data from disk must be coordinated carefully usually  data are fetched in periodic cycles in each cycle  say of n seconds  n seconds ? worth of data is fetched for each consumer and stored in memory buffers,while the data fetched in the previous cycle is being sent to the consumers from the memory buffers the cycle period is a compromise  a short period uses less memory but requires more disk-arm movement  which is a waste of resources  while a long period reduces disk-armmovement but increasesmemory requirements and may delay initial delivery of data when a new request arrives  admission control comes into play  that is  the system checks if the request can be satisfied with available resources  in each period  ; if so  it is admitted ; otherwise it is rejected extensive research on delivery of continuous-media data has dealt with such issues as handling arrays of disks and dealing with disk failure see the bibliographical references for details several vendors offer video-on-demand servers current systems are based on file systems  because existing database systems do not provide the real-time response that these applications need the basic architecture of a video-on-demand system comprises  ? video server multimedia data are stored on several disks  usually in a raid configuration   systems containing a large volume of data may use tertiary storage for less frequently accessed data thesumit67.blogspot.com 25.5 mobility and personal databases 1079 ? terminals people view multimedia data through various devices  collectively referred to as terminals examples are personal computers and televisions attached to a small  inexpensive computer called a set-top box ? network transmission of multimedia data from a server to multiple terminals requires a high-capacity network video-on-demand service over cable networks is widely available 25.4.3 similarity-based retrieval in many multimedia applications  data are described only approximately in the database an example is the fingerprint data in section 25.4 other examples are  ? pictorial data two pictures or images that are slightly different as represented in the database may be considered the same by a user for instance  a database may store trademark designs.when a new trademark is to be registered  the system may need first to identify all similar trademarks that were registered previously ? audio data speech-based user interfaces are being developed that allow the user to give a command or identify a data item by speaking the input from the user must then be tested for similarity to those commands or data items stored in the system ? handwritten data handwritten input can be used to identify a handwritten data item or command stored in the database here again  similarity testing is required the notion of similarity is often subjective and user specific however  similarity testing is often more successful than speech or handwriting recognition  because the input can be compared to data already in the system and  thus  the set of choices available to the system is limited several algorithms exist for finding the best matches to a given input by similarity testing many voice-activated systems have been deployed commercially  particularly for phone applications and in-vehicle controls see the bibliographical notes for references 25.5 mobility and personal databases large-scale  commercial databases have traditionally been stored in central computing facilities in distributed database applications  there has usually been strong central database and network administration several technology trends have combined to create applications in which this assumption of central control and administration is not entirely correct  ? the widespread use of laptop  notebook  or netbook computers ? the widespread use of cell phones with the capabilities of a computer thesumit67.blogspot.com 1080 chapter 25 spatial and temporal data and mobility ? the development of a relatively low-cost wireless digital communication infrastructure  based on wireless local-area networks  cellular digital packet networks  and other technologies mobile computing has proved useful in many applications many business travelers use laptop computers so that they can work and access data en route delivery services use mobile computers to assist in package tracking emergencyresponse services use mobile computers at the scene of disasters  medical emergencies  and the like to access information and to enter data pertaining to the situation cell phones are increasingly becoming devices that provide not only phone services  but are also mobile computers allowing email and web access new applications of mobile computers continue to emerge wireless computing creates a situation where machines no longer have fixed locations and network addresses location-dependent queries are an interesting class of queries that are motivated by mobile computers ; in such queries  the location of the user  computer  is a parameter of the query the value of the location parameter is provided by a global positioning system  gps   an example is a traveler ? s information system that provides data on hotels  roadside services  and the like to motorists processing of queries about services that are ahead on the current route must be based on knowledge of the user ? s location  direction of motion  and speed increasingly  navigational aids are being offered as a built-in feature in automobiles energy  battery power  is a scarce resource for most mobile computers this limitation influences many aspects of system design.among the more interesting consequences of the need for energy efficiency is that smallmobile devices spend most of their time sleeping  waking up for a fraction of a second every second or so to check for incoming data and to send outgoing data this behavior has a significant impact on protocols used to communicate with mobile devices the use of scheduled data broadcasts to reduce the need for mobile systems to transmit queries is another way to reduce energy requirements increasing amounts of data may reside on machines administered by users  rather than by database administrators furthermore  these machines may  at times  be disconnected fromthe network inmany cases  there is a conflict between the user ? s need to continue to work while disconnected and the need for global data consistency a user is likely to use more than one mobile device such users need to be able to view their data in its most up-to-date version regardless of which device is being used at a given time often  this capability is supported by some variant of cloud computing  which we discussed in section 19.9 in sections 25.5.1 through 25.5.4  we discuss techniques in use and under development to deal with the problems of mobility and personal computing 25.5.1 a model of mobile computing the mobile-computing environment consists of mobile computers  referred to as mobile hosts  and a wired network of computers mobile hosts communicate thesumit67.blogspot.com 25.5 mobility and personal databases 1081 with the wired network via computers referred to as mobile support stations each mobile support station manages those mobile hosts within its cell ? that is  the geographical area that it covers mobile hosts may move between cells  thus necessitating a handoff of control from one mobile support station to another sincemobile hostsmay  at times  be powered down  a host may leave one cell and rematerialize later at some distant cell therefore  moves between cells are not necessarily between adjacent cells.within a small area  such as a building  mobile hosts may be connected by a wireless local-area network  lan  that provides lower-cost connectivity thanwould awide-area cellular network  and that reduces the overhead of handoffs it is possible for mobile hosts to communicate directly without the intervention of a mobile support station however  such communication can occur only between nearby hosts such direct forms of communication often use bluetooth  a short-range digital radio standard that allows wireless connectivitywithin a 10 meter range at high speed  up to 721 kilobits per second   initially conceived as a replacement for cables  bluetooth ? s greatest benefit is in easy ad hoc connection of mobile computers  pdas  mobile phones  and so-called intelligent appliances wireless local-area network systems based on the 801.11  a/b/g/n  standards are very widely used today  and systems based on the 802.16  wi-max  standard are being deployed the network infrastructure formobile computing consists in large part of two technologies  wireless local-area networks and packet-based cellular telephony networks early cellular systems used analog technology and were designed for voice communication second-generation digital systems retained the focus on voice applications third-generation  3g  and so-called 2.5g systems use packetbased networking and are more suited to data applications in these networks  voice is just one of many applications  albeit an economically important one   fourth-generation  4g  technologies include wi-max as well as several competitors bluetooth  wireless lans  and 2.5g and 3g cellular networks make it possible for a wide variety of devices to communicate at low cost while such communication itself does not fit the domain of a usual database application  the accounting  monitoring  and management data pertaining to this communication generate huge databases the immediacy of wireless communication generates a need for real-time access to many of these databases this need for timeliness adds another dimension to the constraints on the system ? a matter we shall discuss further in section 26.4 the size and power limitations of many mobile computers have led to alternative memory hierarchies instead of  or in addition to  disk storage  flash memory  which we discussed in section 10.1  may be included if the mobile host includes a hard disk  the disk may be allowed to spin down when it is not in use  to save energy the same considerations of size and energy limit the type and size of the display used in a mobile device designers of mobile devices often create special-purpose user interfaces to work within these constraints however  the need to presentweb-based data has necessitated the creation of presentation standards wireless application protocol  wap  is a standard for wireless internet thesumit67.blogspot.com 1082 chapter 25 spatial and temporal data and mobility access wap-based browsers access special web pages that use wireless markup language  wml   an xml-based language designed for the constraints of mobile and wirelessweb browsing 25.5.2 routing and query processing the route between a pair of hosts may change over time if one of the two hosts is mobile this simple fact has a dramatic effect at the network level  since locationbased network addresses are no longer constants within the system mobility also directly affects database query processing as we saw in chapter 19  we must consider the communication costs when we choose a distributed query-processing strategy mobility results in dynamically changing communication costs  thus complicating the optimization process furthermore  there are competing notions of cost to consider  ? user time is a highly valuable commodity in many business applications ? connection time is the unit bywhich monetary charges are assigned in some cellular systems ? number of bytes  or packets  transferred is the unit by which charges are computed in some digital cellular systems ? time-of-day-based charges vary  depending on whether communication occurs during peak or off-peak periods ? energy is limited often  battery power is a scarce resource whose use must be optimized a basic principle of radio communication is that it requires less energy to receive than to transmit radio signals thus  transmission and reception of data impose different power demands on the mobile host 25.5.3 broadcast data it is often desirable for frequently requested data to be broadcast in a continuous cycle by mobile support stations  rather than transmitted to mobile hosts on demand a typical application of such broadcast data is stock-market price information there are two reasons for using broadcast data first  the mobile host avoids the energy cost for transmitting data requests second  the broadcast data can be received by a large number of mobile hosts at once  at no extra cost thus  the available transmission bandwidth is utilized more effectively a mobile host can then receive data as they are transmitted  rather than consuming energy by transmitting a request the mobile host may have local nonvolatile storage available to cache the broadcast data for possible later use given a query  the mobile hostmayoptimize energy costs by determiningwhether it can process that querywith only cached data if the cached data are insufficient  there are two options  wait for the data to be broadcast  or transmit a request for data to make this decision  the mobile host must know when the relevant data will be broadcast thesumit67.blogspot.com 25.5 mobility and personal databases 1083 broadcast data may be transmitted according to a fixed schedule or a changeable schedule in the former case  the mobile host uses the known fixed schedule to determine when the relevant data will be transmitted in the latter case  the broadcast schedule must itself be broadcast at a well-known radio frequency and at well-known time intervals in effect  the broadcast medium can be modeled as a disk with a high latency requests for data can be thought of as being serviced when the requested data are broadcast the transmission schedules behave like indices on the disk the bibliographical notes list recent research papers in the area of broadcast data management 25.5.4 disconnectivity and consistency since wireless communication may be paid for on the basis of connection time  there is an incentive for certain mobile hosts to be disconnected for substantial periods mobile computers without wireless connectivity are disconnected most of the timewhen they are being used  except periodically when they are connected to their host computers  either physically or through a computer network during these periods of disconnection  the mobile host may remain in operation the user of the mobile host may issue queries and updates on data that reside or are cached locally this situation creates several problems  in particular  ? recoverability  updates entered on a disconnected machine may be lost if the mobile host experiences a catastrophic failure since the mobile host represents a single point of failure  stable storage can not be simulated well ? consistency  locally cached data may become out-of-date  but the mobile host can not discover this situation until it is reconnected likewise  updates occurring in themobile host can not be propagated until reconnection occurs we explored the consistency problem in chapter 19  where we discussed network partitioning  and we elaborate on it here in wired distributed systems  partitioning is considered to be a failure mode ; in mobile computing  partitioning via disconnection is part of the normal mode of operation it is therefore necessary to allow data access to proceed despite partitioning  even at the risk of some loss of consistency for data updated by only the mobile host  it is a simple matter to propagate the updates when the mobile host reconnects however  if the mobile host caches read-only copies of data that may be updated by other computers  the cached data may become inconsistent when the mobile host is connected  it can be sent invalidation reports that inform it of out-of-date cache entries however  when the mobile host is disconnected  it may miss an invalidation report a simple solution to this problem is to invalidate the entire cache on reconnection  but such an extreme solution is highly costly several caching schemes are cited in the bibliographical notes if updates can occur at both the mobile host and elsewhere  detecting conflicting updates is more difficult version-numbering-based schemes allow updates thesumit67.blogspot.com 1084 chapter 25 spatial and temporal data and mobility of shared files from disconnected hosts these schemes do not guarantee that the updates will be consistent rather  they guarantee that  if two hosts independently update the same version of a document  the clash will be detected eventually  when the hosts exchange information either directly or through a common host the version-vector scheme detects inconsistencies when copies of a document are independently updated this scheme allows copies of a document to be stored at multiple hosts although we use the term document  the scheme can be applied to any other data items  such as tuples of a relation the basic idea is for each host i to store  with its copy of each document d  a version vector ? that is  a set of version numbers  vd,i  j    with one entry for each other host j on which the document could potentially be updated when a host i updates a document d  it increments the version number vd,i  i  by one whenever two hosts i and j connect with each other  they exchange updated documents  so that both obtain new versions of the documents however  before exchanging documents  the hosts have to discover whether the copies are consistent  1 if the version vectors are the same on both hosts ? that is  for each k  vd,i  k  = vd  j  k  ? then the copies of document d are identical 2 if  for each k  vd,i  k  = vd  j  k  and the version vectors are not identical  then the copy of document d at host i is older than the one at host j that is  the copy of document d at host j was obtained by one or more modifications of the copy of the document at host i host i replaces its copy of d  aswell as its copy of the version vector for d  with the copies from host j 3 if there are a pair of hosts k and m such that vd,i  k  < vd  j  k  and vd,i  m  > vd  j  m   then the copies are inconsistent ; that is  the copy of d at i contains updates performed by host k that have not been propagated to host j  and  similarly  the copy of d at j contains updates performed by host m that have not been propagated to host i then  the copies of d are inconsistent  since two or more updates have been performed on d independently manual intervention may be required to merge the updates the version-vector scheme was initially designed to deal with failures in distributed file systems the scheme gained importance because mobile computers often store copies of files that are also present on server systems  in effect constituting a distributed file system that is often disconnected another application of the scheme is in groupware systems  where hosts are connected periodically  rather than continuously  and must exchange updated documents the version-vector scheme also has applications in replicated databases  where it can be applied to individual tuples for example  if a calendar or address book is maintained on a mobile device as well as on a host  inserts  deletes and updates can happen either on the mobile device or on the host by applying the version-vector scheme to individual calendar entries or contacts  it is easy to handle situations where a particular entry has been updated on the mobile device thesumit67.blogspot.com 25.6 summary 1085 while a different one has been updated on the host ; such a situation would not be considered a conflict however  if the same entry is updated independently at both places  a conflict would be detected by the version-vector scheme the version-vector scheme  however  fails to address the most difficult and most important issue arising from updates to shared data ? the reconciliation of inconsistent copies of data many applications can perform reconciliation automatically by executing in each computer those operations that had performed updates on remote computers during the period of disconnection this solution works if update operations commute ? that is  they generate the same result  regardless of the order in which they are executed alternative techniques may be available in certain applications ; in the worst case  however  it must be left to the users to resolve the inconsistencies dealing with such inconsistency automatically  and assisting users in resolving inconsistencies that can not be handled automatically  remains an area of research anotherweakness is that the version-vector scheme requires substantial communication between a reconnecting mobile host and that host ? s mobile support station consistency checks can be delayed until the data are needed  although this delay may increase the overall inconsistency of the database the potential for disconnection and the cost of wireless communication limit the practicality of transaction-processing techniques discussed in chapter 19 for distributed systems often  it is preferable to let users prepare transactions on mobile hosts  but to require that  instead of executing the transactions locally  they submit transactions to a server for execution transactions that span more than one computer and that include a mobile host face long-term blocking during transaction commit  unless disconnectivity is rare or predictable 25.6 summary ? time plays an important role in database systems databases are models of the real world whereas most databases model the state of the real world at a point in time  at the current time   temporal databases model the states of the real world across time ? facts in temporal relations have associated times when they are valid  which can be represented as a union of intervals temporal query languages simplify modeling of time  as well as time-related queries ? spatial databases are finding increasing use today to store computer-aideddesign data as well as geographic data ? design data are stored primarily as vector data ; geographic data consist of a combination of vector and raster data spatial-integrity constraints are important for design data ? vector data can be encoded as first-normal-form data  or they can be stored using non-first-normal-form structures  such as lists special-purpose index structures are particularly important for accessing spatial data  and for processing spatial queries thesumit67.blogspot.com 1086 chapter 25 spatial and temporal data and mobility ? r-trees are a multidimensional extension of b-trees ; with variants such as r + -trees and r * -trees  they have proved popular in spatial databases index structures that partition space in a regular fashion  such as quadtrees  help in processing spatial join queries ? multimedia databases are growing in importance issues such as similaritybased retrieval and delivery of data at guaranteed rates are topics of current research ? mobile computing systems have become common  leading to interest in database systems that can run on such systems query processing in such systems may involve lookups on server databases the query cost modelmust include the cost of communication  including monetary cost and battery-power cost  which is relatively high for mobile systems ? broadcast is much cheaper per recipient than is point-to-point communication  and broadcast of data such as stock-market data helps mobile systems to pick up data inexpensively ? disconnected operation  use of broadcast data  and caching of data are three important issues being addressed in mobile computing review terms ? temporal data ? valid time ? transaction time ? temporal relation ? bitemporal relation ? universal coordinated time  utc  ? snapshot relation ? temporal query languages ? temporal selection ? temporal projection ? temporal join ? spatial and geographic data ? computer-aided-design  cad  data ? geographic data ? geographic information systems ? triangulation ? design databases ? geographic data ? raster data ? vector data ? global positioning system  gps  ? spatial queries ? nearness queries ? nearest-neighbor queries ? region queries ? spatial join ? indexing of spatial data ? k-d trees ? k-d-b trees ? quadtrees ? pr quadtree ? region quadtree ? r-trees ? bounding box thesumit67.blogspot.com practice exercises 1087 ? quadratic split ? multimedia databases ? isochronous data ? continuous-media data ? similarity-based retrieval ? multimedia data formats ? video servers ? mobile computing ? mobile hosts ? mobile support stations ? cell ? handoff ? location-dependent queries ? broadcast data ? consistency ? invalidation reports ? version-vector scheme practice exercises 25.1 what are the two types of time  and how are they different ? why does it make sense to have both types of time associated with a tuple ? 25.2 suppose you have a relation containing the x  y coordinates and names of restaurants suppose also that the only queries that will be asked are of the following form  the query specifies a point  and asks if there is a restaurant exactly at that point.which type of index would be preferable  r-tree or b-tree ? why ? 25.3 suppose you have a spatial database that supports region queries  with circular regions  but not nearest-neighbor queries describe an algorithm to find the nearest neighbor by making use of multiple region queries 25.4 suppose you want to store line segments in an r-tree if a line segment is not parallel to the axes  the bounding box for it can be large  containing a large empty area ? describe the effect on performance of having large bounding boxes on queries that ask for line segments intersecting a given region ? briefly describe a technique to improve performance for such queries and give an example of its benefit hint  you can divide segments into smaller pieces 25.5 give a recursive procedure to efficiently compute the spatial join of two relations with r-tree indices  hint  use bounding boxes to check if leaf entries under a pair of internal nodes may intersect  25.6 describe how the ideas behind the raid organization  section 10.3  can be used in a broadcast-data environment  where there may occasionally be noise that prevents reception of part of the data being transmitted 25.7 define a model of repeatedly broadcast data in which the broadcast medium is modeled as a virtual disk describe how access time and datathesumit67 blogspot.com 1088 chapter 25 spatial and temporal data and mobility transfer rate for this virtual disk differ from the corresponding values for a typical hard disk 25.8 consider a database of documents in which all documents are kept in a central database copies of some documents are kept on mobile computers suppose that mobile computer a updates a copy of document 1while it is disconnected  and  at the same time  mobile computer b updates a copy of document 2while it is disconnected show how the version-vector scheme can ensure proper updating of the central database and mobile computers when a mobile computer reconnects exercises 25.9 will functional dependencies be preserved if a relation is converted to a temporal relation by adding a time attribute ? howis the problem handled in a temporal database ? 25.10 consider two-dimensional vector data where the data items do not overlap is it possible to convert such vector data to raster data ? if so  what are the drawbacks of storing raster data obtained by such conversion  instead of the original vector data ? 25.11 study the support for spatial data offered by the database system that you use  and implement the following  a a schema to represent the geographic location of restaurants along with features such as the cuisine served at the restaurant and the level of expensiveness b aquery to find moderately priced restaurants that serve indian food and are within 5 miles of your house  assume any location for your house   c a query to find for each restaurant the distance from the nearest restaurant serving the same cuisine and with the same level of expensiveness 25.12 what problems can occur in a continuous-media system if data are delivered either too slowly or too fast ? 25.13 list three main features of mobile computing over wireless networks that are distinct from traditional distributed systems 25.14 list three factors that need to be considered in query optimization for mobile computing that are not considered in traditional query optimizers 25.15 give an example to show that the version-vector scheme does not ensure serializability  hint  use the example from practice exercise 25.8  with the assumption that documents 1 and 2 are available on both mobile thesumit67.blogspot.com bibliographical notes 1089 computers a and b  and take into account the possibility that a document may be read without being updated  bibliographical notes stam and snodgrass  1988  and soo  1991  provide surveys on temporal data management jensen et al  1994  presents a glossary of temporal-database concepts  aimed at unifying the terminology tansel et al  1993  is a collection of articles on different aspects of temporal databases chomicki  1995  presents techniques for managing temporal integrity constraints heywood et al  2002  provides textbook coverage of geographical information systems samet  1995b  provides an overview of the large amount of work on spatial index structures samet  1990  and samet  2006  provides a textbook coverage of spatial data structures an early description of the quad tree is provided by finkel and bentley  1974   samet  1990  and samet  1995b  describe numerous variants of quad trees bentley  1975  describes the k-d tree  and robinson  1981  describes the k-d-b tree the r-tree was originally presented in guttman  1984   extensions of the r-tree are presented by sellis et al  1987   which describes the r + tree  and beckmann et al  1990   which describes the r * tree brinkhoff et al  1993  discusses an implementation of spatial joins using rtrees lo and ravishankar  1996  and patel anddewitt  1996  present partitioningbased methods for computation of spatial joins samet and aref  1995  provides an overview of spatial data models  spatial operations  and the integration of spatial and nonspatial data revesz  2002  provides textbook coverage of the area of constraint databases ; temporal intervals and spatial regions can be thought of as special cases of constraints samet  1995a  describes research issues in multimedia databases indexing of multimedia data is discussed in faloutsos and lin  1995   dashti et al  2003  provides a textbook description of streaming media server design  including extensive coverage of data organization on disk subsystems video servers are discussed in anderson et al  1992   rangan et al  1992   ozden et al  1994   freedman anddewitt  1995   and ozden et al  1996b   fault tolerance is discussed in berson et al  1995  and ozden et al  1996a   information management in systems that include mobile computers is studied in alonso and korth  1993  and imielinski and badrinath  1994   imielinski and korth  1996  presents an introduction to mobile computing and a collection of research papers on the subject the version-vector scheme for detecting inconsistency in distributed file systems is described by popek et al  1981  and parker et al  1983   thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter26 advanced transaction processing in chapters 14  15  and 16  we introduced the concept of a transaction  a program unit that accesses ? and possibly updates ? various data items  and whose execution ensures the preservation of the acid properties we discussed in those chapters a variety of techniques for ensuring the acid properties in an environment where failure can occur  and where the transactions may run concurrently in this chapter  we go beyond the basic schemes discussed previously  and cover advancedtransaction-processing concepts  including transaction-processing monitors  transactional workflows  and transaction processing in the context of electronic commerce.we also cover main-memory databases  real-time databases  long-duration transactions  and nested transactions 26.1 transaction-processing monitors transaction-processing monitors  tp monitors  are systems thatwere developed in the 1970s and 1980s  initially in response to a need to support a large number of remote terminals  such as airline-reservation terminals  from a single computer the term tp monitor initially stood for teleprocessing monitor tp monitors have since evolved to provide the core support for distributed transaction processing  and the term tp monitor has acquired its current meaning the cics tp monitor from ibm was one of the earliest tp monitors  and has been very widely used other tp monitors include oracle tuxedo and microsoft transaction server web application server architectures  including servlets  which we studied earlier in section 9.3  support many of the features of tp monitors and are sometimes referred to as ? tp lite ? web application servers are in widespread use  and have supplanted traditional tp monitors for many applications however  the concepts underlying them  which we study in this section  are essentially the same 1091 thesumit67.blogspot.com 1092 chapter 26 advanced transaction processing 26.1.1 tp-monitor architectures large-scale transaction-processing systems are built around a client ? server architecture one way of building such systems is to have a server process for each client ; the server performs authentication  and then executes actions requested by the client this process-per-client model is illustrated in figure 26.1a this model presents several problems with respect to memory utilization and processing speed  ? per-process memory requirements are high even if memory for program code is shared by all processes  each process consumes memory for local data and open file descriptors  as well as for operating-system overhead  such as page tables to support virtual memory ? the operating system divides up available cpu time among processes by switching among them ; this technique is called multitasking each context switch between one process and the next has considerable cpu overhead ; even on today ? s fast systems  a context switch can take hundreds of microseconds the above problems can be avoided by having a single-server process to which all remote clients connect ; this model is called the single-server model  remote clients server files remote clients  a  process-per-client model  b  single-server model server files remote clients router servers files remote clients  c  many-server  single-router model  d  many-server  many-router model routers monitor servers files figure 26.1 tp-monitor architectures thesumit67.blogspot.com 26.1 transaction-processing monitors 1093 illustrated in figure 26.1b remote clients send requests to the server process  which then executes those requests this model is also used in client ? server environments  where clients send requests to a single-server process the server process handles tasks  such as user authentication  that would normally be handled by the operating system to avoid blocking other clients when processing a long request for one client  the server process is multithreaded  the server process has a thread of control for each client  and  in effect  implements its own low-overhead multitasking it executes code on behalf of one client for a while  then saves the internal context and switches to the code for another client unlike the overhead of full multitasking  the cost of switching between threads is low  typically only a few microseconds   systems based on the single-server model  such as the original version of the ibm cics tp monitor and file servers such as novel ? s netware  successfully provided high transaction rateswith limited resources.however  they had problems  especially when multiple applications accessed the same database  ? since all the applications run as a single process  there is no protection among them a bug in one application can affect all the other applications as well it would be best to run each application as a separate process ? such systems are not suited for parallel or distributed databases  since a server process can not execute on multiple computers at once  however  concurrent threads within a process can be supported in a shared-memory multiprocessor system  this is a serious drawback in large organizations  where parallel processing is critical for handling large workloads  and distributed data are becoming increasingly common one way to solve these problems is to run multiple application-server processes that access a common database  and to let the clients communicate with the application through a single communication process that routes requests this model is called the many-server  single-router model  illustrated in figure 26.1c this model supports independent server processes for multiple applications ; further  each application can have a pool of server processes  any one of which can handle a client session the request can  for example  be routed to themost lightly loaded server in a pool.as before  each server process can itself be multithreaded  so that it can handle multiple clients concurrently as a further generalization  the application servers can run on different sites of a parallel or distributed database  and the communication process can handle the coordination among the processes the above architecture is also widely used in web servers a web server has a main process that receives http requests  and then assigns the task of handling each request to a separate process  chosen from among a pool of processes   each of the processes is itself multithreaded  so that it can handle multiple requests the use of safe programming languages  such as java  c #  or visual basic  allowsweb application servers to protect threads from errors in other threads in contrast  with a language like c or c + +  errors such as memory allocation errors in one thread can cause other threads to fail thesumit67.blogspot.com 1094 chapter 26 advanced transaction processing input queue authorization network output queue lock manager recovery manager log manager application servers database and resource managers figure 26.2 tp-monitor components a more general architecture has multiple processes  rather than just one  to communicate with clients the client communication processes interact with one or more router processes  which route their requests to the appropriate server later-generation tp monitors therefore have a different architecture  called the many-server  many-router model  illustrated in figure 26.1d.acontroller process starts up the other processes and supervises their functioning very high performance web-server systems also adopt such an architecture the router processes are often network routers that direct traffic addressed to the same internet address to different server computers  depending on where the traffic comes from what looks like a single server with a single address to the outside world may be a collection of servers the detailed structure of a tp monitor appears in figure 26.2 a tp monitor does more than simply pass messages to application servers when messages arrive  they may have to be queued ; thus  there is a queue manager for incoming messages the queue may be a durable queue  whose entries survive system failures using a durable queue helps ensure that once received and stored in the queue  the messages will be processed eventually  regardless of system failures authorization and application-server management  for example  server start-up and routing of messages to servers  are further functions of a tp monitor tp monitors often provide logging  recovery  and concurrency-control facilities  allowing application servers to implement the acid transaction properties directly if required finally  tp monitors also provide support for persistent messaging recall that persistent messaging  section 19.4.3  provides a guarantee that the message will be delivered if  and only if  the transaction commits in addition to these facilities  many tp monitors also provided presentation facilities to create menus/forms interfaces for dumb clients such as terminals ; thesumit67.blogspot.com 26.1 transaction-processing monitors 1095 these facilities are no longer important since dumb clients are no longer widely used 26.1.2 application coordination using tp monitors applications today often have to interact with multiple databases they may also have to interact with legacy systems  such as special-purpose data-storage systems built directly on file systems finally  they may have to communicate with users or other applications at remote sites hence  they also have to interact with communication subsystems it is important to be able to coordinate data accesses  and to implement acid properties for transactions across such systems modern tp monitors provide support for the construction and administration of such large applications  built up from multiple subsystems such as databases  legacy systems  and communication systems a tp monitor treats each subsystem as a resource manager that provides transactional access to some set of resources the interface between the tp monitor and the resourcemanager is defined by a set of transaction primitives  such as begin transaction  commit transaction  abort transaction  and prepare to commit transaction  for two-phase commit   of course  the resource managermust also provide other services  such as supplying data  to the application the resource-manager interface is defined by the x/open distributed transaction processing standard many database systems support the x/open standards  and can act as resource managers tp monitors ? as well as other products  such as sql systems  that support the x/open standards ? can connect to the resource managers in addition  services provided by a tp monitor  such as persistent messaging and durable queues  act as resource managers supporting transactions the tp monitor can act as coordinator of two-phase commit for transactions that access these services as well as database systems for example  when a queued update transaction is executed  an output message is delivered  and the request transaction is removed from the request queue two-phase commit between the database and the resource managers for the durable queue and persistent messaging helps ensure that  regardless of failures  either all these actions occur or none occurs we can also use tp monitors to administer complex client ? server systems consisting of multiple servers and a large number of clients the tp monitor coordinates activities such as system checkpoints and shutdowns it provides security and authentication of clients it administers server pools by adding servers or removing servers without interruption of the the database system finally  it controls the scope of failures if a server fails  the tp monitor can detect this failure  abort the transactions in progress  and restart the transactions if a node fails  the tp monitor can migrate transactions to servers at other nodes  again backing out incomplete transactions when failed nodes restart  the tp monitor can govern the recovery of the node ? s resource managers tp monitors can be used to hide database failures in replicated systems ; remote backup systems  section 16.9  are an example of replicated systems transaction requests are sent to the tp monitor  which relays the messages to one of the thesumit67.blogspot.com 1096 chapter 26 advanced transaction processing database replicas  the primary site  in case of remote backup systems   if one site fails  the tp monitor can transparently route messages to a backup site  masking the failure of the first site in client ? server systems  clients often interact with servers via a remoteprocedure call  rpc  mechanism  where a client invokes a procedure call  which is actually executed at the server  with the results sent back to the client as far as the client code that invokes the rpc is concerned  the call looks like a local procedure-call invocation tp monitor systems provide a transactional rpc interface to their services in such an interface  the rpc mechanism provides calls that canbe used to enclose a seriesof rpc calls within a transaction thus  updates performed by an rpc are carried out within the scope of the transaction  and can be rolled back if there is any failure 26.2 transactional workflows a workflow is an activity in which multiple tasks are executed in a coordinated way by different processing entities a task defines some work to be done and can be specified in a number of ways  including a textual description in a file or electronic-mail message  a form  a message  or a computer program the processing entity that performs the tasks may be a person or a software system  for example  a mailer  an application program  or a database-management system   figure 26.3 shows a fewexamples ofworkflows.asimple example is that of an electronic-mail system the delivery of a single mail message may involve several mail systems that receive and forward the mail message  until the message reaches its destination  where it is stored other terms used in the database and related literature to refer to workflows include task flow and multisystem applications workflow tasks are also sometimes called steps in general  workflows may involve one or more humans for instance  consider the processing of a loan the relevant workflow appears in figure 26.4 the person who wants a loan fills out a form  which is then checked by a loan officer an employee who processes loan applications verifies the data in the form  using sources such as credit-reference bureaus when all the required information has been collected  the loan officer may decide to approve the loan ; that decision may workflow typical typical processing application task entity electronic-mail routing electronic-mail message mailers loan processing form processing form processing humans  application so  ware purchase-order processing humans  application so  ware  dbmss figure 26.3 examples of workflows thesumit67.blogspot.com 26.2 transactional workflows 1097 customer loan officer verification superior officer loan disbursement loan application reject accept figure 26.4 workflow in loan processing then have to be approved by one or more superior officers  after which the loan can be made each human here performs a task ; in a bank that has not automated the task of loan processing  the coordination of the tasks is typically carried out by passing of the loan application  with attached notes and other information  from one employee to the next other examples of workflows include processing of expense vouchers  of purchase orders  and of credit-card transactions today  all the information related to aworkflow is more than likely to be stored in a digital form on one or more computers  and  with the growth of networking  information can be easily transferred from one computer to another hence  it is feasible for organizations to automate their workflows for example  to automate the tasks involved in loan processing  we can store the loan application and associated information in a database the workflow itself then involves handing of responsibility fromone human to the next  and possibly even to programs that can automatically fetch the required information humans can coordinate their activities by means such as electronic mail workflows are becoming increasingly important for multiple reasons within as well as between organizations many organizations today have multiple software systems that need to work together for example  when an employee joins an organization  information about the employee may have to be provided to the payroll system  to the library system  to authentication systems that allow the user to log on to computers  to a system that manages cafeteria accounts  an so on updates  such as when the employee changes status or leaves the organization  also have to be propagated to all the systems organizations are increasingly automating their services ; for example  a supplier may provide an automated system for customers to place orders several tasks may need to be carried out when an order is placed  including reserving production time to create the ordered product and delivery services to deliver the product we have to address two activities  in general  to automate a workflow the first is workflow specification  detailing the tasks that must be carried out and defining the execution requirements the second problem is workflow execution  which we must do while providing the safeguards of traditional database thesumit67.blogspot.com 1098 chapter 26 advanced transaction processing systems related to computation correctness and data integrity and durability for example  it is not acceptable for a loan application or a voucher to be lost  or to be processed more than once  because of a system crash the idea behind transactional workflows is to use and extend the concepts of transactions to the context of workflows both activities are complicated by the fact that many organizations use several independently managed information-processing systems that  in most cases  were developed separately to automate different functions workflow activities may require interactions among several such systems  each performing a task  as well as interactions with humans a number of workflow systems have been developed in recent years here  we study properties of workflow systems at a relatively abstract level  without going into the details of any particular system 26.2.1 workflow specification internal aspects of a task do not need to be modeled for the purpose of specification and management of a workflow in an abstract view of a task  a task may use parameters stored in its input variables  may retrieve and update data in the local system  may store its results in its output variables  and may be queried about its execution state at any time during the execution  the workflow state consists of the collection of states of the workflow ? s constituent tasks  and the states  values  of all variables in the workflow specification the coordination of tasks can be specified either statically or dynamically a static specification defines the tasks ? and dependencies among them ? before the execution of the workflow begins for instance  the tasks in an expensevoucher workflow may consist of the approvals of the voucher by a secretary  a manager  and an accountant  in that order  and finally the delivery of a check the dependencies among the tasks may be simple ? each task has to be completed before the next begins a generalization of this strategy is to have a precondition for execution of each task in the workflow  so that all possible tasks in a workflow and their dependencies are known in advance  but only those tasks whose preconditions are satisfied are executed the preconditions can be defined through dependencies such as the following  ? execution states of other tasks ? for example  ? task ti can not start until task tj has ended  ? or ? task ti must abort if task tj has committed ? ? output values of other tasks ? for example  ? task ti can start if task tj returns a value greater than 25  ? or ? the manager-approval task can start if the secretary-approval task returns a value of ok ? ? external variables modified by external events ? for example  ? task ti can not be started before 9 a.m  ? or ? task ti must be started within 24 hours of the completion of task tj  ? thesumit67.blogspot.com 26.2 transactional workflows 1099 we can combine the dependencies by the regular logical connectors  or  and  not  to form complex scheduling preconditions an example of dynamic scheduling of tasks is an electronic-mail routing system the next task to be scheduled for a given mail message depends on what the destination address of the message is  and on which intermediate routers are functioning 26.2.2 failure-atomicity requirements of a workflow theworkflow designermayspecify the failure-atomicity requirements of aworkflow according to the semantics of the workflow the traditional notion of failure atomicity would require that a failure of any task result in the failure of the workflow however  a workflow can  in many cases  survive the failure of one of its tasks ? for example  by executing a functionally equivalent task at another site therefore  we should allow the designer to define failure-atomicity requirements of a workflow the system must guarantee that every execution of a workflow will terminate in a state that satisfies the failure-atomicity requirements defined by the designer we call those states acceptable termination states of a workflow all other execution states of a workflow constitute a set of nonacceptable termination states  in which the failure-atomicity requirements may be violated an acceptable termination state can be designated as committed or aborted a committed acceptable termination state is an execution state in which the objectives of a workflow have been achieved in contrast  an aborted acceptable termination state is a valid termination state in which a workflow has failed to achieve its objectives if an aborted acceptable termination state has been reached  all undesirable effects of the partial execution of the workflow must be undone in accordance with that workflow ? s failure-atomicity requirements a workflow must reach an acceptable termination state even in the presence of system failures thus  if a workflow is in a nonacceptable termination state at the time of failure  during system recovery it must be brought to an acceptable termination state  whether aborted or committed   for example  in the loan-processing workflow  in the final state  either the loan applicant is told that a loan can not be made or the loan is disbursed in case of failures such as a long failure of the verification system  the loan application could be returned to the loan applicant with a suitable explanation ; this outcome would constitute an aborted acceptable termination a committed acceptable termination would be either the acceptance or the rejection of the loan in general  a task can commit and release its resources before the workflow reaches a termination state however  if the multitask transaction later aborts  its failure atomicity may require that we undo the effects of already completed tasks  for example  committed subtransactions  by executing compensating tasks  as subtransactions   the semantics of compensation requires that a compensating transaction eventually complete its execution successfully  possibly after a number of resubmissions in an expense-voucher-processing workflow  for example  a departmentbudget balance may be reduced on the basis of an initial approval of a voucher thesumit67.blogspot.com 1100 chapter 26 advanced transaction processing by the manager if the voucher is later rejected  whether because of failure or for other reasons  the budget may have to be restored by a compensating transaction 26.2.3 execution of workflows the execution of the tasks may be controlled by a human coordinator or by a software system called a workflow-management system a workflow-management system consists of a scheduler  task agents  and a mechanism to query the state of theworkflow system.atask agent controls the execution of a task by a processing entity a scheduler is a program that processes workflows by submitting various tasks for execution  monitoring various events  and evaluating conditions related to intertask dependencies a scheduler may submit a task for execution  to a task agent   or may request that a previously submitted task be aborted in the case of multidatabase transactions  the tasks are subtransactions  and the processing entities are local database-management systems in accordance with the workflow specifications  the scheduler enforces the scheduling dependencies and is responsible for ensuring that tasks reach acceptable termination states there are three architectural approaches to the development of a workflowmanagement system.acentralized architecture has a single scheduler that schedules the tasks for all concurrently executingworkflows the partially distributed architecture has one scheduler instantiated for each workflow when the issues of concurrent execution can be separated from the scheduling function  the latter option is a natural choice a fully distributed architecture has no scheduler  but the task agents coordinate their execution by communicating with one another to satisfy task dependencies and other workflow execution requirements the simplest workflow-execution systems follow the fully distributed approach just described and are based on messaging messaging may be implemented by persistent messaging mechanisms  to provide guaranteed delivery some implementations use email for messaging ; such implementations provide many of the features of persistent messaging  but generally do not guarantee atomicity of message delivery and transaction commit each site has a task agent that executes tasks received through messages execution may also involve presenting messages to humans  who have then to carry out some action when a task is completed at a site  and needs to be processed at another site  the task agent dispatches a message to the next site the message contains all relevant information about the task to be performed such message-based workflow systems are particularly useful in networks that may be disconnected for part of the time the centralized approach is used in workflow systems where the data are stored in a central database the scheduler notifies various agents  such as humans or computer programs  that a task has to be carried out  and keeps track of task completion it is easier to keep track of the state of a workflow with a centralized approach than it is with a fully distributed approach the scheduler must guarantee that a workflow will terminate in one of the specified acceptable termination states ideally  before attempting to execute a workflow  the scheduler should examine that workflow to check whether the thesumit67.blogspot.com 26.2 transactional workflows 1101 workflow may terminate in a nonacceptable state if the scheduler can not guarantee that a workflow will terminate in an acceptable state  it should reject such specifications without attempting to execute the workflow as an example  let us consider a workflow consisting of two tasks represented by subtransactions s1 and s2  with the failure-atomicity requirements indicating that either both or neither of the subtransactions should be committed if s1 and s2 do not provide prepared-to-commit states  for a two-phase commit   and further do not have compensating transactions  then it is possible to reach a state where one subtransaction is committed and the other aborted  and there is no way to bring both to the same state therefore  such a workflow specification is unsafe  and should be rejected safety checks such as the one just described may be impossible or impractical to implement in the scheduler ; it then becomes the responsibility of the person designing the workflow specification to ensure that the workflows are safe 26.2.4 recovery of a workflow the objective of workflow recovery is to enforce the failure atomicity of the workflows the recovery procedures must make sure that  if a failure occurs in any of the workflow-processing components  including the scheduler   the workflow will eventually reach an acceptable termination state  whether aborted or committed   for example  the scheduler could continue processing after failure and recovery  as though nothing happened  thus providing forward recoverability otherwise  the scheduler could abort the whole workflow  that is  reach one of the global abort states   in either case  some subtransactions may need to be committed or even submitted for execution  for example  compensating subtransactions   we assume that the processing entities involved in the workflow have their own recovery systems and handle their local failures to recover the executionenvironment context  the failure-recovery routines need to restore the state information of the scheduler at the time of failure  including the information about the execution states of each task therefore  the appropriate status information must be logged on stable storage we also need to consider the contents of the message queues when one agent hands off a task to another  the handoff should be carried out exactly once  if the handoff happens twice a task may get executed twice ; if the handoff does not occur  the task may get lost persistent messaging  section 19.4.3  provides exactly the features to ensure positive  single handoff 26.2.5 workflow-management systems workflows are often hand coded as part of application systems for instance  enterprise resource planning  erp  systems  which help coordinate activities across an entire enterprise  have numerous workflows built into them the goal of workflow-management systems is to simplify the construction of workflows and make them more reliable  by permitting them to be specified in a high-level manner and executed in accordance with the specification there are a thesumit67.blogspot.com 1102 chapter 26 advanced transaction processing large number of commercial workflow-management systems ; some are generalpurpose workflow-management systems  while others are specific to particular workflows  such as order processing or bug/failure reporting systems in today ? s world of interconnected organizations  it is not sufficient to manage workflows only within an organization.workflows that cross organizational boundaries are becoming increasingly common for instance  consider an order placed by an organization and communicated to another organization that fulfills the order in each organization there may be a workflow associated with the order  and it is important that the workflows be able to interoperate  in order to minimize human intervention the term business process management is used to refer to the management of workflows related to business processes today  applications are increasingly making their functionality available as services that can be invoked by other applications  often using aweb service architecture a system architecture based on invoking services provided by multiple applications is referred to as a service oriented architecture soa such services are the base layer on top of which workflow management is implemented today the process logic that controls the workflow by invoking the services is referred to as orchestration business process management systems based on the soa architecture include microsoft ? s biztalk server  ibmswebsphere business integration server foundation  and beas weblogic process edition  among others theweb servicesbusiness process execution language  ws-bpel  is anxml based standard for specifying web services and business processes  workflows  based on theweb services  which can be executed by a business process management system the business process modeling notation  bpmn   is a standard for graphical modeling of business processes in a workflow  and xml process definition language  xpdl  is an xml based representation of business process definitions  based on bpmn diagrams 26.3 e-commerce e-commerce refers to the process of carrying out various activities related to commerce  through electronic means  primarily through the internet the types of activities include  ? presale activities  needed to inform the potential buyer about the product or service being sold ? the sale process  which includes negotiations on price and quality of service  and other contractual matters ? the marketplace  when there are multiple sellers and buyers for a product  a marketplace  such as a stock exchange  helps in negotiating the price to be paid for the product auctions are used when there is a single seller and multiple buyers  and reverse auctions are used when there is a single buyer and multiple sellers thesumit67.blogspot.com 26.3 e-commerce 1103 ? payment for the sale ? activities related to delivery of the product or service some products and services can be delivered over the internet ; for others the internet is used only for providing shipping information and for tracking shipments of products ? customer support and postsale service databases are used extensively to support these activities for some of the activities  the use of databases is straightforward  but there are interesting application development issues for the other activities 26.3.1 e-catalogs any e-commerce site provides users with a catalog of the products and services that the site supplies the services provided by an e-catalog may vary considerably at the minimum  an e-catalog must provide browsing and search facilities to help customers find the product for which they are looking to help with browsing  products should be organized into an intuitive hierarchy  so a few clicks on hyperlinks can lead customers to the products in which they are interested keywords provided by the customer  for example  ? digital camera ? or ? computer ?  should speed up the process of finding required products e-catalogs should also provide a means for customers to easily compare alternatives from which to choose among competing products e-catalogs can be customized for the customer for instance  a retailer may have an agreement with a large company to supply some products at a discount an employee of the company  viewing the catalog to purchase products for the company  should see prices with the negotiated discount  instead of the regular prices because of legal restrictions on sales of some types of items  customers who are underage  or from certain states or countries  should not be shown items that can not legally be sold to them catalogs can also be personalized to individual users  on the basis of past buying history for instance  frequent customers may be offered special discounts on some items supporting such customization requires customer information as well as special pricing/discount information and sales restriction information to be stored in a database there are also challenges in supporting very high transaction rates  which are often tackled by caching of query results or generatedweb pages 26.3.2 marketplaces when there are multiple sellers or multiple buyers  or both  for a product  a marketplace helps in negotiating the price to be paid for the product there are several different types of marketplaces  ? in a reverse auction system a buyer states requirements  and sellers bid for supplying the item the supplier quoting the lowest price wins in a closed bidding system  the bids are not made public  whereas in an open bidding system the bids are made public thesumit67.blogspot.com 1104 chapter 26 advanced transaction processing ? in an auction there are multiple buyers and a single seller for simplicity  assume that there is only one instance of each item being sold buyers bid for the items being sold  and the highest bidder for an item gets to buy the item at the bid price when there are multiple copies of an item  things become more complicated  suppose there are four items  and one bidder may want three copies for $ 10 each  while another wants two copies for $ 13 each it is not possible to satisfy both bids if the items will be of no value if they are not sold  for instance  airline seats  which must be sold before the plane leaves   the seller simply picks a set of bids that maximizes the income otherwise the decision is more complicated ? in an exchange  such as a stock exchange  there are multiple sellers and multiple buyers buyers can specify the maximum price they are willing to pay  while sellers specify the minimum price they want there is usually a market maker who matches buy and sell bids  deciding on the price for each trade  for instance  at the price of the sell bid   there are other more complex types of marketplaces among the database issues in handling marketplaces are these  ? bidders need to be authenticated before they are allowed to bid ? bids  buy or sell  need to be recorded securely in a database bids need to be communicated quickly to other people involved in the marketplace  such as all the buyers or all the sellers   who may be numerous ? delays in broadcasting bids can lead to financial losses to some participants ? the volumes of trades may be extremely large at times of stock market volatility  or toward the end of auctions thus  very high performance databases with large degrees of parallelism are used for such systems 26.3.3 order settlement after items have been selected  perhaps through an electronic catalog  and the price determined  perhaps by an electronic marketplace   the order has to be settled settlement involves payment for goods and the delivery of the goods a simple but unsecure way of paying electronically is to send a credit-card number there are twomajor problems first  credit-card fraud is possible.when a buyer pays for physical goods  companies can ensure that the address for delivery matches the cardholder ? s address  so no one else can receive the goods  but for goods delivered electronically no such check is possible second  the seller has to be trusted to bill only for the agreed-on item and to not pass on the card number to unauthorized people who may misuse it several protocols are available for secure payments that avoid both the problems listed above in addition  they provide for better privacy  whereby the seller may not be given any unnecessary details about the buyer  and the credit-card thesumit67.blogspot.com 26.4 main-memory databases 1105 company is not provided any unnecessary information about the items purchased all information transmitted must be encrypted so that anyone intercepting the data on the network can not find out the contents public-/private-key encryption is widely used for this task the protocols must also prevent person-in-the-middle attacks  where someone can impersonate the bank or credit-card company  or even the seller  or buyer  and steal secret information impersonation can be perpetrated by passing off a fake key as someone else ? s public key  the bank ? s or credit-card company ? s  or the merchant ? s or the buyer ? s   impersonation is prevented by a system of digital certificates  whereby public keys are signed by a certification agency  whose public key is well known  or which in turn has its public key certified by another certification agency and so on up to a key that is well known   from the well-known public key  the system can authenticate the other keys by checking the certificates in reverse sequence digital certificates were described earlier  in section 9.8.3.2 several novel payment systems were developed in the early days of theweb one of these was a secure payment protocol called the secure electronic transaction  set  protocol the protocol requires several rounds of communication between the buyer  seller  and the bank  in order to guarantee safety of the transaction there were also systems that provide for greater anonymity  similar to that provided by physical cash the digicash payment system was one such system.when a payment is made in such a system  it is not possible to identify the purchaser in contrast  identifying purchasers is very easy with credit cards  and even in the case of set  it is possible to identify the purchaser with the cooperation of the credit-card company or bank however  none of these systems was successful commercially  for both technical and non-technical reasons today  many banks provide secure payment gateways which allow a purchaser to pay online at the banks web site  without exposing credit card or bank account information to the onlinemerchant when making a purchase at an online merchant  the purchaser ? s web browser is redirected to the gateway to complete the payment by providing credit card or bank account information  after which the purchaser is again redirected back to the merchant ? s site to complete the purchase unlike the set or digicash protocols  there is no software running on the purchasers machine  except a web browser ; as a result this approach has found wide success where the earlier approaches failed an alternative approach which is used by the paypal system is for both the purchaser and the merchant to have an account on a common platform  and the money transfer happens entirely within the common platform the purchaser first loads her account with money using a credit card  and can then transfer money to the merchants account this approach has been very successful with small merchants  since it does not require either the purchaser or the merchant to run any software 26.4 main-memory databases to allow a high rate of transaction processing  hundreds or thousands of transactions per second   we must use high-performance hardware  and must exploit thesumit67.blogspot.com 1106 chapter 26 advanced transaction processing parallelism these techniques alone  however  are insufficient to obtain very low response times  since disk i/o remains a bottleneck ? about 10 milliseconds are required for each i/o  and this number has not decreased at a rate comparable to the increase in processor speeds disk i/o is often the bottleneck for reads  as well as for transaction commits the long disk latency increases not only the time to access a data item  but also limits the number of accesses per second.1 we can make a database system less disk bound by increasing the size of the database buffer.advances in main-memory technology let us construct large main memories at relatively low cost today  commercial 64-bit systems can support main memories of tens of gigabytes oracle timesten is a currently available main-memory database additional information on main-memory databases is given in the references in the bibliographical notes for some applications  such as real-time control  it is necessary to store data in main memory to meet performance requirements the memory size required for most such systems is not exceptionally large  although there are at least a few applications that require multiple gigabytes of data to be memory resident since memory sizes have been growing at a very fast rate  an increasing number of applications can be expected to have data that fit into main memory large main memories allow faster processing of transactions  since data are memory resident however  there are still disk-related limitations  ? log records must be written to stable storage before a transaction is committed the improved performancemade possible by a largemainmemorymay result in the logging process becoming a bottleneck we can reduce commit time by creating a stable log buffer in main memory  using nonvolatile ram  implemented  for example  by battery-backed-up memory   the overhead imposed by logging can also be reduced by the group-commit technique discussed later in this section throughput  number of transactions per second  is still limited by the data-transfer rate of the log disk ? buffer blocks marked as modified by committed transactions still have to be written so that the amount of log that has to be replayed at recovery time is reduced if the update rate is extremely high  the disk data-transfer rate may become a bottleneck ? if the system crashes  all of main memory is lost on recovery  the system has an empty database buffer  and data items must be input from disk when they are accessed therefore  even after recovery is complete  it takes some time before the database is fully loaded in main memory and high-speed processing of transactions can resume on the other hand  a main-memory database provides opportunities for optimizations  1write latency for flash depends on whether an erase operation must be done first thesumit67.blogspot.com 26.4 main-memory databases 1107 ? since memory is costlier than disk space  internal data structures in mainmemory databases have to be designed to reduce space requirements however  data structures can have pointers crossing multiple pages  unlike those in disk databases  where the cost of the i/os to traverse multiple pageswould be excessively high for example  tree structures in main-memory databases can be relatively deep  unlike b + -trees  but should minimize space requirements however  the speed difference between cache memory and main-memory  and the fact that data is transferred between main-memory and cache in units of a cache-line  typically about 64 bytes   results in a situation where the relationship between cache and main-memory is not dissimilar to the relationship between main-memory and disk  although with smaller speed differences  .as a result  b + -trees with small nodes that fit in a cache line have been found quite useful even in main-memory databases ? there is no need to pin buffer pages in memory before data are accessed  since buffer pages will never be replaced ? query-processing techniques should be designed to minimize space overhead  so that main-memory limits are not exceeded while a query is being evaluated ; that situation would result in paging to swap area  and would slow down query processing ? once the disk i/o bottleneck is removed  operations such as locking and latching may become bottlenecks such bottlenecks must be eliminated by improvements in the implementation of these operations ? recovery algorithms can be optimized  since pages rarely need to be written out to make space for other pages the process of committing a transaction t requires these records to be written to stable storage  ? all log records associated with t that have not been output to stable storage ? the < t commit > log record these output operations frequently require the output of blocks that are only partially filled to ensure that nearly full blocks are output  we use the groupcommit technique instead of attempting to commit t when t completes  the system waits until several transactions have completed  or a certain period of time has passed since a transaction completed execution it then commits the group of transactions that are waiting  together blocks written to the log on stable storage would contain records of several transactions by careful choice of group size and maximum waiting time  the system can ensure that blocks are full when they are written to stable storage without making transactions wait excessively this technique results  on average  in fewer output operations per committed transaction thesumit67.blogspot.com 1108 chapter 26 advanced transaction processing although group commit reduces the overhead imposed by logging  it results in a slight delay in commit of transactions that perform updates the delay can be made quite small  say  10 milliseconds  ,which is acceptable formanyapplications these delays can be eliminated if disks or disk controllers support nonvolatile ram buffers for write operations transactions can commit as soon as the write is performed on the nonvolatile ram buffer in this case  there is no need for group commit note that group commit is useful even in databases with disk-resident data  not just for main-memory databases if flash storage is used instead of magnetic disk for storing log records  the commit delay is significantly reduced however  group commit can still be useful since it minimizes the number of pages written ; this translates to performance benefits in flash storage  since pages can not be overwritten  and the erase operation is expensive  flash storage systems remap logical pages to a pre-erased physical page  avoiding delay at the time a page is written  but the erase operation must be performed eventually as part of garbage collection of old versions of pages  26.5 real-time transaction systems the integrity constraints that we have considered thus far pertain to the values stored in the database in certain applications  the constraints include deadlines by which a task must be completed examples of such applications include plant management  traffic control  and scheduling when deadlines are included  correctness of an execution is no longer solely an issue of database consistency rather  we are concerned with how many deadlines are missed  and by how much time they are missed deadlines are characterized as follows  ? hard deadline serious problems  such as system crash  may occur if a task is not completed by its deadline ? firm deadline the task has zero value if it is completed after the deadline ? soft deadlines the task has diminishing value if it is completed after the deadline  with the value approaching zero as the degree of lateness increases systems with deadlines are called real-time systems transaction management in real-time systems must take deadlines into account if the concurrency-control protocol determines that a transaction ti must wait  it may cause ti to miss the deadline in such cases  it may be preferable to pre-empt the transaction holding the lock  and to allow ti to proceed pre-emption must be used with care  however  because the time lost by the pre-empted transaction  due to rollback and restart  may cause the pre-empted transaction to miss its deadline unfortunately  it is difficult to determinewhether rollback orwaiting is preferable in a given situation a major difficulty in supporting real-time constraints arises fromthe variance in transaction execution time in the best case  all data accesses reference data in thesumit67.blogspot.com 26.6 long-duration transactions 1109 the database buffer in the worst case  each access causes a buffer page to be written to disk  preceded by the requisite log records   followed by the reading from disk of the page containing the data to be accessed because the two or more disk accesses required in the worst case take several orders of magnitude more time than the main-memory references required in the best case  transaction execution time can be estimated only very poorly if data are resident on disk hence  main-memory databases are often used if real-time constraints have to be met however  even if data are resident in main memory  variances in execution time arise fromlockwaits  transaction aborts  and so on.researchers have devoted considerable effort to concurrency control for real-time databases they have extended locking protocols to provide higher priority for transactions with early deadlines they have found that optimistic concurrency protocols performwell in real-time databases ; that is  these protocols result in fewer missed deadlines than even the extended locking protocols the bibliographical notes provide references to research in the area of real-time databases in real-time systems  deadlines  rather than absolute speed  are the most important issue designing a real-time system involves ensuring that there is enough processing power to meet deadlines without requiring excessive hardware resources achieving this objective  despite the variance in execution time resulting from transaction management  remains a challenging problem 26.6 long-duration transactions the transaction concept developed initially in the context of data-processing applications  inwhich most transactions are noninteractive and of short duration although the techniques presented here and earlier in chapters 14  15  and 16 work well in those applications  serious problems arise when this concept is applied to database systems that involve human interaction such transactions have these key properties  ? long duration once a human interacts with an active transaction  that transaction becomes a long-duration transaction from the perspective of the computer  since human response time is slow relative to computer speed furthermore  in design applications  the human activity may involve hours  days  or an even longer period thus  transactions may be of long duration in human terms  as well as in machine terms ? exposure of uncommitted data data generated and displayed to a user by a long-duration transaction are uncommitted  since the transaction may abort thus  users ? and  as a result  other transactions ? may be forced to read uncommitted data if several users are cooperating on a project  user transactions may need to exchange data prior to transaction commit ? subtasks an interactive transaction may consist of a set of subtasks initiated by the user the user may wish to abort a subtaskwithout necessarily causing the entire transaction to abort thesumit67.blogspot.com 1110 chapter 26 advanced transaction processing ? recoverability it is unacceptable to abort a long-duration interactive transaction because of a system crash the active transaction must be recovered to a state that existed shortly before the crash so that relatively little human work is lost ? performance good performance in an interactive transaction system is defined as fast response time this definition is in contrast to that in a noninteractive system  in which high throughput  number of transactions per second  is the goal systems with high throughput make efficient use of system resources however  in the case of interactive transactions  the most costly resource is the user if the efficiency and satisfaction of the user is to be optimized  response time should be fast  from a human perspective   in those caseswhere a task takes a long time  response time should be predictable  that is  the variance in response times should be low   so that users can manage their time well in sections 26.6.1 through 26.6.5  we shall see why these five properties are incompatible with the techniques presented thus far and shall discuss how those techniques can be modified to accommodate long-duration interactive transactions 26.6.1 nonserializable executions the properties that we discussed make it impractical to enforce the requirement used in earlier chapters that only serializable schedules be permitted each of the concurrency-control protocols of chapter 15 has adverse effects on long-duration transactions  ? two-phase locking.when a lock can not be granted  the transaction requesting the lock is forced to wait for the data item in question to be unlocked the duration of this wait is proportional to the duration of the transaction holding the lock if the data item is locked by a short-duration transaction  we expect that the waiting time will be short  except in case of deadlock or extraordinary system load   however  if the data item is locked by a longduration transaction  the wait will be of long duration long waiting times lead to both longer response time and an increased chance of deadlock ? graph-based protocols graph-based protocols allow for locks to be released earlier than under the two-phase locking protocols  and they prevent deadlock however  they impose an ordering on the data items transactions must lock data items in a manner consistent with this ordering as a result  a transaction may have to lock more data than it needs furthermore  a transaction must hold a lock until there is no chance that the lock will be needed again thus  long-duration lock waits are likely to occur ? timestamp-based protocols timestamp protocols never require a transaction towait however  they do require transactions to abort under certain circumstances if a long-duration transaction is aborted  a substantial amount of thesumit67.blogspot.com 26.6 long-duration transactions 1111 work is lost for noninteractive transactions  this lost work is a performance issue for interactive transactions  the issue is also one of user satisfaction it is highly undesirable for a user to find that several hours ? worth of work have been undone ? validation protocols like timestamp-based protocols  validation protocols enforce serializability by means of transaction abort thus  it appears that the enforcement of serializability results in long-duration waits  in abort of long-duration transactions  or in both there are theoretical results  cited in the bibliographical notes  that substantiate this conclusion further difficulties with the enforcement of serializability arise when we consider recovery issues.we previously discussed the problem of cascading rollback  in which the abort of a transaction may lead to the abort of other transactions this phenomenon is undesirable  particularly for long-duration transactions if locking is used  exclusive locks must be held until the end of the transaction  if cascading rollback is to be avoided this holding of exclusive locks  however  increases the length of transaction waiting time thus  it appears that the enforcement of transaction atomicity must either lead to an increased probability of long-duration waits or create a possibility of cascading rollback snapshot isolation  described in section 15.7  can provide a partial solution to these issues  as can the optimistic concurrency control without read validation protocol described in section 15.9.3 the latter protocol was in fact designed specifically to deal with long duration transactions that involve user interaction although it does not guarantee serializability  optimistic concurrency control without read validation is quite widely used however  when transactions are of long duration  conflicting updates are more likely  resulting in additional waits or aborts these considerations are the basis for the alternative concepts of correctness of concurrent executions and transaction recovery that we consider in the remainder of this section 26.6.2 concurrency control the fundamental goal of database concurrency control is to ensure that concurrent execution of transactions does not result in a loss of database consistency the concept of serializability can be used to achieve this goal  since all serializable schedules preserve consistency of the database however  not all schedules that preserve consistency of the database are serializable for an example  consider again a bank database consisting of two accounts a and b  with the consistency requirement that the sum a + b be preserved although the schedule of figure 26.5 is not conflict serializable  it nevertheless preserves the sum of a + b it also illustrates two important points about the concept of correctness without serializability ? correctness depends on the specific consistency constraints for the database ? correctness depends on the properties of operations performed by each transaction thesumit67.blogspot.com 1112 chapter 26 advanced transaction processing t1 t2 read  a  a  = a  50 write  a  read  b  b  = b  10 write  b  read  b  b  = b + 50 write  b  read  a  a  = a + 10 write  a  figure 26.5 a non-conflict-serializable schedule in general it is not possible to perform an automatic analysis of low-level operations by transactions and check their effect on database consistency constraints however  there are simpler techniques one is to use the database consistency constraints as the basis for a split of the database into subdatabases onwhich concurrency can be managed separately another is to treat some operations besides read and write as fundamental low-level operations and to extend concurrency control to deal with them the bibliographical notes reference other techniques for ensuring consistency without requiring serializability many of these techniques exploit variants of multiversion concurrency control  see section 15.6   for older data-processing applications that need only one version  multiversion protocols impose a high space overhead to store the extra versions since many of the new database applications require the maintenance of versions of data  concurrency-control techniques that exploit multiple versions are practical 26.6.3 nested and multilevel transactions a long-duration transaction can be viewed as a collection of related subtasks or subtransactions by structuring a transaction as a set of subtransactions  we are able to enhance parallelism  since it may be possible to run several subtransactions in parallel furthermore  it is possible to deal with failure of a subtransaction  due to abort  system crash  and so on  without having to roll back the entire long-duration transaction a nested or multilevel transaction t consists of a set t =  t1  t2      tn  of subtransactions and a partial order p on t a subtransaction ti in t may abort without forcing t to abort instead  t may either restart ti or simply choose not to run ti if ti commits  this action does not make ti permanent  unlike the situation in chapter 16   instead  ti commits to t  and may still abort  or require compensation ? see section 26.6.4  if t aborts an execution of t must not violate the partial thesumit67.blogspot.com 26.6 long-duration transactions 1113 order p that is  if an edge ti ? tj appears in the precedence graph  then tj ? ti must not be in the transitive closure of p nesting may be several levels deep  representing a subdivision of a transaction into subtasks  subsubtasks  and so on at the lowest level of nesting  we have the standard database operations read and write that we have used previously if a subtransaction of t is permitted to release locks on completion  t is called a multilevel transaction.when a multilevel transaction represents a longduration activity  the transaction is sometimes referred to as a saga alternatively  if locks held by a subtransaction ti of t are automatically assigned to t on completion of ti  t is called a nested transaction although the main practical value of multilevel transactions arises in complex  long-duration transactions  we shall use the simple example of figure 26.5 to show how nesting can create higher-level operations that may enhance concurrency we rewrite transaction t1  using subtransactions t1,1 and t1,2  which perform increment or decrement operations  ? t1 consists of  ? t1,1  which subtracts 50 from a ? t1,2  which adds 50 to b similarly  we rewrite transaction t2  using subtransactions t2,1 and t2,2  which also perform increment or decrement operations  ? t2 consists of  ? t2,1  which subtracts 10 from b ? t2,2  which adds 10 to a no ordering is specified on t1,1  t1,2  t2,1  and t2,2 any execution of these subtransactions will generate a correct result the schedule of figure 26.5 corresponds to the schedule < t1,1  t2,1  t1,2  t2,2 >  26.6.4 compensating transactions to reduce the frequency of long-duration waiting  we arrange for uncommitted updates to be exposed to other concurrently executing transactions indeed  multilevel transactions may allow this exposure however  the exposure of uncommitted data creates the potential for cascading rollbacks the concept of compensating transactions helps us to deal with this problem let transaction t be divided into several subtransactions t1  t2      tn after a subtransaction ti commits  it releases its locks now  if the outer-level transaction t has to be aborted  the effect of its subtransactions must be undone suppose that subtransactions t1      tk have committed  and that tk + 1 was executing when the decision to abort is made we can undo the effects of tk + 1 by aborting that thesumit67.blogspot.com 1114 chapter 26 advanced transaction processing subtransaction however  it is not possible to abort subtransactions t1      tk  since they have committed already instead,we execute a newsubtransaction cti  called a compensating transaction  to undo the effect of a subtransaction ti  each subtransaction ti is required to have a compensating transaction cti  the compensating transactions must be executed in the inverse order ctk      ct1 here are several examples of compensation  ? consider the schedule of figure 26.5  which we have shown to be correct  although not conflict serializable each subtransaction releases its locks once it completes suppose that t2 fails just prior to termination  after t2,2 has released its locks.we then run a compensating transaction for t2,2 that subtracts 10 from a and a compensating transaction for t2,1 that adds 10 to b ? consider a database insert by transaction ti that  as a side effect  causes a b + -tree index to be updated the insert operation may havemodified several nodes of the b + -tree index other transactions may have read these nodes in accessing data other than the record inserted by ti  as mentioned in section 16.7  we can undo the insertion by deleting the record inserted by ti the result is a correct  consistent b + -tree  but is not necessarily one with exactly the same structure as the one we had before ti started thus  deletion is a compensating action for insertion ? consider a long-duration transaction ti representing a travel reservation transaction t has three subtransactions  ti,1  which makes airline reservations ; ti,2  which reserves rental cars ; and ti,3  which reserves a hotel room suppose that the hotel cancels the reservation instead of undoing all of ti  we compensate for the failure of ti,3 by deleting the old hotel reservation and making a new one if the system crashes in the middle of executing an outer-level transaction  its subtransactions must be rolled back when it recovers the techniques described in section 16.7 can be used for this purpose compensation for the failure of a transaction requires that the semantics of the failed transaction be used for certain operations  such as incrementation or insertion into a b + -tree  the corresponding compensation is easily defined for more complex transactions  the application programmers may have to define the correct form of compensation at the time that the transaction is coded for complex interactive transactions  it may be necessary for the system to interact with the user to determine the proper form of compensation 26.6.5 implementation issues the transaction concepts discussed in this section create serious difficulties for implementation.we present a few of them here  and discuss how we can address these problems long-duration transactions must survive system crashes.we can ensure that they will by performing a redo on committed subtransactions  and by performthesumit67 blogspot.com 26.7 summary 1115 ing either an undo or compensation for any short-duration subtransactions that were active at the time of the crash however  these actions solve only part of the problem in typical database systems  such internal system data as lock tables and transaction timestamps are kept in volatile storage for a long-duration transaction to be resumed after a crash  these data must be restored therefore  it is necessary to log not only changes to the database  but also changes to internal system data pertaining to long-duration transactions logging of updates is made more complex when certain types of data items exist in the database a data item may be a cad design  text of a document  or another form of composite design such data items are physically large thus  storing both the old and newvalues of the data itemin a log record is undesirable there are two approaches to reducing the overhead of ensuring the recoverability of large data items  ? operation logging only the operation performed on the data item and the data-item name are stored in the log operation logging is also called logical logging for each operation  an inverse operation must exist we perform undo using the inverse operation and redo using the operation itself recovery through operation logging is more difficult  since redo and undo are not idempotent further  using logical logging for an operation that updates multiple pages is greatly complicated by the fact that some  but not all  of the updated pages may have been written to the disk  so it is hard to apply either the redo or the undo of the operation on the disk image during recovery using physical redo logging and logical undo logging  as described in section 16.7  provides the concurrency benefits of logical logging while avoiding the above pitfalls ? logging and shadow paging logging is used for modifications to small data items  but large data items are often made recoverable via a shadowing  or copy-on-write  technique when we use shadowing  it is possible to reduce the overhead by keeping copies of only those pages that are actuallymodified regardless of the technique used  the complexities introduced by long-duration transactions and large data items complicate the recovery process thus  it is desirable to allow certain noncritical data to be exempt from logging  and to rely instead on offline backups and human intervention 26.7 summary ? workflows are activities that involve the coordinated execution of multiple tasks performed by different processing entities they exist not just in computer applications  but also in almost all organizational activities with the growth of networks  and the existence of multiple autonomous database systems  workflows provide a convenient way of carrying out tasks that involve multiple systems ? although the usual acid transactional requirements are too strong or are unimplementable for such workflow applications  workflows must satisfy a thesumit67.blogspot.com 1116 chapter 26 advanced transaction processing limited set of transactional properties that guarantee that a process is not left in an inconsistent state ? transaction-processing monitors were initially developed as multithreaded servers that could service large numbers of terminals from a single process they have since evolved  and today they provide the infrastructure for building and administering complex transaction-processing systems that have a large number of clients and multiple servers they provide services such as durable queueing of client requests and server responses  routing of client messages to servers  persistent messaging  load balancing  and coordination of two-phase commit when transactions access multiple servers ? e-commerce systems have become a core part of commerce there are several database issues in e-commerce systems catalog management  especially personalization of the catalog  is done with databases electronic marketplaces help in pricing of products through auctions  reverse auctions  or exchanges high-performance database systems are needed to handle such trading orders are settled by electronic payment systems  which also need high-performance database systems to handle very high transaction rates ? large main memories are exploited in certain systems to achieve high system throughput in such systems  logging is a bottleneck under the groupcommit concept  the number of outputs to stable storage can be reduced  thus releasing this bottleneck ? the efficient management of long-duration interactive transactions is more complex  because of the long-duration waits and because of the possibility of aborts since the concurrency-control techniques used in chapter 15 use waits  aborts  or both  alternative techniques must be considered these techniques must ensure correctness without requiring serializability ? along-duration transaction is represented as a nested transaction with atomic database operations at the lowest level if a transaction fails  only active shortduration transactions abort active long-duration transactions resume once any short-duration transactions have recovered a compensating transaction is needed to undo updates of nested transactions that have committed  if the outer-level transaction fails ? in systems with real-time constraints  correctness of execution involves not only database consistency but also deadline satisfaction the wide variance of execution times for read and write operations complicates the transactionmanagement problem for time-constrained systems review terms ? tp monitor ? tp-monitor architectures ? process per client ? single server thesumit67.blogspot.com practice exercises 1117 ? many server  single router ? many server  many router ? multitasking ? context switch ? multithreaded server ? queue manager ? application coordination ? resource manager ? remote procedure call  rpc  ? transactional workflows ? task ? processing entity ? workflow specification ? workflow execution ? workflow state ? execution states ? output values ? external variables ? workflow failure atomicity ? workflow termination states ? acceptable ? nonacceptable ? committed ? aborted ? workflow recovery ? workflow-management system ? workflow-management system architectures ? centralized ? partially distributed ? fully distributed ? business process management ? orchestration ? e-commerce ? e-catalogs ? marketplaces ? auctions ? reverse auctions ? exchange ? order settlement ? digital certificates ? main-memory databases ? group commit ? real-time systems ? deadlines ? hard deadline ? firm deadline ? soft deadline ? real-time databases ? long-duration transactions ? exposure of uncommitted data ? nonserializable executions ? nested transactions ? multilevel transactions ? saga ? compensating transactions ? logical logging practice exercises 26.1 like database systems  workflow systems also require concurrency and recovery management list three reasons why we can not simply apply a relational database system using 2pl  physical undo logging  and 2pc thesumit67.blogspot.com 1118 chapter 26 advanced transaction processing 26.2 consider a main-memory database system recovering from a system crash explain the relative merits of  ? loading the entire database back into main memory before resuming transaction processing ? loading data as it is requested by transactions 26.3 is a high-performance transaction system necessarily a real-time system ? why or why not ? 26.4 explain why it may be impractical to require serializability for longduration transactions 26.5 consider a multithreaded process that delivers messages from a durable queue of persistent messages different threads may run concurrently  attempting to deliver different messages in case of a delivery failure  the message must be restored in the queue model the actions that each thread carries out as a multilevel transaction  so that locks on the queue need not be held until a message is delivered 26.6 discuss the modifications that need to be made in each of the recovery schemes covered in chapter 16 if we allow nested transactions also  explain any differences that result if we allow multilevel transactions exercises 26.7 explain howa tp monitor manages memory and processor resources more effectively than a typical operating system 26.8 compare tp-monitor features with those provided by web servers supporting servlets  such servers have been nicknamed tp-lite   26.9 consider the process of admitting new students at your university  or new employees at your organization   a give a high-level picture of the workflow starting from the student application procedure b indicate acceptable termination states and which steps involve human intervention c indicate possible errors  including deadline expiry  and how they are dealt with d study how much of the workflow has been automated at your university 26.10 answer the following questions regarding electronic payment systems  thesumit67.blogspot.com bibliographical notes 1119 a explain why electronic transactions carried out using credit-card numbers may be insecure b an alternative is to have an electronic payment gatewaymaintained by the credit-card company  and the site receiving payment redirects customers to the gateway site to make the payment i explain what benefits such a system offers if the gateway does not authenticate the user ii explain what further benefits are offered if the gateway has a mechanism to authenticate the user c some credit-card companies offer a one-time-use credit-card number as a more secure method of electronic payment customers connect to the credit-card company ? s web site to get the one-time-use number explain what benefit such a system offers  as compared to using regular credit-card numbers also explain its benefits and drawbacks as compared to electronic payment gateways with authentication d does either of the above systems guarantee the same privacy that is available when payments are made in cash ? explain your answer 26.11 if the entire database fits in main memory  do we still need a database system to manage the data ? explain your answer 26.12 in the group-commit technique  how many transactions should be part of a group ? explain your answer 26.13 in a database system using write-ahead logging  what is the worst-case number of disk accesses required to read a data item from a specified disk page explain why this presents a problem to designers of real-time database systems hint  consider the case when the disk buffer is full 26.14 what is the purpose of compensating transactions ? present two examples of their use 26.15 explain the connections between a workflow and a long-duration transaction bibliographical notes gray and reuter  1993  provides a detailed  and excellent  textbook description of transaction-processing systems  including chapters on tp monitors x/open  1991  defines the x/open xa interface fischer  2006  is a handbook on workflow systems  which is published in association with theworkflow management coalition theweb site of the coalition is www.wfmc.org our description ofworkflows follows the model of rusinkiewicz and sheth  1995   loeb  1998  provides a detailed description of secure electronic transactions thesumit67.blogspot.com 1120 chapter 26 advanced transaction processing garcia-molina and salem  1992  provides an overview of main-memory databases jagadish et al  1993  describes a recovery algorithm designed for mainmemory databases a storage manager for main-memory databases is described in jagadish et al  1994   real-time databases are discussed by lam and kuo  2001   concurrency control and scheduling in real-time databases are discussed by haritsa et al  1990   hong et al  1993   and pang et al  1995   ozsoyoglu and snodgrass  1995  is a survey of research in real-time and temporal databases nested and multilevel transactions are presented by moss  1985   lynch and merritt  1986   moss  1987   haerder and rothermel  1987   rothermel and mohan  1989  ,weikum et al  1990   korth and speegle  1990  ,weikum  1991   and korth and speegle  1994   theoretical aspects of multilevel transactions are presented in lynch et al  1988   the concept of saga was introduced in garcia-molina and salem  1987   thesumit67.blogspot.com part 9 case studies this part describes how different database systems integrate the various concepts described earlier in the book we begin by covering a widely used opensource database system  postgresql  inchapter 27 threewidely used commercial database systems ? ibm db2  oracle  and microsoft sql server ? are covered in chapters 28  29  and 30 these three represent three of the most widely used commercial database systems each of these chapters highlights unique features of each database system  tools  sql variations and extensions  and system architecture  including storage organization  query processing  concurrency control and recovery  and replication the chapters cover only key aspects of the database products they describe  and therefore should not be regarded as a comprehensive coverage of the product furthermore  since products are enhanced regularly  details of the product may change when using a particular product version  be sure to consult the user manuals for specific details keep in mind that the chapters in this part often use industrial rather than academic terminology for instance  they use table instead of relation  rowinstead of tuple  and column instead of attribute 1121 thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter27 postgresql anastasia ailamaki  sailesh krishnamurthy  spiros papadimitriou  bianca schroeder  karl schnaitter  and gavin sherry postgresql is an open-source object-relational database management system it is a descendant of one of the earliest such systems  the postgres system developed under professormichael stonebraker at the university of california  berkeley the name ? postgres ? is derived from the name of a pioneering relational database system  ingres  also developed under stonebraker at berkeley currently  postgresql supports many aspects of sql  2003 and offers features such as complex queries  foreign keys  triggers  views  transactional integrity  full-text searching  and limited data replication in addition  users can extend postgresql with new data types  functions  operators  or index methods postgresql supports a variety of programming languages  including c  c + +  java  perl  tcl  and python  as well as the database interfaces jdbc and odbc another notable point of postgresql is that it  along with mysql  is one of the two most widely used open-source relational database systems postgresql is released under the bsd license  which grants permission to anyone for the use  modification  and distribution of the postgresql code and documentation for any purpose without fee 27.1 introduction in the course of two decades  postgresql has undergone several major releases the first prototype system  under the name postgres  was demonstrated at the 1988 acm sigmod conference the first version  distributed to users in 1989  provided features such as extensible data types  a preliminary rule system  and a query language named postquel after the subsequent versions added a new rule system  support for multiple storage managers  and an improved query executor  the system developers focused on portability and performance until 1994  when an sql language interpreter was added under a new name  postgres95  the system was released to theweb and later commercialized by illustra informa 1123 thesumit67.blogspot.com 1124 chapter 27 postgresql tion technologies  later merged into informix  which is now owned by ibm   by 1996  the name postgres95 was replaced by postgresql  to reflect the relationship between the original postgres and the more recent versions with sql capability postgresql runs under virtually all unix-like operating systems  including linux and apple macintosh os x early versions of the postgresql server can be run under microsoftwindows in the cygwin environment,which provides linux emulation under windows version 8.0  released in january 2005  introduced native support for microsoftwindows today  postgresql is used to implement several different research and production applications  such as the postgis system for geographic information  and an educational tool at several universities the system continues to evolve through the contributions of a community of about 1000 developers in this chapter  we explain how postgresql works  starting from user interfaces and languages and continuing into the heart of the system  the data structures and the concurrencycontrol mechanism   27.2 user interfaces the standard distribution of postgresql comes with command-line tools for administering the database however  there is a wide range of commercial and open-source graphical administration and design tools that support postgresql software developers may also access postgresql through a comprehensive set of programming interfaces 27.2.1 interactive terminal interfaces like most database systems  postgresql offers command-line tools for database administration the main interactive terminal client is psql  which is modeled after the unix shell and allows execution of sql commands on the server  as well as several other operations  such as client-side copying   some of its features are  ? variables psql provides variable substitution features  similar to common unix command shells ? sql interpolation the user can substitute  ? interpolate ?  psql variables into regular sql statements by placing a colon in front of the variable name ? command-line editing psql uses the gnu readline library for convenient line editing  with tab-completion support postgresql may also be accessed from a tcl/tk shell  which provides a flexible scripting language commonly used for rapid prototyping this functionality is enabled intcl/tkby loading the pgtcl library,which is distributed as an optional extension to postgresql 27.2.2 graphical interfaces the standard distribution of postgresql does not contain any graphical tools however  several graphical user interface tools exist  and users can choose among thesumit67.blogspot.com 27.2 user interfaces 1125 figure 27.1 pgadmin iii  an open-source database administration gui commercial and open-source alternatives.many of these go through rapid release cycles ; the following list reflects the state of affairs at the time of this writing there are graphical tools for administration  including pgaccess and pgadmin  the latter of which is shown in figure 27.1 tools for database design include tora and data architect  the latter of which is shown in figure 27.2 postgresql works with several commercial forms-design and report-generation tools opensource alternatives include rekall  shown in figures 27.3 and 27.4   gnu report generator  and a more comprehensive tool suite  gnu enterprise 27.2.3 programming language interfaces postgresql provides native interfaces for odbc and jdbc  as well as bindings for most programming languages  including c  c + +  php  perl  tcl/tk  ecpg  python  and ruby the libpq library provides the c api for postgresql ; libpq is also the underlying engine for most programming-language bindings the libpq library supports both synchronous and asynchronous execution of sql commands and prepared statements  through a reentrant and thread-safe interface the connection parameters of libpq may be configured in several flexible ways  such as thesumit67.blogspot.com 1126 chapter 27 postgresql figure 27.2 data architect  a multiplatform database design gui setting environment variables  placing settings in a local file  or creating entries on an ldap server 27.3 sql variations and extensions the current version of postgresql supports almost all entry-level sql-92 features  as well as many of the intermediate and full-level features it also supports many sql  1999 and sql  2003 features  including most object-relational features described in chapter 22 and the sql/xml features for parsed xml data described in chapter 23 in fact  some features of the current sql standard  such as arrays  functions  and inheritance  were pioneered by postgresql or its ancestors it lacks olap features  most notably  cube and rollup   but data from postgresql can be easily loaded into open-source external olap servers  such as mondrian  as well as commercial products 27.3.1 postgresql types postgresql has support for several nonstandard types  useful for specific application domains furthermore  users can define new types with the create type thesumit67.blogspot.com 27.3 sql variations and extensions 1127 figure 27.3 rekall  form-design gui command this includes new low-level base types  typically written in c  see section 27.3.3.1   27.3.1.1 the postgresql type system postgresql types fall into the following categories  ? base types base types are also known as abstract data types ; that is,modules that encapsulate both state and a set of operations these are implemented below the sql level  typically in a language such as c  see section 27.3.3.1   examples are int4  already included in postgresql  or complex  included as an optional extension type   a base type may represent either an individual scalar value or a variable-length array of values for each scalar type that exists in a database  postgresql automatically creates an array type that holds values of the same scalar type ? composite types these correspond to table rows ; that is  they are a list of field names and their respective types a composite type is created implicitly whenever a table is created  but users may also construct them explicitly thesumit67.blogspot.com 1128 chapter 27 postgresql figure 27.4 rekall  report-design gui ? domains adomain type is defined by coupling a base type with a constraint that values of the type must satisfy values of the domain type and the associated base type may be used interchangeably  provided that the constraint is satisfied a domain may also have an optional default value  whose meaning is similar to the default value of a table column ? enumerated types these are similar to enum types used in programming languages such as c and java an enumerated type is essentially a fixed list of named values in postgresql  enumerated types may be converted to the textual representation of their name  but this conversion must be specified explicitly in some cases to ensure type safety for instance  values of different enumerated types may not be compared without explicit conversion to compatible types ? pseudotypes currently  postgresql supports the following pseudotypes  any  anyarray  anyelement  anyenum  anynonarray cstring  internal  opaque  language handler  record  trigger  and void these can not be used in composite types  and thus can not be used for table columns   but can be used as argument and return types of user-defined functions ? polymorphic types four of the pseudotypes anyelement  anyarray  anynonarray  and anyenum are collectively known as polymorphic functions with arguments of these types  correspondingly called polymorphic functions  may operate on any actual type postgresql has a simple type-resolution scheme thesumit67.blogspot.com 27.3 sql variations and extensions 1129 that requires that   1  in any particular invocation of a polymorphic function  all occurrences of a polymorphic typemust be bound to the same actual type  that is  a function defined as f  anyelement  anyelement  may operate only on pairs of the same actual type   and  2  if the return type is polymorphic  then at least one of the arguments must be of the same polymorphic type 27.3.1.2 nonstandard types the types described in this section are included in the standard distribution furthermore  thanks to the open nature of postgresql  there are several contributed extension types  such as complex numbers  and isbn/issns  see section 27.3.3   geometric data types  point  line  lseg  box  polygon  path  circle  are used in geographic information systems to represent two-dimensional spatial objects such as points  line segments  polygons  paths  and circles numerous functions and operators are available in postgresql to perform various geometric operations such as scaling  translation  rotation  and determining intersections furthermore  postgresql supports indexing of these types using r-trees  sections 25.3.5.3 and 27.5.2.1   full-text searching is performed in postgresql using the tsvector type that represents a document and the tsquery type that represents a full-text query a tsvector stores the distinct words in a document  after converting variants of each word to a common normal form  for example  removingword stems   postgresql provides functions to convert raw text to a tsvector and concatenate documents a tsquery specifieswords to search for in candidate documents,withmultiplewords connected by boolean operators for example  the query ? index & !  tree | hash  ? finds documents that contain ? index ? without using the words ? tree ? or ? hash ? postgresql natively supports operations on full-text types  including language features and indexed search postgresql offers data types to store network addresses these data types allow network-management applications to use a postgresql database as their data store for those familiar with computer networking  we provide a brief summary of this feature here separate types exist for ipv4  ipv6  and media access control  mac  addresses  cidr  inet and macaddr  respectively   both inet and cidr types can store ipv4 and ipv6 addresses  with optional subnet masks their main difference is in input/output formatting  as well as the restriction that classless internet domain routing  cidr  addresses do not accept valueswith nonzero bits to the right of the netmask the macaddr type is used to store mac addresses  typically  ethernet card hardware addresses   postgresql supports indexing and sorting on these types  as well as a set of operations  including subnet testing  and mapping mac addresses to hardware manufacturer names   furthermore  these types offer input-error checking thus  they are preferable over plain text fields the postgresql bit type can store both fixed and variable-length strings of 1s and 0s postgresql supports bit-logical operators and string-manipulation functions for these values thesumit67.blogspot.com 1130 chapter 27 postgresql 27.3.2 rules and other active-database features postgresql supports sql constraints and triggers  and stored procedures ; see section 27.3.3   furthermore  it features query-rewriting rules that can be declared on the server postgresql allows check constraints  not null constraints  and primary-key and foreign-key constraints  with restricting and cascading deletes   like many other relational database systems  postgresql supports triggers  which are useful for nontrivial constraints and consistency checking or enforcement trigger functions can bewritten in a procedural language such as pl/pgsql  see section 27.3.3.4  or in c  but not in plain sql triggers can execute before or after insert  update  or delete operations and either once per modified row  or once per sql statement the postgresql rules system allows users to define query-rewrite rules on the database server.unlike stored procedures and triggers  the rule system intervenes between the query parser and the planner and modifies queries on the basis of the set of rules after the original query tree has been transformed into one or more trees  they are passed to the query planner thus  the planner has all the necessary information  tables to be scanned  relationships between them  qualifications  join information  and so forth  and can come upwith an efficient execution plan  even when complex rules are involved the general syntax for declaring rules is  create rule rule name as on  select | insert | update | delete  to table  where rule qualification  do  instead   nothing | command |  command ; command    the rest of this section provides examples that illustrate the rule system ? s capabilities more details on how rules are matched to query trees and how the latter are subsequently transformed can be found in the postgresql documentation  see the bibliographical notes   the rule system is implemented in the rewrite phase of query processing and explained in section 27.6.1 first  postgresql uses rules to implement views a view definition such as  create view myview as select * from mytab ; is converted into the following rule definition  create table myview  same column list as mytab  ; create rule return as on select to myview do instead select * from mytab ; queries on myview are transformed before execution to queries on the underlying table mytab the create view syntax is considered better programming form in this case  since it is more concise and it also prevents creation of views that thesumit67.blogspot.com 27.3 sql variations and extensions 1131 reference each other  which is possible if rules are carelessly declared  resulting in potentially confusing runtime errors   however  rules can be used to define update actions on views explicitly  create view statements do not allow this   as another example  consider the casewhere the user wants to log all increases of instructor salaries this could be achieved by a rule such as  create rule salary audit as on update to instructor where new.salary < > old.salary do insert into salary audit values  current timestamp  current user  new.name  old.salary  new.salary  ; finally  we give a slightly more complicated insert/update rule assume that pending salary increases are stored in a table salary increases  name  increase   we can declare a ? dummy ? table approved increases with the same fields and then define the following rule  create rule approved increases insert as on insert to approved increases do instead update instructor set salary = salary + new.increase where name = new.name ; then the following query  insert into approved increases select * from salary increases ; will update all salaries in the instructor table at once since the instead keyword was specified in the rule  the approved increases table is unchanged there is some overlap between the functionality provided by rules and perrowtriggers the postgresql rule system can be used to implement most triggers  but some kinds of constraints  in particular  foreign keys  can not be implemented by rules also  triggers have the added ability to generate error messages to signal constraint violations  whereas a rule may only enforce data integrity by silently suppressing invalid values on the other hand  triggers can not be used for the update or delete actions that rules enable on views since there is no real data in a view relation  the trigger would never be called an important difference between triggers and views is that a trigger is executed iteratively for every affected row a rule  on the other hand  manipulates the query tree before query planning so if a statement affects many rows  a rule is far more efficient than a trigger the implementation of triggers and constraints in postgresql is outlined briefly in section 27.6.4 thesumit67.blogspot.com 1132 chapter 27 postgresql 27.3.3 extensibility like most relational database systems  postgresql stores information about databases  tables  columns  and so forth  in what are commonly known as system catalogs  which appear to the user as normal tables other relational database systems are typically extended by changing hard-coded procedures in the source code or by loading special extension modules written by the vendor unlike most relational database systems  postgresqlgoes one step further and stores much more information in its catalogs  not only information about tables and columns  but also information about data types  functions  access methods  and so on therefore  postgresql is easy for users to extend and facilitates rapid prototyping of new applications and storage structures postgresql can also incorporate user-written code into the server  through dynamic loading of shared objects this provides an alternative approach to writing extensions that can be used when catalog-based extensions are not sufficient furthermore  the contrib module of the postgresql distribution includes numerous user functions  for example  array iterators  fuzzy string matching  cryptographic functions   base types  for example  encrypted passwords  isbn/issns  n-dimensional cubes  and index extensions  for example  rd-trees  indexing for hierarchical labels   thanks to the open nature of postgresql  there is a large community of postgresql professionals and enthusiasts who also actively extend postgresql on an almost daily basis extension types are identical in functionality to the built-in types  see also section 27.3.1.2  ; the latter are simply already linked into the server and preregistered in the system catalog similarly  this is the only difference between built-in and extension functions 27.3.3.1 types postgresql allows users to define composite types  enumeration types  and even new base types a composite-type definition is similar to a table definition  in fact  the latter implicitly does the former   stand-alone composite types are typically useful for function arguments for example  the definition  create type city t as  name varchar  80   state char  2   ; allows functions to accept and return city t tuples  even if there is no table that explicitly contains rows of this type enumeration types are easy to define  by simply listing the names of the values the following example creates an enumerated type to represent the status of a software product create type status t as enum  ? alpha ?  ? beta ?  ? release ?  ; the order of listed names is significant in comparing values of an enumerated type this can be useful for a statement such as  thesumit67.blogspot.com 27.3 sql variations and extensions 1133 select name from products where status > ? alpha ? ; which retrieves the names of products that have moved past the alpha stage adding base types to postgresql is straightforward ; an example can be found in complex.sql and complex.c in the tutorials of the postgresql distribution the base type can be declared in c  for example  typedef struct complex  double x ; double y ;  complex ; the next step is to define functions to read and write values of the new type in text format  see section 27.3.3.2   subsequently  the new type can be registered using the statement  create type complex  internallength = 16  input = complex in  output = complex out  alignment = double  ; assuming the text i/o functions have been registered as complex in and complex out the user has the option of defining binary i/o functions as well  for more efficient data dumping   extension types can be used like the existing base types of postgresql in fact  their only difference is that the extension types are dynamically loaded and linked into the server furthermore  indices may be extended easily to handle new base types ; see section 27.3.3.3 27.3.3.2 functions postgresql allows users to define functions that are stored and executed on the server postgresql also supports function overloading  that is  functions may be declared by using the same name but with arguments of different types   functions can be written as plain sql statements  or in several procedural languages  covered in section 27.3.3.4   finally  postgresql has an application programmer interface for adding functions written in c  explained in this section   user-defined functions can be written in c  or a language with compatible calling conventions  such as c + +   the actual coding conventions are essentially the same for dynamically loaded  user-defined functions  as well as for internal functions  which are statically linked into the server  .hence  the standard internal function library is a rich source of coding examples for user-defined c functions thesumit67.blogspot.com 1134 chapter 27 postgresql once the shared library containing the function has been created  a declaration such as the following registers it on the server  create function complex out  complex  returns cstring as ? shared object filename ? language c immutable strict ; the entry point to the shared object file is assumed to be the same as the sql function name  here  complex out   unless otherwise specified the example here continues the one from section 27.3.3.1 the application program interface hides most of postgresql ? s internal details hence  the actual c code for the above text output function of complex values is quite simple  pg function info v1  complex out  ; datum complex out  pg function args   complex * complex =  complex *  pg getarg pointer  0  ; char * result ; result =  char *  palloc  100  ; snprintf  result  100  "  % g  % g  "  complex > x  complex > y  ; pg return cstring  result  ;  the first line declares the function complex out  and the following lines implement the output function the code uses several postgresql-specific constructs  such as the palloc function  which dynamically allocates memory controlled by postgresql ? s memory manager more details may be found in the postgresql documentation  see the bibliographical notes   aggregate functions in postgresql operate by updating a state value via a state transition function that is called for each tuple value in the aggregation group for example  the state for the avg operator consists of the running sum and the count of values as each tuple arrives  the transition function should simply add its value to the running sum and increment the count by one optionally  a final function may be called to compute the return value based on the state information for example  the final function for avg would simply divide the running sum by the count and return it thus  defining a new aggregate is as simple as defining these two functions for the complex type example  if complex add is a user-defined function that takes two complex arguments and returns their sum  then the sum aggregate operator can be extended to complex numbers using the simple declaration  thesumit67.blogspot.com 27.3 sql variations and extensions 1135 create aggregate sum  sfunc = complex add  basetype = complex  stype = complex  initcond = ?  0,0  ?  ; note the use of function overloading  postgresql will call the appropriate sum aggregate function  on the basis of the actual type of its argument during invocation the basetype is the argument type and stype is the state value type in this case  a final function is unnecessary  since the return value is the state value itself  that is  the running sum in both cases   user-defined functions can also be invoked by using operator syntax beyond simple ? syntactic sugar ? for function invocation  operator declarations can also provide hints to the query optimizer in order to improve performance these hints may include information about commutativity  restriction and join selectivity estimation  and various other properties related to join algorithms 27.3.3.3 index extensions postgresql currently supports the usual b-tree and hash indices  as well as two index methods that are unique to postgresql  the generalized search tree  gist  and the generalized inverted index  gin   which is useful for full-text indexing  these index structures are explained in section 27.5.2.1   finally  postgresql provides indexing of two-dimensional spatial objects with an r-tree index  which is implemented using a gist index behind the scenes all of these can be easily extended to accommodate new base types adding index extensions for a type requires definition of an operator class  which encapsulates the following  ? index-method strategies these are a set of operators that can be used as qualifiers in where clauses the particular set depends on the index type for example  b-tree indices can retrieve ranges of objects  so the set consists of five operators  <  < =  =  > =  and >   all of which can appear in a where clause involving a b-tree index a hash index allows only equality testing and an r-tree index allows a number of spatial relationships  for example contained  to-the-left  and so forth   ? index-method support routines the above set of operators is typically not sufficient for the operation of the index for example  a hash index requires a function to compute the hash value for each object an r-tree index needs to be able to compute intersections and unions and to estimate the size of indexed objects thesumit67.blogspot.com 1136 chapter 27 postgresql for example  if the following functions and operators are defined to compare the magnitude of complex numbers  see section 27.3.3.1   then we can make such objects indexable by the following declaration  create operator class complex abs ops default for type complex using btree as operator 1 <  complex  complex   operator 2 < =  complex  complex   operator 3 =  complex  complex   operator 4 > =  complex  complex   operator 5 >  complex  complex   function 1 complex abs cmp  complex  complex  ; the operator statements define the strategymethods and the function statements define the support methods 27.3.3.4 procedural languages stored functions and procedures can be written in a number of procedural languages furthermore  postgresql defines an application programmer interface for hooking up any programming language for this purpose programming languages can be registered on demand and are either trusted or untrusted the latter allow unlimited access to the dbms and the file system  and writing stored functions in them requires superuser privileges ? pl/pgsql this is a trusted language that adds procedural programming capabilities  for example  variables and control flow  to sql it is very similar to oracle ? s pl/sql although code can not be transferred verbatim from one to the other  porting is usually simple ? pl/tcl  pl/perl  and pl/python these leverage the power of tcl  perl  and python to write stored functions and procedures on the server the first two come in both trusted and untrusted versions  pl/tcl  pl/perl and pl/tclu  pl/perlu  respectively   while pl/python is untrusted at the time of this writing each of these has bindings that allow access to the database system via a language-specific interface 27.3.3.5 server programming interface the server programming interface  spi  is an application programmer interface that allows user-defined c functions  see section 27.3.3.2  to run arbitrary sql commands inside their functions this gives writers of user-defined functions the ability to implement only essential parts in c and easily leverage the full power of the relational database system engine to do most of the work thesumit67.blogspot.com 27.4 transaction management in postgresql 1137 27.4 transaction management in postgresql transaction management in postgresql uses both both snapshot isolation and two-phase locking.which one of the two protocols is used depends on the type of statement being executed for dml statements1 the snapshot isolation technique presented in section 15.7 is used ; the snapshot isolation scheme is referred to as themultiversion concurrency control  mvcc  scheme inpostgresql concurrency control for ddl statements  on the other hand  is based on standard two-phase locking 27.4.1 postgresql concurrency control since the concurrency control protocol used by postgresql depends on the isolation level requested by the application  we begin with an overview of the isolation levels offered by postgresql we then describe the key ideas behind the mvcc scheme  followed by a discussion of their implementation in postgresql and some of the implications of mvcc.we conclude this sectionwith an overview of locking for ddl statements and a discussion of concurrency control for indices 27.4.1.1 postgresql isolation levels the sql standard defines three weak levels of consistency  in addition to the serializable level of consistency  on which most of the discussion in this book is based the purpose of providing the weak consistency levels is to allow a higher degree of concurrency for applications that don ? t require the strong guarantees that serializability provides examples of such applications include long-running transactions that collect statistics over the database and whose results do not need to be precise the sql standard defines the different isolation levels in terms of three phenomena that violate serializability the three phenomena are called dirty read  nonrepeatable read  and phantom read  and are defined as follows  ? dirty read the transaction reads values written by another transaction that hasn ? t committed yet ? nonrepeatable read a transaction reads the same object twice during execution and finds a different value the second time  although the transaction has not changed the value in the meantime ? phantom read a transaction re-executes a query returning a set of rows that satisfy a search condition and finds that the set of rows satisfying the condition has changed as a result of another recently committed transaction  a more detailed explanation of the phantom phenomenon  including the 1a dml statement is any statement that updates or reads data within a table  that is  select  insert  update  fetch  and copy ddl statements affect entire tables ; they can remove a table or change the schema of a table  for example ddl statements and some other postgresql-specific statements will be discussed later in this section thesumit67.blogspot.com 1138 chapter 27 postgresql isolated level dirty read non repeatable read phantom read read uncommitted maybe maybe maybe read committed no maybe maybe repeated read no no maybe serializable no no no figure 27.5 definition of the four standard sql isolation levels concept of a phantom conflict  can be found in section 15.8.3 ; eliminating phantom reads does not eliminate all phantom conflicts  it should be obvious that each of the above phenomena violates transaction isolation  and hence violates serializability figure 27.5 shows the definition of the four sql isolation levels specified in the sql standard ? read uncommitted  read committed  repeatable read  and serializable ? in terms of these phenomena postgresql supports two of the four different isolation levels  read committed  which is the default isolation level in postgresql  and serializable however  the postgresql implementation of the serializable isolation level uses snapshot isolation  which does not truly ensure serializability as we have seen earlier in section 15.7 27.4.1.2 concurrency control for dml commands the mvcc scheme used in postgresql is an implementation of the snapshot isolation protocol which we saw in section 15.7 the key idea behind mvcc is to maintain different versions of each row that correspond to instances of the row at different points in time this allows a transaction to see a consistent snapshot of the data  by selecting the most recent version of each row that was committed before taking the snapshot themvccprotocol uses snapshots to ensure that every transaction sees a consistent view of the database  before executing a command  the transaction chooses a snapshot of the data and processes the row versions that are either in the snapshot or created by earlier commands of the same transaction this view of the data is ? consistent ? since it only takes full transactions into account  but the snapshot is not necessarily equal to the current state of the data the motivation for using mvcc is that readers never block writers and vice versa readers access the most recent version of a row that is part of the transaction ? s snapshot.writers create their own separate copy of the row to be updated section 27.4.1.3 shows that the only conflict that causes a transaction to be blocked arises if two writers try to update the same row in contrast  under the standard two-phase locking approach  both readers and writers might be blocked  since there is only one version of each data object and both read and write operations are required to obtain a lock before accessing any data the mvcc scheme in postgresql implements the first-updater-wins version of the snapshot isolation protocol  by acquiring exclusive locks on rows that are written  but using a snapshot  without any locking  when reading rows ; thesumit67.blogspot.com 27.4 transaction management in postgresql 1139 additional validation is done when exclusive locks are obtained  as outlined earlier in section 15.7 27.4.1.3 postgresql implementation of mvcc at the core of postgresql mvcc is the notion of tuple visibility a postgresql tuple refers to a version of a row tuple visibility defines which of the potentially many versions of a row in a table is valid within the context of a given statement or transaction a transaction determines tuple visibility based on a database snapshot that is chosen before executing a command a tuple is visible for a transaction t if the following two conditions hold  1 the tuple was created by a transaction that committed before transaction t took its snapshot 2 updates to the tuple  if any  were executed by a transaction that is either ? aborted  or ? started running after t took its snapshot  or ? was active when t took its snapshot to be precise  a tuple is also visible to t if it was created by t and not subsequently updated by t.we omit the details of this special case for simplicity the goal of the above conditions is to ensure that each transaction sees a consistent viewof the data postgresql maintains the following state information to check these conditions efficiently  ? a transaction id  which at the same time serves as a timestamp  is assigned to every transaction at transaction start time postgresql uses a logical counter  as described in section 15.4.1  for assigning transaction ids ? a log file called pg clog contains the current status of each transaction the status can be either in progress  committed  or aborted ? each tuple in a table has a header with three fields  xmin  which contains the transaction id of the transaction that created the tuple and which is therefore also called the creation-transaction id ; xmax  which contains the transaction id of the replacing/deleting transaction  or null if not deleted/replaced  and which is also referred to as the expire-transaction id ; and a forward link to new versions of the same logical row  if there are any ? a snapshotdata data structure is created either at transaction start time or at query start time  depending on the isolation level  described in more detail below   its main purpose is to decide whether a tuple is visible to the current command the snapshotdata stores information about the state of transactions at the time it is created  which includes a list of active transactions and xmax  a value equal to 1 + the highest id of any transaction that has started so far thesumit67.blogspot.com 1140 chapter 27 postgresql database table forward xmin xmax department  dept_name  building  budget      100 102 106    100 102 104   106    10 10 00   00    102 106 x transaction 104 select budget from department where dept_name = ? physics ? 00 in progress 01 aborted 10 committed watson watson watson 70000 64000 68000   physics physics physics  pg_clog file xid status flags figure 27.6 the postgresql data structures used for mvcc the value xmax serves as a ? cutoff ? for transactions that may be considered visible figure 27.6 illustrates some of this state information through a simple example involving a database with only one table  the department table from figure 27.7 the department table has three columns  the name of the department  the building where the department is located  and the budget of the department figure 27.6 shows a fragment of the department table containing only the  versions of  the row corresponding to the physics department the tuple headers indicate that the row was originally created by transaction 100  and later updated by transaction 102 and transaction 106 figure 27.6 also shows a fragment of the corresponding pg clog file on the basis of the pg clog file  transactions 100 and 102 are committed  while transactions 104 and 106 are in progress given the above state information  the two conditions that need to be satisfied for a tuple to be visible can be rewritten as follows  dept name building budget biology watson 90000 comp sci taylor 100000 elec eng taylor 85000 finance painter 120000 history painter 50000 music packard 80000 physics watson 70000 figure 27.7 the department relation thesumit67.blogspot.com 27.4 transaction management in postgresql 1141 1 the creation-transaction id in the tuple header a is a committed transaction according to the pg clog file  and b is less than the cutoff transaction id xmax recorded by snapshotdata  and c is not one of the active transactions stored in snapshotdata 2 the expire-transaction id  if it exists  a is an aborted transaction according to the pg clog file  or b is greater than or equal to the cutoff transaction id xmax recorded by snapshotdata  or c is one of the active transactions stored in snapshotdata consider the example database in figure 27.6 and assume that the snapshot data used by transaction 104 simply uses 103 as the cutoff transaction id xmax and does not show any earlier transactions to be active in this case  the only version of the row corresponding to the physics department that is visible to transaction 104 is the second version in the table  created by transaction 102 the first version  created by transaction 100  is not visible  since it violates condition 2  the expiretransaction id of this tuple is 102  which corresponds to a transaction that is not aborted and that has a transaction id less than or equal to 103 the third version of the physics tuple is not visible  since it was created by transaction 106  which has a transaction id larger than transaction 103  implying that this version had not been committed at the time snapshotdata was created moreover  transaction 106 is still in progress  which violates another one of the conditions the second version of the row meets all the conditions for tuple visibility the details of how postgresql mvcc interacts with the execution of sql statements depends on whether the statement is an insert  select  update  or delete statement the simplest case is an insert statement  which may simply create a new tuple based on the data in the statement  initialize the tuple header  the creation id   and insert the new tuple into the table.unlike two-phase locking  this does not require any interactionwith the concurrency-control protocol unless the insertion needs to be checked for integrity conditions  such as uniqueness or foreign key constraints when the system executes a select  update  or delete statement the interaction with themvccprotocol depends on the isolation level specified by the application if the isolation level is read committed  the processing of a new statement begins with creating a new snapshotdata data structure  independent of whether the statement starts a new transaction or is part of an existing transaction   next  the system identifies target tuples  that is  the tuples that are visible with respect to the snapshotdata and that match the search criteria of the statement in the case of a select statement  the set of target tuples make up the result of the query in the case of an update or delete statement in read committedmode  an extra step is necessary after identifying the target tuples  before the actual update or thesumit67.blogspot.com 1142 chapter 27 postgresql delete operation can take place the reason is that visibility of a tuple ensures only that the tuple has been created by a transaction that committed before the update/delete statement in question started however  it is possible that  since query start  this tuple has been updated or deleted by another concurrently executing transaction this can be detected by looking at the expire-transaction id of the tuple if the expire-transaction id corresponds to a transaction that is still in progress  it is necessary to wait for the completion of this transaction first if the transaction aborts  the update or delete statement can proceed and perform the actual modification if the transaction commits  the search criteria of the statement need to be evaluated again  and only if the tuple still meets these criteria can the row be modified if the row is to be deleted  the main step is to update the expire-transaction id of the old tuple a row update also performs this step  and additionally creates a new version of the row  sets its creation-transaction id  and sets the forward link of the old tuple to reference the new tuple going back to the example fromfigure 27.6  transaction 104  which consists of a select statement only  identifies the second version of the physics rowas a target tuple and returns it immediately if transaction 104 were an update statement instead  for example  trying to increment the budget of the physics department by some amount  it would have to wait for transaction 106 to complete it would then re-evaluate the search condition and  only if it is still met  proceed with its update using the protocol described above for update and delete statements provides only the read-committed isolation level serializability can be violated in several ways first  nonrepeatable reads are possible since each query within a transaction may see a different snapshot of the database  a query in a transaction might see the effects of an update command completed in the meantime that weren ? t visible to earlier queries within the same transaction following the same line of thought  phantom reads are possible when a relation is modified between queries in order to provide the postgresql serializable isolation level  postgresql mvcc eliminates violations of serializability in two ways  first  when it is determining tuple visibility  all queries within a transaction use a snapshot as of the start of the transaction  rather than the start of the individual query this way successive queries within a transaction always see the same data second  the way updates and deletes are processed is different in serializable mode compared to read-committed mode as in read-committed mode  transactionswait after identifying a visible target rowthat meets the search condition and is currently updated or deleted by another concurrent transaction if the concurrent transaction that executes the update or delete aborts  the waiting transaction can proceedwith its own update.however  if the concurrent transaction commits  there is no way for postgresql to ensure serializability for the waiting transaction therefore  the waiting transaction is rolled back and returns with the error message ? could not serialize access due to concurrent update ? it is up to the application to handle an error message like the above appropriately  by aborting the current transaction and restarting the entire transaction from the beginning observe that rollbacks due to serializability issues are possithesumit67 blogspot.com 27.4 transaction management in postgresql 1143 ble only for update and delete statements it is still the case that select statements never conflict with any other transactions 27.4.1.4 implications of usingmvcc using the postgresql mvcc scheme has implications in three different areas   1  extra burden is placed on the storage system  since it needs to maintain different versions of tuples ;  2  developing concurrent applications takes some extra care  since postgresql mvcc can lead to subtle  but important  differences in how concurrent transactions behave  compared to systems where standard two-phase locking is used ;  3  postgresql performance depends on the characteristics of the workload running on it the implications of postgresql mvcc are described in more detail below creating and storing multiple versions of every row can lead to excessive storage consumption to alleviate this problem  postgresql frees up space when possible by identifying and deleting versions of tuples that can not be visible to any active or future transactions  and are therefore no longer needed the task of freeing space is nontrivial  because indices may refer to the location of an unneeded tuple  so these references need to be deleted before reusing the space to lessen this issue  postgresql avoids indexing multiple versions of a tuple that have identical index attributes this allows the space taken by nonindexed tuples to be freed efficiently by any transaction that finds such a tuple for more aggressive space reuse  postgresql provides the vacuum command  which correctly updates indices for each freed tuple postgresql employs a background process to vacuum tables automatically  but the command can also be executed by the user directly the vacuum command offers two modes of operation  plain vacuum simply identifies tuples that are not needed  and makes their space available for reuse this form of the command can operate in parallel with normal reading and writing of the table vacuum full does more extensive processing  including moving of tuples across blocks to try to compact the table to the minimum number of disk blocks this form is much slower and requires an exclusive lock on each table while it is being processed because of the use of multiversion concurrency control in postgresql  porting applications fromother environments to postgresql might require some extra care to ensure data consistency as an example  consider a transaction ta executing a select statement since readers in postgresql don ? t lock data  data read and selected by ta can be overwritten by another concurrent transaction tb  while ta is still running as a result some of the data that ta returns might not be current anymore at the time of completion of ta ta might return rows that in the meantime have been changed or deleted by other transactions to ensure the current validity of a rowand protect it against concurrent updates  an application must either use select for share or explicitly acquire a lock with the appropriate lock table command postgresql ? s approach to concurrency control performs best for workloads containing many more reads than updates  since in this case there is a very low chance that two updates will conflict and force a transaction to roll back twothesumit67 blogspot.com 1144 chapter 27 postgresql phase locking may be more efficient for some update-intensive workloads  but this depends onmany factors  such as the length of transactions and the frequency of deadlocks 27.4.1.5 ddl concurrency control the mvcc mechanisms described in the previous section do not protect transactions against operations that affect entire tables  for example  transactions that drop a table or change the schema of a table toward this end  postgresql provides explicit locks that ddl commands are forced to acquire before starting their execution these locks are always table based  rather than row based  and are acquired and released in accordance with the strict two-phase locking protocol figure 27.8 lists all types of locks offered by postgresql  which locks they conflict with  and some commands that use them  the create index concurrently figure 27.8 table-level lock modes thesumit67.blogspot.com 27.4 transaction management in postgresql 1145 command is covered in section 27.5.2.3   the names of the lock types are often historical and don ? t necessarily reflect the use of the lock for example  all the locks are table-level locks  although some contain the word ? row ? in the name dml commands acquire only locks of the first three types these three lock types are compatible with each other  since mvcc takes care of protecting these operations against each other dml commands acquire these locks only for protection against ddl commands while their main purpose is providing postgresql internal concurrency control for ddl commands  all locks in figure 27.8 can also be acquired explicitly by postgresql applications through the lock table command locks are recorded in a lock table that is implemented as a shared-memory hash table keyed by a signature that identifies the object being locked if a transaction wants to acquire a lock on an object that is held by another transaction in a conflicting mode  it needs to wait until the lock is released lock waits are implemented through semaphores  each of which is associated with a unique transaction when waiting for a lock  a transaction actually waits on the semaphore associated with the transaction holding the lock once the lock holder releases the lock  it will signal the waiting transaction  s  through the semaphore by implementing lock waits on a per-lock-holder basis  rather than on a per-object basis  postgresql requires at most one semaphore per concurrent transaction  rather than one semaphore per lockable object deadlock detection in postgresql is based on time-outs by default  deadlock detection is triggered if a transaction has been waiting for a lock for more than 1 second the deadlock-detection algorithm constructs a wait-for graph based on the information in the lock table and searches this graph for circular dependencies if it finds any,meaning a deadlockwas detected  the transaction that triggered the deadlock detection aborts and returns an error to the user if no cycle is detected  the transaction continues waiting on the lock unlike some commercial systems  postgresql does not tune the lock time-out parameter dynamically  but it allows the administrator to tune it manually ideally  this parameter should be chosen on the order of a transaction lifetime  in order to optimize the trade-off between the time it takes to detect a deadlock and the work wasted for running the deadlock detection algorithm when there is no deadlock 27.4.1.6 locking and indices all current types of indices in postgresql allow for concurrent access by multiple transactions this is typically enabled by page-level locks  so that different transactions may access the index in parallel if they do not request conflicting locks on a page these locks are usually held for a short time to avoid deadlock  with the exception of hash indices  which lock pages for longer periods and may participate in deadlock 27.4.2 recovery historically  postgresql did not use write-ahead logging  wal  for recovery  and thereforewas not able to guarantee consistency in the case of crash a crash could thesumit67.blogspot.com 1146 chapter 27 postgresql potentially result in inconsistent index structures or  worse  totally corrupted table contents  because of partially written data pages.as a result  startingwith version 7.1  postgresql employs wal-based recovery the approach is similar to standard recovery techniques such as aries  section 16.8   but recovery in postgresql is simplified in some ways because of the mvcc protocol first  under postgresql  recovery doesn ? t have to undo the effects of aborted transactions  an aborting transaction makes an entry in the pg clog file  recording the fact that it is aborting consequently  all versions of rows it leaves behind will never be visible to any other transactions the only case where this approach could potentially lead to problems is when a transaction aborts because of a crash of the corresponding postgresql process and the postgresql process doesn ? t have a chance to create the pg clog entry before the crash postgresql handles this as follows  before checking the status of another transaction in the pg clog file  it checks whether the transaction is running on any of the postgresql processes if no postgresql process is currently running the transaction and the pg clog file shows the transaction as still running  it is safe to assume that the transaction crashed and the transaction ? s pg clog entry is updated to ? aborted ?  second  recovery is simplified by the fact that postgresql mvcc already keeps track of some of the information required by wal logging more precisely  there is no need for logging the start  commit  and abort of transactions  since mvcc logs the status of every transaction in the pg clog 27.5 storage and indexing postgresql ? s approach to data layout and storage is aimed at the goals of  1  a simple and clean implementation and  2  ease of administration.as a step toward these goals  postgresql relies on ? cooked ? file systems  instead of handling the physical layout of data on raw disk partitions by itself postgresql maintains a list of directories in the file hierarchy to use for storage  which are conventionally referred to as tablespaces eachpostgresql installation is initialized with a default tablespace  and additional tablespaces may be added at any time.when creating a table  index  or entire database  the user may specify any existing tablespace in which to store the related files it is particularly useful to create multiple tablespaces if they reside on different physical devices  so that the faster devices may be dedicated to data that are in higher demand moreover  data that are stored on separate disks may be accessed in parallel more efficiently the design of the postgresql storage system potentially leads to some performance limitations  due to clashes between postgresql and the file system the use of cooked file systems results in double buffering  where blocks are first fetched from disk into the file system ? s cache  in kernel space  before being copied to postgresql ? s buffer pool performance can also be limited by the fact that postgresql stores data in 8-kb blocks  which may not match the block size used by the kernel it is possible to change the postgresql block size when the server is installed  but this may have undesired consequences  small blocks limit thesumit67.blogspot.com 27.5 storage and indexing 1147 page header data linp1 linp2 linp3 linp4  linpn pd_lower pd_upper tuple2 tuple1 ? special space ? tuple3 tuplen  figure 27.9 slotted-page format for postgresql tables the ability of postgresql to store large tuples efficiently  while large blocks are wasteful when a small region of a file is accessed on the other hand  modern enterprises increasingly use external storage systems  such as network-attached storage and storage-area networks  instead of disks attached to servers the philosophy here is that storage is a service that is easily administered and tuned for performance separately one approach used by these systems is raid  which offers both parallelism and redundant storage as explained in section 10.3 postgresql may directly leverage these technologies because of its reliance on cooked file systems thus  the feeling of many postgresql developers is that  for a vast majority of applications  and indeed postgresql ? s audience  the performance limitations are minimal and justified by the ease of administration and management  as well as simplicity of implementation 27.5.1 tables the primary unit of storage in postgresql is a table in postgresql  tables are stored in heap files these files use a form of the standard slotted-page format described in section 10.5 the postgresql format is shown in figure 27.9 in each page  a header is followed by an array of ? line pointers ? a line pointer holds the offset  relative to the start of the page  and length of a specific tuple in the page the actual tuples are stored in reverse order of line pointers from the end of the page a record in a heap file is identified by its tuple id  tid   the tid consists of a 4-byte block id that specifies the page in the file containing the tuple and a 2-byte slot id the slot id is an index into the line pointer array that in turn is used to access the tuple although this infrastructure permits tuples in a page to be deleted or updated  under postgresql ? s mvcc approach  neither operation actually deletes or replaces old versions of rows immediately as explained in section 27.4.1.4  expired tuples may be physically deleted by later commands  causing holes to be formed in a thesumit67.blogspot.com 1148 chapter 27 postgresql page the indirection of accessing tuples through the line pointer array permits the reuse of such holes the length of a tuple is normally limited by the length of a data page this makes it difficult to store very long tuples when postgresql encounters such a large tuple  it tries to ? toast ? individual large attributes in some cases  toasting an attribute may be accomplished by compressing the value if this does not shrink the tuple enough to fit in the page  often the case   the data in the toasted attribute is replaced with a reference to a copy that is stored outside the page 27.5.2 indices a postgresql index is a data structure that provides a dynamic mapping from search predicates to sequences of tuple ids from a particular table the returned tuples are intended to match the search predicate  although in some cases the predicatemust be rechecked in the heap file postgresql supports several different index types  including those that are based on user-extensible access methods although an access method may use a different page format  all the indices available in postgresql use the same slotted-page format described above in section 27.5.1 27.5.2.1 index types postgresql supports the following types of indices  ? b-tree the default index type is a b + -tree index based onlehmanandyao ? s blink trees  b-link trees  described in section 15.10  support high concurrency of operations   these indices are useful for equality and range queries on sortable data and also for certain pattern-matching operations such as the like expression ? hash postgresql ? s hash indices are an implementation of linear hashing  for more information on hash indices  see section 11.6.3   such indices are useful only for simple equality operations the hash indices used by postgresqlhave been shown to have lookup performance no better than that of b-trees  but have considerably larger size and maintenance costs moreover  hash indices are the only indices in postgresql that do not support crash recovery thus it is almost always preferable to use b-tree indices instead of hash indices ? gist postgresql supports a highly extensible index called gist  or generalized search tree gist is a balanced  tree-structured access method that makes it easy for a domain expert who is well versed in a particular data type  such as image data  to develop performance-enhancing indices without having to deal with the internal details of the database system examples of some indices built using gist include b-trees and r-trees  as well as less conventional indices for multidimensional cubes and full-text search new gist access methods can be implemented by creating an operator class as explained in section 27.3.3.3 operator classes for gist are different from btrees  as each gist operator class may have a different set of strategies that thesumit67.blogspot.com 27.5 storage and indexing 1149 indicate the search predicates implemented by the index gist also relies on seven support functions for operations such as testing set membership and splitting sets of entries for page overflows it is interesting to note that the original postgresql implementation of r-trees  section 25.3.5.3  was replaced by gist operator classes in version 8.2 this allowed r-trees to take advantage of the wal logging and concurrency capabilities that were added to gist in version 8.1 since the original rtree implementation did not have these features  this change illustrates the benefits of an extensible indexing method see the bibliographical notes for references to more information on the gist index ? gin the newest type of index in postgresql is the generalized inverted index  gin   a gin index interprets both index keys and search keys as sets  making the index type appropriate for set-oriented operators one of the intended uses of gin is to index documents for full-text search  which is implemented by reducing documents and queries to sets of search terms like gist  a gin index may be extended to handle any comparison operator by creating an operator class with appropriate support functions to evaluate a search  gin efficiently identifies index keys that overlap the search key  and computes a bitmap indicating which searched-for elements are members of the index key this is accomplished using support functions that extract members from a set and compare individual members another support function decides if the search predicate is satisfied based on the bitmap and the original predicate if the search predicate can not be resolved without the full indexed attribute  the decision function must report a match and recheck the predicate in the heap file 27.5.2.2 other index variations for some of the index types described above  postgresql supports more complex variations such as  ? multicolumn indices these are useful for conjuncts of predicates over multiple columns of a table multicolumn indices are only supported for b-tree and gist indices ? unique indices unique and primary-key constraints can be enforced by using unique indices in postgresql only b-tree indices may be defined as being unique ? indices on expressions in postgresql  it is possible to create indices on arbitrary scalar expressions of columns  and not just specific columns  of a table this is especially useful when the expressions in question are ? expensive ? ? say  involving complicated user-defined computation an example is to support case-insensitive comparisons by defining an index on the expression lower  column  and using the predicate lower  column  = ? value ? in queries one disadvantage is that the maintenance costs of indices on expressions is high thesumit67.blogspot.com 1150 chapter 27 postgresql ? operator classes the specific comparison functions used to build,maintain  and use an index on a column are tied to the data type of that column each data type has associated with it a default ? operator class ?  described in section 27.3.3.3  that identifies the actual operators that would normally be used for it while this default operator class is normally sufficient for most uses  some data types might possess multiple ? meaningful ? classes for instance  in dealing with complex numbers  it might be desirable to index either the real or imaginary component postgresql provides some built-in operator classes for pattern-matching operations  such as like  on text data that do not use the standard locale-specific collation rules  in other words  language specific sort orders   ? partial indices these are indices built over a subset of a table defined by a predicate the index contains only entries for tuples that satisfy the predicate partial indices are suited for cases where a column might contain a large number of occurrences of a very small number of values in such cases  the common values are not worth indexing  since index scans are not beneficial for queries that require a large subset of the base table a partial index that excludes the common values is small and incurs less i/o the partial indices are less expensive to maintain  as a large fraction of inserts do not participate in the index 27.5.2.3 index construction an index may be added to the database using the create index command for example  the following ddl statement creates a b-tree index on instructor salaries create index inst sal idx on instructor  salary  ; this statement is executed by scanning the instructor relation to find row versions that might be visible to a future transaction  then sorting their index attributes and building the index structure during this process  the building transaction holds a lock on the instructor relation that prevents concurrent insert  delete  and update statements once the process is finished  the index is ready to use and the table lock is released the lock acquired by the create index command may present a major inconvenience for some applications where it is difficult to suspend updates while the index is built for these cases  postgresql provides the create index concurrently variant  which allows concurrent updates during index construction this is achieved by a more complex construction algorithm that scans the base table twice the first table scan builds an initial version of the index  in a way similar to normal index construction described above this index may be missing tuples if the table was concurrently updated ; however  the index is well formed  so it is flagged as being ready for insertions finally  the algorithm scans the table a second time and inserts all tuples it finds that still need to be indexed this scan may also miss concurrently updated tuples  but the algorithm synchronizes with other transactions to guarantee that tuples that are updated during the second thesumit67.blogspot.com 27.6 query processing and optimization 1151 scan will be added to the index by the updating transaction hence  the index is ready to use after the second table scan since this two-pass approach can be expensive  the plain create index command is preferred if it is easy to suspend table updates temporarily 27.6 query processing and optimization when postgresql receives a query  it is first parsed into an internal representation  which goes through a series of transformations  resulting in a query plan that is used by the executor to process the query 27.6.1 query rewrite the first stage of a query ? s transformation is rewrite and it is this stage that is responsible for the postgresql rules system as explained in section 27.3  in postgresql  users can create rules that are fired on different events such as update  delete  insert  and select statements a view is implemented by the system by converting a view definition into a select rule when a query involving a select statement on the view is received  the select rule for the view is fired  and the query is rewritten using the definition of the view a rule is registered in the system using the create rule command  at which point information on the rule is stored in the catalog this catalog is then used during query rewrite to uncover all candidate rules for a given query the rewrite phase first dealswith all update  delete  and insert statements by firing all appropriate rules such statements might be complicated and contain select clauses subsequently  all the remaining rules involving only select statements are fired since the firing of a rule may cause the query to be rewritten to a form that may require another rule to be fired  the rules are repeatedly checked on each form of the rewritten query until a fixed point is reached and no more rules need to be fired there exist no default rules in postgresql ? only those defined explicitly by users and implicitly by the definition of views 27.6.2 query planning and optimization once the query has been rewritten  it is subject to the planning and optimization phase here  each query block is treated in isolation and a plan is generated for it this planning begins bottom-up from the rewritten query ? s innermost subquery  proceeding to its outermost query block the optimizer in postgresql is  for the most part  cost based the idea is to generate an access plan whose estimated cost is minimal the cost model includes as parameters the i/o cost of sequential and random page fetches  as well as the cpu costs of processing heap tuples  index tuples  and simple predicates the actual process of optimization is based on one of the following two forms  thesumit67.blogspot.com 1152 chapter 27 postgresql ? standard planner the standard planner uses the the bottom-up dynamic programming algorithm for join order optimization  originally used in system r  the pioneering relational system developed by ibm research in the 1970s the system r dynamic programming algorithm is described in detail in section 13.4.1 the algorithm is used on a single query block at a time ? genetic query optimizer when the number of tables in a query block is very large  system r ? s dynamic programming algorithmbecomes very expensive unlike other commercial systems that default to greedy or rule-based techniques  postgresql uses a more radical approach  a genetic algorithm that was developed initially to solve traveling-salesman problems there exists anecdotal evidence of the successful use of genetic query optimization in production systems for queries with around 45 tables since the planner operates in a bottom-up fashion on query blocks  it is able to perform certain transformations on the query plan as it is being built one example is the common subquery-to-join transformation that is present in many commercial systems  usually implemented in the rewrite phase   when postgresql encounters a noncorrelated subquery  such as one caused by a query on a view   it is generally possible to ? pull up ? the planned subquery and merge it into the upper-level query block however  transformations that push duplicate elimination into lower-level query blocks are generally not possible in postgresql the query-optimization phase results in a query plan that is a tree of relational operators each operator represents a specific operation on one or more sets of tuples the operators can be unary  for example  sort  aggregation   binary  for example  nested-loop join   or n-ary  for example  set union   crucial to the cost model is an accurate estimate of the total number of tuples that will be processed at each operator in the plan this is inferred by the optimizer on the basis of statistics that are maintained on each relation in the system these indicate the total number of tuples for each relation and specific information on each column of a relation  such as the column cardinality  a list of most common values in the table and the number of occurrences  and a histogram that divides the column ? s values into groups of equal population  that is  an equi-depth histogram  described in section 13.3.1   in addition  postgresql also maintains a statistical correlation between the physical and logical row orderings of a column ? s values ? this indicates the cost of an index scan to retrieve tuples that pass predicates on the column the dba must ensure that these statistics are current by running the analyze command periodically 27.6.3 query executor the executor module is responsible for processing a query plan produced by the optimizer the executor follows the iterator model with a set of four functions implemented for each operator  open  next  rescan  and close   iterators are also discussed as part of demand-driven pipelining in section 12.7.2.1 postgresql iterators have an extra function  rescan  which is used to reset a subplan  say for an inner loop of a join  with parameters such as index key ranges thesumit67.blogspot.com 27.6 query processing and optimization 1153 some of the important operators of the executor can be categorized as follows  1 access methods the actual access methods that are used to retrieve data from on-disk objects in postgresql are sequential scans of heap files  index scans  and bitmap index scans ? sequential scans the tuples of a relation are scanned sequentially from the first to last blocks of the file each tuple is returned to the caller only if it is ? visible ? according to the transaction isolation rules in section 27.4.1.3 ? index scans given a search condition such as a range or equality predicate  this access method returns a set of matching tuples from the associated heap file the operator processes one tuple at a time  starting by reading an entry from the index and then fetching the corresponding tuple from the heap file this can result in a random page fetch for each tuple in the worst case ? bitmap index scans a bitmap index scan reduces the danger of excessive random page fetches in index scans this is achieved by processing tuples in two phases the first phase reads all index entries and stores the heap tuple ids in a bitmap  and the second phase fetches the matching heap tuples in sequential order this guarantees that each heap page is accessed only once  and increases the chance of sequential page fetches moreover  bitmaps from multiple indexes can be merged and intersected to evaluate complex boolean predicates before accessing the heap 2 join methods postgresql supports three join methods  sorted merge joins  nested-loop joins  including index-nested loop variants for the inner   and a hybrid hash join  section 12.5   3 sort external sorting is implemented in postgresql by algorithms explained in section 12.4 the input is divided into sorted runs that are then merged in a polyphase merge the initial runs are formed using replacement selection  using a priority tree instead of a data structure that fixes the number of inmemory records this is because postgresql may deal with tuples that vary considerably in size and tries to ensure full utilization of the configured sort memory space 4 aggregation grouped aggregation in postgresql can be either sort-based or hash-based when the estimated number of distinct groups is very large the former is used and otherwise the hash-based approach is preferred 27.6.4 triggers and constraints in postgresql  unlike some commercial systems  active-database features such as triggers and constraints are not implemented in the rewrite phase instead they are implemented as part of the query executor.when the triggers and constraints thesumit67.blogspot.com 1154 chapter 27 postgresql are registered by the user  the details are associated with the catalog information for each appropriate relation and index the executor processes an update  delete  andinsert statement by repeatedly generating tuple changes for a relation for each row modification  the executor explicitly identifies  fires  and enforces candidate triggers and constraints  before or after the change as required 27.7 system architecture the postgresql system architecture follows the process-per-transaction model a running postgresql site is managed by a central coordinating process  called the postmaster the postmaster process is responsible for initializing and shutting down the server and also for handling connection requests from new clients the postmaster assigns each new connecting client to a back-end server process that is responsible for executing the queries on behalf of the client and for returning the results to the client this architecture is depicted in figure 27.10 client applications can connect to the postgresql server and submit queries through one of the many database application programmer interfaces supported by postgresql  libpq  jdbc  odbc  perl dbd  that are provided as client-side libraries an example client application is the command-line psql program  included in the standard postgresql distribution the postmaster is responsible for handling the initial client connections for this  it constantly listens for new connections on a known port after performing initialization steps such as user authentication  the postmaster will spawn a new back-end server process to handle the new client after this initial connection  the client interacts only with the back-end server process  submitting queries and receiving query results this is the essence of the process-per-connection model adopted by postgresql the back-end server process is responsible for executing the queries submitted by the client by performing the necessary query-execution steps  including parsing  optimization  and execution each back-end server process can handle library interface client postmaster daemon process postgresql server sql queries  back end  and results read/ write shared tables shared disk buffers disk storage client processes client application server processes create disk buffers kernel shared memory unix system initial connection request and library api authentication through communication figure 27.10 the postgresql system architecture thesumit67.blogspot.com bibliographical notes 1155 only a single query at a time in order to execute more than one query in parallel  an application must maintain multiple connections to the server at any given time  there may be multiple clients connected to the system and thus multiple back-end server processes may be executing concurrently the back-end server processes access database data through the main-memory buffer pool  which is placed in shared memory  so that all the processes have the same view of the data shared memory is also used to implement other forms of synchronization between the server processes  for example  the locking of data items the use of shared memory as a communication medium suggests that a postgresql server should run on a single machine ; a single-server site can not be spread across multiple machines without the assistance of third-party packages  such as the slony replication tool.however  it is possible to build a shared-nothing parallel database system with an instance of postgresql running on each node ; in fact  several commercial parallel database systems have been built with exactly this architecture  as described in section 18.8 bibliographical notes there is extensive online documentation of postgresql at www.postgresql.org this web site is the authoritative source for information on new releases of postgresql  which occur on a frequent basis until postgresql version 8  the only way to run postgresql under microsoftwindows was by using cygwin cygwin is a linuxlike environment that allows rebuilding of linux applications from source to run underwindows details are at www.cygwin.com books on postgresql include douglas and douglas  2003  and stinson  2002   rules as used in postgresql are presented in stonebraker et al  1990   the gist structure is described in hellerstein et al  1995   many tools and extensions for postgresql are documented by the pgfoundry at www.pgfoundry.org these include the pgtcl library and the pgaccess administration tool mentioned in this chapter the pgadmin tool is described on theweb at www.pgadmin.org the database-design tools  tora and data architect  are described at tora.sourceforge.net and www.thekompany.com/products/dataarchitect  respectively the report-generation tools  gnu report generator and gnu enterprise  are described at www.gnu.org/software/grg and www.gnuenterprise.org  respectively the open-source mondrian olap server is described at mondrian.pentaho.org an open-source alternative to postgresql is mysql  which is available for noncommercial use under the gnu general public license mysql may be embedded in commercial software that does not have freely distributed source code  but this requires a special license to be purchased comparisons between themost recent versions of the two systems are readily available on theweb thesumit67.blogspot.com this page intentionally left blank thesumit67.blogspot.com chapter28 oracle hakan jakobsson when oracle was founded in 1977 as software development laboratories by larry ellison  bob miner  and ed oates  there were no commercial relational database products the company  which was later renamed oracle  set out to build a relational database management system as a commercial product  and became a pioneer of the rdbms market and has held a leading position in this market ever since over the years  its product and service offerings have grown beyond the relational database server to include middleware and applications in addition to products developed inside the company  oracle ? s offerings include software that was originally developed in companies that oracle acquired oracle ? s acquisitions have ranged from small companies to large  publicly traded ones  including peoplesoft  siebel  hyperion  and bea as a result of these acquisitions  oracle has a very broad portfolio of enterprise software products this chapter is focused on oracle ? s main relational database server and closely related products new versions of the products are being developed continually  so all product descriptions are subject to change the feature set described here is based on the first release of oracle11g  which is oracle ? s flagship database product 28.1 database design and querying tools oracle provides a variety of tools for database design  querying  report generation and data analysis  including olap these tools  along with various other application development tools  are part of a portfolio of software products called oracle fusion middleware products include both traditional tools using oracle ? s pl/sql programming language and newer ones based on java/j2ee technologies the software supports open standards such as soap  xml  bpel  and uml 28.1.1 database and application design tools the oracle application development framework  adf  is an end-to-end j2eebased development framework for a model-view-control design pattern in this 1157 thesumit67.blogspot.com 1158 chapter 28 oracle framework  an application consists of multiple layers the model and business services layers handle the interaction with the data sources and contains the business logic the viewlayer handles the user interface  and the controller layer handles the flow of the application and the interaction between the other layers the primary development tool for oracle adf is oracle jdeveloper  which provides an integrated development environment with support for java  xml  php  html  javascript  bpel  sql  andpl/sql development it has built-in support for uml modeling oracle designer is a database design tool,which translates business logic and data flows into schema definitions and procedural scripts for application logic it supports such modeling techniques as e-r diagrams  information engineering  and object analysis and design oracle also has an application development tool for data warehousing  oracle warehouse builder warehouse builder is a tool for design and deployment of all aspects of a data warehouse  including schema design  data mapping and transformations  data load processing  and metadata management oraclewarehouse builder supports both 3nf and star schemas and can also import designs from oracle designer this tool  in conjunction with database features  such as external tables and table functions  typically eliminates the need for third-party extraction  transformation  and loading  etl  tools 28.1.2 querying tools oracle provides tools for ad hoc querying  report generation  and data analysis  including olap oracle business intelligence suite  obi  is a comprehensive suite of tools sharing a common service-oriented architecture components include a business intelligence server and tools for ad hoc querying  dashboard generation  reporting  and alerting the components share infrastructure and services for data access and metadata management and have a common security model and administration tool the component for ad hoc querying  oracle bi answers  is an interactive tool that presents the user with a logical view of the data hiding the details of the physical implementation objects available to the user are displayed graphically and the user can build a query with a point-and-click interface this logical query is sent to the oracle bi server component,which then generates the physical query or queries multiple physical data sources are supported  and a query could combine data stored in relational databases  olap sources  and excel spreadsheets results can be presented as charts  reports  pivot tables  or dashboards that are drillable and can be saved and later modified 28.2 sql variations and extensions oracle supports all core sql  2003 features fully or partially  with the exception of features-and-conformance views in addition  oracle supports a large number of thesumit67.blogspot.com 28.2 sql variations and extensions 1159 other language constructs  some of which conform with optional features of sql foundation  2003  while others are oracle-specific in syntax or functionality 28.2.1 object-relational features oracle has extensive support for object-relational constructs  including  ? object types a single-inheritance model is supported for type hierarchies ? collection types oracle supports varrays  which are variable length arrays  and nested tables ? object tables these are used to store objects while providing a relational view of the attributes of the objects ? table functions these are functions that produce sets of rows as output  and can be used in the from clause of a query table functions in oracle can be nested if a table function is used to express some form of data transformation  nesting multiple functions allows multiple transformations to be expressed in a single statement ? object views these provide a virtual object table view of data stored in a regular relational table they allow data to be accessed or viewed in an objectoriented style even if the data are really stored in a traditional relational format ? methods these can be written in pl/sql  java  or c ? user-defined aggregate functions these can be used in sql statements in the same way as built-in functions such as sum and count 28.2.2 oracle xml db oracle xml db provides in-database storage for xml data and support for a broad set of xml functionality including xml schema and xquery it is built on the xmltype abstract data type  which is treated as a native oracle data type xml db provides several options for how data of this data type are stored in the database  including  ? structured in object-relational format this format is usually space efficient and allows the use of a variety of standard relational features  such as b-tree indices  but incurs some overhead when mapping xml documents to the storage format and back it is mainly suitable for xml data that are highly structured and the mapping includes a manageable number of relational tables and joins ? unstructured as a text string this representation does not require any mapping and provides high throughput when inserting or retrieving an entire xml document however  it is usually not very space efficient and provides for less intelligent processing when operating on parts of an xml document thesumit67.blogspot.com 1160 chapter 28 oracle ? binary xml storage this representation is a post-parse  xml schema-aware  binary format it is more space efficient than unstructured storage and can handle operations against parts of an xml document it is also better than the structured format at handling data that are highly unstructured  but may not always be as space efficient this format may make the processing of xquery statements less efficient than when the structured format is used both the binary and unstructured representation can be indexedwith a special type of index called xmlindex this type of index allows document fragments to be indexed based on their corresponding xpath expression storing xml data inside the database means that they get the benefit of oracle ? s functionality in areas such as backup  recovery  security  and query processing it allows for accessing relational data as part of doing xml processing as well as accessing xml data as part of doing sql processing some very high-level features of xml db include  ? support for the xquery language  w3c xquery 1.0 recommendation   ? an xslt process that lets xsl transformations be performed inside the database ? an xpath rewrite optimization that can speed up queries against data stored in object-relational representation by translating an expression used in an xquery into conditions directly on the object-relational columns  regular indices on these columns can be used to speed up query processing 28.2.3 procedural languages oracle has two main procedural languages  pl/sql and java pl/sql was oracle ? s original language for stored procedures and it has syntax similar to that used in the ada language java is supported through a java virtual machine inside the database engine oracle provides a package to encapsulate related procedures  functions  and variables into single units oracle supports sqlj  sql embedded in java  and jdbc  and provides a tool to generate java class definitions corresponding to user-defined database types 28.2.4 dimensions dimensional modeling is a commonly used design technique for relational star schemas as well as multidimensional databases oracle supports the creation of dimensions as metadata objects in order to support query processing against databases designed based on this technique the metadata objects can be used to store information about various kinds of attributes of a dimension  but perhaps more importantly  about hierarchical relationships see section 28.3.10 for examples thesumit67.blogspot.com 28.2 sql variations and extensions 1161 28.2.5 olap oracle provides support for analytical database processing in several different ways in addition to support for a rich set of analytical sql constructs  cube  rollup  grouping sets  window functions  etc   oracle provides native multidimensional storage inside the relational database server the multidimensional data structures allow for array-based access to the data  and  in the right circumstances  this type of access can be vastly more efficient than traditional relational access methods using these data structures as an integrated part of a relational database provides a choice of storing data in a relational or multidimensional format while still taking advantage of oracle features in areas such as backup and recovery  security  and administration tools oracle provides storage containers for multidimensional data known as analytic workspaces an analyticworkspace contains both the dimensional data and measures  or facts  of an olap cube and is stored inside an oracle table from a traditional relational perspective  a cube stored inside a tablewould be an opaque object where the data could not normally be interpreted directly in terms of the table ? s rows and columns however  oracle ? s olap server inside the database has the knowledge to interpret and access the data and makes it possible to give sql access to it as if it had been stored in a regular table format hence  it is possible to store data in either a multidimensional format or a traditional relational format  depending on what is optimal  and still be able to join data stored in both types of representations in a single sqlquery a materialized view can use either representation in addition to oracle ? s olap support inside its relational database  oracle ? s product suite includes essbase essbase is a widely used multidimensional database that came to be part of oracle with the acquisition of hyperion 28.2.6 triggers oracle provides several types of triggers and several options for when and how they are invoked  see section 5.3 for an introduction to triggers in sql  triggers can be written in pl/sql or java or as c callouts for triggers that execute on dml statements such as insert  update  and delete  oracle supports row triggers and statement triggers row triggers execute once for every row that is affected  updated or deleted  for example  by the dml operation a statement trigger is executed just once per statement in each case  the trigger can be defined as either a before or after trigger  depending on whether it is to be invoked before or after the dml operation is carried out oracle allows the creation of instead of triggers for views that can not be subject to dml operations depending on the view definition  it may not be possible for oracle to translate a dml statement on a view to modifications of the underlying base tables unambiguously hence  dml operations on views are subject to numerous restrictions a user can create an instead of trigger on a view to specify manually what operations on the base tables are to be carried out in response to a dml operation on the view oracle executes the trigger instead of the thesumit67.blogspot.com 1162 chapter 28 oracle dml operation and therefore provides a mechanism to circumvent the restrictions on dml operations against views oracle also has triggers that execute on a variety of other events  such as database start-up or shutdown  server error messages  user logon or logoff  and ddl statements such as create  alter and drop statements 28.3 storage and indexing in oracle parlance  a database consists of information stored in files and is accessed through an instance,which is a sharedmemory area and a set of processes that interact with the data in the files the control file is a small file that contains some very high-level metadata required to start or operate an instance the storage structure of the regular data and metadata is described in the next section 28.3.1 table spaces a database consists of one or more logical storage units called table spaces each table space  in turn  consists of one or more physical structures called data files these may be either part of a file system or raw devices usually  an oracle database has the following table spaces  ? the system and the auxiliary sysaux table spaces  which are always created they contain the data dictionary tables and storage for triggers and stored procedures ? table spaces created to store user data while user data can be stored in the system table space  it is often desirable to separate the user data from the system data usually  the decision about what other table spaces should be created is based on performance  availability  maintainability  and ease of administration for example  having multiple table spaces can be useful for partial backup and recovery operations ? the undo table space  which is used solely for storing undo information for transaction management and recovery ? temporary table spaces many database operations require sorting the data  and the sort routine may have to store data temporarily on disk if the sort can not be done in memory temporary table spaces are allocated for sorting and hashing to make the space management operations involved in spilling to disk more efficient table spaces can also be used as a means of moving data between databases for example  it is common to move data from a transactional system to a data warehouse at regular intervals oracle allows moving all the data in a table space from one system to the other by simply copying the data files and exporting and importing a small amount of data-dictionary metadata these operations can be much faster than unloading the data from one database and then using a loader thesumit67.blogspot.com 28.3 storage and indexing 1163 to insert it into the other this oracle feature is known as transportable table spaces 28.3.2 segments the space in a table space is divided into units  called segments  each of which contains data for a specific data structure there are four types of segments  ? data segments each table in a table space has its own data segment where the table data are stored unless the table is partitioned ; if so  there is one data segment per partition  partitioning in oracle is described in section 28.3.9  ? index segments each index in a table space has its own index segment  except for partitioned indices  which have one index segment per partition ? temporary segments these are segments used when a sort operation needs to write data to disk or when data are inserted into a temporary table ? undo segments these segments contain undo information so that an uncommitted transaction can be rolled back these segments are automatically allocated in a special undo table space they also play an important role in oracle ? s concurrency control model and for database recovery  described in sections 28.5.1 and 28.5.2 in older implementations of oracle ? s undo management  the term ? rollback segment ? was used below the level of segment  space is allocated at a level of granularity called an extent each extent consists of a set of contiguous database blocks a database block is the lowest level of granularity at which oracle performs disk i/o a database block does not have to be the same as an operating system block in size  but should be a multiple thereof oracle provides storage parameters that allow for detailed control of how space is allocated andmanaged  parameters such as  ? the size of a new extent that is to be allocated to provide roomfor rows that are inserted into a table ? the percentage of space utilization at which a database block is considered full and at which no more rows will be inserted into that block  leaving some free space in a block can allow the existing rows to grow in size through updates  without running out of space in the block  28.3.3 tables a standard table in oracle is heap organized ; that is  the storage location of a row in a table is not based on the values contained in the row  and is fixed when the row is inserted however  if the table is partitioned  the content of the row affects the partition in which it is stored there are several features and variations heap tables can optionally be compressed  as described in section 28.3.3.2 thesumit67.blogspot.com 1164 chapter 28 oracle oracle supports nested tables ; that is  a table can have a column whose data type is another table the nested table is not stored in line in the parent table  but is stored in a separate table oracle supports temporary tables where the duration of the data is either the transaction inwhich the data are inserted  or the user session the data are private to the session and are automatically removed at the end of its duration a cluster is another form of file organization for table data  described earlier in section 10.6.2where it is called multitable clustering the use of the term ? cluster ? in this context  should not be confused with other meanings of the word cluster  such as those relating to hardware architecture in a cluster file organization  rows from different tables are stored together in the same block on the basis of some common columns for example  a department table and an employee table could be clustered so that each row in the department table is stored together with all the employee rows for those employees who work in that department the primary key/foreign key values are used to determine the storage location the cluster organization implies that a row belongs in a specific place ; for example  a new employee row must be inserted with the other rows for the same department therefore  an index on the clustering column is mandatory an alternative organization is a hash cluster here  oracle computes the location of a row by applying a hash function to the value for the cluster column the hash function maps the row to a specific block in the hash cluster since no index traversal is needed to access a row according to its cluster column value  this organization can save significant amounts of disk i/o 28.3.3.1 index-organized tables in an index-organized table  iot   records are stored in an oracle b-tree index instead of in a heap ; this file organization is described earlier in section 11.4.1  where it is called b + -tree file organization an iot requires that a unique key be identified for use as the index key.while an entry in a regular index contains the key value and row-id of the indexed row  an iot replaces the row-id with the column values for the remaining columns of the row compared to storing the data in a regular heap table and creating an index on the key columns  using an iot can improve both performance and space utilization consider looking up all the column values of a row  given its primary key value for a heap table  that would require an index probe followed by a table access by row-id for an iot  only the index probe is necessary secondary indices on nonkey columns of an index-organized table are different from indices on a regular heap table in a heap table  each row has a fixed row-id that does not change however  a b-tree is reorganized as it grows or shrinks when entries are inserted or deleted  and there is no guarantee that a row will stay in a fixed place inside an iot hence  a secondary index on an iot contains not normal row-ids  but logical row-ids instead a logical row-id consists of two parts  a physical row-id corresponding to where the row was when the index was created or last rebuilt and a value for the unique key the physical row-id is referred to as a ? guess ? since it could be incorrect if the row has been thesumit67.blogspot.com 28.3 storage and indexing 1165 moved if so  the other part of a logical row-id  the key value for the row  is used to access the row ; however  this access is slower than if the guess had been correct  since it involves a traversal of the b-tree for the iot from the root all the way to the leaf nodes  potentially incurring several disk i/os however  if a table is highly volatile and a large percentage of the guesses are likely to be wrong  it can be better to create the secondary index with only key values  as described in section 11.4.1   since using an incorrect guess may result in a wasted disk i/o 28.3.3.2 compression oracle ? s compression feature allows data to be stored in a compressed format  something that can drastically reduce the amount of space needed to store the data and the number of i/o operations needed to retrieve it.oracle ? s compression method is a lossless dictionary-based algorithm that compresses each block individually all the information needed to uncompress a block is contained in that block itself the algorithm works by replacing repeated occurrences of a value in that blockwith pointers to an entry for that value in a symbol table  or dictionary  in the block entries can be based on repeated values for individual columns or a combination of columns oracle ? s original table compression generated the compressed block format as the data were bulk-loaded into a table and was mainly intended for data warehousing environments a newer oltp compression feature supports compression in conjunction with regular dml operations as well in the latter case  oracle compresses blocks only after certain thresholds have been reached for how much data have been written into the block as a result  only transactions that cause a threshold to be passed will occur any overhead for compressing a block 28.3.3.3 data security in addition to regular access control features such as passwords  user privileges  and user roles  oracle supports several features to protect the data from unauthorized access  including  ? encryption oracle can automatically store table data in an encrypted format and transparently encrypt and decrypt data using the aes or 3des algorithms encryption can be enabled for an entire database or just for individual table columns the main motivation for this feature is to protect sensitive data outside the normally protected environment  such as when backup media is sent to a remote location ? database vault this feature is aimed at providing a separation of duties for users with access to the database a database administrator is a highly privileged user that typically can do almost anythingwith the database.however  it may be inappropriate or illegal to let that person access sensitive corporate financial data or personal information about other employees database vault includes a variety ofmechanisms that can be used to restrict or monitor access to sensitive data by highly privileged database users thesumit67.blogspot.com 1166 chapter 28 oracle ? virtual private database this feature  described earlier in section 9.7.5  allows additional predicates to be automatically added to the where clause of a query that accesses a given table or view typically  the feature would be used so that the additional predicate filters out all the rows that the user does not have the right to see for example  two users could submit identical queries to find all the employee information in the entire employee table however  if a policy exists that limits each user to seeing only the information for the employee number that matches the user id  the automatically added predicateswill ensure that each query only returns the employee information for the user who submitted the query hence  each user will be left with the impression of accessing a virtual database that contains only a subset of the data of the physical database 28.3.4 indices oracle supports several different types of indices themost commonly used type is a b-tree index  created on one or multiple columns.note that in the terminology of oracle  as also in several other database systems  a b-tree index is what is referred to as a b + -tree index in chapter 11 index entries have the following format  for an index on columns col1  col2  and col3  each row in the table where at least one of the columns has a nonnull value would result in the index entry  < col1 > < col2 > < col3 > < row-id > where < coli > denotes the value for column i and < row-id > is the row-id for the row oracle can optionally compress the prefix of the entry to save space for example  if there are many repeated combinations of < col1 > < col2 > values  the representation of each distinct < col1 > < col2 > prefix can be shared between the entries that have that combination of values  rather than stored explicitly for each such entry prefix compression can lead to substantial space savings 28.3.5 bitmap indices bitmap indices  described in section 11.9  use a bitmap representation for index entries  which can lead to substantial space saving  and therefore disk i/o savings   when the indexed column has a moderate number of distinct values bitmap indices in oracle use the same kind of b-tree structure to store the entries as a regular index however  where a regular index on a column would have entries of the form < col1 > < row-id >  a bitmap index entry has the form  < col1 > < start row-id > < end row-id > < compressed bitmap > the bitmap conceptually represents the space of all possible rows in the table between the start and end row-id the number of such possible rows in a block depends on how many rows can fit into a block  which is a function of the number thesumit67.blogspot.com 28.3 storage and indexing 1167 of columns in the table and their data types each bit in the bitmap represents one such possible row in a block if the column value of that row is that of the index entry  the bit is set to 1 if the row has some other value  or the row does not actually exist in the table  the bit is set to 0  it is possible that the row does not actually exist because a table block may well have a smaller number of rows than the number that was calculated as the maximum possible  if the difference is large  the result may be long strings of consecutive zeros in the bitmap  but the compression algorithm deals with such strings of zeros  so the negative effect is limited the compression algorithm is a variation of a compression technique called byte-aligned bitmap compression  bbc   essentially  a section of the bitmap where the distance between two consecutive 1s is small enough is stored as verbatim bitmaps if the distance between two 1s is sufficiently large ? that is  there is a sufficient number of adjacent 0s between them ? a runlength of 0s  that is  the number of 0s  is stored bitmap indices allow multiple indices on the same table to be combined in the same access path if there are multiple conditions on indexed columns in the where clause of a query bitmaps from the different indices are retrieved and combined using boolean operations corresponding to the conditions in the where clause all boolean operations are performed directly on the compressed representation of the bitmaps ? no decompression is necessary ? and the resulting  compressed  bitmap represents those rows that match all the logical conditions the ability to use the boolean operations to combine multiple indices is not limited to bitmap indices oracle can convert row-ids to the compressed bitmap representation  so it can use a regular b-tree index anywhere in a boolean tree of bitmap operation simply by putting a row-id-to-bitmap operator on top of the index access in the execution plan as a rule of thumb  bitmap indices tend to be more space efficient than regular b-tree indices if the number of distinct key values is less than half the number of rows in the table for example  in a table with 1 million rows  an index on a column with less than 500,000 distinct values would probably be smaller if it were created as a bitmap index for columns with a very small number of distinct values ? for example  columns referring to properties such as country  state  gender  marital status  and various status flags ? a bitmap index might require only a small fraction of the space of a regular b-tree index any such space advantage can also give rise to corresponding performance advantages in the form of fewer disk i/os when the index is scanned 28.3.6 function-based indices in addition to creating indices on one or multiple columns of a table  oracle allows indices to be created on expressions that involve one or more columns  such as col1 + col2 * 5 for example  by creating an index on the expression upper  name   where upper is a function that returns the uppercase version of a string  and name is a column  it is possible to do case-insensitive searches on the name column in order to find all rows with name ? van gogh ? efficiently  the condition  thesumit67.blogspot.com 1168 chapter 28 oracle upper  name  = ? van gogh ? would be used in the where clause of the query oracle thenmatches the condition with the index definition and concludes that the index can be used to retrieve all the rowsmatching ? van gogh ? regardless of how the name was capitalized when it was stored in the database a function-based index can be created as either a bitmap or a b-tree index 28.3.7 join indices a join index is an index where the key columns are not in the table that is referenced by the row-ids in the index oracle supports bitmap join indices primarily for use with star schemas  see section 20.2.2   for example  if there is a column for product names in a product dimension table  a bitmap join index on the fact table with this key column could be used to retrieve the fact table rows that correspond to a product with a specific name  although the name is not stored in the fact table how the rows in the fact and dimension tables correspond is based on a join condition that is specified when the index is created  and becomes part of the index metadata when a query is processed  the optimizer will look for the same join condition in the where clause of the query in order to determine if the join index is applicable oracle can combine a bitmap join index on a fact table with other indices on the same table ? whether join indices or not ? by using the operators for boolean bitmap operations 28.3.8 domain indices oracle allows tables to be indexed by index structures that are not native to oracle this extensibility feature of the oracle server allows software vendors to develop so-called cartridges with functionality for specific application domains  such as text  spatial data  and images  with indexing functionality beyond that provided by the standard oracle index types in implementing the logic for creating  maintaining  and searching the index  the index designer must ensure that it adheres to a specific protocol in its interaction with the oracle server a domain index must be registered in the data dictionary  together with the operators it supports oracle ? s optimizer considers domain indices as one of the possible access paths for a table oracle allows cost functions to be registeredwith the operators so that the optimizer can compare the cost of using the domain index to those of other access paths for example  a domain index for advanced text searches may support an operator contains once this operator has been registered  the domain index will be considered as an access path for a query like  select * from employees where contains  resume  ? linux ?  ; thesumit67.blogspot.com 28.3 storage and indexing 1169 where resume is a text column in the employee table the domain index can be stored in either an external data file or inside an oracle index-organized table a domain index can be combined with other  bitmap or b-tree  indices in the same access path by converting between the row-id and bitmap representation and using boolean bitmap operations 28.3.9 partitioning oracle supports various kinds of horizontal partitioning of tables and indices  and this feature plays a major role in oracle ? s ability to support very large databases the ability to partition a table or index has advantages in many areas ? backup and recovery are easier and faster  since they can be done on individual partitions rather than on the table as a whole ? loading operations in a data warehousing environment are less intrusive  data can be added to a newly created partition  and then the partition added to a table,which is an instantaneous operation likewise  dropping a partition with obsolete data froma table is very easy in a data warehouse that maintains a rolling window of historical data ? query performance benefits substantially  since the optimizer can recognize that only a subset of the partitions of a table need to be accessed in order to resolve a query  partition pruning   also  the optimizer can recognize that in a join  it is not necessary to try to match all rows in one table with all rows in the other  but that the joins need to be done only between matching pairs of partitions  partitionwise join   an index on a partitioned table can be either a global index or a local index entries in a global index can refer to rows in any partition a locally indexed table has one physical index for each partition that only contains entries for that partition unless partition pruning restricts a query to a single partition  a table accessed through a local index will require multiple individual physical index probes.however  a local index has advantages in datawarehousing environments where new data can be loaded into a new partition and indexedwithout the need to maintain any existing index  loading followed by index creation is much more efficient than maintaining an existing index while the data are being loaded  similarly  dropping an old partition and the physical part of its local index can be done without causing any index maintenance each row in a partitioned table is associated with a specific partition this association is based on the partitioning column or columns that are part of the definition of a partitioned table there are several ways to map column values to partitions  giving rise to several types of partitioning  each with different characteristics  range  hash  list  and composite partitioning thesumit67.blogspot.com 1170 chapter 28 oracle 28.3.9.1 range partitioning in range partitioning  the partitioning criteria are ranges of values this type of partitioning is especially well suited to date columns  in which case all rows in the same date range  say a day or a month  belong in the same partition in a data warehouse where data are loaded from the transactional systems at regular intervals  range partitioning can be used to implement a rolling window of historical data efficiently each data load gets its own new partition  making the loading process faster and more efficient the system actually loads the data into a separate table with the same column definition as the partitioned table it can then check the data for consistency  cleanse them  and index them after that  the system can make the separate table a new partition of the partitioned table  by a simple change to the metadata in the data dictionary ? a nearly instantaneous operation up until the metadata change  the loading process does not affect the existing data in the partitioned table in any way there is no need to do any maintenance of existing indices as part of the loading old data can be removed from a table by simply dropping its partition ; this operation does not affect the other partitions in addition  queries in a data warehousing environment often contain conditions that restrict them to a certain time period  such as a quarter or month if date-range partitioning is used  the query optimizer can restrict the data access to those partitions that are relevant to the query  and avoid a scan of the entire table partitions can either be created with explicitly set end points or be defined based on a fixed range  such as a day or amonth in the latter case  called interval partitioning  the creation of the partition happens automatically under the covers when trying to insert a row with a value in a previously nonexistent interval 28.3.9.2 hash partitioning in hash partitioning  a hash function maps rows to partitions according to the values in the partitioning columns this type of partitioning is primarily useful when it is important to distribute the rows evenly among partitions or when partitionwise joins are important for query performance 28.3.9.3 list partitioning in list partitioning  the values associated with a particular partition are stated in a list this type of partitioning is useful if the data in the partitioning column have a relatively small set of discrete values for instance  a table with a state column can be implicitly partitioned by geographical region if each partition list has the states that belong in the same region 28.3.9.4 composite partitioning in composite partitioning  tables that are range  interval  or list partitioned can be subpartitioned by range  list  or hash for example  a table may be range partitioned on a date column and hash subpartitioned on a column that is frequently thesumit67.blogspot.com 28.3 storage and indexing 1171 used as a join column the subpartitioning allows partition-wise joins to be used when the table is joined 28.3.9.5 reference partitioning in reference partitioning  the partitioning key is resolved based on a foreignkey constraint with another table the dependency between the tables allows maintenance operations to be automatically cascaded 28.3.10 materialized views thematerialized-view feature  see section 4.2.3  allows the result of an sql query to be stored in a table and used for later query processing in addition  oracle maintains the materialized result  updating itwhen the tables thatwere referenced in the query are updated materialized views are used in data warehousing to speed up query processing  but the technology is also used for replication in distributed and mobile environments in datawarehousing  a common usage for materialized views is to summarize data for example  a common type of query asks for ? the sum of sales for each quarter during the last 2 years ? precomputing the result  or some partial result  of such a query can speed up query processing dramatically compared to computing it from scratch by aggregating all detail-level sales records oracle supports automatic query rewrites that take advantage of any useful materialized view when resolving a query the rewrite consists of changing the query to use the materialized view instead of the original tables in the query in addition  the rewrite may add additional joins or aggregate processing as may be required to get the correct result for example  if a query needs sales by quarter  the rewrite can take advantage of a view that materializes sales by month  by adding additional aggregation to roll up the months to quarters oracle has a type of metadata object called dimension that allows hierarchical relationships in tables to be defined for example  for a time-dimension table in a star schema  oracle can define a dimension metadata object to specify how days roll up to months  months to quarters  quarters to years  and so forth likewise  hierarchical properties relating to geography can be specified ? for example  how sales districts roll up to regions the query rewrite logic looks at these relationships since they allow a materialized view to be used for wider classes of queries the container object for a materialized view is a table  which means that a materialized view can be indexed  partitioned  or subjected to other controls  to improve query performance when there are changes to the data in the tables referenced in the query that defines a materialized view  the materialized view must be refreshed to reflect those changes oracle supports both complete refresh of a materialized view and fast  incremental refresh in a complete refresh  oracle recomputes the materialized view from scratch  which may be the best option if the underlying tables have had significant changes  for example  changes due to a bulk load in a fast refresh  oracle updates the view using the records that were changed in the underlying tables the refresh to the view can be executed on commit as part of thesumit67.blogspot.com 1172 chapter 28 oracle the transaction that changed the underlying tables or at some later point in time on demand fast refresh may be better if the number of rows that were changed is low there are some restrictions on the classes of queries for which a materialized view can be incrementally refreshed  and others for when a materialized view can be created at all   a materialized view is similar to an index in the sense that  while it can improve query performance  it uses up space  and creating and maintaining it consumes resources to help resolve this trade-off  oracle provides an advisor that can help a user create the most cost-effective materialized views  given a particular query workload as input 28.4 query processing and optimization oracle supports a large variety of processing techniques in its query processing engine some of the more important ones are described here briefly 28.4.1 execution methods data can be accessed through a variety of access methods  ? full table scan the query processor scans the entire table by getting information about the blocks that make up the table from the extent map  and scanning those blocks ? index scan the processor creates a start and/or stop key from conditions in the query and uses it to scan to a relevant part of the index if there are columns that need to be retrieved  that are not part of the index  the index scan would be followed by a table access by index row-id if no start or stop key is available  the scan would be a full index scan ? index fast full scan the processor scans the extents the same way as the table extent in a full table scan if the index contains all the table columns that are needed for that table  and there are no good start/stop keys that would significantly reduce that portion of the index that would be scanned in a regular index scan  this method may be the fastest way to access the data this is because the fast full scan can take full advantage of multiblock disk i/o however  unlike a regular full scan  which traverses the index leaf blocks in order  a fast full scan does not guarantee that the output preserves the sort order of the index ? index join if a query needs only a small subset of the columns of a wide table  but no single index contains all those columns  the processor can use an index join to generate the relevant informationwithout accessing the table  by joining several indices that together contain the needed columns it performs the joins as hash joins on the row-ids from the different indices thesumit67.blogspot.com 28.4 query processing and optimization 1173 ? cluster and hash cluster access the processor accesses the data by using the cluster key oracle has several ways to combine information from multiple indices in a single access path this ability allows multiple where-clause conditions to be used together to compute the result set as efficiently as possible the functionality includes the ability to perform boolean operations and  or  andminus on bitmaps representing row-ids there are also operators that map a list of row-ids into bitmaps and vice versa,which allows regular b-tree indices and bitmap indices to be used together in the same access path in addition  for many queries involving count  *  on selections on a table  the result can be computed by just counting the bits that are set in the bitmap generated by applying the where clause conditions  without accessing the table oracle supports several types of joins in the execution engine  inner joins  outer joins  semijoins  and antijoins  an antijoin in oracle returns rows from the left-hand side input that do not match any row in the right-hand side input ; this operation is called anti-semijoin in other literature  it evaluates each type of join by one of three methods  hash join  sort ? merge join  or nested-loop join 28.4.2 optimization chapter 13 discusses the general topic of query optimization here  we discuss optimization in the context of oracle 28.4.2.1 query transformations oracle does query optimization in several steps one such step is to perform various query transformations and rewrites that fundamentally change the structure of the query another step is to perform access path selection to determine access paths  join methods  and join order since some transformations are not always beneficial  oracle uses cost-based query transformations where the transformations and access path selection are interleaved for each transformation that is tried  access path selection is performed in order to generate a cost estimate  and the transformation is accepted or rejected based on the cost for the resulting execution plan some of the major types of transformations and rewrites supported by oracle are as follows  ? view merging.aview reference in a query is replaced by the view definition this transformation is not applicable to all views ? complex viewmerging oracle offers this feature for certain classes of views that are not subject to regular view merging because they have a group by or select distinct in the view definition if such a view is joined to other tables  oracle can commute the joins and the sort or hash operation used for the group by or distinct thesumit67.blogspot.com 1174 chapter 28 oracle ? subquery flattening oracle has a variety of transformations that convert various classes of subqueries into joins  semijoins  or antijoins such conversion is also called decorrelation  and is described briefly in section 13.4.4 ? materialized view rewrite oracle has the ability to rewrite a query automatically to take advantage ofmaterialized views if some part of the query can be matched up with an existing materialized view  oracle can replace that part of the query with a reference to the table in which the view is materialized if need be  oracle adds join conditions or group by operations to preserve the semantics of the query if multiple materialized views are applicable  oracle picks the one that gives the greatest advantage in reducing the amount of data that have to be processed in addition  oracle subjects both the rewritten query and the original version to the full optimization process producing an execution plan and an associated cost estimate for each oracle then decides whether to execute the rewritten or the original version of the query on the basis of the cost estimates ? star transformation oracle supports a technique for evaluating queries against star schemas  known as the star transformation when a query contains a join of a fact table with dimension tables  and selections on attributes from the dimension tables  the query is transformed by deleting the join condition between the fact table and the dimension tables  and replacing the selection condition on each dimension table by a subquery of the form  fact table.fki in  select pk from dimension tablei where < conditions on dimension tablei >  one such subquery is generated for each dimension that has some constraining predicate if the dimension has a snowflake schema  see section 20.2   the subquery will contain a join of the applicable tables that make up the dimension oracle uses the values that are returned from each subquery to probe an index on the corresponding fact table column  getting a bitmap as a result the bitmaps generated from different subqueries are combined by a bitmap and operation the resultant bitmap can be used to access matching fact table rows hence  only those rows in the fact table that simultaneously match the conditions on the constrained dimensions will be accessed both the decision on whether the use of a subquery for a particular dimension is cost-effective  and the decision on whether the rewritten query is better than the original  are based on the optimizer ? s cost estimates 28.4.2.2 access path selection oracle has a cost-based optimizer that determines join order  join methods  and access paths each operation that the optimizer considers has an associated cost thesumit67.blogspot.com 28.4 query processing and optimization 1175 function  and the optimizer tries to generate the combination of operations that has the lowest overall cost in estimating the cost of an operation  the optimizer relies on statistics that have been computed for schema objects such as tables and indices the statistics contain information about the size of the object  the cardinality  the data distribution of table columns  and so forth oracle supports height-balanced and frequency histograms for data distributions height-balanced histograms are also referred to as equi-depth histograms  and are described in section 13.3.1 to facilitate the collection of optimizer statistics  oracle can monitor modification activity on tables and keep track of those tables that have been subject to enough changes that recalculating the statistics may be appropriate oracle also tracks what columns are used in where clauses of queries  which makes them potential candidates for histogram creation with a single command  a user can tell oracle to refresh the statistics for those tables that were marked as sufficiently changed oracle uses sampling to speed up the process of gathering the new statistics and automatically chooses the smallest adequate sample percentage it also determines whether the distribution of the marked columns merits the creation of histograms ; if the distribution is close to uniform  oracle uses a simpler representation of the column statistics in some cases  it may be impossible for the optimizer to accurately estimate the selectivity of a condition in the where clause of a query just based on simple column statistics for example  the condition may be an expression involving a column  such as f  col + 3  > 5 another class of problematic queries is those that have multiple predicates on columns that have some form of correlation assessing the combined selectivity of those predicates may be hard oracle therefore allows statistics to be created for expressions as well as for groups of columns in addition  oracle can address these issues through dynamic sampling the optimizer can randomly sample a small portion of a table and apply all the relevant predicates to the sample to see the percentage of the rows that match this feature can also handle temporary tables where the lifespan and visibility of the data may prevent regular statistics collection oracle uses both cpu cost and disk i/os in the optimizer cost model to balance the two components  it stores measures about cpu speed and disk i/o performance as part of the optimizer statistics oracle ? s package for gathering optimizer statistics computes these measures for queries involving a nontrivial number of joins  the search space is an issue for a query optimizer oracle addresses this issue in several ways the optimizer generates an initial join order and then decides on the best joinmethods and access paths for that join order it then changes the order of the tables and determines the best join methods and access paths for the new join order and so forth  while keeping the best plan that has been found so far oracle cuts the optimization short if the number of different join orders that have been considered becomes so large that the time spent in the optimizer may be noticeable compared to the time it would take to execute the best plan found so far since this cutoff depends on the cost estimate for the best plan found so far  finding a good plan early is important so that the optimization can be stopped after a smaller number of thesumit67.blogspot.com 1176 chapter 28 oracle join orders  resulting in better response time oracle uses several initial ordering heuristics to increase the likelihood that the first join order considered is a good one for each join order that is considered  the optimizer may make additional passes over the tables to decide join methods and access paths such additional passes would target specific global side effects of the access path selection for instance  a specific combination of join methods and access paths may eliminate the need to perform an order by sort since such a global side effect may not be obvious when the costs of the different join methods and access paths are considered locally  a separate pass targeting a specific side effect is used to find a possible execution plan with a better overall cost 28.4.2.3 partition pruning for partitioned tables  the optimizer tries to match conditions in the where clause of a query with the partitioning criteria for the table  in order to avoid accessing partitions that are not needed for the result for example  if a table is partitioned by date range and the query is constrained to data between two specific dates  the optimizer determines which partitions contain data between the specified dates and ensures that only those partitions are accessed this scenario is very common  and the speedup can be dramatic if only a small subset of the partitions are needed 28.4.2.4 sql tuning advisor in addition to the regular optimization process  oracle ? s optimizer can be used in tuning mode as part of the sql tuning advisor in order to generate more efficient execution plans than it normally would this feature is especially useful for packaged applications that generate the same set of sql statements repeatedly so that effort to tune these statements for performance can have future benefits oracle monitors the database activity and automatically stores information about high-load sql statements in aworkload repository ; see section 28.8.2 highload sql statements are those that use up the most resources because they are executed a very large number of times or because each execution is very expensive such statements are logical candidates for tuning since their impact on the system is the greatest the sql tuning advisor can be used to improve the performance of these statements by making making various kinds of recommendations that fall into the following different categories  ? statistics analysis oracle checks whether statistics needed by the optimizer are missing or stale and makes recommendations for collecting them ? sql profiling a profile for an sql statement is a set of information that is intended to help the optimizer make better decisions the next time the statement is optimized.anoptimizer can sometimes generate inefficient execution plans if it is unable to accurately estimate cardinalities and selectivities  something that can happen because of data correlation or the use of certain types thesumit67.blogspot.com 28.4 query processing and optimization 1177 of constructs.when running the optimizer in tuning mode to create a profile  the optimizer tries to verify that its assumptions are correct using dynamic sampling and partial evaluation of the sql statement if it finds that there are steps in the optimization process where the optimizer ? s assumptions are wrong  it will generate a correction factor for that step that will become part of the profile optimizing in tuning mode can be very time-consuming  but it can be worthwhile if the use of the profile significantly improves the performance of the statement if a profile is created  itwill be stored persistently and used whenever the statement is optimized in the future profiles can be used to tune sql statements without changing the text of the statement  something that is important since it is often impossible for the database administrator to modify statements generated by an application ? access path analysis based on analysis by the optimizer  oracle suggests the creation of additional indices that could speed up the statement ? sql structure analysis oracle suggests changes in the structure of the sqlstatement that would allow for more efficient execution 28.4.2.5 sql plan management packaged applications often generate a large number of sql statements that are executed repeatedly if the application is performing adequately  it is common that database administrators are averse to changes in database behavior if the change results in better performance  there is limited perceived upside since the performance was already good enough on the other hand  if the change leads to a performance degradation  it may break an application if a critical query deteriorates to a response time that is unacceptable an example of a change of behavior is a change of an execution plan for a query such a change may be a perfectly legitimate reflection of changes to properties of the data  such as a table having grown much larger but the change could also be an unintended consequence of a number of other actions  such as a change in the routines for collecting optimizer statistics or an upgrade to a new version of the rdbms with new optimizer behavior oracle ? s sql plan management feature addresses the risk associated with execution plan changes by maintaining a set of trusted execution plans for a workload and phasing in plans changed by the query optimizer only after they have been verified not to cause any performance degradations the feature has three major components  1 sql plan baseline capture oracle can capture execution plans for a workload and store a plan history for each sql statement the plan baseline is a set of plans for a workload with trusted performance characteristics and against which future plan changes can be compared a statement could have more than one baseline plan 2 sql plan baseline selection after the optimizer generates a plan for an sql statement  it checks whether there exists a baseline plan for the statement thesumit67.blogspot.com 1178 chapter 28 oracle if the statement exists in the baseline but the new plan is different from any existing one  the baseline plan that the optimizer considers to be the best will be used the newly generated plan will be added to the plan history for the statement and could become part of a future baseline 3 sql plan baseline evolution periodically  it may make sense to try to make newly generated execution plans part of the trusted plans in the baseline oracle supports adding new plans to the baseline with or without verification if verification is the chosen option  oracle will execute a newly generated plan and compare its performance to the baseline in order to make sure it does not cause performance regressions 28.4.3 parallel execution oracle allows the execution of a single sqlstatement to be parallelized by dividing the work between multiple processes on a multiprocessor computer this feature is especially useful for computationally intensive operations thatwould otherwise take an unacceptably long time to perform representative examples are decision support queries that need to process large amounts of data  data loads in a data warehouse  and index creation or rebuild in order to achieve good speedup through parallelism  it is important that the work involved in executing the statement be divided into granules that can be processed independently by the different parallel processors depending on the type of operation  oracle has several ways to split up the work for operations that access base objects  tables and indices   oracle can divide the work by horizontal slices of the data for some operations  such as a full table scan  each such slice can be a range of blocks ? each parallel query process scans the table from the block at the start of the range to the block at the end for some operations on a partitioned table  such as an index range scan  the slice would be a partition parallelism based on block ranges is more flexible since these can be determined dynamically based on a variety of criteria and are not tied to the table definition joins can be parallelized in several differentways oneway is to divide one of the inputs to the join between parallel processes and let each process join its slice with the other input to the join ; this is the asymmetric fragment-and-replicate method of section 18.5.2.2 for example  if a large table is joined to a small one by a hash join  oracle divides the large table among the processes and broadcasts a copy of the small table to each process  which then joins its slice with the smaller table if both tables are large  itwould be prohibitively expensive to broadcast one of them to all processes in that case  oracle achieves parallelism by partitioning the data among processes by hashing on the values of the join columns  the partitioned hash-join method of section 18.5.2.1   each table is scanned in parallel by a set of processes and each row in the output is passed on to one of a set of processes that are to perform the join which one of these processes gets the row is determined by a hash function on the values of the join column hence  each join process gets only rows that could potentially match  and no rows that could match could end up in different processes thesumit67.blogspot.com 28.4 query processing and optimization 1179 oracle parallelizes sort operations by value ranges of the column on which the sort is performed  that is  using the range-partitioning sort of section 18.5.1   each process participating in the sort is sent rows with values in its range  and it sorts the rows in its range to maximize the benefits of parallelism  the rows need to be divided as evenly as possible among the parallel processes  and the problem of determining range boundaries that generates a good distribution then arises oracle solves the problem by dynamically sampling a subset of the rows in the input to the sort before deciding on the range boundaries 28.4.3.1 process structure the processes involved in the parallel execution of an sql statement consist of a coordinator process and a number of parallel server processes the coordinator is responsible for assigning work to the parallel servers and for collecting and returning data to the user process that issued the statement the degree of parallelism is the number of parallel server processes that are assigned to execute a primitive operation as part of the statement the degree of parallelism is determined by the optimizer  but can be throttled back dynamically if the load on the system increases the parallel servers operate on a producer/consumer model when a sequence of operations is needed to process a statement  the producer set of servers performs the first operation and passes the resulting data to the consumer set for example  if a full table scan is followed by a sort and the degree of parallelism is 32  there would be 32 producer servers performing the table scan and passing the result to 32 consumer servers that perform the sort if a subsequent operation is needed  such as another sort  the roles of the two sets of servers switch the servers that originally performed the table scan take on the role of consumers of the output produced by the first sort and use it to perform the second sort hence  a sequence of operations proceeds by passing data back and forth between two sets of servers that alternate in their roles as producers and consumers the servers communicate with each other through memory buffers on shared-memory hardware and through high-speed network connections on mpp  shared nothing  configurations and clustered  shared disk  systems for shared-nothing systems  the cost of accessing data on disk is not uniform among processes a process running on a node that has direct access to a device is able to process data on that device faster than a process that has to retrieve the data over a network oracle uses knowledge about device-to-node and device-toprocess affinity ? that is  the ability to access devices directly ? when distributing work among parallel execution servers 28.4.4 result caching oracle ? s result caching feature allows the result of a query or query block  e.g  a view referenced in a query  to be cached in memory and reused if the same query is executed again updates of the data in the underlying tables invalidate the cached results  so this feature works best for queries against tables that are relatively static and where the result sets are relatively small consider  as a thesumit67.blogspot.com 1180 chapter 28 oracle usage example  some part of a web page that is stored in the database and does not change very frequently compared to how often it accessed for such an application  result cachingwould be amuchmore lightweight alternative to using materialized views  which would require explicitly creating and administering new persistent database objects 28.5 concurrency control and recovery oracle supports concurrency control and recovery techniques that provide a number of useful features 28.5.1 concurrency control oracle ? s multiversion concurrency control mechanism is based on the snapshot isolation protocol described in section 15.7 read-only queries are given a readconsistent snapshot  which is a view of the database as it existed at a specific point in time  containing all updates that were committed by that point in time  and not containing any updates that were not committed at that point in time thus  read locks are not used and read-only queries do not interfere with other database activity in terms of locking oracle supports both statement and transaction-level read consistency  at the beginning of the execution of either a statement or a transaction  depending on what level of consistency is used   oracle determines the current system change number  scn   the scn essentially acts as a timestamp  where the time is measured in terms of transaction commits instead of wall-clock time if in the course of a query a data block is found that has a higher scn than the one being associated with the query  it is evident that the data block has been modified after the time of the original query ? s scn by some other transaction that may ormay not have committed hence  the data in the block can not be included in a consistent view of the database as it existed at the time of the query ? s scn instead  an older version of the data in the block must be used ; specifically  the one that has the highest scn that does not exceed the scn of the query oracle retrieves that version of the data from the undo segment  undo segments are described in section 28.5.2   hence  provided that the undo space is sufficiently large  oracle can return a consistent result of the query even if the data items have been modified several times since the query started execution should the block with the desired scn no longer exist in the undo  the query will return an error it would be an indication that the undo table space has not been properly sized  given the activity on the system in the oracle concurrency model  read operations do not block write operations and write operations do not block read operations  a property that allows a high degree of concurrency in particular  the scheme allows for long-running queries  for example  reporting queries  to run on a system with a large amount of transactional activity this kind of scenario is often problematic for database systems where queries use read locks  since the query may either fail to acquire thesumit67.blogspot.com 28.5 concurrency control and recovery 1181 them or lock large amounts of data for a long time  thereby preventing transactional activity against that data and reducing concurrency  an alternative that is used in some systems is to use a lower degree of consistency  such as degree-two consistency  but that could result in inconsistent query results  oracle ? s concurrency model is used as a basis for the flashback feature this feature allows a user to set a certain scn number or wall-clock time in his session and perform operations on the data that existed at that point in time  provided that the data still exist in the undo   normally in a database system  once a change has been committed  there is no way to get back to the previous state of the data other than performing point-in-time recovery from backups however  recovery of a very large database can be very costly  especially if the goal is just to retrieve some data item that had been inadvertently deleted by a user the flashback feature provides a much simpler mechanism to deal with user errors the flashback feature includes the ability to restore a table or an entire database to an earlier point in time without recovering from backups  the ability to perform queries on the data as they existed at an earlier point in time  the ability to track how one or more rows have changed over time  and the ability to examine changes to the database at the transaction level it may be desirable to be able to track changes to a table beyond what would be possible through normal undo retention  for instance  corporate governance regulations may require that such changes be trackable for a certain number of years  for this purpose  a table can be tracked by the flashback archive feature  which creates an internal  history version of the table a background process converts the undo information into entries in the history table  which can be used to provide flashback functionality for arbitrarily long periods of time oracle supports two ansi/iso isolation levels  read committed and serializable there is no support for dirty reads since it is not needed statement-level read consistency corresponds to the read committed isolation level  while transactionlevel read consistency corresponds to the serializable isolation level the isolation level can be set for a session or an individual transaction statement-level read consistency  that is  read committed  is the default oracle uses row-level locking updates to different rows do not conflict if two writers attempt to modify the same row  one waits until the other either commits or is rolled back  and then it can either return a write-conflict error or go ahead and modify the row ; write-conflict errors are detected based on the first-updaterwins version of snapshot isolation  described in section 15.7  section 15.7 also describes certain cases of non-serializable execution that can occur with snapshot isolation  and outlines techniques for preventing such problems  locks are held for the duration of a transaction in addition to row-level locks that prevent inconsistencies due to dml activity  oracle uses table locks that prevent inconsistencies due to ddl activity these locks prevent one user from  say  dropping a table while another user has an uncommitted transaction that is accessing that table oracle does not use lock escalation to convert row locks to table locks for the purpose of its regular concurrency control thesumit67.blogspot.com 1182 chapter 28 oracle oracle detects deadlocks automatically and resolves them by rolling back one of the transactions involved in the deadlock oracle supports autonomous transactions  which are independent transactions generated within other transactions when oracle invokes an autonomous transaction  it generates a new transaction in a separate context the new transaction can be either committed or rolled back before control returns to the calling transaction oracle supports multiple levels of nesting of autonomous transactions 28.5.2 basic structures for recovery oracle ? s flashback technology  described in section 28.5.1  can be used as a recovery mechanism  but oracle also supports media recovery where files are backed up physically we describe this more traditional form of backup and recovery here in order to understand how oracle recovers from a failure  such as a disk crash  it is important to understand the basic structures that are involved in addition to the data files that contain tables and indices  there are control files  redo logs  archived redo logs  and undo segments the control file contains various metadata that are needed to operate the database  including information about backups oracle records any transactional modification of a database buffer in the redo log  which consists of two or more files it logs the modification as part of the operation that causes it and regardless of whether the transaction eventually commits it logs changes to indices and undo segments as well as changes to table data as the redo logs fill up  they are archived by one or several background processes  if the database is running in archivelog mode   the undo segment contains information about older versions of the data  that is  undo information   in addition to its role in oracle ? s consistency model  the information is used to restore the old version of data items when a transaction that has modified the data items is rolled back to be able to recover from a storage failure  the data files and control files should be backed up regularly the frequency of the backup determines the worst-case recovery time  since it takes longer to recover if the backup is old oracle supports hot backups ? backups performed on an online database that is subject to transactional activity during recovery from a backup  oracle performs two steps to reach a consistent state of the database as it existed just prior to the failure first  oracle rolls forward by applying the  archived  redo logs to the backup this action takes the database to a state that existed at the time of the failure  but not necessarily a consistent state since the redo logs include uncommitted data second  oracle rolls back uncommitted transactions by using the undo segment data the database is now in a consistent state recovery on a database that has been subject to heavy transactional activity since the last backup can be time-consuming oracle supports parallel recovery in which several processes are used to apply redo information simultaneously thesumit67.blogspot.com 28.6 system architecture 1183 oracle provides a gui tool  recovery manager  which automates most tasks associated with backup and recovery 28.5.3 oracle data guard to ensure high availability  oracle provides a standby database feature  data guard  this feature is the same as remote backups  described in section 16.9  a standby database is a copy of the regular database that is installed on a separate system if a catastrophic failure occurs on the primary system  the standby system is activated and takes over  thereby minimizing the effect of the failure on availability oracle keeps the standby database up-to-date by constantly applying archived redo logs that are shipped from the primary database the backup database can be brought online in read-only mode and used for reporting and decision support queries 28.6 system architecture whenever a database application executes an sql statement  there is an operating system process that executes code in the database server oracle can be configured so that the operating system process is dedicated exclusively to the statement it is processing or so that the process can be shared among multiple statements the latter configuration  known as the shared server  has somewhat different properties with regard to the process and memory architecture.we shall discuss the dedicated server architecture first and the multithreaded server architecture later 28.6.1 dedicated server  memory structures the memory used by oracle falls mainly into three categories  software code areas  which are the parts of the memory where the oracle server code resides  the system global area  sga   and the program global area  pga   a pga is allocated for each process to hold its local data and control information this area contains stack space for various session data and the private memory for the sql statement that it is executing it also contains memory for sorting and hashing operations thatmay occur during the evaluation of the statement the performance of such operations is sensitive to the amount of memory that is available for example  a hash join that can be performed in memory will be faster than if it is necessary to spill to disk since there can be a large number of sorting and hashing operations active simultaneously  because of multiple queries as well as multiple operations within each query   deciding how much memory should be allocated to each operation is nontrivial  especially as the load on the system may fluctuate underallocation of memory can lead to extra disk i/os if an operation needlessly spills to disk and overallocation of memory can lead to thrashing oracle lets the database administrator specify a target parameter for the total amount of memory that should be considered available for these operations the size of this target would typically be based on the total amount of thesumit67.blogspot.com 1184 chapter 28 oracle memory available on the system and some calculation as to how it should be divided between various oracle and non-oracle activities oracle will dynamically decide the bestway to divide thememory availablewithin the target between the active operations in order to maximize throughput the memory allocation algorithmknows the relationship between memory and performance for the different operations and seeks to ensure that the available memory is used as efficiently as possible the sga is a memory area for structures that are shared among users it is made up of several major structures  including the following ? buffer cache this cache keeps frequently accessed data blocks  from tables as well as indices  in memory to reduce the need to perform physical disk i/o a least recently used replacement policy is used except for blocks accessed during a full table scan however  oracle allows multiple buffer pools to be created that have different criteria for aging out data some oracle operations bypass the buffer cache and read data directly from disk ? redo log buffer this buffer contains the part of the redo log that has not yet been written to disk ? shared pool oracle seeks to maximize the number of users that can use the database concurrently by minimizing the amount of memory that is needed for each user one important concept in this context is the ability to share the internal representation of sql statements and procedural code written in pl/sql.whenmultiple users execute the same sql statement  they can share most data structures that represent the execution plan for the statement.only data that are local to each specific invocation of the statement need to be kept in private memory the sharable parts of the data structures representing the sql statement are stored in the shared pool  including the text of the statement the caching of sql statements in the shared pool also saves compilation time  since a new invocation of a statement that is already cached does not have to go through the complete compilation process the determination of whether an sql statement is the same as one existing in the shared pool is based on exact text matching and the setting of certain session parameters oracle can automatically replace constants in an sql statement with bind variables ; future queries that are the same except for the values of constants will then match the earlier query in the shared pool the shared pool also contains caches for dictionary information and various control structures caching dictionary metadata is important for speeding up the compilation time for sql statements in addition  the shared pool is used for oracle ? s result cache feature 28.6.2 dedicated server  process structures there are two types of processes that execute oracle server code  server processes that process sql statements and background processes that perform varthesumit67 blogspot.com 28.6 system architecture 1185 ious administrative and performance-related tasks some of these processes are optional  and in some cases  multiple processes of the same type can be used for performance reasons oracle can generate about two dozen different types of background processes some of the most important ones are  ? database writer.when a buffer is removed from the buffer cache  it must be written back to disk if it has been modified since it entered the cache this task is performed by the database writer processes  which help the performance of the system by freeing up space in the buffer cache ? log writer the log-writer process writes entries in the redo log buffer to the redo log file on disk it also writes a commit record to disk whenever a transaction commits ? checkpoint the checkpoint process updates the headers of the data file when a checkpoint occurs ? system monitor this process performs crash recovery if needed it also performs some space management to reclaim unused space in temporary segments ? process monitor this process performs process recovery for server processes that fail  releasing resources and performing various cleanup operations ? recoverer the recoverer process resolves failures and conducts cleanup for distributed transactions ? archiver the archiver copies the online redo log file to an archived redo log every time the online log file fills up 28.6.3 shared server the shared-server configuration increases the number of users that a given number of server processes can support by sharing server processes among statements it differs from the dedicated server architecture in these major aspects  ? a background dispatch process routes user requests to the next available server process in doing so  it uses a request queue and a response queue in the sga the dispatcher puts a new request in the request queuewhere itwill be picked up by a server process as a server process completes a request  it puts the result in the response queue to be picked up by the dispatcher and returned to the user ? since a server process is shared among multiple sql statements  oracle does not keep private data in the pga instead  it stores the session-specific data in the sga thesumit67.blogspot.com 1186 chapter 28 oracle 28.6.4 oracle real application clusters oracle real application clusters  rac  is a feature that allows multiple instances of oracle to run against the same database  recall that  in oracle terminology  an instance is the combination of background processes and memory areas  this feature enables oracle to run on clustered and mpp  shared disk and shared nothing  hardware architectures the ability to cluster multiple nodes has important benefits for scalability and availability that are useful in both oltp and data warehousing environments the scalability benefits of the feature are obvious  since more nodes mean more processing power on shared-nothing architectures  adding nodes to a cluster typically requires redistributing the data between the nodes oracle uses a shared-disk architecture where all the nodes have access to all the data and as a result  more nodes can be added to a rac cluster without worrying how the data should be divided between the nodes oracle further optimizes the use of the hardware through features such as affinity and partitionwise joins rac can also be used to achieve high availability if one node fails  the remaining ones are still available to the application accessing the database the remaining instances will automatically roll back uncommitted transactions that were being processed on the failed node in order to prevent them from blocking activity on the remaining nodes rac also allows rolling patching so that software patches can be applied to one node at a time without database downtime oracle ? s shared-disk architecture avoids many of the issues that sharednothing architectures have with data on disk either being local to a node or not still  having multiple instances run against the same database gives rise to some technical issues that do not exist on a single instance while it is sometimes possible to partition an application among nodes so that nodes rarely access the same data  there is always the possibility of overlaps  which affects cache management in order to achieve efficient cache management over multiple nodes  oracle ? s cache fusion feature allows data blocks to flow directly among caches on different instances using the interconnect  without being written to disk 28.6.5 automatic storage manager the automatic storage manager  asm  is a volume manager and file system developed by oracle while oracle can be used with other volume managers and file systems as well as raw devices  asm is specifically designed to simplify storage management for the oracle database while optimizing performance asm manages collections of disks  known as disk groups  and exposes a file system interface to the database  recall that an oracle table space is defined in terms of data files  examples of what could constitute asm disks include disks or partitions of disk arrays  logical volumes  and network attached files asm automatically stripes the data over the disks in a disk group and provides several options for different levels of mirroring if the disk configuration changes  e.g  when more disks are added to increase storage capacity  a disk group may need to be rebalanced so that the data are spread evenly over all the disks the rebalancing operation can be done in thesumit67.blogspot.com 28.6 system architecture 1187 the background while the database remains fully operational and with minimal impact on database performance 28.6.6 oracle exadata exadata is a set of oracle libraries that can run on the storage array cpus on certain types of storage hardware while oracle is fundamentally based on a shared-disk architecture  exadata contains a shared-nothing flavor in that some operations that would normally be executed on the database server are moved to storage cells that can only access data that are local to each cell  each storage cell consists of a number of disks and several multicore cpus  the are major advantages to offloading certain types of processing to storage cpus  ? it allows a large  but relatively inexpensive  expansion of the amount of processing power that is available ? the amount of data that needs to be transferred from a storage cell to the database server can be dramatically reduced  which can be very important since the bandwidth between the storage cell and database server is usually expensive and often a bottleneck when executing a query against exadata storage  the reduction of the amount of data that needs to be retrieved comes from several techniques that can be pushed to the storage cells and executed there locally  ? projection a table may have hundreds of columns  but a given query may only need to access a very small subset of them the storage cells can project out the unneeded columns and only send the relevant ones back to the database server ? table filtering the database server can send a list of predicates that are local to a table to the storage cells and only rows matching these predicates get sent back to the server ? join filtering the filtering mechanism allows for predicates that are bloom filters allowing rows to be filtered out based on join conditions as well in combination  offloading these techniques to the storage cells can speed up query processing by orders of magnitude it requires that the storage cells  in addition to sending back regular  unaltered database blocks to the server  can send back a compacted version where certain columns and rows have been removed this ability in turn requires the storage software to understand oracle ? s block format and data types  and to include oracle ? s expression and predicate evaluation routines in addition to providing benefits for query processing  exadata can also speed up incremental backups by performing block-level change tracking and only thesumit67.blogspot.com 1188 chapter 28 oracle returning blocks that have changed also  the work of formatting extents when creating a new table space is offloaded to exadata storage exadata storage supports all regularoracle features  and it is possible to have a database that includes both exadata and non-exadata storage 28.7 replication  distribution  and external data oracle provides support for replication and distributed transactions with twophase commit 28.7.1 replication oracle supports several types of replication  see section 19.2.1 for an introduction to replication  in one form  data in a master site are replicated to other sites in the form of materialized views a materialized view does not have to contain all the master data ? it can  for example  exclude certain columns from a table for security reasons oracle supports two types of materialized views for replication  read-only and updatable an updatable materialized view can be modified and the modifications propagated to the master table however  read-only materialized views allow for a wider range of view definitions for instance  a read-only materialized view can be defined in terms of set operations on tables at the master site changes to the master data are propagated to the replicas through the materialized view refresh mechanism oracle also supports multiple master sites for the same data,where all master sites act as peers a replicated table can be updated at any of the master sites and the update is propagated to the other sites the updates can be propagated either asynchronously or synchronously for asynchronous replication  the update information is sent in batches to the othermaster sites and applied since the same data could be subject to conflicting modifications at different sites  conflict resolution based on some business rules might be needed.oracle provides a number of built-in conflict resolution methods and allows users to write their own if need be with synchronous replication  an update to one master site is propagated immediately to all other sites 28.7.2 distributed databases oracle supports queries and transactions spanning multiple databases on different systems.with the use of gateways  the remote systems can include non-oracle databases oracle has built-in capability to optimize a query that includes tables at different sites  retrieve the relevant data  and return the result as if it had been a normal  local query oracle also transparently supports transactions spanning multiple sites by a built-in two-phase-commit protocol thesumit67.blogspot.com 28.8 database administration tools 1189 28.7.3 external data sources oracle has several mechanisms for supporting external data sources the most common usage is in data warehousing when large amounts of data are regularly loaded from a transactional system 28.7.3.1 sql * loader oracle has a direct-load utility  sql * loader  that supports fast parallel loads of large amounts of data from external files it supports a variety of data formats and it can perform various filtering operations on the data being loaded 28.7.3.2 external tables oracle allows external data sources  such as flat files  to be referenced in the from clause of a query as if they were regular tables an external table is defined by metadata that describe the oracle column types and the mapping of the external data into those columns an access driver is also needed to access the external data oracle provides a default driver for flat files the external table feature is primarily intended for extraction  transformation  and loading  etl  operations in a data warehousing environment data can be loaded into the data warehouse from a flat file using create table table as select  from < external table > where  by adding operations on the data in either the select list or where clause  transformations and filtering can be done as part of the same sql statement since these operations can be expressed either in native sql or in functions written in pl/sql or java  the external table feature provides a very powerful mechanism for expressing all kinds of data transformation and filtering operations for scalability  the access to the external table can be parallelized by oracle ? s parallel execution feature 28.7.3.3 data pump export and import oracle provides an export utility for unloading data and metadata into dump files these files are regular files using a proprietary format that can be moved to another system and loaded into another oracle database using the corresponding import utility 28.8 database administration tools oracle provides users a number of tools and features for system management and application development in recent releases of oracle  a lot of emphasis was put on the concept of manageability  that is  reducing the complexity of all thesumit67.blogspot.com 1190 chapter 28 oracle aspects of creating and administering an oracle database this effort covered a wide variety of areas  including database creation  tuning  space management  storage management  backup and recovery  memory management  performance diagnostics  and workload management 28.8.1 oracle enterprise manager oracle enterprise manager  oem  is oracle ? s main tool for database systems management it provides an easy-to-use graphical user interface for most tasks associated with administering an oracle database including configuration  performance monitoring  resource management  security management  and access to the various advisors in addition to database management  oem provides integrated management of oracle ? s applications and middleware software stack 28.8.2 automatic workload repository the automatic workload repository  awr  is one of the central pieces of infrastructure for oracle ? s manageability effort oracle monitors the activity on the database system and records a variety of information relating to workloads and resource consumption and records them in awr at regular intervals by tracking the characteristics of a workload over time  oracle can detect and help diagnose deviations from normal behavior such as a significant performance degradation of a query  lock contention  and cpu bottlenecks the information recorded in awr provides a basis for a variety of advisors that provide analysis of various aspects of the performance of the system and advice for how it can be improved oracle has advisors for sql tuning  creating access structures  such as indices and materialized views  and memory sizing oracle also provides advisors for segment defragmentation and undo sizing 28.8.3 database resource management a database administrator needs to be able to control how the processing power of the hardware is divided among users or groups of users some groups may execute interactive queries where response time is critical ; others may execute long-running reports that can be run as batch jobs in the background when the system load is low it is also important to be able to prevent a user from inadvertently submitting an extremely expensive ad hoc query that will unduly delay other users oracle ? s database resource management feature allows the database administrator to divide users into resource consumer groups with different priorities and properties for example  a group of high-priority  interactive users may be guaranteed at least 60 percent of the cpu the remainder  plus any part of the 60 percent not used up by the high-priority group  would be allocated among resource consumer groups with lower priority a really low-priority group could get assigned 0 percent  which would mean that queries issued by this group would run only when there are spare cpu cycles available limits for the degree of parallelism for parallel execution can be set for each group the database adthesumit67 blogspot.com bibliographical notes 1191 ministrator can also set time limits for how long an sql statement is allowed to run for each group when a user submits a statement  the resource manager estimates how long it would take to execute it and returns an error if the statement violates the limit the resource manager can also limit the number of user sessions that can be active concurrently for each resource consumer group other resources that can be controlled by the resource manager include undo space 28.9 data mining oracle data mining provides a variety of algorithms that embed the data mining process inside the database both for building a model on a training set of data and for applying the model for scoring the actual production data the fact the data never needs to leave the database is a significant advantage compared to using other data mining engines having to extract and insert potentially very large data sets into a separate engine is cumbersome  costly  and may prevent new data from being scored instantaneously as they are entered into the database oracle provides algorithms for both supervised and unsupervised learning including  ? classification ? naive bayes  generalized linear models  support vector machines  and decision trees ? regression ? support vector machines and generalized linear models ? attribute importance ? minimum description length ? anomaly detection ? one class support vector machines ? clustering ? enhanced k-means clustering and orthogonal partitioning clustering ? association rules ? apriori ? feature extraction ? nonnegative matrix factorization in addition  oracle provides a wide range of statistical functions inside the database covering areas including linear regression  correlation  cross tabs  hypothesis testing  distribution fitting  and pareto analysis oracle provides two interfaces to the data mining functionality  one based on java and one that is based on oracle ? s procedural language pl/sql once amodel has been built on an oracle database  it can be shipped to be deployed on other oracle databases bibliographical notes up-to-date product information  including documentation  on oracle products can be found at the web sites http  //www.oracle.com and http  //technet.oracle.com thesumit67.blogspot.com 1192 chapter 28 oracle oracle ? s intelligent algorithms for allocating available memory for operations such as hashing and sorting are discussed in dageville and za ? it  2002   murthy and banerjee  2003  discussed xml schemas table compression in oracle is described in p ? oss and potapov  2003   automatic sql tuning is described in dageville et al  2004   the optimizer ? s cost-based query transformation framework is described in ahmed et al  2006   the sql plan-management feature is discussed inziauddin et al  2008  .antoshenkov  1995  describes the byte-aligned bitmap compression technique used in oracle ; see also johnson  1999   thesumit67.blogspot.com chapter29 ibm db2 universal database sriram padmanabhan ibm ? s db2 universal database family of products consists of flagship database servers and suites of related products for business intelligence  information integration  and content management the db2 universal database server is available on a variety of hardware and operating-system platforms the list of server platforms supported includes high-end systems such as mainframes  massively parallel processors  mpp   and large symmetric multiprocessors  smp  servers ; medium-scale systems such as four-way and eight-way smps ; workstations ; and even small handheld devices operating systems that are supported include unix variants such as linux  ibm aix  solaris,andhp-ux  aswell as microsoftwindows  ibmmvs  ibm vm  ibm os/400  and a number of others.the db2 everyplace edition supports operating systems such as palmos and windows ce there is even a no-charge  free  version of db2 called db2 express-c applications can migrate seamlessly from the low-end platforms to high-end servers because of the portability of the db2 interfaces and services besides the core database engine  the db2 family consists of several other products that provide tooling  administration  replication  distributed data access  pervasive data access  olap  and many other features figure 29.1 describes the different products in the family 29.1 overview the origin of db2 can be traced back to the system r project at ibm ? s almaden research center  then called the ibm san jose research laboratory   the first db2 product was released in 1984 on the ibm mainframe platform  and this was followed over time with versions for the other platforms ibm research contributions have continually enhanced the db2 product in areas such as transaction processing  write-ahead logging and aries recovery algorithms   query processing and optimization  starburst   parallel processing  db2 parallel edition   active-database support  constraints  triggers   advanced query andwarehousing techniques such as materialized views  multidimensional clustering  ? autonomic ? features  and object-relational support  adts  udfs   1193 thesumit67.blogspot.com 1194 chapter 29 ibm db2 universal database ? database servers ? db2 udb for linux  unix  windows ? db2 udb for z/os ? db2 udb for os/400 ? db2 udb for vm/vse ? business intelligence ? db2 data warehouse edition ? db2 olap server ? db2 alphablox ? db2 cubeviews ? db2 intelligent miner ? db2 query patroller ? data integration ? db2 information integrator ? db2 replication ? db2 connect ? omnifind  for enterprise search  ? content management ? db2 content ? i bm enterprise content manager ? application development ? ibm rational application developer studio ? db2 forms for z/os ? qmf ? database-management tools ? db2 control center ? db2 admin tool for z/os ? db2 performance expert ? db2 query patroller ? db2 visual explain ? embedded and mobile databases ? db2e  everyplace  manager figure 29.1 the db2 family of products since ibm supports a number of server and operating-system platforms  the db2 database engine consists of four code base types   1  linux  unix  and windows   2  z/os  3  vm  and  4  os/400 all of these support a common subset of data-definition language  sql  and administration interfaces however  the engines have somewhat different features due to their platform origins in this chapter  the focus is on the db2 universal database  udb  engine that supports linux  unix  and windows specific features of interest in other db2 systems are highlighted in appropriate sections the latest version of db2 udb for linux  unix  and windows as of 2009 is version 9.7 db2 version 9.7 includes several new feature such as extension of native support for xml to shared-nothing environments  native compression for tables and indexes  automatic storage management  and improved support for procedural languages such as sql pl and oracle ? s pl/sql 29.2 database-design tools most industry database-design and case tools can be used to design a db2 database in particular  data modeling tools such as erwin and rational rose allow the designer to generate db2-specific ddl syntax for instance  rational rose ? s uml data modeler tool can generate db2-specific create distinct type ddl statements for user-defined types and use them subsequently in column definithesumit67 blogspot.com 29.3 sql variations and extensions 1195 tions most design tools also support a reverse-engineering feature that reads the db2 catalog tables and builds a logical design for additional manipulation the tools support the generation of constraints and indices db2 provides support for many logical and physical database features using sql the features include constraints  triggers  and recursion using sqlconstructs likewise  certain physical database features such as tablespaces  bufferpools  and partitioning are also supported by using sql statements the control center gui tool for db2 allows a designer or an administrator to issue the appropriate ddl for these features another tool  db2look  allows the administrator to obtain a full set of ddl statements for a database including tablespaces  tables  indices  constraints  triggers  and functions that can be used to create an exact replica of the database schema for testing or replication the db2 control center includes a variety of design and administrationrelated tools for design  the control center provides a tree view of a server  its databases  tables  views  and all other objects it also allows users to define new objects  create ad hoc sql queries  and view query results design tools for etl  olap  replication  and federation also integrate into the control center the entire db2 family supports the control center for database definition as well as related tools db2 also provides plug-in modules for application development in the ibm rational application developer product as well as in the microsoft visual studio product 29.3 sql variations and extensions db2 provides support for a rich set of sql features for various aspects of database processing many of the db2 features and syntax have provided the basis for standards in sql-92  or sql  1999 in this section  we highlight the xml objectrelational and application-integration features in db2 udb version 8  along with some new features from version 9 29.3.1 xml features a rich set of xml functions have been included in db2 the following is a list of several important xml functions that can be used in sql  as part of the sql/xml extension to sql  described earlier in section 23.6.3   ? xmlelement constructs an element tag with given name for example the function call  xmlelement  book  creates the book element ? xmlattributes constructs the set of attributes for an element ? xmlforest constructs a sequence of xml elements from arguments ? xmlconcat returns the concatenation of a variable number of xml arguments ? xmlserialize provides a character-oriented serialized version of the argument thesumit67.blogspot.com 1196 chapter 29 ibm db2 universal database select xmlemement  name ? po ?  xmlattributes  poid  orderdate    select xmlagg  xmlelement  name ? item ?  xmlattributes  itemid  qty  shipdate    select xmlelement  name ? itemdesc ?  xmlattributes  name  price   from product where product.itemid = lineitem.itemid    from lineitem where lineitem.poid = orders.poid   from orders where orders.poid = 349 ; figure 29.2 db2 sql xml query ? xmlagg returns a concatenation of a set of xml values ? xml2clob constructs a character large object  clob  representation of the xml this clob can then be retrieved by sql applications the xml functions can be incorporated into sql effectively to provide extensive xml manipulation capabilities for instance  suppose that one needs to construct a purchase-order xml document from relational tables orders  lineitem  and product for order number 349 in figure 29.2  we show an sql query with xml extensions that can be used to create such a purchase order the resultant output is as shown in figure 29.3 version 9 of db2 supports native storage of xmldata as an xml type and native support for the xquery language specialized storage  indexing  query processing and optimization techniques have been introduced for efficient processing of xml data and queries in the xquery language  and apis have been extended to deal with xml data and xquery 29.3.2 support for data types db2 provides support for user-defined data types  udts  .users can define distinct or structured data types distinct data types are based on db2 built-in data types < po poid = " 349 " orderdate = " 2004-10-01 " > < item itemid = " 1 "  qty = " 10 "  shipdate = " 2004-10-03 " > < itemdesc name = " ibm thinkpad t41 "  price = " 1000.00 usd " / > < /item > < /po > figure 29.3 purchase order in xml for id = 349 thesumit67.blogspot.com 29.3 sql variations and extensions 1197 however  the user can define additional or alternative semantics for these new types for example  the user can define a distinct data type called us dollar  using  create distinct type us dollar as decimal  9,2  ; subsequently  the user can create a field  e.g  price  in a table with type us dollar queries may now use the typed field in predicates such as the following  select product from us sales where price > us dollar  1000  ; structured data types are complex objects that usually consist of two or more attributes for example  one can use the following ddl to create a structured type called department t  create type department t as  deptname varchar  32   depthead varchar  32   faculty count integer  mode db2/sql ; create type point t as  x coord float  y coord float  mode db2/sql ; structured types can be used to define typed tables  create table dept of department t ; one can create a type hierarchy and tables in the hierarchy that can inherit specificmethods and privileges structured types can also be used to define nested attributes inside a column of a table although such a definition would violate normalization rules  it may be suitable for object-oriented applications that rely on encapsulation and well-defined methods on objects 29.3.3 user-defined functions and methods another important feature is the ability for users to define their own functions and methods these functions can subsequently be included in sql statements and queries functions can generate scalars  single attribute  or tables  multiattribute row  as their result users can register functions  scalar or table  using the create function statement the functions can be written in common programming languages such as c or java or scripts such as rexx or perl user-defined functions  udfs  can operate in fenced or unfenced modes in fenced mode  the functions are executed by a separate thread in its own address space in unfenced thesumit67.blogspot.com 1198 chapter 29 ibm db2 universal database create function db2gse.gsegefilterdist  operation integer  g1xmin double  g1xmax double  g1ymin double  g1ymax double  dist double  g2xmin double  g2xmax double  g2ymin double  g2ymax double  returns integer specific db2gse.gsegefilterdist external name ? db2gsefn ! gsegefilterdist ? language c parameter style db2 sql deterministic not fenced threadsafe called on null input no sql no external action no scratchpad no final call allow parallel no dbinfo ; figure 29.4 definition of a udf mode  the database-processing agent is allowed to execute the function in the server ? s address space udfs can define a scratch pad  work  area where they can maintain local and static variables across different invocations thus  udfs can perform powerful manipulations of intermediate rows that are its inputs in figure 29.4  we show a definition of a udf  db2gse.gsegefilterdist  in db2 pointing to a particular external method that performs the actual function methods are another feature that define the behavior of objects unlike udfs  they are tightly encapsulated with a particular structured data type methods are registered by using the create method statement db2 also supports procedural extensions to sql  using the db2 ? s sql pl extension  including procedures  functions  and control flow procedural features of the sql standard are described in section 5.2   in addition  as of version 9.7  db2 also supports much of oracle ? s pl/sql language  for compatibility with applications developed on oracle 29.3.4 large objects new database applications require the ability to manipulate text  images  video  and other types of data that are typically quite large in size db2 supports these requirements by providing three different large object  lob  types each lob can be as large as two gigabytes in size the large objects in db2 are  1  binary large objects  blobs    2  single byte character large objects  clobs   and  3  double byte character large objects  dbclobs   db2organizes these lobs as separateobjectswith thesumit67.blogspot.com 29.3 sql variations and extensions 1199 create index extension db2gse.spatial index  gs1 double  gs2 double  gs3 double  from source key  geometry db2gse.st geometry  generate key using db2gse.gsegrididxkeygen  geometry..srid  geometry..xmin  geometry..xmax  geometry..ymin  geometry..ymax  gs1  gs2  gs3  with target key  srsid integer  lvl integer  gx integer  gy integer  xmin double  xmax double  ymin double  ymax double  search methods < conditions > < actions > figure 29.5 spatial index extension in db2 each row in the table maintaining pointers to its corresponding lobs users can register udfs that manipulate these lobs according to application requirements 29.3.5 indexing extensions and constraints a recent feature of db2 enables users to create index extensions to generate keys from structured data types by using the create index extension statement for example  one can create an index on an attribute based on the department t data type defined earlier by generating keys  using the department name db2 ? s spatial extender uses the index extensionmethod to create indices as shown in figure 29.5 finally  users can take advantage of the rich set of constraint checking features available in db2 for enforcing object semantics such as uniqueness  validity  and inheritance 29.3.6 web services db2 can integrate web services as producer or consumer a web service can be defined to invoke db2  using sql statements the resultant web-service call is processed by an embeddedweb-service engine in db2 and the appropriate soap response generated for example  if there is a web service called getrecentactivity  cust id  that invokes the following sql  the result should be the last transaction for this customer select trn id  amount  date from transactions where cust id = < input > order by date fetch first 1 row only ; thesumit67.blogspot.com 1200 chapter 29 ibm db2 universal database the following sql shows db2 acting as a consumer of a web service in this example  the getquote   user-defined function is a web service db2 makes the web-service call using an embedded web-service engine in this case  getquote returns a numeric quote value for each ticker id in the portfolio table select ticker id  getquote  ticker id  from portfolio ; 29.3.7 other features db2 also supports ibm ? s websphere mq product by defining appropriate udfs udfs are defined for both read and write interfaces these udfs can be incorporated in sql for reading from or writing to message queues from version 9  db2 supports fine-grained authorization through the labelbased access control feature  which plays a role similar to oracle ? s virtual private database  described earlier in section 9.7.5   29.4 storage and indexing the storage and indexing architecture in db2 consists of the file-system or diskmanagement layer  the services to manage the buffer pools  data objects such as tables  lobs  index objects  and concurrency and recoverymanagers.we overview the general storage architecture in this section in addition  we describe a new feature in db2 version 8 called multidimensional clustering in the following section 29.4.1 storage architecture db2 provides storage abstractions for managing logical database tables usefully in amultinode andmultidisk environment nodegroups can be defined to support table partitioning across a specific set of nodes in a multinode system this allows complete flexibility in allocating table partitions to different nodes in a system for example  large tables may be partitioned across all nodes in a system while small tables may reside on a single node within a node  db2 uses tablespaces to organize tables a tablespace consists of one or more containers  which are references to directories  devices  or files a tablespace may contain zero or more database objects such as tables  indices  or lobs figure 29.6 illustrates these concepts in this figure  two tablespaces have been defined for a nodegroup the humanres tablespace is assigned four containers  while the sched tablespace has only one container the employee and department tables are assigned to the humanres tablespace  while the project table is in the sched tablespace striping is used to allocate fragments  extents  of the employee and department table to the containers of the humanres tablespace db2 permits the administrator to create either system-managed or dbms-managed tablespaces system-managed spaces  sms  are directories or file systems that are maintained by the underlying operating system in sms  db2 creates file objects thesumit67.blogspot.com 29.4 storage and indexing 1201 nodegroup mydepts tablespace humanres tablespace sched containers department employee project figure 29.6 tablespaces and containers in db2 in the directories and allocates data to each of the files data-managed spaces  dms  are raw devices or preallocated files that are then controlled by db2 the size of these containers can never grow or shrink db2 creates allocation maps and manages the dms tablespace itself in both cases  an extent of pages is the unit of space management the administrator can choose the extent size for a tablespace db2 supports striping across the different containers as a default behavior for example  when data are inserted into a newly created table  the first extent is assigned to a container once the extent is full  the next data items are allocated to the next container in round-robin fashion striping provides two significant benefits  parallel i/o and load balancing 29.4.2 buffer pools one or more buffer pools may be associated with each tablespace for managing different objects such as data and indices the buffer pool is a common shared data area that maintains memory copies of objects these objects are typically organized as pages for management in the buffer pool db2 allows buffer pools to be defined by sql statements db2 version 8 has the ability to grow or shrink buffer pools online and also automatically by choosing the automatic setting for the buffer pool configuration parameter an administrator can add more pages to a buffer pool or decrease its size without quiescing the database activity create bufferpool < buffer-pool >  alter bufferpool < buffer-pool > size < n > thesumit67.blogspot.com 1202 chapter 29 ibm db2 universal database 0 2 500 page 0 contains a set of internal records  e.g  fscr  user records every 500th page contains another fscr 1 3 more user records rid  record id  = page 3  slot 2 c 3  2 a rid kk rid s rid leaf pages logical table view logical index view k rid k rid c rid figure 29.7 logical view of tables and indices in db2 db2 also supportsprefetching and asynchronous writes using separate threads the data manager component triggers prefetch of data and index pages based on the query access patterns for instance  a table scan always triggers prefetch of data pages index scans can trigger prefetch of index pages as well as data pages if they are being accessed in a clustered fashion the number of prefetchers and the prefetch size are configurable parameters that need to be initialized according to the number of disks or containers in the tablespace 29.4.3 tables  records  and indices db2 organizes the relational data as records in pages figure 29.7 shows the logical view of a table and an associated index the table consists of a set of pages each page consists of a set of records that are either user data records or special system records page zero of the table contains special system records about the table and its status db2 uses a space-map record called free space control record  fscr  to find free space in the table the fscr record usually contains a space map for 500 pages the fscr entry is a bit mask that provides a rough indication of the possibility of free space in a page the insert or update algorithm must validate the fscr entries by performing a physical check of the available space in a page indices are also organized as pages containing index records and pointers to child and sibling pages db2 provides support for the b + -tree index mechanisms internally the b + -tree index contains internal pages and leaf pages the indices have bidirectional pointers at the leaf level to support forward and reverse scans leaf pages contain index entries that point to records in the table each record in the table can be uniquely identified by using its page and slot information  which are called the record id or rid thesumit67.blogspot.com 29.5 multidimensional clustering 1203 embedded free space  usable a  er on-line page reorganization *  * exception  any space reserved by an uncommited delete is not usable free space  usable without page reorganization *  -1 record 0 record 0 record 2 page header page header 3800 3400 3800 3700 page 473 page 1056 473,2 set on tablespace creation 1056 1 3 bytes 1 byte page # slot # figure 29.8 data page and record layout in db2 db2 supports ? include columns ? in the index definition  as  create unique index i1 on t1  c1  include  c2  ; the included index columns enable db2 to extend the use of ? index-only ? query-processing techniques whenever possible additional directives such as minpctused and pctfree can be used to control the merge and initial space allocation of index pages figure 29.8 shows the typical data page format in db2 each data page contains a header and a slot directory the slot directory is an array of 255 entries that points to record offsets in the page the figure shows that page number 473 contains record zero at offset 3800 and record 2 at offset 3400 page 1056 contains record 1 at offset 3700  which is a forward pointer to the record < 473,2 >  hence  record < 473,2 > is an overflow record that was created as a result of an update operation of the original record < 1056,1 >  db2 supports different page sizes such as 4  8  16  and 32 kilobytes however  each page may contain only 255 user records in it larger page sizes are useful in applications such as data warehousing  where the table contains many columns smaller page sizes are useful for operational data with frequent updates 29.5 multidimensional clustering this section provides a brief overview of the main features of mdc with this feature  a db2 tablemay be created by specifying one or more keys as dimensions thesumit67.blogspot.com 1204 chapter 29 ibm db2 universal database along which to cluster the table ? s data db2 includes a clause called organize by dimensions for this purpose for example  the following ddl describes a sales table organized by storeid  year  orderdate   and itemid attributes as dimensions create table sales  storeid int  orderdate date  shipdate date  receiptdate date  region int  itemid int  price float yearod int generated always as year  orderdate   organized by dimensions  region  yearod  itemid  ; each of these dimensions may consist of one or more columns  similar to index keys in fact  a ? dimension block index ?  described below  is automatically created for each of the dimensions specified and is used to access data quickly and efficiently a composite block index  containing all dimension key columns  is created automatically if necessary  and is used to maintain the clustering of data over insert and update activity every unique combination of dimension values forms a logical ? cell  ? that is physically organized as blocks of pages  where a block is a set of consecutive pages on disk the set of blocks that contain pages with data having a certain key value of one of the dimension block indices is called a ? slice ? every page of the table is part of exactly one block  and all blocks of the table consist of the same number of pages  namely  the block size db2 has associated the block size with the extent size of the tablespace so that block boundaries line up with extent boundaries figure 29.9 illustrates these concepts this mdc table is clustered along the dimensions year  orderdate  ,1 region  and itemid the figure shows a simple logical cube with only two values for each dimension attribute in reality  dimension attributes can easily extend to large numbers of values without requiring any administration logical cells are represented by the subcubes in the figure records in the table are stored in blocks  which contain an extent ? s worth of consecutive pages on disk in the diagram  a block is represented by a shaded oval  and is numbered according to the logical order of allocated extents in the table we show only a few blocks of data for the cell identified by the dimension values < 1997,canada,2 > .acolumn or rowin the grid represents a slice for a particular dimension for example  all records containing the value ? canada ? in the region dimension are found in the blocks contained in the slice defined by the ? canada ? column in the cube in fact  each block in this slice only contains records having ? canada ? in the region field 1dimensions can be created by using a generated function thesumit67.blogspot.com 29.5 multidimensional clustering 1205 1997  mexico  1 1997  canada  2 1997  canada  1 1997  mexico  2 1998  canada  2 1998  mexico  1997  2 mexico  2 year  orderdate  itemid region 31 45 127 1997 1998 1 2 mexico canada figure 29.9 logical view of physical layout of an mdc table 29.5.1 block indices in our example  a dimension block index is created on each of the year  orderdate   region  anditemid attributes each dimension block index is structured in the same manner as a traditional b-tree index except that  at the leaf level  the keys point to a block identifier  bid  instead of a record identifier  rid   since each block contains potentially many pages of records  these block indices are much smaller than rid indices and need be updated only when a new block is added to a cell or existing blocks are emptied and removed from a cell a slice  or the set of blocks containing pages with all records having a particular key value in a dimension  are represented in the associated dimension block index by a bid list for that key value figure 29.10 illustrates slices of blocks for specific values of region and itemid dimensions  respectively in the example above  to find the slice containing all records with ? canada ? for the region dimension  we would look up this key value in the region dimension block index and find a key as shown in figure 29.10a this key points to the exact set of bids for the particular value 29.5.2 block map a block map is also associated with the table this map records the state of each block belonging to the table a block may be in a number of states such as in use  free  loaded  requiring constraint enforcement the states of the block are used thesumit67.blogspot.com 1206 chapter 29 ibm db2 universal database canada 21 31 45 77 127 376 501 719 key key bid list bid list  a  dimension block index entry for region 'canada '  b  dimension block index entry for itemid = 1 1 2 7 20 65 101 273 274 476 figure 29.10 block index key entries by the data-management layer in order to determine various processing options figure 29.11 shows an example block map for a table element 0 in the block map represents block 0 in the mdc table diagram its availability status is ? u  ? indicating that it is in use however  it is a special block and does not contain any user records blocks 2  3  9  10  13  14  and 17 are not being used in the table and are considered ? f  ? or free  in the block map blocks 7 and 18 have recently been loaded into the table block 12 was previously loaded and requires that a constraint check be performed on it 29.5.3 design considerations a crucial aspect of mdc is to choose the right set of dimensions for clustering a table and the right block size parameter to minimize the space utilization if the dimensions and block sizes are chosen appropriately  then the clustering benefits translate into significant performance and maintenance advantages on the other hand  if chosen incorrectly  the performancemay degrade and the space utilization could be significantly worse there are a number of tuning knobs that can be exploited to organize the table these include varying the number of dimensions  and varying the granularity of one or more dimensions  varying the 0 1 2 3 4 5 6 7 8 9 19 11 12 13 14 15 16 17 18 19 u u f f u u u l u f f u c f f u u f l  figure 29.11 block map entries thesumit67.blogspot.com 29.6 query processing and optimization 1207 block size  extent size  and page size of the tablespace containing the table one or more of these techniques can be used jointly to identify the best organization of the table 29.5.4 impact on existing techniques it is natural to ask whether the newmdc feature has an adverse impact or disables some existing features of db2 for normal tables all existing features such as secondary rid indices  constraints  triggers  defining materialized views  and query processing options  are available for mdc tables.hence  mdc tables behave just like normal tables except for their enhanced clustering and processing aspects 29.6 query processing and optimization db2 queries are transformed into a tree of operations by the query compiler the query operator tree is used at execution time for processing db2 supports a rich set of query operators that enables it to consider the best processing strategies and provides the flexibility to execute complex query tasks figures 29.12 and 29.13 show a query and its associated query plan in db2 the query is a representative complex query  query 5  from the tpc-h benchmark and contains several joins and aggregations the query plan chosen for this particular example is rather simple since many indices and other auxiliary structures such as materialized views were not defined for these tables db2 provides various ? explain ? facilities including a powerful visual explain feature in the control center that can help users understand the details of a query-execution plan the query plan shown in the figure is based on the visual explain for the query visual ? ? ? tpcd local supplier volume query  q5  ? ; select n name  sum  l extendedprice *  1-l discount   as revenue from tpcd.customer  tpcd.orders  tpcd.lineitem  tpcd.supplier  tpcd.nation  tpcd.region where c custkey = o custkey and o orderkey = l orderkey and l suppkey = s suppkey and c nationkey = s nationkey and s nationkey = n nationkey and n regionkey = r regionkey and r name = ? middle east ? and o orderdate > = date  ? 1995-01-01 ?  and o orderdate < date  ? 1995-01-01 ?  + 1 year group by n name order by revenue desc ; figure 29.12 sql query thesumit67.blogspot.com 1208 chapter 29 ibm db2 universal database scan scan scan nljoin scan nljoin sort sort merge join sort index scan sort merge join scan hash join sort scan group by sort scan results customer supplier nation region orders lineitem figure 29.13 db2 query plan  graphical explain   explain allows the user to understand cost and other relevant properties of the different operations of the query plan all sql queries and statements  however complex they may be  are transformed into a query tree the base or leaf operators of the query tree manipulate records in database tables these operations are also called as access methods intermediate operations of the tree include relational-algebra operations such as join  set operations  and aggregation the root of the tree produces the results of the query or sql statement 29.6.1 access methods db2 supports a comprehensive set of access methods on relational tables the list of access methods includes  thesumit67.blogspot.com 29.6 query processing and optimization 1209 ? table scan this is the most basicmethod and performs a page-by-page access of all records in the table ? index scan an index is used to select the specific records that satisfy the query the qualifying records are accessed using the rids in the index db2 detects opportunities to prefetch data pages when it observes a sequentialaccess pattern ? block index scan this is a new access method for mdc tables one of the block indices is used to scan a specific set of mdc data blocks the qualifying blocks are accessed and processed in block table scan operations ? index only in this case  the index contains all the attributes that are required by the query hence  a scan of the index entries is sufficient the index-only technique is usually a good performance solution ? list prefetch this access method is chosen for an unclustered index scan with a significant number of rids db2 has a sort operation on the rids and performs a fetch of the records in sorted order from the data pages sorted access changes the i/o pattern from random to sequential and also enables prefetching opportunities list prefetch has been extended to dealwith block indices as well ? block and record index anding this method is used when db2 determines that more than one index can be used to constrain the number of satisfying records in a base table the most selective index is processed to generate a list of bids or rids the next selective index is then processed to return the bids or rids that it qualifies a bid or rid qualifies for further processing only if it is present in the intersection  and operation  of the index scan results the result of an index and operation is a small list of qualifying bids or rids which are used to fetch the corresponding records from the base table ? block and record index ordering this strategy is used if two ormore block or record indices can be used to satisfy query predicates that are combined by using the or operator db2 eliminates duplicate bids or rids by performing a sort and then fetching the resulting set of records index oring has been extended to consider block and rid index combinations all the selection and projection predicates of a query are usually pushed down to the access methods in addition  db2 performs certain operations such as sorting and aggregation in ? pushed down ? mode in order to reduce instruction paths this mdc feature takes advantage of the new set of access-method improvements for block index scans  block index prefetch  block index anding  and block index oring to process blocks of data 29.6.2 join  aggregation  and set operations db2 supports a number of techniques for these operations for join  db2 can choose between nested-loop  sort-merge  and hash-join techniques in describing thesumit67.blogspot.com 1210 chapter 29 ibm db2 universal database the join and set binary operations  we use the notation of ? outer ? and ? inner ? tables to distinguish the two input streams the nested-loop technique is useful if the inner table is very small or can be accessed by using an index on a join predicate sort-merge-join and hash-join techniques are used for joins involving large outer and inner tables set operations are implemented by using sorting and merging techniques the merging technique eliminates duplicates in the case of unionwhile duplicates are forwarded in the case of intersection db2 also supports outer-join operations of all kinds db2 processes aggregation operations in early or ? push-down ? mode whenever possible for instance  a group by aggregation can be performed by incorporating the aggregation into the sort phase the join and aggregation algorithms can take advantage of superscalar processing in modern cpus using block-oriented and cache-conscious techniques 29.6.3 support for complex sql processing one of the most important aspects of db2 is that it uses the query-processing infrastructure in an extensible fashion to support complex sql operations the complex sql operations include support for deeply nested and correlated queries as well as constraints  referential integrity  and triggers because most of these actions are built into the query plan  db2 is able to scale and provide support for a larger number of these constraints and actions constraints and integrity checks are built as query tree operations on insert  delete  or update sql statements db2 also supports maintenance of materialized view by using built-in triggers 29.6.4 multiprocessor query-processing features db2 extends the base set of query operations with control and data exchange primitives to support smp  that is  shared memory   mpp  that is  shared nothing   and smp cluster  that is  shared disk  modes of query processing db2 uses a ? tablequeue ? abstraction for data exchange between threads on different nodes or on the same node the tablequeue is used as a buffer that redirects data to appropriate receivers using broadcast  one-to-one  or directedmulticast methods control operations are used to create threads and coordinate the operation of different processes and threads in all these modes  db2 employs a coordinator process to control the query operations and final result gathering coordinator processes can also perform some global database-processing actions if required an example is the global aggregation operation to combine the local aggregation results subagents or slave threads perform the base database operations in one or more nodes in smp mode  the subagents use shared memory to synchronize between themselves when sharing data in an mpp  the tablequeue mechanisms provide buffering and flow control to synchronize across different nodes during execution db2 employs extensive techniques to optimize and process queries efficiently in an mpp or smp environment figure 29.14 shows a simple query executing in a fournode mpp system in this example  the sales table is partitioned across the four nodes p1      p4 the query is executed by spawning agents that execute at each thesumit67.blogspot.com 29.6 query processing and optimization 1211 ? distribute subsection ? tablequeue  tq  receive ? bind out subsection ? table access  sales  ? predicate  quantity > 10  ? tq send to coordinator ? nodes p1 p2 p3 p4 sql query  select * from sales where quantity > 10 coordinator receive bind out scan sales filter quantity > 10 send to coordinator scan sales filter quantity > 10 send to coordinator figure 29.14 db2 mpp query processing using function shipping of these nodes to scan and filter the rows of the sales table at that node  called function shipping   and the resulting rows are sent to the coordinator node 29.6.5 query optimization db2 ? s query compiler uses an internal representation of the query  called the query-graph model  qgm   in order to perform transformations and optimizations after parsing the sql statement  db2 performs semantic transformations on the qgm to enforce constraints  referential integrity  and triggers the result of these transformations is an enhanced qgm next  db2 attempts to perform query rewrite transformations that are considered mostly beneficial rewrite rules are fired if applicable to perform the required transformations examples of rewrite transformations include  1  decorrelation of correlated subqueries   2  transforming certain subqueries into joins using early-out processing   3  pushing the group by operation below joins if applicable  and  4  using materialized views for portions of the original query the query optimizer component uses this enhanced and transformed qgm as its input for optimization the optimizer is cost based and uses an extensible  rule-driven framework the optimizer can be configured to operate at different levels of complexity at the highest level  it uses a dynamic-programming algorithm to consider all query-plan options and chooses the optimal cost plan at an intermediate level  the optimizer does not consider certain plans  access methods  e.g  index oring   or rewrite rules at the lowest level of complexity  the optimizer uses a simple greedy heuristic to choose a good but not necessarily optimal query plan the optimizer uses detailed models of the query-processing operations  including memory sizes and prefetching  to obtain accurate estimates of the i/o and cpu costs it relies on the statistics of the data to estimate the cardinality and selectivities of the operations db2 allows the user to obtain detailed histograms of column-level distributions and combinations of columns using the runstats utility the detailed histograms contain information about the most frequent value occurrences as well as quantile-based frequency distributions of the attributes the optimizer generates an internal query plan that is considered the thesumit67.blogspot.com 1212 chapter 29 ibm db2 universal database create table emp dept  dept id integer  emp id integer  emp name varchar  100   mgr id integer  as select dept id  emp id  emp name  mgr id from employee  department data initially deferred refresh immediate ? ?  or deferred  maintained by user ? ?  or system  figure 29.15 db2 materialized query tables best query plan for the particular optimization level this query plan is converted into threads of query operators and associated data structures for execution by the query-processing engine 29.7 materialized query tables materialized views are supported in db2 in linux  unix  andwindows as well as on the z/os platforms a materialized view can be any general view definition on one or more tables or views a materialized view is useful since it maintains a persistent copy of the view data to enable faster query processing in db2 a materialized view is called a materialized query table  mqt   mqts are specified by using the create table statement as shown by the example in figure 29.15 in db2  mqts can reference other mqts to create a tree or forest of dependent views these mqts are highly scalable as they can be partitioned in an mpp environment and can have mdc clustering keys mqts are most valuable if the database engine can route queries to them seamlessly and also if the database engine can maintain them efficiently whenever possible db2 provides both of these features 29.7.1 query routing to mqts the query-compiler infrastructure in db2 is ideally suited to leverage the full power of mqts the internal qgm model allows the compiler to match the input query against the available mqt definitions and choose appropriate mqts for consideration after matching  the compiler considers several options for optimization they include the base query as well as suitable mqt reroute versions the optimizer loops through these options before choosing the optimal version for execution the entire flow of the reroute and optimization is shown in figure 29.16 29.7.2 maintenance of mqts mqts are useful only if the database engine provides efficient techniques for maintenance there are two dimensions to maintenance  time and cost in the time dimension  the two choices are immediate or deferred db2 supports both these choices if one selects immediate  then internal triggers are created and compiled thesumit67.blogspot.com 29.7 materialized query tables 1213 sql query query semantics  validate reroute possibility  mqt candidate match phase mqt definitions query candidates optimization phase select best plan figure 29.16 mqt matching and optimization in db2 into the insert  update  or delete statements of the source objects to process the updates to the dependent mqts in the case of deferred maintenance  the updated tables are moved into an integrity mode and an explicit refresh statement must be issued to perform the maintenance in the size dimension  the choices are incremental or full incremental maintenance implies that only the recently updated rows should be used for maintenance full maintenance implies that the entire mqt be refreshed from its sources the matrix in figure 29.17 shows the two dimensions and the options that aremost useful along these dimensions for instance  immediate and full maintenance are not compatible unless the sources are extremely small db2 also allows for the mqts to be maintained by user in this case  the refresh of the mqts is determined by users performing explicit processing using sql or utilities the following commands provide one simple example of performing deferred maintenance for the emp dept materialized view after a load operation to one of its sources yes yes  usually no a  er insert/update/delete yes  a  er load immediate deferred choices incremental full figure 29.17 options for mqt maintenance in db2 thesumit67.blogspot.com 1214 chapter 29 ibm db2 universal database load from newdata.txt of type del insert into employee ; refresh table emp dept 29.8 autonomic features in db2 db2 udb provides features for simplifying the design and manageability of databases autonomic computing encompasses a set of techniques that allow the computing environment to manage itself and reduce the external dependencies in the face of external and internal changes in security  system load  or other factors configuration  optimization  protection  and monitoring are examples of subject areas that benefit from autonomic-computing enhancements the following sections briefly describe the configuration and optimization areas 29.8.1 configuration db2 is providing support for automatic tuning of various memory and system configuration parameters for instance  parameters such as buffer pool sizes and sort heap sizes can be specified as automatic in this case  db2 monitors the system and slowly grows or shrinks these heap memory sizes  depending on the workload characteristics 29.8.2 optimization auxiliary data structures  indices  mqts  and data organization features  partitioning  clustering  are important aspects of improving the performance of database processing in db2 in the past  the database administrator  dba  had to use experience and known guidelines to choose meaningful indices  mqts  partition keys  and clustering keys given the potential number of choices  even the best experts are not capable of finding the right mix of these features for a given workload in a short time db2 includes a design advisor that provides workload-based advice for all of these features the design advisor tool automatically analyzes a workload  using optimization techniques to present a set of recommendations the design advisor command syntax is  db2advis -d < db name > -i < workloadfile > -m micp the ? -m ? parameter allows the user to specify the following options  ? m ? materialized query tables ? i ? indices ? c ? clustering  namely  mdc ? p ? partitioning key selection thesumit67.blogspot.com 29.9 tools and utilities 1215 the advisor uses the full power of the db2 query-optimization framework in these recommendations it uses an input workload and constraints on size and time of advise as its parameters given that it leverages the db2 optimization framework  it has full knowledge of the schema and statistics of the underlying data the advisor uses several combinatorial techniques to identify indices  mqts  mdcs  and partitioning keys to improve the performance of the given workload another aspect of optimization is balancing the processing load on the system in particular  utilities tend to increase the load on a system and cause significant reduction in user workload performance given the trend toward online utilities  there is a need to balance the load consumption of utilities db2 includes a utility load-throttlingmechanism the throttling technique is based on feedback control theory it continually adjusts and throttles the performance of the backup utility  using specific control parameters 29.9 tools and utilities db2 provides a number of tools for ease of use and administration this core set of tools is augmented and enhanced by a large number of tools from vendors the db2 control center is the primary tool for use and administration of db2 databases the control center runs on many workstation platforms it is organized from data objects such as servers  databases  tables  and indices it contains task-oriented interfaces to perform commands and allows users to generate sql scripts figure 29.18 shows a screen shot of the main panel of the control center this screen shot shows a list of tables in the sample database in the db2 instance on node crankarm the administrator can use the menu to invoke a suite of component tools the main components of the control center include command center  script center  journal  license management  alert center  performance monitor  visual explain  remote database management  storage management  and support for replication the command center allows users and administrators to issue database commands and sql the script center allows users to run sql scripts constructed interactively or from a file the performance monitor allows users to monitor various events in the database system and obtain snapshots of performance ? smartguides ? provide help on configuring parameters and setting up the db2 system a stored-procedure builder helps the user to develop and install stored procedures visual explain allows the user to obtain graphical views of the query-execution plan an index wizard helps the administrator by suggesting indices for performance while the control center is an integrated interface for many of the tasks  db2 also provides direct access to most tools for users  tools such as the explain facility  explain tables  and graphical explain provide a detailed breakdown of the query plans users are also allowed to modify statistics  if permitted  in order to generate the best query plans administrators are supported by a number of tools db2 provides comprehensive support for load  import  export  reorg  redistribute  and other data-related utilities most of these support incremental and online processing capability for thesumit67.blogspot.com 1216 chapter 29 ibm db2 universal database figure 29.18 db2 control center instance  one can issue a load command in online mode to allow applications to access the original contents of a table concurrently db2 ? s utilities are all fully enabled to run in parallel mode additionally  db2 supports a number of tools such as  ? audit facility for maintaining the audit trace of database actions ? governor facility for controlling the priority and execution times of different applications ? query patroller facility for managing the query jobs in the system ? trace and diagnostic facilities for debugging ? event monitoring facilities for tracking the resources and events during system execution db2 for os/390 has a very rich set of tools qmf is a widely used tool for generating ad hoc queries and integrating it into applications thesumit67.blogspot.com 29.10 concurrency control and recovery 1217 29.10 concurrency control and recovery db2 supports a comprehensive set of concurrency-control  isolation  and recovery techniques 29.10.1 concurrency and isolation for isolation  db2 supports the repeatable read  rr   read stability  rs   cursor stability  cs   and uncommitted read  ur  modes rr  cs  and ur modes need no further explanation the rs isolation mode locks only the rows that an application retrieves in a unit of work on a subsequent scan  the application is guaranteed to see all these rows  like rr  but might also see new rows that qualify however  this might be an acceptable trade-off for some applications with respect to strict rr isolation typically  the default isolation level is cs applications can choose their level of isolation at the binding stage most commercially available applications are bound using most isolation levels  enabling users to choose the right version of the application for their requirement the various isolation modes are implemented by using locks db2 supports record-level and table-level locks a separate lock-table data structure is maintained with the lock information db2 escalates from record-level to table-level locks if the space in the lock table becomes tight db2 implements strict two-phase locking for all update transactions write locks or update locks are held until commit or rollback time figure 29.19 shows the different lock modes and their descriptions the set of lock modes supported includes intent locks at the table lock mode in  intent none  is  intent share  ns  next key share  s  share  ix  intent exclusive  six  share with intent exclusive  u  update  nx  next-key exclusive  x  exclusive  z  superexclusive  objects interpretation tablespaces  tables tablespaces  tables rows rows  tables tablespaces  tables tables rows  tables rows rows  tables tablespaces  tables read with no row locks read with row locks read locks for rs or cs isolation levels read lock intend to update rows no read locks on rows but x locks on updated rows update lock but allows others to read next key lock for inserts/deletes to prevent phantom reads during rr index scans only uncommi  ed readers allowed complete exclusive access figure 29.19 db2 lock modes thesumit67.blogspot.com 1218 chapter 29 ibm db2 universal database level in order to maximize concurrency also  db2 implements next-key locking and variant schemes for updates affecting index scans to eliminate the halloween and phantom-read problems the transaction can set the lock granularity to table level by using the lock table statement this is useful for applications that know their desired level of isolation is at the table level also  db2 chooses the appropriate locking granularities for utilities such as reorg and load the offline versions of these utilities usually lock the table in exclusive mode the online versions of the utilities allow other transactions to proceed concurrently by acquiring row locks a deadlock detection agent is activated for each database and periodically checks for deadlocks between transactions the interval for deadlock detection is a configurable parameter in case of a deadlock  the agent chooses a victim and aborts it with a deadlock sql error code 29.10.2 commit and rollback applications can commit or roll back by using explicit commit or rollback statements applications can also issue begin transaction and end transaction statements to control the scope of transactions nested transactions are not supported normally  db2 releases all locks that it holds on behalf of a transaction at commit or rollback however  if a cursor statement has been declared by using the with hold clause  then some locks are maintained across commits 29.10.3 logging and recovery db2 implements strict aries logging and recovery schemes.write-ahead logging is employed to flush log records to the persistent log file before data pages are written or at commit time db2 supports two types of log modes  circular logging and archive logging in circular logging  a predefined set of primary and secondary log files is used circular logging is useful for crash recovery or application failure recovery in archival logging  db2 creates new log files and the old log files must be archived in order to free up space in the file system archival logging is required to perform roll-forward recovery in both cases  db2 allows the user to configure the number of log files and the sizes of the log files in update-intensive environments  db2 can be configured to look for group commits in order to bunch log writes db2 supports transaction rollback and crash recovery as well as point-in-time or roll-forward recovery in the case of crash recovery  db2 performs the standard phases of undo processing and redo processing up to and from the last checkpoint in order to recover the proper committed state of the database for point-in-time recovery  the database can be restored from a backup and can be rolled forward to a specific point in time  using the archived logs the roll-forward recovery command supports both database and tablespace levels it can also be issued on specific nodes on a multinode system a parallel recovery scheme improves the performance in smp systems by utilizing many cpus db2 performs coordinated recovery across mpp nodes by implementing a global checkpointing scheme thesumit67.blogspot.com 29.11 system architecture 1219 29.11 system architecture figure 29.20 shows some of the different processes or threads in a db2 server remote client applications connect to the database server by using communication agents such as db2tcpcm each application is assigned an agent  coordinator agent in mpp or smp environments  called the db2agent thread this agent and its subordinate agents perform the application-related tasks each database has a set of processes or threads that performs tasks such as prefetching  page cleaning from buffer pool  logging  and deadlock detection finally  there is a set of agents at the level of the server to perform tasks such as crash detection  license server  process creation  and control of system resources db2 provides configuration parameters to control the number of threads and processes in a server.almost all the different types of agents can be controlled by using the configuration parameters figure 29.21 shows the different types of memory segments in db2 private memory in agents or threads is mainly used for local variables and data structures that are relevant only for the current activity for example  a private sort could allocate memory from the agent ? s private heap shared memory is partitioned into server shared memory  database shared memory  and application shared memory processing model  single partition remote client machine server machine db2agent db2agntp db2agntp db2pclnr db2wdog db2gds db2resyn db2dart db2sysc db2cart db2pfchr db2loggi db2dlock db2udfp db2dari db2agent db2agntp db2agntp db2agntp db2agent db2agent db2agntp app b " sql connect to test " app c " sql connect to prod " userdb2 processes per-instance edus fenced udfs processes fenced stored procedure processes active subagent coordinator agent tcpip idle subagentsg unassociated idle agents per-request edus per-connection edus per-active database app a app b database " test " db2pclnr db2pfchr db2loggi db2dlock db2bm  db2med   app c database " prod " db2ipccm db2tcpcm app a " sql connect to test " shared memory and semaphores processes  threads  figure 29.20 process model in db2 thesumit67.blogspot.com 1220 chapter 29 ibm db2 universal database database shared memory database shared memory instance shared memory application shared memory agent private memory internal structures  appl_ctl_heap_sz  private sorts  sortheap  sheapthresh  application heap  applheapsz  agent stack  agent_stack_sz  query heap  query_heap_sz  statement heap  stmtheap  statistics heap  stat_heap_sz  buffer pools  buffpage or alterbuf  lock list  locklist  package cache  pckcachesz  shared sorts  sortheap  sheapthresh  database heap  dbheap  log buffer  logbufsz  catalog cache  catalogcache_sz  utility heap  util_heap_sz  includes fcm  fast 1...maxappls 1...maxagents 1...numdb communication manager  figure 29.21 db2 memory model the database-level shared memory contains useful data structures such as the buffer pool  lock lists  application package caches  and shared sort areas the server and application shared memory areas are primarily used for common data structures and communication buffers db2 supports multiple buffer pools for a database buffer pools can be created by using the create bufferpool statement and can be associated with tablespaces multiple buffer pools are useful for a variety of reasons but they should be defined after a careful analysis of the workload requirements db2 supports a comprehensive list of memory configuration and tuning parameters this includes parameters for all the large data structure heap areas such as the default buffer pool  the sort heap  package cache  application-control heaps  and the lock-list area 29.12 replication  distribution  and external data db2 replication is a product in the db2 family that provides replication capabilities among other db2 relational data sources such as oracle  microsoft sql server  sybase adaptive server enterprise  and informix  as well as nonrelational data sources such as ibm ? s ims it consists of capture and apply components  which are controlled by administration interfaces the change-capture mechanisms are either ? log-based ? for db2 tables or ? trigger-based ? in the case of other data sources the captured changes are stored in temporary staging table areas under the control of db2 replication these staged intermediate tables with changes are thesumit67.blogspot.com 29.13 business intelligence features 1221 then applied to destination tables using regular sql statements  inserts  updates  and deletes sql-based transformations can be performed on the intermediate staging tables by using filtering conditions as well as aggregations the resulting rows can be applied to one or more target tables.all of these actions are controlled by the administration facility db2 supports a feature called queue replication queue  q  replication creates a queue transport mechanism using ibm ? s message-queue product to ship captured log records as messages these messages are extracted from the queues at the receiving end and applied against targets the apply process can be parallelized and allows for user-specified conflict resolution rules anothermember of the db2 family is the db2 information-integrator product  which provides federation  replication  using the replication engine described above   and search capabilities the federated edition integrates tables in remote db2 or other relational databases into a single distributed database users and developers can access various nonrelational data sources in tabular format  using wrapper technology the federation engine provides a cost-based method for query optimization across the different data sites db2 supports user-defined table functions that enable access to nonrelational and external data sources user-defined table functions are created by using the create function statementwith the clause returns table using these features  db2 is able to participate in the ole db protocols finally  db2 provides full support for distributed transaction processing using the two-phase commit protocol db2 can act as the coordinator or agent for distributed xa support as a coordinator  db2 can perform all stages of the twophase commit protocol as a participant  db2 can interact with any commercial distributed transaction manager 29.13 business intelligence features db2 datawarehouse edition is an offering in the db2 family that incorporates business intelligence features data warehouse edition has at its foundation the db2 engine  and enhances it with features for etl  olap  mining  and online reporting the db2 engine provides scalability using its mpp features in the mpp mode  db2 can support configurations that can scale to several hundreds of nodes for large database sizes  terabytes   additionally  features such as mdc and mqt provide support for the complex query-processing requirements of business intelligence another aspect of business intelligence is online analytical processing or olap the db2 family includes a feature called cube views that provides a mechanism to construct appropriate data structures and mqts inside db2 that can be used for relational olap processing cube views provide modeling support for multidimensional cubes and provides a mapping mechanism to a relational star schema this model is then used to recommend appropriate mqts  indices  and mdcdefinitions to improve the performance of olap queries against the database in addition  cube views can take advantage of db2 ? s native support for the cube by and rollup operations for generating aggregated cubes cube views is a tool thesumit67.blogspot.com 1222 chapter 29 ibm db2 universal database that can be used to integrate db2 tightly with olap vendors such as business objects  microstrategy  and cognos in addition  db2 also provides multidimensional olap support using the db2 olap server the db2 olap server can create a multidimensional data mart from an underlying db2 database for analysis by olap techniques the olap engine fromthe essbase product is used in the db2 olap server db2 alphablox is a new feature that provides online  interactive  reporting  and analysis capabilities a very attractive feature of the alphablox feature is the ability to construct newweb-based analysis forms rapidly  using a building block approach called blox for deep analytics  db2 intelligent miner provides various components for modeling  scoring  and visualizing data mining enables users to perform classification  prediction  clustering  segmentation  and association against large data sets bibliographical notes the origin of db2 can be traced back to the system r project  chamberlin et al  1981    ibm research contributions include areas such as transaction processing  write-ahead logging and aries recovery algorithms   mohan et al  1992    query processing and optimization  starburst   haas et al  1990    parallel processing  db2 parallel edition   baru et al  1995    active database support  constraints  triggers   cochrane et al  1996    advanced query and warehousing techniques such as materialized views  zaharioudakis et al  2000   lehner et al  2000    multidimensional clustering  padmanabhan et al  2003   bhattacharjee et al  2003    autonomic features  zilio et al  2004    and object-relational support  adts  udfs   carey et al  1999    multiprocessor query-processing details can be found in baru et al  1995   don chamberlin ? s books provide a good review of the sql and programming features of earlier versions of db2  chamberlin  1996   chamberlin  1998    earlier books by c j date and others provide a good review of the features of db2 universal database for os/390  date  1989   martin et al  1989    the db2 manuals provide the definitive view of each version of db2 most of these manuals are available online  http  //www.software.ibm.com/db2   books on db2 for developers and administrators include gunning  2008   zikopoulos et al  2004   zikopoulos et al  2007  and zikopoulos et al  2009   thesumit67.blogspot.com chapter30 microsoft sql server sameet agarwal  jose ? a blakeley  thierry d ? hers  ketan duvedi  ce ? sar a galindo-legaria  gerald hinson  dirk myers  vaqar pirzada  bill ramos  balaji rathakrishnan  jack richins  michael rys  florian waas  michael zwilling microsoft sql server is a relational database-management system that scales from laptops and desktops to enterprise servers  with a compatible version  based on the windows mobile operating system  available for handheld devices such as pocket pcs  smartphones  and portablemedia centers sql serverwas originally developed in the 1980s at sybase for unix systems and later ported to windows nt systems by microsoft since 1994  microsoft has shipped sql server releases developed independently of sybase,which stopped using the sql server name in the late 1990s the latest release  sql server 2008  is available in express  standard  and enterprise editions and localized for many languages around the world in this chapter  the term sql server refers to all of these editions of sql server 2008 sql server provides replication services among multiple copies of sql server and with other database systems its analysis services  an integral part of the system  includes online analytical processing  olap  and data-mining facilities sql server provides a large collection of graphical tools and ? wizards ? that guide database administrators through tasks such as setting up regular backups  replicating data among servers  and tuning a database for performance many development environments support sql server  including microsoft ? s visual studio and related products  in particular the .net products and services 30.1 management  design  and querying tools sql server provides a suite of tools for managing all aspects of sql server development  querying  tuning  testing  and administration most of these tools center around the sql server management studio sql servermanagement stu 1223 thesumit67.blogspot.com 1224 chapter 30 microsoft sql server dio provides a common shell for administering all services associated with sql server  which includes database engine  analysis services  reporting services  sql servermobile  and integration services 30.1.1 database development and visual database tools while designing a database  the database administrator creates database objects such as tables  columns  keys  indices  relationships  constraints  and views to help create these objects  the sql server management studio provides access to visual database tools these tools provide three mechanisms to aid in database design  the database designer  the table designer  and the view designer the database designer is a visual tool that allows the database owner or the owner ? s delegates to create tables  columns  keys  indices  relationships  and constraints within this tool  a user can interact with database objects through database diagrams  which graphically show the structure of the database the viewdesigner provides a visual query tool that allows the user to create or modify sql views through the use of windows drag-and-drop capabilities figure 30.1 shows a view opened from the management studio 30.1.2 database query and tuning tools sql server management studio provides several tools to aid the application development process queries and stored procedures can be developed and tested using the integrated query editor the query editor supports creating and editing scripts for a variety of environments  including transact-sql  the sql server scripting language sqlcmd  the multidimensional expression language mdx which is used for data analysis  the sql server data-mining language dmx  the xml-analysis language xmla  and sql server mobile further analysis can be done using the sql serverprofiler database tuning recommendations are provided by the database tuning advisor 30.1.2.1 query editor the integrated query editor provides a simple graphical user interface for running sql queries and viewing the results the query editor also provides a graphical representation of showplan  the steps chosen by the optimizer for query execution the query editor is integrated with management studio ? s object explorer  which lets a user drag and drop object or table names into a query window and helps build select  insert  update  or delete statements for any table a database administrator or developer can use query editor to  ? analyze queries  query editor can show a graphical or textual execution plan for any query  as well as displaying statistics regarding the time and resources required to execute any query ? format sql queries  including indenting and color syntax coding thesumit67.blogspot.com 30.1 management  design  and querying tools 1225 figure 30.1 the view designer opened for the humanresources.vemployee view ? use templates for stored procedures  functions  and basic sql statements  the management studio comes with dozens of predefined templates for building ddl commands  or users can define their own figure 30.2 shows the management studio with the query editor displaying the graphical execution plan for a query involving a four-table join and an aggregation 30.1.2.2 sql profiler sql profiler is a graphical utility that allows database administrators to monitor and record database activity of the sql server database engine and analysis services sql profiler can display all server activity in real time  or it can create filters thesumit67.blogspot.com 1226 chapter 30 microsoft sql server figure 30.2 a showplan for a four-table join with group by aggregation that focus on the actions of particular users  applications  or types of commands sql profiler can display any sql statement or stored procedure sent to any instance of sql server  if the security privileges allow it  in addition to performance data indicating how long the query took to run  how much cpu and i/o was needed  and the execution plan that the query used sql profiler allows drilling down even deeper into sql server to monitor every statement executed as part of a stored procedure  every data modification operation  every lock acquired or released  or every occurrence of a database file growing automatically dozens of different events can be captured  and dozens of data items can be captured for each event sql server actually divides the tracing functionality into two separate but connected components the sql profiler is the client-side trace facility using sql profiler  a user can choose to save the captured data to a file or a table  in addition to displaying it in the profiler user interface  ui   the profiler tool displays every event that meets the filter criteria as it occurs once trace data are saved  sql profiler can read the saved data for display or analysis purposes thesumit67.blogspot.com 30.1 management  design  and querying tools 1227 on the server side is the sql trace facility  which manages queues of events generated by event producers a consumer thread reads events from the queues and filters them before sending them to the process that requested them events are the main unit of activity as far as tracing is concerned  and an event can be anything that happens inside sql server  or between sql server and a client for example  creating or dropping an object is an event  executing a stored procedure is an event  acquiring or releasing a lock is an event  and sending a transact-sql batch from a client to the sql server is an event there is a set of stored system procedures to define which events should be traced  what data for each event are interesting  and where to save the information collected from the events filters applied to the events can reduce the amount of information collected and stored sql server guarantees that certain critical information will always be gathered  and it can be used as a useful auditing mechanism sql server is certified for u.s government c2-level security  and many of the traceable events are available solely to support c2-certification requirements 30.1.2.3 the database tuning advisor queries and updates can often execute much faster if an appropriate set of indices is available designing the best possible indices for the tables in a large database is a complex task ; it not only requires a thorough knowledge of how sql server uses indices and how the query optimizer makes its decisions  but how the data will actually be used by applications and interactive queries the sql server database tuning advisor  dta  is a powerful tool for designing the best possible indices and indexed  materialized  views based on observed query and update workloads dta can tune across multiple databases and it bases its recommendations on a workload that can be a file of captured trace events  a file of sql statements  or an xml input file sql profiler can be used to capture all sql statements submitted by all users over a period of time dta can then look at the data access patterns for all users  for all applications  for all tables  and make balanced recommendations 30.1.3 sql server management studio in addition to providing access to the database design and visual database tools  the easy-to-use sql server management studio supports centralized management of all aspects of multiple installations of the sql server database engine  analysis services  reporting services  integration services  and sql server mobile  including security  events  alerts  scheduling  backup  server configuration  tuning  full-text search  and replication sql server management studio allows a database administrator to create  modify  and copy sql server database schemas and objects such as tables  views  and triggers because multiple installations of sql server can be organized into groups and treated as a unit  sql server management studio can manage hundreds of servers simultaneously although it can run on the same computer as the sql server engine  sql server management studio offers the same management capabilities while running on thesumit67.blogspot.com 1228 chapter 30 microsoft sql server figure 30.3 the sql server management studio interface any windows 2000  or later  machine in addition  the efficient client ? server architecture of sql server makes it practical to use the remote-access  dial-up networking  capabilities of windows for administration and management sql server management studio relieves the database administrator from having to know the specific steps and syntax to complete a job it provides wizards to guide the database administrator through the process of setting up and maintaining an installation of sql server management studio ? s interface is shown in figure 30.3 and illustrates how a script for database backup can be created directly from its dialogs 30.2 sql variations and extensions sql server allows application developers towrite server-side business logic using transact-sql or a .net programming language such as c #  visual basic  cobol  or j + +  transact-sql is a complete database programming language that includes data-definition and data-manipulation statements  iterative and conditional statements  variables  procedures  and functions transact-sql supports most of the mandatory ddl query and data modification statements and constructs in the thesumit67.blogspot.com 30.2 sql variations and extensions 1229 sql  2003 standard see section 30.2.1 for the list of sql  2003 data types supported in addition to the mandatory features  transact-sql also supports many optional features in the sql  2003 standard such as recursive queries  common table expressions  user-defined functions  and relational operators such as intersect and except among others 30.2.1 data types sql server 2008 supports all the mandatory scalar data types in the sql  2003 standard sql server also supports the ability to alias system types using usersupplied names ; the aliasing is similar in functionality to the sql  2003 distinct types  but not fully compliant with them some primitive types unique to sql server include  ? large character and binary string types of variable size up to 231  1 bytes  using the varchar/nvarchar/varbinary  max  data type,which has a programming model that is similar to the small-character and byte-string types additionally  they support a storage attribute called filestream to specify that data for each individual column value is stored as a separate file in the filesystem filestream storage allows higher performance streaming access to applications using the native filesystem api ? an xml type  described in section 30.11  which is used to store xml data inside a table column the xml type can optionally have an associated xml schema collection specifying a constraint that the instances of the type should adhere to one of the xml types defined in the schema collection ? sql variant is a scalar data type that can contain values of any sql scalar type  except large character and binary types and sql variant   this type is used by applications that need to store data whose type can not be anticipated at data-definition time sql variant is also the type of a column formed from the execution of an unpivot relational operator  see section 30.2.2   internally  the system keeps track of the original type of the data it is possible to filter  join  and sort on sql variant columns the system function sql variant property returns details on the actual data stored in a column of type sql variant  including the base type and size information ? the hierarchyid data type makes it easier to store and query hierarchical data hierarchical data are defined as a set of data items related to one another by hierarchical relationships where one item of data is the parent of another item common examples include  an organizational structure  a hierarchical file system  a set of tasks in a project  a taxonomy of language terms  a singleinheritance type hierarchy  part-subpart relationships  and a graph of links among web pages ? sql server supports storing and querying of geospatial data  that is  location data referenced to the earth common models of these data are the planar and geodetic coordinate systems the main distinction between these two systems is that the latter takes into account the curvature of the earth sql thesumit67.blogspot.com 1230 chapter 30 microsoft sql server server supports geometry and geography  which correspond to the planar and geodetic models in addition  sql server supports a table type and a cursor type that can not be used as columns in a table  but can be used in the transact-sql language as variables  ? a table type enables a variable to hold a set of rows an instance of this type is used primarily to hold temporary results in a stored procedure or as the return value of a table-valued function a table variable behaves like a local variable it has a well-defined scope  which is the function  stored procedure  or batch in which it is declared.within its scope  a table variable may be used like a regular table it may be applied anywhere a table or table expression is used in select  insert  update  and delete statements ? a cursor type that enables references to a cursor object the cursor type can be used to declare variables  or routine input/output arguments to reference cursors across routine calls 30.2.2 query language enhancements in addition to the sql relational operators such as inner join and outer join  sql server supports the relational operators pivot  unpivot  and apply ? pivot is an operator that transforms the shape of its input result set from two columns that represent name-value pairs intomultiple columns  one for each name from the input the name column from the input is called the pivot column the user needs to indicatewhich names to transpose fromthe input into individual columns in the output consider the table monthlysales  productid  month  salesqty   the following query  using the pivot operator  returns the salesqty for each of the months jan  feb  and mar as separate columns note that the pivot operator also performs an implicit aggregation on all the other columns in the table and an explicit aggregation on the pivot column select * from monthlysales pivot  sum  salesqty  for month in  ? jan ?  ? feb ?  ? mar ?   t ; the inverse operation of pivot is unpivot ? the apply operator is similar to join  except its right input is an expression that may contain references to columns in the left input  for example a tablevalued function invocation that takes as arguments one ormore columns from the left input the set of columns produced by the operator is the union of the columns from its two inputs the apply operator can be used to evaluate its right input for each row of its left input and perform a union all of the rows across all these evaluations there are two flavors of the apply operator similar to join  namely  cross and outer the two flavors differ in terms of thesumit67.blogspot.com 30.2 sql variations and extensions 1231 how they handle the case of the right input producing an empty result-set in the case of cross apply  this causes the corresponding row from the left input to not appear in the result in the case of outer apply  the row appears from the left input with null values for the columns in the right input consider a table-valued function called findreports that takes as input the id of a given employee and returns the set of employees reporting directly or indirectly to that employee in an organization the following query calls this function for the manager of each department from the departments table  select * from departments d cross apply findreports  d.managerid  30.2.3 routines users can write routines that run inside the server process as scalar or table functions  stored procedures  and triggers using transact-sql or a .net language all these routines are defined to the database by using the corresponding create  function  procedure  trigger  ddl statement scalar functions can be used in any scalar expression inside an sql dml or ddl statement table-valued functions can be used anywhere a table is allowed in a select statement transact-sql tablevalued functions whose body contains a single sql select statement are treated as a view  expanded inline  in the query that references the function since tablevalued functions allow input arguments  inline table-valued functions can be considered parameterized views 30.2.3.1 indexed views in addition to traditional views as defined in ansi sql  sql server supports indexed  materialized  views indexed views can substantially enhance the performance of complex decision support queries that retrieve large numbers of base table rows and aggregate large amounts of information into concise sums  counts  and averages sql server supports creating a clustered index on a view and subsequently any number of nonclustered indices once a view is indexed  the optimizer can use its indices in queries that reference the view or its base tables there is no need for queries to refer to the view explicitly for the indexed view to be used in the query plan  as the matching is done automatically from the view definition this way  existing queries can benefit from the improved efficiency of retrieving data directly from the indexed view without having to be rewritten the indexed view is maintained consistent with the base tables by automatically propagating all updates 30.2.4 filtered indexes a filtered index is an optimized nonclustered index  especially suited to cover queries that select from a well-defined subset of data it uses a filter predicate to index a portion of rows in the table a well-designed filtered index can improve thesumit67.blogspot.com 1232 chapter 30 microsoft sql server query performance  reduce index-maintenance costs  and reduce index-storage costs compared with full-table indices filtered indices can provide the following advantages over full-table indices  ? improved query performance and plan quality a well-designed filtered index improves query performance and execution plan quality because it is smaller than a full-table nonclustered index and has filtered statistics the filtered statistics are more accurate than full-table statistics because they cover only the rows in the filtered index ? reduced index maintenance costs an index is maintained only when data manipulation language  dml  statements affect the data in the index a filtered index reduces index maintenance costs compared to a full-table nonclustered index because it is smaller and is only maintained when the data in the index are affected it is possible to have a large number of filtered indices  especially when they contain data that are affected infrequently similarly  if a filtered index contains only the frequently affected data  the smaller size of the index reduces the cost of updating the statistics ? reduced index storage costs.creating a filtered index can reduce disk storage for nonclustered indices when a full-table index is not necessary you can replace a full-table nonclustered index with multiple filtered indices without significantly increasing the storage requirements filtered statistics can also be created explicitly  independently from filtered indices 30.2.4.1 updatable views and triggers generally  views can be the target of update  delete  or insert statements if the data modification applies to only one of the view ? s base tables updates to partitioned views can be propagated to multiple base tables for example  the following update will increase the prices for publisher ? 0736 ? by 10 percent  update titleview set price = price * 1.10 where pub id = ? 0736 ? ; for data modifications that affect more than one base table  the view can be updated if there is an instead trigger defined for the operation ; instead triggers for insert  update  or delete operations can be defined on a view  to specify the updates that must be performed on the base tables to reflect the corresponding modifications on the view triggers are transact-sql or .net procedures that are automatically executed when either a dml  update  insert  or delete  or ddl statement is issued against a base table or view triggers are mechanisms that enable enforcement of business logic automatically when data are modified or when ddl statements are executed triggers can extend the integrity checking logic of declarative constraints  thesumit67.blogspot.com 30.3 storage and indexing 1233 defaults  and rules  although declarative constraints should be used preferably whenever they suffice  as they can be used by the query optimizer to reason about the data contents triggers can be classified into dml and ddl triggers depending on the kind of event that fires the trigger dml triggers are defined against a table or view that is being modified ddl triggers are defined against an entire database for one or more ddl statements such as create table  drop procedure  etc triggers can be classified into after and instead triggers according to when the trigger gets invoked relative to the action that fires the trigger after triggers execute after the triggering statement and subsequent declarative constraints are enforced instead triggers execute instead of the triggering action instead triggers can be thought of as similar to before triggers  but they actually replace the triggering action in sql server  dml after triggers can be defined only on base tables,while dml instead triggers can be defined on base tables or views instead triggers allow practically any view to be made updatable via user-provided logic ddl instead triggers can be defined on any ddl statement 30.3 storage and indexing in sql server  a database refers to a collection of files that contain data and are supported by a single transaction log the database is the primary unit of administration in sql server and also provides a container for physical structures such as tables and indices and for logical structures such as constraints and views 30.3.1 filegroups in order to manage space effectively in a database  the set of data files in a database is divided into groups called filegroups each filegroup contains one or more operating-system files every database has at least one filegroup known as the primary filegroup this filegroup contains all the metadata for the database in system tables the primary filegroup may also store user data if additional  user-defined filegroups have been created  a user can explicitly control the placement of individual tables  indices  or the large-object columns of a table by placing them in a particular filegroup for example  the user may choose to store performance critical indices on a filegroup located on solid state disks likewise they may choose to place varbinary  max  columns containing video data on an i/o subsystem optimized for streaming 30.3.2 space management within filegroups one of the main purposes for filegroups is to allow for effective space management all data files are divided into fixed-size 8-kilobyte units called pages the allocation system is responsible for allocating these pages to tables and indices thesumit67.blogspot.com 1234 chapter 30 microsoft sql server the goal of the allocation system is to minimize the amount of space wasted while  at the same time  keeping the amount of fragmentation in the database to a minimum to ensure good scan performance in order to achieve this goal  the allocation manager usually allocates and deallocates all the pages in units of eight contiguous pages called extents the allocation system manages these extents through various bitmaps these bitmaps allow the allocation system to find a page or an extent for allocation quickly these bitmaps are also used when a full table or index scan is executed the advantage of using allocation-based bitmaps for scanning is that it allows disk-order traversals of all the extents belonging to a table or index-leaf level  which significantly improves the scan performance if there is more than one file in a filegroup  the allocation system allocates extents for any object on that filegroup by using a ? proportional fill ? algorithm each file is filled up in the proportion of the amount of free space in that file compared to other files this fills all the files in a filegroup at roughly the same rate and allows the system to utilize all the files in the filegroup evenly files can also be configured to grow automatically if the filegroup is running out of space sql server allows files to shrink in order to shrink a data file  sql server moves all the data from the physical end of the file to a point closer to the beginning of the file and then actually shrinks the file  releasing space back to the operating system 30.3.3 tables sql server supports heap and clustered organizations for tables in a heaporganized table  the location of every row of the table is determined entirely by the system and is not specified in any way by the user the rows of a heap have a fixed identifier known as the row  rid   and this value never changes unless the file is shrunk and the row ismoved if the row becomes large enough that it can not fit in the page in which it was originally inserted  the record is moved to a different place but a forwarding stub is left in the original place so that the record can still be found by using its original rid in a clustered-index organization for a table  the rows of the table are stored in a b + -tree sorted by the clustering key of the index the clustered-index key also serves as the unique identifier for each row the key for a clustered index can be defined to be nonunique  in which case sql server adds an additional hidden column to make the key unique the clustered index also serves as a search structure to identify a row of the table with a particular key or scan a set of rows of the table with keys within a certain range a clustered index is the most common type of table organization 30.3.4 indices sql server also supports secondary  nonclustered  b + -tree indices queries that refer only to columns that are available through secondary indices are processed by retrieving pages from the leaf level of the indices without having to retrieve data from the clustered index or heap nonclustered indices over a table with a thesumit67.blogspot.com 30.3 storage and indexing 1235 clustered index contain the key columns of the clustered index thus  the clustered index rows canmove to a different page  via splits  defragmentation  or even index rebuilds  without requiring changes to the nonclustered indices sql server supports the addition of computed columns to a table.acomputed column is a column whose value is an expression  usually based on the value of other columns in that row sql server allows the user to build secondary indices on computed columns 30.3.5 partitions sql server supports range partitioning on tables and nonclustered indices a partitioned index is made up of multiple b + -trees  one per partition.apartitioned table without an index  a heap  is made up of multiple heaps  one per partition for brevity  we refer only to partitioned indices  clustered or nonclustered  and ignore heaps for the rest of this discussion partitioning a large index allows an administrator more flexibility in managing the storage for the index and can improve some query performance because the partitions act as a coarse-grained index the partitioning for an index is specified by providing both a partitioning function and a partitioning scheme a partitioning function maps the domain of a partitioning column  any column in the index  to partitions numbered 1 to n a partitioning scheme maps partition numbers produced by a partitioning function to specific filegroups where the partitions are stored 30.3.6 online index build building new indices and rebuilding existing indices on a table can be performed online  i.e  while select  insert  delete  and update operations are being performed on the table the creation of a new index happens in three phases the first phase is simply creating an empty b + -tree for the new index with the catalog showing the new index is available for maintenance operations that is  the new index must be maintained by all subsequent insert  delete  and update operations  but it is not available for queries the second phase consists of scanning the table to retrieve the index columns for each row  sorting the rows and inserting them into the new b + -tree these inserts must be careful to interactwith the other rows in the new b + -tree placed there by index maintenance operations from updates on the base table the scan is a snapshot scan that  without locking  ensures the scan sees the entire table with only the results of committed transactions as of the start of the scan this is achieved by using the snapshot isolation technique described in section 30.5.1 the final phase of the index build involves updating the catalog to indicate the index build is complete and the index is available for queries 30.3.7 scans and read-ahead execution of queries in sql server can involve a variety of different scanmodes on the underlying tables and indices these include ordered versus unordered scans  thesumit67.blogspot.com 1236 chapter 30 microsoft sql server serial versus parallel scans  unidirectional versus bidirectional scans  forward versus backward scans  and entire table or index scans versus range or filtered scans each of the scan modes has a read-ahead mechanism that tries to keep the scan ahead of the needs of the query execution  in order to reduce seek and latency overheads and utilize disk idle time the sql server read-ahead algorithm uses the knowledge from the query-execution plan in order to drive the read-ahead and make sure that only data that are actually needed by the query are read also  the amount of read-ahead is automatically scaled according to the size of the buffer pool  the amount of i/o the disk subsystem can sustain  and the rate at which the data are being consumed by query execution 30.3.8 compression sql server supports both row and page compression for tables and indices row compression uses a variable-length format for data types such as integers that are traditionally considered fixed-length page compression removes common prefixes on columns and builds a per-page dictionary for common values 30.4 query processing and optimization the query processor of sql server is based on an extensible framework that allows rapid incorporation of new execution and optimization techniques any sql query can be expressed as a tree of operators in an extended relational algebra abstracting operators of this algebra into iterators  query execution encapsulates data-processing algorithms as logical units that communicate with each other by using a getnextrow   interface starting out with an initial query tree  the query optimizer generates alternatives by using tree transformations and estimates their execution cost by taking into account iterator behavior and statistical models to estimate the number of rows to process 30.4.1 overview of compilation process complex queries present significant optimization opportunities that require reordering operators across query block boundaries and selecting plans solely on the basis of estimated costs to go after these opportunities  the query optimizer deviates from traditional query-optimization approaches used in other commercial systems in favor of a more general  purely algebraic framework that is based on the cascades optimizer prototype query optimization is part of the querycompilation process  which consists of four steps  ? parsing/binding after parsing  the binder resolves table and column names by using the catalogs sql server utilizes a plan cache to avoid repeated optimization of identical or structurally similar queries if no cached plan is available  an initial operator tree is generated the operator tree is simply a thesumit67.blogspot.com 30.4 query processing and optimization 1237 combination of relational operators and is not constrained by concepts such as query blocks or derived tables  which typically obstruct optimization ? simplification/normalization the optimizer applies simplification rules on the operator tree to obtain a normal  simplified form during simplification  the optimizer determines and loads statistics required for cardinality estimation ? cost-based optimization the optimizer applies exploration and implementation rules to generate alternatives  estimates execution cost  and chooses the plan with the cheapest anticipated cost exploration rules implement reordering for an extensive set of operators  including join and aggregation reordering implementation rules introduce execution alternatives such as merge join and hash join ? plan preparation the optimizer creates query-execution structures for the selected plan to achieve best results  cost-based optimization is not divided into phases that optimize different aspects of the query independently ; also  it is not restricted to a single dimension such as join enumeration instead  a collection of transformation rules defines the space of interest  and cost estimation is used uniformly to select an efficient plan 30.4.2 query simplification during simplification  only transformations that are guaranteed to generate less costly substitutes are applied the optimizer pushes selects down the operator tree as far as possible ; it checks predicates for contradictions  taking into account declared constraints it uses contradictions to identify subexpressions that can be removed from the tree a common scenario is the elimination of union branches that retrieve data from tables with different constraints a number of simplification rules are context dependent ; that is  the substitution is valid only in the context of utilization of the subexpression for example  an outer join can be simplified into an inner join if a later filter operation will discard nonmatching rows that were padded with null another example is the elimination of joins on foreign keys  when there are no later uses of columns from the referenced table a third example is the context of duplicate insensitivity  which specifies that delivering one or multiple copies of a row does not affect the query result subexpressions under semijoins and under distinct are duplicate insensitive  which allows turning union into union all  for example for grouping and aggregation  the gbagg operator is used  which creates groups and optionally applies an aggregate function on each group duplicate removal  expressed in sql by the distinct keyword  is simply a gbagg with no aggregate functions to compute during simplification  information about keys and functional dependencies is used to reduce grouping columns subqueries are normalized by removing correlated query specifications and using some join variant instead removing correlations is not a ? subquery executhesumit67 blogspot.com 1238 chapter 30 microsoft sql server tion strategy  ? but simply a normalization step a variety of execution strategies is then considered during cost-based optimization 30.4.3 reordering and cost-based optimization in sql server  transformations are fully integrated into the cost-based generation and selection of execution plans the query optimizer includes about 350 logical and physical transformation rules in addition to inner-join reordering  the query optimizer employs reordering transformations for the operators outer join  semijoin  and antisemijoin  fromthe standard relational algebra  with duplicates  for sql   gbagg is reordered as well  by moving it below or above joins when possible partial aggregation  that is  introducing a new gbagg with grouping on a superset of the columns of a subsequent gbagg  is considered below joins and union all  and also in parallel plans see the references given in the bibliographical notes for details correlated execution is considered during plan exploration  the simplest case being index-lookup join sql server models correlated execution as a single algebraic operator  called apply  which operates on a table t and a parameterized relational expression e  t   apply executes e for each row of t  which provides parameter values correlated execution is considered as an execution alternative  regardless of the use of subqueries in the original sql formulation it is a very efficient strategy when table t is small and indices support efficient parameterized execution of e  t   furthermore  we consider reduction on the number of executions of e  t  when there are duplicate parameter values  by means of two techniques  sort t on parameter values so that a single result of e  t  is reused while the parameter value remains the same  or else use a hash table that keeps track of the result of e  t  for  some subset of  earlier parameter values some applications select rows on the basis of some aggregate result for their group for example  ? find customers whose balance is more than twice the average for their market segment ? the sql formulation requires a self-join during exploration  this pattern is detected and per-segment execution over a single scan is considered as an alternative to self-join materialized-view utilization is also considered during cost-based optimization view matching interacts with operator reordering in that utilization may not be apparent until some other reordering has taken place when a view is found to match some subexpression  the table that contains the view result is added as an alternative for the corresponding expression depending on data distribution and indices available  it may or may not be better than the original expression ? selection will be based on cost estimation to estimate the execution cost of a plan  the model takes into account the number of times a subexpression is executed  as well as the row goal  which is the number of rows expected to be consumed by the parent operator the row goal can be less than the cardinality estimate in the case of top-n queries  and for apply/semijoin for example  apply/semijoin outputs row t from t as soon as a single row is produced by e  t   that is  it tests exists e  t    thus  the row goal of thesumit67.blogspot.com 30.4 query processing and optimization 1239 the output of e  t  is 1  and the row goals of subtrees of e  t  are computed for this row goal for e  t  and used for cost estimation 30.4.4 update plans update plans optimize maintenance of indices  verify constraints  apply cascading actions  and maintain materialized views for index maintenance  instead of taking each row and maintaining all indices for it  update plans may apply modifications per index  sorting rows and applying the update operation in key order this minimizes random i/o  especially when the number of rows to update is large constraints are handled by an assert operator  which executes a predicate and raises an error if the result is false referential constraints are defined by exists predicates,which in turn become semijoins and are optimized by considering all execution algorithms the halloween problem  described earlier in section 13.6  refers to the following anomaly  suppose a salary index is read in ascending order  and salaries are being raised by 10 percent as a result of the update  rows will move forward in the index andwill be found and updated again  leading to an infinite loop one way to address this problem is to separate processing into two phases  first read all rows that will be updated and make a copy of them in some temporary place  then read from this place and apply all updates another alternative is to read from a different index where rows will not move as a result of the update some execution plans provide phase separation automatically  if they sort or build a hash table on the rows to be updated halloween protection is modeled as a property of plans multiple plans that provide the required property are considered  and one is selected on the basis of estimated execution cost 30.4.5 data analysis at optimization time sql pioneered techniques to perform gathering of statistics as part of an ongoing optimization the computation of result size estimates is based on statistics for columns used in a given expression these statistics consist of max-diff histograms on the column values and a number of counters that capture densities and row sizes  among others database administrators may create statistics explicitly by using extended sql syntax if no statistics are available for a given column  however  sql server ? s optimizer puts the ongoing optimization on hold and gathers statistics as needed as soon as the statistics are computed  the original optimization is resumed  leveraging the newly created statistics optimization of subsequent queries reuses previously generated statistics typically  after a short period of time  statistics for frequently used columns have been created and interruptions to gather new statistics become infrequent by keeping track of the number of rows modified in a table  a measure of staleness is maintained for all affected statistics once the staleness exceeds a certain threshold the statistics are recomputed and cached plans are recompiled to take changed data distributions into account statistics can be recomputed asynchronously  which avoids potentially long compile times caused by synchronous computation the optimization that trigthesumit67 blogspot.com 1240 chapter 30 microsoft sql server gers the computation of statistics uses potentially stale statistics however  subsequent queries are able to leverage the recomputed statistics this allows striking an acceptable balance between time spent in optimization and the quality of the resulting query plan 30.4.6 partial search and heuristics cost-based query optimizers face the issue of search-space explosion because applications do issue queries involving dozens of tables to address this  sql server uses multiple optimization stages  each of which uses query transformations to explore successively larger regions of the search space there are simple and complete transformations geared toward exhaustive optimization  as well as smart transformations that implement various heuristics smart transformations generate plans that are very far apart in the search space  while simple transformations explore neighborhoods optimization stages apply a mix of both kinds of transformations  first emphasizing smart transformations  and later transitioning to simple transformations optimum results on subtrees are preserved  so that later stages can take advantage of results generated earlier each stage needs to balance opposing plan generation techniques  ? exhaustive generation of alternatives  to generate the complete space  the optimizer uses complete  local  nonredundant transformations ? a transformation rule that is equivalent to a sequence of more primitive transformations would only introduce additional overhead ? heuristic generation of candidates  a handful of interesting candidates  selected on the basis of estimated cost  are likely to be far apart in terms of primitive transformation rules here  desirable transformations are incomplete  global  and redundant optimization can be terminated at any point after a first plan has been generated such termination is based on the estimated cost of the best plan found and the time spent already in optimization for example  if a query requires only looking up a few rows in some indices  a very cheap plan will likely be produced quickly in the early stages  terminating optimization this approach enabled adding new heuristics easily over time  without compromising either cost-based selection of plans  or exhaustive exploration of the search space  when appropriate 30.4.7 query execution execution algorithms support both sort-based and hash-based processing  and their data structures are designed to optimize use of processor cache hash operations support basic aggregation and join  with a number of optimizations  extensions  and dynamic tuning for data skew the flow-distinct operation is a variant of hash-distinct  where rows are output early  as soon as a new distinct value is found  instead of waiting to process the complete input this operator is thesumit67.blogspot.com 30.5 concurrency and recovery 1241 effective for queries that use distinct and request only a few rows  say using the top n construct correlated plans specify executing e  t   often including some index lookup based on the parameter  for each row t of a table t asynchronous prefetching allows issuing multiple index-lookup requests to the storage engine it is implemented this way  a nonblocking index-lookup request is made for a row t of t  then t is placed in a prefetch queue rows are taken out of the queue and used by apply to execute e  t   execution of e  t  does not require that data be already in the buffer pool  but having outstanding prefetch operations maximizes hardware utilization and increases performance the size of the queue is determined dynamically as a function of cache hits if no ordering is required on the output rows of apply  rows from the queue may be taken out of order  to minimize waiting on i/o parallel execution is implemented by the exchange operator  which manages multiple threads  partitions or broadcasts data  and feeds the data to multiple processes the query optimizer decides exchange placement on the basis of estimated cost the degree of parallelism is determined dynamically at runtime  according to the current system utilization index plans are made up of the pieces described earlier for example  we consider the use of an index join to resolve predicate conjunctions  or index union  for disjunctions   in a cost-based way such a join can be done in parallel  using any of sql server ? s join algorithms we also consider joining indices for the sole purpose of assembling a row with the set of columns needed on a query  which is sometimes faster than scanning a base table taking record ids froma secondary index and locating the corresponding row in a base table is effectively equivalent to performing index-lookup join for this,we use our generic correlated execution techniques such as asynchronous prefetch communication with the storage engine is done through ole-db  which allows accessing other data providers that implement this interface ole-db is the mechanism used for distributed and remote queries  which are driven directly by the query processor data providers are categorized according to the range of functionality they provide  ranging from simple rowset providers with no indexing capabilities to providers with full sql support 30.5 concurrency and recovery sql server ? s transaction  logging  locking  and recovery subsystems realize the acid properties expected of a database system 30.5.1 transactions in sql server all statements are atomic and applications can specify various levels of isolation for each statement a single transaction can include statements that not only select  insert  delete  or update records  but also create or drop tables  build indices  and bulk-import data transactions can span databases on remote servers when transactions are spread across servers  sql server uses a thesumit67.blogspot.com 1242 chapter 30 microsoft sql server windows operating-system service called the microsoft distributed transaction coordinator  ms dtc  to perform two-phase commit processing ms dtc supports the xa transaction protocol and  along with ole-db  provides the foundation for acid transactions among heterogeneous systems concurrency control based on locking is the default for sql server sql server also offers optimistic concurrency control for cursors optimistic concurrency control is based on the assumption that resource conflicts between multiple users are unlikely  but not impossible   and allows transactions to execute without locking any resources only when attempting to change data does sql server check resources to determine if any conflicts have occurred if a conflict occurs  the application must read the data and attempt the change again applications can choose to detect changes either by comparing values or by checking a special row version column on a row sql server supports the sql isolation levels of read uncommitted  read committed  repeatable read  and serializable read committed is the default level in addition  sql server supports two snapshot-based isolation levels  snapshot isolation is described earlier in section 15.7   ? snapshot  specifies that data read by any statement in a transaction will be the transactionally consistent version of the data that existed at the start of the transaction the effect is as if the statements in a transaction see a snapshot of the committed data as it existed at the start of the transaction writes are validated using the validation steps described in section 15.7  and permitted to complete only if the validation is successful ? read committed snapshot  specifies that each statement executed within a transaction sees a transactionally consistent snapshot of the data as it existed at the start of the statement this contrasts with read committed isolation where the statement may see committed updates of transactions that commit while the statement is executing 30.5.2 locking locking is the primary mechanism used to enforce the semantics of the isolation levels all updates acquire sufficient exclusive locks held for the duration of the transaction to prevent conflicting updates from occurring shared locks are held for various durations to provide the different sql isolation levels for queries sql server provides multigranularity locking that allows different types of resources to be locked by a transaction  see figure 30.4  where the resources are listed in order of increasing granularity   to minimize the cost of locking  sql server locks resources automatically at a granularity appropriate to the task locking at a smaller granularity  such as rows  increases concurrency  but has a higher overhead because more locks must be held if many rows are locked the fundamental sql server lock modes are shared  s   update  u   and exclusive  x  ; intent locks are also supported for multigranularity locking.update locks are used to prevent a common form of deadlock that occurs when multiple sessions are reading  locking  and potentially updating resources later.additional thesumit67.blogspot.com 30.5 concurrency and recovery 1243 rid key page extent table db row identifier ; used to lock a single row within a table row lock within an index ; protects key ranges in serializable transactions 8-kilobyte table or index page contiguous group of eight data pages or index pages entire table  including all data and indices database resource description figure 30.4 lockable resources lock modes ? called key-range locks ? are taken only in serializable isolation level for locking the range between two rows in an index 30.5.2.1 dynamic locking fine-granularity locking can improve concurrency at the cost of extra cpu cycles andmemory to acquire and holdmany locks formany queries  a coarser locking granularity provides better performance with no  or minimal  loss of concurrency database systems have traditionally required query hints and table options for applications to specify locking granularity in addition  there are configuration parameters  often static  for how much memory to dedicate to the lock manager in sql server  locking granularity is optimized automatically for optimal performance and concurrency for each index used in a query in addition  the memory dedicated to the lock manager is adjusted dynamically on the basis of feedback from other parts of the system  including other applications on the machine lock granularity is optimized before query execution for each table and index used in the query the lock optimization process takes into account isolation level  that is  how long locks are held   scan type  range  probe  or entire table   estimated number of rows to be scanned  selectivity  percentage of visited rows that qualify for the query   row density  number of rows per page   operation type  scan  update   user limits on the granularity  and available system memory once a query is executing  the lock granularity is escalated automatically to table level if the system acquires significantly more locks than the optimizer expected or if the amount of available memory drops and can not support the number of locks required 30.5.2.2 deadlock detection sql server automatically detects deadlocks involving both locks and other resources for example  if transaction a is holding a lock on table1 and is waiting for memory to become available and transaction b has some memory it can ? t release until it acquires a lock on table1  the transactions will deadlock threads and communication buffers can also be involved in deadlocks when sql server detects a deadlock  it chooses as the deadlock victim the transaction thatwould be thesumit67.blogspot.com 1244 chapter 30 microsoft sql server the least expensive to roll back  considering the amount of work the transaction has already done frequent deadlock detection can hurt system performance sql server automatically adjusts the frequency of deadlock detection to how often deadlocks are occurring if deadlocks are infrequent  the detection algorithm runs every 5 seconds if they are frequent it will begin checking every time a transaction waits for a lock 30.5.2.3 row versioning for snapshot isolation the two snapshot-based isolation levels use row versioning to achieve isolation for queries while not blocking the queries behind updates and vice versa under snapshot isolation  update and delete operations generate versions of the affected rows and store them in a temporary database the versions are garbage-collected when there are no active transactions that could require them therefore  a query run under snapshot isolation does not need to acquire locks and instead can read the older versions of any record that gets updated/deleted by another transaction row versioning is also used to provide a snapshot of a table for online index build operations 30.5.3 recovery and availability sqlserver is designed to recover fromsystem and media failures  and the recovery system can scale to machines with very large buffer pools  100 gigabytes  and thousands of disk drives 30.5.3.1 crash recovery logically  the log is a potentially infinite stream of log records identified by log sequence numbers  lsns   physically  a portion of the stream is stored in log files log records are saved in the log files until they have been backed up and are no longer needed by the system for rollback or replication log files growand shrink in size to accommodate the records that need to be stored additional log files can be added to a database  on new disks  for example  while the system is running and without blocking any current operations  and all logs are treated as if they were one continuous file sql server ? s recovery system has many aspects in common with the aries recovery algorithm  see section 16.8   and some of the key differences are highlighted in this section sql server has a configuration option called recovery interval  which allows an administrator to limit the length of time sql server should take to recover after a crash the server dynamically adjusts the checkpoint frequency to reduce recovery time to within the recovery interval checkpoints flush all dirty pages from the buffer pool and adjust to the capabilities of the i/o system and its current workload to effectively eliminate any impact on running transactions upon start-up after a crash  the system starts multiple threads  automatically scaled to the number of cpus  to start recovering multiple databases in parallel thesumit67.blogspot.com 30.5 concurrency and recovery 1245 the first phase of recovery is an analysis pass on the log  which builds a dirty page table and active transaction list the next phase is a redo pass starting from the last checkpoint and redoing all operations during the redo phase  the dirty page table is used to drive read-ahead of data pages the final phase is an undo phase where incomplete transactions are rolled back the undo phase is actually divided into two parts as sql server uses a two-level recovery scheme transactions at the first level  those involving internal operations such as space allocation and page splits  are rolled back first  followed by user transactions once the transactions at the first level are rolled back  the database is brought online and is available for new user transactions to start while the final rollback operations are performed this is achieved by having the redo pass reacquire locks for all incomplete user transactions that will be rolled back in the undo phase 30.5.3.2 media recovery sql server ? s backup and restore capabilities allow recovery from many failures  including loss or corruption of disk media  user errors  and permanent loss of a server additionally  backing up and restoring databases is useful for other purposes  such as copying a database fromone server to another and maintaining standby systems sql server has three different recoverymodels that users can choose from for each database by specifying a recoverymodel  an administrator declares the type of recovery capabilities required  such as point-in-time restore and log shipping  and the required backups to achieve them backups can be taken on databases  files  file-groups  and the transaction log all backups are fuzzy and completely online ; that is  they do not block any dml or ddl operations while they execute restores can also be done online such that only the portion of the database being restored  e.g  a corrupt disk block  is taken offline backup and restore operations are highly optimized and limited only by the speed of the media onto which the backup is targeted sql server can back up to both disk and tape devices  up to 64 in parallel  and has high-performance backup apis for use by third-party backup products 30.5.3.3 database mirroring database mirroring involves immediately reproducing every update to a database  the principal database  onto a separate  complete copy of the database  the mirror database  generally located on another machine in the event of a disaster on the primary server or even just maintenance  the system can automatically failover to the mirror in a matter of seconds the communication library used by applications is aware of the mirroring and will automatically reconnect to the mirror machine in the event of a failover a tight coupling between the primary database and the mirror is achieved by sending blocks of transaction log to the mirror as it is generated on the primary and redoing the log records on the mirror in full-safety mode  a transaction can not commit until the log records for the transaction have made it to disk on the mirror besides supporting failover  a mirror can also be thesumit67.blogspot.com 1246 chapter 30 microsoft sql server used to automatically restore a page by copying it from the mirror in the event that the page is found to be corrupt during an attempt to read it 30.6 system architecture an sql server instance is a single operating-system process that is also a named endpoint for requests for sql execution applications interact with sql server via various client-side libraries  like odbc  ole-db  and ado.net  in order to execute sql 30.6.1 thread pooling on the server in order to minimize the context switching on the server and to control the degree of multiprogramming  the sql server process maintains a pool of threads that execute client requests as requests arrive from the client  they are assigned a thread on which to execute the thread executes the sql statements issued by the client and sends the results back to it once the user request completes  the thread is returned back to the thread pool in addition to user requests  the thread pool is used to assign threads for internal background tasks such as  ? lazywriter  this thread is dedicated to making sure a certain amount of the buffer pool is free and available at all times for allocation by the system the thread also interacts with the operating system to determine the optimal amount of memory that should be consumed by the sql server process ? checkpoint  this thread periodically checkpoints all databases in order to maintain a fast recovery interval for the databases on server restart ? deadlock monitor  this thread monitors the other threads  looking for a deadlock in the system it is responsible for the detection of deadlocks and also picking a victim in order to allow the system to make progress when the query processor chooses a parallel plan to execute a particular query  it can allocate multiple threads that work on behalf of the main thread to execute the query since the windows nt family of operating systems provides native thread support  sql server uses nt threads for its execution however  sql server can be configured to run with user-mode threads in addition to kernel threads in very high-end systems to avoid the cost of a kernel context switch on a thread switch 30.6.2 memory management there are many different uses of memory within the sql server process  ? buffer pool the biggest consumer of memory in the system is the buffer pool the buffer pool maintains a cache of the most recently used database pages it uses a clock replacement algorithm with a steal  no-force policy ; that is  buffer pages with uncommitted updates may be replaced  ? stolen ?   thesumit67.blogspot.com 30.6 system architecture 1247 and buffer pages are not forced to disk on transaction commit the buffers also obey the write-ahead logging protocol to ensure correctness of crash and media recovery ? dynamic memory allocation this is the memory that is allocated dynamically to execute requests submitted by the user ? plan and execution cache this cache stores the compiled plans for various queries that have been previously executed by users in the system this allows various users to share the same plan  saving memory  and also saves on query compilation time for similar queries ? large memory grants these are for query operators that consume large amounts of memory  such as hash join and sort sql server uses an elaborate scheme of memory management to divide its memory among the various uses described above a single memory manager centrally manages all the memory used by sql server the memory manager is responsible for dynamically partitioning and redistributing the memory between the various consumers of memory in the system it distributes this memory in accordance with an analysis of the relative cost benefit of memory for any particular use a generalized lru infrastructure mechanism is available to all components this caching infrastructure tracks not only the lifetime of cached data but also the relative cpu and i/o costs incurred to create and cache it this information is used to determine the relative costs of various cached data the memory manager focuses on throwing out the cached data that have not been touched recently and were cheap to cache as an example  complex query plans that require seconds of cpu time to compile are more likely to stay in memory than trivial plans  given equivalent access frequencies the memory manager interacts with the operating system to decide dynamically how much memory it should consume out of the total amount of memory in the system this allows sql server to be quite aggressive in using the memory on the system but still return memory back to the system when other programs need it without causing excessive page faults in addition the memory manager is aware of the cpu and memory topology of the system specifically  it leverages the numa  nonuniform memory access  that many machines employ and attempts to maintain locality between the processor that a thread is executing on and the memory it accesses 30.6.3 security sql server provides comprehensive security mechanisms and policies for authentication  authorization  audit  and encryption authentication can be either through a username ? password pair managed by sql server  or through a windows os account authorization is managed by permission grants to schema objects or covering permissions on container objects such as the database or server instance at authorization-check time  permissions are rolled up and calculated  accounting for covering permissions and role memberships of the principal authesumit67 blogspot.com 1248 chapter 30 microsoft sql server dits are defined in the same way as permissions ? they are defined on schema objects for a given principal or containing objects and at the time of the operation they are dynamically calculated based on audit definitions on the object and accounting for any covering audits or role memberships of the principal multiple audits may be defined so that audits for different purposes  such as for sarbanes oxley and hipaa,1 may bemanaged independentlywithout risk of breaking each other audits records are written either in a file or to the windows security log sql server provides both manual encryption of data and transparent data encryption transparent data encryption encrypts all data pages and log pages when written to disk and decrypts when read from the disk so that the data are encrypted at rest on the disk but is plaintext to sql server users without application modification transparent data encryption can be more cpu efficient than manual encryption as data is only encrypted when written to disk and it is done in larger units  pages  rather than individual cells of data two things are even more critical to users ? security   1  the quality of the entire code base itself and  2  the ability for users to determine if they have secured the system properly the quality of the code base is enhanced by using the security development lifecycle all developers and testers of the product go through security training all features are threat modeled to assure assets are appropriately protected wherever possible  sql server utilizes the underlying security features of the operating system rather than implementing its own  such aswindows os authorization and thewindows security log for an audit record furthermore  numerous internal tools are utilized to analyze the code base looking for potential security flaws security is verified using fuzz testing2 and testing of the threatmodel before release  there is a final security review of the product and a response plan is in place for dealing with security issues found after release ? which is then executed as issues are discovered a number of features are provided to help users secure the system properly one such feature is a fundamental policy called off-by-default  where many less commonly used components or those requiring extra care for security  are completely disabled by default another feature is a best-practices analyzer that warns users about configurations of system settings that could lead to a security vulnerability policy-based management further allows users to define what the settings should be and either warns of or prevents changes that would conflict with the approved settings 30.7 data access sql server supports the following application programming interfaces  apis  for building data-intensive applications  1the sarbanes-oxley act is a u.s government financial regulation law hipaa is a u.s government health-care law that includes regulation of health-care-related information 2fuzz testing a randomization-based technique for testing for unexpected  possibly invalid  input thesumit67.blogspot.com 30.7 data access 1249 ? odbc this is microsoft ? s implementation of the standard sql  1999 calllevel interface  cli   it includes object models ? remote data objects  rdos  and data access objects  daos  ? that make it easier to program multitier database applications from programming languages like visual basic ? ole-db this is a low-level  systems-oriented api designed for programmers building database components the interface is architected according to the microsoft component object model  com   and it enables the encapsulation of low-level database services such as rowset providers  isam providers  and query engines ole-db is used inside sql server to integrate the relational query processor and the storage engine and to enable replication and distributed access to sql and other external data sources like odbc  ole-db includes a higher-level object model called activex data objects  ado  to make it easier to program database applications from visual basic ? ado.net this is an api designed for applications written in .net languages such as c # and visual basic.net this interface simplifies some common data access patterns supported by odbc and ole-db in addition  it provides a new data set model to enable stateless  disconnected data access applications ado.net includes the ado.net entity framework  which is a platform for programming against data that raises the level of abstraction fromthe logical  relational  level to the conceptual  entity  level  and thereby significantly reduces the impedance mismatch for applications and data services such as reporting  analysis  and replication the conceptual data model is implemented using an extended relational model  the entity data model  edm  that embraces entities and relationships as first-class concepts it includes a query language for the edm called entity sql  a comprehensive mapping engine that translates from the conceptual to the logical  relational  level  and a set of model-driven tools that help developers define mappings between objects and entities to tables ? linq language-integrated query  or linq for short  allows declarative  setoriented constructs to be used directly in programming languages such as c # and visual basic the query expressions are not processed by an external tool or language preprocessor but instead are first-class expressions of the languages themselves linq allows query expressions to benefit fromthe rich metadata  compile-time syntax checking  static typing and auto-completion that was previously available only to imperative code linq defines a set of general-purpose standard query operators that allow traversal  filter  join  projection  sorting  and grouping operations to be expressed in a direct yet declarative way in any .net-based programming language c # and visual basic also support query comprehensions  i.e  language syntax extensions that leverage the standard query operators ? db-lib the db-library for c api that was developed specifically to be used with earlier versions of sql server that predate the sql-92 standard ? http/soap applications can use http/soap requests to invoke sql server queries and procedures applications can use urls that specify internet inthesumit67 blogspot.com 1250 chapter 30 microsoft sql server formation server  iis  virtual roots that reference an instance of sql server the url can contain an xpath query  a transact-sql statement  or an xml template 30.8 distributed heterogeneous query processing sqlserver distributed heterogeneous query capability allows transactional queries and updates against a variety of relational and nonrelational sources via oledb data providers running in one or more computers sql server supports two methods for referencing heterogeneous ole-db data sources in transact-sql statements the linked-server-names method uses system-stored procedures to associate a server name with an ole-db data source objects in these linked servers can be referenced in transact-sql statements using the four-part name convention described below for example  if a linked server name of deptsqlsrvr is defined against another copy of sql server  the following statement references a table on that server  select * from deptsqlsrvr.northwind.dbo.employees ; an ole-db data source is registered in sql server as a linked server once a linked server is defined  its data can be accessed using the four-part name  < linked server >  < catalog >  < schema >  < object > the following example establishes a linked server to an oracle server via an ole-db provider for oracle  exec sp addlinkedserver orasvr  ? oracle 7.3 ?  ? msdaora ?  ? oracleserver ? a query against this linked server is expressed as  select * from orasvr.corp.admin.sales ; in addition  sql server supports built-in  parameterized table-valued functions called openrowset and openquery  which allow sending uninterpreted queries to a provider or linked server  respectively  in the dialect supported by the provider the following query combines information stored in an oracle server and a microsoft index server it lists all documents and their author containing the words data and access ordered by the author ? s department and name thesumit67.blogspot.com 30.9 replication 1251 select e.dept  f.docauthor  f.filename from orasvr.corp.admin.employee e  openquery  empfiles  ? select docauthor  filename from scope  ? c  \ empdocs ?  where contains  ? ? ? data ? near   ? access ? ? ?  > 0 ?  as f where e.name = f.docauthor order by e.dept  f.docauthor ; the relational engine uses the ole-db interfaces to open the rowsets on linked servers  to fetch the rows  and to manage transactions for each ole-db data source accessed as a linked server  an ole-db provider must be present on the server running sql server the set of transact-sql operations that can be used against a specific ole-db data source depends on the capabilities of the ole-db provider whenever it is cost-effective  sql server pushes relational operations such as joins  restrictions  projections  sorts  and group by operations to the ole-db data source sql server uses microsoft distributed transaction coordinator and the ole-db transaction interfaces of the provider to ensure atomicity of transactions spanning multiple data sources 30.9 replication sql server replication is a set of technologies for copying and distributing data and database objects from one database to another  tracking changes  and synchronizing between databases tomaintain consistency sql server replication also provides inline replication of most database schema changes without requiring any interruptions or reconfiguration data are typically replicated to increase availability of data replication can roll up corporate data fromgeographically dispersed sites for reporting purposes and disseminate data to remote users on a local-area network or mobile users on dial-up connections or the internet microsoft sql server replication also enhances application performance by scaling out for improved total read performance among replicas  as is common in providing midtier data-caching services forweb sites 30.9.1 replication model sql server introduced the publish ? subscribe metaphor to database replication and extends this publishing-industry metaphor throughout its replication administration and monitoring tools the publisher is a server that makes data available for replication to other servers the publisher can have one or more publications  each representing a logically related set of data and database objects the discrete objects within a publication  including tables  stored procedures  user-defined functions  views  thesumit67.blogspot.com 1252 chapter 30 microsoft sql server materialized views  and more  are called articles the addition of an article to a publication allows for extensive customizing of the way the object is replicated  e.g  restrictions on which users can subscribe to receive its data and how the data set should be filtered on the basis of a projection or selection of a table  by a ? horizontal ? or a ? vertical ? filter  respectively subscribers are servers that receive replicated data from a publisher subscribers can conveniently subscribe to only the publications they require from one or more publishers regardless of the number or type of replication options each implements depending on the type of replication options selected  the subscriber either can be used as a read-only replica or can make data changes that are automatically propagated back to the publisher and subsequently to all other replicas subscribers can also republish the data they subscribe to  supporting as flexible a replication topology as the enterprise requires the distributor is a server that plays different roles  depending on the replication options chosen at a minimum it is used as a repository for history and error state information in other cases  it is used additionally as an intermediate store-and-forward queue to scale up the delivery of the replicated payload to all the subscribers 30.9.2 replication options microsoft sql server replication offers a wide spectrum of replication options to decide on the appropriate replication options to use  a database designer must determine the application ? s needs with respect to autonomous operation of the sites involved and the degree of transactional consistency required snapshot replication copies and distributes data and database objects exactly as they appear at a moment in time snapshot replication does not require continuous change tracking because changes are not propagated incrementally to subscribers subscribers are updated with a complete refresh of the data set defined by the publication on a periodic basis options available with snapshot replication can filter published data and can enable subscribers to modify replicated data and propagate those changes back to the publisher this type of replication is best suited for smaller sizes of data and when updates typically affect enough of the data that replicating a complete refresh of the data is efficient with transactional replication  the publisher propagates an initial snapshot of data to subscribers  then forwards incremental data modifications to subscribers as discrete transactions and commands incremental change tracking occurs inside the core engine of sql server  which marks transactions affecting replicated objects in the publishing database ? s transaction log a replication process called the log reader agent reads these transactions from the database transaction log  applies an optional filter  and stores them in the distribution database  which acts as the reliable queue supporting the store-and-forward mechanism of transactional replication  reliable queues are the same as durable queues  described in section 26.1.1  another replication process  called the distribution agent  then forwards the changes to each subscriber like snapshot replication  transactional replication offers subscribers the option to make updates that either use two-phase thesumit67.blogspot.com 30.10 server programming in .net 1253 commit to reflect those changes consistently at the publisher and subscriber or queue the changes at the subscriber for asynchronous retrieval by a replication process that later propagates the change to the publisher this type of replication is suitable when intermediate states between multiple updates need to be preserved merge replication allows each replica in the enterprise to work with total autonomy whether online or offline the system tracks metadata on the changes to published objects at publishers and subscribers in each replicated database  and the replication agent merges those data modifications together during synchronization between replicated pairs and ensures data convergence through automatic conflict detection and resolution numerous conflict resolution policy options are built into the replication agent used in the synchronization process  and custom conflict resolution can be written by using stored procedures or by using an extensible component object model  com  interface this type of replication does not replicate all intermediate states but only the current state of the data at the time of synchronization it is suitable when replicas require the ability to make autonomous updates while not connected to any network 30.10 server programming in .net sql server supports the hosting of the .net common language runtime  clr  inside the sql server process to enable database programmers to write business logic as functions  stored procedures  triggers  data types  and aggregates the ability to run application code inside the database adds flexibility to the design of application architectures that require business logic to execute close to the data and can not afford the cost of shipping data to a middle-tier process to perform computation outside the database the .net common language runtime  clr  is a runtime environment with a strongly typed intermediate language that executes multiple modern programming languages such asc # ,visual basic  c + +  cobol  and j + +  among others  and has garbage-collected memory  preemptive threading,metadata services  type reflection   code verifiability  and code access security the runtime uses metadata to locate and load classes  lay out instances in memory  resolve method invocations  generate native code  enforce security  and set runtime context boundaries application code is deployed inside the database by using assemblies  which are the units of packaging  deployment  and versioning of application code in .net deployment of application code inside the database provides a uniform way to administer  back up  and restore complete database applications  code and data   once an assembly is registered inside the database  users can expose entry points within the assembly via sql ddl statements  which can act as scalar or table functions  procedures  triggers  types  and aggregates  by using well-defined extensibility contracts enforced during the execution of these ddl statements stored procedures  triggers  and functions usually need to execute sql queries and updates this is achieved through a component that implements the ado.net data-access api for use inside the database process thesumit67.blogspot.com 1254 chapter 30 microsoft sql server 30.10.1 basic .net concepts in the .net framework  a programmer writes program code in a high-level programming language that implements a class defining its structure  e.g  the fields or properties of the class  and methods some of these methods can be static functions the compilation of the program produces a file  called an assembly  containing the compiled code in the microsoft intermediate language  msil   and a manifest containing all references to dependent assemblies the manifest is an integral part of every assembly that renders the assembly self-describing the assembly manifest contains the assembly ? s metadata  which describes all structures  fields  properties  classes  inheritance relationships  functions  and methods defined in the program the manifest establishes the assembly identity  specifies the files that make up the assembly implementation  specifies the types and resources that make up the assembly  itemizes the compile-time dependencies on other assemblies  and specifies the set of permissions required for the assembly to run properly this information is used at runtime to resolve references  enforce version-binding policy  and validate the integrity of loaded assemblies the .net framework supports an out-of-band mechanism called custom attributes for annotating classes  properties  functions and methods with additional information or facets the application may want to capture in metadata all .net compilers consume these annotations without interpretation and store them in the assembly ? s metadata all these annotations can be examined in the same way as any other metadata by using a common set of reflection apis managed code refers to msil executed in the clr rather than directly by the operating system managed-code applications gain common-language runtime services such as automatic garbage collection  runtime type checking  and security support these services help provide uniform platform and language-independent behavior of managed-code applications at execution time  a just-in-time  jit  compiler translates the msil into native code  e.g  intel x86 code   during this translation  code must pass a verification process that examines the msil and metadata to find out whether the code can be determined to be type safe 30.10.2 sql clr hosting sql server and the clr are two different runtimes with different internal models for threading  scheduling and memory management sql server supports a cooperative non-preemptive threading model in which the dbms threads voluntarily yield execution periodically or when they are waiting on locks or i/o  whereas the clr supports a preemptive threading model if user code running inside the dbms can directly call the operating-system  os  threading primitives  then it does not integrate well with the sql server task scheduler and can degrade the scalability of the system clr does not distinguish between virtual and physical memory  while sql server directly manages physical memory and is required to use physical memory within a configurable limit the different models for threading  scheduling  and memory management present an integration challenge for a dbms that scales to support thousands of concurrent user sessions sql server solves this challenge by becoming the thesumit67.blogspot.com 30.10 server programming in .net 1255 sql server process sql server engine clr windows sqlclr hosting sql os layer  memory  threads  synchronization  figure 30.5 integration of clr with sql server operating-system services operating system for the clr when it is hosted inside the sql server process the clr calls low-level primitives implemented by sql server for threading  scheduling  synchronization  and memory management  see figure 30.5   this approach provides the following scalability and reliability benefits  common threading  scheduling  and synchronization clr calls sql server apis for creating threads both for running user code and for its own internal use such as the garbage collector and the class finalizer thread in order to synchronize between multiple threads  the clr calls sql server synchronization objects this allows sql server scheduler to schedule other taskswhen a thread is waiting on a synchronization object for instance  when the clr initiates garbage collection  all of its threads wait for garbage collection to finish since the clr threads and the synchronization objects they arewaiting on are knownto the sqlserver scheduler  it can schedule threads that are running other database tasks not involving the clr further  this enables sql server to detect deadlocks that involve locks taken by clr synchronization objects and employ traditional techniques for deadlock removal the sql server scheduler has the ability to detect and stop threads that have not yielded for a significant amount of time the ability to hook clr threads to sql server threads implies that the sql server scheduler can identify runaway threads running in the clr andmanage their priority  so that they do not consume significant cpu resources  thereby affecting the throughput of the system such runaway threads are suspended and put back in the queue repeat offenders are not allowed timeslices that are unfair to other executing workers if an offender took 50 times the allowed quantum  it is punished for 50 ? rounds ? before being allowed to run again because the scheduler can not tell when a computation is long and runaway versus long and legitimate common memory management the clr calls sql server primitives for allocating and deallocating its memory since the memory used by the clr is accounted for in the total memory usage of the system  sql server can staywithin its configured memory limits and ensure the clr and sql server are not competing with each other for memory also  sql server can reject clr memory requests thesumit67.blogspot.com 1256 chapter 30 microsoft sql server when the system is constrained and ask clr to reduce itsmemory use when other tasks need memory 30.10.3 extensibility contracts all user-managed code running within the sql server process interactswith dbms components as an extension current extensions include scalar functions  table functions  procedures  triggers  scalar types  and scalar aggregates for each extension there is a mutual contract defining the properties or services user code must implement to act as one of these extensions as well as the services the extension can expect from the dbms when the managed code is called sql clr leverages the class and custom attributes information stored in assembly metadata to enforce that user code implements these extensibility contracts all user assemblies are stored inside the database all relational and assembly metadata are processed inside the sql engine through a uniform set of interfaces and data structures when data-definition language  ddl  statements registering a particular extension function  type  or aggregate are processed  the system ensures the user code implements the appropriate contract by analyzing its assembly metadata if the contract is implemented  then the ddl statement succeeds  otherwise it fails the next subsections describe key aspects of the specific contracts currently enforced by sql server 30.10.3.1 routines we classify scalar functions  procedures  and triggers generically as routines routines  implemented as static class methods  can specify the following properties through custom attributes ? isprecise if this boolean property is false  then it indicates the routine body involves imprecise computations such as floating-point operations expressions involving imprecise functions can not be indexed ? userdataaccess if the value of this property is read  then the routine reads user-data tables otherwise  the value of the property is none indicating the routine does not access data queries that do not access any user tables  directly or indirectly through views and functions  are not considered to have user-data access ? systemdataaccess if the value of this property is read  then the routine reads system catalogs or virtual system tables ? isdeterministic if this property is true  then the routine is assumed to produce the same output value given the same input values  state of the local database  and execution context ? issystemverified this indicates whether the determinism and precision properties can be ascertained or enforced by sqlserver  e.g  built-ins,transact sql functions  or it is as specified by the user  e.g  clr functions   thesumit67.blogspot.com 30.10 server programming in .net 1257 ? hasexternalaccess if the value of this property is true  then the routine accesses resources outside sql server such as files  network,web access  and registry 30.10.3.2 table functions a class implementing a table-valued function must implement an interface ienumerable to enable iteration over the rows returned by the function  a method to describe the schema of the table returned  i.e  columns  types   a method to describe what columns can be unique keys  and a method to insert rows into the table 30.10.3.3 types classes implementing user-defined types are annotated with an sqluserdefined type   attribute that specifies the following properties  ? format sql server supports three storage formats  native  user-defined  and .net serialization ? maxbytesize this is the maximum size of the serialized binary representation of type instances in bytes udts can be up to 2 gb in length ? isfixedlength this is a boolean property specifying whether the instances of the type have fixed or variable length ? isbyteordered this is a boolean property indicating whether the serialized binary representation of the type instances is binary ordered when this property is true  the system can perform comparisons directly against this representation without the need to instantiate type instances as objects ? nullability all udts in our system must be capable of holding the null value by supporting the inullable interface containing the boolean isnull method ? type conversions all udts must implement conversions to and from character strings via the tostring and parse methods 30.10.3.4 aggregates in addition to supporting the contract for types  user-defined aggregates must implement four methods required by the query-execution engine to initialize the computation of an aggregate instance  to accumulate input values into the function provided by the aggregate  to merge partial computations of the aggregate  and to retrieve the final aggregate result aggregates can declare additional properties  via custom attributes  in their class definition ; these properties are used by the query optimizer to derive alternative plans for the aggregate computation ? isinvarianttoduplicates if this property is true  then the computation delivering the data to the aggregate can be modified by either discarding or introducing new duplication-removal operations thesumit67.blogspot.com 1258 chapter 30 microsoft sql server ? isinvarianttonulls if this property is true  then null rows can be discarded from the input however  care must be taken in the context of group by operations not to discard entire groups ? isinvarianttoorder if this property is true  then the query processor can ignore order by clauses and explore plans that avoid having to sort the data 30.11 xml support relational database systems have embraced xml in many differentways in recent years first-generation xml support in relational database systems was mainly concerned with exporting relational data as xml  ? publish xml ?   and to import relational data in xml markup form back into a relational representation  ? shred xml ?   the main usage scenario supported by these systems is information exchange in contexts where xml is used as the ? wire format ? and where the relational and xml schemas are often predefined independently of each other in order to cover this scenario  microsoft sql server provides extensive functionality such as the for xml publishing rowset aggregator  the openxml rowset provider  and the xml view technology based on annotated schemas shredding of xml data into a relational schema can be quite difficult or inefficient for storing semistructured data whose structure may vary over time  and for storing documents to support such applications sql server implements native xml based on the sql  2003 xml data type figure 30.6 provides a high xml xml parser validation and typing xml datatype  binary xml  xml schema collection openxml/nodes   for xml with type directive relational rowsets modify   query   xml schemas xquery and update execution sql value value   primary xml index node table path index prop index value index sql server metadata sql server indices figure 30.6 architectural overview of the native xml support in sql server thesumit67.blogspot.com 30.11 xml support 1259 level architectural diagram of sql server ? s native xml support in the database it consists of the ability to store xml natively  to constrain and type the stored xml data with collections of xml schemas  and to query and update the xml data in order to provide efficient query executions  several types of xml-specific indices are provided finally  the native xml support also integrates with the ? shredding ? and ? publishing ? to and from relational data 30.11.1 natively storing and organizing xml the xml data type can store xml documents and content fragments  multiple text or element nodes at the top  and is defined on the basis of the xquery 1.0/xpath 2.0 data model the data type can be used for parameters of stored procedures  for variables  and as a column type sql server stores data of type xml in an internal binary format as a blob and provides indexing mechanisms for executing queries the internal binary format provides efficient retrieval and reconstruction of the original xml document  in addition to some space savings  on average  20 percent   the indices support an efficient query mechanism that can utilize the relational query engine and optimizer ; more details are provided later  in section 30.11.3 sql server provides a database-metadata concept called an xml schema collection that associates an sql identifier with a collection of schema components of one or multiple target namespaces 30.11.2 querying and updating the xml data type sql server provides several xquery-based query and modification capabilities on the xml data type these query and modification capabilities are supported by usingmethods defined on the xml data type some of these methods are described in the rest of this section each method takes a string literal as the query string and potentially other arguments the xml data type  on which the method is applied  provides the context item for the path expressions and populates the in-scope schema definitions with all the type information provided by the associated xml schema collection  if no collection is provided  the xml data is assumed to be untyped   the sql server xquery implementation is statically typed  thereby supporting early detection of path expression typing mistakes  type errors  and cardinality mismatch  as well as some additional optimizations the query method takes an xquery expression and returns an untyped xml data type instance  that can then be cast to a target schema collection if the data need to be typed   in xquery specification terminology  we have set the construction mode to ? strip ? the following example shows a simple xquery expression that summarizes a complex customer element in a trip report document that contains among other information a name  anidattribute  and sales-lead information that are contained in the marked-up actual trip report notes the summary shows the name and sales leads for customer elements that have sales leads thesumit67.blogspot.com 1260 chapter 30 microsoft sql server select report.query  ? declare namespace c = " urn  example/customer " ; for $ cust in /c  doc/c  customer where $ cust/c  notes//c  saleslead return < customer id = " $ cust/ @ id " >  $ cust/c  name  $ cust/c  notes//c  saleslead  < /customer > ?  from tripreports ; the above xquery query gets executed on the xml value stored in the doc attribute of each rowof the table tripreports eachrowinthe result of the sql query contains the result of executing the xquery query on the data in one input row the value method takes an xquery expression and an sql type name  extracts a single atomic value from the result of the xquery expression  and casts its lexical form into the specified sql type if the xquery expression results in a node  the typed value of the node will implicitly be extracted as the atomic value to be cast into the sql type  in xquery terminology the node will be ? atomized ? ; the result is cast to sql   note that the value method performs a static type check that at most one value is being returned the exist method takes an xquery expression and returns 1 if the expression produces a nonempty result and 0 otherwise finally  the modify method provides a mechanism to change an xml value at the subtree level  inserting new subtrees at specific locations inside a tree  changing the value of an element or attribute  and deleting subtrees the following example deletes all customer saleslead elements of years previous to the year given by an sql variable or parameter with the name @ year  update tripreports set report.modify  ? declare namespace c = " urn  example/customer " ; delete /c  doc/c  customer//c  saleslead  @ year < sql  variable  " @ year "   ?  ; 30.11.3 execution of xquery expressions as mentioned earlier  the xml data are stored in an internal binary representation however  in order to execute the xquery expressions  the xml data type is internally transformed into a so-called node table the internal node table basically uses a row to represent a node each node receives an ordpath identifier as its nodeid  an ordpath identifier is a modified dewey decimal numbering scheme ; see the bibliographical notes for references to more information on ordpath   each node also contains key information to point back to the original sql row to which the node belongs  information about the name and type  in a tokenized form   values  and more since theordpath encodes both the document order and the hierarchy information  the node table then is clustered on the basis of the key thesumit67.blogspot.com 30.12 sql server service broker 1261 information and ordpath  so that a path expression or recomposition of a subtree can be achieved with a simple table scan all xquery and update expressions are then translated into an algebraic operator tree against this internal node table ; the tree uses the common relational operators and some operators specifically designed for the xquery algebraization the resulting tree is then grafted into the algebra tree of the relational expression so that in the end  the query-execution engine receives a single execution tree that it can optimize and execute in order to avoid costly runtime transformations  a user can prematerialize the node table by using the primary xml index sql server in addition provides three secondary xml indices so that the query execution can take further advantage of index structures  ? the path index provides support for simple types of path expressions ? the properties index provides support for the common scenario of propertyvalue comparisons ? the value index is well suited if the query uses wild-cards in comparisons see the bibliographical notes for references to more information on xml indexing and query processing in sql server 30.12 sql server service broker service broker helps developers create loosely coupled distributed applications by providing support for queued  reliable messaging in sqlserver.manydatabase applications use asynchronous processing to improve scalability and response times for interactive sessions one commonapproach to asynchronous processing is to use work tables instead of performing all of the work for a business process in a single database transaction  an application makes a change indicating that outstandingwork is present and then inserts a record of the work to be performed into a work table as resources permit  the application processes the work table and completes the business process service broker is a part of the database server that directly supports this approach for application development the transact sql language includes ddl and dml statements for service broker in addition  sql server management objects  smo  for service broker are provided in sql server these allow programmatic access to service broker objects from managed code previous message-queuing technologies concentrated on individual messages with service broker  the basic unit of communication is the conversation ? a persistent  reliable  full-duplex stream of messages sql server guarantees that the messages within a conversation are delivered to an application exactly once  in order it is also possible to assign a priority from1 to 10 to a conversation messages from conversations with higher priority are sent and received faster than messages fromconversationswith a lower priority conversations occur between two services a service is a named endpoint for a conversation each conversation thesumit67.blogspot.com 1262 chapter 30 microsoft sql server is part of a conversation group related conversations can be associated with the same conversation group messages are strongly typed  i.e  eachmessage has a specific type sql server can optionally validate that messages are well-formed xml  that messages are empty  or that messages conform to an xml schema a contract defines the message types that are allowable for a conversation  and which participant in the conversation may send messages of that type sql server provides a default contract and message type for applications that only need a reliable stream sql server stores messages in internal tables these tables are not directly accessible ; instead  sql server exposes queues as views of those internal tables applications receive messages from a queue a receive operation returns one or more messages fromthe same conversation group by controlling access to the underlying table  sql server can efficiently enforce message ordering  correlation of related messages  and locking because queues are internal tables  queues require no special treatment for backup  restore  failover  or database mirroring both application tables and the associated  queued messages are backed up  restored  and failed-over with the database broker conversations that exist in mirrored databases continue where they left off when the mirrored failover is complete ? even if the conversationwas between two services that live in separate databases the locking granularity for service broker operations is the conversation group rather than a specific conversation or individual messages by enforcing locking on the conversation group  service broker automatically helps applications avoid concurrency issues while processing messages when a queue contains multiple conversations  sql server guarantees that only one queue reader at a time can process messages that belong to a given conversation group this eliminates the need for the application itself to include deadlock-avoidance logic ? a common source of errors in many messaging applications another nice side effect of this locking semantic is that applications may choose to use the conversation group as a key for storing and retrieving application state these programming-model benefits are just two examples of the advantages that derive from the decision to formalize the conversation as the communication primitive versus the atomic message primitive found in traditional message-queuing systems sql server can automatically activate stored procedures when a queue contains messages to be processed to scale the number of running stored procedures to the incoming traffic  the activation logic monitors the queue to see if there is usefulwork for another queue reader sql server considers both the rate at which existing readers receive messages and the number of conversation groups available to decide when to start another queue reader the stored procedure to be activated  the security context of the stored procedure  and themaximumnumber of instances to be started are configured for an individual queue sql server also provides an external activator this feature allows an application outside of sql server to be activated when new messages are inserted into a queue the application can then receive and process the messages by doing this  cpu-intensivework can be offloaded out of sql server to an application  possibly in a different computer also  long-duration tasks  e.g  invoking a web service  can be executed thesumit67.blogspot.com 30.13 business intelligence 1263 without tying up database resources the external activator follows the same logic as internal activation  and can be configured to activate multiple instances of an application when messages accumulate in a queue as a logical extension to asynchronous messaging within the instance  service broker also provides reliablemessaging between sql server instances to allow developers to easily build distributed applications conversations can occur within a single instance of sql server or between two instances of sql server local and remote conversations use the same programming model security and routing are configured declaratively  without requiring changes to the queue readers sql server uses routes to map a service name to the network address of the other participant in the conversation sql server can also perform message forwarding and simple load balancing for conversations sql server provides reliable  exactly once in-order delivery regardless of the number of instances that a message travels through a conversation that spans instances of sql server can be secured both at the networking level  point to point  and at the conversation level  end to end   when end-to-end security is used  the contents of the message remain encrypted until the message reaches the final destination  while the headers are available to each sqlserver instance that themessage travels through standard sql server permissions apply within an instance encryption occurs when messages leave an instance sql server uses a binary protocol for sending messages between instances the protocol fragments large messages and permits interleaved fragments from multiple messages fragmentation allows sql server to quickly transmit smaller messages even in cases where a large message is in the process of being transmitted the binary protocol does not use distributed transactions or two-phase commit instead  the protocol requires that a recipient acknowledge message fragments sql server simply retries message fragments periodically until the fragment is acknowledged by the recipient acknowledgments are most often included as part of the headers of a return message  although dedicated return messages are used if no return message is available sql server includes a command line diagnostics tool  ssbdiagnose  to help analyze a service broker deployment and investigate problems the tool can run in either configuration or runtime mode in configuration mode  the tool checks whether a pair of services can exchange messages and returns any configuration errors examples of these errors are disabled queues and missing return routes in the second mode  the tool connects to two or more sql server instances and monitors sql profiler events to discover service broker problems at runtime the tool output can be sent into a file for automated processing 30.13 business intelligence the business intelligence component of sqlserver contains three subcomponents  ? sql server integration services  ssis   which provides the means to integrate data from multiple sources  performs transformations related to cleaning the data and bringing it to a common form  and loading the data into a database system thesumit67.blogspot.com 1264 chapter 30 microsoft sql server ? sql server analysis services  ssas   which provides olap and data-mining capabilities ? sql server reporting services  ssrs   integration services  analysis services  and reporting services are each implemented in separate servers and can be installed independently from one another on the same or different machines they can connect to a variety of data sources  such as flat files  spreadsheets  or a variety of relational database systems  through native connectors  ole-db  or odbc drivers together they provide an end-to-end solution for extracting  transforming  and loading data  then modeling and adding analytical capability to the data  and finally building and distributing reports on the data the different business intelligence components of sql server can integrate and leverage each others ? capability here are a few common scenarios that will leverage a combination of components  ? build an ssis package that cleanses data  using patterns generated by ssas data mining ? use ssis to load data to an ssas cube  process it  and execute reports against the ssas cube ? build an ssrs report to publish the findings of a mining model or the data contained in an ssas olap component the following sections give an overview of the capabilities and architecture of each of these server components 30.13.1 sql server integration services microsoft sql server integration services  ssis  is an enterprise data transformation and data integration solution that you can use to extract  transform  aggregate  and consolidate data from disparate sources and move it to single or multiple destinations you can use ssis to perform the following tasks  ? merge data from heterogeneous data stores ? refresh data in data warehouses and data marts ? cleanse data before loading it into destinations ? bulk-load data into online transaction processing  oltp  and online analytical processing  olap  databases ? send notifications ? build business intelligence into a data transformation process ? automate administrative functions thesumit67.blogspot.com 30.13 business intelligence 1265 ssis provides a complete set of services  graphical tools  programmable objects  and apis for the above tasks these provide the ability to build large  robust  and complex data transformation solutions without any custom programming however  an api and programmable objects are availablewhen they are needed to create custom elements or integrate data transformation capabilities into custom applications the ssis data-flow engine provides the in-memory buffers that move data from source to destination and calls the source adapters that extract data from files and relational databases the engine also provides the transformations that modify data and the destination adapters that load data into data stores duplicate elimination based on fuzzy  approximate  match is an example of a transformation provided by ssis users can program their own transformations if required figure 30.7 shows an example of how various transformations can be combined to cleanse and load book sales information ; the book titles from the sales data figure 30.7 loading of data by using fuzzy lookup thesumit67.blogspot.com 1266 chapter 30 microsoft sql server are matched against a publications database  and in case there is no match  fuzzy lookup is performed to handle titles with minor errors  such as spelling errors   information about confidence and data lineage is stored with the cleansed data 30.13.2 sql server analysis services the analysis services component delivers online analytical processing  olap  and data-mining functionality for business intelligence applications analysis services supports a thin client architecture the calculation engine is on the server  so queries are resolved on the server  avoiding the need to transfer large amounts of data between the client and the server 30.13.2.1 sql server analysis services  olap analysis services utilizes a unified dimensional model  udm   which bridges the gap between traditional relational reporting and olap ad hoc analysis the role of a unified dimensional model  udm  is to provide a bridge between the user and the data sources a udm is constructed over one or more physical data sources  and then the end user issues queries against the udm  using one of a variety of client tools  such as microsoft excel more than simply a dimension modeling layer of the datasource schemas  the udm provides a rich environment for defining powerful yet exhaustive business logic  rules  and semantic definition users can browse and generate reports on the udm data in their native language  for example  french or hindi  by defining local language translation of the metadata catalog as well as the dimensional data analysis server defines complex time dimensions  fiscal  reporting  manufacturing  etc   and enables the definition of powerful multidimensional business logic  year-to-year growth  year-to-date  using the multidimensional expression  mdx  language the udm allows users to define business-oriented perspectives  each one presenting only a specific subset of the model  measures  dimensions  attributes  business rules  and so forth  that is relevant to a particular group of users businesses often define key performance indicators  kpis  that are important metrics used to measure the health of the business examples of such kpis include sales  revenue per employee  and customer retention rate the udm allows such kpis to be defined  enabling a much more understandable grouping and presentation of data 30.13.2.2 sql server analysis services  data mining sql server provides a variety of mining techniques,with a rich graphical interface to view mining results mining algorithms supported include  ? association rules  useful for cross-sales applications   ? classification and prediction techniques such as decision trees  regression trees  neural networks  and naive bayes ? time series forecasting techniques including arima and artxp thesumit67.blogspot.com bibliographical notes 1267 ? clustering techniques such as expectation maximization and k-means  coupled with techniques for sequence clustering   in addition  sql server provides an extensible architecture for plugging in third-party data mining algorithms and visualizers sql server also supports the data-mining extensions  dmx  extensions for sql dmx is the language used to interact with data-mining models just as sql is used to interact with tables and views with dmx  models can be created and trained and then stored in an analysis services database the model can then be browsed to look at patterns or  by using a special prediction join syntax  applied against new data to perform predictions the dmx language supports functions and constructs to easily determine a predicted class along with its confidence  predict a list of associated items as in a recommendation engine  or even return information and supporting facts about a prediction data mining in sql server can be used against data stored in relational or multidimensional data sources other data sources are supported as well through specialized tasks and transforms  allowing data mining directly in the operational data pipeline of integration services data-mining results can be exposed in graphical controls  special data-mining dimensions for olap cubes  or simply in reporting services reports 30.13.3 sql server reporting services reporting services is a server-based reporting platform that can be used to create and manage tabular  matrix  graphical  and free-form reports that contain data from relational and multidimensional data sources the reports that you create can be viewed and managed over a web-based connection matrix reports can summarize data for high-level reviews  while providing supporting detail in drilldown reports parameterized reports can be used to filter data on the basis of values that are provided at runtime users can choose from a variety of viewing formats to render reports on the fly in preferred formats for data manipulation or printing an api is also available to extend or integrate report capabilities into custom solutions server-based reporting provides a way to centralize report storage and management  set policies and secure access to reports and folders  control how reports are processed and distributed  and standardize how reports are used in your business bibliographical notes detailed information about using ac2 certified system with sqlserver is available at www.microsoft.com/downloads/release.asp ? releaseid = 25503 sql server ? s optimization framework is based on the cascades optimizer prototype  which graefe  1995  proposed simmen et al  1996  discusses the scheme for reducing grouping columns galindo-legaria and joshi  2001  and elhemali et al  2007  present the variety of execution strategies that sql server thesumit67.blogspot.com 1268 chapter 30 microsoft sql server considers for cost-based optimization of subqueries additional information on the self-tuning aspects of sql server are discussed by chaudhuri et al  1999   chaudhuri and shim  1994  and yan and larson  1995  discuss reordering of aggregation operations chatziantoniou and ross  1997  and galindo-legaria and joshi  2001  proposed the alternative used by sql server for sql queries requiring a self-join under this scheme  the optimizer detects the pattern and considers per-segment execution pellenkoft et al  1997  discusses the optimization scheme for generating the complete search space using a set of transformations that are complete  local and nonredundant graefe et al  1998  offers discussion concerning hash operations that support basic aggregation and join  with a number of optimizations  extensions  and dynamic tuning for data skew graefe et al  1998  presents the idea of joining indices for the sole purpose of assembling a row with the set of columns needed on a query it argues that this sometimes is faster than scanning a base table 