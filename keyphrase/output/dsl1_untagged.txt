welcome to data structures and algorithms we are going to learn about some basic terminologies regarding data structures and the notations that you would be following in the rest of this course we will begin with some simple definitions an algorithm is an outline of the steps that a program or any computational procedure has to take a program on the other hand is an implementation of an algorithm and it could be in any programming language data structure is the way we need to organize the data  so that it can be used effectively by the program hope you are all familiar with certain data structures  an array or a list in this course you will be seeing a lot of data structures and you will see how to use them in various algorithms we will take a particular problem  try to solve it and in the process develop data structures the best way of organizing the data  associated with that problem what is an algorithmic problem an algorithmic problem is essentially  that you have a certain specifications of an input and specify what the output should be like here is one specification a sorted  non decreasing sequence of natural numbers of non-zero  finite length for example  * 1,20,908,909,100000,1000000000 * 3 this is a completely specified input above are the two examples of input  which meets the specification and i have not given any output specification what is an instance a sorted  non-decreasing sequence of natural numbers of non-zero  finite length forms an instance those two examples are the instances of the input you can have any possible number of instances that may take sequence of sorted  nondecreasing numbers as input an algorithm is essentially  describing the actions that one should take on the input instance to get the specified output also there can be infinitely many input instances and algorithms for solving certain problem each one of you could do it in a different way that brings the notion of good algorithm there are so many different algorithms for solving a certain problem what is a good algorithm good algorithm is an efficient algorithm what is efficient efficient is something  which has small running time and takes less memory these will be the two measures of efficiency we will be working with there could also be other measures of efficiency but these are the only two things we will be considering in this course we would be spending more time on analyzing the running time of an algorithm and we will also spend some time on analyzing the space we would be interested in the efficiency of algorithms  as a function of input size clearly you can imagine that  if i have a small input and my algorithm or a program running on that input will take less amount of time if the input becomes 10 times larger  then the time taken by the program may also increase it may become 10  20 or 100 times it is this behavior of increase in the running time  with the increase in the size of input would be of our interest let us see the slide how does one measure the running time of an algorithm let us look at the experimental study you have a certain algorithm and you have to implement the algorithm  which means you have to write a program in a certain programming language you run the program with varying data sets in which some are smaller  some are of larger data sets  some would be of some kinds and some would be of different kinds of varying composition then you clock the time the program takes and clock does not mean that you should sit down near stopwatch perhaps you can use the system utility like system current time millis    to clock the time that program takes and then from that you try to figure out  how good your algorithms is that is what one would call as the experimental study of the algorithm this has certain limitations  let us see them in detail first you have to implement the algorithm in which we will be able to determine how good your algorithm is implementing it is a huge overhead  where you have to spend considerable amount of time experiments can be done only on a limited set of inputs you can run your experiment on a small set of instances and that might not really indicate the time that your algorithm is taking for other inputs  which you have not considered in your experiment if you have two algorithms and you have to decide  which one is better you have to use exactly the same platforms to do the comparison platform means both the hardware and software environment because as you can imagine  different machines would make a difference  in fact even the users who are working on that system at that particular point would make a difference on the running time of an algorithm it becomes very messy  if you have to do it this way hence same hardware and software environments should be used what we are going to do in the part of this course in this very first lecture  we have to develop the general methodology  which will help us to analyze running time of algorithms we are going to do it as follows  first we are going to develop a high level description of an algorithm the way of describing an algorithm and we are going to use this description to figure out the running time and not to implement it to any system a methodology would help us to take into account of all possible input instances and also it will allow us to evaluate the efficiency of the algorithm in a way that it is independent of the platform we are using pseudo-code is the high level description of an algorithm and this is how we would be specifying all our algorithms for the purpose of this course here is an example of pseudo code and you might have seen this in earlier courses also what is this algorithm doing this algorithm takes an array a  which stores an integer in it and it is trying to find the maximum element in this array algorithm array max  a  n  the above mentioned example is not a program  because the syntax is wrong but it is a pseudo code which is a mixture of natural language and some high-level programming concepts i am going to use a for loop  do loop  if-then-else statement and a while loop but i will not bother about whether there should be a semicolon or a colon  because they are required for the compiler but for our understanding  what the program is doing is clear in the beginning it keeps track of the maximum variable in a variable called current max which is initialized to the first element of the array current max a  0  then it is going to run through the remaining element of the array  compare them with the current maximum element if the current maximum element is less than the current element  then it would update the current max a  i  becomes the new max and then when the loop terminates we would just return current max if current max < a  i  then current max a  i  return current max it is a very simple algorithm but just with this pseudo-code  you are able to understand what it is doing this will not run on any computer since it is the pseudo-code  but it conveys the idea or the concepts thus pseudo-code is more structured than usual prose  but it is less formal than a programming language how pseudo-code will look like we will use standard numeric and boolean expressions in it instead of the assignment operator which is = in java  i will use and instead of the equality operator  an equality relationship in java which is = = the same in c  i will just use =  i will declare methods with the algorithmic name and the parameter it takes algorithm name  param 1  param2  i will use all kinds of programming construct like if then statement  if then  else  statement  while do  repeat until  for do and to index array i will say a  i   a  i  j   it should be clear in what it is doing i will use return when the procedure terminates and return value will tell about the value returned by the particular procedure or a function returns  return value when i have to call a method  i will specify that with the name of the method and the argument and the object used calls  object method  args  object specifies the type of the value returned by the particular method you will see more of this  when we come across more pseudo-code how do we analyze algorithms first we identify what are the primitive operations in our pseudo-code what is a primitive operation it is a low level operation example is a data movement in which i do an assignment from one to another  i do a control statement which is a branch  if then else  subroutine call or return i do arithmetic operations or logical operations and these are called as a primitive operation * data movement  assign  * control  branch  subroutine call  return  * arithmetic an logical operations  e.g addition  comparison  in my pseudo code  i just inspect the pseudo code and count the number of primitive operations that are executed by an algorithm let us see an example of sorting the input is some sequence of numbers and output is a permutation of the sequence which is in non decreasing order what are the requirements for the output it should be in nondecreasing order and it should be the permutation of the input any set of numbers which are in non-decreasing order does not make an output algorithm should sort the numbers that were given to it and not just produce the sequence of numbers as an increasing order clearly the running time depends upon  number of elements  n  and often it depends upon how sorted these numbers are if they are already in sorted order then the algorithm will not take a long time it also depends upon the particular algorithm we use the running time would depend upon all these things the first sorting technique we use is the one that you have used very often let us say when you are playing game of cards what is the strategy you follow  when you are picking up a set of cards that have been dealt out to you you like to keep them in a sorted order in your hand you start with the empty hand and you pick up the first card  then you take the next card and insert it at the appropriate place suppose if i have some five cards in your hand already  let us say 2  7  9  jack and queen then i am getting 8  so i am going to put it between 7 and 9 that is the right place it has to be placed in i am inserting it at the appropriate place and that is why this technique is called insertion sort i keep on doing this  till i have picked up all the cards and inserted in the appropriate place let us see the pseudo-code for insertion sort i will give an array of integers as input and output is a permutation of the original numbers  such that it is sorted the output is also going to be in the same array a  1  a  2  _ a  n  this is the input  output specification i am going to have 2 variables or indices i and j the array is going to be sorted from a  1  through a  j-1   the element should be inserted at the th j location  which is the right place to insert clearly j has to vary from 2-n for j 2 to n do i am going to look at th j element and i put that in key key a  j  i have to insert a  j  or the key in to the sorted sequence which is a  1  through a  j-1   i.e a  1_j-1  i am going to use the index i to do this what is index i going to do index i is going to run down from j-1 down to 1 we have to decrease index i  which we are doing in the while do loop it starts with the value j-1 i have to insert 7 and i am going to move 9 to 7th location  because 9 is greater than 7 then i compare 7 with 8 and 8 is still greater than 7  so i will move it right then i compare 7 with 6 as 6 is smaller than 7  i would put 7 in the appropriate place i run through this loop  till i find an element which is less than a key key is the element which i am trying to insert this loop will continue while the element  which i consider is more than key and this loop will terminate  when i see an element which is less than key or the loop will terminate when i reach i = 0 while i > 0 and a  i  > key do a  i + 1  a  i  that means i have moved everything to the right and i should insert the element at the very first place and i am just shifting the element one step to the right do a  i + 1  a  i  note that i have to insert 7 at the right place  so i shift 9 right to 1 step 9th location becomes empty  then i shift 8 to 1 step  so this 8th location becomes empty and now i put 7 there i + 1 is the index  which would be the empty location eventually and i put the key there a  i + 1  key all of you can implement it may be you would have implemented it in a slightly different way  that would give you a different program  but the algorithm is essentially the same you are going to find the right place for the element and insert it let us analyze this algorithm i have put down the algorithm on the left there is a small mistake in the last line of the slide  where there should be a left arrow please make a correction on that a  i + 1  a key let us count key a  j  i j-1 these are all my primitive operations i am comparing i with 0 and i am comparing a  i  with key  also i take and  so there are three primitive operations while i > 0 and a  i  > key each of the operation takes a certain amount of time  depending upon the computer system you have c1 ,c2 ,c3 ,c4 ,c5 ,c6 just represent the amount of time taken for these operations and they can be in any units i am counting the number of times  each of these operations is executed in this entire program why this operation is done n times i start by assigning j = 2 then assign 3  4,5,6,7 and go up to n then when i increment it once and check that there is one more  so i have counted it as n times there might be small errors in n and n + 1  but that is not very important roughly n times we need to do this operation.how about this operation key a  j  i am going to do exactly n-1 times once for 2  once for3  once for 4 up to n that is why this operation is being done up to n-1 times just leave the comment statement again the operation will be done exactly n-1 times we have to look at how many times i come to this statement while i > 0 and a  i  > key j t  counts the number of times i have to shift an element to the right  when i am inserting the th j card in to my hand in the previous example when i am inserting 7  i had to shift 2 elements 8 and 9 j t is going to count that quantity and that is the number of times i am going to reach a  i  part of my while loop while i > 0 and a  i  > key i will be checking this condition for many times for one iteration or for the th j iteration of this for loop  i am going to reach this condition for j t times the total number of times i am saying that condition is the sum of j t as j goes from 2 to n 2 n j j t while i > 0 and a  i  > key do a  i + 1  a  i  every time i see  a  i  > key  condition i also come to a  i   because the last time i see the statement i would exit out of this condition that is why this is j t -1 where j going from 2 to n 1 2   n j j t a  i + 1  a key this statement here is not a part of the while loop rather it is a part of the for loop as it is done exactly n-1 times as the other statement if you knew about the constants then the total time taken by the procedure can be computed you do not know what j t is j t is quantity which depends upon your instance and not problem problem is in the sorting the instance is a set or a sequence of numbers that have given to you thus j t depends upon the instance let us see the difference that j t makes if the input was already sorted  then j t is always 1  j t = 1   i just have to compare the element with the last element and if it is larger than the last element  i would not have to do anything j t is always a 1 if the input is already in increasing order what happens when the input is in decreasing order if the input is in decreasing order  then the number that i am trying to insert is going to be smaller than all the numbers that i have sorted in my array what am i going to do i am going to compare with the 1st element  2nd element  3rd element  4th element and all the way up to the1st element when i am trying to insert the th j element  i am going to end up in comparing with all the other j elements in the array in that case when j t is equal to j  note that the quantity becomes its summation of j  where j goes from 2 to n it is of the kind 2 n and the running time of this algorithm would be some constant time 2 n plus some other constant times n minus some other constant thus the behavior of this running time is more like 2 n  we will come to this point later  when we talk about asymptotic analysis but this is what i meant by 2 f   n  on the other hand in the best case when j t = 1  the sum is just n or n-1 and in that case the total time is n times some constant plus n-1 times some constant minus some constant which is roughly n times some constant hence this is called as linear time algorithm on an average what would you expect in the best case you have to compare only against one element and in the worst case you have to compare about j elements in the average case it would compare against half of those elements thus it will compare with 2 j  even when the summation of 2 j where j goes from 2 to n  this will be roughly by 2 4 n and it behaves like 2 n  this is what i mean by the best  worst and average case i take the size of input  suppose if i am interested in sorting n numbers and i look at all possible instances of these n numbers it may be infinitely many  again it is not clear about how to do that what is worst case the worst case is defined as the maximum possible time that your algorithm would take for any instance of that size in the slide 27  08  all the instances are of the same size the best case would be the smallest time that your algorithm takes and the average would be the average of all infinite bars that was for the input for 1size of size n  that would give the values  from that we can compute worst case  best case and the average case if i would consider inputs of all sizes then i can create a plot for each inputs size and i could figure out the worst case  best case and an average case then i would get such a monotonically increasing plots it is clear that as the size of the input increases  the time taken by your algorithm will increase thus when the input size becomes larger  it will not take lesser time which of this is the easiest to work with worst case is the one we will use the most for the purpose of this course this is the only measure we will be working with why is the worst case used often first it provides an upper bound and it tells you how long your algorithm is going to take in the worst case for some algorithms worst case occurs fairly often for many instances the time taken by the algorithm is close to the worst case average case essentially becomes as bad as the worst case in the previous example that we saw  the average case and the worst case were 2 n  there were differences in the constant but it was roughly the same the average case might be very difficult to compute  because you should look at all possible instances and then take some kind of an average or you have to say like  when my input instance is drawn from a certain distribution and the expected time my algorithm will take is typically a much harder quantity to work and to compute with the worst case is the measure of interest in which we will be working with asymptotic analysis is the kind of thing that we have been doing so far as n and 2 n and the goal of this is to analyze the running time while getting rid of superficial details we would like to say that an algorithm  which has the running time of some constant times 2 n squared is the same as an algorithm which has a running time of some other constant times 2 n ,because this constant is typically something which would be dependent upon the hardware that your using 2 3n = 2 n in the previous example 1 c  2 c and 3 c would depend upon the computer system  the hardware  the compiler and many factors we are not interested to distinguish between such algorithms both of these algorithms  one which has the running time of 2 3n and another with running time 2 n have a quadratic behavior when the input size doubles the running time of both of the algorithm increases four fold that is the thing which is of interest to us we are interested in capturing how the running time of algorithm increases  with the size of the input in the limit this is the crucial point here and the asymptotic analysis clearly explains about how the running time of this algorithm increases with increase in input size within the limit let us see about the big-oh o-notation if i have functions f   n  g  n  and n represents the input size f  n  measures the time taken by that algorithm f  n  and g  n  are nonnegative functions and also non-decreasing  because as the input size increases  the running time taken by the algorithm would also increase both of these are nondecreasing functions of n and we say that f  n  is o  g  n    if there exist constants c and 0 n  such that f  n  c times of g  n  0 n  f  n  = o  g  n  f  n  c g  n  for n 0 n what does it mean i have drawn two functions the function in red is f  n  and g  n  is some other function the function in green is some constant times of g  n   as you can see beyond the point 0 n  c  g  n   is always larger than that of f  n   this is the way it continues even beyond then we would say that f  n  is o  g  n  or f  n  is order  g  n    f  n  = o  g  n   few examples would clarify this and we will see those examples the function f  n  = 2n + 6 and g  n  = n if you look at these two functions 2n + 6 is always larger than n and you might be wondering why this 2n + 6 is a non-linear function that is because the scale here is an exponential scale the scale increases by 2 on y-axis and similarly on x-axis the red colored line is n and the blue line is 2n and the above next line is 4n as you can see beyond the dotted line f  n  is less than 4 times of n hence the constant c is 4 and 0 n would be this point of crossing beyond which 4n becomes larger than 2n + 6 at what point does 4n becomes larger than 2n + 6 it is three so 0 n becomes three then we say that f  n  which is 2n + 6 is o  n   2n + 6 = o  n  let us look at another example the function in red is g  n  which is n and any constant time g  n  which is as same scale as in the previous slide any constant time g  n  will be just the same straight line displaced by suitable amount the green line will be 4 times n and it depends upon the intercept  but you re 2 n would be like the line which is blue in color so there is no constant c such that 2 n < c  n   can you find out a constant c so that 2 n < c  n  for n more than 0 n  we can not find it any constant that you choose  i can pick a larger n such that this is violated and so it is not the case that 2 n is o  n   how does one figure out these things this is the very simple rule suppose this is my function 50 n log n  i just drop all constants and the lower order terms forget the constant 50 and i get n log n this function 50 n log n is o  n log n   in the function 7n-3  i drop the constant and lower order terms  i get 7n-3 as o  n   i have some complicated function like 8 2 n log n + 5 2 n + n in which i just drop all lower order terms this is the fastest growing term because this has 2 n as well as log n in it i just drop 2 n  n term and also i drop my constant and get 2 n log n this function is o  2 n log n   in the limit this quantity  8 2 n log n + 5 2 n + n  will be less than some constant times this quantity  o  2 n log n    you can figure out what should be the value of c and 0 n  for that to happen this is a common error the function 50 n log n is also o  5 n   whether it is yes or no it is yes  because this quantity  50 n log n  in fact is 50 times 5 n always  for all n and that is just a constant so this is o  5 n   but when we use the o-notation we try and provide as strong amount as possible instead of saying this statement is true we will rather call this as o  n log n    we will see more of this in subsequent slides how are we going to use the o-notation we are going to express the number of primitive operations that are executed during run of the program as a function of the input size we are going to use o-notation for that if i have an algorithm which takes the number of primitive operations as o  n  and some other algorithm for which the number of primitive operations is o  2 n   then clearly the first algorithm is better than the second why because as the input size doubles then the running time of the algorithm is also going to double  while the running time of o  2 n  algorithm will increase four fold similarly our algorithm which has the running time of o  log n  is better than the one which has running time of o  n   thus we have a hierarchy of functions in the order of log n  n  2 n  3 n  2n  there is a word of caution here you might have an algorithm whose running time is 1,000,000 n  because you may be doing some other operations i can not see how you would create such an algorithm  but you might have an algorithm of this running time 1,000,000n is o  n   because this is some constant time n and you might have some other algorithm with the running time of 2 2 n  hence from what i said before  you would say that 1,000,000 n algorithm is better than 2 2 n  the one with the linear running time which is o  n  running time is better than o  2 n   it is true but in the limit and the limit is achieved very late when n is really large for small instances this 2 2 n might actually take less amount of time than your 1,000,000 n you have to be careful about the constants also we will do some examples of asymptotic analysis i have a pseudo code and i have an array of n numbers sitting in an array called x and i have to output an array a  in which the element a  i  is the average of the numbers x  0  through x  i   one way of doing it is  i basically have a for loop in which i compute each element of the array a to compute a  10   i just have to sum up x  0  through x  10   which i am doing here for j 0 to i do a a + x  j  a  i  a/  i + 1  to compute a  10   i is taking the value 10 and i am running the index j from 0-10 i am summing up the value of x from x  0   x  10  in this accumulator a and then i am eventually dividing the value of this accumulator with 11  because it is from x  0  to x  10   that gives me the number i should have in a  10   i am going to repeat this for 11,12,13,14 and for all the elements it is an algorithm and let us compute the running time this is one step it is executed for i number of times and initially i take a value from 0,1,2,3 and all the way up to n-1 this entire thing is done n times this gives you the total running time of roughly 2 n  a a + x  j  this one step is getting executed 2 n times and this is the dominant thing how many times the steps given below are executed a  i  a/  j + 1  a 0 these steps are executed for n times a a + x  j  but the step mentioned above is getting executed roughly for some constant 2 n times thus the running time of the algorithm is o  2 n   it is a very simple problem but you can have a better solution what is a better solution we will have a variable s in which we would keep accumulating the x  i   initially s = 0 when i compute a  i   which i already have in s  x  0  through x  i-1  because they used that at the last step that is the problem here a a + x  j  every time we are computing x first we are computing x  0  + x  1   then we are computing x  0  + x  1  + x  2  and goes on it is a kind of repeating computations why should we do that we will have a single variable which will keep track of the sum of the prefixes s at this point  s s + x  i    when i am in the th i run of this loop has some of x  0  through x  i-1  and then some x  i  in it to compute th i element  i just need to divide this sum by i + 1 s s + x  i  a  i  s/  i + 1  i keep this accumulator  s  around with me when i finish the th i iteration of this loop  i have an s  the sum x  0  through x  i   i can reuse it for the next step how much time does this take in each run of this loop i am just doing two primitive operations that makes an order n times  because this loop is executed n times i have been using this freely linear and quadratic  but the slide given below just tells you the other terms i might be using linear is when an algorithm has an asymptotic running time of o  n   then we call it as a linear algorithm if it has asymptotic running time of 2 n  we called it as a quadratic and logarithmic if it is log n it is polynomial if it is k n for some constant k algorithm is called exponential if it has running time of n a  where a is some number more than 1 till now i have introduced only the big-oh notation  we also have the bigomega notation and big-theta notation the big-omega notation provides a lower bound the function f  n  is omega of g  n   f  n  =  g  n   if constant time g  n  is always less than f  n   earlier that was more than f  n  but now it is less than f  n  in the limit  beyond a certain 0 n as the picture given below illustrates c g  n  f  n  for n 0 n f  n  is more than c  g  n   beyond the point 0 n  that case we will say that f  n  is omega of g  n   f  n  =  g  n    in notation f  n  is  g  n   if there exist constant c1 and c2 such that f  n  is sandwiched between c1 g  n  and c2 g  n   beyond a certain point  f  n  lies between 1 constant time g  n  and another constant time of g  n   then f  n  is  g  n   where f  n  grows like g  n  in the limit another way of thinking of it is  f  n  is  g  n    if f  n  is o  g  n   and it also  g  n   there are two more related asymptotic notations  one is called little-oh notation and the other is called little-omega notation they are the non-tight analogs of big-oh and big-omega it is best to understand this through the analogy of real numbers when f  n  is o  g  n   and the function f is less than or equal to g or f  n  is less than c  g  n   the analogy with the real numbers is when the number is less than or equal to another number is for and is for =   g  n  is function and f = g are real numbers if these are real numbers  you can talk of equality but you can not talk of equality for a function unless they are equal little-oh corresponds to strictly less than g and littleomega corresponds to strictly more we are not going to use these  infact we will use big-oh you should be very clear with that part the formal definition for little-oh is that  for every constant c there should exist some 0 n such that f  n  is < c  g  n  for n > 0 n  f  n  c  g  n   for n 0 n how it is different from big oh in that case i said  there exist c and 0 n such that this is true here we will say for every c there should exist an 0 n  the slide which is below defines the difference between the functions i have an algorithm whose running times are like 400n  20n log n  2 2 n  4 n and 2n  also i have listed out  the largest problem size that you can solve in 1 second or 1 minute or 1 hour the largest problem size that you can solve is roughly 2500 let us say if you have 20n log n as running time then the problem size would be like 4096 why did you see that 4096 is larger than 2500  although 20n log n is the worst running time than 400n  because of the constant you can see the differences happening if it is 2 2 n then the problem size is 707 and when it is 2n the problem size is 19 see the behavior as the time increases an hour is 3600seconds and there is a huge increase in the size of the problem you solve  if it is linear time algorithm still there is a large increase  when it is n log n algorithm and not so large increase when it is an 2 n algorithm and almost no increase when it is 2n algorithm if you have an algorithm whose running time is something like 2n  you can not solve for problem of more than size 100 it will take millions of years to solve it.this is the behavior we are interested in our course hence we consider asymptotic analysis for this 