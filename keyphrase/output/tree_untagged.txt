tree  data structure  from wikipedia  the free encyclopedia not to be confused with trie  a specific type of tree data structure this article needs additional citations for verification please help improve this article by adding citations to reliable sources unsourced material may be challenged and removed  august 2010  a simple unordered tree ; in this diagram  the node labeled 7 has two children  labeled 2 and 6  and one parent  labeled 2 the root node  at the top  has no parent in computer science  a tree is a widely used abstract data type  adt  or data structure implementing this adt that simulates a hierarchical tree structure  with a root value and subtrees of children with a parent node  represented as a set of linked nodes a tree data structure can be defined recursively  locally  as a collection of nodes  starting at a root node   where each node is a data structure consisting of a value  together with a list of references to nodes  the " children "   with the constraints that no reference is duplicated  and none points to the root alternatively  a tree can be defined abstractly as a whole  globally  as an ordered tree  with a value assigned to each node both these perspectives are useful  while a tree can be analyzed mathematically as a whole  when actually represented as a data structure it is usually represented and worked with separately by node  rather than as a list of nodes and an adjacency list of edges between nodes  as one may represent a digraph  for instance   for example  looking at a tree as a whole  one can talk about " the parent node " of a given node  but in general as a data structure a given node only contains the list of its children  but does not contain a reference to its parent  if any   contents  hide  * 1 definition * 2 terminologies used in trees o 2.1 data type vs data structure o 2.2 recursive o 2.3 type theory o 2.4 mathematical * 3 terminology * 4 drawing trees * 5 representations * 6 generalizations o 6.1 digraphs * 7 traversal methods * 8 common operations * 9 common uses * 10 see also o 10.1 other trees * 11 notes * 12 references * 13 external links definition  edit  not a tree  two non-connected parts  a b and c d e not a tree  undirected cycle 1-2-4-3 not a tree  cycle b c e d b not a tree  cycle a a each linear list is trivially a tree a tree is a  possibly non-linear  data structure made up of nodes or vertices and edges without having any cycle the tree with no nodes is called the null orempty tree a tree that is not empty consists of a root node and potentially many levels of additional nodes that form a hierarchy terminologies used in trees  edit  * root the top node in a tree * parent the converse notion of child * siblings nodes with the same parent * descendant a node reachable by repeated proceeding from parent to child * ancestor a node reachable by repeated proceeding from child to parent * leaf a node with no children * internal node a node with at least one child * external node a node with no children * degree number of sub trees of a node * edge connection between one node to another * path a sequence of nodes and edges connecting a node with a descendant * level the level of a node is defined by 1 + the number of connections between the node and the root * height of tree the height of a tree is the number of edges on the longest downward path between the root and a leaf * height of node the height of a node is the number of edges on the longest downward path between that node and a leaf * depth the depth of a node is the number of edges from the node to the tree 's root node * forest a forest is a set of n 0 disjoint trees data type vs data structure  edit  there is a distinction between a tree as an abstract data type and as a concrete data structure  analogous to the distinction between a list and a linked list as a data type  a tree has a value and children  and the children are themselves trees ; the value and children of the tree are interpreted as the value of the root node and the subtrees of the children of the root node to allow finite trees  one must either allow the list of children to be empty  in which case trees can be required to be non-empty  an " empty tree " instead being represented by a forest of zero trees   or allow trees to be empty  in which case the list of children can be of fixed size  branching factor  especially 2 or " binary "   if desired as a data structure  a linked tree is a group of nodes  where each node has a value and a list of references to other nodes  its children   this data structure actually defines a directed graph   a  because it may have loops or several references to the same node  just as a linked list may have a loop thus there is also the requirement that no two references point to the same node  that each node has at most a single parent  and in fact exactly one parent  except for the root   and a tree that violates this is " corrupt "  due to the use of references to trees in the linked tree data structure  trees are often discussed implicitly assuming that they are being represented by references to the root node  as this is often how they are actually implemented for example  rather than an empty tree  one may have a null reference  a tree is always non-empty  but a reference to a tree may be null recursive  edit  recursively  as a data type a tree is defined as a value  of some data type  possibly empty   together with a list of trees  possibly an empty list   the subtrees of its children ; symbolically  t  v  t  1     t  k    a tree t consists of a value v and a list of other trees  more elegantly  via mutual recursion  of which a tree is one of the most basic examples  a tree can be defined in terms of a forest  a list of trees   where a tree consists of a value and a forest  the subtrees of its children   f   t  1     t  k   t  v f note that this definition is in terms of values  and is appropriate in functional languages  it assumes referential transparency  ; different trees have no connections  as they are simply lists of values as a data structure  a tree is defined as a node  the root   which itself consists of a value  of some data type  possibly empty   together with a list of references to other nodes  list possibly empty  references possibly null  ; symbolically  n  v  &n  1     &n  k    a node n consists of a value v and a list of references to other nodes  this data structure defines a directed graph   b  and for it to be a tree one must add a condition on its global structure  its topology   namely that at most one reference can point to any given node  a node has at most a single parent   and no node in the tree point to the root in fact  every node  other than the root  must have exactly one parent  and the root must have no parents indeed  given a list of nodes  and for each node a list of references to its children  one can not tell if this structure is a tree or not without analyzing its global structure and that it is in fact topologically a tree  as defined below type theory  edit  as an adt  the abstract tree type t with values of some type e is defined  using the abstract forest type f  list of trees   by the functions  value  t e children  t f nil    f node  e f t with the axioms  value  node  e  f   = e children  node  e  f   = f in terms of type theory  a tree is an inductive type defined by the constructors nil  empty forest  and node  tree with root node with given value and children   mathematical  edit  viewed as a whole  a tree data structure is an ordered tree  generally with values attached to each node concretely  it is  if required to be non-empty   * a rooted tree with the " away from root " direction  a more narrow term is an " arborescence "   meaning  * a directed graph  * whose underlying undirected graph is a tree  any two vertices are connected by exactly one simple path   * with a distinguished root  one vertex is designated as the root   * which determines the direction on the edges  arrows point away from the root ; given an edge  the node that the edge points from is called the parent and the node that the edge points to is called the child   together with  * an ordering on the child nodes of a given node  and * a value  of some data type  at each node often trees have a fixed  more properly  bounded  branching factor  outdegree   particularly always having two child nodes  possibly empty  hence at most two non-empty child nodes   hence a " binary tree "  allowing empty trees makes some definitions simpler  some more complicated  a rooted tree must be non-empty  hence if empty trees are allowed the above definition instead becomes " an empty tree  or a rooted tree such that  "  on the other hand  empty trees simplify defining fixed branching factor  with empty trees allowed  a binary tree is a tree such that every node has exactly two children  each of which is a tree  possibly empty  .the complete sets of operations on tree must include fork operation terminology  edit  a node is a structure which may contain a value or condition  or represent a separate data structure  which could be a tree of its own   each node in a tree has zero or morechild nodes  which are below it in the tree  by convention  trees are drawn growing downwards   a node that has a child is called the child 's parent node  or ancestor node  orsuperior   a node has at most one parent an internal node  also known as an inner node  inode for short  or branch node  is any node of a tree that has child nodes similarly  an external node  also known as anouter node  leaf node  or terminal node  is any node that does not have child nodes the topmost node in a tree is called the root node depending on definition  a tree may be required to have a root node  in which case all trees are non-empty   or may be allowed to be empty  in which case it does not necessarily have a root node being the topmost node  the root node will not have a parent it is the node at which algorithms on the tree begin  since as a data structure  one can only pass from parents to children note that some algorithms  such as post-order depth-first search  begin at the root  but first visit leaf nodes  access the value of leaf nodes   only visit the root last  i.e  they first access the children of the root  but only access the value of the root last   all other nodes can be reached from it by following edges or links  in the formal definition  each such path is also unique  in diagrams  the root node is conventionally drawn at the top in some trees  such as heaps  the root node has special properties every node in a tree can be seen as the root node of the subtree rooted at that node the height of a node is the length of the longest downward path to a leaf from that node the height of the root is the height of the tree the depth of a node is the length of the path to its root  i.e  its root path   this is commonly needed in the manipulation of the various self-balancing trees  avl trees in particular the root node has depth zero  leaf nodes have height zero  and a tree with only a single node  hence both a root and leaf  has depth and height zero conventionally  an empty tree  tree with no nodes  if such are allowed  has depth and height 1 a subtree of a tree t is a tree consisting of a node in t and all of its descendants in t  c   1  nodes thus correspond to subtrees  each node corresponds to the subtree of itself and all its descendants  the subtree corresponding to the root node is the entire tree  and each node is the root node of the subtree it determines ; the subtree corresponding to any other node is called a proper subtree  by analogy to a proper subset   drawing trees  edit  trees are often drawn in the plane ordered trees can be represented essentially uniquely in the plane  and are hence called plane trees  as follows  if one fixes a conventional order  say  counterclockwise   and arranges the child nodes in that order  first incoming parent edge  then first child edge  etc   this yields an embedding of the tree in the plane  unique up to ambient isotopy conversely  such an embedding determines an ordering of the child nodes if one places the root at the top  parents above children  as in a family tree  and places all nodes that are a given distance from the root  in terms of number of edges  the " level " of a tree  on a given horizontal line  one obtains a standard drawing of the tree given a binary tree  the first child is on the left  the " left node "   and the second child is on the right  the " right node "   representations  edit  there are many different ways to represent trees ; common representations represent the nodes as dynamically allocated records with pointers to their children  their parents  or both  or as items in an array  with relationships between them determined by their positions in the array  e.g  binary heap   indeed  a binary tree can be implemented as a list of lists  a list where the values are lists   the head of a list  the value of the first term  is the left child  subtree   while the tail  the list of second and future terms  is the right child  subtree   this can be modified to allow values as well  as in lisp s-expressions  where the head  value of first term  is the value of the node  the head of the tail  value of second term  is the left child  and the tail of the tail  list of third and future terms  is the right child in general a node in a tree will not have pointers to its parents  but this information can be included  expanding the data structure to also include a pointer to the parent  or stored separately alternatively  upward links can be included in the child node data  as in a threaded binary tree generalizations  edit  digraphs  edit  if edges  to child nodes  are thought of as references  then a tree is a special case of a digraph  and the tree data structure can be generalized to represent directed graphs by removing the constraints that a node may have at most one parent  and that no cycles are allowed edges are still abstractly considered as pairs of nodes  however  the termsparent and child are usually replaced by different terminology  for example  source and target   different implementation strategies exist  a digraph can be represented by the same local data structure as a tree  node with value and list of children   assuming that " list of children " is a list of references  or globally by such structures as adjacency lists in graph theory  a tree is a connected acyclic graph ; unless stated otherwise  in graph theory trees and graphs are assumed undirected there is no one-to-one correspondence between such trees and trees as data structure we can take an arbitrary undirected tree  arbitrarily pick one of its vertices as the root  make all its edges directed by making them point away from the root node producing an arborescence and assign an order to all the nodes the result corresponds to a tree data structure picking a different root or different ordering produces a different one given a node in a tree  its children define an ordered forest  the union of subtrees given by all the children  or equivalently taking the subtree given by the node itself and erasing the root   just as subtrees are natural for recursion  as in a depth-first search   forests are natural for corecursion  as in a breadth-first search   via mutual recursion  a forest can be defined as a list of trees  represented by root nodes   where a node  of a tree  consists of a value and a forest  its children   f   n  1     n  k   n  v f traversal methods  edit  main article  tree traversal stepping through the items of a tree  by means of the connections between parents and children  is called walking the tree  and the action is a walk of the tree often  an operation might be performed when a pointer arrives at a particular node a walk in which each parent node is traversed before its children is called a pre-order walk ; a walk in which the children are traversed before their respective parents are traversed is called a post-order walk ; a walk in which a node 's left subtree  then the node itself  and finally its right subtree are traversed is called an in-order traversal  this last scenario  referring to exactly two subtrees  a left subtree and a right subtree  assumes specifically a binary tree  a level-order walk effectively performs a breadth-first search over the entirety of a tree ; nodes are traversed level by level  where the root node is visited first  followed by its direct child nodes and their siblings  followed by its grandchild nodes and their siblings  etc  until all nodes in the tree have been traversed common operations  edit  * enumerating all the items * enumerating a section of a tree * searching for an item * adding a new item at a certain position on the tree * deleting an item * pruning  removing a whole section of a tree * grafting  adding a whole section to a tree * finding the root for any node common uses  edit  * representing hierarchical data * storing data in a way that makes it easily searchable  see binary search tree and tree traversal  * representing sorted lists of data * as a workflow for compositing digital images for visual effects * routing algorithms see also  edit  * tree structure * tree  graph theory  * tree  set theory  * hierarchy  mathematics  * dialog tree * single inheritance other trees  edit  * trie * dsw algorithm * enfilade * left child-right sibling binary tree * hierarchical temporal memory notes  edit  1 jump up ^ properly  a rooted  ordered directed graph 2 jump up ^ properly  a rooted  ordered directed graph 3 jump up ^ this is different from the formal definition of subtree used in graph theory  which is a subgraph that forms a tree it need not include all descendants for example  the root node by itself is a subtree in the graph theory sense  but not in the data structure sense  unless there are no descendants   references  edit  1 jump up ^ weisstein  eric w  " subtree "  mathworld * donald knuth the art of computer programming  fundamental algorithms  third edition addison-wesley  1997 isbn 0-201-89683-4  section 2.3  trees  pp 308 423 * thomas h cormen  charles e leiserson  ronald l rivest  and clifford stein introduction to algorithms  second edition mit press and mcgraw-hill  2001 isbn 0-262-03293-7  section 10.4  representing rooted trees  pp 214 217 chapters 12 14  binary search trees  red-black trees  augmenting data structures   pp 253 320 external links  edit  wikimedia commons has media related to tree structures * data trees as a means of presenting complex data analysis by sally knipe * description from the dictionary of algorithms and data structures * wormweb.org  interactive visualization of the c elegans cell tree visualize the entire cell lineage tree of the nematode c elegans  javascript  * binary trees by l allison  show  * v * t * e tree data structures  show  * v * t * e data structures categories  * data types * trees  data structures  * knowledge representation binary tree from wikipedia  the free encyclopedia not to be confused with b-tree a labeled binary tree of size 9 and height 3  with a root node whose value is 2 the above tree is unbalanced and not sorted in computer science  a binary tree is a tree data structure in which each node has at most two children  which are referred to as the left child and the right child a recursive definition using just set theory notions is that a  non-empty  binary tree is a triple  l  s  r   where l and r are binary trees or the empty set and s is a singleton set  1  some authors allow the binary tree to be the empty set as well  2  from a graph theory perspective  binary  and k-ary  trees as defined here are actually arborescences  3  a binary tree may thus be also called a bifurcating arborescence  3  a term which actually appears in some very old programming books   4  before the modern computer science terminology prevailed it is also possible to interpret a binary tree as an undirected  rather than a directed graph  in which case a binary tree is an ordered  rooted tree  5  some authors use rooted binary tree instead of binary tree to emphasize the fact that the tree is rooted  but as defined above  a binary tree is always rooted  6  a binary tree is a special case of an ordered k-ary tree  where k is 2 in computing  binary trees are seldom used solely for their structure much more typical is to define a labeling function on the nodes  which associates some value to each node  7  binary trees labelled this way are used to implement binary search trees and binary heaps  and are used for efficient searching and sorting the designation of non-root nodes as left or right child even when there is only one child present matters in some of these applications  in particular it is significant in binary search trees  8  in mathematics  what is termed binary tree can vary significantly from author to author some use the definition commonly used in computer science   9  but others define it as every non-leaf having exactly two children and do n't necessarily order  as left/right  the children either  10  contents  hide  * 1 definitions o 1.1 recursive definition o 1.2 using graph theory concepts * 2 types of binary trees * 3 properties of binary trees * 4 combinatorics * 5 methods for storing binary trees o 5.1 nodes and references o 5.2 arrays * 6 encodings o 6.1 succinct encodings o 6.2 encoding general trees as binary trees * 7 common operations o 7.1 insertion * 7.1.1 leaf nodes * 7.1.2 internal nodes o 7.2 deletion * 7.2.1 node with zero or one children * 7.2.2 node with two children o 7.3 traversal * 7.3.1 depth-first order * 7.3.2 breadth-first order * 8 see also * 9 references o 9.1 citations o 9.2 bibliography * 10 external links definitions  edit  recursive definition  edit  this article may need to be rewritten entirely to comply with wikipedia 's quality standards  as section you can help thediscussion page may contain suggestions  july 2014  another way of defining a full binary tree is a recursive definition a full binary tree is either   11  * a single vertex * a graph formed by taking two  full  binary trees  adding a vertex  and adding an edge directed from the new vertex to the root of each binary tree this also does not establish the order of children  but does fix a specific root node to actually define a binary tree in general  we must allow for the possibility that only one of children may be empty an artifact  which in some textbooks is called an extended binary tree is needed for that purpose an extended binary tree is thus recursively defined as   11  * the empty set is an extended binary tree * if t1 and t2 are extended binary trees  then denote by t1 t2 the extended binary tree obtained by adding a root r connected to the left to t1 and to the right to t2 by adding edges when these sub-trees are non-empty another way of imagining this construction  and understanding the terminology  is to consider instead of the empty set a different type of node for instance square nodes if the regular ones are circles  12  using graph theory concepts  edit  a binary tree is a rooted tree that is also an ordered tree  a.k.a plane tree  in which every node has at most two children a rooted tree naturally imparts a notion of levels  distance from the root   thus for every node a notion of children may be defined as the nodes connected to it a level below ordering of these children  e.g  by drawing them on a plane  makes possible to distinguish left child from right child  13  but this still does n't distinguish between a node with left but not a right child from a one with right but no left child the necessary distinction can be made by first partitioning the edges  i.e  defining the binary tree as triplet  v  e1  e2   where  v  e1 e2  is a rooted tree  equivalently arborescence  and e1 e2 is empty  and also requiring that for all j  1  2  every node has at most one ej child  14  a more informal way of making the distinction is to say  quoting the encyclopedia of mathematics  that " every node has a left child  a right child  neither  or both " and to specify that these " are all different " binary trees  9  types of binary trees  edit  tree terminology is not well-standardized and so varies in the literature * a rooted binary tree has a root node and every node has at most two children * a full binary tree  sometimes referred to as a proper  citation needed  or plane binary tree   15   16  is a tree in which every node in the tree has either 0 or 2 children an ancestry chart which maps to a perfect depth-4 binary tree * a perfect binary tree is a binary tree in which all leaves have the same depth or same level  17   this is ambiguously also called acomplete or full binary tree  citation needed   an example of a perfect binary tree is the ancestry chart of a person to a given depth  as each person has exactly two biological parents  one mother and one father   * in a complete binary tree every level  except possibly the last  is completely filled  and all nodes in the last level are as far left as possible it can have between 1 and 2h nodes at the last level h  18  a binary tree is called an almost complete binary tree or nearly complete binary tree if the last level is not completely filled  clarification needed  this type of binary tree is used as a specialized data structure called a binary heap  18  * in the infinite complete binary tree  every node has two children  and so the set of levels is countably infinite   the set of all nodes is countably infinite  but the set of all infinite paths from the root is uncountable  having the cardinality of the continuum these paths correspond by an order-preserving bijection to the points of the cantor set  or  using the example of a stern brocot tree  to the set of positive irrational numbers * a balanced binary tree has the minimum possible maximum height  a.k.a depth  for the leaf nodes  because for any given number of leaf nodes the leaf nodes are placed at the greatest height possible  clarification needed  h balanced unbalanced  h =  n + 1  /2  1 0  abcde abcde / \ / \ 1  abcd e abcd e / \ / \ 2  ab cd abc d / \ / \ / \ 3  a b c d ab c / \ 4  a b one common balanced tree structure is a binary tree structure in which the left and right subtrees of every node differ in height by no more than 1  19  one may also consider binary trees where no leaf is much farther away from the root than any other leaf  different balancing schemes allow different definitions of " much farther "   20   * a degenerate  or pathological  tree is where each parent node has only one associated child node  citation needed  this means that performance-wise  clarification needed   the tree will behave like a linked list data structure properties of binary trees  edit  * the number of nodes in a full binary tree  is at least and at most  where is the height of the tree a tree consisting of only a root node has a height of 0 * the number of leaf nodes in a perfect binary tree  is because the number of non-leaf  a.k.a internal  nodes  * this means that a perfect binary tree with leaves has nodes * in a balanced full binary tree   see ceiling function   * in a perfect full binary tree  thus  * the maximum possible number of null links  i.e  absent children of the nodes  in a complete binary tree of n nodes is  n + 1   where only 1 node exists in bottom-most level to the far left * the number of internal nodes in a complete binary tree of n nodes is n/2  * for any non-empty binary tree with n0 leaf nodes and n2 nodes of degree 2  n0 = n2 + 1  21  combinatorics  edit  this section does not cite any references or sources please help improve this section by adding citations to reliable sources unsourced material may be challenged and removed  july 2014  in combinatorics one considers the problem of counting the number of full binary trees of a given size here the trees have no values attached to their nodes  this would just multiply the number of possible trees by an easily determined factor   and trees are distinguished only by their structure ; however the left and right child of any node are distinguished  if they are different trees  then interchanging them will produce a tree distinct from the original one   the size of the tree is taken to be the number n of internal nodes  those with two children  ; the other nodes are leaf nodes and there are n + 1 of them the number of such binary trees of size n is equal to the number of ways of fully parenthesizing a string of n + 1 symbols  representing leaves  separated by n binary operators  representing internal nodes   so as to determine the argument subexpressions of each operator for instance for n = 3 one has to parenthesize a string like  which is possible in five ways  the correspondence to binary trees should be obvious  and the addition of redundant parentheses  around an already parenthesized expression or around the full expression  is disallowed  or at least not counted as producing a new possibility   there is a unique binary tree of size 0  consisting of a single leaf   and any other binary tree is characterized by the pair of its left and right children ; if these have sizes i and jrespectively  the full tree has size i + j + 1 therefore the number of binary trees of size n has the following recursive description  and for any positive integer n it follows that is the catalan number of index n the above parenthesized strings should not be confused with the set of words of length 2n in the dyck language  which consist only of parentheses in such a way that they are properly balanced the number of such strings satisfies the same recursive description  each dyck word of length 2n is determined by the dyck subword enclosed by the initial '  ' and its matching '  ' together with the dyck subword remaining after that closing parenthesis  whose lengths 2i and 2j satisfy i + j + 1 = n  ; this number is therefore also the catalan number  so there are also five dyck words of length 10   these dyck words do not correspond in an obvious way to binary trees a bijective correspondence can nevertheless be defined as follows  enclose the dyck word in an extra pair of parentheses  so that the result can be interpreted as a lisp list expression  with the empty list   as only occurring atom  ; then the dotted-pair expression for that proper list is a fully parenthesized expression  with nil as symbol and ' ' as operator  describing the corresponding binary tree  which is in fact the internal representation of the proper list   the ability to represent binary trees as strings of symbols and parentheses implies that binary trees can represent the elements of a free magma on a singleton set methods for storing binary trees  edit  binary trees can be constructed from programming language primitives in several ways nodes and references  edit  in a language with records and references  binary trees are typically constructed by having a tree node structure which contains some data and references to its left child and its right child sometimes it also contains a reference to its unique parent if a node has fewer than two children  some of the child pointers may be set to a special null value  or to a special sentinel node this method of storing binary trees wastes a fair bit of memory  as the pointers will be null  or point to the sentinel  more than half the time ; a more conservative representation alternative is threaded binary tree  22  in languages with tagged unions such as ml  a tree node is often a tagged union of two types of nodes  one of which is a 3-tuple of data  left child  and right child  and the other of which is a " leaf " node  which contains no data and functions much like the null value in a language with pointers for example  the following line of code in ocaml  an ml dialect  defines a binary tree that stores a character in each node  23  type chr_tree = empty | node of char * chr_tree * chr_tree arrays  edit  binary trees can also be stored in breadth-first order as an implicit data structure in arrays  and if the tree is a complete binary tree  this method wastes no space in this compact arrangement  if a node has an index i  its children are found at indices  for the left child  and  for the right   while its parent  if any  is found at index  assuming the root has index zero   this method benefits from more compact storage and better locality of reference  particularly during a preorder traversal however  it is expensive to grow and wastes space proportional to 2h  n for a tree of depth h with n nodes this method of storage is often used for binary heaps no space is wasted because nodes are added in breadth-first order encodings  edit  succinct encodings  edit  a succinct data structure is one which occupies close to minimum possible space  as established by information theoretical lower bounds the number of different binary trees on nodes is  the th catalan number  assuming we view trees with identical structure as identical   for large  this is about ; thus we need at least about bits to encode it a succinct binary tree therefore would occupy bits one simple representation which meets this bound is to visit the nodes of the tree in preorder  outputting " 1 " for an internal node and " 0 " for a leaf  1  if the tree contains data  we can simply simultaneously store it in a consecutive array in preorder this function accomplishes this  function encodesuccinct  node n  bitstring structure  array data   if n = nil then append 0 to structure ; else append 1 to structure ; append n.data to data ; encodesuccinct  n.left  structure  data  ; encodesuccinct  n.right  structure  data  ;  the string structure has only bits in the end  where is the number of  internal  nodes ; we do n't even have to store its length to show that no information is lost  we can convert the output back to the original tree like this  function decodesuccinct  bitstring structure  array data   remove first bit of structure and put it in b if b = 1 then create a new node n remove first element of data and put it in n.data n.left = decodesuccinct  structure  data  n.right = decodesuccinct  structure  data  return n else return nil  more sophisticated succinct representations allow not only compact storage of trees but even useful operations on those trees directly while they 're still in their succinct form encoding general trees as binary trees  edit  there is a one-to-one mapping between general ordered trees and binary trees  which in particular is used by lisp to represent general ordered trees as binary trees to convert a general ordered tree to binary tree  we only need to represent the general tree in left child-right sibling way the result of this representation will be automatically binary tree  if viewed from a different perspective each node n in the ordered tree corresponds to a node n ' in the binary tree ; the left child of n ' is the node corresponding to the first child ofn  and the right child of n ' is the node corresponding to n 's next sibling  that is  the next node in order among the children of the parent of n this binary tree representation of a general order tree is sometimes also referred to as a left child-right sibling binary tree  lcrs tree   or a doubly chained tree  or a filial-heir chain one way of thinking about this is that each node 's children are in a linked list  chained together with their right fields  and the node only has a pointer to the beginning or head of this list  through its left field for example  in the tree on the left  a has the 6 children  b,c,d,e,f,g   it can be converted into the binary tree on the right the binary tree can be thought of as the original tree tilted sideways  with the black left edges representing first child and the blue right edges representing next sibling the leaves of the tree on the left would be written in lisp as     n o  i j  c d   p   q   f  m   which would be implemented in memory as the binary tree on the right  without any letters on those nodes that have a left child common operations  edit  tree rotations are very common internal operations on self-balancing binary trees there are a variety of different operations that can be performed on binary trees some are mutator operations  while others simply return useful information about the tree insertion  edit  nodes can be inserted into binary trees in between two other nodes or added after a leaf node in binary trees  a node that is inserted is specified as to which child it is leaf nodes  edit  to add a new node after leaf node a  a assigns the new node as one of its children and the new node assigns node a as its parent internal nodes  edit  the process of inserting a node into a binary tree insertion on internal nodes is slightly more complex than on leaf nodes say that the internal node is node a and that node b is the child of a  if the insertion is to insert a right child  then b is the right child of a  and similarly with a left child insertion  a assigns its child to the new node and the new node assigns its parent to a then the new node assigns its child to b and b assigns its parent as the new node deletion  edit  deletion is the process whereby a node is removed from the tree only certain nodes in a binary tree can be removed unambiguously  24  node with zero or one children  edit  the process of deleting an internal node in a binary tree suppose that the node to delete is node a if a has no children  deletion is accomplished by setting the child of a 's parent to null if a has one child  set the parent of a 's child to a 's parent and set the child of a 's parent to a 's child node with two children  edit  in a binary tree  a node with two children can not be deleted unambiguously  24  however  in certain binary trees  including binary search trees  these nodes can be deleted  though with a rearrangement of the tree structure traversal  edit  main article  tree traversal pre-order  in-order  and post-order traversal visit each node in a tree by recursively visiting each node in the left and right subtrees of the root depth-first order  edit  in depth-first order  we always attempt to visit the node farthest from the root node that we can  but with the caveat that it must be a child of a node we have already visited unlike a depth-first search on graphs  there is no need to remember all the nodes we have visited  because a tree can not contain cycles pre-order is a special case of this seedepth-first search for more information breadth-first order  edit  contrasting with depth-first order is breadth-first order  which always attempts to visit the node closest to the root that it has not already visited see breadth-first search for more information also called a level-order traversal in a complete binary tree  a node 's breadth-index  i   2d  1   can be used as traversal instructions from the root reading bitwise from left to right  starting at bit d  1  where d is the node 's distance from the root  d = floor  log2  i + 1    and the node in question is not the root itself  d > 0   when the breadth-index is masked at bit d  1  the bit values 0 and 1mean to step either left or right  respectively the process continues by successively checking the next bit to the right until there are no more the rightmost bit indicates the final traversal from the desired node 's parent to the node itself there is a time-space trade-off between iterating a complete binary tree this way versus each node having pointer/s to its sibling/s see also  edit  * 2 3 tree * 2 3 4 tree * aa tree * ahnentafel * avl tree * b-tree * binary space partitioning * huffman tree * k-ary tree * kraft 's inequality * optimal binary search tree * random binary tree * recursion  computer science  * red black tree * rope  computer science  * self-balancing binary search tree * splay tree * strahler number * tree of primitive pythagorean triples # alternative methods of generating the tree * unrooted binary tree references  edit  citations  edit  1 jump up ^ rowan garnier ; john taylor  2009   discrete mathematics  proofs  structures and applications  third edition crc press p 620 isbn 978-1-4398-1280-8 2 jump up ^ steven s skiena  2009   the algorithm design manual springer science & business media p 77 isbn 978-1-84800-070-4 3 ^ jump up to  a b knuth  1997   the art of computer programming  volume 1  3/e pearson education p 363 isbn 0-201-89683-4 4 jump up ^ iv n flores  1971   computer programming system/360 prentice-hall p 39 5 jump up ^ kenneth rosen  2011   discrete mathematics and its applications  7th edition mcgraw-hill science p 749 isbn 978-0-07-338309-5 6 jump up ^ david r mazur  2010   combinatorics  a guided tour mathematical association of america p 246 isbn 978-0-88385-762-5 7 jump up ^ david makinson  2009   sets  logic and maths for computing springer science & business media p 199 isbn 978-1-84628-845-6 8 jump up ^ jonathan l gross  2007   combinatorial methods with computer applications crc press p 248 isbn 978-1-58488-743-0 9 ^ jump up to  a b hazewinkel  michiel  ed  2001   " binary tree "  encyclopedia of mathematics,springer  isbn 978-1-55608-010-4 also in print as michiel hazewinkel  1997  .encyclopaedia of mathematics supplement i springer science & business media p 124 isbn 978-0-7923-4709-5 10 jump up ^ l.r foulds  1992   graph theory applications springer science & business media p 32 isbn 978-0-387-97599-3 11 ^ jump up to  a b kenneth rosen  2011   discrete mathematics and its applications 7th edition mcgraw-hill science pp 352 353 isbn 978-0-07-338309-5 12 jump up ^ te chiang hu ; man-tak shing  2002   combinatorial algorithms courier dover publications p 162 isbn 978-0-486-41962-6 13 jump up ^ lih-hsing hsu ; cheng-kuan lin  2008   graph theory and interconnection networks crc press p 66 isbn 978-1-4200-4482-9 14 jump up ^ j flum ; m grohe  2006   parameterized complexity theory springer p 245.isbn 978-3-540-29953-0 15 jump up ^ " full binary tree "  nist 16 jump up ^ richard stanley  enumerative combinatorics  volume 2  p.36 17 jump up ^ " perfect binary tree "  nist 18 ^ jump up to  a b " complete binary tree "  nist 19 jump up ^ aaron m tenenbaum  et al data structures using c  prentice hall  1990 isbn 0-13-199746-7 20 jump up ^ paul e black  ed   entry for data structure in dictionary of algorithms and data structures u.s national institute of standards and technology 15 december 2004.online version accessed 2010-12-19 21 jump up ^ mehta  dinesh ; sartaj sahni  2004   handbook of data structures and applications.chapman and hall isbn 1-58488-435-5 22 jump up ^ d samanta  2004   classic data structures phi learning pvt ltd pp 264 265.isbn 978-81-203-1874-8 23 jump up ^ michael l scott  2009   programming language pragmatics  3rd ed   morgan kaufmann p 347 isbn 978-0-08-092299-7 24 ^ jump up to  a b dung x nguyen  2003   " binary tree structure "  rice.edu retrieved december 28,2010 bibliography  edit  * donald knuth the art of computer programming vol 1 fundamental algorithms  third edition addison-wesley  1997 isbn 0-201-89683-4 section 2.3  especially subsections 2.3.1 2.3.2  pp 318 348   external links  edit  wikimedia commons has media related to binary trees * binary trees entry in the findstat database * gamedev.net introduction on binary trees * binary tree proof by induction * balanced binary search tree on array how to create bottom-up an ahnentafel list  or a balanced binary search tree on array  hide  * v * t * e tree data structures search trees  dynamic sets/associative arrays  * 2 3 * 2 3 4 * aa *  a,b  * avl * b * b + * b * * bx *  optimal  binary search * dancing * htree * interval * order statistic *  left-leaning  red-black * scapegoat * splay * t * treap * ub * weight-balanced heaps * binary * binomial * fibonacci * leftist * pairing * skew * van emde boas tries * hash * radix * suffix * ternary search * x-fast * y-fast spatial data partitioning trees * bk * bsp * cartesian * hilbert r * k-d  implicit k-d  * m * metric * mvp * octree * priority r * quad * r * r + * r * * segment * vp * x other trees * cover * exponential * fenwick * finger * fusion * hash calendar * idistance * k-ary * left-child right-sibling * link/cut * log-structured merge * merkle * pq * range * spqr * top categories  * binary trees * data structures binary search tree from wikipedia  the free encyclopedia binary search tree type tree invented 1960 invented by p.f windley  a.d booth  a.j.t colin  and t.n hibbard time complexity in big o notation average worst case space o  n  o  n  search o  log n  o  n  insert o  log n  o  n  delete o  log n  o  n  a binary search tree of size 9 and depth 3  with 8 at the root the leaves are not drawn in computer science  binary search trees  bst   sometimes called ordered or sorted binary trees  are a particular type ofcontainers  data structures that store " items "  such as numbers  names  etc  in memory they allow fast lookup  addition and removal of items  and can be used to implement either dynamic sets of items  or lookup tables that allow finding an item by its key  e.g  finding the phone number of a person by name   binary search trees keep their keys in sorted order  so that lookup and other operations can use the principle of binary search  when looking for a key in a tree  or a place to insert a new key   they traverse the tree from root to leaf  making comparisons to keys stored in the nodes of the tree and deciding  based on the comparison  to continue searching in the left or right subtrees on average  this means that each comparison allows the operations to skip about half of the tree  so that each lookup  insertion or deletion takes time proportional to the logarithm of the number of items stored in the tree this is much better than the linear timerequired to find items by key in an  unsorted  array  but slower than the corresponding operations on hash tables contents  hide  * 1 definition * 2 operations o 2.1 searching o 2.2 insertion o 2.3 deletion o 2.4 traversal o 2.5 sort o 2.6 verification o 2.7 priority queue operations * 3 types o 3.1 performance comparisons o 3.2 optimal binary search trees * 4 see also * 5 references * 6 further reading * 7 external links definition  edit  a binary search tree is a rooted binary tree  whose internal nodes each store a key  and optionally  an associated value  and each have two distinguished sub-trees  commonly denoted left and right the tree additionally satisfies the binary search tree property  which states that the key in each node must be greater than all keys stored in the left sub-tree  and smaller than all keys in right sub-tree  1   the leaves  final nodes  of the tree contain no key and have no structure to distinguish them from one another leaves are commonly represented by a special leaf or nil symbol  a null pointer  etc  generally  the information represented by each node is a record rather than a single data element however  for sequencing purposes  nodes are compared according to their keys rather than any part of their associated records the major advantage of binary search trees over other data structures is that the related sorting algorithms and search algorithms such as in-order traversal can be very efficient ; they are also easy to code binary search trees are a fundamental data structure used to construct more abstract data structures such as sets  multisets  and associative arrays some of their disadvantages are as follows  * the shape of the binary search tree totally depends on the order of insertions  and it can be degenerated * when inserting or searching for an element in binary search tree  the key of each visited node has to be compared with the key of the element to be inserted or found  i.e  it takes a long time to search an element in a binary search tree * the keys in the binary search tree may be long and the run time may increase * after a long intermixed sequence of random insertion and deletion  the expected height of the tree approaches square root of the number of keys  n  which grows much faster than log n operations  edit  binary search trees support three main operations  insertion of keys  deletion of keys  and lookup  checking whether a key is present   each requires a comparator  a subroutinethat computes the total order  linear order  on any two keys this comparator can be explicitly or implicitly defined  depending on the language in which the binary search tree was implemented a common comparator is the less-than function   dubious discuss  for example  a < b  where a and b are keys of two nodes a and b in a binary search tree searching  edit  searching a binary search tree for a specific key can be a recursive or an iterative process we begin by examining the root node if the tree is null  the key we are searching for does not exist in the tree otherwise  if the key equals that of the root  the search is successful and we return the node if the key is less than that of the root  we search the left subtree similarly  if the key is greater than that of the root  we search the right subtree this process is repeated until the key is found or the remaining subtree is null if the searched key is not found before a null subtree is reached  then the item must not be present in the tree this is easily expressed as a recursive algorithm  function find-recursive  key  node   // call initially with node = root if node = null or node.key = key then return node else if key < node.key then return find-recursive  key  node.left  else return find-recursive  key  node.right  the same algorithm can be implemented iteratively  function find  key  root   current-node  = root while current-node is not null do if current-node.key = key then return current-node else if key < current-node.key then current-node current-node.left else current-node current-node.right return null because in the worst case this algorithm must search from the root of the tree to the leaf farthest from the root  the search operation takes time proportional to the tree 's height  see tree terminology   on average  binary search trees with n nodes have o  log n  height however  in the worst case  binary search trees can have o  n  height  when the unbalanced tree resembles a linked list  degenerate tree   insertion  edit  insertion begins as a search would begin ; if the key is not equal to that of the root  we search the left or right subtrees as before eventually  we will reach an external node and add the new key-value pair  here encoded as a record 'newnode '  as its right or left child  depending on the node 's key in other words  we examine the root and recursively insert the new node to the left subtree if its key is less than that of the root  or the right subtree if its key is greater than or equal to the root here 's how a typical binary search tree insertion might be performed in a binary tree in c + +  void insert  node * & root  int data   if  ! root  root = new node  data  ; else if  data < root > data  insert  root > left  data  ; else if  data > root > data  insert  root > right  data  ;  the above destructive procedural variant modifies the tree in place it uses only constant heap space  and the iterative version uses constant stack space as well   but the prior version of the tree is lost alternatively  as in the following python example  we can reconstruct all ancestors of the inserted node ; any reference to the original tree root remains valid  making the tree a persistent data structure  def binary_tree_insert  node  key  value   if node is none  return treenode  none  key  value  none  if key = = node.key  return treenode  node.left  key  value  node.right  if key < node.key  return treenode  binary_tree_insert  node.left  key  value   node.key  node.value  node.right  else  return treenode  node.left  node.key  node.value  binary_tree_insert  node.right  key  value   the part that is rebuilt uses o  log n  space in the average case and o  n  in the worst case  see big-o notation   in either version  this operation requires time proportional to the height of the tree in the worst case  which is o  log n  time in the average case over all trees  but o  n  time in the worst case another way to explain insertion is that in order to insert a new node in the tree  its key is first compared with that of the root if its key is less than the root 's  it is then compared with the key of the root 's left child if its key is greater  it is compared with the root 's right child this process continues  until the new node is compared with a leaf node  and then it is added as this node 's right or left child  depending on its key there are other ways of inserting nodes into a binary tree  but this is the only way of inserting nodes at the leaves and at the same time preserving the bst structure deletion  edit  there are three possible cases to consider  * deleting a node with no children  simply remove the node from the tree * deleting a node with one child  remove the node and replace it with its child * deleting a node with two children  call the node to be deleted n do not delete n instead  choose either its in-order successor node or its in-order predecessor node  r copy the value of r to n  then recursively call delete on r until reaching one of the first two cases if you choose in-order successor of a node  as right sub tree is not nil  our present case is node has 2 children   then its in-order successor is node with least value in its right sub tree  which will have at a maximum of 1 sub tree  so deleting it would fall in one of first 2 cases broadly speaking  nodes with children are harder to delete as with all binary trees  a node 's in-order successor is its right subtree 's left-most child  and a node 's in-order predecessor is the left subtree 's right-most child in either case  this node will have zero or one children delete it according to one of the two simpler cases above deleting a node with two children from a binary search tree first the rightmost node in the left subtree  the inorder predecessor 6  is identified its value is copied into the node being deleted the inorder predecessor can then be easily deleted because it has at most one child the same method works symmetrically using the inorder successor labelled 9 consistently using the in-order successor or the in-order predecessor for every instance of the two-child case can lead to an unbalanced tree  so some implementations select one or the other at different times runtime analysis  although this operation does not always traverse the tree down to a leaf  this is always a possibility ; thus in the worst case it requires time proportional to the height of the tree it does not require more even when the node has two children  since it still follows a single path and does not visit any node twice def find_min  self   # gets minimum node in a subtree current_node = self while current_node.left_child  current_node = current_node.left_child return current_node def replace_node_in_parent  self  new_value = none   if self.parent  if self = = self.parent.left_child  self.parent.left_child = new_value else  self.parent.right_child = new_value if new_value  new_value.parent = self.parent def binary_tree_delete  self  key   if key < self.key  self.left_child.binary_tree_delete  key  elif key > self.key  self.right_child.binary_tree_delete  key  else  # delete the key here if self.left_child and self.right_child  # if both children are present successor = self.right_child.find_min   self.key = successor.key successor.binary_tree_delete  successor.key  elif self.left_child  # if the node has only a * left * child self.replace_node_in_parent  self.left_child  elif self.right_child  # if the node has only a * right * child self.replace_node_in_parent  self.right_child  else  # this node has no children self.replace_node_in_parent  none  traversal  edit  main article  tree traversal once the binary search tree has been created  its elements can be retrieved in-order by recursively traversing the left subtree of the root node  accessing the node itself  then recursively traversing the right subtree of the node  continuing this pattern with each node in the tree as it 's recursively accessed as with all binary trees  one may conduct a pre-order traversal or a post-order traversal  but neither are likely to be useful for binary search trees an in-order traversal of a binary search tree will always result in a sorted list of node items  numbers  strings or other comparable items   the code for in-order traversal in python is given below it will call callback for every node in the tree def traverse_binary_tree  node  callback   if node is none  return traverse_binary_tree  node.leftchild  callback  callback  node.value  traverse_binary_tree  node.rightchild  callback  traversal requires o  n  time  since it must visit every node this algorithm is also o  n   so it is asymptotically optimal sort  edit  main article  tree sort a binary search tree can be used to implement a simple sorting algorithm similar to heapsort  we insert all the values we wish to sort into a new ordered data structure in this case a binary search tree and then traverse it in order the worst-case time of build_binary_tree is if you feed it a sorted list of values  it chains them into a linked list with no left subtrees for example,build_binary_tree   1  2  3  4  5   yields the tree  1  2  3  4  5       there are several schemes for overcoming this flaw with simple binary trees ; the most common is the self-balancing binary search tree if this same procedure is done using such a tree  the overall worst-case time is o  nlog n   which is asymptotically optimal for a comparison sort in practice  the poor cache performance and added overhead in time and space for a tree-based sort  particularly for node allocation  make it inferior to other asymptotically optimal sorts such as heapsort for static list sorting on the other hand  it is one of the most efficient methods of incremental sorting  adding items to a list over time while keeping the list sorted at all times verification  edit  sometimes we already have a binary tree  and we need to determine whether it is a bst this problem has a simple recursive solution the bst property every node on the right subtree has to be larger than the current node and every node on the left subtree has to be smaller than  or equal to  should not be the case as only unique values should be in the tree  this also poses the question as to if such nodes should be left or right of this parent  the current node is the key to figuring out whether a tree is a bst or not the greedy algorithm simply traverse the tree  at every node check whether the node contains a value larger than the value at the left child and smaller than the value on the right child does not work for all cases consider the following tree  20 / \ 10 30 / \ 5 40 in the tree above  each node meets the condition that the node contains a value larger than its left child and smaller than its right child hold  and yet it is not a bst  the value 5 is on the right subtree of the node containing 20  a violation of the bst property instead of making a decision based solely on the values of a node and its children  we also need information flowing down from the parent as well in the case of the tree above  if we could remember about the node containing the value 20  we would see that the node with value 5 is violating the bst property contract so the condition we need to check at each node is  * if the node is the left child of its parent  then it must be smaller than  or equal to  the parent and it must pass down the value from its parent to its right subtree to make sure none of the nodes in that subtree is greater than the parent * if the node is the right child of its parent  then it must be larger than the parent and it must pass down the value from its parent to its left subtree to make sure none of the nodes in that subtree is lesser than the parent a recursive solution in c + + can explain this further  struct treenode  int data ; treenode * left ; treenode * right ;  ; bool isbst  treenode * node  int mindata  int maxdata   if  node = = null  return true ; if  node > data < mindata | | node > data > maxdata  return false ; return isbst  node > left  mindata  node > data  && isbst  node > right  node > data  maxdata  ;  the initial call to this function can be something like this  if  isbst  root  int_min  int_max    puts  " this is a bst "  ;  else  puts  " this is not a bst ! "  ;  essentially we keep creating a valid range  starting from  min_value  max_value   and keep shrinking it down for each node as we go down recursively priority queue operations  edit  binary search trees can serve as priority queues  structures that allow insertion of arbitrary key as well as lookup and deletion of the minimum  or maximum  key insertion works as previously explained find-min walks the tree  following left pointers as far as it can without hitting a leaf  // precondition  t is not a leaf function find-min  t   while hasleft  t   t left  t  return key  t  find-max is analogous  follow right pointers as far as possible delete-min  max  can simply look up the minimum  maximum   then delete it this way  insertion and deletion both take logarithmic time  just as they do in a binary heap  but unlike a binary heap and most other priority queue implementations  a single tree can support all of find-min  find-max,delete-min and delete-max at the same time  making binary search trees suitable as double-ended priority queues  2   156 types  edit  there are many types of binary search trees avl trees and red-black trees are both forms of self-balancing binary search trees a splay tree is a binary search tree that automatically moves frequently accessed elements nearer to the root in a treap  tree heap   each node also holds a  randomly chosen  priority and the parent node has higher priority than its children tango trees are trees optimized for fast searches two other titles describing binary search trees are that of a complete and degenerate tree a complete binary tree is a binary tree  which is completely filled  with the possible exception of the bottom level  which is filled from left to right in complete binary tree  all nodes are far left as possible it is a tree with n levels  where for each level d < = n  1  the number of existing nodes at level d is equal to 2d this means all possible nodes exist at these levels an additional requirement for a complete binary tree is that for the nth level  while every node does not have to exist  the nodes that do exist must fill from left to right a degenerate tree is a tree where for each parent node  there is only one associated child node it is unbalanced and  in the worst case  performance degrades to that of a linked list if your added node function does not handle re-balancing  then you can easily construct a degenerate tree by feeding it with data that is already sorted what this means is that in a performance measurement  the tree will essentially behave like a linked list data structure performance comparisons  edit  d a heger  2004   3  presented a performance comparison of binary search trees treap was found to have the best average performance  while red-black tree was found to have the smallest amount of performance variations optimal binary search trees  edit  main article  optimal binary search tree tree rotations are very common internal operations in binary trees to keep perfect  or near-to-perfect  internal balance in the tree if we do not plan on modifying a search tree  and we know exactly how often each item will be accessed  we can construct  4  an optimal binary search tree  which is a search tree where the average cost of looking up an item  the expected search cost  is minimized even if we only have estimates of the search costs  such a system can considerably speed up lookups on average for example  if you have a bst of english words used in a spell checker  you might balance the tree based on word frequency intext corpora  placing words like the near the root and words like agerasia near the leaves such a tree might be compared with huffman trees  which similarly seek to place frequently used items near the root in order to produce a dense information encoding ; however  huffman trees store data elements only in leaves  and these elements need not be ordered if we do not know the sequence in which the elements in the tree will be accessed in advance  we can use splay trees which are asymptotically as good as any static search tree we can construct for any particular sequence of lookup operations alphabetic trees are huffman trees with the additional constraint on order  or  equivalently  search trees with the modification that all elements are stored in the leaves faster algorithms exist for optimal alphabetic binary trees  oabts   see also  edit  * search tree * binary search algorithm * randomized binary search tree * tango trees * self-balancing binary search tree * geometry of binary search trees * red-black tree * avl trees * day stout warren algorithm references  edit  1 jump up ^ cormen  thomas h ; leiserson  charles e  rivest  ronald l  stein  clifford  2009   1990   introduction to algorithms  3rd ed   mit press and mcgraw-hill p 287 isbn 0-262-03384-4 2 jump up ^ mehlhorn  kurt ; sanders  peter  2008   algorithms and data structures  the basic toolbox springer 3 jump up ^ heger  dominique a  2004   " a disquisition on the performance behavior of binary search tree data structures "  pdf   european journal for the informatics professional 5  5   67 75 4 jump up ^ gonnet  gaston " optimal binary search trees "  scientific computation eth z rich retrieved 1 december 2013 further reading  edit  * black  paul e " binary search tree "  dictionary of algorithms and data structures nist * cormen  thomas h ; leiserson  charles e ; rivest  ronald l ; stein  clifford  2001   " 12  binary search trees  15.5  optimal binary search trees "  introduction to algorithms  2nd ed   mit press & mcgraw-hill pp 253 272  356 363 isbn 0-262-03293-7 * jarc  duane j  3 december 2005   " binary tree traversals "  interactive data structure visualizations university of maryland * knuth  donald  1997   " 6.2.2  binary tree searching "  the art of computer programming 3  " sorting and searching "  3rd ed   addison-wesley pp 426 458 isbn 0-201-89685-0 * long  sean " binary search tree "  ppt   data structures and algorithms visualization-a powerpoint slides based approach suny oneonta * parlante  nick  2001   " binary trees "  cs education library stanford university external links  edit  * literate implementations of binary search trees in various languages on literateprograms * binary tree visualizer  javascript animation of various bt-based data structures  * kovac  kubo " binary search trees "  java applet   kore ponden n semin r z programovania * madru  justin  18 august 2009   " binary search tree "  jdserver c + + implementation * binary search tree example in python * " references to pointers  c + +  "  msdn microsoft 2005 gives an example binary tree implementation  show  * v * t * e tree data structures  show  * v * t * e data structures categories  * binary trees * data types * search trees avl tree from wikipedia  the free encyclopedia avl tree type tree invented 1962 invented by georgy adelson-velsky and e m landis time complexity in big o notation average worst case space o  n  o  n  search o  log n  o  log n  insert o  log n  o  log n  delete o  log n  o  log n  example avl tree in computer science  an avl tree  georgy adelson-velsky and landis ' tree  named after the inventors  is a self-balancing binary search tree it was the first such data structure to be invented  1  in an avl tree  the heights of the two child subtrees of any node differ by at most one ; if at any time they differ by more than one  rebalancing is done to restore this property lookup  insertion  and deletion all take o  log n  time in both the average and worst cases  where n is the number of nodes in the tree prior to the operation insertions and deletions may require the tree to be rebalanced by one or more tree rotations the avl tree is named after its two soviet inventors  georgy adelson-velsky and e m landis  who published it in their 1962 paper " an algorithm for the organization of information "   2  avl trees are often compared with red-black trees because both support the same set of operations and take o  log n  time for the basic operations for lookup-intensive applications  avl trees are faster than red-black trees because they are more rigidly balanced  3  similar to red-black trees  avl trees are height-balanced both are in general not weight-balanced nor -balanced for any ;  4  that is  sibling nodes can have hugely differing numbers of descendants contents  hide  * 1 operations o 1.1 searching o 1.2 traversal o 1.3 insertion o 1.4 deletion * 2 comparison to other structures * 3 see also * 4 references * 5 further reading * 6 external links operations  edit  tree rotations basic operations of an avl tree involve carrying out the same actions as would be carried out on an unbalanced binary search tree  but modifications are followed by zero or more operations called tree rotations  which help to restore the height balance of the subtrees searching  edit  searching for a specific key in an avl tree can be done the same way as that of a normal unbalanced binary search tree traversal  edit  once a node has been found in a balanced tree  the next or previous nodes can be explored in amortized constant time some instances of exploring these " nearby " nodes require traversing up to log  n  links  particularly when moving from the rightmost leaf of the root 's left subtree to the root or from the root to the leftmost leaf of the root 's right subtree ; in the example avl tree  moving from node 14 to the next but onenode 19 takes 4 steps   however  exploring all n nodes of the tree in this manner would use each link exactly twice  one traversal to enter the subtree rooted at that node  another to leave that node 's subtree after having explored it and since there are n 1 links in any tree  the amortized cost is found to be 2  n 1  /n  or approximately 2 insertion  edit  after inserting a node  it is necessary to check each of the node 's ancestors for consistency with the invariants of avl trees  this is called " retracing "  this is achieved by considering the balance factor of each node  which is defined as follows  balancefactor = height  left subtree   height  right subtree  pictorial description of how rotations rebalance a node in avl tree the numbered circles represent the nodes being rebalanced the lettered triangles represent subtrees which are themselves balanced avl trees a blue number next to a node denotes possible balance factors  those in parentheses occurring only in case of deletion   thus the balance factor of any node of an avl tree is in the integer range  -1  + 1   this balance factor is stored in the node  but may have to be corrected after an insertion or a deletion  which is also done during retracing since with a single insertion the height of an avl subtree can not increase by more than one  the temporarily recomputed balance factor of a node after an insertion will be in the range  2  + 2   for each node checked  if the recomputed balance factor remains in the range from 1 to + 1 then only corrections of the balance factor  but no rotations are necessary however  if the recomputed balance factor becomes less than 1 or greater than + 1  the subtree rooted at this node is unbalanced  and a rotation is needed description of the rotations let us first assume the balance factor of a node p is 2  as opposed to the other possible unbalanced value 2   this case is depicted in the left column of the illustration with p  = 5 we then look at the left subtree  the higher one  with root n if this subtree does not lean to the right  i.e n has balance factor 1  or  when deletion also 0   we can rotate the whole tree to the right to get a balanced tree this is labelled as the " left left case " in the illustration with n  = 4 if the subtree does lean to the right  i.e n  = 3 has balance factor 1  we first rotate the subtree to the left and end up the previous case this second case is labelled as " left right case " in the illustration if the balance factor of the node p is 2  this case is depicted in the right column of the illustration p  = 3  we can mirror the above algorithm i.e if the root n of the  higher  right subtree has balance factor 1  or  when deletion also 0  we can rotate the whole tree to the left to get a balanced tree this is labelled as the " right right case " in the illustration with n  = 4 if the root n  = 5 of the right subtree has balance factor 1  " right left case "  we can rotate the subtree to the right to end up in the " right right case "  the whole retracing loop for an insertion looks like this  // n is the child of p whose height increases by 1 do  if  n = = left_child  p    if  balance_factor  p  = = 1   // the left column in the picture // temporary balance_factor  p  = = 2 = = > rebalancing is required if  balance_factor  n  = = -1   // left right case rotate_left  n  ; // reduce to left left case  // left left case rotate_right  p  ; break ; // leave the loop  if  balance_factor  p  = = -1   balance_factor  p  = 0 ; // n s height increase is absorbed at p break ; // leave the loop  balance_factor  p  = 1 ; // height increases at p  else  // n = = right_child  p   the child whose height increases by 1 if  balance_factor  p  = = -1   // the right column in the picture // temporary balance_factor  p  = = -2 = = > rebalancing is required if  balance_factor  n  = = 1   // right left case rotate_right  n  ; // reduce to right right case  // right right case rotate_left  p  ; break ; // leave the loop  if  balance_factor  p  = = 1   balance_factor  p  = 0 ; // n s height increase is absorbed at p break ; // leave the loop  balance_factor  p  = -1 ; // height increases at p  n = p ; p = parent  n  ;  while  p ! = null  ; // possibly up to the root after a rotation a subtree has the same height as before  so retracing can stop in order to restore the balance factors of all nodes  first observe that all nodes requiring correction lie along the path used during the initial insertion if the above procedure is applied to nodes along this path  starting from the bottom  i.e the inserted node   then every node in the tree will again have a balance factor of 1  0  or 1 the time required is o  log n  for lookup  plus a maximum of o  log n  retracing levels on the way back to the root  so the operation can be completed in o  log n  time deletion  edit  let node x be the node with the value we need to delete  and let node y be a node in the tree we need to find to take node x 's place  and let node z be the actual node we take out of the tree deleting a node with two children from a binary search tree using the in-order predecessor  rightmost node in the left subtree  labelled 6   steps to consider when deleting a node in an avl tree are the following  1 if node x is a leaf or has only one child  skip to step 5 with z  = x 2 otherwise  determine node y by finding the largest  citation needed  node in node x 's left subtree  the in-order predecessor of x it does not have a right child  or the smallest in its right subtree  the in-order successor of x it does not have a left child   3 exchange all the child and parent links of node x with those of node y in this step  the in-order sequence between nodes x and y is temporarily disturbed  but the tree structure does n't change 4 choose node z to be all the child and parent links of old node y = those of new node x 5 if node z has a subtree  which then is a leaf   attach it to z 's parent 6 if node z was the root  its parent is null   update root 7 delete node z 8 retrace the path back up the tree  starting with node z 's parent  to the root  adjusting the balance factors as needed since with a single deletion the height of an avl subtree can not decrease by more than one  the temporary balance factor of a node will be in the range from 2 to + 2 if the balance factor becomes 2 then the subtree is unbalanced and needs to be rotated the various cases of rotations are depicted in section " insertion "  the whole retracing loop for a deletion looks like this  // n is the child of p whose height decreases by 1 do  if  n = = right_child  p    if  balance_factor  p  = = 1   // the left column in the picture // temporary balance_factor  p  = = 2 = = > rebalancing is required s = left_child  p  ; // sibling of n b = balance_factor  s  ; if  b = = -1   // left right case rotate_left  s  ; // reduce to left left case  // left left case rotate_right  p  ; if  b = = 0  //  in the picture the small blue  0  at node 4  break ; // height does not change  leave the loop  if  balance_factor  p  = = 0   balance_factor  p  = 1 ; // n s height decrease is absorbed at p break ; // leave the loop  balance_factor  p  = 0 ; // height decreases at p  else  // n = = left_child  p   the child whose height decreases by 1 if  balance_factor  p  = = -1   // the right column in the picture // temporary balance_factor  p  = = -2 = = > rebalancing is required s = right_child  p  ; // sibling of n b = balance_factor  s  ; if  b = = 1   // right left case rotate_right  s  ; // reduce to right right case  // right right case rotate_left  p  ; if  b = = 0  //  in the picture the small blue  0  at node 4  break ; // height does not change  leave the loop  if  balance_factor  p  = = 0   balance_factor  p  = -1 ; // n s height decrease is absorbed at p break ; // leave the loop  balance_factor  p  = 0 ; // height decreases at p  n = p ; p = parent  n  ;  while  p ! = null  ; // possibly up to the root the retracing can stop if the balance factor becomes 1 indicating that the height of that subtree has remained unchanged this can also result from a rotation when the higher child tree has a balance factor of 0 if the balance factor becomes 0 then the height of the subtree has decreased by one and the retracing needs to continue this can also result from a rotation the time required is o  log n  for lookup  plus a maximum of o  log n  retracing levels on the way back to the root  so the operation can be completed in o  log n  time comparison to other structures  edit  both avl trees and red-black trees are self-balancing binary search trees and they are very similar mathematically  5  the operations to balance the trees are different  but both occur on the average in o  1  with maximum in o  log n   the real difference between the two is the limiting height for a tree of size  * an avl tree 's height is strictly less than   6   7  where is the golden ratio * a red-black tree 's height is at most  8  avl trees are more rigidly balanced than red-black trees  leading to slower insertion and removal but faster retrieval see also  edit  * trees * tree rotation * red-black tree * splay tree * scapegoat tree * b-tree * t-tree * list of data structures references  edit  1 jump up ^ robert sedgewick  algorithms  addison-wesley  1983  isbn 0-201-06672-6  page 199  chapter 15  balanced trees 2 jump up ^ georgy adelson-velsky  g ; e m landis  1962   " an algorithm for the organization of information "  proceedings of the ussr academy of sciences  in russian  146  263 266 english translation by myron j ricci in soviet math doklady  3  1259 1263  1962 3 jump up ^ pfaff  ben  june 2004   " performance analysis of bsts in system software "  pdf   stanford university 4 jump up ^ avl trees are not weight-balanced  meaning  avl trees are not -balanced  thereby  a binary tree is called -balanced  with  if for every node  the inequality holds and is minimal with this property is the number of nodes below the tree with as root  including the root  and is the left child node of  5 jump up ^ in fact  each avl tree can be colored red-black 6 jump up ^ burkhard  walt  spring 2012   " avl dictionary data type implementation "  advanced data structures  pdf   la jolla  a.s soft reserves  uc san diego p 103 7 jump up ^ knuth  donald e  2000   sorting and searching  2 ed  6 printing  newly updated and rev ed   boston  u.a   addison-wesley p 460 isbn 0-201-89685-0 8 jump up ^ proof of asymptotic bounds further reading  edit  * donald knuth the art of computer programming  volume 3  sorting and searching  third edition addison-wesley  1997 isbn 0-201-89685-0 pages 458 475 of section 6.2.3  balanced trees external links  edit  the wikibook algorithm implementation has a page on the topic of  avl tree wikimedia commons has media related to avl-trees * xdg library by dmitriy vilkov  serializable straight c-implementation could easily be taken from this library under gnu-lgpl and afl v2.0 licenses * description from the dictionary of algorithms and data structures * python implementation * single c header file by ian piumarta * avl tree demonstration * avl tree applet all operations * fast and efficient implementation of avl trees * php implementation * avl threaded tree php implementation * c + + implementation which can be used as an array * self balancing avl tree with concat and split operations  show  * v * t * e tree data structures  show  * v * t * e data structures categories  * 1962 in computer science * binary trees * soviet inventions * search trees 2 3 4 tree from wikipedia  the free encyclopedia in computer science  a 2 3 4 tree  also called a 2 4 tree  is a self-balancing data structure that is commonly used to implement dictionaries  citation needed  the numbers mean atree where every node with children  internal node  has either two  three  or four child nodes  * a 2-node has one data element  and if internal has two child nodes ; * a 3-node has two data elements  and if internal has three child nodes ; * a 4-node has three data elements  and if internal has four child nodes * 2-node * 3-node * 4-node 2 3 4 trees are b-trees of order 4 ;  1  like b-trees in general  they can search  insert and delete in o  log n  time one property of a 2 3 4 tree is that all external nodes are at the same depth 2 3 4 trees are an isometry of red black trees  meaning that they are equivalent data structures in other words  for every 2 3 4 tree  there exists at least one red black tree with data elements in the same order moreover  insertion and deletion operations on 2 3 4 trees that cause node expansions  splits and merges are equivalent to the color-flipping and rotations in red black trees introductions to red black trees usually introduce 2 3 4 trees first  because they are conceptually simpler 2 3 4 trees  however  can be difficult to implement in most programming languages because of the large number of special cases involved in operations on the tree red black trees are simpler to implement   2  so tend to be used instead contents  hide  * 1 properties * 2 insertion o 2.1 example * 3 deletion * 4 see also * 5 references * 6 external links properties  edit  * every node  leaf or internal  is a 2-node  3-node or a 4-node  and holds one  two  or three data elements  respectively * all leaves are at the same depth  the bottom level   * all data is kept in sorted order insertion  edit  to insert a value  we start at the root of the 2 3 4 tree   1 if the current node is a 4-node  * remove and save the middle value to get a 3-node * split the remaining 3-node up into a pair of 2-nodes  the now missing middle value is handled in the next step   * if this is the root node  which thus has no parent   * the middle value becomes the new root 2-node and the tree height increases by 1 ascend into the root * otherwise  push the middle value up into the parent node ascend into the parent node 2 find the child whose interval contains the value to be inserted 3 if that child is a leaf  insert the value into the child node and finish * otherwise  descend into the child and repeat from step 1  3   4  example  edit  to insert the value " 25 " into this 2 3 4 tree  * begin at the root  10  20  and descend towards the rightmost child  22  24  29    its interval  20   contains 25  * node  22  24  29  is a 4-node  so its middle element 24 is pushed up into the parent node * the remaining 3-node  22  29  is split into a pair of 2-nodes  22  and  29   ascend back into the new parent  10  20  24   * descend towards the rightmost child  29    its interval  24  29  contains 25  * node  29  has no leftmost child  the child for interval  24  29  is empty  stop here and insert value 25 into this node deletion  edit  consider just leaving the element there  marking it deleted  possibly to be re-used for a future insertion to remove a value from the 2 3 4 tree  1 find the element to be deleted * if the element is not in a leaf node  remember its location and continue searching until a leaf  which will contain the element s successor  is reached the successor can be either the largest key that is smaller than the one to be removed  or the smallest key that is larger than the one to be removed it is simplest to make adjustments to the tree from the top down such that the leaf node found is not a 2-node that way  after the swap  there will not be an empty leaf node * if the element is in a 2-node leaf  just make the adjustments below make the following adjustments when a 2-node except the root node is encountered on the way to the leaf we want to remove  1 if a sibling on either side of this node is a 3-node or a 4-node  thus having more than 1 key   perform a rotation with that sibling  * the key from the other sibling closest to this node moves up to the parent key that overlooks the two nodes * the parent key moves down to this node to form a 3-node * the child that was originally with the rotated sibling key is now this node 's additional child 2 if the parent is a 2-node and the sibling is also a 2-node  combine all three elements to form a new 4-node and shorten the tree  this rule can only trigger if the parent 2-node is the root  since all other 2-nodes along the way will have been modified to not be 2-nodes this is why " shorten the tree " here preserves balance ; this is also an important assumption for the fusion operation  3 if the parent is a 3-node or a 4-node and all adjacent siblings are 2-nodes  do a fusion operation with the parent and an adjacent sibling  * the adjacent sibling and the parent key overlooking the two sibling nodes come together to form a 4-node * transfer the sibling 's children to this node once the sought value is reached  it can now be placed at the removed entry 's location without a problem because we have ensured that the leaf node has more than 1 key deletion in a 2 3 4 tree is o  log n   assuming transfer and fusion run in constant time  o  1     3   5  see also  edit  computer programming portal * 2 3 tree * red black tree * b-tree references  edit  1 jump up ^ knuth  donald  1998   sorting and searching the art of computer programming volume 3  second ed   addison wesley isbn 0-201-89685-0 section 6.2.4  multiway trees  pp 481 491 also  pp 476 477 of section 6.2.3  balanced trees  discusses 2-3 trees 2 jump up ^ sedgewick  robert  2008   " left-leaning red-black trees "  pdf   left-leaning red-black trees department of computer science  purdue university 3 ^ jump up to  a b ford  william ; topp  william  2002   data structures with c + + using stl  2nd ed   new jersey  prentice hall  p 683  isbn 0-13-085850-1 4 jump up ^ goodrich  michael t ; tamassia  roberto ; mount  david m  2002   data structures and algorithms in c + +  wiley  isbn 0-471-20208-8 5 jump up ^ grama  ananth  2004   "  2,4  trees "  pdf   cs251  data structures lecture notes department of computer science  purdue university retrieved 2008-04-10 external links  edit  wikimedia commons has media related to 2-3-4-trees * algorithms in action  with 2 3 4 tree animation * animation of a 2 3 4 tree * java applet showing a 2 3 4 tree * left-leaning red black trees princeton university  2008 * open data structures section 9.1 2 4 trees  hide  * v * t * e tree data structures search trees  dynamic sets/associative arrays  * 2 3 * 2 3 4 * aa *  a,b  * avl * b * b + * b * * bx *  optimal  binary search * dancing * htree * interval * order statistic *  left-leaning  red-black * scapegoat * splay * t * treap * ub * weight-balanced heaps * binary * binomial * fibonacci * leftist * pairing * skew * van emde boas tries * hash * radix * suffix * ternary search * x-fast * y-fast spatial data partitioning trees * bk * bsp * cartesian * hilbert r * k-d  implicit k-d  * m * metric * mvp * octree * priority r * quad * r * r + * r * * segment * vp * x other trees * cover * exponential * fenwick * finger * fusion * hash calendar * idistance * k-ary * left-child right-sibling * link/cut * log-structured merge * merkle * pq * range * spqr * top categories  * b-tree b-tree from wikipedia  the free encyclopedia not to be confused with binary tree b-tree type tree invented 1972 invented by rudolf bayer  edward m mccreight time complexity in big o notation average worst case space o  n  o  n  search o  log n  o  log n  insert o  log n  o  log n  delete o  log n  o  log n  in computer science  a b-tree is a tree data structure that keeps data sorted and allows searches  sequential access  insertions  and deletions in logarithmic time the b-tree is a generalization of a binary search tree in that a node can have more than two children  comer 1979  p 123   unlike self-balancing binary search trees  the b-tree is optimized for systems that read and write large blocks of data b-trees are a good example of a data structure for external memory it is commonly used in databases and filesystems contents  hide  * 1 overview o 1.1 variants o 1.2 etymology * 2 the database problem o 2.1 time to search a sorted file o 2.2 an index speeds the search o 2.3 insertions and deletions cause trouble o 2.4 the b-tree uses all those ideas * 3 technical description o 3.1 terminology o 3.2 definition * 4 best case and worst case heights * 5 algorithms o 5.1 search o 5.2 insertion o 5.3 deletion * 5.3.1 deletion from a leaf node * 5.3.2 deletion from an internal node * 5.3.3 rebalancing after deletion o 5.4 sequential access o 5.5 initial construction * 6 in filesystems * 7 variations o 7.1 access concurrency * 8 see also * 9 notes * 10 references o 10.1 original papers * 11 external links overview  edit  a b-tree of order 2  bayer & mccreight 1972  or order 5  knuth 1998   in b-trees  internal  non-leaf  nodes can have a variable number of child nodes within some pre-defined range when data is inserted or removed from a node  its number of child nodes changes in order to maintain the pre-defined range  internal nodes may be joined or split because a range of child nodes is permitted  b-trees do not need re-balancing as frequently as other self-balancing search trees  but may waste some space  since nodes are not entirely full the lower and upper bounds on the number of child nodes are typically fixed for a particular implementation for example  in a 2-3 b-tree  often simply referred to as a 2-3 tree   each internal node may have only 2 or 3 child nodes each internal node of a b-tree will contain a number of keys the keys act as separation values which divide its subtrees for example  if an internal node has 3 child nodes  or subtrees  then it must have 2 keys  a1 and a2 all values in the leftmost subtree will be less than a1  all values in the middle subtree will be between a1 and a2  and all values in the rightmost subtree will be greater than a2 usually  the number of keys is chosen to vary between and  where is the minimum number of keys  and is the minimum degree or branching factor of the tree in practice  the keys take up the most space in a node the factor of 2 will guarantee that nodes can be split or combined if an internal node has keys  then adding a key to that node can be accomplished by splitting the key node into two key nodes and adding the key to the parent node each split node has the required minimum number of keys similarly  if an internal node and its neighbor each have keys  then a key may be deleted from the internal node by combining with its neighbor deleting the key would make the internal node have keys ; joining the neighbor would add keys plus one more key brought down from the neighbor 's parent the result is an entirely full node of keys the number of branches  or child nodes  from a node will be one more than the number of keys stored in the node in a 2-3 b-tree  the internal nodes will store either one key  with two child nodes  or two keys  with three child nodes   a b-tree is sometimes described with the parameters or simply with the highest branching order   a b-tree is kept balanced by requiring that all leaf nodes be at the same depth this depth will increase slowly as elements are added to the tree  but an increase in the overall depth is infrequent  and results in all leaf nodes being one more node farther away from the root b-trees have substantial advantages over alternative implementations when the time to access the data of a node greatly exceeds the time spent processing that data  because then the cost of accessing the node may be amortized over multiple operations within the node this usually occurs when the node data are in secondary storage such as disk drives by maximizing the number of keys within each internal node  the height of the tree decreases and the number of expensive node accesses is reduced in addition  rebalancing of the tree occurs less often the maximum number of child nodes depends on the information that must be stored for each child node and the size of a full disk block or an analogous size in secondary storage while 2-3 b-trees are easier to explain  practical b-trees using secondary storage need a large number of child nodes to improve performance variants  edit  the term b-tree may refer to a specific design or it may refer to a general class of designs in the narrow sense  a b-tree stores keys in its internal nodes but need not store those keys in the records at the leaves the general class includes variations such as the b + tree and the b *  * in the b + tree  copies of the keys are stored in the internal nodes ; the keys and records are stored in leaves ; in addition  a leaf node may include a pointer to the next leaf node to speed sequential access  comer 1979  p 129   * the b * -tree balances more neighboring internal nodes to keep the internal nodes more densely packed  comer 1979  p 129   this variant requires non-root nodes to be at least 2/3 full instead of 1/2  knuth 1998  p 488   to maintain this  instead of immediately splitting up a node when it gets full  its keys are shared with a node next to it when both nodes are full  then the two nodes are split into three deleting nodes is somewhat more complex than inserting however * b-trees can be turned into order statistic trees to allow rapid searches for the nth record in key order  or counting the number of records between any two records  and various other related operations  1  etymology  edit  rudolf bayer and ed mccreight invented the b-tree while working at boeing research labs in 1971  bayer & mccreight 1972   but they did not explain what  if anything  the bstands for douglas comer explains  the origin of " b-tree " has never been explained by the authors as we shall see  " balanced  " " broad  " or " bushy " might apply others suggest that the " b " stands for boeing because of his contributions  however  it seems appropriate to think of b-trees as " bayer " -trees  comer 1979  p 123 footnote 1  donald knuth speculates on the etymology of b-trees in his may  1980 lecture on the topic " cs144c classroom lecture about disk storage and b-trees "  suggesting the " b " may have originated from boeing or from bayer 's name  2  after a talk at cpm 2013  24th annual symposium on combinatorial pattern matching  bad herrenalb  germany  june 17 19  2013   ed mccreight answered a question on b-tree 's name by martin farach-colton saying  " bayer and i were in a lunch time where we get to think a name and we were  so  b  we were thinking b is  you know we were working for boeing at the time  we could n't use the name without talking to lawyers so  there is a b it has to do with balance  another b bayer was the senior author  who did have several years older than i am and had many more publications than i did so there is another b and so  at the lunch table we never did resolve whether there was one of those that made more sense than the rest what really lives to say is  the more you think about what the b in b-trees means  the better you understand b-trees "  3  the database problem  edit  this section describes a problem faced by database designers  outlines a series of increasingly effective solutions to the problem  and ends by describing how the b-tree solves the problem completely time to search a sorted file  edit  usually  sorting and searching algorithms have been characterized by the number of comparison operations that must be performed using order notation a binary search of a sorted table with records  for example  can be done in roughly comparisons if the table had 1,000,000 records  then a specific record could be located with at most 20 comparisons   large databases have historically been kept on disk drives the time to read a record on a disk drive far exceeds the time needed to compare keys once the record is available the time to read a record from a disk drive involves a seek time and a rotational delay the seek time may be 0 to 20 or more milliseconds  and the rotational delay averages about half the rotation period for a 7200 rpm drive  the rotation period is 8.33 milliseconds for a drive such as the seagate st3500320ns  the track-to-track seek time is 0.8 milliseconds and the average reading seek time is 8.5 milliseconds  4  for simplicity  assume reading from disk takes about 10 milliseconds naively  then  the time to locate one record out of a million would take 20 disk reads times 10 milliseconds per disk read  which is 0.2 seconds the time wo n't be that bad because individual records are grouped together in a disk block a disk block might be 16 kilobytes if each record is 160 bytes  then 100 records could be stored in each block the disk read time above was actually for an entire block once the disk head is in position  one or more disk blocks can be read with little delay with 100 records per block  the last 6 or so comparisons do n't need to do any disk reads the comparisons are all within the last disk block read to speed the search further  the first 13 to 14 comparisons  which each required a disk access  must be sped up an index speeds the search  edit  a significant improvement can be made with an index in the example above  initial disk reads narrowed the search range by a factor of two that can be improved substantially by creating an auxiliary index that contains the first record in each disk block  sometimes called a sparse index   this auxiliary index would be 1 % of the size of the original database  but it can be searched more quickly finding an entry in the auxiliary index would tell us which block to search in the main database ; after searching the auxiliary index  we would have to search only that one block of the main database at a cost of one more disk read the index would hold 10,000 entries  so it would take at most 14 comparisons like the main database  the last 6 or so comparisons in the aux index would be on the same disk block the index could be searched in about 8 disk reads  and the desired record could be accessed in 9 disk reads the trick of creating an auxiliary index can be repeated to make an auxiliary index to the auxiliary index that would make an aux-aux index that would need only 100 entries and would fit in one disk block instead of reading 14 disk blocks to find the desired record  we only need to read 3 blocks reading and searching the first  and only  block of the aux-aux index identifies the relevant block in aux-index reading and searching that aux-index block identifies the relevant block in the main database instead of 150 milliseconds  we need only 30 milliseconds to get the record the auxiliary indices have turned the search problem from a binary search requiring roughly disk reads to one requiring only disk reads where is the blocking factor  the number of entries per block  entries per block ; reads   in practice  if the main database is being frequently searched  the aux-aux index and much of the aux index may reside in a disk cache  so they would not incur a disk read insertions and deletions cause trouble  edit  if the database does not change  then compiling the index is simple to do  and the index need never be changed if there are changes  then managing the database and its index becomes more complicated deleting records from a database does n't cause much trouble the index can stay the same  and the record can just be marked as deleted the database stays in sorted order if there is a large number of deletions  then the searching and storage become less efficient insertions can be very slow in a sorted sequential file because room for the inserted record must be made inserting a record before the first record in the file requires shifting all of the records down one such an operation is just too expensive to be practical a trick is to leave some space lying around to be used for insertions instead of densely storing all the records in a block  the block can have some free space to allow for subsequent insertions those records would be marked as if they were " deleted " records both insertions and deletions are fast as long as space is available on a block if an insertion wo n't fit on the block  then some free space on some nearby block must be found and the auxiliary indices adjusted the hope is that enough space is nearby such that a lot of blocks do not need to be reorganized alternatively  some out-of-sequence disk blocks may be used the b-tree uses all those ideas  edit  the b-tree uses all of the ideas described above in particular  a b-tree  * keeps keys in sorted order for sequential traversing * uses a hierarchical index to minimize the number of disk reads * uses partially full blocks to speed insertions and deletions * keeps the index balanced with an elegant recursive algorithm in addition  a b-tree minimizes waste by making sure the interior nodes are at least half full a b-tree can handle an arbitrary number of insertions and deletions technical description  edit  terminology  edit  unfortunately  the literature on b-trees is not uniform in its terminology  folk & zoellick 1992  p 362   bayer & mccreight  1972   comer  1979   and others define the order of b-tree as the minimum number of keys in a non-root node folk & zoellick  1992  points out that terminology is ambiguous because the maximum number of keys is not clear an order 3 b-tree might hold a maximum of 6 keys or a maximum of 7 keys knuth  1998  p 483  avoids the problem by defining the order to be maximum number of children  which is one more than the maximum number of keys   the term leaf is also inconsistent bayer & mccreight  1972  considered the leaf level to be the lowest level of keys  but knuth considered the leaf level to be one level below the lowest keys  folk & zoellick 1992  p 363   there are many possible implementation choices in some designs  the leaves may hold the entire data record ; in other designs  the leaves may only hold pointers to the data record those choices are not fundamental to the idea of a b-tree  5  there are also unfortunate choices like using the variable k to represent the number of children when k could be confused with the number of keys for simplicity  most authors assume there are a fixed number of keys that fit in a node the basic assumption is the key size is fixed and the node size is fixed in practice  variable length keys may be employed  folk & zoellick 1992  p 379   definition  edit  according to knuth 's definition  a b-tree of order m is a tree which satisfies the following properties  1 every node has at most m children 2 every non-leaf node  except root  has at least m 2 children 3 the root has at least two children if it is not a leaf node 4 a non-leaf node with k children contains k 1 keys 5 all leaves appear in the same level each internal node s keys act as separation values which divide its subtrees for example  if an internal node has 3 child nodes  or subtrees  then it must have 2 keys  a1 and a2 all values in the leftmost subtree will be less than a1  all values in the middle subtree will be between a1 and a2  and all values in the rightmost subtree will be greater than a2 internal nodes internal nodes are all nodes except for leaf nodes and the root node they are usually represented as an ordered set of elements and child pointers every internal node contains a maximum of u children and a minimum of l children thus  the number of elements is always 1 less than the number of child pointers  the number of elements is between l 1 and u 1   u must be either 2l or 2l 1 ; therefore each internal node is at least half full the relationship between u and l implies that two half-full nodes can be joined to make a legal node  and one full node can be split into two legal nodes  if there s room to push one element up into the parent   these properties make it possible to delete and insert new values into a b-tree and adjust the tree to preserve the b-tree properties the root node the root node s number of children has the same upper limit as internal nodes  but has no lower limit for example  when there are fewer than l 1 elements in the entire tree  the root will be the only node in the tree  with no children at all leaf nodes leaf nodes have the same restriction on the number of elements  but have no children  and no child pointers a b-tree of depth n + 1 can hold about u times as many items as a b-tree of depth n  but the cost of search  insert  and delete operations grows with the depth of the tree as with any balanced tree  the cost grows much more slowly than the number of elements some balanced trees store values only at leaf nodes  and use different kinds of nodes for leaf nodes and internal nodes b-trees keep values in every node in the tree  and may use the same structure for all nodes however  since leaf nodes never have children  the b-trees benefit from improved performance if they use a specialized structure best case and worst case heights  edit  let h be the height of the classic b-tree let n > 0 be the number of entries in the tree  6  let m be the maximum number of children a node can have each node can have at most m 1 keys it can be shown  by induction for example  that a b-tree of height h with all its nodes completely filled has n = mh 1 entries hence  the best case height of a b-tree is  let d be the minimum number of children an internal  non-root  node can have for an ordinary b-tree  d = m/2  comer  1979  p 127  and cormen et al  2001  pp 383 384  give the worst case height of a b-tree  where the root node is considered to have height 0  as algorithms  edit  this article may be confusing or unclear to readers in particular  the discussion below uses " element "  " value "  " key "  " separator "  and " separation value " to mean essentially the same thing the terms are not clearly defined there are some subtle issues at the root and leaves please help us clarify the article ; suggestions may be found on the talk page  february 2012  search  edit  searching is similar to searching a binary search tree starting at the root  the tree is recursively traversed from top to bottom at each level  the search chooses the child pointer  subtree  whose separation values are on either side of the search value binary search is typically  but not necessarily  used within nodes to find the separation values and child tree of interest insertion  edit  a b tree insertion example with each iteration the nodes of this b tree have at most 3 children  knuth order 3   all insertions start at a leaf node to insert a new element  search the tree to find the leaf node where the new element should be added insert the new element into that node with the following steps  1 if the node contains fewer than the maximum legal number of elements  then there is room for the new element insert the new element in the node  keeping the node 's elements ordered 2 otherwise the node is full  evenly split it into two nodes so  1 a single median is chosen from among the leaf 's elements and the new element 2 values less than the median are put in the new left node and values greater than the median are put in the new right node  with the median acting as a separation value 3 the separation value is inserted in the node 's parent  which may cause it to be split  and so on if the node has no parent  i.e  the node was the root   create a new root above this node  increasing the height of the tree   if the splitting goes all the way up to the root  it creates a new root with a single separator value and two children  which is why the lower bound on the size of internal nodes does not apply to the root the maximum number of elements per node is u 1 when a node is split  one element moves to the parent  but one element is added so  it must be possible to divide the maximum number u 1 of elements into two legal nodes if this number is odd  then u = 2l and one of the new nodes contains  u 2  /2 = l 1 elements  and hence is a legal node  and the other contains one more element  and hence it is legal too if u 1 is even  then u = 2l 1  so there are 2l 2 elements in the node half of this number is l 1  which is the minimum number of elements allowed per node an improved algorithm  mond & raz 1985  supports a single pass down the tree from the root to the node where the insertion will take place  splitting any full nodes encountered on the way this prevents the need to recall the parent nodes into memory  which may be expensive if the nodes are on secondary storage however  to use this improved algorithm  we must be able to send one element to the parent and split the remaining u 2 elements into two legal nodes  without adding a new element this requires u = 2l rather than u = 2l 1  which accounts for why some textbooks impose this requirement in defining b-trees deletion  edit  there are two popular strategies for deletion from a b-tree 1 locate and delete the item  then restructure the tree to regain its invariants  or 2 do a single pass down the tree  but before entering  visiting  a node  restructure the tree so that once the key to be deleted is encountered  it can be deleted without triggering the need for any further restructuring the algorithm below uses the former strategy there are two special cases to consider when deleting an element  1 the element in an internal node is a separator for its child nodes 2 deleting an element may put its node under the minimum number of elements and children the procedures for these cases are in order below deletion from a leaf node  edit  1 search for the value to delete 2 if the value is in a leaf node  simply delete it from the node 3 if underflow happens  rebalance the tree as described in section " rebalancing after deletion " below deletion from an internal node  edit  each element in an internal node acts as a separation value for two subtrees  therefore we need to find a replacement for separation note that the largest element in the left subtree is still less than the separator likewise  the smallest element in the right subtree is still greater than the separator both of those elements are in leaf nodes  and either one can be the new separator for the two subtrees algorithmically described below  1 choose a new separator  either the largest element in the left subtree or the smallest element in the right subtree   remove it from the leaf node it is in  and replace the element to be deleted with the new separator 2 the previous step deleted an element  the new separator  from a leaf node if that leaf node is now deficient  has fewer than the required number of nodes   then rebalance the tree starting from the leaf node rebalancing after deletion  edit  rebalancing starts from a leaf and proceeds toward the root until the tree is balanced if deleting an element from a node has brought it under the minimum size  then some elements must be redistributed to bring all nodes up to the minimum usually  the redistribution involves moving an element from a sibling node that has more than the minimum number of nodes that redistribution operation is called a rotation if no sibling can spare an element  then the deficient node must be merged with a sibling the merge causes the parent to lose a separator element  so the parent may become deficient and need rebalancing the merging and rebalancing may continue all the way to the root since the minimum element count does n't apply to the root  making the root be the only deficient node is not a problem the algorithm to rebalance the tree is as follows   citation needed  * if the deficient node 's right sibling exists and has more than the minimum number of elements  then rotate left 1 copy the separator from the parent to the end of the deficient node  the separator moves down ; the deficient node now has the minimum number of elements  2 replace the separator in the parent with the first element of the right sibling  right sibling loses one node but still has at least the minimum number of elements  3 the tree is now balanced * otherwise  if the deficient node 's left sibling exists and has more than the minimum number of elements  then rotate right 1 copy the separator from the parent to the start of the deficient node  the separator moves down ; deficient node now has the minimum number of elements  2 replace the separator in the parent with the last element of the left sibling  left sibling loses one node but still has at least the minimum number of elements  3 the tree is now balanced * otherwise  if both immediate siblings have only the minimum number of elements  then merge with a sibling sandwiching their separator taken off from their parent 1 copy the separator to the end of the left node  the left node may be the deficient node or it may be the sibling with the minimum number of elements  2 move all elements from the right node to the left node  the left node now has the maximum number of elements  and the right node empty  3 remove the separator from the parent along with its empty right child  the parent loses an element  * if the parent is the root and now has no elements  then free it and make the merged node the new root  tree becomes shallower  * otherwise  if the parent has fewer than the required number of elements  then rebalance the parent note  the rebalancing operations are different for b + trees  e.g  rotation is different because parent has copy of the key  and b * -tree  e.g  three siblings are merged into two siblings   sequential access  edit  while freshly loaded databases tend to have good sequential behavior  this behavior becomes increasingly difficult to maintain as a database grows  resulting in more random i/o and performance challenges  7  initial construction  edit  in applications  it is frequently useful to build a b-tree to represent a large existing collection of data and then update it incrementally using standard b-tree operations in this case  the most efficient way to construct the initial b-tree is not to insert every element in the initial collection successively  but instead to construct the initial set of leaf nodes directly from the input  then build the internal nodes from these this approach to b-tree construction is called bulkloading initially  every leaf but the last one has one extra element  which will be used to build the internal nodes  citation needed  for example  if the leaf nodes have maximum size 4 and the initial collection is the integers 1 through 24  we would initially construct 4 leaf nodes containing 5 values each and 1 which contains 4 values  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 we build the next level up from the leaves by taking the last element from each leaf node except the last one again  each node except the last will contain one extra value in the example  suppose the internal nodes contain at most 2 values  3 child pointers   then the next level up of internal nodes would be  5 10 15 20 1 2 3 4 6 7 8 9 11 12 13 14 16 17 18 19 21 22 23 24 this process is continued until we reach a level with only one node and it is not overfilled in the example only the root level remains  15 5 10 20 1 2 3 4 6 7 8 9 11 12 13 14 16 17 18 19 21 22 23 24 in filesystems  edit  in addition to its use in databases  the b-tree is also used in filesystems to allow quick random access to an arbitrary block in a particular file the basic problem is turning the file block address into a disk block  or perhaps to a cylinder-head-sector  address some operating systems require the user to allocate the maximum size of the file when the file is created the file can then be allocated as contiguous disk blocks converting to a disk block  the operating system just adds the file block address to the starting disk block of the file the scheme is simple  but the file can not exceed its created size other operating systems allow a file to grow the resulting disk blocks may not be contiguous  so mapping logical blocks to physical blocks is more involved ms-dos  for example  used a simple file allocation table  fat   the fat has an entry for each disk block   note 1  and that entry identifies whether its block is used by a file and if so  which block  if any  is the next disk block of the same file so  the allocation of each file is represented as a linked list in the table in order to find the disk address of file block  the operating system  or disk utility  must sequentially follow the file 's linked list in the fat worse  to find a free disk block  it must sequentially scan the fat for ms-dos  that was not a huge penalty because the disks and files were small and the fat had few entries and relatively short file chains in the fat12 filesystem  used on floppy disks and early hard disks   there were no more than 4,080  note 2  entries  and the fat would usually be resident in memory as disks got bigger  the fat architecture began to confront penalties on a large disk using fat  it may be necessary to perform disk reads to learn the disk location of a file block to be read or written tops-20  and possibly tenex  used a 0 to 2 level tree that has similarities to a b-tree  citation needed   a disk block was 512 36-bit words if the file fit in a 512  29  word block  then the file directory would point to that physical disk block if the file fit in 218 words  then the directory would point to an aux index ; the 512 words of that index would either be null  the block is n't allocated  or point to the physical address of the block if the file fit in 227 words  then the directory would point to a block holding an aux-aux index ; each entry would either be null or point to an aux index consequently  the physical disk block for a 227 word file could be located in two disk reads and read on the third apple 's filesystem hfs +  microsoft 's ntfs   8  aix  jfs2  and some linux filesystems  such as btrfs and ext4  use b-trees b * -trees are used in the hfs and reiser4 file systems variations  edit  access concurrency  edit  lehman and yao  9  showed that all read locks could be avoided  and thus concurrent access greatly improved  by linking the tree blocks at each level together with a " next " pointer this results in a tree structure where both insertion and search operations descend from the root to the leaf write locks are only required as a tree block is modified this maximizes access concurrency by multiple users  an important consideration for databases and/or other b-tree based isam storage methods the cost associated with this improvement is that empty pages can not be removed from the btree during normal operations  however  see  10  for various strategies to implement node merging  and source code at  11   united states patent 5283894  granted in 1994  appears to show a way to use a 'meta access method '  12  to allow concurrent b + tree access and modification without locks the technique accesses the tree 'upwards ' for both searches and updates by means of additional in-memory indexes that point at the blocks in each level in the block cache no reorganization for deletes is needed and there are no 'next ' pointers in each block as in lehman and yao see also  edit  * r-tree * 2 3 tree * 2 3 4 tree notes  edit  1 jump up ^ for fat  what is called a " disk block " here is what the fat documentation calls a " cluster "  which is fixed-size group of one or more contiguous whole physical disk sectors for the purposes of this discussion  a cluster has no significant difference from a physical sector 2 jump up ^ two of these were reserved for special purposes  so only 4078 could actually represent disk blocks  clusters   references  edit  1 jump up ^ counted b-trees  retrieved 2010-01-25 2 jump up ^ knuth 's video lectures from stanford 3 jump up ^ talk 's video  retrieved 2014-01-17 4 jump up ^ seagate technology llc  product manual  barracuda es.2 serial ata  rev f  publication 100468393  2008  1   page 6 5 jump up ^ bayer & mccreight  1972  avoided the issue by saying an index element is a  physically adjacent  pair of  x  a  where x is the key  and a is some associated information the associated information might be a pointer to a record or records in a random access  but what it was did n't really matter bayer & mccreight  1972  states  " for this paper the associated information is of no further interest " 6 jump up ^ if n is zero  then no root node is needed  so the height of an empty tree is not well defined 7 jump up ^ " cache oblivious b-trees "  state university of new york  suny  at stony brook retrieved 2011-01-17 8 jump up ^ mark russinovich " inside win2k ntfs  part 1 "  microsoft developer network.archived from the original on 13 april 2008 retrieved 2008-04-18 9 jump up ^ " efficient locking for concurrent operations on b-trees "  portal.acm.org.doi  10.1145/319628.319663 retrieved 2012-06-28 10 jump up ^ http  //www.dtic.mil/cgi-bin/gettrdoc ad = ada232287&location = u2&doc = gettrdoc.pdf 11 jump up ^ " downloads  high-concurrency-btree  high concurrency b-tree code in c  github project hosting "  retrieved 2014-01-27 12 jump up ^ lockless concurrent b + tree general * bayer  r ; mccreight  e  1972   " organization and maintenance of large ordered indexes "  pdf   acta informatica 1  3   173 189  doi  10.1007/bf00288683 * comer  douglas  june 1979   " the ubiquitous b-tree "  computing surveys 11  2   123 137  doi  10.1145/356770.356776  issn 0360-0300 * cormen  thomas ; leiserson  charles ; rivest  ronald ; stein  clifford  2001   introduction to algorithms  second ed   mit press and mcgraw-hill  pp 434 454  isbn 0-262-03293-7 chapter 18  b-trees * folk  michael j ; zoellick  bill  1992   file structures  2nd ed   addison-wesley  isbn 0-201-55713-4 * knuth  donald  1998   sorting and searching  the art of computer programming  volume 3  second ed   addison-wesley  isbn 0-201-89685-0 section 6.2.4  multiway trees  pp 481 491 also  pp 476 477 of section 6.2.3  balanced trees  discusses 2-3 trees * mond  yehudit ; raz  yoav  1985   " concurrency control in b + -trees databases using preparatory operations "  vldb'85  proceedings of 11th international conference on very large data bases  331 334 original papers  edit  * bayer  rudolf ; mccreight  e  july 1970   organization and maintenance of large ordered indices  mathematical and information sciences report no 20  boeing scientific research laboratories * bayer  rudolf  1971   " binary b-trees for virtual memory "  proceedings of 1971 acm-sigfidet workshop on data description  access and control  san diego  californiamissing or empty | title =  help   november 11 12  1971 external links  edit  * b-tree lecture by david scot taylor  sjsu * b-tree animation applet by slady * b-tree and ub-tree on scholarpedia curator  dr rudolf bayer * b-trees  balanced tree data structures * nist 's dictionary of algorithms and data structures  b-tree * b-tree tutorial * the infinitydb btree implementation * cache oblivious b  +  -trees * dictionary of algorithms and data structures entry for b * -tree * open data structures  section 14.2  b-trees * counted b-trees  show  * v * t * e tree data structures  show  * v * t * e data structures categories  * 1971 introductions * b-tree * database index techniques red black tree from wikipedia  the free encyclopedia red black tree type tree invented 1972 invented by rudolf bayer time complexity in big o notation average worst case space o  n  o  n  search o  log n  o  log n  insert o  log n  o  log n  delete o  log n  o  log n  a red black tree is a binary search tree with an extra bit of data per node  its color  which can be either red or black  1  the extra bit of storage ensures an approximately balanced tree by constraining how nodes are colored from any path from the root to the leaf  1  thus  it is a data structure which is a type of self-balancing binary search tree balance is preserved by painting each node of the tree with one of two colors  typically called 'red ' and 'black '  in a way that satisfies certain properties  which collectively constrain how unbalanced the tree can become in the worst case when the tree is modified  the new tree is subsequently rearranged and repainted to restore the coloring properties the properties are designed in such a way that this rearranging and recoloring can be performed efficiently the balancing of the tree is not perfect but it is good enough to allow it to guarantee searching in o  log n  time  where n is the total number of elements in the tree the insertion and deletion operations  along with the tree rearrangement and recoloring  are also performed in o  log n  time  2  tracking the color of each node requires only 1 bit of information per node because there are only two colors the tree does not contain any other data specific to its being a red black tree so its memory footprint is almost identical to a classic  uncolored  binary search tree in many cases the additional bit of information can be stored at no additional memory cost contents  hide  * 1 history * 2 terminology * 3 properties * 4 analogy to b-trees of order 4 * 5 applications and related data structures * 6 operations o 6.1 insertion o 6.2 removal * 7 proof of asymptotic bounds o 7.1 insertion complexity * 8 parallel algorithms * 9 see also * 10 notes * 11 references * 12 external links history  edit  the original data structure was invented in 1972 by rudolf bayer  3  and named " symmetric binary b-tree "  but acquired its modern name in a paper in 1978 by leonidas j guibas and robert sedgewick entitled " a dichromatic framework for balanced trees "   4  the color " red " was chosen because it was the best-looking color produced by the color laser printer available to the authors while working at xerox parc  5  terminology  edit  a red black tree is a special type of binary tree  used in computer science to organize pieces of comparable data  such as text fragments or numbers the leaf nodes of red black trees do not contain data these leaves need not be explicit in computer memory a null child pointer can encode the fact that this child is a leaf but it simplifies some algorithms for operating on red black trees if the leaves really are explicit nodes to save memory  sometimes a single sentinel node performs the role of all leaf nodes ; all references from internal nodes to leaf nodes then point to the sentinel node red black trees  like all binary search trees  allow efficient in-order traversal  that is  in the order left root right  of their elements the search-time results from the traversal from root to leaf  and therefore a balanced tree of n nodes  having the least possible tree height  results in o  log n  search time properties  edit  an example of a red black tree in addition to the requirements imposed on a binary search tree the following must be satisfied by a red black tree   6  1 a node is either red or black 2 the root is black  this rule is sometimes omitted since the root can always be changed from red to black  but not necessarily vice versa  this rule has little effect on analysis   3 all leaves  nil  are black all leaves are of the same color as the root 4 every red node must have two black child nodes  and therefore it must have a black parent 5 every path from a given node to any of its descendant nil nodes contains the same number of black nodes these constraints enforce a critical property of red black trees  the path from the root to the farthest leaf is no more than twice as long as the path from the root to the nearest leaf the result is that the tree is roughly height-balanced since operations such as inserting  deleting  and finding values require worst-case time proportional to the height of the tree  this theoretical upper bound on the height allows red black trees to be efficient in the worst case  unlike ordinary binary search trees to see why this is guaranteed  it suffices to consider the effect of properties 4 and 5 together for a red black tree t  let b be the number of black nodes in property 5 let the shortest possible path from the root of t to any leaf consist of b black nodes longer possible paths may be constructed by inserting red nodes however  property 4 makes it impossible to insert more than one consecutive red node therefore the longest possible path consists of 2b nodes  alternating black and red  this is the worst case   the shortest possible path has all black nodes  and the longest possible path alternates between red and black nodes since all maximal paths have the same number of black nodes  by property 5  this shows that no path is more than twice as long as any other path analogy to b-trees of order 4  edit  the same red black tree as in the example above  seen as a b-tree a red black tree is similar in structure to a b-tree of order  note 1  4  where each node can contain between 1 and 3 values and  accordingly  between 2 and 4 child pointers in such a b-tree  each node will contain only one value matching the value in a black node of the red black tree  with an optional value before and/or after it in the same node  both matching an equivalent red node of the red black tree one way to see this equivalence is to " move up " the red nodes in a graphical representation of the red black tree  so that they align horizontally with their parent black node  by creating together a horizontal cluster in the b-tree  or in the modified graphical representation of the red black tree  all leaf nodes are at the same depth the red black tree is then structurally equivalent to a b-tree of order 4  with a minimum fill factor of 33 % of values per cluster with a maximum capacity of 3 values this b-tree type is still more general than a red black tree though  as it allows ambiguity in a red black tree conversion multiple red black trees can be produced from an equivalent b-tree of order 4 if a b-tree cluster contains only 1 value  it is the minimum  black  and has two child pointers if a cluster contains 3 values  then the central value will be black and each value stored on its sides will be red if the cluster contains two values  however  either one can become the black node in the red black tree  and the other one will be red   so the order-4 b-tree does not maintain which of the values contained in each cluster is the root black tree for the whole cluster and the parent of the other values in the same cluster despite this  the operations on red black trees are more economical in time because you do n't have to maintain the vector of values  citation needed  it may be costly if values are stored directly in each node rather than being stored by reference b-tree nodes  however  are more economical in space because you do n't need to store the color attribute for each node instead  you have to know which slot in the cluster vector is used if values are stored by reference  e.g objects  null references can be used and so the cluster can be represented by a vector containing 3 slots for value pointers plus 4 slots for child references in the tree in that case  the b-tree can be more compact in memory  improving data locality the same analogy can be made with b-trees with larger orders that can be structurally equivalent to a colored binary tree  you just need more colors suppose that you add blue  then the blue red black tree defined like red black trees but with the additional constraint that no two successive nodes in the hierarchy will be blue and all blue nodes will be children of a red node  then it becomes equivalent to a b-tree whose clusters will have at most 7 values in the following colors  blue  red  blue  black  blue  red  blue  for each cluster  there will be at most 1 black node  2 red nodes  and 4 blue nodes   for moderate volumes of values  insertions and deletions in a colored binary tree are faster compared to b-trees because colored trees do n't attempt to maximize the fill factor of each horizontal cluster of nodes  only the minimum fill factor is guaranteed in colored binary trees  limiting the number of splits or junctions of clusters   b-trees will be faster for performing rotations  because rotations will frequently occur within the same cluster rather than with multiple separate nodes in a colored binary tree   however for storing large volumes  b-trees will be much faster as they will be more compact by grouping several children in the same cluster where they can be accessed locally all optimizations possible in b-trees to increase the average fill factors of clusters are possible in the equivalent multicolored binary tree notably  maximizing the average fill factor in a structurally equivalent b-tree is the same as reducing the total height of the multicolored tree  by increasing the number of non-black nodes the worst case occurs when all nodes in a colored binary tree are black  the best case occurs when only a third of them are black  and the other two thirds are red nodes   notes 1 jump up ^ using knuth 's definition of order  the maximum number of children applications and related data structures  edit  red black trees offer worst-case guarantees for insertion time  deletion time  and search time not only does this make them valuable in time-sensitive applications such as real-time applications  but it makes them valuable building blocks in other data structures which provide worst-case guarantees ; for example  many data structures used incomputational geometry can be based on red black trees  and the completely fair scheduler used in current linux kernels uses red black trees the avl tree is another structure supporting o  log n  search  insertion  and removal it is more rigidly balanced than red black trees  leading to slower insertion and removal but faster retrieval this makes it attractive for data structures that may be built once and loaded without reconstruction  such as language dictionaries  or program dictionaries  such as the opcodes of an assembler or interpreter   red black trees are also particularly valuable in functional programming  where they are one of the most common persistent data structures  used to construct associative arraysand sets which can retain previous versions after mutations the persistent version of red black trees requires o  log n  space for each insertion or deletion  in addition to time for every 2-4 tree  there are corresponding red black trees with data elements in the same order the insertion and deletion operations on 2-4 trees are also equivalent to color-flipping and rotations in red black trees this makes 2-4 trees an important tool for understanding the logic behind red black trees  and this is why many introductory algorithm texts introduce 2-4 trees just before red black trees  even though 2-4 trees are not often used in practice in 2008  sedgewick introduced a simpler version of the red black tree called the left-leaning red black tree  7  by eliminating a previously unspecified degree of freedom in the implementation the llrb maintains an additional invariant that all red links must lean left except during inserts and deletes red black trees can be made isometric to either 2-3 trees   8  or 2-4 trees   7  for any sequence of operations the 2-4 tree isometry was described in 1978 by sedgewick  this quote needs a citation  with 2-4 trees  the isometry is resolved by a " color flip  " corresponding to a split  in which the red color of two children nodes leaves the children and moves to the parent node the tango tree  a type of tree optimized for fast searches  usually  when  uses red black trees as part of its data structure operations  edit  read-only operations on a red black tree require no modification from those used for binary search trees  because every red black tree is a special case of a simple binary search tree however  the immediate result of an insertion or removal may violate the properties of a red black tree restoring the red black properties requires a small number  o  log n  or amortized o  1   of color changes  which are very quick in practice  and no more than three tree rotations  two for insertion   although insert and delete operations are complicated  their times remain o  log n   insertion  edit  insertion begins by adding the node as any binary search tree insertion does and by coloring it red whereas in the binary search tree  we always add a leaf  in the red black tree  leaves contain no information  so instead we add a red interior node  with two black leaves  in place of an existing black leaf what happens next depends on the color of other nearby nodes the term uncle node will be used to refer to the sibling of a node 's parent  as in human family trees note that  * property 3  all leaves are black  always holds * property 4  both children of every red node are black  is threatened only by adding a red node  repainting a black node red  or a rotation * property 5  all paths from any given node to its leaf nodes contain the same number of black nodes  is threatened only by adding a black node  repainting a red node black  or vice versa   or a rotation note  the label n will be used to denote the current node  colored red   at the beginning  this is the new node being inserted  but the entire procedure may also be applied recursively to other nodes  see case 3   p will denote n 's parent node  g will denote n 's grandparent  and u will denote n 's uncle note that in between some cases  the roles and labels of the nodes are exchanged  but in each case  every label continues to represent the same node it represented at the beginning of the case any color shown in the diagram is either assumed in its case or implied by those assumptions a numbered triangle represents a subtree of unspecified depth a black circle atop the triangle designates a black root node  otherwise the root node 's color is unspecified there are several cases of red black tree insertion to handle  * n is the root node  i.e  first node of red black tree * n 's parent  p  is black * n 's parent  p  and uncle  u  are red * n is added to right of left child of grandparent  or n is added to left of right child of grandparent  p is red and u is black  * n is added to left of left child of grandparent  or n is added to right of right child of grandparent  p is red and u is black  each case will be demonstrated with example c code the uncle and grandparent nodes can be found by these functions  struct node * grandparent  struct node * n   if   n ! = null  &&  n > parent ! = null   return n > parent > parent ; else return null ;  struct node * uncle  struct node * n   struct node * g = grandparent  n  ; if  g = = null  return null ; // no grandparent means no uncle if  n > parent = = g > left  return g > right ; else return g > left ;  case 1  the current node n is at the root of the tree in this case  it is repainted black to satisfy property 2  the root is black   since this adds one black node to every path at once  property 5  all paths from any given node to its leaf nodes contain the same number of black nodes  is not violated void insert_case1  struct node * n   if  n > parent = = null  n > color = black ; else insert_case2  n  ;  case 2  the current node 's parent p is black  so property 4  both children of every red node are black  is not invalidated in this case  the tree is still valid property 5  all paths from any given node to its leaf nodes contain the same number of black nodes  is not threatened  because the current node n has two black leaf children  but because n is red  the paths through each of its children have the same number of black nodes as the path through the leaf it replaced  which was black  and so this property remains satisfied void insert_case2  struct node * n   if  n > parent > color = = black  return ; / * tree is still valid * / else insert_case3  n  ;  note  in the following cases it can be assumed that n has a grandparent node g  because its parent p is red  and if it were the root  it would be black thus  n also has an uncle node u  although it may be a leaf in cases 4 and 5 case 3  if both the parent p and the uncle u are red  then both of them can be repainted black and the grandparent g becomes red  to maintain property 5  all paths from any given node to its leaf nodes contain the same number of black nodes    now  the current red node n has a black parent since any path through the parent or uncle must pass through the grandparent  the number of black nodes on these paths has not changed however  the grandparent g may now violate properties 2  the root is black  or 4  both children of every red node are black   property 4 possibly being violated since g may have a red parent   to fix this  the entire procedure is recursively performed on g from case 1 note that this is a tail-recursive call  so it could be rewritten as a loop ; since this is the only loop  and any rotations occur after this loop  this proves that a constant number of rotations occur void insert_case3  struct node * n   struct node * u = uncle  n   * g ; if   u ! = null  &&  u > color = = red    n > parent > color = black ; u > color = black ; g = grandparent  n  ; g > color = red ; insert_case1  g  ;  else  insert_case4  n  ;   note  in the remaining cases  it is assumed that the parent node p is the left child of its parent if it is the right child  left and right should be reversed throughout cases 4 and 5 the code samples take care of this case 4  the parent p is red but the uncle u is black ; also  the current node n is the right child of p  and p in turn is the left child of its parent g in this case  a left rotation on p that switches the roles of the current node n and its parent p can be performed ; then  the former parent node p is dealt with using case 5  relabeling n and p  because property 4  both children of every red node are black  is still violated the rotation causes some paths  those in the sub-tree labelled " 1 "  to pass through the node n where they did not before it also causes some paths  those in the sub-tree labelled " 3 "  not to pass through the node p where they did before however  both of these nodes are red  so property 5  all paths from any given node to its leaf nodes contain the same number of black nodes  is not violated by the rotation after this case has been completed  property 4  both children of every red node are black  is still violated  but now we can resolve this by continuing to case 5 void insert_case4  struct node * n   struct node * g = grandparent  n  ; if   n = = n > parent > right  &&  n > parent = = g > left    rotate_left  n > parent  ; / * * rotate_left can be the below because of already having * g = grandparent  n  * * struct node * saved_p = g > left  * saved_left_n = n > left ; * g > left = n ; * n > left = saved_p ; * saved_p > right = saved_left_n ; * * and modify the parent 's nodes properly * / n = n > left ;  else if   n = = n > parent > left  &&  n > parent = = g > right    rotate_right  n > parent  ; / * * rotate_right can be the below to take advantage of already having * g = grandparent  n  * * struct node * saved_p = g > right  * saved_right_n = n > right ; * g > right = n ; * n > right = saved_p ; * saved_p > left = saved_right_n ; * * / n = n > right ;  insert_case5  n  ;  case 5  the parent p is red but the uncle u is black  the current node n is the left child of p  and p is the left child of its parent g in this case  a right rotation on g is performed ; the result is a tree where the former parentp is now the parent of both the current node n and the former grandparent g g is known to be black  since its former child p could not have been red otherwise  without violating property 4   then  the colors of p and g are switched  and the resulting tree satisfies property 4  both children of every red node are black   property 5  all paths from any given node to its leaf nodes contain the same number of black nodes  also remains satisfied  since all paths that went through any of these three nodes went through g before  and now they all go through p in each case  this is the only black node of the three void insert_case5  struct node * n   struct node * g = grandparent  n  ; n > parent > color = black ; g > color = red ; if  n = = n > parent > left  rotate_right  g  ; else rotate_left  g  ;  note that inserting is actually in-place  since all the calls above use tail recursion removal  edit  in a regular binary search tree when deleting a node with two non-leaf children  we find either the maximum element in its left subtree  which is the in-order predecessor  or the minimum element in its right subtree  which is the in-order successor  and move its value into the node being deleted  as shown here   we then delete the node we copied the value from  which must have fewer than two non-leaf children  non-leaf children  rather than all children  are specified here because unlike normal binary search trees  red black trees can have leaf nodes anywhere  so that all nodes are either internal nodes with two children or leaf nodes with  by definition  zero children in effect  internal nodes having two leaf children in a red black tree are like the leaf nodes in a regular binary search tree  because merely copying a value does not violate any red black properties  this reduces to the problem of deleting a node with at most one non-leaf child once we have solved that problem  the solution applies equally to the case where the node we originally want to delete has at most one non-leaf child as to the case just considered where it has two non-leaf children therefore  for the remainder of this discussion we address the deletion of a node with at most one non-leaf child we use the label m to denote the node to be deleted ; c will denote a selected child of m  which we will also call " its child "  if m does have a non-leaf child  call that its child  c ; otherwise  choose either leaf as its child  c if m is a red node  we simply replace it with its child c  which must be black by property 4  this can only occur when m has two leaf children  because if the red node m had a black non-leaf child on one side but just a leaf child on the other side  then the count of black nodes on both sides would be different  thus the tree would violate property 5  all paths through the deleted node will simply pass through one fewer red node  and both the deleted node 's parent and child must be black  so property 3  all leaves are black  and property 4  both children of every red node are black  still hold another simple case is when m is black and c is red simply removing a black node could break properties 4  both children of every red node are black  and 5  all paths from any given node to its leaf nodes contain the same number of black nodes   but if we repaint c black  both of these properties are preserved the complex case is when both m and c are black  this can only occur when deleting a black node which has two leaf children  because if the black node m had a black non-leaf child on one side but just a leaf child on the other side  then the count of black nodes on both sides would be different  thus the tree would have been an invalid red black tree by violation of property 5  we begin by replacing m with its child c we will call  or label that is  relabel  this child  in its new position  n  and its sibling  its new parent 's other child  s  s was previously the sibling of m  in the diagrams below  we will also use p for n 's new parent  m 's old parent   sl for s 's left child  and sr for s 's right child  scannot be a leaf because if m and c were black  then p 's one subtree which included m counted two black-height and thus p 's other subtree which includes s must also count two black-height  which can not be the case if s is a leaf node   note  in between some cases  we exchange the roles and labels of the nodes  but in each case  every label continues to represent the same node it represented at the beginning of the case any color shown in the diagram is either assumed in its case or implied by those assumptions white represents an unknown color  either red or black   we will find the sibling using this function  struct node * sibling  struct node * n   if  n = = n > parent > left  return n > parent > right ; else return n > parent > left ;  note  in order that the tree remains well-defined  we need that every null leaf remains a leaf after all transformations  that it will not have any children   if the node we are deleting has a non-leaf  non-null  child n  it is easy to see that the property is satisfied if  on the other hand  n would be a null leaf  it can be verified from the diagrams  or code  for all the cases that the property is satisfied as well we can perform the steps outlined above with the following code  where the function replace_node substitutes child into n 's place in the tree for convenience  code in this section will assume that null leaves are represented by actual node objects rather than null  the code in the insertion section works with either representation   void delete_one_child  struct node * n   / * * precondition  n has at most one non-null child * / struct node * child = is_leaf  n > right  n > left  n > right ; replace_node  n  child  ; if  n > color = = black   if  child > color = = red  child > color = black ; else delete_case1  child  ;  free  n  ;  note  if n is a null leaf and we do not want to represent null leaves as actual node objects  we can modify the algorithm by first calling delete_case1   on its parent  the node that we delete  n in the code above  and deleting it afterwards we can do this because the parent is black  so it behaves in the same way as a null leaf  and is sometimes called a 'phantom ' leaf   and we can safely delete it at the end as n will remain a leaf after all operations  as shown above if both n and its original parent are black  then deleting this original parent causes paths which proceed through n to have one fewer black node than paths that do not as this violates property 5  all paths from any given node to its leaf nodes contain the same number of black nodes   the tree must be rebalanced there are several cases to consider  case 1  n is the new root in this case  we are done we removed one black node from every path  and the new root is black  so the properties are preserved void delete_case1  struct node * n   if  n > parent ! = null  delete_case2  n  ;  note  in cases 2  5  and 6  we assume n is the left child of its parent p if it is the right child  left and right should be reversed throughout these three cases again  the code examples take both cases into account case 2  s is red in this case we reverse the colors of p and s  and then rotate left at p  turning s into n 's grandparent note that p has to be black as it had a red child although all paths still have the same number of black nodes  now n has a black sibling and a red parent  so we can proceed to step 4  5  or 6  its new sibling is black because it was once the child of the red s  in later cases  we will relabel n 's new sibling as s void delete_case2  struct node * n   struct node * s = sibling  n  ; if  s > color = = red   n > parent > color = red ; s > color = black ; if  n = = n > parent > left  rotate_left  n > parent  ; else rotate_right  n > parent  ;  delete_case3  n  ;  case 3  p  s  and s 's children are black in this case  we simply repaint s red the result is that all paths passing throughs  which are precisely those paths not passing through n  have one less black node because deleting n 's original parent made all paths passing through n have one less black node  this evens things up however  all paths through p now have one fewer black node than paths that do not pass through p  so property 5  all paths from any given node to its leaf nodes contain the same number of black nodes  is still violated to correct this  we perform the rebalancing procedure on p  starting at case 1 void delete_case3  struct node * n   struct node * s = sibling  n  ; if   n > parent > color = = black  &&  s > color = = black  &&  s > left > color = = black  &&  s > right > color = = black    s > color = red ; delete_case1  n > parent  ;  else delete_case4  n  ;  case 4  s and s 's children are black  but p is red in this case  we simply exchange the colors of s and p this does not affect the number of black nodes on paths going through s  but it does add one to the number of black nodes on paths going through n  making up for the deleted black node on those paths void delete_case4  struct node * n   struct node * s = sibling  n  ; if   n > parent > color = = red  &&  s > color = = black  &&  s > left > color = = black  &&  s > right > color = = black    s > color = red ; n > parent > color = black ;  else delete_case5  n  ;  case 5  s is black  s 's left child is red  s 's right child is black  and n is the left child of its parent in this case we rotate right at s  so thats 's left child becomes s 's parent and n 's new sibling we then exchange the colors of s and its new parent all paths still have the same number of black nodes  but now n has a black sibling whose right child is red  so we fall into case 6 neither n nor its parent are affected by this transformation  again  for case 6  we relabel n 's new sibling as s  void delete_case5  struct node * n   struct node * s = sibling  n  ; if  s > color = = black   / * this if statement is trivial  due to case 2  even though case 2 changed the sibling to a sibling 's child  the sibling 's child ca n't be red  since no red parent can have a red child   * / / * the following statements just force the red to be on the left of the left of the parent  or right of the right  so case six will rotate correctly * / if   n = = n > parent > left  &&  s > right > color = = black  &&  s > left > color = = red    / * this last test is trivial too due to cases 2-4 * / s > color = red ; s > left > color = black ; rotate_right  s  ;  else if   n = = n > parent > right  &&  s > left > color = = black  &&  s > right > color = = red    / * this last test is trivial too due to cases 2-4 * / s > color = red ; s > right > color = black ; rotate_left  s  ;   delete_case6  n  ;  case 6  s is black  s 's right child is red  and n is the left child of its parent p in this case we rotate left at p  so that sbecomes the parent of p and s 's right child we then exchange the colors of p and s  and make s 's right child black the subtree still has the same color at its root  so properties 4  both children of every red node are black  and 5  all paths from any given node to its leaf nodes contain the same number of black nodes  are not violated however  n now has one additional black ancestor  either p has become black  or it was black and s was added as a black grandparent thus  the paths passing through n pass through one additional black node meanwhile  if a path does not go through n  then there are two possibilities  * it goes through n 's new sibling then  it must go through s and p  both formerly and currently  as they have only exchanged colors and places thus the path contains the same number of black nodes * it goes through n 's new uncle  s 's right child then  it formerly went through s  s 's parent  and s 's right child  which was red   but now only goes through s  which has assumed the color of its former parent  and s 's right child  which has changed from red to black  assuming s 's color  black   the net effect is that this path goes through the same number of black nodes either way  the number of black nodes on these paths does not change thus  we have restored properties 4  both children of every red node are black  and 5  all paths from any given node to its leaf nodes contain the same number of black nodes   the white node in the diagram can be either red or black  but must refer to the same color both before and after the transformation void delete_case6  struct node * n   struct node * s = sibling  n  ; s > color = n > parent > color ; n > parent > color = black ; if  n = = n > parent > left   s > right > color = black ; rotate_left  n > parent  ;  else  s > left > color = black ; rotate_right  n > parent  ;   again  the function calls all use tail recursion  so the algorithm is in-place in the algorithm above  all cases are chained in order  except in delete case 3 where it can recurse to case 1 back to the parent node  this is the only case where an in-place implementation will effectively loop  after only one rotation in case 3   additionally  no tail recursion ever occurs on a child node  so the tail recursion loop can only move from a child back to its successive ancestors no more than o  log n  loops back to case 1 will occur  where n is the total number of nodes in the tree before deletion   if a rotation occurs in case 2  which is the only possibility of rotation within the loop of cases 1 3   then the parent of the node n becomes red after the rotation and we will exit the loop therefore at most one rotation will occur within this loop since no more than two additional rotations will occur after exiting the loop  at most three rotations occur in total proof of asymptotic bounds  edit  a red black tree which contains n internal nodes has a height of o  log  n    definitions  * h  v  = height of subtree rooted at node v * bh  v  = the number of black nodes  not counting v if it is black  from v to any leaf in the subtree  called the black-height   lemma  a subtree rooted at node v has at least internal nodes proof of lemma  by induction height   basis  h  v  = 0 if v has a height of zero then it must be null  therefore bh  v  = 0 so  inductive step  v such that h  v  = k  has at least internal nodes implies that such that h   = k + 1 has at least internal nodes since has h   > 0 it is an internal node as such it has two children each of which have a black-height of either bh   or bh   -1  depending on whether the child is red or black  respectively   by the inductive hypothesis each child has at least internal nodes  so has at least  internal nodes using this lemma we can now show that the height of the tree is logarithmic since at least half of the nodes on any path from the root to a leaf are black  property 4 of a red black tree   the black-height of the root is at least h  root  /2 by the lemma we get  therefore the height of the root is o  log  n    insertion complexity  edit  in the tree code there is only one loop where the node of the root of the red black property that we wish to restore  x  can be moved up the tree by one level at each iteration since the original height of the tree is o  log n   there are o  log n  iterations so overall the insert routine has o  log n  complexity parallel algorithms  edit  parallel algorithms for constructing red black trees from sorted lists of items can run in constant time or o  log log n  time  depending on the computer model  if the number of processors available is asymptotically proportional to the number of items fast search  insertion  and deletion parallel algorithms are also known  9  see also  edit  * tree data structure * tree rotation * scapegoat tree * splay tree * avl tree * b-tree  2-3 tree  2-3-4 tree  b + tree  b * -tree  ub-tree  * t-tree * list of data structures notes  edit  1 ^ jump up to  a b cormen  thomas h  2001   introduction to algorithms charles e leiserson  ronald l rivest  clifford stein mit press p 273 isbn 0262032937 2 jump up ^ john morris " red black trees "  3 jump up ^ rudolf bayer  1972   " symmetric binary b-trees  data structure and maintenance algorithms "  acta informatica 1  4   290 306 doi  10.1007/bf00289509 4 jump up ^ leonidas j guibas and robert sedgewick  1978   " a dichromatic framework for balanced trees "  proceedings of the 19th annual symposium on foundations of computer science pp 8 21 doi  10.1109/sfcs.1978.3 5 jump up ^ robert sedgewick  2012   red-black bsts coursera a lot of people ask why did we use the name red black well  we invented this data structure  this way of looking at balanced trees  at xerox parc which was the home of the personal computer and many other innovations that we live with today entering  sic  graphic user interfaces  ethernet and object-oriented programmings  sic  and many other things but one of the things that was invented there was laser printing and we were very excited to have nearby color laser printer that could print things out in color and out of the colors the red looked the best so  that s why we picked the color red to distinguish red links  the types of links  in three nodes so  that s an answer to the question for people that have been asking 6 jump up ^ cormen  thomas ; leiserson  charles ; rivest  ronald ; stein  clifford  2009   " 13 "  introduction to algorithms  3rd ed   mit press pp 308 309 isbn 978-0-262-03384-8 7 ^ jump up to  a b http  //www.cs.princeton.edu/ ~ rs/talks/llrb/redblack.pdf 8 jump up ^ http  //www.cs.princeton.edu/courses/archive/fall08/cos226/lectures/10balancedtrees-2x2.pdf 9 jump up ^ park  heejin ; park  kunsoo  2001   " parallel algorithms for red black trees "  theoretical computer science  elsevier  262  1 2   415 435 doi  10.1016/s0304-3975  00  00287-5 our parallel algorithm for constructing a red black tree from a sorted list of n items runs in o  1  time with n processors on the crcw pram and runs in o  log log n  time with n / log log nprocessors on the erew pram references  edit  this article includes a list of references  but its sources remain unclear because it has insufficient inline citations please help to improve this article by introducing more precise citations  july 2013  * mathworld  red black tree * san diego state university  cs 660  red black tree notes  by roger whitney * thomas h cormen  charles e leiserson  ronald l rivest  and clifford stein introduction to algorithms  second edition mit press and mcgraw-hill  2001 isbn 0-262-03293-7  chapter 13  red black trees  pp 273 301 * pfaff  ben  june 2004   " performance analysis of bsts in system software "  pdf   stanford university * okasaki  chris " red black trees in a functional setting "  ps   external links  edit  * a complete and working implementation in c * red black tree demonstration * ocw mit lecture by prof erik demaine on red black trees  * binary search tree insertion visualization on youtube visualization of random and pre-sorted data insertions  in elementary binary search trees  and left-leaning red black trees * an intrusive red-black tree written in c + +  show  * v * t * e tree data structures  show  * v * t * e data structures categories  * 1972 in computer science * binary trees * search trees 