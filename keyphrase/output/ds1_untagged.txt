welcome to data structures and algorithms we are going to learn today some basic terminology regarding data structures and the notations that you have been following in the rest of this course will begin with just some very simple definitions an algorithm which is an outline of these steps that a program has to take right or any computational procedure has to take a program on the other hand is an implementation of an algorithm um it could be any programming language um a data structure is the way we need to organize the data so that it can be used effectively by the program yeah so you all familiar with certain data structures an array or list for instance so this course is about you will be seeing lot more data structures in this course and you will see how to use them in various algorithms so we will take a particular problem try to solve that problem and in the process develop data structures the best way of organizing the data associated with that problem okay whats an algorithmic problem an algorithmic problem is essentially that you have certain specifications of an input as is given here and i use specify what the output should be like and specification of an input could be well here is one specification a sorted non decreasing sequence of natural numbers of non zero finite length right that 's an input that is a comp completely specified input i have given two examples here of input which meet the specification i have not given any output specification um right now what is an instance these are two instances of the input right this is the specification for the input and you can have you know any any possible instance you can take any sequence of sorted non decreasing numbers that would form a input instance so there are infinitely many input instances possible here so an algorithm is essentially describing the actions that one should take on the input instance to get an output as it decide as it specified and again just as they can infinitely many input instances they can be infinitely many algorithms for solving the certain problem yeah each one of you could do it in slightly different way so that bring to the notion of good algorithm right if there are so many different algorithms for solving the certain problem what is a good algorithm right so good algorithm for us in an efficient algorithm yeah anything that is efficient is good what is efficient efficient is something which has small running time and takes less memory these will be the two measures of the efficiency that will be working with right they could be other measures of efficiency but these are the only two will be considering this course and most of our time we would be spending with running time really yeah space of course will be analyzing this space but most of the time would be spend in worrying about the running time for an algorithm and we would be interested in the efficiency of algorithms as the function of input sizes so clearly you can imagine in that if i have a small input right and my algorithm running on that input will take or my program  noise  running on that input will take less amount of time if the input becomes ten times larger then the time taken by the program but also grow may be it becomes ten times may be it becomes twenty times may be it becomes hundred times i do not know right so it is this behavior of the increase in the running time with the increase in the size of input that we would also be of interest to us okay will come to all of these in a short while as we go to these slides okay how does one measure running time right i said efficiency running time very important for us how does one um measure running time if an algorithm so one way would be to what i have put down here as an experimental study you have certain algorithm you implement it the algorithm which means you write a program in a certain programming language you run the program with varying data sets right so some smaller some larger data sets you know some would be some kinds some would be different kinds varying composition and then you clock the time program takes right and clock does it means that you sits down the clock watch you had perhaps you use the system utility like system current time millis to clock the time that program takes and then from that you try an figure out how good your algorithms yeah so that would that 's it one would call an experimental study of the algorithm okay this has certain limitations so put them down first you have to implement the algorithm to be able to determine mine how good your algorithm that 's you implemented it thats already a huge overhead right you have to considerable amount of time has to be spent doing this then your experiments can be done only on an limited set of inputs right after all i said the number of instances is infinitely large you can not be running you can only run your experimental small set of instances and that might not really indicative of the time that your algorithm is taking for other inputs which you have not considered in your in your experiment further if you have two algorithms and you have to decide which is better you have to use exactly the same platforms to do the comparison right by platform i mean both the hardware and software environment yeah because as you can imagine different machines would make different not just different machines in fact even the users were working on that a system might that particular point make a difference on the running time of a algorithm right so it becomes very messy you have to do it this one so what we are going to do is part of this course infact in this very first lecture is to develop the general methodology which will help us analyze running time of algorithms okay and we are going to do at is follows we are going to first develop a high level description of an algorithm way of describing an algorithm describing an algorithm and we are going to use this description to det figure out the running time and not you know implemented it to any system a methodology would help us taken account to all possible input instances and it would help us it will allow us to evaluate the efficency of the algorithm in a way that is independent of the environment independent of the platform we are using yeah okay so i said i will give a high level description of the algorithm right this very first point here we will so what is this so this bring me to what will call a pseudo code and this how would be specifying all over algorithms right for the purpose of this course yeah so here is an example of pseudo code and you might have seen this in earlier courses also so what is this algorithm doing this algorithm takes an array a which stores an integer in it  noise  and its trying to find the maximum element in this array right what i have written here is not a program yeah because i think the syntax is all wrong yeah but it is it is pseudo code it is a mixture of natural language and some high level programming construct so iam going to be using i am going to come all of that but i am going to use like a for do loop i am going to use if then else i meant to use while loop all of that i will use okay but i will not bother about okay there should be semicolon here or they should be colon here and i am not going to bother about those are things are required by the compiler but for our understanding this is completely clear what this program is doing right what it is doing is starting its keeping track of the maximum variable maximum in an in variable called current max which is initialized to the first element of the array yeah and then it is going to run through the remaining element of the array compare with them current maximum element that we have seen if the current maximum that we have is less than the current element the element under consideration then it would update current max ai becomes the new max yeah and then when the loop terminates we just return current max it 's a very simple algorithm but just with this doing pseudo code you are able to understand what if it is doing this will not run on any computer its pseudo code but it it coveys the idea it coveys the concepts yeah any question up to this point okay that 's what i saying most structured pseudo code most structured than usual course but it is less for than formal programming yeah and what pseudo code look like we will use standard numerical numeric boolean expression in it right instead of the assignment operator which is equal to in java i will use this left arrow yeah and instead of the quality operator quality relationship in java which is two equals is same in c i will use just single equivalent yeah and ill declare methods in this manner so name of algorithm the name what are the parameter it takes ill use all kinds of programming construct so ill use if then statement if then else statement while do repeat until for do to index array ill say a  i  a  i.j  so on right it should be clear what this is doing and ill use return when the procedure terminates in it and return value tell you what is the value return by the particular procedure or a function and when i have to make a call to a method ill specify with that method name of that method the argument and what is the object it is any question to this point  noise  this is specifying the type of the value return by the particular method yeah you will see more of this the when we come across more pseudo code okay so how do we analyze algorithms so we identify what are primitive operations in our pseudo code right so what is primitive operations it 's a low level operation examples data movement i do an assignment from to an another i do a control statement which is a branch if then else as a routine call return or i do arithmetic operations or logical operations these are all we call primitive operations and in my pseudo code i just inspect the pseudo code count the number of primitive operations that are executed by the algorithm yeah okay 13.11 so we need to an example now ill take an example of sorting you all know what is sorting is the input is some sequence of numbers output is a permutation of the sequence which is in non decreasing order  hindi  if this my input this should be my output okay what are the requirements of the output we said it should be in non decreasing order and it should be permutation of the input any set of numbers which are in non decreasing order does n't make an output right your algorithm should sort the numbers that were given to it not just produce the sequence of numbers as an increasing order okay clearly the running time depend upon  hindi  how many elements are there and quite often it depends upon how sorted these numbers are if there are already al most sorted then perhaps your algorithms not going to take a long time of course it also depends upon what particular algorithm using right so the running time would depend on all these things okay and the first sorting technique we are going to use is one that you have use very often right lets say when your playing game of cards okay um so what is the strategy you follow when you are picking up set of cards that have dealt out to you right so you like to keep them in sorted order in your hand so you start with empty hand you pick up the first card then you take the next card and you inserted it at the appropriate place suppose if you have some five cards in your hand already lets say two five seven nine jack and queen then you getting eight so i am going to put it between seven and nine that 's the right place it has to be in yeah so i am inserting it at the appropriate place and thats why this technique is called insertion sort and i keep doing this still i have picked up all the cards and inserted in appropriately in my in the the hand that i have okay  noise  so  noise  what idea this is the pseudo code for insertion sort and i will run through this so that you all understand i give you as input an array of integers let say it contains the input the output is a permutation of the original numbers such that it is sorted so my output is also going to be in same array this is input output specification okay i am going to have two variables two indices i and j yeah okay the array is going to be sorted from a one through ai at any point in time its going to be sorry a one through a j minus one is going to be sorted j the jth location is the element which i have to insert appropriately at the right place yeah so clearly j then has to vary from two to n is it right so j has to vary from two to n so i am going to look at jth element i put that in key and now this is have to do i have to insert a j or i have to insert the key in to the sorted sequence which is a one through a j minus one a one through a j minus one is sorted sequence and i am going to use the index i to help me to do this yes what is index i going to do index i 's going to run down from j minus one down to one yeah we have to decrease xi what we are doing here in this while do loop its starts with the value okay its start with the value it starts with the value j minus one and what i am going to do lets see what i am going to do here i have to insert seven i am going to move this nine to this location yeah why because nine is more than seven nine is more than seven i move it here then i compare seven with eight seven eight is still more than seven so i will move it right then i compare seven with six six is smaller than seven s i have now found the right place for seven and i would put seven in here that 's exactly what is happening here i run through this loop till i find an element which is less than key key is the element i am trying to insert so this loop will continue while the element i am considering is more than key this loop will terminate when i see an element which is less than key all the loop will terminate when i reach i equals zero in which case that means that i have moved every thing to the right and i should assured insert the element at the very first place and what i am doing here i am shifting the element shifted to right i should insert the element at the very first place and what i am doing here i am just shifting the element once step to the right yeah so note that i have to insert the right place i shift nine right one step now this location becomes empty to say then i shift eight one step so this location now becomes empty and now i come and put seven in here do which is what i do eventually so i plus one would be the index where this location which would be empty location eventually and i put him key there okay so it 's a very you can all implement it may be you would have implemented in a slightly in a different way that would give you a different program right but the algorithm is is is essentially the same you are going to find the right place for the element and insert it okay and now lets start try and analyze this algorithm okay put down the algorithm here on left um there is small mistake there should be a left arrow please make a correction on that okay what we are going to do is lets count so these are all my primitive operations right this is the primitive operation this is primitive operation here i have two primitive operation because i am comparing i with zero and i am comparing ai with key and actually i am also taking and so there are three primitive operations yeah so a certain number of primitive operations here this is also primitive so all these are primitive operations and what i am doing here each of these operations take a certain amount of time depending upon the computer system you have right c one c two c three c four c five c six just reflect just represent in the amount of time these operations taken in whatever unit i take care yeah and here i am counting the number of times each of these operations is executed is done in this entire program why this operation um why this operation done n times is n times so i start assigning by j equals two then assign three four five six seven i go all the way up to n and then when incremented may be once check that one is no more so i have counted as n times right they might be small errors in n n plus one its not very important so this is roughly n times at we do this operation how about this operation i am going to do exactly n minus one times ones for two ones for three ones for four up to n right that 's why i have this operation is being done n minus one times lets forget this is a comment statement again this operation will be done exactly n minus one times right now we have to look at how many times this particular how many times do i come to this statement right okay um t sub j t sub j reflects the counts the number of times i have to shift an element to the right when i am inserting the jth card in to my hand okay in the previous example when i am inserting seven i had to shift two elements eight and nine yeah so t sub j is going to be counting that quantity okay and that is the number of times i am going to reach this part of my while loop yeah i am going to be checking this condition so first i come to this condition then may be i come again to this condition i come again over and over again to this condition i am going to for one iteration of this for loop or in the jth iteration of this for loop i am going to reach this condition t subject times and so the total number of times i am saying that condition is sum of t sub j as j go from two to n okay so every time i see this condition i also come here right i am going to be see this condition i am going to come this statement one more time then i come here because you know the last time i see the statement i would exit out of here so that 's why this is t j minus one and similarly for the same reason this is tj minus one j going from t to n okay and this statement here is not part of this while loop okay this is an assignment operation please correct this this is not part of the while loop and so this statement is part of the for loop is done exactly n minus one times as the other statement right and so the total time this procedure takes if you knew what this constant work can be computed in this manner of course you do not know what it is t sub j is right t sub j is quantity which depends upon your upon up your instances not problem right difference here a problem is one of sorting the instance is set of numbers the sequence of numbers that you have given to you so t subject depends upon the instance lets see what difference t sub j makes so we can so if the input was already sorted if the input was already sorted t sub j is always one i just have to compare the element with the last element and i will immediately see that is larger than the last element and so on i would not have to do anything at all so t sub is always a one if the input is already is in increasing order what happens when the input is in decreasing order yeah if the input is in decreasing order then the number that i am trying to insert is going to be smaller then all the numbers that i already have yeah because the input is in decreasing order the number i am trying to insert is smaller than the numbers i have sorted in my array so what i am going to do i am going to compare with the first element the second element the third element the fourth element all the way up to the very first element right so when i am trying to insert the jth element i am actually going to end up in comparing with all the other jth element in the array and so in that case t sub j will be equal to j and when t sub j equal j note what this quantity becomes its summation j j going from two to n you all know its of the kind n squared right in that case the running time of this algorithm would be some constant time n squared plus some other constant times n minus some other constants right the behavior of this running time is more of is more like n n squared we come to this point later when your talking of asymptotic analysis but this is what mean by f of n squared it will behave more like n squared here on the other hand in the best case when tj was one this sum here is just n or n minus one and in that case the total time is n times some constant plus n minus one time some constant minus some constant which is roughly n times some constant right sorry and so its what we call linear time algorithm i come to these again all in average okay now on an average what would you expect in the best case it is some thing like that you have to compare only a against one element in the worst case you have to compare about j the jth element in the average case would expect that it would take compare against half of this element right an average case if you were to take it as you comparing again j by two even when this quantity summation j by two j going from two to n what will this be this will be roughly by n squared four now yes and its behavior again like n squared and will come to these points in a minute okay this is what i mean by the best worst and average case so i take a specific i take the size of input i suppose interest in sorting n numbers and i look at all possible instances of these n numbers yeah infinitely many so again its not clear what how exactly one would do that so what is worst case define the worst case is defined as the maximum possible time that your algorithm would take for any instance of that size so these are all instances of the same size yes the best case would be the smallest time your algorithm take and the average would be the average of all of these infinitely many bars yes so that was for um for inputs of one size size n that would give as values of we can then the compute worst case best case average case right if i would to consider inputs of all sizes i can then create a plot of this kind for each inputs size i can figure out suppose i could figure what the worst case best case an average case were then i would get such topic monotonically increasing parts yeah why is that its clear that as size the input increases the size of your time taken your algorithm will increase its its not going to happen that your input size become larger and it takes lesser time so which of these of interest to which of this is easiest to work with worst case is that one we will use most of it for the purpose of this course only measure we will be working with why is that um first it provides an upper bound it tells you how long your algorithm is going to take in the worst case many algorithms um occurs fairly often quite often it is the case that the worst case that for many instances the time taken by the algorithm is close to the worst case right so that average case essentially becomes as bad as the worst case right in fact for the previous example that we saw average case was like n squared and worst case was also n squared of course there were differences in the constant but it was roughly the same average case might be very difficult quantity to compete because as you said average case if you have to compute look at all possible instances and then take some kind of average right all you have to say okay my input instances are drawn from certain distribution and so the expected time my algorithm will take if an input instance is drawn from the distribution um its typically a much harder quantity to work with and compute with so the worst case is is the is the measure of interest is what i will be working with okay so symptotonic analysis is the kind of thing we have been doing we have been so far when n squared n and the goal of this really to analyze the running time while getting get read of superficial details right we would like to say that an algorithm which has the running time of some constant n squared is the same as an algorithm which has a running time of the some other constant time n squared why because this constant is typically something which would be dependent upon the hardware that your using right in the example that i have showed you the c one c two c three all of those times would depend upon the computer system that your using the hardware the compiler many many factors right and show we would not like to distinguish we are not interested we are not bother about distinguish between such algorithms both of these algorithms one which ahs the running time of the three n square and another with running time n squared have a quadratic behavior as in when the input size its doubles both of the algorithm running time increases fore fold yeah and that is the thing that its interest to us so we are interested in capturing how the running time of algorithm increases with the size of input in the limit yeah this is the crucial point here and that 's what the asymptotic analysis is all about here is all about that what the asymptotic here is in the limit how does the running time of this algorithm increase with increase in 32.07 input instance in input size okay that brings us to something that some of you might have seen before the big oh notation if i have functions f of n and g of n f of n and g of n n here is typically represent the input size f of n is measuring the time that your algorithm is taking so f of n and g of n are both non negative functions not just non negative they are also non decreasing right because as input size is increases the value of the running time the time taken by your algorithm would also increase yeah so these are both non decreasing functions of n and we say that f of n is big oh of g of n if there exist constants c and n not such that f of n is less than c times g of n for n larger than n not so what all of these mean i have drawn two functions here the function is read f of n and g of n  noise  is some other function this function green is some constant time g of n now as you can see beyond this point n not c times g of n is always larger than f of n so i have not suppose this is the way continues even beyond beyond this point yeah then we would say that f of n is big oh of g of n or f of n is order g of n okay so few examples would clarify this and we come to that okay my function f of n here is two n plus x and g of n is n right it is the case if you look at these two functions two n plus six is always larger than n yes so which is why this function two n plus six is always about n you might be wondering why two n plus six is non linear function here that 's because the scale here is an exponential scale here yeah two two to zero two to one two to the two and so on similarly on this axis yeah so this is n and this blue line here is two n and this is four n okay as you can see beyond this point f of n is less than four times n so what are our constants c and n not the constant c is four and n not would this point of crossing beyond which four n becomes larger than two n plus six right what is the point of crossing what point four n become larger than two n plus six three right so so n not becomes three right and then we say that f of n which is two n plus six is big oh of g of n not big oh of four n but big oh of n right so f of n here is big oh of n then so two n plus six is big oh of n another example the function red is g of n which is n and any constant time g of n so its same scale as in the previous line any constant time g of n will be just the same line displaces by suitable amount right so this will be four this will be four times and this will be whatever depends upon what they intercept here is right we just like this but your n squared would be line like this okay so there is no constant c such that n squared is less than c of n can you find out a constant c you can see n squared is less than c of n can you find out a constant c so that n square is less than c n for n more than n not no any constant that you choose i can pick a larger n i can pick a n such that this is violated and so it is not the case that n squared is big oh of n yeah so how does one figure out this things this is the very simple rule suppose this is my function if fifty n log n i just drop all constants an lower order terms so forget the constant fifty here and i get n log n so this is big oh of n log n  noise  this function fifty n log n is big oh of n log n yeah seven n minus three i drop the constant an lower order terms i get seven n minus three big oh of n have some complicated function like eight n log squared plus five n squared plus n i just drop all lower order terms so this is the fastest growing term because this is n squared plus well as log n in it so i just drop the n squared n term and from here i drop my constant and i get n squared log n can right this is this function is big oh of n square log n there is a constant c such that this quantity this large sum here is less than c times n square log n for n larger than sum n not so in the limit this quantity will be less than some constant times this quantity right and you can figure out what the value of c should be what the value of n not should be for that to happen okay this is a common error so this function fifty n log n is also big oh of n two the five yes or no yes right because well this quantity in fact is less than or equal to fifty times in to the five always for all n yes and that is just a constant so this is big oh of n to the five when we use the big oh notation we try and provide as strong amount as possible right so instead of saying while this statement is true we will rather call this as big oh of n log n right we will see more of this in in sub sequence slides okay how is the how to use the big oh notation so we are going to express the number of primitive operations  noise  that i have executed during run of the program as of function input says we are going to use big oh notation for that right if i have an algorithm which takes um which the number of primitive operations is big oh of n and some other algorithm for which the number of primitive operations big of n square then the clearly first algorithm is better than the second why because as the input size grows you know if the input size doubles then this running time of the algorithm is only going to double while the inputs while the running time of this algorithm will increase fore fold right similarly our algorithm  noise  which are as the running time of big oh of log n is better than one which has running time of big oh of n and we have the hierarchy this kind log n n square and algorithm with the running time of big oh of log n is better than one by big oh of n with better than one with big oh of running time big oh of n square running time of big oh n cube which is better than algorithm with running time big oh of two to the n there is a word of caution here right you might have an algorithm whose running time is million times n right because your doing whole lot of may be some other operations i cant see how u would create such algorithm but you might have an algorithm of this running time right so this is big oh of n right because this is less than equal to some constant time n and you might have some other algorithm with the running time of two n squared right so from what i said before you would say that this algorithm is better than this yeah the one with linear running time big oh of n running time is better than this it is true but in the limit and the limit here is achieved very late when n is really really really large for small instances you know this two n square might actually take less time than your million times n right so you have to be careful about the constants also okay so we will do some examples of a asymptotic analysis so i have a pseudo code here and what i am doing here is i have an array of numbers n numbers sitting in an array called x and i have to output an array a in which each element in which the element ai is the average of the numbers x not through xi okay this is what i have to do so what one way of doing it so i compute so i basically have a for loop in which i compute each element of the array a right so to compute a ten what should i do i just have to sum up x not through x ten which i am doing here right so what is this doing to compute a ten so when i computing a ten i is taking the value ten and i am running the index j from zero to ten i am summing up the value of x x zero plus x one to x ten this accumulator a and then i am eventually dividing the value of this accumulator with eleven right because its x not x ten and that gives me the number i should have in a ten yeah i am going to repeat this for eleven twelve thirteen fourteen all the elements same to get is this a good algorithm its not a good algorithm it is an algorithm right and lets compute what its running time would be like okay this is one step how many times this step executed its executed i times and what are the values that i takes initially i take a value zero one two three all the way up to n minus one right and this entire thing is done n times right so what does this give you is total running time of roughly n square this one step is getting executed n square times and this is the dominant thing right of course this will how many this step executed this step is getting executed n times and so this step which is getting executed n times but this step is getting executed roughly n square time some constant times um and so running time of algorithm is big oh of n square right so very a simple problem but you can have better solution here perhaps all of you you can see what the better solution should be whats a better solution we will have a variable s in which we would keep accumulating um the xi 's right so initially s is just zero and then i when i am computing ai i already have in s before the point i already have have in s x not through xi minus one right because use that at the last step thats the problem that is happening here see every time we are computing x so first we are computing x not plus x one then we are computing x not plus x one plus x two then we are computing x not plus x one plus x two plus x three we can find repeating of computation so hand over again why should we do that right the single variable which will keep track of the prefix the sum of the prefixes so far right so s at this point when i am in the ith run of this loop s at that point i already has some x not x through minus one and then some xi in it and to compute ith element i just need to divide some way i plus one right and i keep this accumulator around with me when i finish ith run ith iteration of this loop i have an s x not through xi x not through xi the sum x not through xi right i can reuse for it in the next step so how much time this take now in each run of this loop i am just doing two steps two primitive operations right that makes order n times right because this loop is executed n times okay so i have been using this free linear and quadratic but here is the slide which just tells you the other terms i might be using so linear if an algorithm has an running time um asymptotic running time of order n then we will call it as linear algorithm it has a asymptotic running time n squared we called it quadratic logarithmic if it is log n polynomial if it is end to the k for some constant k k is a constant and algorithm is called exponential if it has running time of eight to the n with a has some number than one yeah so till now i have only introduce to you the big oh notation we also have the big omega notation and big theta notation the big omega notation provides a lower bound right function f of n is omega of g of n so this is f of n this is g of constant time g of n if constant time g of n is always less than f of n earlier this more than f of n less than f of n in the limit so beyond a certain n not right has this picture illustrate so f of n is more than c times g of n beyond this point n not okay that case we say that f of n is omega of g of n theta notation f of n is theta of g of n if there exist constant c one and c two such that f of n is and which between c one times g of one and c two times g of n that is beyond a certain point f of n lies between one constant time g of n another constant times g of n so then we say that f of n is theta of g of n that is f of n grows like g of n in the limit okay another way of thinking of it it is f of n is theta of g of n if f of n is big oh of g of n and it also omega of g of n right  noise  this part tells you that f of n is big oh of g of n this part tells you that f of n is omega of g of n right and so we have the f of n is theta of g of n there are two more related asymptotic notations one is called little oh notation the other one is called little omega notation they are the non tight logs of big oh and sorry they are non tight analogs of big oh and big omega and it is best to understand this to do the analogy the real numbers so when i say that f of n is big oh of g of n and really saying that the function f is in some sense less than or equal to g right that 's in fact what we used in the definition f of n is less than c times g of n f is kind of this analogy with the real numbers when number is less than or equal to another number omega is for greater than or equal to theta is equal to of course these are functions these are real numbers if these are real numbers you can talk of equality you cant talk of equality a functions unless they are equal so this is the corresponding notion for that little o corresponds strictly less okay and little omega corresponds to strictly more we are not going to be using these in fact we will miss most of our times we just big oh okay that 's a part you should be very clear with  noise  for that ill just may be look at the slides the formal definition for  noise  little oh is that for every constant c they should exist some n not such that f of n is less than c times g of n for n more than n not how its different from big oh there you said there exist c and n not such that this is true here we want to say for every c there should exist an n not for this okay great so this is just one slide i had put up to show what the differences between these functions is like so suppose i have a algorithms whose running time is like four hundred n twenty n log n two n square n to the four n to the n yeah and here i will list out what is the largest problems size you can solve in one second one minute one hour okay so largest problem size you can solve roughly twenty five hundred say you using some um some constant so it lets say twenty five hundred if you had this has running time let say your this is what the problem size would be like okay why did you see that this is larger than this although this is this is worst running time than this um because of this constant yeah so you see that differences happening here if it is two n square then its already one a problem of size oh seven of seven that you can solve two to the n only solve the problem nineteen and see how the behavior is as the time increases right and r is thirty sixth hundred second and so you see a huge increase in the size of the problem you can solve if it were linear time algorithm still large increase when its n logging algorithm not so large in increase when its an n square algorithm and almost know to increase when its two to the n algorithm right if you an algorithm which is running time its something like two to the n you can not solve for problem of more than size hundred can not solve means it will take millions of years for you to do that right so that is the behavior that bothers us that 's the behavior we are interested in course that 's why a asymptotic analysis is is is what we we considering most of it  hindi  so any questions till this point so with that we are going to stop this lecture today we have looked at um asymptotic analysis and some initial notation and terminology that we be following with this course okay 