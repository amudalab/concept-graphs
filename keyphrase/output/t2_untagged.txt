however  the cost of this compaction is time and it can be particularly severe for large hard disks that use contiguous allocation  where compacting all the space 11.4 473 may take hours and may be necessary on a weekly basis some systems require that this function be done with the file system unmounted during this normal system operation generally can not be permitted  so such compaction is avoided at all costs on production machines most modern systems that need defragmentation can perform it during normal system operations  but the performance penalty can be substantial  another problem with contiguous allocation is determining how much space is needed for a file when the file is created  the total amount of space it will need must be found and allocated how does the creator  program or person  know the size of the file to be created in some cases  this detennination may be fairly simple  copying an existing file  for example  ; in general  however  the size of an output file may be difficult to estimate  if we allocate too little space to a file  we may find that the file can not be extended especially with a best-fit allocation strategy  the space on both sides of the file may be in use hence  we can not make the file larger in place  two possibilities then exist first  the user program can be terminated  with an appropriate error message the user must then allocate more space and run the program again these repeated runs may be costly to prevent them  the user will normally overestimate the amount of space needed  resulting in considerable wasted space the other possibility is to find a larger hole  copy the contents of the file to the new space  and release the previous space this series of actions can be repeated as long as space exists  although it can be time consuming however  the user need never be informed explicitly about what is happening ; the system continues despite the problem  although more and more slowly  even if the total amount of space needed for a file is known in advance  preallocation may be inefficient a file that will grow slowly over a long period  months or years  must be allocated enough space for its final size  even though much of that space will be unused for a long time the file therefore has a large amount of internal fragmentation  to minimize these drawbacks  some operating systems use a modified contiguous-allocation scheme here  a contiguous chunk of space is allocated initially ; then  if that amount proves not to be large enough  another chunk of contiguous space  known as an is added the location of a file 's blocks is then recorded as a location and a block count  plus a link to the first block of the next extent on some systems  the owner of the file can set the extent size  but this setting results in inefficiencies if the owner is incorrect internal fragm.entation can still be a problem if the extents are too large  and external fragmentation can become a problem as extents of varying sizes are allocated and deallocated the commercial veritas file system uses extents to optimize performance it is a high-performance replacement for the standard unix ufs  11.4.2 linked allocation solves all problems of contiguous allocation with linked allocation  each file is a linked list of disk blocks ; the disk blocks may be scattered anywhere on the disk the directory contains a pointer to the first and last blocks of the file for example  a file of five blocks might start at block 9 and continue at block 16  then block 1  then block 10  and finally block 25  figure 11.6   each block contains a pointer to the next block these pointers 474 chapter 11 directory 12 16 170180190 20021 ~ _20_ ~ 23_0-4 ~ 2402sc.51  260270 280290300310 figure i 1.6 linked allocation of disk space  are not made available to the user thus  if each block is 512 bytes in size  and a disk address  the poileter  requires 4 bytes  then the user sees blocks of 508 bytes  to create a new file  we simply create a new entry ile the directory with linked allocation  each directory entry has a pointer to the first disk block of the file this pointer is initialized to nil  the end-of-list pointer value  to signify an empty file the size field is also set to 0 a write to the file causes the free-space management system to filed a free block  and this new block is written to and is linked to the end of the file to read a file  we simply read blocks by following the pointers from block to block there is no external fragmentation with linked allocation  and any free block on the free-space list can be used to satisfy a request the size of a file need not be declared when that file is created  a file can continue to grow as long as free blocks are available consequently  it is never necessary to compact disk space  linked allocation does have disadvantages  however the major problem is that it can be used effectively only for sequential-access files to filed the ith block of a file  we must start at the begirueing of that file and follow the pointers rnetil we get to the ith block each access to a pointer requires a disk read  and some require a disk seek consequently  it is inefficient to support a direct-access capability for linked-allocation files  another disadvantage is the space required for the pointers if a pointer requires 4 bytes out of a 512-byte block  then 0.78 percent of the disk is being used for pointers  rather than for information each file requires slightly more space than it would otherwise  the usual solution to this problem is to collect blocks into multiples  called and to allocate clusters rather than blocks for instance  the file system may define a cluster as four blocks and operate on the disk only in cluster units pointers then use a much smaller percentage of the file 's disk space  this method allows the logical-to-physical block mapping to remain simple 11.4 475 but improves disk throughput  because fewer disk-head seeks are required  and decreases the space needed for block allocation and free-list management  the cost of this approach is an increase in internal fragmentation  because more space is wasted when a cluster is partially full than when a block is partially full clusters can be used to improve the disk-access time for many other algorithms as welt so they are used in most file systems  yet another problem of linked allocation is reliability recall that the files are linked together by pointers scattered all over the disk  and consider what would happen if a pointer were lost or damaged a bug in the operating-system software or a disk hardware failure might result in picking up the wrong pointer this error could in turn result in linking into the free-space list or into another file one partial solution is to use doubly linked lists  and another is to store the file name and relative block number in each block ; however  these schemes require even more overhead for each file  an important variation on linked allocation is the use of a  fat !  this simple but efficient method of disk-space allocation is used by the ms-dos and os/2 operating systems a section of disk at the beginning of each volume is set aside to contain the table the table has one entry for each disk block and is indexed by block number the fat is used in much the same way as a linked list the directory entry contains the block number of the first block of the file the table entry indexed by that block number contains the block number of the next block in the file this chain continues until it reaches the last block  which has a special end-of-file value as the table entry  an unused block is indicated by a table value of 0 allocating a new block to a file is a simple matter of finding the first 0-valued table entry and replacing the previous end-of-file value with the address of the new block the 0 is then replaced with the end-of-file value an illustrative example is the fat structure shown in figure 11.7 for a file consisting of disk blocks 217  618  and 339  directory entry name start block 0 217 618 339  618 339 number of disk blocks -1 fat figure 11.7 file-allocation table  476 chapter 11 the fat allocation scheme can result in a significant number of disk head seeks  unless the fat is cached the disk head must move to the start of the volume to read the fat and find the location of the block in question  then move to the location of the block itself in the worst case  both moves occur for each of the blocks a benefit is that random-access time is improved  because the disk head can find the location of any block by reading the information in the fat  11.4.3 indexed allocation linked allocation solves the external-fragmentation and size-declaration problems of contiguous allocation however  in the absence of a fat  linked allocation can not support efficient direct access  since the pointers to the blocks are scattered with the blocks themselves all over the disk and must be retrieved in order solves this problem by bringil1.g all the pointers together into one location  the blo ; ct   each file has its own index block  which is an array of disk-block addresses  the i th entry in the index block points to the i 111 block of the file the directory contains the address of the index block  figure 11.8   to find and read the i 1jz block  we use the pointer in the i 1lz index-block entry this scheme is similar to the paging scheme described il1 section 8.4  when the file is created  all pointers in the index block are set to nil when the ith block is first written  a block is obtained from the free-space manage1 ~ and its address is put in the ith index-block entry  indexed allocation supports direct access  without suffering from external fragmentation  because any free block on the disk can satisfy a request for more space indexed allocation does suffer from wasted space  however the pointer overhead of the index block is generally greater than the pointer overhead of linked allocation consider a common case in which we have a file of only one or two blocks with linked allocation  we lose the space of only one pointer per directory file jeep 16 figure 11.8 indexed allocation of disk space  11.4 allocation methods 477 block with indexed allocation  an entire index block must be allocated  even if only one or two pointers will be non-nil  this point raises the question of how large the index block should be every file must have an index block  so we want the index block to be as small as possible if the index block is too small  however  it will not be able to hold enough pointers for a large file  and a mechanism will have to be available to deal with this issue mechanisms for this purpose include the following  c linked scheme an index block is normally one disk block thus  it can be read and written directly by itself to allow for large files  we can link together several index blocks for example  an index block might contain a small header giving the name of the file and a set of the first 100 disk-block addresses the next address  the last word in the index block  is nil  for a small file  or is a pointer to another index block  for a large file   multilevel index a variant of linked representation uses a first-level index block to point to a set of second-level index blocks  which in tum point to the file blocks to access a block  the operating system uses the first-level index to find a second-level index block and then uses that block to find the desired data block this approach could be continued to a third or fourth level  depending on the desired maximum file size with 4,096-byte blocks  we could store 1,024 four-byte pointers in an index block two levels of indexes allow 1,048,576 data blocks and a file size of up to 4gb  combined scheme another alternative  used in the ufs  is to keep the first  say  15 pointers of the index block in the file 's inode the first 12 of these pointers point to direct blocks ; that is  they contain addresses of blocks that contain data of the file thus  the data for small files  of no more than 12 blocks  do not need a separate index block if the block size is 4 kb  then up to 48 kb of data can be accessed directly the next three pointers point to indirect blocks the first points to a single indirect block  which is an index block containing not data but the addresses of blocks that do contain data the second points to a double indirect block  which contains the address of a block that contains the addresses of blocks that contain pointers to the actual data blocks the last pointer contains the address of a triple indirect block under this method  the number of blocks that can be allocated to a file exceeds the amount of space addressable by the four-byte file pointers used by many operating systems a 32-bit file pointer reaches only 232 bytes  or 4gb many unix implementations  including solaris and ibm 's aix  now support up to 64-bit file pointers pointers of this size allow files and file systems to be terabytes in size a unix inode is shown in figure 11.9  indexed-allocation schemes suffer from some of the same performance problems as does linked allocation specifically  the index blocks can be cached in memory  but the data blocks may be spread all over a volume  11.4.4 performance the allocation methods that we have discussed vary in their storage efficiency and data-block access times both are important criteria in selecting the proper method or methods for an operating system to implement  478 chapter 11 implementing file systems figure 11.9 the unix inode  before selecting an allocation method  we need to determine how the systems will be used a system with mostly sequential access should not use the same method as a system with mostly random access  for any type of access  contiguous allocation requires only one access to get a disk block since we can easily keep the initial address of the file in memory  we can calculate immediately the disk address of the ith block  or the next block  and read it directly  for linked allocation  we can also keep the address of the next block in memory and read it directly this method is fine for sequential access ; for direct access  however  an access to the ith block might require i disk reads this problem indicates why linked allocation should not be used for an application requiring direct access  as a result  some systems support direct-access files by using contiguous allocation and sequential-access files by using linked allocation for these systems  the type of access to be made must be declared when the file is created a file created for sequential access will be linked and can not be used for direct access a file created for direct access will be contiguous and can support both direct access and sequential access  but its maximum length must be declared when it is created in this case  the operating system must have appropriate data structures and algorithms to support both allocation methods  files can be converted from one type to another by the creation of a new file of the desired type  into which the contents of the old file are copied the old file may then be deleted and the new file renamed  indexed allocation is more complex if the index block is already in memory  then the access can be made directly however  keeping the index block in memory requires considerable space if this memory space is not available  then we may have to read first the index block and then the desired data block for a two-level index  two index-block reads might be necessary for an 11.5 11.5 479 extremely large file  accessing a block near the end of the file would require reading in all the index blocks before the needed data block finally could be read thus  the performance of indexed allocation depends on the index structure  on the size of the file  and on the position of the block desired  some systems combine contiguous allocation with indexed allocation by using contiguous allocation for small files  up to three or four blocks  and automatically switching to an indexed allocation if the file grows large since most files are small  and contiguous allocation is efficient for small files  average performance can be quite good  for instance  the version of the unix operating system from sun microsystems was changed in 1991 to improve performance in the file-system allocation algorithm the performance measurements indicated that the maximum disk throughput on a typical workstation  a 12-mips sparcstation1  took 50 percent of the cpu and produced a disk bandwidth of only 1.5 me per second to improve performance  sun made changes to allocate space in clusters of 56 kb whenever possible  56 kb was the maximum size of a dma transfer on sun systems at that time   this allocation reduced external fragmentation  and thus seek and latency times in addition  the disk-reading routines were optimized to read in these large clusters the inode structure was left unchanged as a result of these changes  plus the use of read-ahead and free-behind  discussed in section 11.6.2   25 percent less cpu was used  and throughput substantially improved  many other optimizations are in use given the disparity between cpu speed and disk speed  it is not unreasonable to add thousands of extra instructions to the operating system to save just a few disk-head movements  furthermore  this disparity is increasing over time  to the point where hundreds of thousands of instructions reasonably could be used to optimize head movements  since disk space is limited  we need to reuse the space from deleted files for new files  if possible  write-once optical disks only allow one write to any given sector  and thus such reuse is not physically possible  to keep track of free disk space  the system maintains a the free-space list records all free disk blocks-those not allocated to some file or directory to create a file  we search the free-space list for the required amount of space and allocate that space to the new file this space is then removed from the free-space list when a file is deleted  its disk space is added to the free-space list the free-space list  despite its name  might not be implemented as a list  as we discuss next  11.5.1 bit vector frequently  the free-space list is implemented as a or each block is represented by 1 bit if the block is free  the bit is 1 ; if the block is allocated  the bit is 0  for example  consider a disk where blocks 2  3  4  5  8  9  10  11  12  13  17  18  25  26  and 27 are free and the rest of the blocks are allocated the free-space bit map would be 480 chapter 11 001111001111110001100000011100000   the main advantage of this approach is its relative simplicity and its efficiency in finding the first free block or n consecutive free blocks on the disk indeed  many computers supply bit-manipulation instructions that can be used effectively for that purpose for example  the intel family starting with the 80386 and the motorola family starting with the 68020 have instructions that return the offset in a word of the first bit with the value 1  these processors have powered pcs and macintosh systems  respectively   one technique for finding the first free block on a system that uses a bit-vector to allocate disk space is to sequentially check each word in the bit map to see whether that value is not 0  since a 0-valued word contains only 0 bits and represents a set of allocated blocks the first non-0 word is scanned for the first 1 bit  which is the location of the first free block the calculation of the block number is  number of bits per word  x  number of 0-value words  + offset of first 1 bit  again  we see hardware features driving software functionality unfortunately  bit vectors are inefficient unless the entire vector is kept in main memory  and is written to disk occasionally for recovery needs   keeping it in main memory is possible for smaller disks but not necessarily for larger ones  a 1.3-gb disk with 512-byte blocks would need a bit map of over 332 kb to track its free blocks  although clustering the blocks in groups of four reduces this number to around 83 kb per disk a 1-tb disk with 4-kb blocks requires 32 mb to store its bit map given that disk size constantly increases  the problem with bit vectors will continue to escalate a 1-pb file system would take a 32-gb bitmap just to manage its free space  11.5.2 linked list another approach to free-space management is to link together all the free disk blocks  keeping a pointer to the first free block in a special location on the disk and caching it in memory this first block contains a pointer to the next free disk block  and so on recall our earlier example  section 11.5.1   in which blocks 2  3  4  5  8  9  10  11  12  13  17  18  25  26  and 27 were free and the rest of the blocks were allocated in this situation  we would keep a pointer to block 2 as the first free block block 2 would contain a pointer to block 3  which would point to block 4  which would point to block 5  which would point to block 8  and so on  figure 11.10   this scheme is not efficient ; to traverse the list  we must read each block  which requires substantial i/0 time fortunately  however  traversing the free list is not a frequent action usually  the operating system simply needs a free block so that it can allocate that block to a file  so the first block in the free list is used the fat method incorporates free-block accounting into the allocation data structure no separate method is needed  11.5.3 grouping a modification of the free-list approach stores the addresses of n free blocks in the first free block the first n-1 of these blocks are actually free the last block contains the addresses of another n free blocks  and so on the addresses 11.5 481 figure 11.10 linked free-space list on disk  of a large number of free blocks can now be found quickly  unlike the situation when the standard linked-list approach is used  11.5.4 counting another approach takes advantage of the fact that  generally  several contiguous blocks may be allocated or freed simultaneously  particularly when space is allocated with the contiguous-allocation algorithm or through clustering thus  rather than keeping a list of n free disk addresses  we can keep the address of the first free block and the number  n  of free contiguous blocks that follow the first block each entry in the free-space list then consists of a disk address and a count although each entry requires more space than would a simple disk address  the overall list is shorter  as long as the count is generally greater than 1 note that this method of tracking free space is similar to the extent method of allocating blocks these entries can be stored in a b-tree  rather than a linked list for efficient lookup  insertion  and deletion  11.5.5 space maps sun 's zfs file system was designed to encompass huge numbers of files  directories  and even file systems  in zfs  we can create file-system hierarchies   the resulting data structures could have been large and inefficient if they had not been designed and implemented properly on these scales  metadata i/0 can have a large performance impact conside1 ~ for example  that if the freespace list is implemented as a bit map  bit maps must be modified both when blocks are allocated and when they are freed freeing 1gb of data on a 1-tb disk could cause thousands of blocks of bit maps to be updated  because those data blocks could be scattered over the entire disk  482 chapter 11 11.6 zfs uses a combination of techniques in its free-space managem.ent algorithm to control the size of data structures and minimize the i/0 needed to manage those structures first  zfs creates to divide the space on the device into chucks of manageable size a given volume may contain hundreds of metaslabs each metaslab has an associated space map zfs uses the counting algorithm to store information about free blocks rather than write count structures to disk  it uses log-structured file system techniques to record them the space map is a log of all block activity  allocatil g and freemg   in time order  in countil g format when zfs decides to allocate or free space from a metaslab  it loads the associated space map into memory in a balanced-tree structure  for very efficient operation   indexed by offset  and replays the log into that structure the in-memory space map is then an accurate representation of the allocated and free space in the metaslab zfs also condenses the map as much as possible by combining contiguous free blocks into a sil gle entry finally  the free-space list is updated on disk as part of the transaction-oriented operations of zfs during the collection and sortmg phase  block requests can still occur  and zfs satisfies these requests from the log in essence  the log plus the balanced tree is the free list  now that we have discussed various block-allocation and directorymanagement options  we can further consider their effect on performance and efficient disk use disks tend to represent a major bottleneck in system performance  since they are the slowest main computer component in this section  we discuss a variety of techniques used to improve the efficiency and performance of secondary storage  11.6.1 efficiency the efficient use of disk space depends heavily on the disk allocation and directory algorithms in use for instance  unix inodes are preallocated on a volume even an empty disk has a percentage of its space lost to inodes  however  by preallocating the inodes and spreading them across the volume  we improve the file system 's performance this improved performance results from the unix allocation and free-space algorithms  which try to keep a file 's data blocks near that file 's inode block to reduce seek time  as another example  let 's reconsider the clustermg scheme discussed in section 11.4  which aids in file-seek and file-transfer performance at the cost of internal fragmentation to reduce this fragmentation  bsd unix varies the cluster size as a file grows large clusters are used where they can be filled  and small clusters are used for small files and the last cluster of a file this system is described in appendix a  the types of data normally kept in a file 's directory  or inode  entry also require consideration commonly  a last write date is recorded to supply information to the user and to determine whether the file needs to be backed up some systems also keep a last access date  so that a user can determine when the file was last read the result of keeping this information is that  whenever the file is read  a field in the directory structure must be written 11.6 483 to that means the block must be read into memory  a section changed  and the block written back out to disk  because operations on disks occur only in block  or cluster  chunks so any time a file is opened for reading  its directory entry must be read and written as well this requirement can be inefficient for frequently accessed files  so we must weigh its benefit against its performance cost when designing a file system generally  every data item associated with a file needs to be considered for its effect on efficiency and performance  as an example  consider how efficiency is affected by the size of the pointers used to access data most systems use either 16 or 32-bit pointers throughout the operating system these pointer sizes limit the length of a file to either 216  64 kb  or 232 bytes  4 gb   some systems implement 64-bit pointers to increase this limit to 264 bytes  which is a very large number indeed however  64-bit pointers take more space to store and in turn make the allocation and free-space-management methods  linked lists  indexes  and so on  use more disk space  one of the difficulties in choosing a pointer size  or indeed any fixed allocation size within an operating system  is planning for the effects of changing technology consider that the ibm pc xt had a 10-mb hard drive and an ms-dos file system that could support only 32 mb  each fat entry was 12 bits  pointing to an 8-kb cluster  as disk capacities increased  larger disks had to be split into 32-mb partitions  because the file system could not track blocks beyond 32mb as hard disks with capacities of over 100mb became common  the disk data structures and algorithms in ms-dos had to be modified to allow larger file systems  each fat entry was expanded to 16 bits and later to 32 bits  the initial file-system decisions were made for efficiency reasons ; however  with the advent of ms-dos version 4  millions of computer users were inconvenienced when they had to switch to the new  larger file system sun 's zfs file system uses 128-bit pointers  which theoretically should never need to be extended  the minimum mass of a device capable of storing 2128 bytes using atomic-level storage would be about 272 trillion kilograms  as another example  consider the evolution of sun 's solaris operating system originally  many data structures were of fixed length  allocated at system startup these structures included the process table and the open-file table when the process table became full  no more processes could be created  when the file table became full  no more files could be opened the system would fail to provide services to users table sizes could be increased only by recompiling the kernel and rebooting the system since the release of solaris 2  almost all kernel structures have been allocated dynamically  eliminating these artificial limits on system performance of course  the algorithms that manipulate these tables are more complicated  and the operating system is a little slower because it must dynamically allocate and deallocate table entries ; but that price is the usual one for more general functionality  11.6.2 performance even after the basic file-system algorithms have been selected  we can still improve performance in several ways as will be discussed in chapter 13  most disk controllers include local memory to form an on-board cache that is large enough to store entire tracks at a time once a seek is performed  the track is read into the disk cache starting at the sector under the disk head 484 chapter 11 1/0 using read   and write   tile system figure 11.11 1/0 without a unified buffer cache   reducing latency time   the disk controller then transfers any sector requests to the operating system once blocks make it from the disk controller into main memory  the operating system may cache the blocks there  some systems maintain a separate section of main memory for a where blocks are kept under the assumption that will be used again shortly other systems cache file data using a the page cache uses virtual memory techniques to cache file data as pages rather than as file-system-oriented blocks cachii lg file data using virtual addresses is far more efficient than caching through physical disk blocks  as accesses interface with virtual memory rather than the file system several systems-including solaris  linux  and windows nt  2000  and xp-use caching to cache both process pages and file data this is known as some versions of unix and linux provide a to illustrate the benefits of the unified buffer cache  consider the two alternatives for opening and accessing a file one approach is to use memory mapping  section 9.7  ; the second is to use the standard system calls read   and write    without a unified buffer cache  we have a situation similar to figure 11.11 here  the read   and write   system calls go through the buffer cache  the memory-mapping call  however  requires using two caches-the page cache and the buffer cache a memory mapping proceeds by reading in disk blocks from the file system and storing them in the buffer cache because the virtual memory system does not interface with the buffer cache  the contents of the file in the buffer cache must be copied into the page cache this situation is known as and requires caching file-system data twice not only does it waste memory but it also wastes significant cpu and i/o cycles due to the extra data movement within system memory in addition  inconsistencies between the two caches can result in corrupt files in contrast  when a unified buffer cache is provided  both memory mapping and the read   and write   system calls use the same page cache this has the benefit of a voiding double 11.6 485 memory-mapped 1/0 buffer cache file system figure 11.12 1/0 using a unified buffer cache  caching  and it allows the virtual memory system to manage file-system data  the unified buffer cache is shown in figure 11.12  regardless of whether we are caching disk blocks or pages  or both   lru  section 9.4.4  seems a reasonable general-purpose algorithm for block or page replacement however  the evolution of the solaris page-caching algorithms reveals the difficulty in choosil1.g an algorithm solaris allows processes and the page cache to share unused memory versions earlier than solaris 2.5.1 made no distmction between allocatmg pages to a process and allocating them to the page cache as a result  a system performing many i/0 operations used most of the available memory for caching pages because of the high rates of i/0  the page scanner  section 9.10.2  reclaimed pages from processesrather than from the page cache-when free memory ran low solaris 2.6 and solaris 7 optionally implemented priority paging  in which the page scanner gives priority to process pages over the page cache solaris 8 applied a fixed limit to process pages and the file-system page cache  preventing either from forcing the other out of memory solaris 9 and 10 again changed the algorithms to maximize memory use and mmimize thrashing  another issue that can affect the performance of i/0 is whether writes to the file system occur synchronously or asynchronously  occur in the order in which the disk subsystem receives and the writes are not buffered thus  the calling routine must wait for the data to reach the disk drive before it can proceed in an the data are stored in the cache  and control returns to the caller asynchronous writes are done the majority of the time however  metadata writes  among others  can be synchronous operating systems frequently include a flag in the open system call to allow a process to request that writes be performed synchxonously for example  databases use this feature for atomic transactions  to assure that data reach stable storage in the required order  some systems optimize their page cache by using different replacement algorithms  depending on the access type of the file a file being read or written sequentially should not have its pages replaced in lru order  because the most recently used page will be used last  or perhaps never again instead  sequential access can be optimized by techniques known as free-behind and read-ahead removes a page from the buffer as soon as the next 486 chapter 11 11.7 page is requested the previous are not likely to be used again and waste buffer space with a requested page and several subsequent pages are read and cached these pages are likely to be requested after the current page is processed retrieving these data from the disk in one transfer and caching them saves a considerable ancount of time one might think that a track cache on the controller would elincinate the need for read-ahead on a multiprogrammed system however  because of the high latency and overhead involved in making many small transfers from the track cache to main memory  performing a read-ahead remains beneficial  the page cache  the file system  and the disk drivers have some interesting interactions when data are written to a disk file  the pages are buffered in the cache  and the disk driver sorts its output queue according to disk address  these two actions allow the disk driver to minimize disk-head seeks and to write data at times optimized for disk rotation unless synchronous writes are required  a process writing to disk simply writes into the cache  and the system asynchronously writes the data to disk when convenient the user process sees very fast writes when data are read from a disk file  the block i/0 system does some read-ahead ; however  writes are much more nearly asynchronous than are reads thus  output to the disk through the file system is often faster than is input for large transfers  counter to intuition  files and directories are kept both in main memory and on disk  and care must be taken to ensure that a system failure does not result in loss of data or in data inconsistency we deal with these issues in this section as well as how a system can recover from such a failure  a system crash can cause inconsistencies among on-disk file-system data structures  such as directory structures  free-block pointers  and free fcb pointers many file systems apply changes to these structures in place a typical operation  such as creating a file  can involve many structural changes within the file system on the disk directory structures are modified  fcbs are allocated  data blocks are allocated  and the free counts for all of these blocks are decreased these changes can be interrupted by a crash  and inconsistencies among the structures can result for example  the free fcb count might indicate that an fcb had been allocated  but the directory structure might not point to the fcb compounding this problem is the caching that operating systems do to optimize i/0 performance some changes may go directly to disk  while others may be cached if the cached changes do not reach disk before a crash occurs  more corruption is possible  in addition to crashes  bugs in file-system implementation  disk controllers  and even user applications can corrupt a file system file systems have varying methods to deal with corruption  depending on the file-system data structures and algorithms we deal with these issues next  11.7.1 consistency checking whatever the cause of corruption  a file system must first detect the problems and then correct them for detection  a scan of all the metadata on each file 11.7 487 system can confirm or deny the consistency of the systenl unfortunately  this scan can take minutes or hours and should occur every time the system boots  alternatively  a file system can record its state within the file-system metadata  at the start of any metadata change  a status bit is set to indicate that the metadata is in flux if all updates to the metadata complete successfully  the file system can clear that bit it however  the status bit remains set  a consistency checker is run  the systems program such as f s ck in unix or chkdsk in windows-compares the data in the directory structure with the data blocks on disk and tries to fix any inconsistencies it finds the allocation and free-space-management algorithms dictate what types of problems the checker can find and how successful it will be in fixing them for instance  if linked allocation is used and there is a link from any block to its next block  then the entire file can be reconstructed from the data blocks  and the directory structure can be recreated in contrast the loss of a directory entry on an indexed allocation system can be disastrous  because the data blocks have no knowledge of one another for this reason  unix caches directory entries for reads ; but any write that results in space allocation  or other metadata changes  is done synchronously  before the corresponding data blocks are written of course  problems can still occur if a synchronous write is interrupted by a crash  11.7.2 log-structured file systems computer scientists often fin.d that algorithms and technologies origil1.ally used in one area are equally useful in other areas such is the case with the database log-based recovery algorithms described in section 6.9.2 these logging algorithms have been applied successfully to the of consistency '-.il'c' .ll the resulting implementations are known as  or file systems  note that with the consistency-checking approach discussed in the preceding section  we essentially allow structures to break and repair them on recovery however  there are several problems with this approach one is that the inconsistency may be irreparable the consistency check may not be able to recover the structures  resulting in loss of files and even entire directories  consistency checking can require human intervention to resolve conflicts  and that is inconvenient if no human is available the system can remain unavailable until the human tells it how to proceed consistency checking also takes system and clock time to check terabytes of data  hours of clock time may be required  the solution to this problem is to apply log-based recovery techniques to file-system metadata updates both ntfs and the veritas file system use this method  and it is included in recent versions of ufs on solaris in fact it is becoming common on many operating systems  fundamentally  all metadata changes are written each set of operations for performing a specific task is a the changes are written to this log  they are considered to be committed  and the system call can return to the user process  allowing it to continue execution meanwhile  these log entries are replayed across the actual filesystem structures as the changes are made  a pointer is updated to indicate 488 chapter 11 which actions have completed and which are still incomplete when an entire committed transaction is completed  it is removed from the log file  which is actually a circular buffer a cb  ul ; n writes to the end of its space and then continues at the beginning  overwriting older values as it goes we would not want the buffer to write over data that had not yet been saved  so that scenario is avoided the log may be in a separate section of the file system or even on a separate disk spindle it is more efficient  but more complex  to have it under separate read and write heads  thereby decreasing head contention and seek times  if the system crashes  the log file will contain zero or more transactions  any transactions it contains were not completed to the file system  even though they were committed by the operating system  so they must now be completed  the transactions can be executed from the pointer until the work is complete so that the file-system structures remain consistent the only problem occurs when a transaction was aborted -that is  was not committed before the system crashed any changes from such a transaction that were applied to the file system must be undone  again preserving the consistency of the file system  this recovery is all that is needed after a crash  elimil ating any problems with consistency checking  a side benefit of using logging on disk metadata updates is that those updates proceed much faster than when they are applied directly to the on-disk data structures the reason for this improvement is found in the performance advantage of sequential i/0 over random i/0 the costly synchronous random meta data writes are turned into much less costly synchronous sequential writes to the log-structured file system 's loggil g area those changes in turn are replayed asynchronously via random writes to the appropriate structures  the overall result is a significant gain in performance of metadata-oriented operations  such as file creation and deletion  11.7.3 other solutions another alternative to consistency checking is employed by network appliance 's wafl file system and sun 's zfs file system these systems never overwrite blocks with new data rather  a transaction writes all data and metadata changes to new blocks when the transaction is complete  the metadata structures that pointed to the old versions of these blocks are updated to point to the new blocks the file system can then remove the old pointers and the old blocks and make them available for reuse if the old pointers and blocks are kept  a is created ; the snapshot is a view of the file system before the last update took place this solution should require no consistency checking if the pointer update is done atomically wafl does have a consistency checke1 ~ however  so some failure scenarios can still cause metadata corruption  see 11.9 for details of the wafl file system  sun 's zfs takes an even more im ovative approach to disk consistency  it never overwrites blocks  just as is the case with wafl however  zfs goes further and provides check-summing of all metadata and data blocks this solution  when combined with raid  assures that data are always correct zfs therefore has no consistency checker  more details on zfs are found in section 12.7.6  11.7 489 11.7.4 backup and restore magnetic disks sometimes fail  and care must be taken to ensure that the data lost in such a failure are not lost forever to this end  system programs can be used to data from disk to another storage device  such as a floppy disk  magnetic tape  optical disk  or other hard disk recovery from the loss of an individual file  or of an entire disk  may then be a matter of the data from backup  to minimize the copying needed  we can use information from each file 's directory entry for instance  if the backup program knows when the last backup of a file was done  and the file 's last write date in the directory indicates that the file has not changed since that date  then the file does not need to be copied again a typical backup schedule may then be as follows  1 copy to a backup medium all files from the disk this is called a to another medium all files changed since day 1 this is an day 3 copy to another medium all files changed since day 2  day n copy to another medium all files changed since day n-1 then go back to day 1  the new cycle can have its backup written over the previous set or onto a new set of backup media in this manner  we can restore an entire disk by starting restores with the full backup and continuing through each of the incremental backups of course  the larger the value of n  the greater the number of media that must be read for a complete restore an added advantage of this backup cycle is that we can restore any file accidentally deleted during the cycle by retrieving the deleted file from the backup of the previous day the length of the cycle is a compromise between the amount of backup medium needed and the number of days back from which a restore can be done to decrease the number of tapes that must be read to do a restore  an option is to perform a full backup and then each day back up all files that have changed since the full backup in this way  a restore can be done via the most recent incremental backup and the full backup  with no other incremental backups needed the trade-off is that more files will be modified each day  so each successive incremental backup involves more files and more backup media  a user ncay notice that a particular file is missing or corrupted long after the damage was done for this reason  we usually plan to take a full backup from time to time that will be saved forever it is a good idea to store these permanent backups far away from the regular backups to protect against hazard  such as a fire that destroys the computer and all the backups too  and if the backup cycle reuses media  we must take care not to reuse the 490 chapter 11 11.8 media too many times-if the media wear out  it might not be possible to restore any data from the backups  network file systems are commonplace they are typically integrated with the overall directory structure and interface of the client system nfs is a good example of a widely used  well-implemented client-server network file system here  we use it as an example to explore the implementation details of network file systems  nfs is both an implementation and a specification of a software system for accessing remote files across lans  or even wans   nfs is part of onc +  which most unix vendors and some pc operating systems support the implementation described here is part of the solaris operating system  which is a modified version of unix svr4 running on sun workstations and other hardware it uses either the tcp or udp /ip protocol  depending on the interconnecting network   the specification and the implementation are intertwined in our description of nfs whenever detail is needed  we refer to the sun implementation ; whenever the description is general  it applies to the specification also  there are multiple versions of nfs  with the latest being version 4 here  we describe version 3  as that is the one most commonly deployed  11.8.1 overview nfs views a set of interconnected workstations as a set of independent machines with independent file systems the goal is to allow some degree of sharing among these file systems  on explicit request  in a transparent manner sharing is based on a client-server relationship a machine may be  and often is  both a client and a server sharing is allowed between any pair of machines to ensure machine independence  sharing of a remote file system affects only the client machine and no other machine  so that a remote directory will be accessible in a transparent manner from a particular machine-say  from ml-a client of that machine must first carry out a mount operation the semantics of the operation involve mounting a remote directory over a directory of a local file system once the mount operation is completed  the mounted directory looks like an integral subtree of the local file system  replacing the subtree descending from the local directory the local directory becomes the name of the root of the newly mounted directory specification of the remote directory as an argument for the mount operation is not done transparently ; the location  or host name  of the remote directory has to be provided however  fron l then on  users on machine ml can access files in the remote directory in a totally transparent manner  to illustrate file mounting  consider the file system depicted in figure 11.13  where the triangles represent subtrees of directories that are of interest the figure shows three independent file systems of machines named u  51  and 52 at this point  on each machine  only the local files can be accessed figure 11.14  a  shows the effects of mounting 81  /usr/shared over u  /usr/local  this figure depicts the view users on u have of their file system notice that after the mount is complete  they can access any file within the dirl directory 11.8 491 u  s1  s2  usr usr usr figure 11.13 three independent file systems  using the prefix /usr /local/ dir1 the original directory /usr /local on that machine is no longer visible  subject to access-rights accreditation  any file system  or any directory within a file system  can be mounted remotely on top of any local directory  diskless workstations can even mount their own roots from servers cascading mounts are also permitted in some nfs implementations that is  a file system can be mounted over another file system that is remotely mounted  not local a machine is affected by only those mounts that it has itself invoked mounting a remote file system does not give the client access to other file systems that were  by chance  mounted over the former file system thus  the mount mechanism does not exhibit a transitivity property  in figure 11.14  b   we illustrate cascading mounts the figure shows the result of mounting s2  /usr /dir2 over u  /usr/local/dir1  which is already remotely mounted from 51 users can access files within dir2 on u using the u  u   a   b  figure 11.14 mounting in nfs  a  mounts  b  cascading mounts  492 chapter 11 prefix /usr/local/dir1 if a shared file system is mounted over a user 's home directories on all machines in a network  the user can log into any workstation and get his honce environment this property permits one of the design goals of nfs was to operate in a heterogeneous environment of different machines  operating systems  and network architectures  the nfs specification is independent of these media and thus encourages other implementations this independence is achieved through the use of rpc primitives built on top of an external data representation  xdr  protocol used between two implementation-independent interfaces hence  if the system consists of heterogeneous machines and file systems that are properly interfaced to nfs  file systems of different types can be mounted both locally and remotely  the nfs specification distinguishes between the services provided by a mount mechanism and the actual remote-file-access services accordingly  two separate protocols are specified for these services  a mount protocol and a protocol for remote file accesses  the the protocols are specified as sets of rpcs these rpcs are the building blocks used to implement transparent remote file access  11.8.2 the mount protocol the establishes the initial logical connection between a server and a client in sun 's implementation  each machine has a server process  outside the kernel  performing the protocol functions  a mount operation includes the name of the remote directory to be mounted and the name of the server machine storing it the mount request is mapped to the corresponding rpc and is forwarded to the mount server running on the specific server machine the server maintains an that specifies local file systems that it exports for mounting  along with names of machines that are permitted to mount them  in solaris  this list is the i etc/dfs/dfstab  which can be edited only by a superuser  the specification can also include access rights  such as read only to simplify the maintenance of export lists and mount tables  a distributed naming scheme can be used to hold this information and make it available to appropriate clients  recall that any directory within an exported file system can be mounted remotely by an accredited machine a component unit is such a directory when the server receives a mount request that conforms to its export list  it returns to the client a file handle that serves as the key for further accesses to files within the mounted file system the file handle contains all the information that the server needs to distinguish an individual file it stores in unix terms  the file handle consists of a file-system identifier and an inode number to identify the exact mounted directory within the exported file system  the server also maintains a list of the client machines and the corresponding currently mounted directories this list is used mainly for administrative purposes-for instance  for notifying all clients that the server is going down  only through addition and deletion of entries in this list can the server state be affected by the mount protocol  usually  a system has a static mounting preconfiguration that is established at boot time  i etc/vfstab in solaris  ; however  this layout can be modified in 11.8 493 addition to the actual mount procedure  the mount protocol includes several other procedures  such as unmount and return export list  11.8.3 the nfs protocol the nfs protocol provides a set of rpcs for remote file operations the procedures support the following operations  searching for a file within a directory reading a set of directory entries manipulating links and directories accessing file attributes reading and writing files these procedures can be invoked only after a file handle for the remotely mounted directory has been established  the omission of open   and close   operations is intentional a prominent feature of nfs servers is that they are stateless servers do not maintain information about their clients from one access to another no parallels to unix 's open-files table or file structures exist on the server side consequently  each request has to provide a full set of arguments  including a unique file identifier and an absolute offset inside the file for the appropriate operations  the resulting design is robust ; no special measures need be taken to recover a server after a crash file operations must be idempotent for this purpose  every nfs request has a sequence number  allowing the server to determine if a request is duplicated or if any are missing  maintaining the list of clients that we mentioned seems to violate the statelessness of the server howeve1 ~ this list is not essential for the correct operation of the client or the server  and hence it does not need to be restored after a server crash consequently  it might include inconsistent data and is treated as only a hint  a further implication of the stateless-server philosophy and a result of the synchrony of an rpc is that modified data  including indirection and status blocks  must be committed to the server 's disk before results are returned to the client that is  a client can cache write blocks  but when it flushes them to the server  it assumes that they have reached the server 's disks the server must write all nfs data synchronously thus  a server crash and recovery will be invisible to a client ; all blocks that the server is managing for the client will be intact the consequent performance penalty can be large  because the advantages of caching are lost performance can be increased using storage with its own nonvolatile cache  usually battery-backed-up memory   the disk controller ackiwwledges the disk write when the write is stored in the nonvolatile cache in essence  the host sees a very fast synchronous write  these blocks remain intact even after system crash and are written from this stable storage to disk periodically  a single nfs write procedure call is guaranteed to be atomic and is not intermixed with other write calls to the same file the nfs protocol  however  does not provide concurrency-control mechanisms a write   system call may 494 chapter 11 client server figure 11.15 schematic view of the nfs architecture  be broken down into several rpc writes  because each nfs write or read call can contain up to 8 kb of data and udp packets are limited to 1,500 bytes as a result  two users writing to the same remote file may get their data intermixed  the claim is that  because lock management is inherently stateful  a service outside the nfs should provide locking  and solaris does   users are advised to coordinate access to shared files using mechanisms outside the scope of nfs  nfs is integrated into the operating system via a vfs as an illustration of the architecture  let 's trace how an operation on an already open remote file is handled  follow the example in figure 11.15   the client initiates the operation with a regular system call the operating-system layer maps this call to a vfs operation on the appropriate vnode the vfs layer identifies the file as a remote one and invokes the appropriate nfs procedure an rpc call is made to the nfs service layer at the remote server this call is reinjected to the vfs layer on the remote system  which finds that it is local and invokes the appropriate file-system operation this path is retraced to return the result  an advantage of this architecture is that the client and the server are identical ; thus  a machine may be a client  or a server  or both the actual service on each server is performed by kernel threads  11.8.4 path-name translation in nfs involves the parsing of a path name such as /usr/local/dir1/file txt into separate directory entries  or components   1  usr   2  local  and  3  dir1 path-name translation is done by breaking the path into component names and perform.ing a separate nfs lookup call for every pair of component name and directory vnode once a n10unt point is crossed  every component lookup causes a separate rpc to the server this 11.8 495 expensive path-name-traversal scheme is needed  since the layout of each client 's logical name space is unique  dictated by the mounts the client has performed it would be itluch more efficient to hand a server a path name and receive a target vnode once a mount point is encountered at any point  however  there might be another mount point for the particular client of whicb the stateless server is unaware  so that lookup is fast  a directory-name-lookup cache on the client side holds the vnodes for remote directory names this cache speeds up references to files with the same initial path name the directory cache is discarded when attributes returned from the server do not match the attributes of the cached vnode  recall that mounting a remote file system on top of another already mounted remote file system  a cascading mount  is allowed in some implementations of nfs however  a server can not act as an intermediary between a client and another server instead  a client must establish a direct client-server com1ection with the second server by directly mounting the desired directory  when a client has a cascading mount  more than one server can be involved in a path-name traversal however  each component lookup is performed between the original client and some server therefore  when a client does a lookup on a directory on which the server has mounted a file system  the client sees the underlying directory instead of the mounted directory  11.8.5 remote operations with the exception of opening and closing files  there is almost a one-to-one correspondence between the regular unix system calls for file operations and the nfs protocol rpcs thus  a remote file operation can be translated directly to the corresponding rpc conceptually  nfs adheres to the remote-service paradigm ; but in practice  buffering and caching techniques are employed for the sake of performance no direct correspondence exists between a remote operation and an rpc instead  file blocks and file attributes are fetched by the rpcs and are cached locally future remote operations use the cached data  subject to consistency constraints  there are two caches  the file-attribute  inode-infonnation  cache and the file-blocks cache when a file is opened  the kernel checks with the remote server to determine whether to fetch or revalidate the cached attributes the cached file blocks are used only if the corresponding cached attributes are up to date the attribute cache is updated whenever new attributes arrive from the server cached attributes are  by default  discarded after 60 seconds both read-ahead and delayed-write techniques are used between the server and the client clients do not free delayed-write blocks until the server confirms that the data have been written to disk delayed-write is retained even when a file is opened concurrently  in conflicting modes hence  unix semantics  section 10.5.3.1  are not preserved  tuning the system for performance makes it difficult to characterize the consistency semantics of nfs new files created on a machine may not be visible elsewhere for 30 seconds furthermore  writes to a file at one site may or may not be visible at other sites that have this file open for reading new opens of a file observe only the changes that have already been flushed to the server thus  nfs provides neither strict emulation of unix semantics nor the 496 chapter 11 11.9 session sen antics of andrew  section 10.5.3.2  .ln spite of these drawbacks  the utility and good performance of the mechanism make it the most widely used multi-vendor-distributed system in operation  disk i/o has a huge impact on system performance as a result  file-system design and implementation command quite a lot of attention from system designers some file systems are general purpose  in that they can provide reasonable performance and functionality for a wide variety of file sizes  file types  and i/0 loads others are optimized for specific tasks in an attempt to provide better performance in those areas than general-purpose file systems  the wafl file system from network appliance is an example of this sort of optimization wafl  the write-anywhere file layout  is a powerful  elegant file system optimized for random writes  wafl is used exclusively on network file servers produced by network appliance and so is meant for use as a distributed file system it can provide files to clients via the nfs  cifs  ftp  and http protocols  although it was designed just for nfs and cifs when many clients use these protocols to talk to a file server  the server may see a very large demand for random reads and an even larger demand for random writes the nfs and cifs protocols cache data from read operations  so writes are of the greatest concern to file-server creators  wafl is used on file servers that include an nvram cache for writes  the wafl designers took advantage of running on a specific architecture to optimize the file system for random i/0  with a stable-storage cache in front  ease of use is one of the guiding principles of wafl  because it is designed to be used in an appliance its creators also designed it to include a new snapshot functionality that creates multiple read-only copies of the file system at different points in time  as we shall see  the file system is similar to the berkeley fast file system  with many modifications it is block-based and uses inodes to describe files each inode contains 16 pointers to blocks  or indirect blocks  belonging to the file described by the inode each file system has a root inode all of the metadata lives in files  all inodes are in one file  the free-block map in another  and the free-inode root inode 1 free blotk map i figure 11.16 the wafl file layout 11.9 497 map in a third  as shown in figure 11.16 because these are standard files  the data blocks are not limited in location and can be placed anywhere if a file system is expanded by addition of disks  the lengths of the metadata files are automatically expanded by the file systen  thus  a wafl file system is a tree of blocks with the root inode as its base to take a snapshot  wafl creates a copy of the root inode any file or metadata updates after that go to new blocks rather than overwriting their existing blocks the new root inode points to metadata and data changed as a result of these writes meanwhile  the snapshot  the old root inode  still points to the old blocks  which have not been updated it therefore provides access to the file system just as it was at the instant the snapshot was made-and takes very little disk space to do so ! in essence  the extra disk space occupied by a snapshot consists of just the blocks that have been modified since the snapshot was taken  an important change from more standard file systems is that the free-block map has more than one bit per block it is a bitmap with a bit set for each snapshot that is using the block when all snapshots that have been using the block are deleted  the bit map for that block is all zeros  and the block is free to be reused used blocks are never overwritten  so writes are very fast  because a write can occur at the free block nearest the current head location there are many other performance optimizations in wafl as well  many snapshots can exist simultaneously  so one can be taken each hour of the day and each day of the month a user with access to these snapshots can access files as they were at any of the times the snapshots were taken  the snapshot facility is also useful for backups  testing  versioning  and so on  wafl 's snapshot facility is very efficient in that it does not even require that copy-on-write copies of each data block be taken before the block is modified  other file systems provide snapshots  but frequently with less efficiency wafl snapshots are depicted in figure 11.17  newer versions of wafl actually allow read-write snapshots  known as ,.hj ' ' ' clones are also efficient  using the same techniques as shapshots in this case  a read-only snapshot captures the state of the file system  and a clone refers back to that read-only snapshot any writes to the clone are stored in new blocks  and the clone 's pointers are updated to refer to the new blocks  the original snapshot is unmodified  still giving a view into the file system as it was before the clone was updated clones can also be promoted to replace the original file system ; this involves throwing out all of the old pointers and any associated old blocks clones are useful for testing and upgrades  as the original version is left untouched and the clone deleted when the test is done or if the upgrade fails  another feature that naturally falls from the wafl file system implementation is the duplication and synchronization of a set of data over a network to another system first  a snapshot of a wafl file system is duplicated to another system when another snapshot is taken on the source system  it is relatively easy to update the remote system just by sending over all blocks contained in the new snapshot these blocks are the ones that have changed between the times the two snapshots were taken the remote system adds these blocks to the file system and updates its pointers  and the new system then is a duplicate of the source system as of the time of the second snapshot repeating this process maintains the remote system as a nearly up-to-date copy of the first 498 chapter 11 11.10  a  before a snapshot   b  after a snapshot  before any blocks change   c  after block d has changed to o  figure 11.17 snapshots in wafl  system such replication is used for disaster recovery should the first system be destroyed  most of its data are available for use on the remote system  finally  we should note that sun 's zfs file system supports similarly efficient snapshots  clones  and replication  the file system resides permanently on secondary storage  which is designed to hold a large amount of data permanently the most common secondary-storage medium is the disk  physical disks may be segmented into partitions to control media use and to allow multiple  possibly varying  file systems on a single spindle  these file systems are mounted onto a logical file system architecture to make then available for use file systems are often implemented in a layered or modular structure the lower levels deal with the physical properties of storage devices upper levels deal with symbolic file names and logical properties of files intermediate levels map the logical file concepts into physical device properties  any file-system type can have different structures and algorithms a vfs layer allows the upper layers to deal with each file-system type uniformly even 499 remote file systems can be integrated into the system 's directory structure and acted on by standard system calls via the vfs interface  the various files can be allocated space on the disk in three ways  through contiguous  linked  or indexed allocation contiguous allocation can suffer from external fragmentation direct access is very inefficient with linked allocation indexed allocation may require substantial overhead for its index block these algorithms can be optimized in many ways contiguous space can be enlarged through extents to increase flexibility and to decrease external fragmentation indexed allocation can be done in clusters of multiple blocks to increase throughput and to reduce the number of index entries needed indexing in large clusters is similar to contiguous allocation with extents  free-space allocation methods also influence the efficiency of disk-space use  the performance of the file system  and the reliability of secondary storage  the methods used include bit vectors and linked lists optimizations include grouping  countilcg  and the fat  which places the linked list in one contiguous area  directory-management routines must consider efficiency  performance  and reliability a hash table is a commonly used method  as it is fast and efficient unfortunately  damage to the table or a system crash can result in inconsistency between the directory information and the disk 's contents  a consistency checker can be used to repair the damage operating-system backup tools allow disk data to be copied to tape  enabling the user to recover from data or even disk loss due to hardware failure  operating system bug  or user error  network file systems  such as nfs  use client-server methodology to allow users to access files and directories from remote machines as if they were on local file systems system calls on the client are translated into network protocols and retranslated into file-system operations on the server  networking and multiple-client access create challenges in the areas of data consistency and performance  due to the fundamental role that file systems play in system operation  their performance and reliability are crucial techniques such as log structures and cachirtg help improve performance  while log structures and raid improve reliability the wafl file system is an example of optimization of performance to match a specific i/o load  11.1 in what situations would using memory as a ram disk be more useful than using it as a disk cache 11.2 consider a file systenc that uses a modifed contiguous-allocation scheme with support for extents a file is a collection of extents  with each extent corresponding to a contiguous set of blocks a key issue in such systems is the degree of variability in the size of the 500 chapter 11 extents what are the advantages and disadvantages of the following schemes a all extents are of the same size  and the size is predetermined  b extents can be of any size and are allocated dynamically  c extents can be of a few fixed sizes  and these sizes are predetermined  11.3 some file systems allow disk storage to be allocated at different levels of granularity for instance  a file system could allocate 4 kb of disk space as a single 4-kb block or as eight 512-byte blocks how could we take advantage of this flexibility to improve performance what modifications would have to be made to the free-space management scheme in order to support this feature 11.4 what are the advantages of the variant of linked allocation that uses a fat to chain together the blocks of a file 11.5 consider a file currently consisting of 100 blocks assume that the filecontrol block  and the index block  in the case of indexed allocation  is already in memory calculate how many disk i/0 operations are required for contiguous  linked  and indexed  single-level  allocation strategies  if  for one block  the following conditions hold in the contiguous-allocation case  assume that there is no room to grow at the beginning but there is room to grow at the end also assume that the block information to be added is stored in memory  a the block is added at the beginning  b the block is added in the middle  c the block is added at the end  d the block is removed from the beginning  e the block is removed from the middle  f the block is removed from the end  11.6 consider a file system that uses inodes to represent files disk blocks are 8 kb in size  and a pointer to a disk block requires 4 bytes this file system has 12 direct disk blocks  as well as single  double  and triple indirect disk blocks what is the maximum size of a file that can be stored in this file system 11.7 assume that in a particular augmentation of a reinote-file-access protocol  each client maintains a name cache that caches translations from file names to corresponding file handles what issues should we take into account in implementing the name cache 11.8 consider the following backup scheme  day 1 copy to a backup medium all files from the disk  day 2 copy to another m.edium all files changed since day 1  day 3 copy to another medium all files changed since day 1  501 this differs from the schedule given in section 11.7.4 by having all subsequent backups copy all files modified since the first full backup  what are the benefits of this system over the one in section 11.7.4 what are the drawbacks are restore operations made easier or more difficult explain your answer  11.9 why must the bit map for file allocation be kept on mass storage  rather than in main memory 11.10 consider a file system on a disk that has both logical and physical block sizes of 512 bytes assume that the information about each file is already in memory for each of the three allocation strategies  contiguous  linked  and indexed   answer these questions  a how is the logical-to-physical address mapping accomplished in this system  for the indexed allocation  assume that a file is always less than 512 blocks long  b if we are currently at logical block 10  the last block accessed was block 10  and want to access logical block 4  how many physical blocks must be read from the disk 11.11 why is it advantageous to the user for an operating system to dynamically allocate its internal tables what are the penalties to the operating system for doing so 11.12 explain why logging metadata updates ensures recovery of a file system after a file-system crash  11.13 fragmentation on a storage device can be eliminated by recompaction of the information typical disk devices do not have relocation or base registers  such as those used when memory is to be compacted   so how can we relocate files give three reasons why recompacting and relocation of files are often avoided  11.14 consider a system where free space is kept in a free-space list  a suppose that the pointer to the free-space list is lost can the system reconstruct the free-space list explain your answer  b consider a file system similar to the one used by unix with indexed allocation how many disk i/0 operations might be 502 chapter 11 required to read the contents of a small local file at /a/b/c assume that none of the disk blocks is currently being cached  c suggest a scheme to ensure that the pointer is never lost as a result of memory failure  11.15 one problem with contiguous allocation is that the user must preallocate enough space for each file if the file grows to be larger than the space allocated for it  special actions must be taken one solution to this problem is to define a file structure consisting of an initial contiguous area  of a specified size   if this area is filled  the operating system automatically defines an overflow area that is linked to the initial contiguous area if the overflow area is filled  another overflow area is allocated compare this implementation of a file with the standard contiguous and linked implementations  11.16 discuss how performance optimizations for file systems might result in difficulties in maintaining the consistency of the systems in the event of com.puter crashes  the ms-dos fat system is explained in norton and wilton  1988   and the os/2 description can be found in iacobucci  1988   these operating systems use the intel 8086 cpus  intel  1985b   intel  1985a   intel  1986   and intel  1990    ibm allocation methods are described in deitel  1990   the internals of the bsd unl ' system are covered in full in mckusick et al  1996   mcvoy and kleiman  1991  discusses optimizations of these methods made in solaris the coogle file system is described in ghemawat et al  2003   fuse can be found at http  / /fuse.sourceforge.net/  disk file allocation based on the buddy system is covered in koch  1987   a file-organization scheme that guarantees retrieval in one access is described by larson and kajla  1984   log-structured file organizations for enhancing both performance and consistency are discussed in rosenblum and ousterhout  1991   seltzer et al  1993   and seltzer et al  1995   algorithms such as balanced trees  and much more  are covered by knuth  1998  and carmen et al  2001   the zfs source code for space maps can be found at http  //src.opensolaris.org/source/xref/onnv/onnvgate/ usr i src/uts/ common/ fs/ zfs/ space_map.c  disk caching is discussed by mckeon  1985  and smith  1985   caching in the experimental sprite operating system is described in nelson et al  1988   general discussions concerning mass-storage technology are offered by chi  1982  and hoagland  1985   folk and zoellick  1987  covers the gamut of file structures silvers  2000  discusses implementing the page cache in the netbsd operating system  the network file system  nfs  is discussed in sandberg et al  1985   sandberg  1987   sun  1990   and callaghan  2000   nfs version 4 is a standard described at http  / /www.ietf.org/rfc/rfc3530.txt the characteristics of 503 workloads in distributed file systems are examined in baker et al  1991   ousterhout  1991  discusses the role of distributed state in networked file systems log-structured designs for networked file systems are proposed in hartman and ousterhout  1995  and thekkath et al  1997   nfs and the unix file system  ufs  are described in vahalia  1996  and mauro and mcdougall  2007   the windows nt file system  ntfs  is explained in solomon  1998   the ext2 file system used in linux is described in bovet and cesati  2002  and the wafl file system in hitz et al  1995   zfs documentation can be found at http  / /www.opensolaris.org/ os/ community /zfs/ docs  12.1 the file system can be viewed logically as consisting of three parts in chapter 10  we examined the user and programmer interface to the file system in chapter 11  we described the internal data structures and algorithms used by the operating system to implement this interface in this chapter  we discuss the lowest level of the file system  the secondary and tertiary storage structures we first describe the physical structure of magenetic disks and magnetic tapes we then describe disk-scheduling algorithms  which schedule the order of disk i/ os to improve performance next  we discuss disk formatting and management of boot blocks  damaged blocks  and swap space we then examine secondary storage structure  covering disk reliability and stablestorage implementation we conclude with a brief description of tertiary storage devices and the problems that arise when an operating system uses tertiary storage  to describe the physical structure of secondary and tertiary storage devices and its effects on the uses of the devices  to explain the performance characteristics of mass-storage devices  to discuss operating-system services provided for mass storage  including raid and hsm  in this section  we present a general overview of the physical structure of secondary and tertiary storage devices  12.1.1 magnetic disks provide the bulk of secondary storage for modern computer systems conceptually  disks are relatively simple  figure 12.1   each disk platter has a flat circular shape  like a cd common platter diameters range 505 506 chapter 12 arm assembly rotation figure 12.1 moving-head disk mechanism  from 1.8 to 5.25 inches the two surfaces of a platter are covered with a magnetic material we store information by recording it magnetically on the platters  a read -write head flies just above each surface of every platter the heads are attached to a that moves all the heads as a unit the surface of a platter is logically divided into circular which are subdivided into the set of tracks that are at one arm position makes up a there may be thousands of concentric cylinders in a disk drive  and each track may contain hundreds of sectors the storage capacity of common disk drives is measured iil gigabytes  when the disk is in use  a drive motor spins it at high speed most drives rotate 60 to 200 times per second disk speed has two parts the is the rate at which data flow between the drive and the computer the sometimes called the consists of the time necessary to move the disk arm to the desired cylinder  called the and the time necessary for the desired sector to rotate to the disk head  called the typical disks can transfer several megabytes of data per second  and they seek times and rotational latencies of several milliseconds  because the disk head flies on an extremely thin cushion of air  measured in microns   there is a danger that the head will make contact with the disk surface although the disk platters are coated with a thin protective laye1 ~ the head will sometimes damage the magnetic surface this accident is called a a head crash normally can not be repaired ; the entire disk must be replaced  a disk can be allowing different disks to be mounted as needed  removable magnetic disks generally consist of one platter  held in a plastic case to prevent damage while not in the disk drive are inexpensive removable magnetic disks that have a soft plastic case containing a flexible platter the head of a floppy-disk drive generally sits directly on the disk 12.1 507 disk transfer rates as with many aspects of computingf published performance numbers for disks are not the same as real-world performance numbers stated transfer rates are always lower than for example the transfer rate may be the rate at which bits can be read from the magnetic media by the disk head  but that is different from the rate at which blocks are delivered to the operating system  surface  so the drive is designed to rotate more slowly than a hard-disk drive to reduce the wear on the disk surface the storage capacity of a floppy disk is typically only 1.44mb or so removable disks are available that work much like normal hard disks and have capacities measured in gigabytes  a disk drive is attached to a computer by a set of wires called an several kinds of buses are available  including buses the data transfers on a bus are carried out by special electronic processors called the is the controller at the computer end of the bus a is built into each disk drive to perform a disk i/0 operation  the computer places a command into the host controller  typically using memory-mapped i/0 portsf as described in section 9.7.3 the host controller then sends the command via messages to the disk controller  and the disk controller operates the disk-drive hardware to carry out the command disk controllers usually have a built-in cache data transfer at the disk drive happens between the cache and the disk surface  and data transfer to the host  at fast electronic speeds  occurs between the cache and the host controller  12.1.2 magnetic tapes was used as an early secondary-storage medium although it is relatively permanent and can hold large quantities of dataf its access time is slow compared with that of main memory and magnetic disk in addition  random access to magnetic tape is about a thousand times slower than random access to magnetic disk  so tapes are not very useful for secondary storage  tapes are used mainly for backup  for storage of infrequently used information  and as a medium for transferring information from one system to another  a tape is kept in a spool and is wound or rewound past a read-write head  moving to the correct spot on a tape can take minutes  but once positioned  tape drives can write data at speeds comparable to disk drives tape capacities vary greatly  depending on the particular kind of tape drive typically  they store from 20gb to 200gb some have built-in compression that can more than double the effective storage tapes and their drivers are usually categorized by width  includil1.g 4  8f and 19 millimeters and 1/4 and 1/2 inch some are named according to technology  such as lt0-2 and sdlt tape storage is further described in section 12.9  508 chapter 12 12.2 fire wire refers to an interface designed for connecting peripheral devices such as hard drives  dvd drives  and digital video cameras to a computer system fire wire was first developed by apple computer and became the ieee 1394 standard in 1995 the originalfirewire standard provided bandwidth up to 400 megabits per second recently  a new standardfirewire 2-has emerged and is identified by the ieee 1394b standard  firewire 2 provides double the data rate of the original firewire-800 megabits per second  modern disk drives are addressed as large one-dimensional arrays of where the logical block is the smallest unit of transfer the size of a logical block is usually 512 bytes  although some disks can be to have a different logical block size  such as 1,024 bytes this option is described in section 12.5.1 the one-dimensional array of logical blocks is mapped onto the sectors of the disk sequentially sector 0 is the first sector of the first track on the outermost cylinder the mapping proceeds in order through that track  then through the rest of the tracks in that cylinder  and then through the rest of the cylinders from outermost to innermost  by using this mapping  we can -at least in theory-convert a logical block number into an old-style disk address that consists of a cylinder number  a track number within that cylinder  and a sector number within that track in practice  it is difficult to perform this translation  for two reasons first  most disks have some defective sectors  but the mapping hides this by substituting spare sectors from elsewhere on the disk second  the number of sectors per track is not a constant on smne drives  let 's look more closely at the second reason on media that use the density of bits per track is uniform the farther a track is from the center of the disk  the greater its length  so the more sectors it can hold as we move from outer zones to inner zones  the number of sectors per track decreases tracks in the outermost zone typically hold 40 percent more sectors than do tracks in the innermost zone the drive increases its rotation speed as the head moves from the outer to the inner tracks to keep the same rate of data moving under the head this method is used in cd-rom and dvd-rom drives alternatively  the disk rotation speed can stay constant ; in this case  the density of bits decreases from inner tracks to outer tracks to keep the data rate constant this method is used in hard disks and is known as the number of sectors per track has been increasing as disk technology improves  and the outer zone of a disk usually has several hundred sectors per track similarly  the number of cylinders per disk has been increasing ; large disks have tens of thousands of cylinders  12.3 12.3 509 computers access disk storage in two ways one way is via i/o ports  or this is common on small systems the other way is via a remote host in a distributed file system ; this is referred to as 12.3.1 host-attached storage host-attached storage is storage accessed through local i/0 ports these ports use several technologies the typical desktop pc uses an i/0 bus architecture called ide or ata this architecture supports a maximum of two drives per i/0 bus a newer  similar protocol that has simplified cabling is sata high-end workstations and servers generally use more sophisticated i/0 architectures  such as scsi and fiber charmel  fc   scsi is a bus architecture its physical medium is usually a ribbon cable with a large number of conductors  typically 50 or 68   the scsi protocol supports a maximum of 16 devices per bus generally  the devices include one controller card in the host  the and up to 15 storage devices  the to.rgr    ts   a scsi disk is a common scsi target  but the protocol provides the ability to address up to 8 in each scsi target a typical use of logical unit addressing is to commands to components of a raid array or components of a removable media library  such as a cd jukebox sendil g commands to the media-changer mechanism or to one of the drives   fc is a high-speed serial architecture that can operate over optical fiber or over a four-conductor copper cable it has two variants one is a large switched fabric having a 24-bit address space this variant is expected to dominate in the future and is the basis of  sjld '  ; s   discussed in section 12.3.3 because of the large space and the switched nature of the communication  multiple hosts and storage devices can attach to the fabric  allowing great flexibility in i/0 communication the other fc variant is an that can address 126 devices  drives and controllers   a wide variety of storage devices are suitable for use as host-attached storage among these are hard disk drives  raid arrays  and cd  dvd  and tape drives the i/0 commands that initiate data transfers to a host-attached storage device are reads and writes of logical data blocks directed to specifically identified storage units  such as bus id  scsi id  and target logical unit   12.3.2 network-attached storage a network-attached storage  nas  device is a special-purpose storage system that is accessed remotely over a data network  figure 12.2   clients access network-attached storage via a remote-procedure-call interface such as nfs for unix systems or cifs for windows machines the remote procedure calls  rpcs  are carried via tcp or udp over an ip network-usually the same local-area network  lan  that carries all data traffic to the clients the networkattached storage unit is usually implemented as a raid array with software that implements the rpc interface it is easiest to thil k of nas as simply another storage-access protocol for example  rather than using a scsi device driver and scsi protocols to access storage  a system using nas would use rpc over tcp /ip  510 chapter 12 12.4 lan/wan figure 12.2 network-attached storage  network-attached storage provides a convenient way for all the computers on a lan to share a pool of storage with the same ease of naming and access enjoyed with local host-attached storage however  it tends to be less efficient and have lower performance than some direct-attached storage options  is the latest network-attached storage protocol in essence  it uses the ip network protocol to carry the scsi protocol thus  networks-rather than scsi cables-can be used as the interconnects between hosts and their storage  as a result  hosts can treat their storage as if it were directly attached  even if the storage is distant from the host  12.3.3 storage-area network one drawback of network-attached storage systems is that the storage i/o operations consume bandwidth on the data network  thereby increasing the latency of network communication this problem can be particularly acute in large client-server installations-the communication between servers and clients competes for bandwidth with the communication among servers and storage devices  a storage-area network  san  is a private network  using storage protocols rather than networking protocols  connecting servers and storage units  as shown in figure 12.3 the power of a san lies in its flexibility multiple hosts and multiple storage arrays can attach to the same san  and storage can be dynamically allocated to hosts a san switch allows or prohibits access between the hosts and the storage as one example  if a host is running low on disk space  the san can be configured to allocate more storage to that host  sans make it possible for clusters of servers to share the same storage and for storage arrays to include multiple direct host com1.ections sans typically have more ports  and less expensive ports  than storage arrays  fc is the most common san interconnect  although the simplicity of iscsi is increasing its use an emerging alternative is a special-purpose bus architecture named infiniband  which provides hardware and software support for highspeed interconnection networks for servers and storage units  one of the responsibilities of the operating system is to use the hardware efficiently for the disk drives  meeting this responsibility entails having 12.4 511 figure 12.3 storage-area network  fast access time and large disk bandwidth the access time has two major components  also see section 12.1.1   the is the time for the disk arm to move the heads to the cylinder containing the desired sector the is the additional time for the disk to rotate the desired sector to the disk head the disk is the total number of bytes transferred  divided by the total time between the first request for service and the completion of the last transfer we can improve both the access time and the bandwidth by managing the order in which disk i/o requests are serviced  whenever a process needs i/0 to or from the disk  it issues a system call to the operating system the request specifies several pieces of information  whether this operation is input or output what the disk address for the transfer is what the memory address for the transfer is what the number of sectors to be transferred is if the desired disk drive and controller are available  the request can be serviced immediately if the drive or controller is busy  any new requests for service will be placed in the queue of pending requests for that drive  for a multiprogramming system with many processes  the disk queue may often have several pending requests thus  when one request is completed  the operating system chooses which pending request to service next how does the operating system make this choice any one of several disk-scheduling algorithms can be used  and we discuss them next  12.4.1 fcfs scheduling the simplest form of disk scheduling is  of course  the first-come  first-served  fcfs  algorithm this algorithm is intrinsically fair  but it generally does not provide the fastest service consider  for example  a disk queue with requests for i/0 to blocks on cylinders 98  183  37  122  14  124  65  67  512 chapter 12 queue = 98  183,37,122  14,124,65,67 head starts at 53 0 14 37 536567 98 122124 figure 12.4 fcfs disk scheduling  183199 in that order if the disk head is initially at cylinder 53  it will first move from 53 to 98  then to 183  37  122  14  124  65  and finally to 67  for a total head movement of 640 cylinders this schedule is diagrammed in figure 12.4  the wild swing from 122 to 14 and then back to 124 illustrates the problem with this schedule if the requests for cylinders 37 and 14 could be serviced together  before or after the requests for 122 and 124  the total head movement could be decreased substantially  and performance could be thereby improved  12.4.2 sstf scheduling it seems reasonable to service all the requests close to the current head position before moving the head far to service other this assumption is the basis for the the sstf algorithm selects the request with the least seek time from the current head position  since seek time increases with the number of cylinders traversed by the head  sstf chooses the pending request closest to the current head position  for our example request queue  the closest request to the initial head position  53  is at cylinder 65 once we are at cylinder 65  the next closest request is at cylinder 67 from there  the request at cylinder 37 is closer than the one at 98  so 37 is served next continuing  we service the request at cylinder 14  then 98  122  124  and finally 183  figure 12.5   this scheduling method results in a total head movement of only 236 cylinders-little more than one-third of the distance needed for fcfs scheduling of this request queue clearly  this algorithm gives a substantial improvement in performance  sstf scheduling is essentially a form of shortest-job-first  sjf  scheduling ; and like sjf scheduling  it may cause starvation of some requests remember that requests may arrive at any time suppose that we have two requests in the queue  for cylinders 14 and 186  and while the request from 14 is being serviced  a new request near 14 arrives this new request will be serviced next  making the request at 186 wait while this request is being serviced  another request close to 14 could arrive in theory  a continual stream of requests near one another could cause the request for cylinder 186 to wait indefinitely  12.4 queue = 98  183  37  122  14  124  65  67 head starts at 53 0 14 37 536567 98 122124 figure 12.5 sstf disk scheduling  513 183199 this scenario becomes increasingly likely as the pending-request queue grows longer  although the sstf algorithm is a substantial improvement over the fcfs algorithm  it is not optimal in the example  we can do better by moving the head from 53 to 37  even though the latter is not closest  and then to 14  before turning around to service 65  67  98  122  124  and 183 this strategy reduces the total head movement to 208 cylinders  12.4.3 scan scheduling in the toward the end  servicing requests as it reaches each cylinder  until it gets to the other end of the disk at the other end  the direction of head movement is reversed  and servicing continues the head continuously scans back and forth across the disk the scan algorithm is sometimes called the since the disk arm behaves just like an elevator in a building  first servicing all the requests going up and then reversing to service requests the other way  let 's return to our example to illustrate before applying scan to schedule the requests on cylinders 98  183,37  122  14  124  65  and 67  we need to know the direction of head movement in addition to the head 's current position  assuming that the disk arm is moving toward 0 and that the initial head position is again 53  the head will next service 37 and then 14 at cylinder 0  the arm will reverse and will move toward the other end of the disk  servicil lg the requests at 65  67  98  122  124  and 183  figure 12.6   if a request arrives in the queue just in front of the head  it will be serviced almost immediately ; a request arriving just behind the head will have to wait until the arm moves to the end of the disk  reverses direction  and comes back  assuming a uniform distribution of requests for cylinders  consider the density of requests when the head reaches one end and reverses direction at this point  relatively few requests are immediately in front of the head  since these cylinders have recently been serviced the heaviest density of requests 514 chapter 12 queue = 98  183,37,122  14,124,65,67 head starts at 53 0 14 37 536567 98 122124 figure 12.6 scan disk scheduling  183199 is at the other end of the disk these requests have also waited the longest so why not go there first that is the idea of the next algorithm  12.4.4 c-scan scheduling is a variant of scan designed to provide a more uniform wait time like scan  c-scan moves the head from one end of the disk to the other  servicing requests along the way when the head reaches the other end  however  it immediately returns to the beginning of the disk without servicing any requests on the return trip  figure 12.7   the c-scan scheduling algorithm essentially treats the cylinders as a circular list that wraps around from the final cylinder to the first one  queue = 98  183  37  122  14  124  65  67 head starts at 53 0 1 4 37 53 65 67 98 1 22 1 24 figure 12.7 c-scan disk scheduling  183199 12.4 queue = 98  183  37  122  14  124  65  67 head starts at 53 0 14 37 536567 98 122124 figure 12.8 c-look disk scheduling  12.4.5 look scheduling 515 183199 as we described themf both scan and c-scan move the disk arm across the full width of the disk in practicef neither algorithm is often implemented this way more commonlyf the arm goes only as far as the final request in each direction then  it reverses direction immediatelyf without going all the way to the end of the disk versions of scan and c-scan that follow this pattern are called and because they look for a request before continuing to move in a given direction  figure 12.8   12.4.6 selection of a disk-scheduling algorithm given so many disk-scheduling algorithmsf how do we choose the best one sstf is common and has a natural appeal because it increases performance over fcfs scan and c-scan perform better for systems that place a heavy load on the diskf because they are less likely to cause a starvation problem for any particular list of requestsf we can define an optimal order of retrievat but the computation needed to find an optimal schedule may not justify the savings over sstf or scan with any scheduling algoritlunf howeverf performance depends heavily on the number and types of requests for instance  suppose that the queue usually has just one outstanding request thenf all scheduling algorithms behave the samef because they have only one choice of where to move the disk head  they all behave like fcfs scheduling  requests for disk service can be greatly influenced by the file-allocation method a program reading a contiguously allocated file will generate several requests that are close together on the disk  resulting in limited head movement  a linked or indexed fik in contrastf may include blocks that are widely scattered on the diskf resulting in greater head movement  the location of directories and index blocks is also important since every file must be opened to be usedf and opening a file requires searching the directory structuref the directories will be accessed frequently suppose that a directory entry is on the first cylinder and a filef s data are on the final cylinder in this casef the disk head has to move the entire width of the disk if the directory 516 chapter 12 12.5 entry were on the middle cylinder  the head would have to move only one-half the width caching the directories and index blocks in main memory can also help to reduce disk-arm movement particularly for read requests  because of these complexities  the disk-scheduling algorithm should be written as a separate module of the operating system  so that it can be replaced with a different algorithm if necessary either sstf or look is a reasonable choice for the default algorithm  the scheduling algorithms described here consider only the seek distances  for modern disks  the rotational latency can be nearly as large as the average seek time it is difficult for the operating system to schedule for improved rotational latency  though  because modern disks do not disclose the physical location of logical blocks disk manufacturers have been alleviating this problem by implementing disk-scheduling algorithms in the controller hardware built into the disk drive if the operating system sends a batch of requests to the controller  the controller can queue them and then schedule them to improve both the seek time and the rotational latency  if i/o performance were the only consideration  the operating system would gladly turn over the responsibility of disk scheduling to the disk hardware  in practice  however  the operating system may have other constraints on the service order for requests for instance  demand paging may take priority over application i/0  and writes are more urgent than reads if the cache is running out of free pages also  it may be desirable to guarantee the order of a set of disk writes to make the file system robust in the face of system crashes  consider what could happen if the operating system allocated a disk page to a file and the application wrote data into that page before the operating system had a chance to flush the modified inode and free-space list back to disk to accommodate such requirements  an operating system may choose to do its own disk scheduling and to spoon-feed the requests to the disk controller  one by one  for some types of i/0  the operating system is responsible for several other aspects of disk management  too here we discuss disk initialization  booting from disk  and bad-block recovery  12.5.1 disk formatting a new magnetic disk is a blank slate  it is just a platter of a magnetic recording material before a disk can store data  it must be divided into sectors that the disk controller can read and write this process is called or low-level formatting fills the disk with a special data structure for each sector the data structure for a sector typically consists of a header  a data area  usually 512 bytes in size   and a trailer the header and trailer contain information used by the disk controller  such as a sector number and an  when the controller writes a sector of data during normal i/0  the ecc is updated with a value calculated from all the bytes in the data area when the sector is read  the ecc is recalculated and compared with the stored value if the stored and calculated numbers are different  this 12.5 517 mismatch indicates that the data area of the sector has become corrupted and that the disk sector may be bad  section 12.5.3   the ecc is an error-correcting code because it contains enough information  if only a few bits of data have been corrupted  to enable the controller to identify which bits have changed and calculate what their correct values should be it then reports a recoverable  the controller automatically does the ecc processing whenever a sector is read or written  most hard disks are low-level-formatted at the factory as a part of the manufacturing process this formatting enables the manufacturer to test the disk and to initialize the mapping from logical block numbers to defect-free sectors on the disk for many hard disks  when the disk controller is instructed to low-level-format the disk  it can also be told how many bytes of data space to leave between the header and trailer of all sectors it is usually possible to choose among a few sizes  such as 256,512  and 1,024 bytes formatting a disk with a larger sector size means that fewer sectors can fit on each track ; but it also means that fewer headers and trailers are written on each track and more space is available for user data some operating systems can handle only a sector size of 512 bytes  before it can use a disk to hold files  the operating system still needs to record its own data structures on the disk it does so in two steps the first step is to the disk into one or more groups of cylinders the operatiltg system can treat each partition as though it were a separate disk for instance  one partition can hold a copy of the operating system 's executable code  while another holds user files the second step is icgicz ; i or creation of a file system in this step  the operating system stores the iltitial file-system data structures onto the disk these data structures may include maps of free and allocated space  a fat or inodes  and an initial empty directory  to increase efficiency  most file systems group blocks together into larger chunks  frequently called disk i/0 is done via blocks  but file system ii 0 is done via clusters  effectively assuring that ii 0 has more sequential-access and fewer random-access characteristics  some operating systems give special programs the ability to use a disk partition as a large sequential array of logical blocks  without any file-system data structures this array is sometimes called the raw disk  and ii 0 to this array is termed raw l/0 for example  some database systems prefer raw iio because it enables them to control the exact disk location where each database record is stored raw l/0 bypasses all the file-system services  such as the buffer cache  file locking  prefetching  space allocation  file names  and directories we can make certain applications more efficient by allowing them to implement their own special-purpose storage services on a raw partition  but most applications perform better when they use the regular file-system services  12.5.2 boot block for a computer to start running-for instance  when it is powered up or rebooted -it must have an initial program to run this initial bootstrap program tends to be simple it initializes all aspects of the system  from cpu registers to device controllers and the contents of main memory  and then starts the operating system to do its job  the bootstrap program finds the 518 chapter 12 operating-system kernel on disk  loads that kernel into memory  and jumps to an initial address to begin the operating-system execution  for most computers  the bootstrap is stored in this location is convenient  because rom needs no initialization and is at a fixed location that the processor can start executing when powered up or reset and  since rom is read only  it can not be infected by a computer virus the problem is that changing this bootstrap code requires changing the rom hardware chips  for this reason  most systems store a tiny bootstrap loader program in the boot rom whose only job is to bring in a full bootstrap program from disk the full bootstrap program can be changed easily  a new version is simply written onto the disk the full bootstrap program is stored in the boot blocks at a fixed location on the disk a disk that has a boot partition is called a or the code in the boot rom instructs the disk controller to read the boot blocks into memory  no device drivers are loaded at this point  and then starts executing that code the full bootstrap program is more sophisticated than the bootstrap loader in the boot rom ; it is able to load the entire operating system from a non-fixed location on disk and to start the operating system ruru1ing  even so  the full bootstrap code may be small  let 's consider as an example the boot process in windows 2000 the windows 2000 system places its boot code in the first sector on the hard disk  which it terms the or furthermore  windows 2000 allows a hard disk to be divided into one or more partitions ; one partition  identified as the contains the operating system and device drivers bootil1g begins in a windows 2000 system by running code that is resident in the system 's rom memory this code directs the system to read the boot code from the mbr in addition to containing boot code  the mbr contains a table listing the partitions for the hard disk and a flag indicating which partition the system is to be booted from  as illustrated in figure 12.9  once the system identifies the boot partition  it reads the first sector from that partition  which is called the and contilmes with the remainder of the boot process  which includes loading the various subsystems and system services  mbr partition 1 partition 2 partition 3 partition 4 boot code partition table boot partition figure 12.9 booting from disk in windows 2000  12.5 519 12.5.3 bad blocks because disks have moving parts and small tolerances  recall that the disk head flies just above the disk surface   they are prone to failure sometimes the failure is complete ; in this case  the disk needs to be replaced and its contents restored from backup media to the new disk more frequently  one or more sectors become defective most disks even con'le from the factory with depending on the disk and controller in use  these blocks are handled in a variety of ways  on simple disks  such as some disks with ide controllers  bad blocks are handled manually for instance  the ms-dos format command performs logical formatting and  as a part of the process  scans the disk to find bad blocks if format finds a bad block  it writes a special value into the corresponding fat entry to tell the allocation routines not to use that block if blocks go bad during normal operation  a special program  such as chkdsk  must be run manually to search for the bad blocks and to lock them away data that resided on the bad blocks usually are lost  more sophisticated disks  such as the scsi disks used in high-end pcs and most workstations and servers  are smarter about bad-block recovery the controller maintains a list of bad blocks on the disk the list is initialized during the low-level formatting at the factory and is updated over the life of the disk  low-level formatting also sets aside spare sectors not visible to the operating system the controller can be told to replace each bad sector logically with one of the spare sectors this scheme is known as or a typical bad-sector transaction might be as follows  the operating system tries to read logical block 87  the controller calculates the ecc and finds that the sector is bad it reports this finding to the operating system  the next time the system is rebooted  a special command is run to tell the scsi controller to replace the bad sector with a spare  after that  whenever the system requests logical block 87  the request is translated into the replacement sector 's address by the controller  note that such a redirection by the controller could invalidate any optimization by the operating system 's disk-scheduling algorithm ! for this reason  most disks are formatted to provide a few spare sectors in each cylinder and a spare cylinder as well when a bad block is remapped  the controller uses a spare sector from the same cylinder  if possible  as an alternative to sector some controllers can be instructed to replace a bad block by here is an example  suppose that logical block 17 becomes defective and the first available spare follows sector 202 then  sector slipping remaps all the sectors front 17 to 202  moving them all down one spot that is  sector 202 is copied into the spare  then sector 201 into 202  then 200 into 201  and so on  until sector 18 is copied into sector 19  slipping the sectors in this way frees up the space of sector 18  so sector 17 can be mapped to it  the replacement of a bad block generally is not totally automatic because the data in the bad block are usually lost soft errors may trigger a process in 520 chapter 12 12.6 which a copy of the block data is made and the block is spared or slipped  an unrecoverable howeverf results in lost data whatever file was using th.at block must be repaired  for instancef by restoration from a backup tape  f and that requires manual intervention  swapping was first presented in section 8.2f where we discussed moving entire processes between disk and main memory swapping in that setting occurs when the amount of physical memory reaches a critically low point and processes are moved from memory to swap space to free available memory  in practicef very few modern operating systems implement swapping in this fashion rathel ~ systems now combine swapping with virtual memory techniques  chapter 9  and swap pagesf not necessarily entire processes in fact some systems now use the terms swapping and paging interchangeablyf reflecting the merging of these two concepts  is another low-level task of the operating system virtual memory uses disk space as an extension of main memory  since disk access is much slower than memory accessf using swap space significantly decreases system performance the main goal for the design and implementation of swap space is to provide the best throughput for the virtual memory system in this sectionf we discuss how swap space is usedf where swap space is located on diskf and how swap space is managed  12.6.1 swap-space use swap space is used in various ways by different operating systemsf depending on the memory-management algorithms in use for instancef systems that implement swapping may use swap space to hold an entire process imagef including the code and data segments paging systems may simply store pages that have been pushed out of main memory the amount of swap space needed on a system can therefore vary from a few megabytes of disk space to gigabytesf depending on the amow1.t of physical memoryf the amount of virtual memory it is backingf and the way in which the virtual memory is used  note that it may be safer to overestimate than to underestimate the amount of swap space requiredf because if a system runs out of swap space it may be forced to abort processes or may crash entirely overestimation wastes disk space that could otherwise be used for filesf but it does no other harm some systems recommend the amount to be set aside for swap space solarisf for examplef suggests setting swap space equal to the amount by which virtual memory exceeds pageable physical memory in the past linux has suggested setting swap space to double the amount of physical memoryf although most linux systems now use considerably less swap space in factf there is currently much debate in the linux community about whether to set aside swap space at all ! some operating systems-including linux-allow the use of multiple swap spaces these swap spaces are usually put on separate disks so that the load placed on the i/0 system by paging and swapping can be spread over the systemfs i/o devices  12.6 521 12.6.2 swap-space location a swap space can reside in one of two places  it can be carved out of the normal file system  or it can be in a separate disk partition if the swap space is simply a large file within the file system  normal file-system routines can be used to create it  name it and allocate its space this approach  though easy to implement is inefficient navigating the directory structure and the diskallocation data structures takes time and  possibly  extra disk accesses external fragmentation can greatly increase swapping times by forcing multiple seeks during reading or writing of a process image we can improve performance by caching the block location information in physical memory and by using special tools to allocate physically contiguous blocks for the swap file  but the cost of traversing the file-system data structures remains  alternatively  swap space can be created in a separate partition no file system or directory structure is placed in this space rather  a separate swap-space storage manager is used to allocate and deallocate the blocks from the raw partition this manager uses algorithms optimized for speed rather than for storage efficiency  because swap space is accessed much more frequently than file systems  when it is used   internal fragmentation may increase  but this trade-off is acceptable because the life of data in the swap space generally is much shorter than that of files in the file system since swap space is reinitialized at boot time  any fragmentation is short-lived the raw-partition approach creates a fixed amount of swap space during disk partitioning adding more swap space requires either repartitioning the disk  which involves moving the other file-system partitions or destroying them and restoring them from backup  or adding another swap space elsewhere  some operating systems are flexible and can swap both in raw partitions and in file-system space linux is an example  the policy and implementation are separate  allowing the machine 's administrator to decide which type of swapping to use the trade-off is between the convenience of allocation and management in the file system and the performance of swapping in raw partitions  12.6.3 swap-space management  an example we can illustrate how swap space is used by following the evolution of swapping and paging in various unix systems the traditional unix kernel started with an implementation of swapping that copied entire processes between contiguous disk regions and memory unix later evolved to a combination of swapping and paging as pagiltg hardware became available  in solaris 1  sunos   the designers changed standard unix methods to improve efficiency and reflect technological developments when a process executes  text-segment pages containing code are brought in from the file system  accessed in main memory  and thrown away if selected for pageout it is more efficient to reread a page from the file system than to write it to swap space and then reread it from there swap space is only used as a backing store for pages of memory  which includes memory allocated for the stack  heap  and uninitialized data of a process  more changes were made in later versions of solaris the biggest change is that solaris now allocates swap space only when a page is forced out of physical memory  rather than when the virtual memory page is first created  522 chapter 12 12.7 swap partition or swap file swap map 1---------swap area--------1 page i slot -1 l ~  ~ ---_l __ _l _ ~ figure 12.10 the data structures for swapping on linux systems  this scheme gives better performance on modern computers  which have more physical memory than older systems and tend to page less  linux is similar to solaris in that swap space is only used for anonymous memory or for regions of memory shared by several processes linux allows one or more swap areas to be established a swap area may be in either a swap file on a regular file system or a raw-swap-space partition each swap area consists of a series of 4-kb which are used to hold swapped pages  associated with each swap area is a .u1.2.p-an array of integer counters  each corresponding to a page slot in the swap area if the value of a counter is 0  the corresponding page slot is available values greater than 0 indicate that the page slot is occupied by a swapped page the value of the counter ij.l.dicates the number of mappings to the swapped page ; for example  a value of 3 indicates that the swapped page is mapped to three different processes  which can occur if the swapped page is storing a region of memory shared by three processes   the data structures for swapping on linux systems are shown in figure 12.10  disk drives have continued to get smaller and cheaper  so it is now economically feasible to attach many disks to a computer system having a large number of disks in a system presents opportunities for improving the rate at which data can be read or written  if the disks are operated in parallel furthermore  this setup offers the potential for improving the reliability of data storage  because redundant information can be stored on multiple disks thus  failure of one disk does not lead to loss of data a of disk-organization techniques  collectively called disks  raids   are commonly used to address the performance and reliability issues  in the past  raids composed of small  cheap disks were viewed as a cost-effective alternative to large  expensive disks ; today  raids are used for their higher reliability and higher data-transfer rate  rather than for economic reasons hence  the i in raid  which once stood for inexpensive/ ' now stands for ij.l.dependent  12.7.1 improvement of reliability via redundancy let us first consider the reliability of raids the chance that some disk out of a set of n disks will fail is much higher than the chance that a specific single 12.7 523 structuring raid raid storage can be structured in a variety of ways for example  a system can have disks directly attached to its buses in this case  the operating system or system software can implement raid flmctionality alternatively  an intelligent host controller can control multiple attached disks and can implement raid on those disks in hardware finally  a  or can be used a raid array is a standalone unit with its own controller  cache  usually   and disks it is attached to the host via one or more standard ata scsi or fc controllers this common setup allows any operating system and software without raid functionality to have raid-protected disks it is even used on systems that do have raid software layers because of its simplicity and flexibility  disk will fail suppose that the of a single disk is 100,000 hours then the mean time to failure of some disk in an array of 100 disks will be 100,000/100 = 1,000 hours  or 41.66 days  which is not long at all ! if we store only one copy of the data  then each disk failure will result in loss of a significant amount of data -and such a high rate of data loss is unacceptable  the solution to the problem of reliability is to introduce  we store extra information that is not normally needed but that can be used in the event of failure of a disk to rebuild the lost information thus  even if a disk fails  data are not lost  the simplest  but most expensive  approach to introducing redundancy is to duplicate every disk this technique is called with mirroring  a logical disk consists of two physical disks  and every write is carried out on both disks the result is called a mirrored volume if one of the disks in the volume fails  the data can be read from the other data will be lost only if the second disk fails before the first failed disk is replaced  the mean time to failure of a mirrored volume-where failure is the loss of data depends on two factors one is the mean time to failure of the individual disks the other is the which is the time it takes  on average  to replace a failed disk and to restore the data on it suppose that the failures of the two disks are that is  the failure of one disk is not connected to the failure of the other then  if the mean time to failure of a single disk is 100,000 hours and the mean time to repair is 10 hours  the of a mirrored disk system is 100  0002 /  2 10  = 500 106 hours  or 57,000 years ! you should be aware that the assumption of independence of disk failures is not valid power failures and natural disasters  such as earthquakes  fires  and floods  may result in damage to both disks at the same time also  manufacturing defects in a batch of disks can cause correlated failures as disks age  the probability of failure grows  increasing the chance that a second disk will fail while the first is being repaired in spite of all these considerations  however  n1.irrored-disk systems offer much higher reliability than do singledisk systems  power failures are a particular source of concern  since they occur far more frequently than do natural disasters even with mirroring of disks  if writes are 524 chapter 12 in progress to the same block in both disks  and power fails before both blocks are fully written  the two blocks can be in an inconsistent state one solution to this is to write one copy first then the next another is to add a cache to the raid array this write-back cache is protected from data loss during power failures  so the write can be considered complete at that point  assuming the nvram has some kind of error protection and correction  such as ecc or mirroring  12.7.2 improvement in performance via parallelism now let 's consider how parallel access to multiple disks improves performance  with disk mirroring  the rate at which read requests can be handled is doubled  since read requests can be sent to either disk  as long as both disks in a pair are functionat as is almost always the case   the transfer rate of each read is the same as in a single-disk system  but the number of reads per unit time has doubled  with multiple disks  we can improve the transfer rate as well  or instead  by striping data across the disks in its simplest form  consists of the bits of each byte across multiple disks ; such striping is called for example  if we have an array of eight disks  we write bit i of each byte to disk i the array of eight disks can be treated as a single disk with sectors that are eight times the normal size and  more important that have eight times the access rate in such an organization  every disk participates in every access  read or write  ; so the number of accesses that can be processed per second is about the same as on a single disk  but each access can read eight times as many data in the same time as on a single disk  bit-level striping can be generalized to include a number of disks that either is a multiple of 8 or divides 8 for example  if we use an array of four disks  bits i and 4 + i of each go to disk i further  striping need not occur at the bit level in for instance  blocks of a file are striped across multiple disks ; with n disks  block i of a file goes to disk  i mod n  + 1  other levels of striping  such as bytes of a sector or sectors of a block  also are possible block-level striping is the most common  parallelism in a disk system  as achieved through striping  has two main goals  increase the throughput of multiple small accesses  that is  page accesses  by load balancing  reduce the response time of large accesses  12.7.3 raid levels mirroring provides high reliability  but it is expensive striping provides high data-transfer rates  but it does not improve reliability numerous schemes to provide redundancy at lower cost by using disk striping combined with parity bits  which we describe next  l1.ave been proposed these schemes have different cost-performance trade-offs and are classified according to levels called we describe the various levels here ; figure 12.11 shows them pictorially  in the figure  p indicates error-correcting bits  and c 12.7 raid structure 525  a  raid 0  non-redundant striping   b  raid 1  mirrored disks   c  raid 2  memory-style error-correcting codes   d  raid 3  bit-interleaved parity   e  raid 4  block-interleaved parity   f  raid 5  block-interleaved distributed parity   g  raid 6  p + q redundancy  figure 12.11 raid levels  indicates a second copy of the data   in all cases depicted in the figure  four disks ' worth of data are stored  and the extra disks are used to store redundant information for failure recovery  raid level 0 raid level 0 refers to disk arrays with striping at the level of blocks but without any redundancy  such as mirroring or parity bits   as shown in figure 12.1l  a   raid ievell raid level1 refers to disk mirroring figure 12.1l  b  shows a mirrored organization  ' raid level2 raid level2 is also known as memory-style error-correctingcode  ecc  organization memory systems have long detected certain errors by using parity bits each byte in a memory system may have a parity bit associated with it that records whether the number of bits in the byte set to 1 is even  parity = 0  or odd  parity = 1   if one of the bits in the 526 chapter 12 byte is damaged  either a 1 becomes a 0  or a 0 becomes an the parity of the byte changes and thus does not match the stored parity similarly  if the stored parity bit is damaged  it does not match the computed parity thus  all single-bit errors are detected by the menwry system  error-correcting schemes store two or more extra bits and can reconstruct the data if a single bit is damaged the idea of ecc can be used directly in disk arrays via striping of bytes across disks for example  the first bit of each byte can be stored in disk 1  the second bit in disk 2  and so on until the eighth bit is stored in disk 8 ; the error-correction bits are stored in further disks this scheme is shown pictorially in figure 12.1l  c   where the disks labeled p store the error-correction bits if one of the disks fails  the remaining bits of the byte and the associated error-correction bits can be read from other disks and used to reconstruct the damaged data note that raid level 2 requires only three disks ' overhead for four disks of data  unlike raid level 1  which requires four disks ' overhead  raid level 3 raid level 3  or improves on level 2 by taking into account the fact that  unlike memory systems  disk controllers can detect whether a sector has been read correctly  so a single parity bit can be used for error correction as well as for detection the idea is as follows  if one of the sectors is damaged  we know exactly which sector it is  and we can figure out whether any bit in the sector is a 1 or a 0 by computing the parity of the corresponding bits from sectors in the other disks if the parity of the remaining bits is equal to the stored parity  the missing bit is 0 ; otherwise  it is 1 raid level3 is as good as level 2 but is less expensive in the number of extra disks required  it has only a one-disk overhead   so level 2 is not used in practice this scheme is shown pictorially in figure 12.1l  d   raid level 3 has two advantages over level 1 first  the storage overhead is reduced because only one parity disk is needed for several regular disks  whereas one mirror disk is needed for every disk in level1 second  since reads and writes of a byte are spread out over multiple disks with n-way striping of data  the transfer rate for reading or writing a single block is n times as fast as with raid level 1 on the negative side  raid level3 supports fewer l/os per second  since every disk has to participate in every i/0 request  a further performance problem with raid 3-and with all paritybased raid levels-is the expense of computing and writing the parity  this overhead results in significantly slower writes than with non-parity raid arrays to moderate this performance penalty  many raid storage arrays include a hardware controller with dedicated parity hardware this controller offloads the parity computation from the cpu to the array the array has an nvram cache as well  to store the blocks while the parity is computed and to buffer the writes from the controller to the spindles this combination can make parity raid almost as fast as non-parity in fact  a caching array doing parity raid can outperform a non-caching non-parity raid  raid level 4 raid level4  or uses block-level striping  as in raid 0  and in addition keeps a parity block on a separate disk for corresponding blocks from n other disks this scheme is 12.7 527 diagramed in figure 12.1l  e   if one of the disks fails  the parity block can be used with the corresponding blocks from the other disks to restore the blocks of the failed disk  a block read accesses only one disk  allowing other requests to be processed by the other disks thus  the data-transfer rate for each access is slowe1 ~ but multiple read accesses can proceed in parallel  leading to a higher overall i/0 rate the transfer rates for large reads are high  since all the disks can be read in parallel ; large writes also have high transfer rates  since the data and parity can be written in parallel  small independent writes can not be performed in parallel an operatingsystem write of data smaller than a block requires that the block be read  modified with the new data  and written back the parity block has to be updated as well this is known as the syti  .e  thus  a single write requires four disk accesses  two to read the two old blocks and two to write the two new blocks  wafl  chapter 11  uses raid level4 because this raid level allows disks to be added to a raid set seamlessly if the added disks are initialized with blocks containing all zeros  then the parity value does not change  and the raid set is still correct  raid levels raid levels  or  differs from level 4 by spreading data and parity among all n + 1 disks  rather than storing data in n disks and parity in one disk for each block  one of the disks stores the parity  and the others store data for example  with an array of five disks  the parity for the nth block is stored in disk  n mod 5  + 1 ; the nth blocks of the other four disks store actual data for that block this setup is shown in figure 12.11  f   where the ps are distributed across all the disks a parity block can not store parity for blocks in the same disk  because a disk failure would result in loss of data as well as of parity  and hence the loss would not be recoverable by spreading the parity across all the disks in the set  raid 5 avoids potential overuse of a single parity disk  which can occur with raid 4 raid 5 is the most common parity raid system  raid level 6 raid level 6  also called the is much like raid level 5 but stores extra redundant information to guard against disk failures instead of parity  error-correcting codes such as the are used in the scheme shown in figure 12.11  g   2 bits of redundant data are stored for every 4 bits of datacompared with 1 parity bit in level 5-and the system can tolerate two disk failures  raid levels 0 + 1 and 1 + 0 raid level 0 + 1 refers to a combination of raid levels 0 and 1 raid 0 provides the performance  while raid 1 provides the reliability generally  this level provides better performance than raid 5 it is common in enviromnents where both performance and reliability are important unfortunately  like raid 1  it doubles the number of disks needed for storage  so it is also relatively expensive in raid 0 + 1  a set of disks are striped  and then the stripe is mirrored to another  equivalent stripe  528 chapter 12 stripe a  raid 0 + 1 with a single disk failure  ua mirror b  raid 1 + 0 with a single disk failure  figure 12.12 raid 0 + 1 and 1 + 0  another raid option that is becoming available commercially is raid level 1 + 0  in which disks are mirrored in pairs and then the resulti.j.l.g mirrored pairs are striped this scheme has some theoretical advantages over raid 0 + 1 for example  if a single disk fails in raid 0 + 1  an entire stripe is inaccessible  leaving only the other stripe available with a failure in raid 1 + 0  a single disk is unavailable  but the disk that mirrors it is still available  as are all the rest of the disks  figure 12.12   numerous variations have been proposed to the basic raid schemes described here as a result  some confusion may exist about the exact definitions of the different raid levels  the implementation of raid is another area of variation consider the following layers at which raid can be implemented  volume-management software can implement raid within the kernel or at the system software layer in this case  the storage hardware can provide a minimum of features and still be part of a full raid solution parity raid is fairly slow when implemented in software  so typically raid 0  1  or 0 + 1 is used  raid can be implemented in the host bus-adapter  hba  hardware only the disks directly connected to the hba can be part of a given raid set  this solution is low in cost but not very flexible  12.7 529 raid can be implemented in the hardware of the storage array the storage array can create raid sets of various levels and can even slice these sets into smaller volumes  which are then presented to the operating system  the operating system need only implement the file system on each of the volumes arrays can have multiple connections available or can be part of a san  allowing multiple hosts to take advantage of the array 's features  raid can be implemented in the san interconnect layer by disk virtualization devices in this case  a device sits between the hosts and the storage  it accepts commands from the servers and manages access to the storage  it could provide mirroring  for example  by writing each block to two separate storage devices  other features  such as and replication  can be implemented at each of these levels as well involves the automatic duplication of writes between separate sites for redundancy and disaster recovery replication can be synchronous or asynchronous in synchronous replication  each block must be written locally and remotely before the write is considered complete  whereas in asynchronous replication  the writes are grouped together and written periodically asynchronous replication can result in data loss if the primary site fails  but it is faster and has no distance limitations  the implementation of these features differs depending on the layer at which raid is implemented for example  if raid is implemented in software  then each host may need to carry out and manage its own replication if replication is implemented in the storage array or in the san intercom1ect  however  then whatever the host operating system or its features  the host 's data can be replicated  one other aspect of most raid implementations is a hot spare disk or disks  a is not used for data but is configured to be used as a replacement in case disk failure for instance  a hot spare can be used to rebuild a mirrored pair should one of the disks in the pair fail in this way  the raid level can be reestablished automatically  without waiting for the failed disk to be replaced  allocating more than one hot spare allows more than one failure to be repaired without human intervention  12.7.4 selecting a raid level given the many choices they have  how do system designers choose a raid level one consideration is rebuild performance if a disk fails  the time needed to rebuild its data can be significant this may be an important factor if a continuous supply of data is required  as it is in high-performance or interactive database systems furthermore  rebuild performance influences the mean time to failure  rebuild performance varies with the raid level used rebuilding is easiest or raid level1  since data can be copied from another disk ; for the other levels  we need to access all the other disks in the array to rebuild data in a failed disk  rebuild times can be hours for raid 5 rebuilds of large disk sets  raid level 0 is used in high-performance applications where data loss is not critical raid level1 is popular for applications that require high reliability with fast recovery raid 0 + 1 and 1 + 0 are used where both performance and reliability are important-for example  for small databases due to raid 1 's 530 chapter 12 the inserv storage array im1ovation  in an effort to provide better  faster  and less expensive solutions  frequently blurs the lines that separated previous technologies consider the inserv storage array from 3par unlike most other storage arrays  inserv does not require that a set of disks be configured at a specific raid level  rather  each disk is broken into 256-mb chunklets ram is then applied at the chunklet level a disk can thus participate in multiple and various raid levels as its chunklets are used for multiple volumes  inserv also provides snapshots similar to those created by the wafl file system the format of inserv snapshots can be read-write as well as readonly  allowing multiple hosts to mount copies of a given file system without needing their own copies of the entire file system any changes a host makes in its own copy are copy-on-write and so are not reflected in the other copies  a further innovation is  some file systems do not expand or shrink on these systems  the original size is the only size  and any change requires copying data an administrator can configure inserv to provide a host with a large amount of logical storage that initially occupies only a small amount of physical storage as the host starts using the storage  unused disks are allocated to the host  up to the original logical level the host thus can believe that it has a large fixed storage space  create its file systems there  and so on disks can be added or removed from the file system by inserv without the file systems noticing the change this feature can reduce the number of drives needed by hosts  or at least delay the purchase of disks until they are really needed  high space overhead  raid levels is often preferred for storing large volumes of data level6 is not supported currently by many raid implementations  but it should offer better reliability than levels  raid system designers and administrators of storage have to make several other decisions as well for example  how many disks should be in a given raid set how many bits should be protected by each parity bit if more disks are in an array  data-transfer rates are higher  but the system is more expensive  if more bits are protected by a parity bit  the space overhead due to parity bits is lower  but the chance that a second disk will fail before the first failed disk is repaired is greater  and that will result in data loss  12.7.5 extensions the concepts of raid have been generalized to other storage devices  including arrays of tapes  and even to the broadcast of data over wireless systems when applied to arrays of tapes  raid structures are able to recover data even if one of the tapes in an array is damaged when applied to broadcast of data  a block of data is split into short units and is broadcast along with a parity unit ; if one of the units is not received for any reason  it can be reconstructed from the other units comrnonly  tape-drive robots containing multiple tape drives will stripe data across all the drives to increase throughput and decrease backup time  12.7 531 12.7.6 problems with raid unfortunately  raid does not always assure that data are available for the operating system and its users a pointer to a file could be wrong  for example  or pointers within the file structure could be wrong incomplete writes  if not properly recovered  could result in corrupt data some other process could accidentally write over a file system 's structures  too raid protects against physical media errors  but not other hardware and software errors as large as is the landscape of software and hardware bugs  that is how numerous are the potential perils for data on a system  the solaris zfs file system takes an innovative approach to solving these problems through the use of  a technique which is used to verify the integrity of data zfs maintains internal checksums of all blocks  including data and metadata these checksums are not kept with the block that is being checksummed rathel ~ they are stored with the pointer to that block  see figure 12.13  consider an inode with pointers to its data within the inode is the checksum of each block of data if there is a problem with the data  the checksum will be incorrect and the file system will know about it if the data are mirrored  and there is a block with a correct checksum and one with an incorrect checksum  zfs will automatically update the bad block with the good one similarly  the directory entry that points to the inode has a checksum for the inode any problem in the inode is detected when the directory is accessed  this checksumming takes places throughout all zfs structures  providing a much higher level of consistency  error detection  and error correction than is found in raid disk sets or standard file systems the extra overhead that is created by the checksum calculation and extra block read-modify-write cycles is not noticeable because the overall performance of zfs is very fast  another issue with most raid implementations is lack of flexibility  consider a storage array with twenty disks divided into four sets of five disks  each set of five disks is a raid level 5 set as a result  there are four separate data 1 figure 12.13 zfs checksums all metadata and data  532 chapter 12 volumes  each holding a file system but what if one file system is too large to fit on a five-disk raid level 5 set and what if another file system needs very little space if such factors are known ahead of time  then the disks and volumes can be properly allocated very frequently  however  disk use and requirements change over time  even if the storage array allowed the entire set of twenty disks to be created as one large raid set other issues could arise several volumes of various sizes could be built on the set but some volume managers do not allow us to change a volume 's size in that case  we would be left with the same issue described above-mismatched file-system sizes some volume n lanagers allow size changes  but some file systems do not allow for file-system growth or shrinkage the volumes could change sizes  but the file systems would need to be recreated to take advantage of those changes  zfs combines file-system management and volume management into a unit providing greater functionality than the traditional separation of those functions allows disks  or partitions of disks  are gathered together via raid sets into of storage a pool can hold one or more zfs file systems the entire pool 's free space is available to all file systems within that pool zfs uses the memory model of malloc and free to allocate and release storage for each file system as blocks are used and freed within the file system as a result there are no artificial limits on storage use and no need to relocate file systems between volumes or resize volumes zfs provides quotas to limit the size of a file system and reservations to assure that a file system can grow by a specified amount  but those variables may be changed by the file system owner at any time figure 12.14  a  depicts traditional volumes and file systems  and figure 12.14  b  shows the zfs model  i fs i ~  a  traditional volumes and file systems   b  zfs and pooled storage  figure 12.14  a  traditional volumes and file systems  b  a zfs pool and file systems  12.8 12.8 533 in chapter 6  we introduced the write-ahead log  which requires the availability of stable storage by definition  information residing in stable storage is never lost to implement such storage  we need to replicate the required information on multiple storage devices  usually disks  with independent failure modes  we also need to coordinate the writing of updates in a way that guarantees that a failure during an update will not leave all the copies in a damaged state and that  when we are recovering from a failure  we can force all copies to a consistent and correct value  even if another failure occurs during the recovery  in this section  we discuss how to meet these needs  a disk write results in one of three outcomes  successful completion the data were written correctly on disk  partial failure a failure occurred in the midst of transfer  so only some of the sectors were written with the new data  and the sector being written during the failure may have been corrupted  total failure the failure occurred before the disk write started  so the previous data values on the disk remain intact  whenever a failure occurs during writing of a block  the system needs to detect it and invoke a recovery procedure to restore the block to a consistent state to do that  the system must maintain two physical blocks for each logical block an output operation is executed as follows  write the information onto the first physical block  when the first write completes successfully  write the same injormation onto the second physical block  declare the operation complete only after the second write completes successfully  during recovery from a failure  each pair of physical blocks is examined  if both are the same and no detectable error exists  then no further action is necessary if one block contains a detectable error  then we replace its contents with the value of the other block if neither block contains a detectable error  but the blocks differ in content  then we replace the content of the first block with that of the second this recovery procedure ensures that a write to stable storage either succeeds completely or results in no change  we can extend this procedure easily to allow the use of an arbitrarily large number of copies of each block of stable storage although having a large number of copies further reduces the probability of a failure  it is usually reasonable to simulate stable storage with only two copies the data in stable storage are guaranteed to be safe unless a failure destroys all the copies  because waiting for disk writes to complete  synchronous i/o  is time consuming  many storage arrays add nvram as a cache since the memory is nonvolatile  it usually has battery power to back up the unit 's power   it can be trusted to store the data en route to the disks it is thus considered part of 534 chapter 12 12.9 the stable storage writes to it are much faster than to disk  so performance is greatly improved  would you buy a dvd or cd player that had one disk sealed inside of course not you expect to use a dvd or cd player with many relatively inexpensive disks on a computer as well  using many inexpensive cartridges with one drive lowers the overall cost low cost is the defining characteristic of tertiary storage  which we discuss in this section  12.9.1 tertiary-storage devices because cost is so important  in practice  tertiary storage is built with the most common examples are floppy disks  tapes  and read-only  write-once  and rewritable cds and dvds many any other kinds of tertiarystorage devices are available as well  including removable devices that store data in flash memory and interact with the computer system via a usb interface  12.9.1.1 removable disks removable disks are one kind of tertiary storage floppy disks are an example of removable magnetic disks they are made from a thin  flexible disk coated with magnetic material and enclosed in a protective plastic case although common floppy disks can hold only about 1 mb  similar technology is used for removable magnetic disks that hold more than 1 gb removable magnetic disks can be nearly as fast as hard disks  although the recording stuface is at greater risk of from scratches  a is another kind of removable disk it records data on a rigid platter coated with magnetic material  but the recording technology is quite different from that for a magnetic disk the magneto-optic head flies much farther from the disk surface than a magnetic disk head does  and the magnetic material is covered with a thick protective layer of plastic or glass  this arrangement makes the disk much more resistant to head crashes  the magneto-optic disk drive has a coil that produces a magnetic field ; at room temperature  the field is too large and too weak to magnetize a bit on the disk to write a bit  the disk head flashes a laser beam at the disk surface the laser is aimed at a tiny spot where a bit is to be written the laser heats this spot  which makes the spot susceptible to the magnetic field now the large  weak magnetic field can record a tiny bit  the magneto-optic head is too far from the disk surface to read the data by detecting the tiny magnetic fields in the way that the head of a hard disk does  instead  the drive reads a bit using a property of laser light called the when a laser beam is bounced off of a magnetic spot  the polarization of the laser beam is rotated clockwise or counterclockwise  dependin ~ g on the orientation of the magnetic field this rotation is what the head detects to read a bit  another category of removable disk is the optical disks do not use magnetism at all instead  they use special materials that can be altered by laser light to have relatively dark or bright spots one exarnple of optical-disk 12.9 535 technology is the which is coated with a material that can freeze into either a crystalline or an amorphous state the crystalline state is more transparent  and hence a laser beam is brighter when it passes through the ltlaterial and bounces off the reflective layer the phase-change drive uses laser light at three different powers  low power to read data  medium power to erase the disk by melting and refreezing the recording medium into the crystalline state  and high power to melt the medium into the amorphous state to write to the disk the most common examples of this technology are the re-recordable cd-rw and dvd-rw  the kinds of disks just described can be used over and over they are called in contrast  can be written only once an old way to make a worm disk is to manufacture a thin aluminum film sandwiched between two glass or plastic platters to write a bit  the drive uses a laser light to burn a small hole through the aluminum this burning can not be reversed although it is possible to destroy the information on a worm disk by burning holes everywhere  it is virtually impossible to alter data on the disk  because holes can only be added  and the ecc code associated with each sector is likely to detect such additions worm disks are considered durable and reliable because the metal layer is safely encapsulated between the protective glass or plastic platters and magnetic fields can not damage the recording a newer write-once technology records on an organic polymer dye instead of an aluminum layer ; the dye absorbs laser light to form marks this technology is used in the recordable cd-r and dvd-r  read-oniv such as cd-rom and dvd-rom  come from the factory with the data prerecorded they use technology similar to that of worm disks  although the bits are pressed  not burned   and they are very durable  most removable disks are slower than their nonremovable counterparts  the writing process is slower  as are rotation and sometimes seek time  12.9.1.2 tapes magnetic tape is another type of removable medium as a general rule  a tape holds more data than an optical or magnetic disk cartridge tape drives and disk drives have similar transfer rates but random access to tape is much slower than a disk seek  because it requires a fast-forward or rewind operation that takes tens of seconds or even minutes  although a typical tape drive is more expensive than a typical disk drive  the price of a tape cartridge is lower than the price of the equivalent capacity of magnetic disks so tape is an economical medium for purposes that do not require fast random access tapes are commonly used to hold backup copies of disk data they are also used in large supercomputer centers to hold the enornwus volumes of data used in scientific research and by large commercial enterprises  large tape installations typically use robotic tape changers that move tapes between tape drives and storage slots in a tape library these mechanisms give the computer automated access to many tape cartridges  a robotic tape library can lower the overall cost of data storage a diskresident file that will not be needed for a while can be to tape  where the cost per gigabyte is lower ; if the file is needed in the future  the computer can it back into disk storage for active use a robotic tape library is 536 chapter 12 sometimes called storage  since it is between the high performance of on-line magnetic disks and the low cost of off-line tapes sitting on shelves in a storage room  12.9.1.3 future technology in the future  other storage technologies may become important sometimes old technologies are used in new ways  as economics change or the technologies evolve for example  solid-state disks  or are growing in importance and becoming more common simply described  an ssd is a disk that is used like a hard drive depending on the memory technology used  it can be volatile or nonvolatile the memory technology also affects performance nonvolatile ssds have the same characteristics as traditional hard disks but can be more reliable because they have no moving parts and faster because they have no seek time or latency in addition  they use less energy however  they are more expensive per megabyte than traditional hard disks  have lower capacity than the larger hard disks  and may have shorter life-spans than hard disks ; so their uses are limited in one example  ssds are being used in storage arrays to hold metadata which requires high-performance such as the journal of a journaling file system ssds are also being added to notebook computers to make them smaller  faster  and more energy efficient  another promising storage technology  bologt ; ;  phk uses laser light to record holographic photographs on special media we can think of a hologram as a three-dimensional array of pixels each pixel represents one bit  0 for black or 1 for white and all the pixels in a hologram are transferred in one flash of laser light  so the data transfer rate is extremely high with continued development  holographic storage may become commercially viable  another technology under active research is based on  iv ! e \ 1s   the idea is to apply the fabrication technologies that produce electronic chips to the manufacture of small datastorage machines one proposal calls for the fabrication of an array of 10,000 tiny disk heads  with a square centimeter of magnetic storage material suspended above the array when the storage material is moved lengthwise over the heads  each head accesses its own linear track of data on the material the storage material can be shifted sideways slightly to enable all the heads to access their next track although it remains to be seen whether this technology can be successful  it may provide a nonvolatile data-storage technology that is faster than magnetic disk and cheaper than semiconductor dram  whether the storage medium is a removable magnetic disk  a dvd  or a 