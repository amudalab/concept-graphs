we continuing discussion on um execution of transactions database systems we in fact looked at various things like how the execution of various transactions to be controlled in the database systems to produce consistency results in fact transaction execution is one of the most important things in database system because it affect the performance of the system if you typically notice during the results time when cbse results or state board results announced now lot of people try to access the results through the net the results is stored more or else on a on a database system what you will realize is large number of people simultaneously trying to access these results at the same time is very important same time because everybody wants to know the result or whats result at the same time has somebody else because everybody is curious to know about the results now this is the time when there are simultaneous heats on the database system the large number of people trying to access these systems at the same time now normally we notice that at this point of time the system fails no that s the time respond to so many requests at the same time which is equivalent to saying that the throughput of the system number of simultaneous users it can catered to is coming down drastically that s the time the system will be challenge in terms of how many number of transactions it can process in a given point of time that is per second how many transactions you are able to execute on your system another very interesting example is our railway information system you notice summer the lot of people trying to book the railway tickets online or go to the counters and book the tickets you can notice that the number of the trying to book the tickets in the last you know couple of days this being summer seems to be peeking at thirty thousand per day which is you know the system should be able to handle that many number of peoples simultaneously coming and try to book the railway reservation simultaneously so database system one of the very critical systems database systems is how many number of transactions database is able to execute without losing consistency now very important is that if one user is booked the ticket somebody else simultaneously accesing the ticket overwrites all the details and the same seat is allotted to two people at the same time same birth is given to two people the system is in inconsistent state when one user is trying to book and gets the birth it should be guaranteed that no body is going to get the same birth that s were basically we were trying to understand how database actually solve the problem of trying to give as much as throughput as possible and at the same time marinating that transaction um the consistency of system state this is the most important requirement of database system what we did in he last class we were looking at two phases at very populated protocol that is employed by database systems to ensure that when simultaneous transactions are executing they log the data items for example in this case if you um what we are discussing few minutes back on the railway reservation case the birth will be for particular train if it is locked by one transaction it will not be allow by another transaction till this transaction finishes operating on the particular item that is um how exactly coordination is done by the transaction manager to ensure that more transactions to be executed at the same the consistencies is ensured unfortunately um what we observe in the two locking phase is beside it is prone to be dead locked because when you locked its possible two different transactions could have locked themselves in a different way and that could have result in a deadlock scenario and it is also possible that since the locks are only released to the commit point is possible that more throughput can not be achieved by two phase locking that is were basically we see that two phase locking needs lot of um you know um those kind of algorithms need change to ensure a better throughput two phase locking suddenly doesn t gives that much throughput and so if you actually want to improve the transaction throughput possibly you should explore other algorithms as well now one of the things we did in the last class was to actually classify the algorithms into optimistic algorithms and pessimistic algorithm and we showed that the optimistic class of algorithms we basically used what is called the time stamp algorithms were as in the pessimistic we have conflicts a more we said we basically using locking algorithms this is what we actually did in the last class and we said we will continue to look optimistic based algorithms in this class we are going to look at  refer slide time 7.11  typically time based algorithms they are very popular algorithms in terms of producing a better throughput i am going to compare both optimistic and pessimistic algorithms in a liitle more detail fashion for a few minutes now and then get down to look at time based algorithms in much more detail but before we go in to time based algorithms let us understand essentially the difference between pessimistic based algorithms optimistic based algorithms now really what happens in the case of pessimistic and optimistic based algorithms case they are in two different spectrums in terms of what actually they do for checking the consistencies of the  refer slide time 8.28  database transactions notice that when actually we admit the transactions into the databases if you admit only those transactions that are going to be consistent  refer slide time 8.49  then you have actually doing the admission control for consistency in the beginning of that transaction execution now its possible for you let the transaction execution till the last point and start doing for the consistencies check for the transactions at the end of the executions so its possible for you to actually do the consistency check at the start of execution verses the end of the execution okay now if you basically do check at the end of execution you typically waste the execution time of transactions for example its possible transactions actually executed up to it finish and it is at the point of time is told that the execution is not consistent with respect to other transactions and that transactions is aborted then it is like saying that you did lot of work but some body says at the end of the work  refer slide time 9.11  whatever you have done is not consistent so please come back again and redo whatever you done so that is one way of ensuring that you know the consistencies is applied at the end of the execution on the other hand everything you do from the beginning a teacher is you by side and looking your shoulder and looking at every step what your doing and if you actually took a um the value is not actually correct value actually stops you at that point and says wait till you correct value and then your executing then basically that is what we actually see it as a conservative way of executing transactions the other way of executions is let people proceed and let them submit to you at the end of the execution whatever they done and now check whether what they have done is correct or not when two people simultaneously working assuming wrong values that you are thing is wrong because you have already accepted somebody you have actually done it before now you go back and redo assuming that the this is the correct value so its depends on were exactly this consistency criteria actually apply but if you assume lots of times they are not going to be conflicts the people going to operate on different set of data items its possible for you to actually assume that the execution can be allowed to be proceed independently and if you check for the consistencies at the end of executions um that is one approach the other approach is to actually look at these two ends of spectrum seeing one end of spectrum is what we are marking as as the optimistic spectrum and the other end we have marking as pessimistic end of the spectrum now we have the a number of protocols which lie in between these two spectrum this full spectrum of things which we have what we have actually seen is two two pl in grade detail  refer slide time 11.45  and falls under the pessimistic side of the things we have variety of time stamping protocols some of them seems to fall fully optimistic time stamping fully optimistic protocols there are protocols which are time based algorithms which completely optimistic they fall in between the pessimistic in the optimistic protocols what we are going to do is we are going to looked at protocol call the basic time stamping protocol and then we will see how this protocol  refer slide time 13.05  is different from the two pl locking kind of the algorithms that we have seen earlier what we will do when we start the basic time stamping algorithm is to understand how time stamp based protocols is work in the concurrency then we will modify this basic time stamping protocol to produce what we see as a fully optimistic time version of the time stamping protocol the also other protocols more notably the multi version protocol its possible to map it somewhere here the multi version protocols what they basically do is they produce multi versions of the data items  refer slide time 14.06  when reads and writes are going on which means that they keep multiple copies of the data item being manipulated in the database and consequently what you have what are called the multi version um multi version protocols concurrency control protocols one modification of the two pl two phase locking protocol for multi version is the other thing we are going to looked at which is the multi version two pl is another which we are going to looked at as part of the multi version protocol  refer slide time 14.46  we will look at basic version multi protocol and we are also we will look at two version locking protocol which is called the modified two pl multi version protocol what this actually shows this there is whole gamet of algorithms as shown in this picture basically looked at this spectrum is quite wide in terms of number of protocols that can be poured in between the pessimistic and optimistic kind of algorithms it depends to the large extends to what side  refer slide time 15.29  the algorithm should be applied depends to a large extent on the system configuration for example if you assume that going to be too many conflicts the data items going to be having many conflicts for particular data items then it is not worth actually putting optimistic kind of protocols on the other hand where there are not likely would of lot of conflicts and you start to mapping it to the optimistic side um optimistic side then you are likely to have you know um the throughput drastically comes down and its not worth actually mapping putting the kind of pessimistic  refer slide time 16.20  algorithms to your database systems a good example is here again is to look at what we see as typically when a new movie is released a large number of conflicts um you know large number of people try to book for the same movie you know on a particular theatre and also you going to see that there is a choice for a particular you know set of seats because there are more preferred seats in the theatre compared to other kinds of seats in the theatre okay visited the theatre you know many times you know which is convenient place to sit to view an movie um what in aspect means is large number of people when the new movie is released we try to book the tickets and many of them will start asking um if you ask the preferences will ask for those kind of seats that s basically high conflict data items basically look at the data items you viewed the data items you can see a large number of people trying to access or trying to modify a manipulate a small set of data items and this what mean by conflicts being very high for the small percentage of data items now it suddenly it requires some kind of concurrency control now in this particular case if you if you actually applied a pessimistic algorithm which ensures from the beginning that transactions operated in a constructed way um works well because ultimately s single seat can be booked by one one customer you can have multiple people trying to book for the same seat whereas ten people compete for the one ticket and ultimately nine have to be aborted even if they have all gone ahead whatever the manipulations they have to do but at the end database is going to say only one of them going to get the ticket which means that nine of them abort after proceeding taking all the information they abort at the end whereas the pessimistic concurrency control would have actually aborted all the other would not alow the nine to proceed in the beginning which means that the transaction to been blocked from actually trying to manipulate the data item it is actually booked that s the essentially the difference between optimistic and pessimistic kind of an algorithms what we going to looked at look at deeper in this and understand what exactly is the is the way this consistency enforced by the  refer slide time 19.24  pessimistic and optimistic algorithms as far the transactions concerned now what we will do is we will actually take a sample graph and start showing how this graph actually as grown if you typically looked at the two cases of optimistic and pessimistic concurrency control the idea of looking at this graph is to understand  refer slide time 19.44  in a more deeper way how the consistency checks different points we really help you know in terms of resolving the conflicts as you see in the diagram there is typically at this point of the time there are set of transactions which are executing with in the time sense that t one which is actually before t two and now which is now basically before t three in order in which the transactions are  refer slide time 20.32  trying to execute one after other now if you keep a new transaction an incoming transaction t four now there are two points which we are talking in the graph one is entry point and other is end point now t four if you allowed t four to execute  refer slide time 21.00  without really looking at now what is trying to do is be consistent or in consistent what would have happen is t four would have proceed to execute and when it comes at the end of the execution then you try figuring out whether whatever the transactions is trying to do needs sense  refer slide time 21.30  um makes consistency whether it falls under the consistency criterion whether it satisfies the criterion now all the means is this has to be satisfied if there is lightly to be an arc you know something which shows that t four at the end of the thing has to actually t four at the end of the um thing will have precedence relationship which is shown here carefully follow the diagram to understand what we actually  refer slide time 21.51  trying to discuss here there is a t four that entered after i had this graph now where do i replace this t four if i am actually saying that t four comes okay after t three this is fine because  refer slide time 22.29  this is really doesn t disturb the order because t one comes before t two t two comes before t three t three comes before t four so this specific order in which things are happening for you  refer slide time 22.49  and this is correct order because one after other the relationship is maintain but some other conflicting data items t four has to come okay before t one which means that now i want actually force the relationship on the graph something like this you wont be able to insert this relationship okay in to the sequence of actions your doing and this actually violates the consistency which means that it wont be possible for me no longer to say that is because from this the relation that i that is t one before t four by the transive relationship this is like that i ate breakfast  refer slide time 23.05  then i ate lunch then i actually ate my evening snacks now i cant say suddenly by dinner comes after snacks but actually my breakfast which is the first event i actually perform um occurs after my dinner okay that is what exactly what is happening here which is violative because by this relationship the breakfast should be coming before the dinner event whereas i am saying that but my dinner event comes before breakfast event and this violates this consistency criterion because one after other the other has actually able to see here one after the other the relation is correct as long as future relations doesn t violate  refer slide time 24.23  whatever the order i able to come with if you really understand what really we trying to say is graph is following know i have a set of transactions and actually um i write the relationships that i have here all that been talked about is if there is cycle in this graph it is violative in this criterion because at cycle you cant break the cycle one after other what essentially is showing this this is before relationship now what is saying that t one is before t two t two is before t three t three is before t four now suddenly i am saying t four is before t one and this actually what introduces the intensities in to the  refer slide time 24.45  execution of the transactions as one after the other essentially this transition cycle in the graph is to be avoided if you want to produce consistent execution of the transactions now understand what we will try to understand here is how does a pessimistic kind of algorithms will try to solve this problem verses how optimistic concurrency control algorithms try to solve this problem i will try use this transaction same graph to illustrate this point that an incoming transaction we first map in to the transaction graph by the two pl which means that it will never allow a transaction to be executed unless it position in the transaction graph fix by the algorithm which means that there is no way t four could have executed in the two pl case once it start executing this condition would have checked if this condition is not possible what two pl does is it essentially makes t four execution possible okay t four would have not executed t four can not execute okay which means that i prevent t four from execution from the beginning okay this is what we call as basically not allowing disallow okay disallow the transaction from the beginning and this is what two pl would have done okay now what the optimistic concurrency control algorithm do is t four is allow to execute okay allow to execute but then it would have fail when it wants to commit t four can not commit because at before commitment we basically check okay whether what t four is done consistent or not which is the two spectrum we are talking the spectrum is where you don t have to allow in the beginning itself in the other case you allow to execute and stop it from committing after it is executed its still can not commit because its violating the restriction now let us go deeper and understand this  refer slide time 26.46  this is basis of further discussion we go on to timestamp algorithm also what we try to understand is how the two pl wil disallow the transactions from executing from beginning itself the consistency check will be done at the beginning itself and as i do this i will also informally try to improve by this two pl actually produces serializable schedules serializable now lets us look at a set of conflicting operations ultimately transaction um transaction boils down to a set of operations performed by their transaction for example if you look at transaction t one a set of operations transaction performs and that can be o one to o n to get the correct subscribe i am going to do is i will actually make this order this operation with subscribe which is transaction number in this for particular case under that i will typically look at the further subscribe which is the operation number what this means is in operation i of j means that ith transcation and this is the jth operation this is the first transaction i am talking about what i am going to do is i will try to make this just the one of the j okay and this becomes operation one and we will also to complete the notation what i will basically indicate also is that data item on which this operation being performed for example you can indicate here this actually accesses or does something of x or y what ever it is now the meaning of this is operation of first transaction this is the jth operation this is the manipulation this is the xth data item this is the operation n transaction one manipulate including data item y now basically what we do is when this executing context of two pl we need to a lot on data item before you actually proceed on x or y because that could be how actual shows that your not operating unless you lot now in this particular case we are typically looking at two transactions that are executing simultaneously to understand how they could be manipulate now let us say theses are the sample transactions of one and two transactions t one and t two  refer slide time 28.54  now we will say that a operation will be conflicting if it is operated on the same data item what is this actually means if um in case of banking transactions if i go on withdraw a cash from my account and somebody also simultaneously withdrawing cash from his account not my account we both are essentially operating on our own individual accounts there is no conflict in this particular case because he is operating in his account and i am operating on my account the minute the both of us go to the bank and try to withdraw the amount from the same account then we will be conflictive again if we are both looking at balance amount that is available in the account we still not conflicting because both of us simultaneously can view what is the current balance in the account without really withdrawing if you just looking at the balance still your not producing actually consistent results so a simple read though on the same data item is still not conflicting if either of us are withdrawing both of us withdrawing which means that if one of the operation is right operation if both are operating on the same data item in this particular item you can see both are actually same data item x and one of them is right operation then only there will be conflict because i am withdrawing  refer slide time 33.00  and is looking at the current balance available in the account now we depends on whether i read it after i withdraw it or before it starts now becomes a conflict operation if one of its actually right so this is what means by two operation being conflicting now when only when operations are conflicting we need to worry about the order in which its executed if the operations are not executing for example if two us are not withdrawing money from two accounts it doesn t really matter because may just operate on two accounts but the minute actually we started operating on the same account we need to know what exactly happen with respect to that account who has seen that account how much has been withdraw by a person x how it is actually added somebody else all these details one after other needs to be there the after relation is important otherwise is going to be inconsistencies in the final result that you see has far the account is concerned so when we actually looked at t one an t two what we interested in is if there is conflict between t one and t two then only we are interested finding out the relationship between t one before t two or t two before t one and this relation essentially boils down to looking at some operation of one and some operation of two and saying how this conflicting two operations have actually been executed with respect to each other this is what we actually at the end of it interested to see  refer slide time 34.32  t one before or t one after for example one of its right operation then we say that its conflicting just explain now look at operation o one j and operation two two j t one and t two since the conflicting now not conflicting we cant write this order because they can execute in any order  refer slide time 35.30  they actually wish whether it produces inconsistent results if they are conflicting we have to understand which operation executed before the other based on that we are going to say transaction t one is coming before t two or t two is coming before t one now what has an end of the serious of execution if you typically looked at now basically only the one transaction suffix ill drop the actual operation suffix ill say two conflicting operations o one and o two belong to t one and t two executed in this particular order  refer slide time 36.00  which means that there is an order before t one and t two now imagine i have conflicting o two verses o three for the third transactions which means that t two executed before t two executed before t three okay now if you looked at another this is what actually we looked at o four this means that t three is executed before t four now look at the discussion that we actually had while ago transaction graph this is what exactly happened  refer slide time 36.32  transaction graph we just proceeding one after other but when we actually came to t four what is really happening was we are trying to say o four also has a conflicting operation o one but these were executed in this particular way which actually means that this is the order would have happened and which is what actually produces the inconsistent results now basically looked at two pl should this execution would have happened if i consider two pl now o one would have actually locked the data item let us say this is conflicting operation is defined here on some data item x that i will indicate here which means that there is a lock on data item x which was obtained by o one okay now let say this is a conflicting operation between o two and o three on data item y which means that in this particular case the lock was obtained by o two before o three now let us say this is a conflicting operation z here okay and on which actually we got a lock on jet for o three because if the lock was not obtained this sequence is not possible because the transaction will never execute in the case of two pl unless the lock was granted so t one would not able to execute unless is obtained a lock on x because o one and o two conflicting t one as locked before t two thats the relationship is possible otherwise this relationship is possible you cant have the relation has shown in particular equation here right now let us say between there is t four and t one let us say there is an item data item a this is doesn t fit into regular that s why i am using different xyz there is a sequence a is out of the sequence now between o four and o one there is a conflicting operation being performed on a right now o four is saying that it locked it get locked on a before o one now for a minute thing if this is possible in two pl what o one what transaction t one would have done is it actually requires a lock on x a lock on a right before its start executed now what this set of equations i had here say is i have got a lock on x now o two would have not got transaction t two would not got unless i released locked to it which means that i should have release a lock on x before getting the lock on a because o two saying that actually lock on y before o three similarly o three is saying i have a lock on z four but o four is saying that i got lock on a before o one right if you carefully understand this unless this lock is released by t one t two would not have get that lock unless t two as got the lock it would not have actually proceeded to get another lock so actually if you use this equation we are writing here is inconsistent because two pl prevent this saying if i have a lock i wont released that lock till i get all the other lock remember the condition that imposed by two pl  refer slide time 37.18  which says that unless all the locks are obtained um your not to released previous lock that is what we meant by lock point right all the locks will be obtained that two pl finishes transaction finishes the execution then it basically releases the lock if you release one lock you are not supposed to asked any more locks with that condition i have released locked x i would not have release locked a on transaction t two would not have able to execute unless i finish all executions since t two would not have lock till i finish there is no way i can say it is come before me and obtained lock a a lock on a if this is the way its supposed to execute this is intuitively what happen with two pl  refer slide 41.55  and essentially the reason y this importance condition is put into two pl saying that if you actually release one lock you are excepted to ask for any more locks if doesn t follow you have ended up by having the problem cycle in the transaction graph would have happen if i allowed the condition that x could be released but still you can ask for a that is what actually prevented two pl saying that once you have a lock on x you have to ask lock before you release any of the lock you have required earlier because if you release one lock you can not ask for any more locks this essentially prevents the cycle in the transaction graph this explanation makes you understand  refer slide time 42.42  how two pl checks for the consistency at the entrance when the transaction is start to executing how two pl ensures the consistency requirement once the transaction starts executing and its start getting the lock there is no way you can say transaction is inconsistent you will never get in to an inconsistent state for execution what really happens in the particular context is the transaction can get blocked when its get blocked this result in scenario of the transaction either um permanently waiting if there is a deadlock of kind of scenario or waiting for sufficient long to acquire a lot but the transaction will never start execution unless in a inconsistent state there are certain issues relate to what is the overhead of the this kind of the algorithm which we will discuss towards the end time stamping algorithm the other issue is when you move to the end of the execution model and check for the consistency how exactly there is going to be done now we will look at typically the scenario and see what all the possibilities of that being done and how that will be that will be done case of optimistic scenario now really what happens um in the case of and optimistic algorithm is we basically to take a transaction data items on which it basically operated okay this set of data items which are actually manipulated in this particular case is going to be say so this set going to be x and a in the earlier case this set transaction t one as actually manipulated t two manipulated set of data items relating to y probably some other set if you take t four this is the set which we get here a and probably some other set now when t four wants to comes in t four can gets execute whatever it wants to do t four wants to execute now it says i want to commit that is basically when it is trying to write the values of a back on the database this is at the commit point what your going to do is it check for the values is a and say whether previous things done by the other transactions consistent with respect to this if the this is basically what we say as the validation check the validation check is done at the n and if t four passes the validation then i actually allow t four to commit what happens in the case of purely optimistic kind of scenario the every transaction is allowed to proceed what really happens in the case is the transactions takes value does it in a local copy of the data item and manipulates whatever it need to on local copy and when it actually to write you perform this validation at the end of it i will give only intuitive explanation at this stage to allow you to understand whats happening here but more detail discussion is going to allow  refer slide time 44.58  when we actually take up the time stamping algorithm which are the optimistic time stamping algorithm we will go in depth see how exactly this works to give to an intuitive explanation of the optimistic scenario what is done in this case is let us say four people are trying to do simultaneously the something each one will be given a local copy this is like four people okay four people in this case as you know you can see the transaction t one t two um t three and t four okay now all the four transactions here will will be allowed to to see by taking a local copy from the database okay now t two pl would have said going to do order all of you and then you have to get in with the with respect to the  refer slide time 47.38  tickets i have given to you t one is number one t two is number two t three is number three and t four is number four so that s the way they are going to execute now i basically will not bother to give them a ticket when actually arrives my place what i say is you actually try doing whatever you want to do that is each one will get a local copy of data item in this particular case as you can see t three t one and t four will get a copy of a okay in this case would be a and y this is going to y this is going to be z other values now when actually finish something for example say how is the value produced by the each one of them they at end of it submit to me when it finish execution they submit whatever they done to me now i will look at what is called validate this validate phase is going to look at if this problem of a being done in a inconsistent way t one and t four will be detected at this stage for example this will show me that there is something t one done on a which is inconsistent t four how that can happen for example what we will say here is t one as actually manipulate the value of a with some particular some one is manipulated value t one actually manipulate the value of x to two which actually has been used by t two and t two has actually manipulated value and produce the value three which isbactually used by okay and now t three manipulated this value four which is used by okay now this is actually manipulated this is to three and its actually to be used by t one this is where exactly problem of i actually should not have used this manipulated because i am working after but with respect to a t one actually has taken t four value and manipulated to one please understand this looking more carefully i actually need something to produce a value for x equals to two and that value is used by t two okay this is like i did something in paper and passed it to a next person t two use that s value and produce value for y and y value is passed to three  refer slide time 48.24  and we used that value to produce a value for z and that is given to three four but t four actually given a slip to t one before this for a which actually means there is a cycle here is now whose result difficult to say because but t four seems to be one a one affets to be t two like this we basically looked at this is what actually produces the intensity and this will be what is to be detected submit their values to me in this case of two pl they will not be able to execute to produce this value but in the case of optimistic algorithm they will be allowed to proceed will be allowed to this manipulation but when given the values to me when i look at them i can see that this is violating the properties of consistency which is essentially the consistent in this particular graph what the validate says does in the case of optimistic algorithm this is essentially check the dependency at the end of execution and say that of now t four should be aborted t four actually produced a value okay so i basically abort all those transactions which is actually produced inconsistent results and i will say that t one and t four should restart their execution and again they will take the new values from the database and start executing this is what will happen in optimistic scenario now the problem here is as you can see  refer slide time 50.55  in the case of pure pessimistic kind of algorithm t four would not have wasted its time its computing something because i used some value and i tried computing and actually sat down everything but at the end of it i submit my value and i was told you consistent because inputs not correct so you go back and redo what you have done so this is basically a loss of execution time actually unnecessarily executed t four and found that at the end of the day t four didn t use the correct value so it has to be abort it has to restart where this as would never happen at all in the case of two pl pessimistic kind of algorithm because they would have been asked to do the all these values in the correct way start executing the transaction so in that sense the pure pessimistic kind of algorithm would not have allowed this scenario where the aborts are required whereas in the case of optimistic things to proceed but then at the end of the thing you check whether you have done is correct or not both as you can see both have the plus and the minus if um you except t one t two t three t four in this particular case this diagram you can see they generally operate for example the scenario what we actually assume here is that a large number of transactions generally operate um different data items not on the same data items okay that means is going to be x this is going to be y this is going to be z and this is going to be p and completely disjoint okay then tou can see that at the end of it they can allow very simply you know without really locking anything because the locking overhead is reduced here  refer slide time 54.11  at the end of validation phase all proceeds because they are not actually conflicting this is one case where all of them are using different doors if an keys so all of them pass validation phase  refer slide time 55.00  this is what you will see if you apply a optimistic scenario try to apply purely pessimistic scenario in this particular case to see what would have happen let us say most of the time all of these transactions operate only on one data item which actually means that it possible for one of them to be able to proceed if they actually conflictive which means thatall of them tend to do the same thing data item but only one of them will be able to go whereas in the other case all of them will be able to go which in affect means that you have to go back and then start reworking for t two t three t four there is going to be larger number of aborts  refer slide time 55.17  okay if you apply where there are large number of effects if you apply for pessimistic algorithm going to be large number of aborts of the transactions and results in wasted time okay in this particular case you can see only one of them and the chance of proceeding look at them all of them are conflicting now if you actually apply pessimistic kind of scenario you know that one of them succeeding get a lot which means that would have serialize then one after other with respect to this conflicting data item and all coming here and trying to rush this would not have happen they will not give the key one after other this is the case of actually they trying to eat each other now if you basically apply the locking is going to be a organized to lock on this data item from x which is something like this lock is going to be is the key for the for the data item this key is going to be used pass to one after other and you can see the all can use this data item one after the other and there is going to be not going to be conflict after this start executing this is the other case of applying the pessimistic scenario  refer slide time  56.20  this lecturer what we did is essentially the look in depth the difference between optimistic kind of algorithm and pessimistic kind of algorithm both have their place in terms of in terms of applications where it should be were there are contentions are very low there is no point to apply pessimistic algorithms where there are large number of know conflicts there is no point to apply an optimistic algorithm so both actually have their place in fact what we going to see in the next few lecturers is whole gamete of algorithms that fall in between which also produce excellent results when there are applied to transaction processing system the next lecturer we are going to take basic time stamping scheme and look at in detail transcriptor  c.udayakumar database management system prof d.janakiram lecture 22 concurrency control part  3 we will continue our discussion on um concurrency control algorithms in this class last few lectures just to recap what we have doing we looked at a class of algorithms try classify them into two broad class pessimistic algorithm and optimistic algorithm in the last class in particular we have see in depth where exactly this difference between optimistic and pessimistic kind of algorithm come class before we have seen how two pl works in the context of pessimistic algorithm and also we mention that this class we are going to looked at one optimistic concurrency control algorithm before we actually look at we also going to look at what we understand as time stamp based algorithms as supposed to locking we will look at time stamp based concurrency algorithms now at all time stamp based algorithms are optimistic algorithms that you have to understand time stamping algorithms essentially used the notion of time for serialiazation that s what they do time stamp algorithms need not necessarily be optimistic algorithms but one can formulate optimistic algorithm and really what we are meaning optimistic algorithms we were saying that the validity of the operations done by the transactions is checked at the end of the execution of transaction that s what we actually mean by the optimistic algorithm now its not necessary that all time stamp based algorithm check for the consistency of the results consistency of operation and by the transaction at the end of the execution they may do something in between they don t need to necessarily do it only at the end of the execution of the transaction its possible to do it so what we will do in this class is we will try to understand in a simple way which we call as basic time stamp algorithm try to understand how time stamp based algorithms work in the context of concurrency control what we are going to do is look at know how you apply time stamp based algorithms how the recovery properties commit properties and recovery properties are integrated in to the time stamp based concurrency algorithm if you remember when earlier lecture two phase locking we did discuss how locking algorithm are integrated with a commit protocol so we are going to do the same excersie of looking at time stamp based concurrency algorithm and also look at how time stamp based algorithms will integrate with commit based algorithms so that s what we are going to do is lecturer now we will proceed to look at basic time stamp based algorithm so what we will do is we ill explain the time stamp based algorithms  refer slide time 5.10  by taking a simple graph of basic algorithm which is called the basic time stamp based algorithm concurrency control algorithm  refer slide time 5.40  the algorithm as several basic things that um that s done like any other concurrency control algorithm the first thing the done is typically in this particular case every transaction is given a timestamp  refer slide time 6.18  this is if you typically look at you know very good example in real life of getting a timestamp and then executing what you want to do the darshan of lord venkataeswara know tirupathi tirupathi devasathanam as this thing called they give the time band know when you actually want to have darshan of the lord they say that you have to obtained a time band you know what exactly the time band is maens they give you the times sticker and says that if you actually go there know before the time or above just the time you will be able to to see the lord so the basically in other words the darshan of the lord is to be serialized because you can see one after other the queue one has to move to see the lord so the basically a serializable now one of the ways we solve this in real life by saying that you need this band know this band is generated what is the band means essentially getting a time stamp time band and says that you all the people whats the time band have to proceed the order of time that they have got that means they will be viewing the lord in terms of the time band okay in a similar way if you look at how the transactions are going to execute each transactions when it enters the system its get the similar time band that s what called time band how exactly this time stamp is given to the transaction later we will look at the step but the idea of giving the time stamp to the transaction is that essentially time stamp serialize the order in which transactions are executed all transactions are executed as um in the increasing order of the time if a transactions as got a time stamp its order in terms of execution is fixed because all those transactions which got time stamp before this transaction will commit before this transaction all those which got a um which got a time stamp higher than this can proceed and are going to commit after this transaction is committed that s sense in serialize order time the notion of the time to serializes the transaction execution this is um very important concept notion of time using the notion of time serilization is very important concept as we go further down we going to look at distributed transactions were a single transaction is split into sub transactions and execute on multiple sides this is like not now using single time stamp for example for each person could be carrying is own wrist watch so you get into additional problem solved of how do i actually take this different times and now put them all in to one zone foe example now london will be in one zone chennai will be in one time zone so if you actually generating transactions in different time zone you get into problems um now all of us using the single clock automatically everything will be serializable but if there use multiple time zones then you get additional problem of making sure that these events across multiple time zone are again serialized we are going to look at the notion when we go on look at the transaction and see how this notion of this time it becomes critical when you want to address the concurrency problem in the case of distributed databases so to understand at you know very simple level what really happening is transactions are coming in to the system giving them a time stamp and your excepting the transactions are executed with respect to the time and essentially produces the serializable to look at th basic steps in little more detail as i was trying to do here every transaction start with will given a time stamp this timestamp which will be used subsequently for checking whether the transactions are executing in a consistent fashion or not now all read write operations are tagged with the time stamp read operations and write operations the transactions at tagged with these time stamp write operations of the transactions are tagged with these keys will call these time stamp as ts so the all the read and write operations are actually tagged with this time stamp okay  refer slide time 12.06  that s the second thing you know when the transaction um issues are read or write operations then let us say transaction ti as got a time stamp ten twenty just understand or at this point of time is eleven thirty we will say the transaction got a time stamp of eleven thirty now subsequently whatever the read or write operations that are being performed by transaction all get the same number that s one one time stamp we are going to use for example in a laterc point of time eleven thirty five or eleven forty you doing something but all belong to same transaction they are not giving the time stamp this is very important all the read and write operations of the transaction are tagged with the same time stamp which is given to the start of the execution of the transaction okay now what do we do after this point of time we also have to actually to know okay for every data item in the data base okay for every data item x we need actually what is the read time stamp of the what is the read time stamp of x and read time stamp of x show the time stamp of the okay the time stamp of the transaction okay that as read the value of um read the read the value of x what is that mean this means let us say this is the data item x and x there is the transaction which reads this data item and time stamp let us say one fifty and say that is the highest transaction they could be transaction time stamp lower than this which also read this data item but we are going to consider the highest of the time stamp transactions that is the read the value of x will be the rts of x similarly the write time stamp of x the time stamp the highest time stamp if you want to actually qualify it the highest time stamp or ts of the transaction that wrote the value of x the transaction that wrote the value of x okay so for every data item you are going to do have two time stamps corresponding to the transaction highest time stamp  refer slide time 15.25  value of the transaction that is the read the value and also the highest time stamp of the transaction which wrote the value why i do need this do because this is essentially tells with respect to this data item what are the earlier transactions that i have actually done on the um whether the read or write operation are performed by the transaction on this particular data item now we will use this time stamp to see the validity criterion when a new transaction comes tries to do some manipulation on this data item we are going to look at and say which are all those values which can be allowed by the transaction manager to proceed and which needs to be aborted now this is the preliminary thing in terms of how exactly the algorithm maintains the data now in terms of actual execution what i does is whenever a transaction now consider the case when a transaction um issue okay a transaction issues the read operation the read operation now this read will be tagged with the time stamp of the transaction so basically this is essentially this is ts of the read operation okay time stamp of the read operation now ts has to be compared with the data item time stamp now let us say  refer slide time 17.14  the read time time stamp of x and write time stamp of read operation we will say on x a transcation with time stamp tx issues a read operations on x now the rts now wts the value of sx are the following now what is the condition now this read operation can be allowed to proceed okay now the read operation can be allowed to go further as long as the condition that the time stamp of x that the current transaction is greater than the write time stamp of x please remember the reads can be shared  refer slide time 18.00  after i write a value any number of people can read that value but if you assume that know if i am actually a value after somebody actually written a value on x which means that the my time stamp value is lesser than what actually been produced then i need to be little careful and avoid such operations understand the condition where ts is less than wts of x okay if okay ts time stamp of the transaction is less than time stamp of this current transaction is write than the time stamp of x what us does this implies that means a later transaction actually return the value later transaction than me as written the value  refer slide time 19.08  um a later transaction has written the value of x now in terms of time stamp order if you want to execute the transactions  refer slide time 19.20  strictly in the time stamp order this should not have happen because now i am actually reading a value produced by somebody who is coming later than me okay this is like saying that know um if you look at senior junior relationship you know now if you essentially says that i am actually i am passing the batch which supposed to passed in two thousand four says that its actually passing after the two thousand three batch there is a violation because two thousand three batch already gone which means that the value return by x is by transaction later than me or older than its me now i cant come back and say to read its value okay so unless i had actually finish the later transaction would not be able to come and o whatever is trying to do so in that sense now this is the case transaction t is to be the issuing transaction as to be aborted the transaction which is actually assuming if this is the case the transaction issuing the the transaction which issues which is sured which is should read the operation needs to be aborted okay and it is restarted okay and restarted with the higher order time stamp restart we are going to add higher values time restarted with a larger time stamp okay now understand what i am saying just going back  refer slide time 21.30  a transaction issues the read operation ts on x now the transaction time stamp is ts here as shown here and the current values in the database as far as rts wts the concern are shown here rts shows the transaction larger time stamp of transaction is read the value of x wts shows the time stamp largest time stamp of transaction which is return the value x now my time stamp is less than wts i don t need to compare this rts rts value is higher since reads can be shareable need not be exclusive i am still not violating any consistency thing only write i have to read something earlier return by transaction this is earlier than me rather than a transaction later than me okay this essentially shows that a later transaction has  refer slide time 22.39  written value of x the transaction which is issued the read operation needs to aborted in case and restarted with the larger time stamp and when you restarted a chance of distractions  refer slide time 22.55  succeeding his higher because now it will take this and proceed further and chance of the transaction succeeding later is very high this is what we will be done as far as the read operation is concerned now if you look at the line operation a transaction let us say with the write operation transaction with the time stamp with the time stamp okay ts issues a issues write operation okay now if issue a write operation you have to check now let us say there are again similar case of rts is the current value wts is the current value of the data item x write operation will say on x okay in which case the rts and wts are the current value now you need to check if either of the conditions are true ts is less than the rts rts that s the lead time stamp or ts less than the write time stamp of x then what us this implies this implies that i am writing a value into x which was read by the later transaction or return by the later transaction in both cases what i am saying is now i am trying to manipulate the value of data item but somebody comes later than me actually read as value return this value  refer slide time 24.55  in either of the case this results in in an inconsistent situation because who is somebody comes later than me as actually read i am actually modifying something would have read some stale value okay similarly i have read value that value will be last come then and start modifying it so in that sense both that conditions the write has to be rejected okay the write operation has to be rejected write on x to be rejected now what are the reject means same as the earlier case that the issuing transaction has to be okay the issuing transaction needs to be aborted and restarted with a higher stamp transaction needs to be aborted and restarted with a higher time stamp now this essentially explains their basic steps of the time stamping algorithm okay  refer slide time 26.17  now what we are do in this particular case so we are trying to say the algorithm tries to provide okay a basis for know checking we give um transaction a time stamp no we didn t still discussed how the transaction will be given an time stamp know several ways it which can be done one way is actually you can use a counter for example first transaction will be given a counter value of one the second transaction will be given higher number than this since all i am interested is logically showing that the transactions comes one after the other a counter is good enough for me only thing is the counter eventually might become un infinitely large means the counter value will become so large that at the end of it some where i have to reach at the counter so um at some point of time when there are more um brief period of time like you know this is period when everybody goes of for lunch know like that when the when there are no no transaction for brief period know in the system you can just forget the entire earlier history you can just set the counter zero and then start doing it again from one that point of time okay that s one way of actually giving time stamp to the transaction this is basically a logical time stamp because all that doing saying is transaction t two which got a counter value of two is higher than a transaction which got a counter value of one so in terms of actually looking at t one and t two all that that you have decide in terms of time order is the counter value okay then are various other ways of actually giving the time stamps for the transaction the other way is actually to use a physical clock okay physical clock that gives the values physical clock that gives okay  refer slide time 28.37  this is this is like saying i will use the clock time of machine okay the clock time and then i will basically say that every clock take i generate okay every clock take is used okay clock take is used okay is used for generating the time stamp generating the okay now there are interesting the this important because every clock take can generate only one time stamp so which means that  refer slide time 29.22  the number of transactions that can come limited now because they are limited by the number of clock takes because the time stamp will be generated only one time stamp can be generated in a clock take because two transactions need to get two different time stamp they cant get the same time stamp if they get it then you will have problems of validating them at the end of the um at at the end seeing how they will basically know you need to order them so you wont be able to order them if both of them get the same time stamp then interesting algorithm for generation of time stamp in fact there is a very very interesting algorithm which doesn t generate time stamp with respect to the starting point he generates with respect to the time when the transaction excepted to finish this is a extremely interesting way of looking at the problem in a entirely different perspective there is like saying when somebody entering in to iit it is at that point i give a tick to him or i give a band to him which means that is excepted now to follow every where else with that band so the band was given with respect to when actually came in to the system the other thing is when is excepted to leave the iit gate i actually put a band there saying that the this is the time were excepted to leave the system if somebody close up at the gate know after his time over you basically know abort that particular thing this is like saying that is excepted to leave the system within that particular time i i come to the gate at ten o clock now one way of saying that is i get the ten o clock at the time stamp the other way interest when we saying is i am supposed to leave this system by twelve o clock so i give twelve o clock as a time stamp which means that ending time is given that means all the transaction if the transaction didn t finish within that particular time period it will be aborted okay so possible for know for the algorithms to generate this time stamp in extremely interesting ways right that used for um some algorithms to actually fixed a band a band of time during which the transaction is excepted to finish the execution so one can intelligently use generation of time to see how the transaction execution can be controlled now has to to just come back to the point of how the algorithm works and then see for further issues with respect to the algorithm and how this algorithm is different from locking kind of an algorithm what we do in this particular algorithm  refer slide time 32.20  is we actually giving a time stamp ts to the transaction every transaction is getting a time stamp and then the read and write operations of the transaction read or write check again  doubt  okay the rts are the wts one of the things i have actually forgot to mention is when you accept a read operation or the write operation  refer slide time 32.54  immediately you need to actually update the time stamp of the data item for example i showed only in the earlier case how the rts and wts will be used to abort a transaction for example if those conditions are not met you go on abort the transaction but if you say that you gone to accept the reads and writes on the data item for example let us say now this read or write on ts accepted okay if the operation is accepted okay if the operation is accepted if the read or write accepted the abort conditions we actually saw earliest so i will just not mention them here if the read or write accepted then the rtx corresponding rtx  refer slide time 33.52  to be updated rts of x okay to ts okay highest of this is what should be made equal to know okay now similarly we are going to look at the write stamp of x and then the ts okay the highest value is set to the highest of rtx ts is set to rts of x similarly for the wtx the highest of wts x and ts when a write operation is accepted the corresponding value is basically updated what this means is the rts and wts always reflect the time stamp the highest time stamp of a transaction  refer slide time 34.52  which either read or written on the data item x that s what actually happens by making this counters typically rts and wts are counters data counters they stored the value of the transaction that have highest transaction value that either read or written on this data items now let us go little more deeper and look at how exactly the basic time stamping algorithm is different from the two pl kind of an algorithm ill take a simple example and show you where exactly the difference is going to come when you apply a basic time stamping algorithm verses the locking algorithm you see remember write we have actually taken a simple case of  refer slide time 35.50  a transaction t one reading the value of x writing the value of x and then subsequently doing a read of y and then a write of y okay here basically x and y are the data items okay now i actually read the value of x write the value of x then read the value of y and write the value of y now let us say there is another transaction t two which exactly does the same thing as done by t one okay now what would have happen if actually i applied a two phase locking kind of an algorithm now t one needs to acquire a lock on x and lock on y okay and after that it does what ever then execute okay the commit okay after that release x and y lock on x and lock on y okay this is done at the end of the execution of the commitment okay so you going to release the lock on x and lock on y after the execution that means only at this point of time t two can acquire lock on x lock on y okay and then execute and this is only way t one and t two can proceed so unless t one completely finishes release is lock t two can not produce this is very important thing that happen if i applied the locking criteria right but if you carefully observe after this initial point of  refer slide time 37.58  t one trying to do rx and wx it is that point you can see there is no more use of x for transaction t one which means that its possible for t two to start executing from this point not at the later point of time okay now there will be a read y write y here okay and then there will be read y write y okay this is an optimal execution of this scenario okay but this wont be possible apply the locking strategy because locking would have required to two phase locking would have required you actually lock both these data item which means that is only at this point of time the lock on x will be released that means this will be shifted up to this point and you basically start executing the second transaction t two from this point okay so if you mark this is t one and t two you can see the overlap a significantly come down because you are not able to execute now this is the zone i could have shifted the execution of rx and wx to this point but this wont be possible if i apply two phase locking and let us see if it is possible for me to actually to do this if i apply time stamping based algorithm  refer slide time 39.38  now what would have happen is t one would have got a time stamp of ts one and t two would have got a time stamp of ts two okay now all that condition that we have is ts one is less than ts two which means that we have a case where transaction t one has been able to get time stamp which is lower than transaction t two now when they start executing at the end of at the end of execution of the first transaction t one it will writes the value of time stamp on data items x it is at this point of time the transaction t two will try attempting accessing data item x since the time stamp of ts two is um is greater than ts one as can be see here um its possible for um t two to access data item x at this point of time so what really will happen after this point is you basically will have transaction t two accessing the data item x and then writing similarly here at this point will have the y being accessed by transaction t one now the t two can start accessing y after this point of time now what can be seen here is that the overlap that we were talking earlier to recollect what we have been talking earlier is that its possible for the transaction t two to overlap with t one when transaction t one is finished with accessing x and now tries to manipulate y now this respectively prevented in two phase locking because t two can access lock only after it is released by t one and t one will not release lock on x till it actually reaches this point is basically the lock point for for the transactions so unless it reaches the lock point its not going to release the lock on x and hence t two will not be able to start executing till this point of time but where it effectly it can start executing from this point onwards that s what we actually looking at when  refer slide time 42.27  looking at the time stamping algorithm all that matters here is the ts one is less than ts two and that is the order in which it will be allowed both x and y will be allowed to be access by the transactions manager 42.45  refer slide time 42.45  so in other words both t one and t two will be execute in time stamp order and this permits in some cases more concurrency then what we seen in the two phase locking but remember this is not straightly a optimistic kind of an execution because we still looking at how the transaction should execute by looking at the transaction time stamps which were given at the beginning of the execution of the transaction not at the end right so it is fully optimistic in that sense in a fully optimistic scenario this would have been done by the transaction at the end of the execution for example t one would have written all its value t two would have written all its value and i will be checking which one should be sort of committing at the end of the execution that means both will execute to their finish and then i will actually use a validation point here and say which one of them will pass the validation and make that transaction commit where as here i am using the time stamp and using the time stamp to order these transactions in the beginning itself i know the t one has the time stamp ts one and t two has got the time stamp ts two when it started executing and now if ts one is less than ts two that is order in which t one and t two will be commit if it is other way around then the commitment is going to be t two before t one now this explain  refer slide time 44.32  what we see as a basic time stamping algorithm how time stamps are used for concurrency control now one of the things that we still didn t understand here is how this get integrated how the concurrency control gets integrated with the commit protocol if you remember we did this exercise even fore two phase locking when we integrating the two phase locking with commit protocol and that is the reason why we actually modify the two phase locking saying that the locks will not be released by transactions till the transaction commits because if it releases earlier the other transaction can look at the value and this will create cascading abort and other problems now similar thing happens in the case of even time stamping algorithm we need to see how the time stamping algorithms get integrated with the commit protocols now what will do is we look at a simple mechanism by which the time stamping algorithms get integrated with the commit protocol a simple exercise here will be to just look at not just the um how the transactions writes its value to explain the problem for example you can you can see here that there is a write takes there is read x followed by write x in the case of transactions x transactions one now if you take the transactions has returned actually the values at this point of time that is actually continuing to execute the other things before it actually reaches the commit point now if you understand the right x here the modified value of x is written this stage the modified value of x is written is strictly not correct because the transaction is not still reached the commit point here now let us say at this stage of commit for some reason this has to be rolled back which means that whatever the value that has actually been written here still needs to be un done which means that any body who is actually reading this modified  refer slide time 47.00  transaction coming after this would be reading the value that is written by this t one and that potentially creates a problem in terms of how the know transaction depend on each other in affect we will be relaxing the concept of isolating one transactions affects on the other and that s what causes this difficulty of relaxing this what we do in this case is we ill actually replaced this write in what we call as a pre write that means every transactions to start with will issue not a write but a pre write that means this write instruction that we write seeing here will be a pre write and after exactly it wants to commit and it reaches this last point here this is the point it should use a write transaction that means the pre write on x and this is a write on x the rep writes are not exactly written on to the database not written on to the permanent storage but they are buffered  refer slide time 48.05  pre writes are buffered and we will actually validate this pre writes and make sure that the pre writes once accepted or not rejected at a later point of time from consistency point of you and when a write is a actually issued by the transaction updating its pre write its never rejected its always accepted but the pre writes the transactions can still not commit a pre write which means that the commit stage it doesn t issue a pre write it doesn t issue a write its possible that the transactions pre writes will all be rolled back which means that the there is not going to be any affect on this pre writes on the actual database what we will see is now a modification taking pre write in to his account how the pre writes will try to solve the problem for for integrating commit protocols with concurrency protocols now if you remember we are actually two checks  refer slide time 49.07  when when a transactions issues now let us say t issues a pre write now this pre write has to be checked for pre write on a data item x now this has to be checked for on the database  refer slide time 49.24  items on the database times stamp now this will be checked again is the read time stamp of data item x and also write time stamp of data item x now if the pre write is pre write um time stamp of the transactions is less than either rts or wts x that means it is actually less than the write time stamp or the read time stamp then t is aborted okay otherwise otherwise we pre write  refer slide time 50.24  is buffered pre write is actually not written but what is done here is pre write x is buffered with its corresponding time stamp buffered with ts ts has its time stamp okay as its time stamp okay now what is this mean this means a essentially the following all the time stamps that we are talking about here are with respect to this buffered item here x for example if x is actually pre write pre write on x is buffered with time stamp ts now any any subsequence reads that we issue here needs to be checked again this pre write  refer slide time 51.24  now at a later point this pre write becomes a write on x then it is at this point of time the actual wts of x is updated to the corresponding ts okay that means this will stay like this for a while in the buffer when this actually comes write comes on x it is at this point of time it will be updated to a write time stamp of x now as these pre writes are buffered as the reads comes in to the system reads of a transaction come in to the system we still needs to check those reads again as any of the pre writes that are already buffered in the system now let us understand how the reads  refer slide time 52.15  needs to be modified in this particular case now if you typically look at read of x issued by a transactions t with a time stamp ts is how we will read this now this read x time stamp is ts now if you remember earlier this will be checked  doubt  the write time stamp of x and if the write stamp of x is less than the ts will allow the read to proceed because any number of reads can be done on the data item doesn t really matter as long as the read time stamp is higher than the write time stamp  refer slide time 52.57  because reads can all be done concurrently where as the writes have to be exclusive now in that sense it doesn t really required to be checked again the read time stamp what you need to do is you need to check only the time stamp of x in this particular case again as the write time stamp of the x now if you only do write time stamp of x okay is less than ts i think i will put other way around which will make things if write time stamp of okay x is greater than ts then basically this read has to be rejected for obvious reasons because we are actually read after some other transactions which came after you will be actually produces the write value the read issuing transaction is aborted transaction is aborted okay this is same as condition of the earlier one but other case read is allowed in the in this case the read is still not allowed when you actually have a pre write buffered okay now what is this pre write buffered means the pre writes buffered means if i have typically there is a pre write time stamp on x with a ts now i have to check this time stamp okay let us say it is tsp to just indicate that now this tsp is  refer slide time 54.45  actually okay less than the ts the ts is the time stamp that i am actually trying to read what does this indicate this condition indicates that there is a pre write buffered and that please remember that pre write will never going to be rejected when the actual writes comes in so i actually need to buffered in such a case we need to buffered we need to buffered the read transaction okay read transactions that means you actually postponed read transaction and allow read only after the write has been committed allow read to happen after the write happen after the write actually comes write on x comes this ensures that this typically will ensure that the read of the transaction happens after the writes affects now if the other conditions were the read on the transaction ts is less than the tsp that s going to be the other condition  refer slide time 56.02  then you can allow the read to happen because this in this particular case there is no no no pre buffered writes on the  refer slide time 56.14  on the transaction and hence this ts can be allowed to proceed read will be allowed to proceed okay will be allowed to proceed okay now what this shows is its requires just a minor modification in terms of how we handle when we want to integrate both commit and the time stamping protocols 56.38  refer slide time 56.38  all that we have to do additionally do is we need to actually make sure that um the writes are handled properly in this case by ensuring that they don t actually write on to the database to start with and produce the pre write what we ill do in the next lecture is we ill look at um host of other protocols which are most optimistic than the time stamp based protocols in the next lecturer transcriptor  c.udayakumar database management system prof d.janakiram lecture 22 concurrency control part  3 we will continue our discussion on um concurrency control algorithms in this class last few lectures just to recap what we have doing we looked at a class of algorithms try classify them into two broad class pessimistic algorithm and optimistic algorithm in the last class in particular we have see in depth where exactly this difference between optimistic and pessimistic kind of algorithm come class before we have seen how two pl works in the context of pessimistic algorithm and also we mention that this class we are going to looked at one optimistic concurrency control algorithm before we actually look at we also going to look at what we understand as time stamp based algorithms as supposed to locking we will look at time stamp based concurrency algorithms now at all time stamp based algorithms are optimistic algorithms that you have to understand time stamping algorithms essentially used the notion of time for serialiazation that s what they do time stamp algorithms need not necessarily be optimistic algorithms but one can formulate optimistic algorithm and really what we are meaning optimistic algorithms we were saying that the validity of the operations done by the transactions is checked at the end of the execution of transaction that s what we actually mean by the optimistic algorithm now its not necessary that all time stamp based algorithm check for the consistency of the results consistency of operation and by the transaction at the end of the execution they may do something in between they don t need to necessarily do it only at the end of the execution of the transaction its possible to do it so what we will do in this class is we will try to understand in a simple way which we call as basic time stamp algorithm try to understand how time stamp based algorithms work in the context of concurrency control what we are going to do is look at know how you apply time stamp based algorithms how the recovery properties commit properties and recovery properties are integrated in to the time stamp based concurrency algorithm if you remember when earlier lecture two phase locking we did discuss how locking algorithm are integrated with a commit protocol so we are going to do the same excersie of looking at time stamp based concurrency algorithm and also look at how time stamp based algorithms will integrate with commit based algorithms so that s what we are going to do is lecturer now we will proceed to look at basic time stamp based algorithm so what we will do is we ill explain the time stamp based algorithms  refer slide time 5.10  by taking a simple graph of basic algorithm which is called the basic time stamp based algorithm concurrency control algorithm  refer slide time 5.40  the algorithm as several basic things that um that s done like any other concurrency control algorithm the first thing the done is typically in this particular case every transaction is given a timestamp  refer slide time 6.18  this is if you typically look at you know very good example in real life of getting a timestamp and then executing what you want to do the darshan of lord venkataeswara know tirupathi tirupathi devasathanam as this thing called they give the time band know when you actually want to have darshan of the lord they say that you have to obtained a time band you know what exactly the time band is maens they give you the times sticker and says that if you actually go there know before the time or above just the time you will be able to to see the lord so the basically in other words the darshan of the lord is to be serialized because you can see one after other the queue one has to move to see the lord so the basically a serializable now one of the ways we solve this in real life by saying that you need this band know this band is generated what is the band means essentially getting a time stamp time band and says that you all the people whats the time band have to proceed the order of time that they have got that means they will be viewing the lord in terms of the time band okay in a similar way if you look at how the transactions are going to execute each transactions when it enters the system its get the similar time band that s what called time band how exactly this time stamp is given to the transaction later we will look at the step but the idea of giving the time stamp to the transaction is that essentially time stamp serialize the order in which transactions are executed all transactions are executed as um in the increasing order of the time if a transactions as got a time stamp its order in terms of execution is fixed because all those transactions which got time stamp before this transaction will commit before this transaction all those which got a um which got a time stamp higher than this can proceed and are going to commit after this transaction is committed that s sense in serialize order time the notion of the time to serializes the transaction execution this is um very important concept notion of time using the notion of time serilization is very important concept as we go further down we going to look at distributed transactions were a single transaction is split into sub transactions and execute on multiple sides this is like not now using single time stamp for example for each person could be carrying is own wrist watch so you get into additional problem solved of how do i actually take this different times and now put them all in to one zone foe example now london will be in one zone chennai will be in one time zone so if you actually generating transactions in different time zone you get into problems um now all of us using the single clock automatically everything will be serializable but if there use multiple time zones then you get additional problem of making sure that these events across multiple time zone are again serialized we are going to look at the notion when we go on look at the transaction and see how this notion of this time it becomes critical when you want to address the concurrency problem in the case of distributed databases so to understand at you know very simple level what really happening is transactions are coming in to the system giving them a time stamp and your excepting the transactions are executed with respect to the time and essentially produces the serializable to look at th basic steps in little more detail as i was trying to do here every transaction start with will given a time stamp this timestamp which will be used subsequently for checking whether the transactions are executing in a consistent fashion or not now all read write operations are tagged with the time stamp read operations and write operations the transactions at tagged with these time stamp write operations of the transactions are tagged with these keys will call these time stamp as ts so the all the read and write operations are actually tagged with this time stamp okay  refer slide time 12.06  that s the second thing you know when the transaction um issues are read or write operations then let us say transaction ti as got a time stamp ten twenty just understand or at this point of time is eleven thirty we will say the transaction got a time stamp of eleven thirty now subsequently whatever the read or write operations that are being performed by transaction all get the same number that s one one time stamp we are going to use for example in a laterc point of time eleven thirty five or eleven forty you doing something but all belong to same transaction they are not giving the time stamp this is very important all the read and write operations of the transaction are tagged with the same time stamp which is given to the start of the execution of the transaction okay now what do we do after this point of time we also have to actually to know okay for every data item in the data base okay for every data item x we need actually what is the read time stamp of the what is the read time stamp of x and read time stamp of x show the time stamp of the okay the time stamp of the transaction okay that as read the value of um read the read the value of x what is that mean this means let us say this is the data item x and x there is the transaction which reads this data item and time stamp let us say one fifty and say that is the highest transaction they could be transaction time stamp lower than this which also read this data item but we are going to consider the highest of the time stamp transactions that is the read the value of x will be the rts of x similarly the write time stamp of x the time stamp the highest time stamp if you want to actually qualify it the highest time stamp or ts of the transaction that wrote the value of x the transaction that wrote the value of x okay so for every data item you are going to do have two time stamps corresponding to the transaction highest time stamp  refer slide time 15.25  value of the transaction that is the read the value and also the highest time stamp of the transaction which wrote the value why i do need this do because this is essentially tells with respect to this data item what are the earlier transactions that i have actually done on the um whether the read or write operation are performed by the transaction on this particular data item now we will use this time stamp to see the validity criterion when a new transaction comes tries to do some manipulation on this data item we are going to look at and say which are all those values which can be allowed by the transaction manager to proceed and which needs to be aborted now this is the preliminary thing in terms of how exactly the algorithm maintains the data now in terms of actual execution what i does is whenever a transaction now consider the case when a transaction um issue okay a transaction issues the read operation the read operation now this read will be tagged with the time stamp of the transaction so basically this is essentially this is ts of the read operation okay time stamp of the read operation now ts has to be compared with the data item time stamp now let us say  refer slide time 17.14  the read time time stamp of x and write time stamp of read operation we will say on x a transcation with time stamp tx issues a read operations on x now the rts now wts the value of sx are the following now what is the condition now this read operation can be allowed to proceed okay now the read operation can be allowed to go further as long as the condition that the time stamp of x that the current transaction is greater than the write time stamp of x please remember the reads can be shared  refer slide time 18.00  after i write a value any number of people can read that value but if you assume that know if i am actually a value after somebody actually written a value on x which means that the my time stamp value is lesser than what actually been produced then i need to be little careful and avoid such operations understand the condition where ts is less than wts of x okay if okay ts time stamp of the transaction is less than time stamp of this current transaction is write than the time stamp of x what us does this implies that means a later transaction actually return the value later transaction than me as written the value  refer slide time 19.08  um a later transaction has written the value of x now in terms of time stamp order if you want to execute the transactions  refer slide time 19.20  strictly in the time stamp order this should not have happen because now i am actually reading a value produced by somebody who is coming later than me okay this is like saying that know um if you look at senior junior relationship you know now if you essentially says that i am actually i am passing the batch which supposed to passed in two thousand four says that its actually passing after the two thousand three batch there is a violation because two thousand three batch already gone which means that the value return by x is by transaction later than me or older than its me now i cant come back and say to read its value okay so unless i had actually finish the later transaction would not be able to come and o whatever is trying to do so in that sense now this is the case transaction t is to be the issuing transaction as to be aborted the transaction which is actually assuming if this is the case the transaction issuing the the transaction which issues which is sured which is should read the operation needs to be aborted okay and it is restarted okay and restarted with the higher order time stamp restart we are going to add higher values time restarted with a larger time stamp okay now understand what i am saying just going back  refer slide time 21.30  a transaction issues the read operation ts on x now the transaction time stamp is ts here as shown here and the current values in the database as far as rts wts the concern are shown here rts shows the transaction larger time stamp of transaction is read the value of x wts shows the time stamp largest time stamp of transaction which is return the value x now my time stamp is less than wts i don t need to compare this rts rts value is higher since reads can be shareable need not be exclusive i am still not violating any consistency thing only write i have to read something earlier return by transaction this is earlier than me rather than a transaction later than me okay this essentially shows that a later transaction has  refer slide time 22.39  written value of x the transaction which is issued the read operation needs to aborted in case and restarted with the larger time stamp and when you restarted a chance of distractions  refer slide time 22.55  succeeding his higher because now it will take this and proceed further and chance of the transaction succeeding later is very high this is what we will be done as far as the read operation is concerned now if you look at the line operation a transaction let us say with the write operation transaction with the time stamp with the time stamp okay ts issues a issues write operation okay now if issue a write operation you have to check now let us say there are again similar case of rts is the current value wts is the current value of the data item x write operation will say on x okay in which case the rts and wts are the current value now you need to check if either of the conditions are true ts is less than the rts rts that s the lead time stamp or ts less than the write time stamp of x then what us this implies this implies that i am writing a value into x which was read by the later transaction or return by the later transaction in both cases what i am saying is now i am trying to manipulate the value of data item but somebody comes later than me actually read as value return this value  refer slide time 24.55  in either of the case this results in in an inconsistent situation because who is somebody comes later than me as actually read i am actually modifying something would have read some stale value okay similarly i have read value that value will be last come then and start modifying it so in that sense both that conditions the write has to be rejected okay the write operation has to be rejected write on x to be rejected now what are the reject means same as the earlier case that the issuing transaction has to be okay the issuing transaction needs to be aborted and restarted with a higher stamp transaction needs to be aborted and restarted with a higher time stamp now this essentially explains their basic steps of the time stamping algorithm okay  refer slide time 26.17  now what we are do in this particular case so we are trying to say the algorithm tries to provide okay a basis for know checking we give um transaction a time stamp no we didn t still discussed how the transaction will be given an time stamp know several ways it which can be done one way is actually you can use a counter for example first transaction will be given a counter value of one the second transaction will be given higher number than this since all i am interested is logically showing that the transactions comes one after the other a counter is good enough for me only thing is the counter eventually might become un infinitely large means the counter value will become so large that at the end of it some where i have to reach at the counter so um at some point of time when there are more um brief period of time like you know this is period when everybody goes of for lunch know like that when the when there are no no transaction for brief period know in the system you can just forget the entire earlier history you can just set the counter zero and then start doing it again from one that point of time okay that s one way of actually giving time stamp to the transaction this is basically a logical time stamp because all that doing saying is transaction t two which got a counter value of two is higher than a transaction which got a counter value of one so in terms of actually looking at t one and t two all that that you have decide in terms of time order is the counter value okay then are various other ways of actually giving the time stamps for the transaction the other way is actually to use a physical clock okay physical clock that gives the values physical clock that gives okay  refer slide time 28.37  this is this is like saying i will use the clock time of machine okay the clock time and then i will basically say that every clock take i generate okay every clock take is used okay clock take is used okay is used for generating the time stamp generating the okay now there are interesting the this important because every clock take can generate only one time stamp so which means that  refer slide time 29.22  the number of transactions that can come limited now because they are limited by the number of clock takes because the time stamp will be generated only one time stamp can be generated in a clock take because two transactions need to get two different time stamp they cant get the same time stamp if they get it then you will have problems of validating them at the end of the um at at the end seeing how they will basically know you need to order them so you wont be able to order them if both of them get the same time stamp then interesting algorithm for generation of time stamp in fact there is a very very interesting algorithm which doesn t generate time stamp with respect to the starting point he generates with respect to the time when the transaction excepted to finish this is a extremely interesting way of looking at the problem in a entirely different perspective there is like saying when somebody entering in to iit it is at that point i give a tick to him or i give a band to him which means that is excepted now to follow every where else with that band so the band was given with respect to when actually came in to the system the other thing is when is excepted to leave the iit gate i actually put a band there saying that the this is the time were excepted to leave the system if somebody close up at the gate know after his time over you basically know abort that particular thing this is like saying that is excepted to leave the system within that particular time i i come to the gate at ten o clock now one way of saying that is i get the ten o clock at the time stamp the other way interest when we saying is i am supposed to leave this system by twelve o clock so i give twelve o clock as a time stamp which means that ending time is given that means all the transaction if the transaction didn t finish within that particular time period it will be aborted okay so possible for know for the algorithms to generate this time stamp in extremely interesting ways right that used for um some algorithms to actually fixed a band a band of time during which the transaction is excepted to finish the execution so one can intelligently use generation of time to see how the transaction execution can be controlled now has to to just come back to the point of how the algorithm works and then see for further issues with respect to the algorithm and how this algorithm is different from locking kind of an algorithm what we do in this particular algorithm  refer slide time 32.20  is we actually giving a time stamp ts to the transaction every transaction is getting a time stamp and then the read and write operations of the transaction read or write check again  doubt  okay the rts are the wts one of the things i have actually forgot to mention is when you accept a read operation or the write operation  refer slide time 32.54  immediately you need to actually update the time stamp of the data item for example i showed only in the earlier case how the rts and wts will be used to abort a transaction for example if those conditions are not met you go on abort the transaction but if you say that you gone to accept the reads and writes on the data item for example let us say now this read or write on ts accepted okay if the operation is accepted okay if the operation is accepted if the read or write accepted the abort conditions we actually saw earliest so i will just not mention them here if the read or write accepted then the rtx corresponding rtx  refer slide time 33.52  to be updated rts of x okay to ts okay highest of this is what should be made equal to know okay now similarly we are going to look at the write stamp of x and then the ts okay the highest value is set to the highest of rtx ts is set to rts of x similarly for the wtx the highest of wts x and ts when a write operation is accepted the corresponding value is basically updated what this means is the rts and wts always reflect the time stamp the highest time stamp of a transaction  refer slide time 34.52  which either read or written on the data item x that s what actually happens by making this counters typically rts and wts are counters data counters they stored the value of the transaction that have highest transaction value that either read or written on this data items now let us go little more deeper and look at how exactly the basic time stamping algorithm is different from the two pl kind of an algorithm ill take a simple example and show you where exactly the difference is going to come when you apply a basic time stamping algorithm verses the locking algorithm you see remember write we have actually taken a simple case of  refer slide time 35.50  a transaction t one reading the value of x writing the value of x and then subsequently doing a read of y and then a write of y okay here basically x and y are the data items okay now i actually read the value of x write the value of x then read the value of y and write the value of y now let us say there is another transaction t two which exactly does the same thing as done by t one okay now what would have happen if actually i applied a two phase locking kind of an algorithm now t one needs to acquire a lock on x and lock on y okay and after that it does what ever then execute okay the commit okay after that release x and y lock on x and lock on y okay this is done at the end of the execution of the commitment okay so you going to release the lock on x and lock on y after the execution that means only at this point of time t two can acquire lock on x lock on y okay and then execute and this is only way t one and t two can proceed so unless t one completely finishes release is lock t two can not produce this is very important thing that happen if i applied the locking criteria right but if you carefully observe after this initial point of  refer slide time 37.58  t one trying to do rx and wx it is that point you can see there is no more use of x for transaction t one which means that its possible for t two to start executing from this point not at the later point of time okay now there will be a read y write y here okay and then there will be read y write y okay this is an optimal execution of this scenario okay but this wont be possible apply the locking strategy because locking would have required to two phase locking would have required you actually lock both these data item which means that is only at this point of time the lock on x will be released that means this will be shifted up to this point and you basically start executing the second transaction t two from this point okay so if you mark this is t one and t two you can see the overlap a significantly come down because you are not able to execute now this is the zone i could have shifted the execution of rx and wx to this point but this wont be possible if i apply two phase locking and let us see if it is possible for me to actually to do this if i apply time stamping based algorithm  refer slide time 39.38  now what would have happen is t one would have got a time stamp of ts one and t two would have got a time stamp of ts two okay now all that condition that we have is ts one is less than ts two which means that we have a case where transaction t one has been able to get time stamp which is lower than transaction t two now when they start executing at the end of at the end of execution of the first transaction t one it will writes the value of time stamp on data items x it is at this point of time the transaction t two will try attempting accessing data item x since the time stamp of ts two is um is greater than ts one as can be see here um its possible for um t two to access data item x at this point of time so what really will happen after this point is you basically will have transaction t two accessing the data item x and then writing similarly here at this point will have the y being accessed by transaction t one now the t two can start accessing y after this point of time now what can be seen here is that the overlap that we were talking earlier to recollect what we have been talking earlier is that its possible for the transaction t two to overlap with t one when transaction t one is finished with accessing x and now tries to manipulate y now this respectively prevented in two phase locking because t two can access lock only after it is released by t one and t one will not release lock on x till it actually reaches this point is basically the lock point for for the transactions so unless it reaches the lock point its not going to release the lock on x and hence t two will not be able to start executing till this point of time but where it effectly it can start executing from this point onwards that s what we actually looking at when  refer slide time 42.27  looking at the time stamping algorithm all that matters here is the ts one is less than ts two and that is the order in which it will be allowed both x and y will be allowed to be access by the transactions manager 42.45  refer slide time 42.45  so in other words both t one and t two will be execute in time stamp order and this permits in some cases more concurrency then what we seen in the two phase locking but remember this is not straightly a optimistic kind of an execution because we still looking at how the transaction should execute by looking at the transaction time stamps which were given at the beginning of the execution of the transaction not at the end right so it is fully optimistic in that sense in a fully optimistic scenario this would have been done by the transaction at the end of the execution for example t one would have written all its value t two would have written all its value and i will be checking which one should be sort of committing at the end of the execution that means both will execute to their finish and then i will actually use a validation point here and say which one of them will pass the validation and make that transaction commit where as here i am using the time stamp and using the time stamp to order these transactions in the beginning itself i know the t one has the time stamp ts one and t two has got the time stamp ts two when it started executing and now if ts one is less than ts two that is order in which t one and t two will be commit if it is other way around then the commitment is going to be t two before t one now this explain  refer slide time 44.32  what we see as a basic time stamping algorithm how time stamps are used for concurrency control now one of the things that we still didn t understand here is how this get integrated how the concurrency control gets integrated with the commit protocol if you remember we did this exercise even fore two phase locking when we integrating the two phase locking with commit protocol and that is the reason why we actually modify the two phase locking saying that the locks will not be released by transactions till the transaction commits because if it releases earlier the other transaction can look at the value and this will create cascading abort and other problems now similar thing happens in the case of even time stamping algorithm we need to see how the time stamping algorithms get integrated with the commit protocols now what will do is we look at a simple mechanism by which the time stamping algorithms get integrated with the commit protocol a simple exercise here will be to just look at not just the um how the transactions writes its value to explain the problem for example you can you can see here that there is a write takes there is read x followed by write x in the case of transactions x transactions one now if you take the transactions has returned actually the values at this point of time that is actually continuing to execute the other things before it actually reaches the commit point now if you understand the right x here the modified value of x is written this stage the modified value of x is written is strictly not correct because the transaction is not still reached the commit point here now let us say at this stage of commit for some reason this has to be rolled back which means that whatever the value that has actually been written here still needs to be un done which means that any body who is actually reading this modified  refer slide time 47.00  transaction coming after this would be reading the value that is written by this t one and that potentially creates a problem in terms of how the know transaction depend on each other in affect we will be relaxing the concept of isolating one transactions affects on the other and that s what causes this difficulty of relaxing this what we do in this case is we ill actually replaced this write in what we call as a pre write that means every transactions to start with will issue not a write but a pre write that means this write instruction that we write seeing here will be a pre write and after exactly it wants to commit and it reaches this last point here this is the point it should use a write transaction that means the pre write on x and this is a write on x the rep writes are not exactly written on to the database not written on to the permanent storage but they are buffered  refer slide time 48.05  pre writes are buffered and we will actually validate this pre writes and make sure that the pre writes once accepted or not rejected at a later point of time from consistency point of you and when a write is a actually issued by the transaction updating its pre write its never rejected its always accepted but the pre writes the transactions can still not commit a pre write which means that the commit stage it doesn t issue a pre write it doesn t issue a write its possible that the transactions pre writes will all be rolled back which means that the there is not going to be any affect on this pre writes on the actual database what we will see is now a modification taking pre write in to his account how the pre writes will try to solve the problem for for integrating commit protocols with concurrency protocols now if you remember we are actually two checks  refer slide time 49.07  when when a transactions issues now let us say t issues a pre write now this pre write has to be checked for pre write on a data item x now this has to be checked for on the database  refer slide time 49.24  items on the database times stamp now this will be checked again is the read time stamp of data item x and also write time stamp of data item x now if the pre write is pre write um time stamp of the transactions is less than either rts or wts x that means it is actually less than the write time stamp or the read time stamp then t is aborted okay otherwise otherwise we pre write  refer slide time 50.24  is buffered pre write is actually not written but what is done here is pre write x is buffered with its corresponding time stamp buffered with ts ts has its time stamp okay as its time stamp okay now what is this mean this means a essentially the following all the time stamps that we are talking about here are with respect to this buffered item here x for example if x is actually pre write pre write on x is buffered with time stamp ts now any any subsequence reads that we issue here needs to be checked again this pre write  refer slide time 51.24  now at a later point this pre write becomes a write on x then it is at this point of time the actual wts of x is updated to the corresponding ts okay that means this will stay like this for a while in the buffer when this actually comes write comes on x it is at this point of time it will be updated to a write time stamp of x now as these pre writes are buffered as the reads comes in to the system reads of a transaction come in to the system we still needs to check those reads again as any of the pre writes that are already buffered in the system now let us understand how the reads  refer slide time 52.15  needs to be modified in this particular case now if you typically look at read of x issued by a transactions t with a time stamp ts is how we will read this now this read x time stamp is ts now if you remember earlier this will be checked  doubt  the write time stamp of x and if the write stamp of x is less than the ts will allow the read to proceed because any number of reads can be done on the data item doesn t really matter as long as the read time stamp is higher than the write time stamp  refer slide time 52.57  because reads can all be done concurrently where as the writes have to be exclusive now in that sense it doesn t really required to be checked again the read time stamp what you need to do is you need to check only the time stamp of x in this particular case again as the write time stamp of the x now if you only do write time stamp of x okay is less than ts i think i will put other way around which will make things if write time stamp of okay x is greater than ts then basically this read has to be rejected for obvious reasons because we are actually read after some other transactions which came after you will be actually produces the write value the read issuing transaction is aborted transaction is aborted okay this is same as condition of the earlier one but other case read is allowed in the in this case the read is still not allowed when you actually have a pre write buffered okay now what is this pre write buffered means the pre writes buffered means if i have typically there is a pre write time stamp on x with a ts now i have to check this time stamp okay let us say it is tsp to just indicate that now this tsp is  refer slide time 54.45  actually okay less than the ts the ts is the time stamp that i am actually trying to read what does this indicate this condition indicates that there is a pre write buffered and that please remember that pre write will never going to be rejected when the actual writes comes in so i actually need to buffered in such a case we need to buffered we need to buffered the read transaction okay read transactions that means you actually postponed read transaction and allow read only after the write has been committed allow read to happen after the write happen after the write actually comes write on x comes this ensures that this typically will ensure that the read of the transaction happens after the writes affects now if the other conditions were the read on the transaction ts is less than the tsp that s going to be the other condition  refer slide time 56.02  then you can allow the read to happen because this in this particular case there is no no no pre buffered writes on the  refer slide time 56.14  on the transaction and hence this ts can be allowed to proceed read will be allowed to proceed okay will be allowed to proceed okay now what this shows is its requires just a minor modification in terms of how we handle when we want to integrate both commit and the time stamping protocols 56.38  refer slide time 56.38  all that we have to do additionally do is we need to actually make sure that um the writes are handled properly in this case by ensuring that they don t actually write on to the database to start with and produce the pre write what we ill do in the next lecture is we ill look at um host of other protocols which are most optimistic than the time stamp based protocols in the next lecturer transcriptor  c.udayakumar database management system prof d.janakiram lecture 23 concurrency control part  4 in the last lecturer we have been looking at time based concurrency control techniques what we have been looking at in more particular is how the basic time stamp mechanism works in terms of ordering the transactions in terms of its time stamp we also look at how the basic time stamp protocol can be integrated with commit protocol towards the end of lecturer we saw how the basic time stamping techniques can be integrated with the commit protocol we just recap what we have been doing there we introduced instead of write pre write and then ask the transaction to issue a pre write instead a write to start with and pre write is going to be buffered and any read transactions incoming read transactions will be checked against the pre write to see if they need to be buffered they can be they can be satisfied the read transactions can be satisfied so this what we have done in the last lecturer we also mention that in the last lecturer basic time stamp mechanism is not truly a optimistic concurrency control algorithm because we are not actually looking at looking at um validation at the end of transaction execution what we are still doing is we ordering the transaction as they enter in to the system rather looking at a the end of the execution whether what they have done is truly satisfies the concurrency requirement what we are going to do in today s lecturer is to further continue looking at models of this concurrency control and see a slightly different kind of algorithms which include the truly optimistic version of a time stamping algorithm to start with what i will do is i will look at a time stamping algorithm time stamp based protocol that is truly optimistic sense that the validation may be done at the end of the execution of the transaction and we are going to look at in depth that particular protocol and also we will look at another different approach to concurrency control which is actually the multi version concurrency control algorithms the multi version solved problems of concurrency control by producing a new version of the data item each time you write the value which means that the old value is still preserved when your are actually producing a new value further data item and that s solve the concurrency control solve in a completely different way for example you actually each time you try to do something since preserving a old value a large extent a problem of concurrency control is elevated by maintaining a multiple copies of the data item but a consequent problem is that you will end up paying a huge you know overhead for this space a completely degenerated case is where you only not maintain the the new version and also the time at which this was done this typically comes down what to see temporal database because we actually record when a value is changed not only the changed value the new version of the changed value but also the time when it actually happen and that becomes what we will as temporal database concepts of temporal database suddenly the beyond the scope of this particular series of lecturers but i encourage to read um you know the material on your own to understand i will finish this encourage you to read a little bit more on your own on temporal database which constitutes a very important and interesting accept of databases by itself now as we go further down um what we going to do is we will start our discussion by first looking at truly optimistic time stamp based protocol to start with and then proceed on multi version protocols i am also going to look at multi version based concurrency algorithm and figures them version of the multi version um two version protocol in the context of two phase locking and we were going to look at in depth in the last class what i am going to do is i am going to review some of this things that i have done in last lecturers with set of questions and then giving more explanation what was actually done by the looking at a series of questions review questions that we can attempt on the last seven lecturers seven lecturer s in the seven lecturers we are going to have a review questions in the next class to we can be prepared on the on the on the things we have done so far so that you will be able to look at the review questions more carefully at the end of the seven lecturer now we will start with with the optimistic protocol now a truly optimistic protocol as we were taking about we have the approach to looking at the problem of concurrency control the cc algorithm we have the approach of actually doing the checking the end of the execution of the transaction now every transaction in this case can be thought of as having several faces the first phase can be read phase which means that the transaction the reads whatever is needed by it  refer slide time 7.09  all the data items that are actually required by it and then also manipulates but only one thing is doesn t actually write them back on to them database till such point actually the transaction gets validation so you have an extra phase here which is called the validation phase right now the validation phase make sure that different transactions if they are conflicting with each other enter in to the validation phase and then get validated and subsequently enter after validation phase in to the write phase okay which actually means that we have these three phases a read phase the three transaction read all the values and then you have the subsequent validation phase after the transaction finishes all the required things it gets in to the validation phase in the validation phase any conflicts are detected and make sure that if the transaction passes the validation phase it is in consistent fashion whatever is trying to do and after it enters the write phase is allowed to write the values of the transactions on the databases the transaction doesn t passes through the validation phase it is actually the transaction doesn t pass through the validation  refer slide time 8.15  phase is actually aborted the transaction that fails validation validation is aborted and restarted okay now as can been seen in the particular case  refer slide time 9.27  what really is happening is the transactions just enter the systems their read in the read phase they take all the required data items they do whatever they need to do and they come in to the validation phase when they come in to the validation phase the system checks for any possible conflicts between the various transactions and once the transactions passes the validation phase you that enter in to the right phase into the right value now the important requirement is here is validation okay how does the transactions get validate when they are conflicting with each other to ensure this what we will do is we will essentially look at the read sets and the write sets of the transactions read sets of the transaction will agree with tr and the write sets of the transaction now this is the important requirement because at the end of the day the end of the transaction execution what essentially we will do is we look at the read sets  refer slide time 10.33  and the write sets and decide whether there is any conflict and based on that we are actually going to decide whether the transactions validate against the each other or there is a possible in consistencies produced by the value transaction and that how exactly there transactions are allowed to commit or mean to abort come back and do the redo what ever they done in earlier this is truly optimistic because what we are doing in this particular case as you can see here is we are actually allowing the transaction to the read phase irrespective of whether they validate of they don t validate only when they come to the validation phase we perform this check once they actually validation is performed and it passes through the validation phase and it actually enter in to the write phase depending on the answer to this yes or no if it is basically the typically the transaction enters the write phase and tries to write the value on the database so this is how exactly as you can see this is validation sice it is performed in the end of the execution this is a truly optimistic concurrency control to this how exactly the system proceeds to execute on the other hand as we saw earlier the pessimistic case the validation is done at the beginning of the transaction execution not at the end of the transaction execution for example even before the transaction is allowed to read its need acquire to corresponding the logs only then it allowed to proceed to other phases which means that it will be blocked at the beginning it will never be asked to abort after its actually the logs for other than actually deadlock or something else happens in the transaction aborted for those reasons  refer slide time 11.39  otherwise the transactions once it get the locks we will proceed to execute finish its results write the values and then only it release the logs when in this particular case the transaction start to executing to start with and its get validated at the end of the execution to see what it as done correct or not and it is aborted if it produce wrong results and it is made to redo the things again so we have actually a completely different approach to actually a two different approaches to solving the same problem now lets us understand what exactly happens if you apply the true optimistic concurrency control now one of the things we will doing here is we need to record though we have looked at three phases  refer slide time 13.39  we will be needing lot more information to see transactions validate or not or they don t validate to explain this what we will what i will do is i will give intuitively what happens when we see algorithm and then go into details of the algorithm physically okay now as you can see if there is no read um if read sets of the transaction don t conflict with each other okay in which case it doesn t really matter how they actually went about at their activities okay if there is write set conflict write write conflict or read write conflict then i need to worried about how to order this transactions now what i look at is for example you imagine there is somebody in front of me okay who is actually gone done something database okay this is what we will call as the committed set  now this committed set as go ahead something in database before actually i came try to do something now i need to worry about what is the thing the committed set done and see if i actually conflict them respect to the committed set which is equivalent to say that this is tj which is committed now um you have to realize that i have to maintain information about the current committed transactions what all the data sets okay what is that 9i have done in the database now ill take at the committed database see which are those committed sets i am actually conflicting to see if i can validate now if there is conflict between them between the committed set and um my set okay i am the current transaction which is trying to validate now the committed set as to see whether there is a overlap between the read and write sets and there is no overlap between the read sets or write sets depending on that you can decide now what exactly um has happened before or after and based on that you can say whether i validated against them or not a simple case is if there is a conflict in the in the sets you have to make sure that a strict order is actually ensuring the ensured between the two transactions that conflicting against each other and you progressively relaxing this requirement of the phases one before or one after depending on how much conflict is really existing between these two transactions okay now we will start with explaining now what exactly is done to see where this conflicts coming how we can look at validating the transactions now to give this experiment explanation will take simple example will start explaining how the algorithm works as i just explain i will see um i will maintain for example tj is the committed transaction will take this is a committed transaction now ti is the transaction trying to validate okay transaction trying to validate now okay now what i am going to look at is how exactly the tj and ti  refer slide time 17.17  ti sets u know read and write sets of ti tj really conflict with each other and based on that we have to see whether tj should come before or ti should come in this case is already cleared before so the order is fixed since the order is fixed what i am going to now is since this order got fixed because tj ti is already committed ti is all operations should be enforcing the requirement comes after the tj now what to level ti should be coming after the after depends on the conflict for example if there is non conflict it doesn t really matter how ti actually work with respect to read and write phases but as you start seeing that is more and more conflicts in the system more and more conflicts in the system um you have to worry about how exactly this order is done okay  refer slide time 18.11  um let us see now how exactly this how can be further there is read sets and write sets the read and the write sets of each of this transactions have to be now looked at okay this is the first thing now the requirement um that i need to know read sets and write sets of the transaction is  refer slide time 19.10  the strict one of this often often causes problem because um this is the restriction that requires that we know appary what is the read sets and write sets of the transactions some of pre processing will be needed for us to be generating the read sets and write sets this is the first thing happened in the optimistic concurrency control some algorithms other than optimistic algorithms also requires that i know the read sets and write sets of the transaction that will be the assumption that means for example if i write my t begin and at this stage i actually gave what are the read sets and write sets of the transaction okay now one way is the user gives the reads and write sets that were the transaction writer programmer gives the reads and write sets  refer slide time 20.13  the other one is the compiler can look at and actually generate the reads sets and write sets if the compiler is generating the read sets and write sets suddenly it is going to be superset of the reads and write it might actually get executed when the transaction is executed i wont be knowing which are all the data item ill be needing i do a particular transaction so i parse it actually a start it time i might indicate a superset of the set of data item might actually access when i go to the um when i actually execute my transaction that s what see in terms of the reads sets and the write sets the other thing that we need to maintain as part of this what we see as the the write phase okay when it actually start and of the write phase okay start and end of the write phase similarly in terms of read phase we have to see the start and the end okay these are the times which we need to maintain each of this phases because based on this we are going to now say  refer slide time 21.22  whether the validation of tj coming before ti is true or not and that is the reason why we need to maintain for each transaction the write phase okay for each transaction we need to do this each tr waited as transaction we need to look at the write phase and read phase starting time and the ending time for both the read and write phase given this now let us look at what really we will we need to do in terms of validating  refer slide time 22.08  these transaction now every transaction as it is explain will have three phases every transaction will get into three phases here every transaction has read phase now a read phase is marked by start of the read phase end of the read phase the start time and the end time for the read phase then it enters the validation phase and then we have the write phase we again we have the um starting time and the ending time the write phase each of this transaction okay now this is maintain as part of the execution so this for the every transaction  refer slide time 22.38  when the read actually started when the write has actually started when the read has actually ended when the write has ended now if you understand the criterion that tj should coming before ti now there is a true conflicting between tj and ti which means that these basically some data item which are produced by tj i have to read by ti which means that data items produced okay what we mean by data items produced is the value of this is return by tj produced by tj or read by are consumed by ti okay consumed by ti now its very clear that unless the write phase of j is finished  refer slide time 23.33  the read phase of i should not started if it starts early then it means that this equation the equation we looking at that tj should have should have lesser know in terms of time order should be coming before ti would not be valid because ti would have read values not produced by tj but by some other transactions this is very important to understand so for this for to be validated conflict data item between ti and tj in terms of tj write some values ti reads some values which is equivalent to  refer slide time 24.24  conflict there are common items between the write item of tj and the read set of ti okay in which case we need to ensure that the criteria that is enforced is write phase of write phase of write phase ti of tj ended before ti read phase started okay before ti read phase started what i am try to explain here the the meaning of this is ti read phase would have read the value tj write phase would have written the values which means that the write phase of tj ending before ti ensures that ti read the correct value is produced by tj  refer slide time 25.04  since tj is already committed this ensures that ti is valid in the case where tj is committed and ti is read a value before tj is committed that means the write phase not ended which means that tj still not return the value but ti start reading those values which means that that conflict has not been resolved properly this will not be valid in which case this condition that tj is coming before ti can not be valid if this is not true to what we will ensure is first condition what we will ensure is that as i said  refer slide time 25.57  the write phase of this is this is the true tj know ended this ensures that before theread phase of ti this ensures that tj is strictly before ti because tj is finished all work and ti is coming the other condition where there are no common data items between the two write sets okay progressively we can relax at the end of the thing we probably we can say that there are no common data items there are no common data items okay before data items between tj and ti what does this actually mean this means tj is working on a separate data items ti is working on a two separate data item in which case for the condition  refer slide time 26.34  tj comes before tj will be notionally correct all that i need to do is their read phase of okay tj read phase of tj know start okay start read phase of tj the starting time is before okay the ti staring time ti read phase staring time read phase starting time okay what is does actually mean i will slightly rewrite the for you so that you understand the meaning of this read phase always has two times remember that read phase has a start time and end time now that all i compare to that is start time of tj and then i need to actually compare the tj read phase this will again compare have start time and the end time okay now all the i have to do is tj read start time this is what i am looking at is less than t is read start time read phase start time because since they don t conflict it doesn t really matter how they actually got executed for tj to come before ti all the should it done is because it read the data items from the database before ti take this condition satisfied then its find me tj is committed coming after that but all my reads happened after tj is actually finish the read phase now what is this exactly done in terms of just recap what we have done in this algorithm all that we doing is we are ensuring that  refer slide time 27.46  the every transaction supposed to be three phases the read phase validation phase and a write phase and in the validation phase we are actually looking for this transactions conflict with any previously committed transactions and the validation phase ensures that whatever has been done has um valid regerates reads and writes get validate the committed transactions and that ensures that whatever onec the validation phase is crossed the transaction safely go and commit all other future transactions come now can validate as transaction right now most interesting fact here is for example imagine there are two vehicles coming in to the to the iit if you know how the iit chennai is organized there is only one gate um to which all four wheelers can get in to the campus now let us say all of them are raising know to go in to some program our open air theatre student activites centre now here there will be a limited parking lot there only limited space in the parking lot okay now where do we enforce know in terms of who will actually win in terms of putting the car in the parking lot one thing is to say that i know my parking lot we will take only two hundred cars then i say that when enter i to the iit in gate i start giving the numbers one two three four five like up to two hundred say that beyond two hundred cars will not enter which means that the cars will be turned back the minute the two hundred and one car tries to enter in to the in gate that means it s a it is actually the in gate level i am actually forcing the concurrency control i don t even let cars inside my system once i know i can not handle but then i have the counter there which make sure this criterion is ensured before it enter in to the system the other one is don t enforce this rule there okay ill let cars go in in to my in to my through in gate but when actually reach the parking lot it is at that point i actually see which of them get in to my parking lot now the one which probably came later at the in gate raised passed the car which was in between which means that though at the in gate level its not really first car enter in to the but at the time reach the parking lot it is faster than the other car then i let that car to get in which means that the transaction is raised with each other for committing okay now when when they raised with each other they committed at the end of it that can al together different transaction committing if the car is entering into parking lot are not necessarily the cars that came in the same order in the gate if you remember a basic time stamp would have been ordering if i would have given a time stamp for each one of them allowed them to enter in to the parking lot has the time stamp so it is not truly optimistic in that sense a locking would have also working in a similar way except in a slightly different way where it have actually ensure that for each one of them specific lot somebody would have a token and that token would have been used for enter into the parking lot so in some sense a truly optimistic algorithm will allow its possible for example a car which entered but tries not to get in the parking lot or park for some reason it get struck okay then there is no point actually trying to reserve the lot for you at the end of the day because you let these who ever comes in you let them get them in and then assume that going to be less than two hundred cars at any given point of time using a strict order at the in gate doesn t really makes sense because you are given point of time assuming that the conflicts are very rare as they enter in there at the end of the day all the cars enter in your gate automatically committed for finding a parking lot but when you start finding there is going to be large number of cars that will be coming in allowing them inside the system doesn t make the sense because many of them to return back this what would have been done in the case of optimistic algorithm optimistic algorithm would have actually allowed all these cars in to parking lot so let us say there are only two hundred cars can be parked in the parking lot but then if you actually allow a large number of cars get in there many of them go back spending all the fuel of coming up to that point and going back whereas you know that you can only take only two hundred um cars inside so in the case where there is high conflict high contention it is good to actually apply pessimistic approach is like two phase locking because it ensure that you don t really know you don t really have lot of aborted transactions but whereas when you assume that there are going to be ten or fifteen cars are going anyway come inside put a large restriction on them ask them to take token all that doesn t really is worth that kind of approach doesn t really worth because at the end of the al this find their true in to the parking lot in which case optimistic concurrency control is quiet good to just sum up we see is optimistic concurrency control allows the trans actions to proceed but there will be wastage when they actually abort because they have to redo all the work they have done pessimistic approach is block the transactions and allow them to proceed only when the road is clear when the road ahead is clear so it make sure that um the transactions proceed and never abort and start proceeding they never abort so both have placed in terms of where there applicable and where there can be used now this sort of thing comes up our basic concurrency control algorithm discussion where put up in broad perspective the class of algorithms belonging to belonging to pessimistic broad class of algorithms belonging to optimistic algorithms what i am going to do in the next fifteen twenty minutes is to see a complete set of different algorithms they actually work in a different approach to just give perspective on is this two classes but a whole set of classes i in between that s what we are going to do in the next few minutes we have to start with i am going to do is i am going to look at a class of algorithms that are termed as multi version protocol now this multi version protocol are different because they tend to actually not you know overwrite the value but what they do is for example if the meaning of the multi version if you understand here is very data item x will have multiple versions okay now obviously for example x value is ten here now you rewrite the x value at later point of time what you actually get is twenty then this is going to be a new version of x it is called x version one assuming that the this is x version gnome okay so you typically tend to produce what we see is several versions each time you change a value for example now return it the ith time now is going to be ith value that your producing now in the traditional what we do is we actually overwrite this value since we overwrite the value the old value is completely lost that is where basically the problem was of what i have done is  refer slide time 37.13  correct or not is multiple people are simultaneously writing this value is possiblw what i have written what written by that is where we have problem concurrency control come in to picture now the problem becomes completely different if you say i maintain multiple versions for example you assume in the case of banking database where i actually simultaneously two people withdrawing from the same account or depositing in the same account you have to ensure that one finishes one finishes writing the value then other the comes a part of it otherwise one of it writes going to be loss but if you actually maintain multiple version the problem is entirely different for example i take one value produce a new version is take another value produces a new version is that all i need to worry is that versions produces a consistent there new versions being produced and these versions are consistent the problems becomes severe the older value is always with respect to that older value only producing the new value and if somebody takes the new value again produces the other value it becomes a new version of the old value now completely a generated case what we are taking in the beginning of the lecturer which is called the time based or temporal database now temporal database is also sometimes called historical databases they preserve the history of what actually is being done for example x value at ten time t okay so this x value twenty at time a different time okay so you basically record the time as an event as a couple that time becomes one of the elements the database values so value plus the time records at what value is this time and the value this is very important if you look at some kind of applications is like the  refer slide time 40.02  you know stock prices for example that stock price this point of time will be different from the same stock price stock value you know for example if you take a particular company like tcs stock value is going to be at a at a particular point of time stock value is something but at different point of time stock value is something is else so the database if you just give the stock value it is not really useful because you also need to know the current time this was actually done and that is what we mean by purely temporal database um the temporal here means component the time component value is also stored if you start recording the entire history that is that is completely degenerate of multi version database in the case of multi version database a multi version concurrency control you certain number of versions not all the versions okay that can be two versions which means that you keep only the current value in the previous value you not keeping values that are beyond that time that becomes a two version database its possible that you maintain n number of versions which means that all the values previous values of the data item is stored now what we will see is set of simple multi version protocols concurrency control protocol is to see how exactly the multi version protocol  refer slide time 42.20  i will also see that as a as a specific case two version two phase locking protocol which have been dealing earlier extension of this two version protocol in the case of a two phase locking protocol now let me explain how a general multi version protocol been working and later explain how it extended for the two version two phase locking what we will do is for every item data xi which i basically going to do record okay this is the meaning of this is the ith version of x okay ith version of data item x okay now ith version would have been produced some transaction now what i will record here is which is the transaction time stamp okay this is the time stamp of the transaction that is actually produced value time stamp of the transaction that as actually return this transaction producing this value okay producing xi now if some other transaction actually try taking the value of x and  refer slide time 43.03  producing this then it would have become xi plus one okay each time you write a value remember that you producing a new value new version of the value which means that it becomes if somebody wants to write on x xi it becomes xi plus one is no longer xi okay now the other time stamp that i will be interested in since this is the write time stamp the other time stamp i will be interested in a read time stamp now this is the it is remember it has to be the highest because they could be several transactions leading xi among them i am interested in the ith highest time stamp of a transaction okay that read the value of this highest tome stamp of this transaction that read okay the value of x okay i am making it simpler for you you can read at the end of it i can give you more refer okay so the two things as we do as shown in the earlier case the write stamp which gives the time stamp of the transaction that produced the read time stamp read time stamp of the transaction read the value of x  refer slide time 44.12  now what we are going to do now is for example to look at the write operation on a data item at the end of it transaction issues a write operation on a data item okay x okay now as it issues the write operations on x i have to find out the xthe ith version which is basically the highest okay okay the highest version produced okay highest version of x with time stamp with write time stamp less than this transaction time stamp okay or equal to most i means i look at the latest value of xi which is the current value which i should be taken for xi to be now operating upon now for this particular value i should see the write time stamp please remember write time stamp is the highest time stamp of xi okay read by another transaction for example there is already other transaction read by value now i am interest in finding out who read this value of xi now if this transaction time stamp which as read is greater than ts is current transaction time time stamp it means potentially this what i will be doing we will be wrong in this particular case because i am trying to produce a new version of this particular xi xi plus one but this is already read by somebody who is coming later than me which means that  refer slide time 45.20  i will be violating i should not be producing a value okay which was already read by somebody who should coming later than me show potentially in this particular case transaction should be aborted okay transaction is aborted and restarted if this condition is not true what i am going to do is if the condition is not true  refer slide time 47.21  that nobody has else read the value xi is now taken and then a new version of this value will be produced which is going to be xi plus one for which the read write time stamp will be said to ts and the read time stamp will also be set to ts that is the current transaction is actually produced this is how exactly a new version of the data item will be produced if the earlier condition is satisfied okay  refer slide time 47.39  what in affect is saying is if um i essentially look at when i am try to write the latest value i should be using to produce this value that i have here what i am going to look at is the write time stamp is suddenly less than the ts then i look at the read time stamp this and make sure that the read time stamp is no more than the xi time stamp which i am setting and if this is correct then i will basically proceed and produce the new version okay condition is here is less than the less than ts is also less than ts in which case actually we produce the new version of the version of the value now read operations are quite simple compare to this read will essentially what it will do is it will look at read operation of transaction on x will first look at all the sizes and make sure that the xi satisfying the rts okay read time stamp highest rts is less than the ts that means this is the latest value of xi which is less than the ts which the time stamp can read which this transaction read after this the ts value will be set to the time stamp of the current read operation that means the xi the highest xi will be taken always you read the latest value of xi and when you reach the latest value of xi then make sure that actually setting the read time stamp of this to the current time stamp which means now the rts the highest read time stamp of xi will be equivalent to the current transaction which read the operation okay how the read operation will be performed on the transaction this explains how the basic multi version protocol is execute  refer slide time 48.22  in the in the in the case of multi version protocol i didn t keep any specific limit on the number of versions that are produced by the multi version protocol what this means is the i can be any number now often the problem will be the that i will be ending in highest cost in this particular case because i have to store a large number of data items because not just current value but the previous values of all have to stored in the database which means that the database storage overhead is going to be somewhat higher in this case and that is one of the tricky versions of the multi version protocol that s were i actually paying a larger cost a more specific is actually the two version protocol which means that that keep only two versions of the data items two version protocol and we will look at these two version protocol extension to two phase locking okay if you remember in the case of two phase locking we actually using two locks one is the read lock and other is the write lock read lock is basically a shared lock and write lock is the basically a exclusive lock now what we have done is in the case of two phase locking every data item is either locked in the read lock or write lock more and in the presence of a read lock  refer slide time 51.15  another read lock can be allowed there is a write lock another write lock another read lock will not be allowed if you look at typically the matrices of what will be allowed in what case you ended up actually having let is this say read write lock which is the current transaction holding then you have a requested ones which are read or write these are requested okay and these are the holding okay currently holding now it is possible at the at the at the end of the day okay if it is basically read lock in the presence of another read lock is yes another read lock item you can also grant another read lock if there is a read lock this is going to be no if there is a write lock read will be disallowed okay if there is a write another write will be disallowed which means a data item on which read clock is there another transaction can not ask for write lock but it can ask for read lock since read is shared  refer slide time 52.12  if the transaction is holding write lock on a particular data item no longer you can ask for on particular data item this is how two phase locking on a particular data item things being prevent in consistent way now what we will do in the multi version protocol is we will introduce a new lock call the certified lock okay now what the certified lock will do is it will allow in the phase of write also reads  refer slide time 53.29  which means that you can read a previous value of the data item when somebody it will wholes the lock because its still not return value on the database okay for example remember x read lock data value item read lock that i have got and another transaction as actually got a write lock database but it still not written any value of this okay this is what we will do here is we will say this is the question mark here which means that it still returning wnow when the value its possible for me to allow write lock in the presence of write lock a read lock to be taken but then when actually this transaction wants to write it will upgrade the certified lock now when a certified lock comes it is going to write the value and both write and read value going to be disallowed in the presence of a certified lock now this becomes a two value in an extension of two version multi version protocol because the two versions old value which will be allow to read in the presence of lock but when actually the value is return that s it will be updated in the earlier it will becomes somebody can use so this is basically two version  refer slide time 54.00  extension of a multi version protocol that we see now to give better explanation what really we gain by the doing this again produce the matrix which we produce the earlier by saying that the earlier three locks now read write and certified lock okay and similarly will have a read write and certified lock now in the presence of a read lock u can still grant a read lock in the presence of write lock you can still grant a write lock but then certifies it is no okay in the case of a somebody holding a read lock you still will be able to grant okay a a write lock here because it can be previous value write value is basically no and all other cases going to be no because in the presence of certified lock you can get any other locks okay now the advantage of this when somebody is actually  refer slide time 55.15  got a write lock please understand intuitively what the mean of what we are trying to do is what we are trying to do here is when somebody is hold a write lock it is only intention lock i am intending to write actually not writing for example if the transaction executes for a sufficient long time others can read the value as long as i have not trying to commit commits again my old value so it comes before me finishes everything i don t need to like somebody trying to write in the railway reservation case take the form tries to fill the form but while is trying to fill there is no time to block everybody else from trying to commit or try to come before him and try to finish is transaction that when actually writes it and gives it that is the time it is going to be blocked not before that this allows a higher level concurrency but at the cost of actually in additional lock that i will be holding when i reach the that particular point this is extension because i am trying to i have two two values of a particular data item and allow to with respect to this two values who can come before me or after me that s what exactly is achieved when you use the two phase locking extension to the two version protocol and that s how its actually is more concurrency is compared to two phase locking this is the interesting extension to two phase locking using the concept of multi version protocol what we are going to do in the next class is take a few example for all the lecturers i have done and do review questions on the on the topic we have done so far okay thank you transcriptor  c.udayakumar database management system prof d.janakiram lecture 24 distributed transcation models um we been considering transactions in the data executing on a single system implicitly this is assumption that we been making do we didn t explicitly state that um underline assumption of the model transaction model is that the transactions are executing on the node and the data is also fully resident on same node what we mean the node is a single computer system on which there is an operating system image and on which the database is running and now the transaction which are part of applications are all running on the same system that s the assumption which we are making when we are actually explaining all the transactions models now consider an example where for example railway reservation system where sitting in chennai and your trying to make booking for train leaving out of delhi which could mean that you are trying to actually access the database that is resident physically on a computer system in delhi sitting in chennai now it could mean several things it could mean just that your actually trying to access the database to a physical connectivity which could be a telephone line a lease line a some kind of connectivity between yours interface which could be just display interface to the system in delhi which means that your still physically working on the computer system that is located in delhi that s could be one model but this will not be one model which could be present when you work with systems which are dispersed and which are connected by know wide area network local area network or this what we call as distributed system when computer systems are connected by by a network they could logically present um single node kind of a abstraction for the end user this is what we mean by the distributed system now when the data is distributed and the transactions can be executed on different nodes of distributed system we call the scenario as distributed transactions it assumes no longer assuming that the transactions execute on a single system computer system but they could be executed on different nodes of a distributed system to explain this scenario what i will do is we take a very example of a banking system and explain how the scenario look like let us say that we have a bank in chennai sbi branch now i have also a branch in let us say mumbai again sbi branch could be a different bank also and it is possible that i have an account one here and have an account two here okay and i am actually doing a fund transfer from account one to account to which means that i am going to do a debate here from account one by let us say two hundred rupees okay now its possible for me to credit the following thing on to the other thing in okay now this is data for account one is resident one which is basically node okay and the account to which is the other account that is resident on node two which is in mumbai now this means that physically the data is distributed in two locations one in chennai branch other in mumbai branch now if you assume that is to are connected by some kind of a network doesn t really matter what is the network but you can you can assume that physically these two nodes are connected by an underlined network okay this could be an atm network okay or could it be a fibre optic network or it could be variety of even a satellite network of between these two nodes so we are actually assuming that some kind of connectivity exist between these two nodes which affectively means that the information from one node to another node can be accessed or the data can be from one node can be accessed the thing in another node otherwise vice versa its possible now suppose to these if your actually seeing a single  refer slide time 7.15  case where both account account one and account two present on a single node both are present in a chennai this is what the case we actually look at account one and account two and all that we had was begin transaction end transaction as part of the transaction here and all this will be executed in the same branch of one system that means physically this entire transaction is executed on a single node now all the thing that we so far have been talking belong to this scenario where both account one account two are present on a single node and the entire transaction here tr is executed on a single node so tr is executed on node which is the single node here and account one and account two are present on the node there is no remote access this tr doesn t access any remote information both accounts are present on the same node this is ideally we are talking about when we are talking about  refer slide time 9.00  one single node of a distributed system as supposed to this if you consider this particular case we are talking about account one and debiting some other two hundred rupees from this account here and account two which is present on different node and adding two hundred rupees here now you imagine what could happen if these are executed on two different nodes now its possible in other case a power has failed in the whole thing is proceeding in the single system case is possible  refer slide time 9.28  that power could fail or other things could happen when the transaction is executing this what we consider we consider the acid properties of the transaction now if the transaction debiting and credting from one account to another account you should ensure that the properties are completely preserved as acid properties are completely preserved as far as the transaction is concerned now in a single node now when the power fails the log records could be used to make sure that the transaction will always be an in consistent state for example for part of a transaction executed that is credit part is executed the debit part is not executed this can be figure out from the log logs of the transaction and either you can do undo or redo the transaction depending upon the state of the current transaction logs record that that are preserved now the scenario becomes quite complicated if you essentially look at distributed system now um you imagine account one debiting is happening in one node account two crediting is happening in another node now its possible that part of the transaction is got executed part of the transaction is not executed for various other reasons firstly to knowing that they both got executed itself is an is an important issue which is like two friends separated in two different locations try to ascertain whether the other friend exactly the other than excepting to do  refer slide time 11.07  it requires phone call and also making sure phone ring is the person is present show many other issues it involved when people are physically separated and they have to coordinate and do some activity but on the other hand the present in the same node in the same room it becomes lot more easier when when they have to coordinate the minute they are physically separated when phone ring when somebody doesn t pick up does it mean is is their busy with something that s why he didn t pick the phone you have to make assumptions really relating to behavior to other node with respect to your node and also if you put a telegram it could be mode of a different communication compare to ringing using a telephone and trying to reach him so what is the mode of communication between these two systems okay between node one and node two what is the kind of communication primitives that are present  refer slide time 12.10  between these two systems that also makes important requirement when you study this of a model what you are trying to say is if you assume the transactions execute on a multiple nodes of distributed system the whole approach to concurrency control and commit protocols takes an extra dimension because you have to consider here the possibilities of distributed system what can happen in distributed system node can fail network can partition network can fail with results in the system can get partition for example there is a temporary network failure between node one and node two the node two may not be reachable from node one so all these are become important now the transactions take these in to account and they have to ensure the acid properties when these thing happen that s the most important thing so we will actually consider this model as a distributed transaction model model and we will study some concepts relating to distributed transaction model starting from this lecturer now i explain at very high level the distributed transaction model  refer slide time 13.46  and get start more details of some of more transaction models to explain the essential difference between centralized system database system and distributed system with respect to the transaction models we will take a very simple example and illustrate with respect to the simple example how exactly we can build distributed transaction model on top of distributed data systems now continuing the the discussion we had on credit debit transaction what we will see essentially in the case of distributed transaction is transaction tr which is seen there will have now two portions which is the debit portion of the transaction and the credit portion of the transaction now debit portion of the transaction is executing on a different account account one which is on a node one which is chennai node this is resident on a different node and this resident on node two now we call this in credit and debit two sub transactions of this root transactions tr now tr becomes the root transaction  refer slide time 15.10  so there is a root transaction which is the complete transaction now what this root transaction will do is it will actually spawn two agents which are nothing but two sub transactions agent one and agent two this is agent one is actually sub transaction of the main transaction agent two is another sub transaction two okay now a single transaction is split into two sub transactions so that these two sub transactions are executed on two different nodes of the system for example the root is going to be one node agent one is going to be in another node agent two is going to be in another node now agent is going to execute only the debit portion of the transaction and agent two will execute only the credit portion of the transaction that means it will access account one here and make sure in the account two hundred rupees is debited and here it will access account two and make sure two hundred added here now while doing this all the acid properties have to be preserved has part of this which means that atomicity concurrency isolation durability all the acid properties independently have to be preserved as part of this particular exercise  refer slide time 16.58  you don t want if power goes of in the middle of transaction you don t want to be unsure what really happen to your account on node one it should the preserver the property that two hundred rupees detected by this transaction atomically you know when it accessing this account other transactions are not allowed to access this account when it is modifying somebody not to allow the results before it finis its um computation now after finish it executing it is completely preserved on the database all this in a atomicity concurrency isolation durability properties now to achieve this issue at the highest level this is the model what we have is the agent okay the root agent spawning other agents okay agent one and agent one on different nodes these agent one and two are responsible for the um for executing the transactions on the local node and together they form a complete transaction  refer slide time 18.19  for example additionally we have to worry about if a to allow is done a one is not done then again we have the inconsistency because a one is debit a one is debit a two is credit so you don t want the debit being done not credit or viceversa on both have to be done simultaneously okay when this is done this also done this is also not done this is also done that s what we mean by atomicity either the whole thing is executed or none of its executed now to ensure that this kind of agreement is reached between the various sub portions or sub transactions that are executing on different nodes we need what is called an exercise a module that i actually does this okay which make sure that transaction properties which includes the commit property that all of them either commit or none of them commit okay the transaction the transaction end all these to preserve these we will have an all nodes manager running which is called the distributed transaction manager this is the responsibility of the distributed transaction manager this dtm stands for distribution distributed transaction manager distributed transaction manager now that the dtm on every node ensures that the transaction properties are obtained on each one of the nodes when they are participating in the execution of the transaction now each local execution which needs to again maintain okay transaction properties will come what we will see in ltm ltm is nothing but the local transaction manager and if you see ltm stands for local transaction manager okay if you see what is exactly um in the earlier side  refer slide time 20.24  the debit and credit here which are executed by agent one and agent two needs to preserve the transaction property and this is obtained by the acid by the preserved acid property for debit transaction and credit credit transaction this is ensure for agent one and agent two by the ltm ltm preservers the logs and all the related things done in the centralized database context the ltm make sure al the things are executed by the agent or as per the acid property and that is what is achieved by actually having the ltm  refer slide time 21.41  on each of the nodes to explain this is the model this is the distributed transaction model what you see at the highest level to just recap what we have been telling along the complete transaction now become the root transaction as part of the root transaction when you will have the sub transaction which needs to be started on different nodes of the distributed system now here agent one gets started node one and agent two started on node two agent one indicates a sub transaction the root transaction agent two indicates the other two sub transaction of the root transaction now for staring this agents and also making sure that the coordinate among themselves when they are committing corresponding properties are achieved by the distributed transaction manager which is called the dtm now the dtm responsible for preserving the transaction properties across the various nodes now when the agent starts on a local machine whatever its start executed in the local node it preserve by this local transaction manager that s means the local transaction manager responsible for ensuring that whatever is executed in the local node obeys the transaction properties and this is the complete model of the transaction distributed transaction model  refer slide time 23.17  what we are going to look at is we are going to loom at with respect to this model how the concurrency control and model the commit protocols are going to be executed in this context and suddenly this terms you know this becomes little more complicated than what we studied in this centralized scenario for example commit in the centralized scenario is just writing the logs and making sure all the values modifying by the transactions are return back on to the database and then if your using two phase locking then make sure that the locks are released so that other transaction can make use of now the data items modify by the current transaction so commit becomes a simple exercise just writing the logs and writing the modified values back on to the database now if you consider distributed scenario where there are multiple agents running on the multiple nodes of the distributed system the commit no longer becomes that simple now you you need to worry about whether node one node two for example some reason node two want to commit each part of its transaction and then you need to know node one should roll back you need some kind of agreement to be reached among the various nodes before actually commit the values on to the database system so this is basically called the commit protocol commit protocol is the most specialized protocol then all general agreement problem now all that you have to agree in the case of commit protocol case either to commit to abort there there is no other agreement that that you try to reach in the case of commit protocol because one could mean i want to commit my part of the transaction zero could mean that i could abort a part of the transaction so all the nodes at the end of this transaction that agents involved in distributed transaction have to agree for one or zero depending on one actually means that all together to commit zero means they don t want to commit so we typically we going to look at commit protocols and study various commit protocols for achieving consciences this is the first part of the distributed transaction model what kind of commit protocols are existing and how do they work with respect to various scenario like node failure  refer slide time 26.15  network failure partitions so we are going to look at various scenarios what could happen in the distributed system with respect to how this commit protocols really work that s going to be the first part of the lecturer going to focus on the commit protocols we are going to essentially looked at two kinds of commit protocols one is two phase commit protocol which has the name suggest does it two phases that means the first phase which participations are are prepared in the second phase actual commitment take place  refer slide time 26.59  and that is what we mean by the two phase commit protocol and this two phase commit protocol natural integrates with the two phase locking protocol that we have studied earlier case of a centralized database system so we will study the two phase commit protocol and study the two phase extension of the protocol in to what we see as the three phase commit protocol now the idea of this three phase commit protocol is when sudden things happen in the two phase protocol which we are going to see the protocol will get lock that means the protocol any way to proceed further till the recovery take place in other words certain kind of failures occur the two phase commit actually locks the protocol blocks the system whereas modifying it in three phase commit protocol we will able to avoid the wait for its actually becomes the wait three protocol in some sense in some kind of failure occurs wait three protocol are extremely important because when failure occurs if the system is not blocked because of the failure  refer slide time 28.24  it s a good property of the system because it allows the system to actually go forward and not blocked by the failure so wait three protocols are very interesting and important in distributed systems now from two phase to three phase commit protocol what we see is you can avoid certain kind of failures and make the system resilent for this kind of failures the system becomes more robust under three phase commit protocol as supposed to two phase commit protocol we are going to study the two phase commit protocol in detail and then going to study the three phase protocol in the context of distributed transactions to start with in this particular class we are going to focus on two phase commit protocol and study the two phase commit protocol in detail and see what under circumstances two phase commit protocol works correctly now after looking at the commit protocol they are going to look at concurrency control protocol in the context of distributed transaction  refer slide time 29.38  is going to be later part of the talk we will focus on the concurrency control mechanism as applied to the distributed transaction systems now the rest of the lecturer from this point will concentrate on the two phase commit protocol and we will try to explain in detail the two phase commit protocol now let us look what kind of scenario will be required when you actually looking at the two phase commit protocol now to explain what really commit protocol means  refer slide time 30.14  we are actually taken a root transaction which actually started executing on this is basically the begin transaction this is begin transaction now i have basically the agent one which is the debit portion of the transaction this is debit on node one debit on node one this is basically the sub transactions agent one agent two is the credit on node two this is particularly operating on account one and this is operating on account two assume that some point of time this end transaction now when the actual agent one and agent two are started on different nodes distributed system and they start executing on the um two different nodes at some point of time when i reach the end of the transaction i am going to look at log okay and at this point of time i have to decide whether i will be committing my transaction right i have reached this point now and when i reach this point i need to understand now whether agent one and agent two are willing to now go with writing um you know doing the debit and credit completely for various reason it possible that agent one or agent two may not willing to complete part of the transaction one reason could be account one is not present on node one or they account has been closed for various reasons which means that account one is not present on node one which means the debit can not produce logically further in which case the agent one whatever agent two wants to do agent one i am say abort my part of the transactions because i cant  refer slide time 32.33  complete my part of the deal i cant finish debit transactions on my node because account one been closed the other case is account one have sufficient funds let us say has no balance and your try to withdraw two hundred rupees from the account when there is no balance in the account again agent is to say look i am not going to commit my part of the transaction because there is no way i can commit this transaction is in this all cases both nodes have to agree both agent one and agent two have to agree to commit their transaction for this whole thing to go through  refer slide time 33.13  otherwise there is no point to trying through have one part of the transaction executed the other part not been executed now to ascertain this this commit protocol gets started ideally at the end transaction for example when you reach the end of the transaction you have to actually now start this commit protocol to ascertain whether this whole transaction can be committed or not which means that at this at this point of time all the agents involved in the transaction have to participate in this commit protocol to ensure whether they are committing the transaction or not committing the transaction okay  refer slide time 33.58  that is the part of the commit protocol now let me explain this by taking what we actually call as a coordinator of the transaction which means that we will first fix a coordinator for the commit protocol okay which means that there is one single node there is a node in the distributed system that is going to act as a coordinator for the commit protocol and ideally this is the one where the transaction has been submitted okay which will try to coordinate transaction has been okay that s what we mean by coordinator now all the agents where this transaction is been executed we call them actually participants they all participate in the commit protocol so they are called participants so all agents where the sub transaction is executed is called the participants where call the sub transactions is executed is called the participants so we have actually  refer slide time 35.37  two things in the two phase commit one the first coordinator and number of participants now the responsibility of the coordinator is to initiate the commit protocol and make sure some response from all the participants and based on to that the coordinator take a decision again inform all the participants and properly terminate the commit protocol at the end of the execution so this is what we will call as a coordinator each participant has a responsibility for its own local transaction it should replying on behalf of the local agent it should ensure that certain things are done by the local agent before the participant reply back to the coordinator with a reply now we are going look at full detail what are step that we will be done by the coordinator and what are the steps done by the coordinator participant in the case of two phase commit protocol now what we do in the case of  refer slide time 36.37  two phase commit protocol is the coordinator imitates the commit protocol at the end of the transaction he imitates the commit protocol by what we mean by imitating is he is send certain messages two various participants and make sure he get some response from this participant when he is ensuring when he is actually executing this commit protocol now this is the responsibility of the coordinator to start the commit protocol at the end of the transaction this he does by sending what he is called a prepare message to all the prepared message this message is a um special message which is called the prepared message okay he prepares the participants for this okay prepare message is sent to all the participants now after sending the prepare message the coordinator enters  refer slide time 38.09  what we call as a the wait state because still now decided on the final outcome of this particular weight state till all replies are received or some of this were basically the problem come if the if there is a problem in terms of some participants not responding because there is a failure either of the network  refer slide time 38.42  or the node then they are not going to respond if they don t respond what is going to happen in the coordinator so the coordinator basically puts a or time out period a actually starts a time out period immediately after sending the prepare message now he will wait for the time out period and if replies doesn t come within this time out period he is going to decide based on  refer slide time 39.18  what replies he got because he not got the reply he will ensure that you know based on this he will make a decision which actually make sure that even in that case we can still make a decision but probably he might make a decision on the negative side saying that i will abort which is the safer decision to make because when the participant comes at layer point of time it is always possible to ask the participant abort rather than to commit so you will wait for a time out period and after the time out period he is going to make a decision relating to he should be um committing or abort okay when time out has reached and replies are not come is going to make a decision relating to abort of the transactions rather than the commit of the transaction as far as the participant is concerned  refer slide time 40.16  participant actually decides now waits for the commit protocol that means to start with is in undecided state wait for prepare message okay from the coordinator now when you get a prepared message from the coordinator okay now we has to apply now the two things which participant can do participant can actually reply with a ready message what ready message means is that the participant is willing to  refer slide time 41.04  commit is part of a transaction that s what actually means by the ready message the ready message indicates that the participant is willing to commit his transaction that means it executed successfully part of the transaction and is willing to actually commit his sub transaction that s what it mean by the ready message now a before sending the ready message if he is willing and before sending the ready message he needs to actually ensures that before sending the um ready message what should be he doing he should be ensuring that all the logs are return because if he doesn t write the locks and reply with the ready message  refer slide time 41.55  the coordinator assumes that whatever may happen later participant is willing to go with the commitment of the transaction tomorrow you can not come back and say look i am not going to commit once you have replied with ready message you are committing your self to commit this transaction so its important for you write all the logs and also if your using locking protocol your not going to release the locks till you know the decision of the coordinator because if the coordinator says please come in you need to write all the values on to the database and then release your locks so before sending the ready message the participant should write all the required locks participant writes the required locks now if you carefully observed all this writing this locks preserving all the different properties of the transaction is achieved by the local transaction manager ltm ensures that all this local transactions properties are maintained  refer slide time 43.16  using the ltm now once the participant it s a ready message the coordinator now can decide if he receives all the ready messages now we can decide what needs to be done up to this point we call this is phase one that means phase one is preparing the participants for commit okay you are preparing  refer slide time 43.41  this is actually preparing phase actually your are preparing the participants for commit0ting only in the next phase phase two you actually commit  refer slide time 43.52  so there are two separate phases thats why called two phase commit protocol in the first phase you are preparing all the participants for committing in the second phase you are actually committing the transaction okay now its possible in phase one you reply back for some reason participant is not willing to commit because as we explained earlier it was sufficient for us all these could be reasons while the participant may not to commit his transactions i all his cases what the participant does is he actually sends an abort answer message okay which means that he sense saying that i am aborting answer for prepare is abort answer message  refer slide time 44.46  when an participant is actually giving an abort message participant is not willing to commit is part as the transaction participant is not willing to commit okay not willing to commit when coordinator receives this abort answer message he will be forced to take only in the second phase we are going to take that commit decision so phase two once the coordinator enters phase two he is actually making the decision to commit is deciding now on decision commit he is made now what are the different decisions its possible in the time out period coordinator receive all the coordinator received all ready messages from all the participant okay all ready messages  refer slide time 46.01  then now the coordinator in this particular case we will take a decision to commit the transaction that s obvious because all the participants in the transaction willing to all the agents transaction are willing to commit their part of the transaction hence logical for the coordinator to take the decision to commit in the entire transaction but before actually we taking the commit decision okay before sending the commit message the agent has coordinator has to write decision on a stable storage so what ever may happen later the coordinator can recover back see what is the decision that was made by him with respect to this particular transaction that s a logical thing which he does so we except in this particular case the coordinator receives all ready message commit decision is taken commit message is sent to commit message is sent  refer slide time 47.10  to all participants okay before sending the commit message before sending these are the steps the coordinator has to follow before sending the commit message he needs to actually write the commit log okay the commit message he needs to write a stable log is written a stable log is written this is important because whatever may happen to the coordinator he should be able to now tell rest of the participants at later point of time what is the decision that was taken on this particular transaction okay  refer slide time 48.03  when the participant receives okay now at the second phase phase two when the participant receives participant receives the commit message receives cm message commit message okay he the transaction committed and an acknowledgement is sent to the transaction is committed and the ack is send acknowledgement is send an ack is send is sent to the coordinator  refer slide time 48.50  to the coordinator now the coordinator when he receives in the second phase all the acknowledgement the coordinator on receiving the coordinator on receiving on receiving all ack that is acknowledgement will write complete log will write the complete log now at this point of this time the transaction is completed and it can be closed successfully by the coordinator now there is various cases in between this was talking about only  refer slide time 49.41  a successful completion on the transaction by the coordinator and it is proceeding from one phase to another phase to recap what we have done phase one this is actually in the phase one coordinator imitates the commit protocol and here he actually sends the prepare message is send to the all participants this is in the phase one first action of the coordinator and enters in to the wait state for a time out period for replies from all the participants now the participant who is waiting for the prepared message can respond with the ready message before sending the ready message participant make sure he write all the stable logs  refer slide time 50.28  and enters into a now a wait state of the decision of the coordinator now the coordinator after this point enter phase two the phase two is entered by the coordinator okay when he receives all the replies from the no all the ready messages from the participant or a time out period as a write now in either case it takes a decision if all the messages are received ready messages are received he commits the transactions writes the commit log and sends the commit messages to all the participants now when the participant in the second phase and receive the commit message they actually write all the transactions is committed committed and an acknowledgement is sent back to the coordinator  refer slide time 51.25  so that now he can write the complete log and close the transaction now its possible that in the first phase we didn t discuss the case were its possible for the participant to reply with an abort message okay now when the participant replies with an abort message or coordinator waits for an time out period in either of its cases the coordinator has going to take a abort decision and this will be communicated to all the participant and when receive the abort decision the participants are abort the transaction this is what actually happens with the two phase commit now actually before going to in details of looking at what are the different things can happen what are the different failures scenario that can be presented in this case and how this two phase protocol is resilent that kind of failure we will try first looking at what was the state transition diagram for the coordinator and the participant that explains this whole scenario in lot more detail and after that going to look at more details how the two phase commit protocol is resilent to certain kind of failures when the commit protocols is executed now to explain this what we are going ton take is this is actually the coordinator state transition diagram coordinator state transition diagram i am i am writing the state diagram of the coordinator to start with the to start with the coordinator is based on the initial state and from the initial state what the coordinator does is he actually sends prepare message to all the participants show this actually we will have some level of undecided state this is the state where the decision was not taken by the enter its wait state which is the undecided state now this is basically prepare message is send by the coordinator and he is waiting for the decisions from all the now its possible that actually he waits from this state possible for the coordinator from this undecided state that he receives the all ready messages this is what we actually seen okay now when he get all the ready messages he is going to take when the input to the coordinator is ready messages  refer slide time 54.27  all the ready messages have been um sent to the coordinator then your going to get the commit command commit message from the coordinator the state in which actually the coordinator reaches the state the commit state now its possible that the coordinator actually okay got abort answer message okay from the participants when he got an actually abort answer message and he is going to taken an abort command message that means is going to send an abort command to al the participants and that is typically when an abort decision is taken by the coordinator now its possible that now when from the initial state the coordinator can reach the abort decision if there are no sufficient replies that is the time out period when a time out period reach okay when s time out period reach its possible for the actually by sending the prepare message and gets to the undecided state so strictly speaking this arrow should come from the from the undecided state here which means that the this state from which arrow will come there there is a time out period and after the time out period the coordinator has to taken an abort command message that he waits for the time out period  refer slide time 56.09  and after the time out period he actually takes the decision to abort the message now if you look at the participant state transition diagram it look something like this to start with actually the participant is in initial state now the participant receives the he receives the prepare message and he answer with the ready message then he basically basically um enters the state of the ready state here now from the ready state its possible for the participant to reach when he reach when he receives the command message he could be receiving the he could be he could be sending an acknowledgement message and could be reaching the commit state if he actually receive an abort message abort command message again he will acknowledge but he will reach the abort state okay  refer slide time 57.09  its possible for the participant from the initial stage itself by actually responding with an abort answer message it could be reaching this state he actually sends an abort answer message with he actually reaches the abort message  refer slide time 57.27  this sort of summarizes what we have doing in two phase commit protocol what we are going to do in the next class is take this two transition diagram and then explain how the two phase commit protocol resilent for failures transcriptor  c.udayakumar database management system prof d.janakiram lecture 25 basic 2-phase & 3-phase commit protocol in the last lecturer we are just look at basic two phase commit protocol just stopped at that point were we just the basic protocol how it works actually two parts there one part is the participant protocol and other is the coordinator protocol how the coordinator protocol working is the coordinators starts with sending a prepare message and once he sends a prepares message from the initial state actually he is the state were he undecided so the prepare message is sent to all the participants once the prepare message has been received by the participants they ill reply back with the ready message to the coordinator if the coordinator actually receives the all ready messages then he will take the commit and he will send the commit message then the coordinator will reach the commit state and if he receives any of the participant actually says he wants to abort that s means for the prepare message gives a abort answer message then your going to taken an actually abort command message and this state actually the coordinator reaches the abort state it also possible for the coordinator to reach the abort state if he times out that means he doesn t receive the message from the participant then one of the participant failed since one of the participant is failed your bound to actually taken abort message right this is the basic part of the coordinator as far as the participant is concerned there is each participant site for the two phase commit is concerned each participant start with an initial state now he is going to get a prepare message in this point for prepare message he can basically give a ready message now he gives the ready message he is in the basically ready state then he is in the ready state now wait for the coordinator decision if the coordinator actually gives the abort command message then going to acknowledge and then reach the abort state even after actually you reply with the ready message it possible for coordinator can still take a abort decision because in that particular case the other participants may not have replied back unless he get the all ready message he cant commit the transaction right now in the other case all the participants you know replied with ready message the coordinator is going to send the commit message in which case the participant it actually commits then sends an acknowledgement that means he written all the values  noise  on to the database this is the basic two phase commit protocol that we have seen in the last lecturer and were we have stopped is just explain this protocol and then said that what happens in terms of different failure scenarios with thought that we looked at in the last class that s why we stopped in the last class now that several things can happen in this basic protocol for example coordinator can fail in the first phase or second phase participant can fail in the first phase or in second phase network could fail which means that its possible for the various nodes to get partition some nodes in one partition and some nodes in another partition so all these are possible scenarios as far as the protocol now one of the things we looked at what are the cases were this protocol is resilient which means that which are the failure scenarios were this will still work okay now lets us first take the case were  refer slide time 5.58  you know we are going to look at different cases for failures now the first kind of failure is a participant fails in phase one okay what happen if the participants fails in the two phase commit in the first phase okay now what happens in the case is the coordinator will time out  refer slide time 6.39  and then once it coordinator times out is going to take the decision of aborting or sending an abort message for example you here the coordinator is going to timeout and he is going to in that response is going to take an abort message abort command message in the minute actually one of the participants is not replied okay  refer slide time 7.02  so i think that s in the particular case the coordinator will time out and then and take a abort decision coordinator will timeout as a result is going to take an abort command message okay that s what going to happen when participant fails so obviously two phase commit is resilient for participant failures in the first phase now what happens if the participants fails in the second phase of the okay participant fails in the second phase that means in the first phase the participant has replied with the ready message it means that he actually replied the ready message and now he is waiting for the coordinator decision coordinator can take either a abort decision or commit decision now if you say that the coordinator has taken an abort decision or commit decision  refer slide time 8.09  you have to after recovery check with the coordinator accordingly as far as the you have concerned you have replied with the ready message that means you actually now all the logs and you cant release any of the locks for any of the resource till such time you know the decision of the coordinator right in this particular case your going to wait for the decision of the wait for the decision now since you wait for the decision since you have failed for the decision all that you have to do is after recovery that means after you recover from failure after recovery the participant should check with the coordinator participant checks with the coordinator for the decision what happen the transaction he has to check with the coordinator and accordingly he has to terminate the transaction that means based on the coordinator tells now coordinator need not wait for the participant because it replied with replied message  refer slide time 9.19  so in this particular case the coordinator is now feed to choose a decision depending on read message or if he get all the ready message rather than he take the decision to commit the message commit the transaction in which case the commit decision would have been taken so that participant has to find out what happens to this transaction he has to do accordingly one of the things is neither other participants will be blocked because of the participant failure the coordinator will be blocked all the coordinator has to do is to read the decision when the participant comes back he has to tell the participant about his decision and when he get the acknowledgement from participant is going to write the final log on this transaction it has been successfully completed that needs to be done in this particular case now its possible also for the failures to happen for the coordinator at different stages and also possible that both coordinator and participant also fail need not be just failures of  refer slide time 10.30  participant alone coordinator alone but right now we looked at what happens if the participant fails one or more participant fail in the first phase or in the second phase so if you want to look at what happens to the coordinator failure suddenly you have to now understand in which phase the coordinator has fail again we can see the phase were coordinator fail in the first phase if the coordinator fail in the first phase that means he sends the prepare message and after that has fail he is not actually taken any from this transaction  refer slide time 11.36  then he fail before the ready messages actually arrived at the coordinator the coordinator has not reach the decision but after sending the prepare message if he fail the coordinator has fail now way to recover is simple that participant have to do is they have to elect the new coordinator and then restart the protocol that means that the now he will send the prepare message and correspondingly he will take the decision about the transaction by running by the two phase commit protocol so in this particular case a new coordinator has elected assuming that there are no failures new coordinator is elected here and then the transaction is restarted  refer slide time 12.12  two phase commit restarted protocol and then based on that the commit protocol will be executed based on that one of the assumption is that the making here is that all the participants are live  refer slide time 12.24  they are not dead but if they are if it is not case were so what will happen is the new coordinator will be elected and he will be time out when he starts the protocol so he will make an abort decision okay so in that sense the protocol become reentered means any number of times any number fails um failures is occurred for example you elected a new coordinator now that coordinator will fails again in the first phase again you are going to again have the election algorithm to elect a new coordinator okay this is basically protocol is very simple now in the other case if the coordinator actually fails in the second phase this is an interesting problem actually the fact your saying that the coordinator actually fail in the second phase means that taken an decision right now it means that um decision has been taken decision of aborting or committing has been taken okay now one of the conditions that can happen here is as been decision known by another partition okay which we can say now there is a case were atleast one participant knows the decision coordinator before he fail okay  refer slide time 13.53  so the scenario one is of the participant received one of the live participant which means that he is actually live and he actually receives the coordinator of decision live participants okay received the decision which actually means that all that you need to do in this particular case is since the coordinator of decision is known the same decision can be conveyed to everybody  refer slide time 14.27  if it is a commit decision everybody will commit if it is an abort decision everybody will abort okay right it is very simple case where the decision of the coordinator is already known the coordinator fails some participant doesn t receives the decision  refer slide time 14.47  all that they have to do now is find out if any of the participants has received the decision of the coordinator if they have then will become simple scenario were that decision can be implemented by other participant also in this particular case it become normal that means is nobody actually block okay this is very important you are not waiting for somebody to recover back okay right for example if shooting is happening our our um  refer slide time 15.25  recording person is going out for tea or something then we are all blocked since the time backs know that s a blocking protocol you don t want to think blocked you wont actually even it goes and put in an automatic modes and off then we are not blocked we can continue our job okay similar way even one of the coordinators fails okay and it still wait for the participant to recover back from the failure it s a nice thing it s a very simple case that s how the model of the protocol we don t want to know any other participant suffer because of this failure suffer means have to wait now is you don t know when that recover is going to happen is you get indefinitely blocked if such a thing happen so one of the all the cases were seen so far two phase commit protocol is non blocked it doesn t block anybody okay now there is a interesting case is going to come in a now where we can see the case were okay the scenario two were the coordinator fail none of the live participants know about the decision of the coordinator  refer slide time 16.44  okay but again there going to be a sub cases here okay now we can see decision of the coordinator is known okay coordinator is not known in this case its possible that you know all the participants are live that means there is no other participant failure okay that means only coordinator has fail the decision of the coordinator is not known but all the okay the case where is only first case is not the second case okay first case were all the participants are live that means there is no participant failure all the participants are live okay in this particular case all the participants are live a simple thing a can do is you can restart the  refer slide time 17.54  two phase commit protocol by electing a new coordinator in this particular case you will elect a new coordinator and restart the okay and restart the two phase um commit protocol okay you going to see is  refer slide time 18.20  the case where if you say that the two phase commit you know um the participant also failed along with the coordinator okay right so there is not only a coordinator failure but there is a participant failure which means that now the rest of the people the interesting case is here  refer slide time 18.51  imagine that the coordinator took a decision of committing and that is only known to a partition and he actually wrote all the values on the of the database and then he he actually fail okay now if there are remaining participant elect the leader coordinator and then terminate the protocol it might contradict if the earlier coordinator were already taken an decision and that is know to this partition there is no way to recover back from this so basically this is the case were all the other participants in this particular case all the live participants will be blocked okay all the live participants are blocked till such time the coordinator recovers now anything except to wait for the coordinator to recover wait for coordinator to recover okay now often coordinator failures also means a participant failure because  refer slide time 20.09  the transaction were it started normally coordinator is one of the participants sides for example if you basically looked at how the transaction is started one of the sub transactions were the were it is been executed he also take up the responsibility of being a coordinator coordinator failure will be equivalent to a participant failure and that is the scenario were two phase commit will not be able to recover back and this is an important case normally you don t have an coordinator failure a coordinator failure normally means a participant failure only thing is if it is fail in the first phase then you are bit lucky because in the particular case the remain participant can reelect the coordinator can make a decision but if you actually failed in the second phase two after actually sending the ready message the you discover that the coordinator actually fail you will be in trouble because you know whether the coordinator is taken an decision or not take the decision so that is the case were the participant is forced to wait till the coordinator recovers back from the this failure okay now have explained for this what we are going to see is how this protocol gets modified to also allow protocol recover in the case where the coordinator fails in the second phase along with the participant and that modification as we see as the three phase commit protocol what the three phase protocol does is intuitively to understand what the three phase commit protocol does is when um the coordinator actually fail you know what is the state it state in which the coordinator is and this is very important distributed system this is the typical problem you will face in the distributed system you need to know the state of other node when things have actually failed or when the system crash if you know the state of the system is easy for you to recover back but you don t know the state of the then you are in trouble recovery back from the recover recover from that failure now in this particular case when the coordinator actually fail you wont know whether it is commit decision or an abort decision now what do you want to decide now is want to see is what is the state in which in it is to get out of this ambiguity for introducing a three phase that means the coordinator will not directly to get in to the phase of committing if you carefully observe the problem is in terms of moving um coordinator moving directly when it gets the ready message in to the commit state if you actually record in between a state where the coordinator says that now i have received all the ready message  refer slide time 23.32  now i am prepared to commit okay is still not committed but then now you introduce a prepare to the commit state for the coordinator which means that now you know whether the coordinator has prepared to reach the commit state or not if it is not reach prepare commit state then no harm done because he failed in the second phase but only in the third phase actual commitment would have taken place right basically you need to know the state of the coordinator when he actually die ow when um fail so in that you remove that you need to introduce a third phase that s were you basically look at the three phase commit protocol what were we are going to do now is see how it get modified for the three phase commit protocol or how the three phase commit protocol works and also look at what kind of failure scenarios the three phase commit protocol can tolerate or it is resilient to what kind of failure what we are going to do is for both the participant as well as the coordinator will introduce a  refer slide time 24.49  new state a new state were they have to first get in to the before commit state or prepared to commit state before they actually commit they cant directly get in to the commit state from the ready state now the coordinator part is going to look at something like this ill start with an initial state and now as usual send a prepare message to all the sides then i will basically reach the undecided state here because i have not still not decided on the state now if i basically get all the ready messages then i will actually enter what i call as a before commit state i have still not committed so i basically the reads a before commit state here now once i reached the state before commit state and i received all the commit participants then i take the commit message that means i get an okay all them participants then i will issue the commit message and then i reach the commit state after receiving okay form all the participants now its possible for me for various reasons to move from before commit state to an abort state if i don t receive okay from all the sides i can still end up in a abort state as usual i can also come to the abort state if there is an abort state message message from one of the participant then in response to this as usual ill take the abort command message it also possible to a timeout message i might reach the abort message okay now this is the diagram for the coordinator state diagram for the coordinator look something like this okay participant state diagram in the three phase commit would look something like this participant will be initial in the initial state when it receives the prepare message it will reach the ready state if it is willing to commit the transaction in that case the participant will reach the ready state in case it is not willing to commit the message then it will answer with an abort answer message and will reach the abort state from the ready state also it possible for the participant to reach the abort state it it receives the abort command from the coordinator which is possible for the um for participant to actually reach the state if there is an abort command message and then the participant reaches that state through the ready state if the other participants are not willing to commit the coordinator can take the decision to abort the transaction in which case participant will move from the debit state to abort state if all the participants are ready to commit the transaction which means that all of them have actually responded ready message then it is for possible for the coordinator to take a commit decision in which case in the three phase commit he will initially enter the state call the prepare to commit state which means that he will prepare to commit message to the participant when the participant receives prepare to commit message he will reach prepare to commit state now he reaches the prepare to commit state is not still commit the transaction is waiting for the final decision coordinator when the coordinator receives for all prepared to commit messages then he will actually issue the commit message it is a commit instruction in which case the participant will reach the commit state here this additional state is needed because is possible that if the coordinator fails um know after one coordinator and one participant fails reach the prepare commit state is still possible for participant to elect a new coordinator in which case it possible that decision to abort have been taken by um by the new coordinator in which case after coming back from um know after recovery from failure its possible for the participant to move from the prepare to commit abort state now this is the participant state diagram we looked at the actual failure scenarios here okay  refer slide time 31.06  now the case where the failures can occur as usual participant fail in the first phase second phase are similar similar the coordinator failing in the first phase is also similar only the case were we need to re look at the second case where the coordinator actually fails that is what we recorded there that means there is a coordinator failure in the second phase okay and there is also one participant failure okay or also we have a participant failure this is the case were we have a problem earlier now let us examine this case you can see what happens in this particular case in terms of you know how the recovery takes place n this particular case um as the coordinator fails in the second phase along with the participant now in this particular case there is going to be third phase obviously the second phase is not the last phase um now if the coordinator nobody receives the decision from the coordinator obviously at most the coordinator is likely to be in the second phase which means that the participant here could have been in the prepared to commit state okay prepared to commit state it has not actually committed okay so at most this state can be prepared to commit it can not be commit state the coordinator would also been this is for participant okay this is the state of the participant okay can be this now the state of the coordinator can be also before commit state now if these two are in these state the rest of the participants can still now elect a coordinator  refer slide time 33.20  and still go ahead with the protocol because they have not still committed yet okay they have not committed the transaction okay now in a simple case were the coordinator actually pass the second phase okay now you imagine this going to be a sub case in this particular case where the coordinator has passed in the third phase means it fail in the third phase not in the second phase okay now this is the becomes very simple case coordinator fails in third phase  refer slide time 34.06  this is the simple case because the coordinator actually taken a decision to commit and this is actually conveyed to all the participants otherwise you would not have not himself in the third phase because unless everybody replied for the prepared to the commit state he will not reach to the second phase in the commit message even means even one of the participant have received it then only it is cleared that actually moved in to the third phase otherwise he is actually not moved in to the third phase so in this particular case is easy for you just commit the message because you know the decision of the coordinator that it has been actually committed okay so this ambiguity of not knowing the coordinators state when um it actually failed  refer slide time 34.58  is taken off in the case were the state of the coordinator is known by introducing an extra phase were the participant and the coordinator directly don t commit but they actually reach before state before commit or before prepared to commit state before actually committing and that s how this ambiguity is actually resolved that s how the failure is handled by introducing an extra phase for commitment now one of the interesting thing is how the election is going to happen because every time actually seeing that if the coordinator is failing these equivalent to actually some kind of a leader election algorithm because your actually electing a leader whenever there is a coordinator failure there doing a leader election okay in a simpler way the leader election can be done by a signing in terms of each participant for each node okay an id okay and these ids could be an ascending order obviously every node is going to get the unique id here that means there is a unique id node is going to get and you can actually assign okay at any given point of time simple way assign the highest of the um you can use the these node id or highest node id okay so basically you can assign the live node okay with least id okay as the leader however this is not simple because now you actually find out you are the you should take as the leader because somebody has to dictate that the coordinator has actually fail that means what really is going to happen is if a participant discovers okay um its going to be only the participant who is going to discover that there is the coordinator failure because he is waiting for some decision from the coordinator that s how the participant discover that there is the coordinator failure so what happen is if participant discovers okay it can be anybody a participant discover that the coordinator has fail now what he do is he has to do an extra okay if you basically discovers that there was the failure of the coordinator then he has the option of finding out  refer slide time 38.04  are he has to do this process finding out who is the next know um highest or least node that is still live okay so he starts now looking for the possible least node live node and tries to know make that node okay as the coordinator that node also discover that the coordinator would have fail participant basically all that need is to is it has to see whether it has a least node live node and if it so basically it now elects himself as the coordinator starts running the recovery protocol okay so in this particular way any number of coordinator failures tolerate by this protocol  refer slide time 39.00  because all that protocol has to do in this particular case is make a participant whenever it discover that there is a coordinator failure figure out the next highest or least node that is supposed to take over and it turn is to take over it will basically become the coordinator tries to run the protocol okay now there are two ways this can be further achieved in terms of how this protocol can be run by the coordinators one of the things the properties has to ensure that protocol is a reentrancy of a protocol that means the protocol can be run any number of times that means a coordinator fail then a new coordinator takes over now while running the algorithm protocol that coordinator also could have fail which means that now another coordinator has to be elected it has to take over this should become reentrance that means any number of such failures should be possible when your actually recovering from the failures in which case basically the protocol is called reenter protocol that means it is tolerant to multiple failures of the coordinator any number of times okay so it becomes a reentrance protocol now there are two ways to reenter protocol can be configured one is a proactive way of actually  refer slide time 40.35  making it move forward the other is basically know a pessimistic way of reentrance okay this simple way what the coordinator can take as the decision is if the coordinator is in a state of before commit state okay he will basically whatever the state is he basically enforces that on the other people okay um let let us understand point carefully the coordinator has failed in the first phase which means that node decision has been taken by the coordinator by now a new coordinator is in place how this new coordinator is to start the protocol okay now a new coordinator realizes that obviously the new coordinator is not likely to be any other state other than the first phase he is not received any of the coordinator so he is going to run the entire protocol staring from the first phase  refer slide time 41.45  now he actually imitates the prepare message to the other participant this the ready message then get in to the second phase then goes in to the third phase things like that okay now the new coordinator can be further improve the protocol is whatever he is state he is in okay he can actually find out if there is any other participant who is in a before commit state which actually means that  refer slide time 42.22  now he can actually need not restart the protocol but he can knows that actually the decision of earlier coordinator is actually to commit the message now you could actually when there is a before commit state it is possible for the protocol to terminate as an abort because the new coordinator now ignore the decision but then just conducts the  refer slide time 42.51  sends a message to the all the participants now some of them not going to reply back to him then he can actually enter the abort decision is quite possible for the new coordinator to find out one of the participant has reach the commit state in which case actually it can force the protocol to a commit state okay you just explain this a little more probably in a using our diagram you can see here the coordinator actually the new coordinator has elected  refer slide time 43.30  now if you basically receive the um you know prepared to commit message which means that he will be in a before state he is still not received this message which could have made the whole protocol terminate the abort decision okay now if you actually checks with the other live participant and at least one of them has actually receive the pcm state but is not the new coordinator this some other has been elected has the coordinator now then it still can take a commit decision because everybody would have reached a that state so it is possible for unless all of them replied with a ready message this state would have not reached right now the new coordinator can take a decision to commit a transaction and then make it know basically all that required for the elected coordinator in this case is  refer slide time 44.29  not only check its state but then checks the state of all live participant based on that restart the protocol at an appropriate point it is done like that the reenter protocol terminates  refer slide time 44.47  in a forward direction in terms of committing whenever there is a possibility of a protocol to commit it will commit rather than always sending out of the abort state depending on the state of the new elected coordinator so this two approach are possible in terms of recovery from the failures as far as the coordinator as concerned i think i am going to do is in the next two minutes is not only look at this scenarios of failures but also introduces now the concept of what happens with the network failure because your now talking about only node failures so far we are not talking about network failures as part of the recovery process now in terms of network failure as far as the protocol is concerned the two ways of actually interpreting the network failure  refer slide time 44.51  network failure actually means that the nodes are not reachable okay so let us say for example node here this node could be a chennai node and this other node could be delhi node okay now all that your looking at in terms of system is both of them are connected by the timings of some let us say back bone network that could be several network its involved connectivity with been these two nodes all that you have it basically a network which is actually providing this connectivity okay now when the network actually fails in a simple case this actually surfaces as the time out on the other side how actually your are going to discover okay you waiting for a reply and this scenario actually comes as the timeout in our diagram as we saw earlier and that timeout could be possibility  refer slide time 47.00  because the network has actually fail not the node has fail okay in either case it wont be able to detect the difference between a network failure and a node failure because the node as far as for example for this chennai node is concerned doesn t receive the reply in time from the delhi node it could be  refer slide time 47.24  network failure it need not be a node failure the node can be still be kicking doing some work there but when it actually sends a message let us say ready message chennai node doesn t receive the message this is equivalent to actually a node failure that means a network failure in some sense actually translates to a node failure the simple case this translates to a node failure okay but then it is not probably appropriate just consider  refer slide time 48.00  a network failure is just a node failure in our case it still works because when the timeout actually reads out the other side all that assuming now is the decision of the node is to abort okay so nothing no harm is done in this particular case but could have done positive thing is ends up in the negative side because you could have still commit the because the node has still respond with a ready message but since it doesn t reach the the other node and it decided that when it didn t have any information that best thing it can do is abort the message or abort the transaction that s what actually assumes then abort so basically network failure and node failure in a um normal case both appear to be know treated in this same way right and that s how this protocol both two phase commit and three phase commit are resilient to network failures in terms of transients and participate them to the failure okay but one has to be careful when the network actually fails but then failure is not just to one failure of one node but it can result in not able to reach to multiple nodes okay so typically this scenario is very simple case where it has been translated to a timeout case  refer slide time 49.43  but then assume that a network failure actually results in multiple nodes are not reachable okay now let us imagine we have node in chennai okay let us say this is chennai node and then we have actually bangalore node okay and then also we have mumbai node here okay and then we have one of delhi okay now its possible that an network actually connecting them let us assume that complicated network here it actually connects here all the nodes with one another but then if there is actually some kind of hub the southern nodes together and the northern nodes together it quite possible that this hub actually fail which means that these nodes are can still each other in this direction and these still nodes can reach each other direction okay now assume that chennai is the coordinator now as far as chennai is concerned the whole problem translates into the failure of both mumbai and delhi because it still reachable bangalore is still reachable and let us say it replied with whatever required messages so this basically results in partitioning of the whole set of nodes now you have actually two partitions there is one partition one here which consisting of mumbai and delhi and there is another partition two consisting of chennai and bangalore now itself if you carefully looked at scenario and try to understand what could have happen in this scenario  refer slide time 51.40  its possible that chennai and bangalore could start detecting that there is participant failure which means that two participants have failed  refer slide time 51.56  and two phase commit is resilient to participant failure so what basically they do is depending on which phase your in you might end up in actual about the transaction okay so basically these two might right to run this protocol saying that there are two participants failure now as far as mumbai and delhi are concerned okay for them translates to  refer slide time 52.29  one coordinator failure one participant failure they also try recovering back from this problem by saying that now they ill have a new coordinator depending on what phase they are in you should be handling this problem they might actually try recovering back from this failure okay now in this case its possible that the decision of two partitions need not be consistent  refer slide time 52.58  and this is happening mainly because the network um the nodes are delivering it s a network failure they are seeing them as failures of the participant nodes the participants nodes are still live and they have working so for each one of them scenario is different unless this is discovered that is network failure not an node failure you wont be actually making an distinction network failure the node failure and this really results in a complicated situation were even if you use a three phase commit protocol were the network actually partitions the nodes into multiple partitions you wont be able to recover back from the problem in other words your making an assumption here is there is no partitioning that is happening this is the case no partitioning of the node and this is an assumption with your actually working on the two phase commit protocol if the network partition you have the problem of recovering back from the failure because all the case which  refer slide time 54.18  discuss which we looked at are actually taking them participant failure and not really participant failures and is a network failure basically will have a difficulty an interesting assignment could be what are those cases were the network partitioning um network partitioning can lead to problem which is the case in which the protocols the states of the coordinator and the participants which case the partitioning could be a problem the other thing is to discover if you are the majority partition probably if you are the majority partition probably you can still go out do something which means that you have to discover that the system as actually partition and whether you are the majority partition and based on that probably can recover out of the failures it still a very open ended problem in terms of how one can recover from network partitioning it all is interest in see how one can make a distinction between network partitions and node failure for example one way is to actually if you just do a thing know you only know node is live or not okay that s still remains the interesting problem in terms of seeing how one can make a distinction failure and a network failure in a distributed system right now what we have seen is the basic commit protocol in the next class what we are going to see is how this commit protocol can be integrated with concurrency protocol for example how the two phase locking can be integrated with two phase commit protocol that s going to continue with the next class transcriptor  c.udayakumar database management system prof d.janakiram lecture 26 concurrency control for distributed transactions okay in the earlier lecturer we just looked at how to face commit and three phase commit protocols work what we are going to look at in the lecturer is concurrency control protocols for distributed system how this concurrency protocol can be integrated with the commit protocol in the first few minutes we are going to look at is how the two phase locking protocol can be integrate with the two phase protocol for the distributed system now what we have is basically in the two phase locking there are two phases of execution which we have seen earlier there is basically a growing phase in which the transactions acquires the lock and then there is an actual point were they start releasing the locks and the rule here is that in terms of time this is the time axis now in terms of the time axis what your looking at here is a particular phase were basically called the growing phase where the transaction is acquiring locks and the this is the shrinking phase were it is releasing the locks now one of the condition is this is basically call the lock point so if you actually released one lock you cant ask more lock that s how basically ensures that transaction is always executing in a serializable order now this is basically transaction execution point for example this is the start point and this is the end point now if you release a lock for a transaction then somebody else between can read the values of this data item that we have actually modified  refer slide time  3.30  for example we assume that even before you have finished you release a lock on the data item it actually means that somebody else can now read the value of that data item right assume that there is a data item x okay and now you actually have acquired a write lock on x during the growing phase and then release the lock in shrinking phase now its possible that you have actually modify the value of x let us say x is now modified from earlier value is twenty now it modified itself thirty now during this phase when you release the lock somebody can acquire the lock which means that they can read the value of x which you have modified before you have actually committed which means that if you now to abort then the transactions which read this value of  refer slide time  4.34  earlier all you have to abort this will result in a cascading abort if you allow transactions to read the values other transaction to read the values of the data item before the transaction is actually committed okay so in general execution of two phase locking is not correct because you can not release the locks okay before you have actually committed so what modification that when you integrate with two phase locking commit is you will basically acquire all the logs you ill come to the commit point but um your lock point and your going to keep this locks till the end of this transaction execution plus the commit phase only after the commit phase after you have committed that this is the point were you have actually commit point your going to release your locks in terms of the time scale this is the number of locks that you have okay so the number of locks are actually increasing with time transaction is actually started at some point here and it is started executing it acquired all the locks it required the point actually where it reach it required all the know all the logs then it continued execution somewhere her it actually finished execution that means this is the execution phase okay now after the execution is over the commit protocol is going to be started now because then it has to decide to commit then this is the zone which the commit protocol gets executed and after the commit protocol is executed the transaction actually finish execution then the locks are released at the same time if you release the locks earlier this will result in  refer slide time  6.50  isolation property being sacrificed and other transaction is able to read the values of still uncommitted modified value of this transaction so your not going to release the locks as shown in the earlier diagram nut its integrate both the locking and the commit protocol together and after execution is over  noise  you actually run the protocol and then release your locks okay this is how the two phase locking is integrate with the commit protocol now if you want to see in the distributed setting  noise  how the distributed two phase locking will work it is actually a simple extension of the two phase locking because what we now going to see is distributed two pl  refer slide time  7.46  plus integration with the two phase commit two phase commit is for distributed transaction because commit protocol make sense only then the you have multiple nodes where your transaction is executed otherwise there is no question of involving two phase commit there because you have actually finding out the participants can all commit transaction there is only single node that nodes knows know already reaches the commit phase all that now is to decide is whether its committing the transaction or not now in the case of distributed two pl how you are going to execute the transactions you basically have a coordinator which we have shown earlier this is basically the route transaction and the node transaction actually spawn sub transaction which means you actually have the tr one is here and there is a tr two here now the tr one is actually execute a two pl which actually means the it acquires all the locks that it actually needs and this other transaction also acquires all the locks it needs in terms of the two pl  refer slide time  9.10  okay now each one of them when they reach the commit phase have to decide whether they have to committing or not committing when the actual commit message has been taken by the coordinator that is the time they will commit and release the lock okay imagine that there is actually a data item x on which this transaction is operating t one is operating there is a data item y transaction two is operating okay now t one will acquire locks on x t two will acquire locks on y they finish the their part of the execution  refer slide time  9.56  now at the end of the execution each one is going to decide whether its going to commit its part of the transaction that s were two phase commit will start know execution out two phase commit protocol is run you figure out whether both of these transaction are willing to commit now based on that based on the commit protocol outcome they will commit the transaction and then release the locks now the important thing is what we discuss is the earlier lecturer is what happens when failure occurs then what is the problem of blocking now you assume that t two gets blocked transaction get blocked which actually means that is not going to release the lock is because it acquired since the time it actually made a decision on committing or abort the locks are not released till the commit has actually finish which means that the other transaction is try to acquire locks now on these data items will be blocked from acquiring the locks so basically blocking in the sense is not good for the simple reason that the resources are blocked in this case data items are block from access by other nodes what i am going to do in the slide is the tension of two phase locking protocol the basic two phase locking is used on the actual nodes that means each node will run a two phase locking for the data items its actually  refer slide time  11.44  accessing but this two phase locking is used for local data items you not further looking at tr one accessing a data item that is remote to that particular node okay and if you basically what look at that model you will realize there could be problems of applying two phase locking  refer slide time  12.12  where each sub transaction could access both locals data item as well as remote data items assume for example have a root transaction here this is the root transaction and then you basically have two sub transaction t one t two here correspond on two different nodes n one and n two now let us say n one as actually data items x y and let us say this have p and q okay now assume that t one actually wants to acquire a lock on x the read lock on x and then also want to acquire a read lock on let us say p remote lock on the now which means it has the send request lock to the other node n two okay now for example already locked by t two you have actually know t two has actually a acquired a write lock already on p  refer slide time  13.25  now unless you have understand t one and t two are the part of the same transaction t two will be blocking t one from acquiring a lock on p normally if you do in the distributed transaction model is to access the local data item you will actually spawn a sub transaction here you wont actually allow a sub transaction on one node  refer slide time  13.49  you acquire locks on basically any time you required to access a data item on a different node you spawn an agent 9is there which is basically a sub transaction of your main transaction okay basically if you want to provide the extension there is an extension possible for a simple two phase locking which is the nested two pl the extension two pl takes care of the extension of two pl to the scenario of distributed transaction where  refer slide time  14.30  all this transaction sub transactions are seen as children of the root transaction now lock requires that basically is needed by the transaction is lock request is send to the root node and it is basically the root node root node which acquires the locks on behalf of the children node this could mean that  refer slide time 15.03  for example the t two t two actually wants a lock on p two p actually this lock is acquired okay in terms of the root node and then inherited by the t two okay now t two actually finishes the this lock is not released but then given back to the root node okay now if t one actually wants the lock it gets it from the root node so all data items so locks for the data items are with root node now any time you actually require a lock your making to request to your root node and acquiring a lock and when you actually finish your actually locking the root the root acquires the locks back okay now if these are actually the now till the same transaction the locks are held by the root so it basically it doesn t conflict the two transaction children transactions of the same root node doesn t basically conflict with each other the conflict is relived because they children of the same node okay now the way is execution is going to be done in the nested two pl  refer slide time 16.29  is the two kind of models is one can thing of one is called the open nested two pl and other is the nested two pl now in the case of open nested two pl we are actually assuming that in the tree structure you have these are actually sub transaction that are possible okay now basically this is the root can take this is as t one and this is as a t two and there actually two um sub transaction for t one and they actually two more for t one t two so basically you have the a tree in terms of how showing the  refer slide time 17.29  transaction are spawn okay now its possible for only the child nodes to acquire the locks in terms of actually operating on data item okay that means t one will never do anything except that there are t one t two  refer slide time 18.00  which basically access or modify the data item that means only the leaf nodes one possible is to say only the leaf nodes are allowed to modify or process data item and if you allow these intermediate nodes  refer slide time 18.22  also to do this processing then it becomes open nested two pl and if you put the restriction saying that only the leaf nodes can be can do the processing on the data item it becomes a closed nested two pl okay typically what the nested transactions are meaning here is t one consist of t one one t two two and it is possible for t one one to further go down and have t one one okay and then this can be you further have t one one two this is the sub transaction of t one one further you have nesting so you can actually make this nesting further nesting which means that you can always divide main transaction in two a sub transaction okay now in terms of an open nested transaction it is possible to actually covert an open nested transaction model in to a closed nested transaction model because if there is processing being done at t one you can typically make that for example this can be here spawn separately as a sub transaction of t one dash which means that the processing is never done at the intermediate node there is a processing involve you spawn a sub transaction to do it okay that means in terms of the root the the other than the leaf nodes everybody else actually is spawning  refer slide time 20.17  sub transaction and what ever the sub transaction do that result is being aggregated higher nodes they don t do any processing by themselves okay a simple extension of nested two pl two pl to nested two pl requires that whatever locks acquired by the children for doing processing are passed on to the parent when they finish execution of a transaction okay and it is easy to see why nested two pl will obey the serializability condition okay all that has nested two pl has is that children never release the locks but they are inherited by the parent when the transaction finishes okay now its possible for another child to acquire the locks from its parent that means when you need a log you have to first see whether your parent has a lock  refer slide time 21.28  then you get the lock from the parent okay your inheriting from the parent the lock we assume that all the locks with parent so the only way you can acquire the lock is by making a request to your parent when you actually finish your execution your returning lock back to your parent okay now you can see why this simple nested two pl will ensure serializability by looking at intuitively what is happening here now if you basically see there are two branches of the tree and there is a lock acquired by any of this children here there is now inherent know serializability the higher level because unless this let me put in terms of simple tree structure nodes one two three okay four five six seven now you can see that the inherent order which this execution proceed is if three requires a lock or six requires a lock that is currently held by any of the other nodes the only way it can obtained is parent which actually means that unless two finishes  refer slide time 22.51  one can not acquire the lock that means the higher level the execution is always proceeds in terms of this tree the nodes of finishing one node is which is earlier till finishes and later the other node can finish otherwise there is no way other node than finish right so inherently that tree structure will be ordering the way the transaction are executed inherently provides a serializability condition okay now you can see that that means the only execution that possible case is four after that five if there is a conflict six and seven if there is conflict  refer slide time 23.42  its possible for tree structure to be executed for example aware okay the leaf nodes are all okay so that same tree structure that we saw earlier its possible that i can have execution if there is a conflict because if there is no conflict it doesn t really matter how six and seven are actually executed if there is a conflict that means if five is asking for lock on data item x and six also asking data item x it wont be possible because the lock would have been held by node unless it finish execution this would not have happen and hence this will be prevented okay you can see in a simple case a nested two pl the extension of two pl where a transaction actually spawns a sub transaction  refer slide time 24.47  and now that sub transaction can spawn further sub transaction and then each sub transaction it finishes gives it lock to the parent and that is the way to execution can proceed that inherently ensure is the serilaizability requirement this is an extension of simple two pl to nested two pl for distributive system this one possible model you also seen a number of actually time stamping scheme as part of our earlier discussion transaction model now its worth actually seeing what exactly happens with time stamping schemes in their distributed scenario okay now what we have is the basic time stamping scheme which shows how exactly transactions can be executed by having a basic time stamp being given to the both transaction as well as to the data items okay now in the basic stamping scheme every transaction is given is given a time stamp at the beginning of the execution okay now this time stamp could be just the logical lock value which we saw to the point of distributed system  refer slide time 26.47  which means that this is a local clock that is maintain by the by the distributed system which in affect means that this is to be either synchronized clock or some kind of logical synchronization happens this clocks such that the um the time stamp reflect the total order among the transaction that have actually executed in the distributed system one way to do is the last two bits know least significant bit a part of the know um each node and the higher bit is actually synchronized bit with the other clocks clocks of the other node okay its possible for you give a time stamp for each of the time transaction originating in each of the nodes of the distributed system now if you assume that every transaction entering the distributed system can be executed on any other node of a distributed system so this time stamp corresponds to the logical time that we saw in terms of the node synchronizing their logical locks okay one extension is a simple case of mutual extension algorithm we had implemented in the case of lamports clock the same scheme is applicable here except that instead of mutual exclusion is going to have a read lock or write lock that you acquired from the data item that means in affect this is equal to that time stamp is being used for writing on the data item reading on the data item now different nodes will decide the order of the in which they can access this data item by using the time stamp value the basic time stamping scheme what your essentially doing is your actually giving time stamp at the start of the execution of the transaction now as the transaction is start executing your actually looking at read set and your seeing its write set and then your deciding in terms of the read set and the write set what are the time stamps of these data item and making sure that the transactions are executing in terms of the time stamp order now you assume that every read and write data item data based have been given time stamp value for example let us take a simple case of actually looking at a  refer slide time 29.58  data item x which is in the database now each data item will have two time stamp corresponding to it one is the read time stamp that means this gives the this is basically the read time stamp what the read time stamp will you give is the value of the timestamp timestamp the highest value of the timestamp of the transaction that has read x that means it tells which is the highest time stamp transaction that has read its value okay now similarly we will have one of write time stamp x which tells correspondingly the write time stamp of the transaction okay this is basically the write time stamp the explanation for the write stamp is similar except that we have could is read now this is the highest value of transaction which is actually written on the data item x okay the value of the time stamp of the transaction that has return on x okay now use the read of time stamp the write time stamp is because serializability you always allow that transaction to only do in terms of increasing value that means if there is new transaction that comes in  refer slide time 32.19  whose time stamp is lower than the write time stamp and if you allow to read the value you obviously will have problem right for example if the write time stamp of the data item is greater than the transaction time stamp then you should prevent it from reading the x value that is basically you to abort the transaction and then ask it to come again the higher time stamp so that your preventing the access to the data item in terms of know always the data items should be access by the transaction in terms of increasing time stamp if you ensure that automatically achieving the serializability condition right now in a simple way in a distributed setting  refer slide time 33.20  you are going to extend the basic time stamp distributed transaction model by ensuring that time stamps are generated in a consistent way by the nodes of the distributed system if each one of them generate the time stamp the transaction time stamp are generated in a global way even when the data items are distributed your still ensuring that globally all the reads and writes are ordered according to the time stamps of the transaction since the time stamps of the transaction are generated in a consistent way the global system we are essentially ensuring a serializability order for your transaction coming in the distributed execution okay right basically that extension of basic time stamp scheme to the distributed scenario the extension is quite simple and straight forward again in this particular case ensuring that the logical clocks of the distributed system are synchronized and executed properly as per that order now one of the interesting extensions one can see is completely optimistic extension of the time stamping scheme were the data is fully replicated all the nodes which means that now  noise  every node has a data item available locally so what your going to do now is your going to look at each node is going to send set the write set to everybody and now everybody can vote on the reads and writes and if whatever reads and writes you have have appropriate number of votes then you will be able to view the modification which means that the majority of copies always consistent and always proceed in the same order as being done by the nodes let us look at that extension is often called the optimistic time stamping scheme and i will actually summarize the different time stamping schemes which have done earlier with respect to the distributed scenario the optimistic scheme is interesting extension because you have is here a fully replicated database that means the data items are fully replicated on all nodes now take a simple case were there is node n one and there are two data items x and y and take another node n two what going to happen now it will also have x and y which is essentially means that all that a transaction is going to do now is going to produce a know list of data items that is willing to modify along with the transaction time stamp which is trying to do okay  refer slide time 37.00  now whats going to do now is it needs to send the information to all the other nodes and other nodes to now say whether they have willing to accept the list by another node that means every transactions modification what is read what is to modify will be send to all the other nodes and now those nodes can decide whether that update list is consistent as far as they concerned if they say yes then enough number of yes votes are come for a transaction then obviously can commit it doesn t get that it doesn t basically commit which means all the processing always going to happen locally but then you have to send your read list and write list for voting to other nodes now other nodes can decide to vote yes in this particular data item let us take little more detail way and see what really happens in this particular case and how the transaction execution is going to proceed in this particular case what we going to see is let us say i have a transaction t one which started at let us say ten twenty here with respect to its time stamp now its basically trying to read x and write y okay so that means its read set consist of x data item and it is basically write set consist of y okay now it will modify whatever it wants to modify as far as this know transaction is concerned and produces this list for other transaction is to vote that means it will communicate this list to node um n one saying that as far as the t one is concerned it is time ten twenty it is modifying x and y and that list is being sent to the other node now you assume that there is basically read time stamp on x which actually shows again what is the highest value of the transaction actually read as x and similarly the write transaction on y okay now what your going to see is if this list actually non conflicting as far as the other node is concerned what what we mean by non conflicting is basically there is set of transactions want to execute on other node as well okay now what you have to check on the other node is before the modification is being done whether this modification is consistent as far as the other node is concerned and the consistency criterion is here is this x and y whatever this transaction is trying to do now is later than whatever has been done already in the other node  refer slide time 40.24  which means that the corresponding rts and wts tools applied on the data item and if it is consistent as far as that is concern rather than vote going to saying that this is okay for me beacsue as far as i am concerned there is no conflict of other transaction for serilazability and if it is basically conflicting and this condition doesn t hold good the rst or the wts not being know in the case of write transaction it has to be more than both rts and wts in the case of read transaction it is it should be more than the write transaction wts so in that sense we apply the consistency rules of rts and wts on other node and decide whether this list is consistent now that wont be enough because this is only proposal to modify is stil not modify they could be several such list come to me now its possible that there is a pending list with me and i have to actually compare these values with the pending list also what is pending now with me okay now let us say in the pending list none of it actually conflicts what i have then um the rule is simple ill be basically give a yes vote for it okay  refer slide time 42.02  if there is a pending list if there is a conflict which means that there is list which came to me which i have actually voted now let us say there is a conflicting list with me and in the conflicting list i have voted and yes for the conflicting list okay now there are two things can happen with the conflicting list let us say there is another transaction whose time stamp value okay is let us say ten twenty now there is a conflicting t dash direction here whose value is let us say ten o clock time stamp value is ten o clock and let us say for this i have obviously voted and as earlier that s why its actually my pending list i have still heard about the list transaction okay now one of the things i can do in this particular case is  refer slide time 43.10  since i have already voted for t one has yes t dash yes the best thing i can do for t one is say no because it is conflicting and an earlier transaction already said yes okay since i have actually said yes to an earlier conflicting transaction which means that t dash has yes for my case now i actually another transaction t one which is conflicting with my know the pending list let us say this x value trying to modify okay and is trying to modify the value of x probably if we take a specific example here to make this little more clearer  refer slide time 44.05  basically what your saying is this node n one when it got a list it is voting yes or no right now let us say there is basically um let us take node n one execution sequence node n two execution sequence it received a know update list of tr dash at ten o clock and this is the list with x and y let us say it receive this is update list now didn t conflict with anything by actually voted for yes it still pending on my side because i have not heard the final un know decision on this because the one who is actually sent me the list if it get enough yes votes is go to committed and send it finally as an update list  refer slide time 45.01  then ill apply the update list to the actual that is my update  noise  but in this particular case this is pending list i have voted but then the decision of this still pending okay there is no decision on this particular list now what happen there is another transaction t one that came by and it is actually ten twenty and then now if it is not conflicting with my pending list there is no problem what so ever i basically saying yes now let us say the one came which came at ten twenty as a pending with my conflict pending list now for that pending list actually voted an yes okay now one for me to do is this ten twenty i wont reply immediately but wait i say okay till i get the decision on ten o clock i am not going to vote ten twenty which means that differ making a decision on that s one possibility that means this know this list is not voted my me i am not going to say yes for it okay a no doesn t create a problem because a no is already been i have actually said i am not willing to go with that pending list okay right so even if it conflicts no really doesn t bother you  refer slide time 46.37  because you have actually voted but some other reason you actually not willing to go with the transaction only when you said to yes something then the current one is conflicting with it then you have you have a problem now whether um will this create the problem or will not be this problem because based on that you have to take decision okay now assume a scenario where this is this scenario one okay this scenario two could be same t one um the same t one arrived at my node and now it is conflicting now here also say i take the decision in to differ making a decision in this case okay now both scenario one and scenario two i differ then imagine a possibility were none of the list the likely would have getting a majority may not be there because everybody waiting but this list would be voted and there could be case were none of them might be receive okay enough votes and everybody is waiting for everybody else suddenly it looks like differing voting on both cases is not a good idea now then the next case which case i better vote no and which case i should better vote yes now if you basically looked at i have already voted at for that ten o clock case is yes the best bet for me is that ten twenty can be differ because it is the later transaction okay suddenly if i have done something at ten o clock and somebody is trying to come to do at ten twenty its still alright with me okay but if it is actually nine fifty i better in this particular case vote as no because i have already this any way i will be only one chance it would have succeeded is the case were ten o clock didn t get enough votes in that particular case the t one has chance otherwise it doesn t have a chance right so i can wait i can always wait both cases then decide only when i know about the know fate of um ten o clock um transaction because i have voted for it okay and make any future things that have coming in till the pending list clear i wont be make any other decision but what happen in the case is the possibility of all the transactions waiting for each other pending list and result in no body progressive so you better vote case where it is nine fifty has you vote for this as a no to avoid this confusion okay now at the end of it the majority votes pending list actually majority vote that basically sent as an update list for all the nodes that means that list is now as to be updated by all the nodes with the now the very fact  refer slide time 50.34  any node has got a know enough votes any pending list as got enough votes okay that becomes a permanent write on it and you can easily show that no to conflicting things can get okay both can get majority votes that is the same time because only one of them can win if the two transactions are conflicting only one of them in end will be able to achieve the majority vote and that is the one which will go through and the other is going to be restarted okay if you basically looked at it here it is a completely optimistic scenario because your executing all the transactions and your never checking the consistency before start execution of the transaction what your doing is your executing the transaction and at the end of it your deciding by sending this updates to all other nodes you seeing whether you can go throw with this execution or not okay so this is typically the pessimistic scenario verses the optimistic scenario pessimistic as we told earlier the consistency check is made at the beginning of the execution and then this is the begin that means you do this checking  noise  a locking would have ensure that the check is done in the beginning unless you acquire the locks will not proceeding on execution okay on the other hand this check is made at the end which actually means that um we are actually executed in transaction fully by applying the consistency check okay now in that sense basically having the two spectrum one is the locking based algorithm which fall in this spectrum and then a fully optimistic time stamping scheme which for in the scheme were the consistency check is only applied at the end of the execution sequence okay a variety of you know models are possible for as the transactions a models are consistent based on the time stamp scheme one of the most interesting scheme is to give we have seen um in the case of example um  refer slide time 53.17  visting tirupathi temple you have the model were the band of time is given were you have to go and then see the you know thing at the point of time um basically your scheduling transaction it can be executed in the future for example you can say as far as t one is concerned you can give a band of time which it can really execute which means that when it start executing you can say that this becomes the um the time before you which commits its alright for you this is like telling if it comes at the gate at ten twenty will not the start the time virtual time of the commit as far as the transaction is concerned if it commits at let us say ten o clock its fine with me yours telling when it commit and it can actually finishes it execution so you can actually give an order inn which this basically end finish time not at the start time that your trying to schedule you can actually do this the um the n for example its possible for you to tell even a band let us say this transaction allow between ten and ten fifteen to execute finish it execution if i generate this band such that conflicting such transaction will not finish in the same band and have actually you know its getting the serializability for example they could be another for example if the band that is given to the is two transactions different if they are conflicting let us say i give a band of ten to fifteen to ten thirty for this it actually means that um these these suddenly they are not you know serializabililty condition not be violated here because one is finishing at  refer slide time 55.17  ten fifteen and other is finishing so i actually get the band of time in which they can finish their execution that s other model i can give completely different kind of model is also possible were um you can say that ill basically do what is called only transaction time stamps but not data time stamp for example all the schemes which i have seen so far have two kinds of time stamp one is time stamp to the transaction and i am giving the time stamp to the data item you could do a scheme were all that your going to do is as the transaction is executing you can give a time stamps to different different points of execution for example you can give start of the transaction finish of the transaction what is the time and used that for actually doing the consistency check but as long as one transaction finish after other their conflicting and that your able to ensuring at your clocks times that you have actually happened your actually able to ensure the consistency condition in a simple way we are talking about is let us say t one there is a start time and there is a finish time of t one okay and similarly t two there is a start time and finish time and these times are actually given by using a clock here okay now as long as the conflicting transactions are all as they are executing times are giving you can ensure that the serializability condition as far as the operation concerned by making sure that the start and the finish time of the properly synchronized as fara s the transaction execution concerned by doing this we actually don t need to maintain time stamp on the data item  refer slide time 57.27  that s a saving in terms of not needing to maintain the time stamp okay a variety of schemes are possible we have seen couple of scheme in this particular lecturer transcriptor  c.udayakumar database management system dr.s.srinath lecture 27 introduction to transaction recovery hello and welcome in this session today staring with a new topic namely that of recovery that is how to recover data or how do we bring back database what we call as a consistent state in the phase of any kind of failures failures could be of any kinds let us say disk crashes power shutdown um network connection failure um many kind of different failures and um we are going to some what specific when we say recovery obviously we can not recover um data that involved that were being processed in main memory during ram um main memory during the crash but only we can recover what ever has been return into persistent storage like this but um is what written on the disk good recovery or um or should we is some what semantics associated to what is written on to the disk do we have to do something more when when in order to recover from crash schedule in order to answer this question we need to know the concept of transaction in a database processing environment therefore the the title of session is called introduction to transaction recovery in fact we are going to define the notion of correct recovery from wrong recovery based on concept of transaction we are not going to study about transactions in detail in um here they are covered in a separate topic under under itself transaction transaction processing itself is a vast topic with several different aspects to it and we shall be concerned mainly with the recovery aspect here when it comes to and we are using the transaction to help us in deciding which kind of recovery is correct recovery which isn t correct recovery let us first define the term oltp i am sure you might you heard of the term oltp in several different type of context its stand for online transaction processing and environment that is the database system plus an application program plus any other associated accessories like networks so on goes into form um an environment it is meant for online  refer slide time 3.22  transaction processing what is mean by online transaction processing is the name suggests it is an processing environment that can interactively process database transaction we ill we ill defines these terms in in much more um in a in a much more accurate fashion later on  refer slide time 3.53  but let us first look at some environments that can be classified as online transaction processing environment some examples are like air line railway reservation systems what are the characteristics of such system one of them main characteristic that you can straight away see railway reservation system for a example is that there are  refer slide time 4.17  there are several number of users who are accessing the um database system simultaneously that is you might have experienced it if you have tried booking a railway ticket over over the internet um suppose a train getting almost full and if you delay in booking your ticket given by let us say some times even by few minutes then never then you may not get a conform ticket at all might go in to waiting list so um that means at that particular instance of time when your checking the status of tickets there were several other people accessing the same database throughout the country that is could be accessing via the internet could be accessing via the um via let us say some kind of queuing system that is across the booth whatever um whatever from there all accessing the same database and several different transactions happening at the same time and um and similar example that are of banking system and especially how cash dispensed in atms or wire transfer mechanism so on and another other um doubt environments require super market checkout systems and where customers come in with the  refer slide time 5.51  with their baggage whatever things they have bought they have to be checked out praised and build and so on and there are hotels and hospitals trading and brokage system where buying and selling of the shares happening continuously whenever trading is on and several such sessions are happening simultaneously now let us take scenario which help us understand what should be the properties of these transaction that go on in oltp environment now consider a small banking example where different accounts are maintained in a bank they could be on different database in the same database or different locations or doesn t matter let us consider there are different accounts and these database  noise  being accessed simultaneously by several users performing several transaction let us say one per one percent using net banking to to transfer some money from his account somebody else account at the same time other person is using a atm  refer slide time 6.40  to to withdraw some money deposit some check or some check is getting enchased um some dd is getting enchased or withdraw whatever several things are happening simultaneously in the bank okay now among this let us consider a small one particular example that account number two five six five sense two thousand five hundred rupees to another account um number one hundred and sixty five okay so let us say at time t equal to zero however we define our time okay um transaction begin are set of database operations begin the application program that is making this wired transferred um will initiate database operations which will read the balance of two five six five okay now it will read the um read the balance amount because it is a withdraw it s a withdrawal from account two thousand will be deducted from this balance and let us say we are using concurrent applications which can have different threads of execution which can run at the same time so when this balance is being deducted here at the same time the the balance of second account is being read okay of account number one sixty five and due to some reason its get this thread process gets swapped of in to disk by the operating system some other process is running because um note that operating system of the scheduling processes to and some other processes running at the time and by the time is this process comes back it is time number four time t equal to four at this time balance amount of two thousand five hundred is added to this account okay now meanwhile let us say there is other transaction let us say the the account holder of this of this account number one sixty five is meanwhile standing in a atm and depositing um certain cash certain amount of cash to the atm or may be he has sent a check that that check is getting clear okay so why this person with the account two five six five performing wire transferred the person having the account one sixty five is also depositing amount of three thousand rupees and it is so happens that the way process are schedule this set of operation that is reading the balance of account number one sixty five here and adding three thousand rupees is done before the the previous transaction finished that is before the balance of before in amount of two thousand five hundred is deducted from um that is based on the previous transaction you can see what what happens whatis happened now the previous transaction has read the old balance amount and added actually this should be added i am sorry there is a small bug here this should be plus equal to that is it is taken it is taken previous balance amount and added to two thousand five hundred rupees here while before it could that the previous balance is was read three thousand rupees was added by the customer who is depositing is cheque from the atm now what is happened here in that this entire transaction is lost this entire serious of operation is lost after time t equal to four because suppose this person had fifty thousand rupees in his account it will be now fifty two thousand five hundred rupees rather than um fifty five thousand five hundred rupees um three thousand plus two thousand five hundred rupees okay so three thousand rupees is just gone  refer slide time 7.37  is just lost now as you can see um this is this is not unrealistic situation especially we just saw how um today when we have facilities like net banking or atm or um booking train tickets over over the web or using telephone calls using sms from mobiles and so on this is not a unrealistic situation because concurrency is concurrent activities um are happening at the same time um i mean concurrent activities are happening all the while and um if your not careful such such kind of activities can result in an inconsistent database the entire transaction of depositing three thousand rupees is lost in this example so what is the um we said that if your not careful what is the clear you have to take what is that um what is that you need to remember when we are dealing with situations like this the thing you need to remember here is that the first two activities let us say the first activity of wired transferred between account two five six five one six five is that completely different or is a conceptually separate distinct activity from the second activity of depositing three thousand rupees okay  refer slide time 12.28  so the first activity is conceptually or logically separate break activity or functionally than the second activity such kind of logical units of work are called transactions and a transaction activities in a transaction the actual database activities transaction should be schedule in such a way such that these kinds of anonyms does not occur okay let us try to formulate this things in a little bit um in much more detail now what is the transaction a transaction is a logical unit of program execution as we saw the entire activity of withdrawing money  refer slide time 13.25  from a s account depositing that money in to the b s account constitutes one logical unit of operation and it is a combination of database updates which have performed together they can not be independent of one another the the withdrawal from a s account is not independent of depositing into b s account vice versa there are several different transactions depending on where and how its its being used let us have a brief look at the definition of transactions which which makes it clear what are the fests to handle transactions are recovering from transactions one firstly we can define a transaction as a logical unit of work that is meaningful in the users environment as we can see here um the wire transferred that is withdrawal money from a s account and depositing in to b s account is a meaningful semantic activity as part of the users environment because that that constitutes semantic process in in the users environment wire transferred semantically depositing three thousand rupees in order to deposit three thousand rupees they were two database operations network done that that is reading previous balance and updating the balance so both these activities reading and updation um one semantic activity that is constitutes one meaningful activity namely that of depositing certain amount of money in to a account now one can define even transaction as a logical unit of work with respect to concurrency control recovery not necessarily semantic activity in terms of users environment many cases users depend on what granularity your looking at users may not be concerned with water considered transaction the database level however we might have to club or might we have to combine database activities in to transactions in order to maintain consistency in the phase of concurrency control and recovery process transactions are also called atomic unit of work instead of calling it as logical unit of work um much more stringent definition is to is to say that is an atomic unit of work with respect to respect to concurrency control recovery um atomic unit of work is is a stringent more requirement than saying logical unit of work an atomic atomic unit of work basically means that it can not be subdivided in to smaller works either all of the activities of a transaction are performed none of them performed you can not perform a half a transaction and leave it at other or u cant perform ninety percent transaction  noise  either you have to perform entire set of activities of a transaction nothing at all and um another definition that s generally used that transaction is an atomic unit of work that will apply to consistent database returns another consistent database  refer slide time 14.02  that is an atomic unit of work is that is meaningful that could be meaningful that is um um atomic unit of work concurrency control recovery however not all atomic units of work that that that can be managed concurrency recovery um could be transactions because some of them could take it take the database to inconsistent state so um the transactions some times define so the transactions are some time defined in those only units of work um make transactions to an valid state of database so what are the properties that a transaction should satisfied that is we have seen we have motivated the need for transaction in several different from angels we first saw an example application an example oltp application where transaction processing in correct transaction lead to anonymous and also we saw different definitions that look at the notion of transactions different levels now how we can consolidate them together and synthesis enter the basic properties transaction should hold so the basic properties that a transaction should hold are um what are called as the acid properties of a transaction acid stands acid stands for atomicity consistency isolation and durability that is shown in the slide here so what is the atomicity in the acid property atomicity we just saw in the previous slide a transaction should be viewed as an inducible unit of work that means either  refer slide time 18.36  all activities in a transaction performed none of them we can not perform after transaction okay consistency consistency of a transaction basically means that databases consistent transaction should be consistent after a transaction the transaction the atomic unit of work should work not lead to the database in to an inconsistent state whatever we mean inconsistent state um any state that violates integrity content of the database we saw how to how to specify integrity constraints  refer slide time 18.55  here enforced database system the third property of transaction is isolation we saw an example of transaction violating isolations in our banking example that we saw before isolation essentially means that even though activities in the database are happening concurrently that is the um readings updates and reads and writes operation what is happening on to the the database level  refer slide time 19.35  are all happening in an simultaneous fashion the net effect in an oltp environment the net effect should be as though the transactions have been executed in some serial order it does not really matter to what should be the serial order as long as we can um we can establish equivalence between the way in which database activities are performed to a serial sequence of execution that is to be as though that transaction was completed a before transaction b begin a b transaction completed before transaction c c begin and so on and the last property of the transaction is called durability that is once the transaction finishes are rather we used the term commit here that is once the transaction says now i have done all my work and you can commit whatever changes made database once it is committed changes are persistent is you can not rollback or you can not undo  refer slide time 20.39  the changes that that are made by the transaction after it is being committed to the database that is commit something like in order to understand the notion of commit it is something like a physical activity for example expensing money from an atm is is it commit operation once it is committed money is dispensed you can not rollback you can not expect the user to say no no no um um we did something wrong we have to put back money that i gave you some other transaction is conflicting so once commit operations performed it is durable that is the transaction can not be roll back and we have to do something in order to undo the operations of a transaction um once you dispense the money from atm something else we chased the person who withdrew the money and get get back from if required and so on so we can not undo the transaction within the per view the database let us look at6 example were were examples that um specify each of these acid properties of the transaction have a look at slide here let us say the transaction involves wire transferred from account a to account b the transaction should be like this account a a dot balance minus equal to amount whatever amount has to be transferred and account b dot balance plus equal to amount let us suppose that the the balance balance that is amount amount number of rupees has been debited from a s account okay and for some reason databases crashes let say there is disk crash the network failure whatever or operating system crash whatever now once the system is brought up game that is there is a recovery process this transaction has not completed therefore in order to make it atomic we have to roll back that is roll back changes that your made since the beginning of the transaction that is we have to put back this amount back into a s account and then restart the transaction once again otherwise it wont be a atomic operation  refer slide time 22.01  this amount to if it did not perform is recovery operation this what of amount perform in of a transaction and we saw that performing half a transaction and we saw that half a transaction is not an atomic transaction what about consistency have a look at the example again okay this is again the the wire transferred example from account a to account b that is the same series of operation are could be performed is a dot balance minus equal to amount b dot equal to b dot balance equal to amount now let us say that the query that has to be performed these two things have has been that is the query planned been performed these two operations are given to two different threads in the operating system  refer slide time 23.33  and it so happen that the the thread performing changes on b s balance is schedule first before that of a okay let us say first b s balance the amount is crediting it is let us say a s sending two thousand five hundred rupees to be so we know the amount two thousand five hundred so these balances added by a value of two thousand five hundred and however when trying to recover remove two thousand five hundred rupees from a s account um we see that a has zero balance in his amount he can not make this payment so um so this transaction fails we can not make this transaction so in order to keep this transaction consistent we have to we have to deduct whatever credit we may in to the account of b in order to bring back the consistency in the database systems  refer slide time 24.25  note that here there is no crash or anything of that sort here there is the normative failure the the normative or failure with respect to nor failure which violated integrity constraint we can think of a integrity constraint that says odd raf are not allowed that is the the the balance amount in in users account may never been negative so when we try to do this operation that is when you try to debit two thousand five hundred rupees from a s account we found that the the balance is becoming negative and it violate the integrity constraint which in turn cause the transaction to roll back not at all to maintain consistencies in the database systems the third property of isolation um and isolation like you said before deals with concurrency is what how do we handle concurrent operations being performed two or more transactions simultaneously so again consider the case of wired transferred another case of wired transferred that is account a s is transferring some account to b at the same time account the the the person holding account a is also withdrawing some money that is the person holding the account a given a check at some time which is getting process now and if the same time the account holder is withdrawing some okay let us say the transaction t one is is reading account balance debiting the amount and crediting the amount account b let us say that a s balance will becomes zero after debiting this amount  noise  says two thousand five hundred rupees and let us say same amount is being also withdraw by being as by account holder for withdrawal okay the net effect of running this two transactions should not be the case that both of them read the database or if the balance okay there is two thousand five hundred rupees and then go ahead independently debiting them debiting the account because that would be in correct because we would have debited more than two thousand five hundred rupees from these two transaction where it is not could not be reflected so the net effect should be t one precedes t two that is t two begins only operation after t two completed is in effect that that should be the case which case t two will fail or t one begin operations after t two completed it doesn t matter which is the serializable schedule which is the serialize schedule that we want is it t two after t one t one after t two so um in either case none of the two transaction will fail that is either the withdrawal will fail or the wired transferred will fail  refer slide time 25.54  and durability like we said is the commit operation that is one thing committed then its not change we gave an example of let us say money dispense information from atm  refer slide time 28.05  once the commit operation is performed it is safe to dispense money from the atm and we can not roll back the transaction once the commit is performed what are the different states in which a transaction is in and this is important to know when we trying to do recover from a failure of a transaction now transaction is set to be in several states its depending on whats happened since it begin on um it is said to be in active set which is the initial set when the transaction is executed when the last statement finish execution and it is ready to commit the transaction is said to be partially committed when the transaction discovers that no longer proceeds with normal execution because something else has happened some crash some violation integrity constraint or some violation of an violation requirement are so on when it discover something like that then it is said to be in the fail state and once roll once the transaction is roll back it is said to be aborted and if the transaction successfully completed that is if the atm successfully dispense money it is said to be committed and either committed or roll back or aborted state is called terminated state  refer slide time 28.30  so this slide schematically depicts the different states in a transaction and also shows from which state to which other state that is from the active state you can go either partial commit state in a fail state and also you can reach fail state from a partially committed state is after performing a few operations and from a partially committed state you can everything is okay then you can go to committed state or if the things are not okay into fail state turns take in to aborted state  refer slide time 29.43  let us have a simple look how these acid properties can be maintained or what it takes to to maintain this acid properties and we are going to look at simple  refer slide time 30.17  example called simple technique called shadow copy shadow copy is extremely simple inefficient is not used in practice um several more sophisticated techniques for handling are maintaining acid properties or taken up in much more detail when we take up the topic of transaction processing itself the here is just to illustrate the concept of what it makes to to perform to maintain certain properties of a transaction shadow copy transaction assume to be a single file and assume that only one transaction that is active at any time note that say it can only  noise  it can only provide acid that is atomicity consistency durability and not isolation  refer slide time 31.07  so shadow copy its simply like this suppose you have database in a file and you have to perform your transaction okay now before performing your transaction make a copy of that database that is copy in to entire file the file okay and make your changes on the copy of the databases okays now if your changes succeed it does not violate any integrity constraint and it is consistent and it is safe to commit then simply you delete delete the orginal database and then you keep the new updated copy of the database in case i have to abort your transaction then you just delete the copy that you have created and let the original database be as simple as that that is um you make the entire database to in to  refer slide time 31.25  shadow copy the database into entire file and make changes on it and if it is safe commit the changes then delete the original file or if it is unsafe delete the new file the original file has it is it is of course how it is intreactiacal and and if inefficient but of course later does it satisfy these um acid properties database let us look at atomicity if i see that i can not do all operations in a transaction is such that atomicity needs to be met then i just delete the new transaction that is i just delete the new file it is all are nothing  refer slide time 32.38  only when all operations are committed all are operations are performed in the new file will i delete the old file so therefore it is all or nothing no operations have been performed consistency if any consistency if any integrity constraint is violated in the new database is deleted so assuming that old database is consistent we are still left in a consistent state okay isolation obviously not supported because when two or more transaction is are copying making different shadow copies can not um we can not support isolation here and durability at any point in time is once the transaction commit it just ensures that either the old file or the new file remains that is once the transaction terminates it is either commit or abort if it commits then the the new file remains if it aborts old file so so it is durable what are changes made are persistent in the database 34.10 let us have look at concept of serializability which is which is again the important when when it comes to recovering from failed transactions like we mention before in the previous shadow copy example isolation was not supported and in order to support isolation we should w should ensure the notion serializability in our transaction processing environment this serializability says that if i set of activities from two or more concurrent transaction taking place they should they should schedule in such a fashion as though transaction were executed in some serial order so have a look at the slide here slide shows two transactions here t one and t two and transaction t one is a wire transfer that is taking fifty rupees from a s account putting in to b s account transaction t two is also wired transferred it is taking ten percent whatever amount is their in a s account and crediting in to the b s account  refer slide time 33.47  now suppose i were to perform all activities of t one and then start with all the activities of t two obviously serialize a serialize schedule such a schedule is called a serial schedule that is performing all activities of one of transaction before starting first activity of the second transaction so this equivalent to performing t one followed by t two on the other hand  refer slide time 35.27  if i perform all activities of t two and then start with the first activity t one and then perform all activities of t one this is also serial schedule this is also correct schedule and this is equivalent to t two followed by t one however serial schedules do not have does not necessarily mean that all activities pertaining to given to transaction completed before the first activity of of the next transaction t one is taken up for example this one this slide shows um shows how um activities from t one and t two are interleaved the color activities here belong to t two that is read a t equal to eight times point one a equal to a minus t write a and then um and the transaction t two has not completed but transaction t one is already begin read a a equal to a minus fifty and so on okay and then transaction t two continues here and transaction t one also continues here however if you notice even this schedule it is a serialize schedule it s a serial schedule this schedule is equivalent to performing t two followed by t one why is this so have a look at here have a look at how the activities of t two and t one and are all activites performing or regarding regarding updation of data element is completed of the transaction t two are from the transaction t two four first operation involved involving data element a is even performed from transaction t one same thing with respect to b and we can actually see that we can rewrite it because we can take these elements of b back here put this back here without changing the semantics let us without changing the semantics of this serialized schedule is once this schedule finishes it equivalent to as though t two was executed first followed by t one that is all activities of t two before finish the first activities of t one started  refer slide time 35.55  this brings to some definitions of how we can serializable over a set of database activites we define the term conflict serilazability by first defining the term called conflict between database activities then we say that then we say that particular schedule is conflict serialzable if there is no conflicts or with respect to or when it is being transformed in to a serialize schedule is schedule in which all transactions all activities of one transaction for all activities of the second transaction now consider two activities i and g belong from in to two different transaction is t one and t two now i and j can be swapped in their execution order if they refer to different data element because it doesn t matter one is referring to element a other is referring to b it doesn t matter we can we can perform them in any order and i and j can be swapped in their execution order even if they refer to same data element however its reading all the contents data element even both are reading the same data element doesn t matter who is reading first who is reading second as long as nothing as else in between them that is and they are said to conflict that i and j can not be swapped in their execution order if atleast one of them is a write operation let us if atleast um if i is trying to write and j is trying to read we can not perform i should be read before j we can not have j read the database before i writes it and so on the same thing is true when both are write operations  refer slide time 38.18  given a schedule a schedule is something is like we saw in this slide here it is a schedule now suppose we are given a schedule here that is the activities of t two will perform will like this and activities of t one are perform and activities of t two continues so on now given a schedule if we in order to determine whether it is safe or not whether it is serialazable or not if we can swap if we make one or more swapping of activities database schedule activities and bring them to serialize schedule  refer slide time 40.12  is were all activities of one transaction perform all activities of one transaction without encountering conflicts as we define in the previous slide then this kind of schedule is said to be a conflict equivalent schedule okay and it is also said to be a conflict serilaizable schedule that is it can be serialized or it can be equivalent to serialize schedule where the equivalent criteria is conflict equivalence it is conflict serializable  refer slide time 41.15  there is a there is an alternate weaker notion serialzability called view serialzability conflict serializablity is is quite strong and many case we don t need need stringent property it is of conflict serializability the view serializability simply says followed suppose  noise  for each data item queue suppose there are set of transactions that are happening in the um in a dbms system now for each data element queue suppose it was transaction ti which reads the initial value queue in a in a serialize schedule it is in a um in a serialize schedule then any other schedule which is view serializable  refer slide time 41.33  it also should be the case that the same transaction um is the first transaction to be reading this data element um or this data element queue and similarly um for inj the given schedule for each data item queue if tj preceeds or um tj writes to q before ti before that is ti reads then the same dependency should be maintained in any other schedule is anybody to writing to a data element before somebody is else reading it um this kind of dependency maintained in whatever um if the schedule has to be view serializable similarly the last operation that is whoever is performing final write in a serializable schedule should be the same transaction performs the final write  refer slide time 42.38  whichever schedule is view serialazable let us take an example there are three transactions shown in this figure the first transaction t one reads data element queue and writes it back to disk and after performing some operations which is not relevant here its only the reads and writes which we have concerned about it and t two just writes some something in to queue and t three alone its something write into queue now following set of following schedule is a view equivalent schedule or a view serializable schedule note that it is not a pure serialize schedule the activities of t one and t two are being interleaved here all activities of t one are not completed before activities are performed however if you see who is reading the first who is the first transaction to to to read the data element to queue suppose we take a serializable schedule that is t one is followed byt two that is t two followed by t three if in that serialize schedule the first transaction to be reading data element is queue is t one that is is the same thing in the schedule okay now is there any read before or write dependency there are for example t one is reading before t two is writing okay or rather um t one is reading before t three is writing is that dependency maintained here that is um read before writes write before writes either of those dependencies there are maintained here okay and who is the last transaction in to queue it is t three is same thing here that is t three is the last transaction that is writing to queue therefore this is this is the view equivalent schedule that is it corresponds to t one um happening before t two happening before t three because the first data element to read was t one read queue was read one last element write in to queue was t three so when the when the t three finish then there is no difference between saying it was performed as t one t two t three or it performed in this fashion however note that this schedule not conflict serializable if he try to swap updates here that is if he try to swap the activities here in order to get serialize schedule then we encounter conflict that is take a look at second and third activities here now in order to bring in a serialize schedule we have to swap the second activities with the third activity so that t one comes here t two comes here however both of them writes and we saw that when both of them are write operations in the same data element and belong to two different transactions then you can not swapped them in conflict therefore this schedule is not conflict serializable however it is view serializable it is as far as database view is concerned it remains the same whenever we look t one after t three it is being the same  refer slide time 43.21  so every conflict serializable schedule it also view serializable however the converse is not true which was the example we saw the previous slide usually this thing has happen  refer slide time 46.36  it is usually we find view serializable schedule not serializable whenever what are called as blind water blind write is something that we saw in the previous slide here that is transaction t two t three shown in slide contains no read operation they just write in to the database without any read operation so such um it is a hall mark of blind writes which bring in schedules that are view serializable but not conflict serializable  refer slide time 47.01  let us look at the last concept of what are called as recoverable schedule and in order to understand what kinds of what are the requirements of database recovery consider the set of following transactions as shown in the slide here there are two transactions here t eight and t nine t eight reads a data element a does some modification and writes set and then goes about reading about some other read data element and so on okay after it writes here transaction t nine reads the data element a and possibly let us say displace it and it does not perform any writes okay so as you can see this is conflict equivalent schedule that is i can swap read a with read b which will give me serialize schedule that is t eight followed by t nine and swapping by read b read a it is it is still conflict equivalent so therefore performing read a write a read a of t nine and read b of t eight is conflict equivalent  refer slide time 47.26  however suppose um suppose let us say t nine that is read a and displayed okay that is display the latest value of this stock price whatever okay so supoose t nine commits and display the value of the value of k but t eight is not committed that is t eight is not completed still and it sees that it can not commit because some problem somewhere and it has to rollback now if it rolls back then t nine also has to be roll backed because it read value of a after it has written by t eight okay however we can not rollback t nine because it is already committed and committed is durability condition here that is we have already made some commitment some sense we have displayed the the new value of the stock whatever okay so such in a situation it is impossible to recover from the failure of t eight because we can not um because the rolling back of t eight we will also require rolling back of t nine it is impossible so this is in an example of a non recoverable schedule so serializability or conflict serializability alone is not enough we need to also look at recoverability of particular schedule of transaction events in if we have to be recover from a database crash so database system whatever recoverable schedule  refer slide time 48.30  and finally let us have look at the problem of cascading roll back which is also quite important when it comes to recovery even if a schedule is a recoverable to recover from a failure of a transaction in some times there is need to roll back in several transactions the previous example is also an example of cascading roll back that is suppose um transaction t nine not committed and transaction t eight roll back then t nine is also to be roll back so in order to make recoverable to defer the commit of t nine until t eight has committed so that will make recoverable however its still contain the problem of cascading roll back okay so this example also shows cascading rollback situation where there are three transactions t t one and t two and has read a value of a and written it and whatever value is return by t is being read by t one and t two now t one and t two can not commit that is can not display the display the value of a until t has committed otherwise it will become non recoverable okay now even if they don t commit um and suppose t has to roll back it has a cascading effect in t one and t two in fact t two is dependent on t one t one is dependent on t roll back of t will cause a rollback of t one which in turn will back as t two so such cascading roll backs will have to will un leading to large amount of work from several different transaction in case of any database crash or system failure  refer slide time 50.02  um cascading rollback is an undesirable undesirable thing to happen in lease role undoing of lot of work so when we are when we are looking at schedules of of operations that are performed by by oltp environments they have not to be only serializable they have to be recoverable cascade list is far as possible we should try to avoid um cascading roll backs  refer slide time 51.57  so that s bring end of the introduction session on database recovery where we have said the ground for for all the issues that make up that are that are that are concerned  refer slide time 52.33  are that are concerned ourselves whenever we are dealing database recovery for example we we saw the notion of the transaction that is when we recovering from the database crash we have to ensure that we don t leave any transaction in a half or partially committed state it should either be um fully committed or um note operation should have performed that is all are nothing atomic atomicity transition have to be obtained when we are recovering from database um crash the system crash and in order of that in happen we have to have recoverable schedules and its not sufficient for schedules to be serializable and also um when it is conflict view serializability its not sufficient for schedules to be serializable they should also be recoverable schedules and as far as possible they should be cascade list schedule it is um crash or or or rollback non transaction or automatically mean that several other transactions the several other work that has been partially completed has to be rollback its its not even rollback completed even though they are completed just waiting for the commits which is what in the operations in the slide that is eventhough transactions even t two of completed they have read and written a value of a they just waiting for the the the orginal transaction into commit and because of some problem the the orginal transaction may may crash and because of that and all the operations that have been completed have to be rollback without any that is without any reason by themselves so this bring to the end of his session lecture # 28 hello and welcome in the previous session we started looking into recovery mechanisms in databases especially  um  we looked into the back ground of transactions and the the idea of a transaction and  um  how recovery  um  should should should maintain consistency in terms of different transactions that is it should not leave a transaction in an un atomic form that is  um  when a data base is consistent or either all transaction is performed completely or they have not been performed and it should not violate in integrity constraints and it should be serializable and so on  um  today s session we are going to look at some mechanisms for recovery specifically we are looking into what are called as log based recovery as the name suggests log based recovery means that recovery mechanisms for the data base are  um  are performed using transaction logs that is whenever transactions happen cert  um  certain elements of the transactions are logged into log files and using these log files we can try to record the data base into a consistent state  um  in in case of any kinds of failures so let us briefly summarize what what we are learnt about  um  the transactional requirement of data bases before we look into recovery mechanisms log based recovery mechanisms  2  37  firstly why recovery or in in what situations do we talk about recovery mechanisms  um  recovery is pertinent in the case of failures and  um  given data base system can be subject to different kinds of failures there could be system failures  um  the the the power could just go off data base there could be media crashes that is the that is the disc crash or  um  something of that there could be communication failures that is network has failed and transaction which was which was partially submitted or or especially if you are having distributed databases started on other machines failed communication between the two machines failed and so on and there are of course transaction failures that is transaction could fail for a variety of reasons including the the above kinds of failures that that we are talking about the transaction could fail because they violated integrity constraints transaction could fail because they could not  um  there is no serializable schedule for for for the set of activities from these transactions or they could fail because  um  whatever schedule that that s being currently performed has led to a dead locker something of that so in many of these failures we we need a recovery mechanisms in the in the last case that is transaction failure usually it is its automatic that the system is still functional the dbms the database everything is still functional so it is just a matter of re submitting the appropriate transactions after waiting for a while and  um  and hoping that it succeeds this time rather than  um  fail or if if the transaction has violated an integrity constraints it it involves something like raising an exception or intimating the application program saying your transaction is wrong or or i cant perform your transaction because your account doesn t enough money to to to to withdraw so much amount you have asked something like that so  um  leaving i said the last  um  point here in most of the other cases  um  the the dbms or the databases has crashed and it has to be booted up it has to be brought up again and once it has brought up again there is no guarantee that what ever data that s there in the data base is consistent and  um  and it will be un safe to just star the data base  um  and have it running  um  from where ever it it left off because any amount of data that that was there in the volatile memory in the ram would have been lost and  um  and we don t know how how we can set these things right  5  30  so what is the properties of transactions that we have to  um  that that that we have to assure when we are providing recovery mechanisms the the  um  the property of transactions as you know is called the acid property of transaction that is atomicity consistency isolation and durability properties so so let us briefly summarize what is meant by the acid properties and what do they require atomicity means either all updates that are performed by the transactions or performed that is either all updates that are required by the transactions are performed or none of them are performed we we cant have a transaction have that has performed half of the updates that are that are made for it that is we can not have a transaction that has debited my account in in a wire transfer  um  transaction and has not credited the the other record and the the the even is lost so so it should be either all or none kind of operation  6  36  similarly consistency requirements is that when a transaction has to finish or if a transaction has to successfully complete it should not violate any integrity constraints of the data base that is given a consistent data base a valid transaction should leave the data base in another consistent state it need not be the same state it could be another state but it should be a consistent state if it violates any kind of cons integrity constraints on the way then you have to roll back you cant you cant complete the transactions and you cant leave it there as well because you then they violating the atomicity requirement of the transaction  7  12  isolation constraint that is the i in the acid property states that whenever there are multiple transaction that are happening on in a dbms the net effect of all the transactional updates should be such that or should be equivalent to schedule in which all updates of one transactions are are performed before the first update of this the next transaction is taken up that is it it should be as though the the transactions have run in some serial order it need not actually be run in serial order that s what we saw in the previous session  um  it activities can be inter lived as long as this inter living is safe that is we we saw notion of what is meant by safe that is the the the the notion of conflict serialisibility that is we should be able to conflict a view serialisibility which we saw that is we should be into a serialize schedule without encountering any kinds of conflicts  8  16  and the last property is that of durability that is once a transaction commits it can not be rolled back it should be the the  um  changes that are made after a transaction commits or durable it is persistent and not only that the changes are made  um  inside the data base that is on to disk it it could the changes could also entile performing some kind of physical operation like you said like we gave the example yesterday of dispensing money from an atm that is if  um  once a transaction for withdrawing money succeeds and it commits then the the the atm dispenses the required amount of money that that was asked for the asked by the customer for withdrawal now once the money is dispensed you can not roll back the transaction the problem  if  um  if it is found that there was some error and the money shouldn t have been dispensed you have to look at solutions that go beyond the data base systems you cant ask the data base to just roll back this transaction and leave it at that  9  30  what is the different states in which a transaction lies in the the first state is is the active state that is when whenever a transaction becomes active and it is executing it is said to be in an active state once a transaction has performed all its updates and it is ready to commit that is it has finished its executions and it is ready to commit then it is called a partially committed state and once a transaction discovers that it can not commit mainly  um  for example it has violated an integrity constraints or its schedule can not be serialized and so on then it is said to be in a failed state and once a transaction has rolled back from its failed state that is it has undone whatever it it had done already then it is said to be in an aborted state and if the transaction has committed successfully then it is in a committed state and either aborted or committed state is called a terminated state  10  31  now let us look at these states of a transactions in terms of  um  in terms of recovery that is what kinds of states require recovery of a transaction ok now if a transaction has terminated its its either aborted or committed then we we  um  wouldn t have lost atomicity as a part as a part of the transaction that is if the data base crashes after a transaction is committed or or or aborted it should be such that atleast the dbms should be designed such that these transactions should not be executed again that is it should not we should not submit this transaction again to the  um  to the dbms for example if the user has requested for withdrawal of say thousand rupees from an atm and the transaction has abort  um  or rather the transaction has committed and thousand rupees has been dispensed from the atm and right after commit the the the system crashes then the dbms should be designed such that  um  whether or not this data is there on or is has been updated on the on the dbms for a for a variety of reasons which will see shortly whether or not this data is updated on the dbms this transactions should not be run again that is the the the user should not be given another set of thousand rupees after the system comes back because the the transaction has already run and the operation has already been performed whatever operation has been asked for on the other hand if if a failure occurs during let us say active or partially committed state then we may have to in some cases undo whatever has been done by the transactions whatever operations has been done by the transactions then  um  probably resubmit the transactions that is re run the transaction once again from from the start this slide shows the state diagram or state transition diagram for the the different stets of a transactions that is we start from the active state and the active state can go to either partially committed state or a fail state depending on whether all operation in the transactions have been executed successfully or whether there are been some problems and even in a partially committed state there is a chance of failure if the transaction finds out that it can not commit for example if the transaction is  um  dependent on on some other transaction to to commit in case of and and the other transaction rolls back and this this transaction is subjected to a cascading roll back so in that case even if all the operations have been  um   there is still a chance of failure even from the partially committed state and if nothing goes wrong then we we can go ahead and go to the committed state or once we reached a fail state then the transaction goes in to the aborted state that is it rolls back whatever  um  been done so it undoes whatever operations has been done and it goes back into an aborted state  13  54  let us look at the concept of serializability again  um  where where we talked about what is meant by serial schedule and how do we know whether a serial schedule is valid or not in order to  um  determine whether a serial schedule of transactional activities that are interlude between one another in order to know whether this is valid or not we have introduced the notion of conflict serializability as  um  if you remember conflict serializability  um  is a mechanism which define s the notion of conflicting data base activities what is what are conflicting activities consider two activities i and j belonging to two different transactions t one and t two now i and j can can be executed in any order that is i before j or j before i doesn t matter if i and j refer to different data elements because they they they don t affect one another and i and j could still be executed in any order if they refer to the same data elements as long as both of them are just read operations so so both of them are just reading the given data elements so it doesn t really matter whether i reads the data data elements first or j reads the data elements first on the other hand if either i or j or both contain a right operation on the same data element and both of them of course refer to the same data element then they are said to be conflicting so we can not swap the the execution of i and j and expect that the swapping is an equivalent schedule to the earlier schedule so if i have a schedule of  um  operations data base update operations i can verify whether the schedule is safe or not by  um  by seeing whether it is conflict equivalent that is can i keep re arranging the operations of this schedule so that i i  um  eventually end up in a serialize schedule that is all activities of one transaction are performed before the activities of the second transaction so so i end up in a serial schedule without encountering any conflicts during the way  16  14  so so if a schedule is can be transformed in such a way or if it is conflict equivalent to a serialize schedule then it is said to be a conflict serialize schedule  16  25  we also saw the notion of non recoverable schedules that is in in what cases can we we can never recover from from a crash and so on ok so the slide here shows an example of a non recoverable schedule there is a transaction t eight which is reading a data element a and writing something back on to a that is it has performed some computation see as long as  um  when we are concerned about recovery we are not really concerned about what kind of operation it makes as long as there is there is some right operation we assume that some update has taken place may be there was no update that is it has just read the data elements and return it back for for whatever reason but at the level of of recovery mechanism  some change there is some modification that has happened ok so  um  this transaction t eight has read a data element a and it has return it back onto onto the data base now after it has return it back onto the data base another transaction t nine read that element of a and of course did some thing and then  um  collect it so this this commit operation could probably involves some kind of physical operation like say displaying the data element may be it is the it is the new stock prize or what ever it it just  um  dis displayed the data element however this transaction t eight try to do something more and crashed ok now  um  because transaction t eight has crashed it has to be rolled back that is whatever operations that are performed by t eight has to be rolled back but we can not  um  roll it back because transaction t nine which has already read the changed data element has already committed and whatever data that is return is already out in the open and its its been displayed so such a schedule is a non recoverable schedule and how do we how do we prevent non recoverable schedules from occurring simple way of preventing non recoverable schedule is  um  from this example  um  is to note that transaction t nine can not commit until transaction t eight has committed that is if a transaction is reading a data element that is written by some other transaction then it can not commit until the  um  previous transaction has committed so  um  in that way transaction t nine can not display the value of data element a and until and unless transaction t eight has successfully completed  19  07  but even then that is even if we stipulate  um  that transaction t nine can not commit until transaction t has  cascading roll backs that is shown in this slide here that is there are three transactions t t one and t two and this transaction t has read data element a and modified it and return it back into the data base now this modified data element now is read by t one and then t one in turn modified it again and wrote it back in to the data base and t two then  um  read this second modified data base second modified element that is the the the  um  data that was modified by t one and then probably dis try to display it or something and of course because we have ensured that none of them can commit until t can commit there they are just ready and waiting for for performing what whatever their commit operation tells them to do that is whether you display it or dispense money or whatever ok now transaction t instead of committing crashes for whatever reason now because transaction t has crashed transactions t one and t two even though they have completed successfully have no option but to roll back so this is the  um  this is the problem of cascading roll back  20  36  so so even if the schedule is recoverable sometimes transaction t is a long running transaction it runs for several minutes or probably sometimes even several hrs transaction t one and t two are short transactions and there are several such transactions are waiting on on  um  transaction t to  um  for transaction t to commit now for whatever reason if the t transaction crashes whether it is a  um  transaction failure or a system failure or a media crash or whatever we have to roll back and all these  um  transactions that are been waiting on transaction t  21  17  so let us look at how to tackle all these problems in in a systematic fashion so the the concept of recovery  um  recovery from transaction failure is a process of restoring the date base to the and  um  where do we restore it we restore it to the most consistent state that was there before the failure and there two kinds of recovery strategies that we are going to be seeing today which are called the deferred update strategies immediate update strategies deferred update essentially means that the data base  um  or or updations to the data base are deferred until after sometime which will which will  um  formalize later on ok and immediate update techniques update the data base as in when transactions are running  um  the data base is physically changed as in when transactions are running  22  16  so this slide defines both of these techniques again the data base is not modified until a transaction reaches it commit point in deferred updates update techniques and in immediate update techniques data base is updated as in when transaction progresses however transaction fails in in in immediate update techniques you you have to undo this operation that is you have to change this therefore they have to be logged whatever update  um  were made to the data base have to be logged before the updates are made obviously you cant log the updates after making the updates because what happens if the system crashes once you have made an update and before writing the log on the other hand if you have written a log and the system crashes before making the update it is still not a so much of a problem as we will see later  23  08  before we go on to recovery techniques there are two things that we have to define and we have to be careful about how  um  these impacts recovery techniques the first issue is that of cache management you might have studied in in an operating systems course that most operating system use what is called as buffer caches and what are buffer caches buffer caches are some  um  buffered areas in in memory that acts as a cache for data that are present on disk that is whenever a disk block or or a disk sector is accessed or is sort by the operating system instead of just reading one disk block usually operating systems perform what is called as read ahead it reads a set of blocks into main memory and all writes that are performed on to disk sectors are initially performed just on the main memory and nor onto the disk and its its only once in while these cache or or this buffer cache is flushed onto disk this is done in the in the interest of performance that is why for example in most operating systems  um  you need to perform some kind of disk sanity checks if the operating systems crashed midway because not all  um  blocks that that are been modified would have actually been written on to disk now this buffer cache is  um  is an operating system construct that is it is in the control of the operating system and application programs or user level programs that run in an operating systems usually don t have control for this buffer cache but for data base recovery we need to have control over this over this cache because we cant  um  we cant assume that the operating systems has written something onto disk after we have we have aid write operating systems in turn has its own mechanisms that that that might defer writings on to disk and which may impact a recovery process therefore typically  um  in many database management systems what is done is  um  a part of the buffer cache that is maintained by the operating systems is given to the dbms that is  um  the dbms is given control of this buffer cache so that  um  it can  um  that is the dbms  um  can control  into the buffer cache and so on ok and  um  such kinds of cache pages which are given to the dbms are called dbms caches and of course  um  there is there is also the problem of what happens if the system crashes when the cache has being written on to on to disk that is i have written something onto cache and now i am flushing this buffer cache but during this buffer cache flush the system crashed and and what do we do  um  the cache is partially written and the data could be inconsistent and so on for that a technique called shadow paging is used which which we are going to study in the next session on on data base recovery technique the the shadow paging technique that is used for data base recovery can also be used for maintaining or recovering  um  cache contents in the case of crashes  26  47  the second issue that we are going to be concerned about is the concept of log now we  um  we we have mentioned in passing that  um  in order to aid the process of recovery from from data bases we maintain logs that is whatever updates are made to the database are all logged in some log file now this log file keeps on growing because every update that is made to the data base the information about this update is is kept in this log files now this log files keeps on growing and we don t know when a crash would occur in and how much of history information we need and so on so how do we prevent this log from growing forever and  um  the probably becoming bigger than the data base itself so  um  the answer to this is the notion of a check point a check point in a log records a state where all committed all transactions that have been committed until this point in in time have been physically modified on the disk in the data base that is the data base has been updated and everything is fine for all the committed transactions until a check point so all committed transactions that have been  um  information about whom have been stored in the log until a check point can be thrown away that is at a check point we can throw away data about all the committed transactions that that have happened before the check point and  um  at what frequencies do do we have to check point the log that is check pointing rather that is the the process of introducing a check point in log as you might have imagine is is  um  a separate process by itself that is we have to decide at what intervals at or at what frequencies are we going to introduce check points into the log and what should be done when a check point is being introduced and check pointing actually involves suspension of all activities of the data base all transaction activities of the data base temporarily until we know for sure that all the that that that this check pointing criteria is made that is all the committed transactions have been successfully updated onto the disk  29  08  the algorithm for check pointing id is quite simple but quite costly in terms of operations that is  um  in order to take a check point we first suspend all transactional activities temporarily because we don t want more data to be written when we are  um  when we are handling this check pointing and then we force write that is we flush all main memory buffers that are been modified to disk that is whatever has been  um  whatever data that was that had to be updated onto the data base we force write all of these committed   and then we write a check point note in the  um  in the log file and and also of course force write this this log on to disk the fact that we are written  um  the fact that we have encountered a check point should also be recorded persistently onto disk because once we have thrown way information about other transactions  um  we cant lose the the fact that we have performed a check pointing operation and then we resume transaction activities  30  20  there is also a notion of fussy check points where that that  um  that are more slightly more efficient than the the usual check pointing techniques note that check pointing requires suspension of all transaction activities and it is done too frequently then it impacts data base performance itself now let us see where is the biggest overhead  um  during a check point and seek can we do something about making this check pointing faster the main overhead in check pointing  um  am sure you would have imagined that is is the flushing of all the buffer cache onto the disk that is each buffer cache contains set of disk blocks and  um  so all of these disk blocks should be physically flushed on to disk and this what is going to take the most time now in fussy check pointing what happens is that transaction activities resume after writing the check point entry into the log even though flushing has not been completed that is even the the check pointing  um  check point entry could be in the cache and all the flushing actives activities are still going on but transaction activities  um  the the transaction activities resume however the previous checkpoint is not released that is the the older log entries are not deleted until after the new check point entry has been flushed onto the disk so so it s a background operation where until we are sure that the new check point operations  um  operations has been written on to disk this can be written onto disk only after all the buffer cache buffer has been flushed on to disk so until we are sure this has been done the previous  um  set of log entries are not deleted  32  11  so let us look at the first kind of recovery technique which we called as the deferred updates recovery so what is the notion of a deferred update recovery as the name suggests deferred updates means that the updates to the data base are deferred until transaction commits that is until transaction has reached a ready to commit state the the overall strategy for a deferred updates recovery is simply this thing that is a transaction can not change the data base so even if a transaction has run half or ninety percent or ninety five percent or whatever it has not made any changes on to the data base as far as  um  until it has committed so until it reaches the commit point database  um  the data base is not updated and a transaction does not reach its commit point until all its update operations are operations  um  are logged and the log is force written on to disk that is the the transaction does not say am not ready to commit until all its operations that have been done have been logged and this log is available on the on the disk therefore even if the transaction crashes now and even if the data base has not updated we have the log entries which says that these are the changes that were made and the transaction is now ready to commit and it can commit  33  42  so using this let us see how we can perform recovery we first look at recovery in a single user environment that is it is as sequential data base engine where transaction  transaction is performed one after the other this is simplistic case but it helps in understanding how the deferred update techniques technique works so deferred update techniques uses two lists of transactions that that is one is the list of all committed transactions the when does it use the two lists of transactions that is after it is  um  after the disk that is after the system has been brought up following a crash and the the system is asked to recover to to a consistent state now once the system is asked to recover to a consistent state the recovery process starts by using two lists of transactions one is the list of all committed transactions till the since the last check point and the list of all active  since it s a single user operation there would be utmost one such transaction which would have failed in a active state and a logs that are that are maintained for these transactions are maintained in the following form which is shown here that is the first element here says that this is a write item that is some thing has been written on to disk note that as far as recovery is concerned we are interested only in these write items logs can be used for a variety of purposes something like to to to to understand the behaviour of transaction to to profile the performance of the data base and so on and so forth but as far as recovery is concerned we are just interested in what changes have been made to the data base there fore we are interested in in only these write items so it says this is a write item belonging to transaction t involving data element x and this is the new value that was written on to the x or  um  that has to be written on to the data base for element x  35  48  now once these two list are maintained that is the the list of all committed transactions since the previous check points and the list of all active transactions we first start by re doing all the committed transactions that is  um  we we take the set of all committed transaction since the previous check points and then go about looking at the logs and see what values  um  they they had written on to what data elements and we start writing those values once again we don t care what is the semantics of these values are  um  whether these values were read and  um  or anything because these transactions were already committed the cash has been dispensed already so so we don t have to do any physical operations we don t have to we don t have to even tell the application program that we are doing these things because these were already committed and we know what are the values that has to go into the data base we just write those values we just start from the the previous check points and start  um  data element x has to have a value of ten data elements y has to have a value of abc or whatever and we just start writing those values back in to the  and then for all the the active transactions which crashed midway  during execution we just have to restart all those active transactions because that because none of the active transactions are physically modified the data base that is what the deferred update technique all about that is transactions don t  um  don t modify the data base until they are ready to commit and they wont be ready to commit until they have made all the log entries on to disk after they have made all the log entries on to disk and said that they have been committed then they have been treated as a committed transaction and the first and all the redo operations from the transaction were performed if for any case the transaction that is they are ready to commit tag does not go on to the log or or  um  whatever has been written by the transaction is not flushed on to the log and and the system crashes then it is treated as an active transaction and it has just started once more and  um  it runs once again and makes changes on to the logs and if if everything goes well that it the log is flushed on to disk then it is ready to commit and  um  and the data base is updated  38  27  what about  um  updates if the data base is actually a multi user environment and there are several concurrent transactions that are running at the same time in a single user environment we don t have to worry about concurrency control that is  um  transactions are serialized they they they run in as serialized fashion by default but in a multi user environment there is the there is the problem of serialization of transactions now serialization of transactions is not the problem of the recovery part of the data base its usually handled by the concurrency manager that is the  um  whatever concurrency control techniques are used for  um  managing concurrency now we assume that for deferred update techniques we require that concurrency control uses what is called as a strict two phase locking you might have heard of the the notion of locking in several contexts in operating systems in and  um  probably in  um  even in systems design and so on where locking ess essentially means that if a transaction is performing some some updates on a data base or some data elements it obtains a lock that is a  um  it locks the data elements so that nobody else  um  can read or write to the data element or can access the data element of course there two kinds of locks that is read locks and write locks so read locks can be shared but write locks are exclusive locks can not be shared that is once a transaction has obtained a lock on a data element that it is going to modify no other transaction can read the element until the lock is released by the transaction so in strict two phase locking all the locks that are held by a data base or or by a transaction are not released until the transaction reaches its commit point now what does it mean that it reaches its commit point that is all these updates it has made or or logged  um  that is are written on to logs and the log is flushed on to disk ok now now the transaction is sure that somewhere  um  whater whatever updates it has made is persistently stored that is it s the updates it has made is safe somewhere its not just lost once  um  if if the system crashes only then it will release its locks so only after it releases its locks can other transactions read the data element read the corresponding data element  41  08  so  um  assuming that we have strict two pl that is strict two phase locking the recovery process can can follow the  um  can follow the technique shown here firstly make two lists of transaction once  um  after a data base has crashed and it has restarted and the recovery algorithm is is begin and it is starting to recover make two lists of transactions that is first is the set of all transactions that have that have committed since the previous check point and and the list of all active transactions now for all the set of transactions that have committed we have to redo the operations this is the same thing we have done in the single user environment however here we have to ensure  um  we have to  um  we have to explicitly state that redo of the operations are performed in the same order as they appear in the log we can not we can not try to optimize this redo operations by introducing some concurrency there because they may violate  um  some kind of  um  serializability conditions if they are performed in some other order  between two or more updates such that they can not be swapped in their ordering and once the read and redo operations are performed that is once all the committed transactions have been  um  persistently written on to disks we then restart all the active transactions and before that we release all the locks that are been held by this active transactions and again the the ground is free so that whoever wants the locks can can now hold those locks so so we release all those locks and resubmit all the active transactions once again  43  01  let us take an example of different transactions and see what happens in a multi user environment so this slides shows a set of transactions and  um  and two different events one is a check pointing event that that happens here and one is the system crash that happens here now there are several transactions in a data base here ok transaction t one has already committed before the the check pointing event happened therefore after the check pointing data about three pone data t one are thrown out we don t even need data about t one anymore however t three and t four have started before the check pointing operation but they have not completed therefore we can not throw out these we can not throw away these data about t three and t four even if check pointing is performed and transaction t two has started only after the previous check point but has committed its operation before the system crash while t four and t five have not yet committed t four is a really a long transaction that has  um  that is taking place and they have not committed when the system has crashed so so what happens what happens in the update operation here that is during the deferred update operation t one is not concerned at all it doesn t figure in to the picture at all because there is there is no data about t one so  um  at the time of the updates that is that is happening here committed but the data is still there because it is its occurring after the check point and because it has committed transaction t two will undergo redo operation that is all its updates are logged into log file and using the log file the data base is updated transaction t three has also committed before the system crash it has started before the check point there its data will not be thrown away and because it has committed before the system crash this is also re done that is  um  redo operation will be performed on transaction t three and transaction t four is still active at the time of  um  system crash as this transaction t five therefore both t four and t five have to be resubmitted to the dbms that is after releasing all their locks that they have held they have to resubmitted back to the dbms  45  27  so that s what this this slide says that is data about t one can be safely removed after check point it doesn t even figure during syste system crash that is it doesn t even figure during updates recovery rather and tranasc data about t three and t four should not be deleted at check point because they are not committed and transaction t one is unaffected during the recovery process t two and t three are subjected to redo operations and t four and t five were aborted and resubmitted back into the dbms  46  00  there is some efficiency issues about redo operations if we  um  small thing we can notice if a data element x has been written  um  it is enough if we just write the last update on to the data because anyways if we write a previous update it is going to be over written by the next updates ok hence for redo operations we can start from the end of the log and start making updates moving backwards in the log and we should not write a data element on to the data base if it has already been written once on during redo because we have already written the latest value during redo  46  45  what are some of the properties of deferred updates there is no undo that is required if you have noticed we have only talked about redo there is no undo operations that that are required why because the data base is not modified at all its only the transaction logs that are modified and since all locks are released in in strict two pl all locks are released only after commit a transaction can read a data element that is being modified  um  that is no transaction can read a data element that is being modified by another transaction therefore there is no possibility of cascading roll backs because  um  one transaction has already read  um  a data element that is been modified by some some other transaction and waiting for it waiting for it to commit so there there is no such possibility so there is no possibility of cascading roll backs however potentially there is a large amount of disk operations during commit because enormous amounts of updates in especially large transactions that have been written on to logs have to be written back on to the disk  47  53  the next technique that we are going to look at is what is called the immediate update techniques in immediate update techniques the data base is updated as in when transactions execute however for for the sake of recovery data base updates are performed after the updates are recorded in the log and the the log is written on to disk that is only after we know that is there is still some kind of  um  some kind of deferred updates happening here that is updates are deferred only after we know that is the log is written on to disk and before which we we modify the data base as in when we know that a particular  um  log entry has been written on to disk all those corresponding entries can be modified and however in the in the phase of a crash we have to undo all un finished transactions since the last check point we we need an undo operation here which is not required in the deferred updates and we still need the redo operations for  um  redoing all the activities of the committed transactions since the last check point  49  03  so let us first look at  um  the undo operations that is what should be done for all the un finished transactions now in order to perform undo we need an extra element in the transaction logs this this is shown here that is a transaction log contains this write element  um  write item element and and transaction t that is this element belongs to transaction t involving data element x and it says that the old value of x is this and the new value was this therefore when we are undoing it we have to replace x by its old value rather  um  and because we dint have an undo we dint have we dint have to store old value in deferred updates and of course undo operations have to be performed in the reverse order obviously because the oldest value have to remain on the data base and these log entries the the way these log entries are created  um  such that undo and redo operations are what are called as idempotent operations what is an idempotent operations  idempotent operation is something which where it does not matter how many times you you perform the operations for example some problems during undo and undo couldn t finish we can just restart this undo process from from the beginning once again and then run it again and it doesn t matter because its just re writing the old values its not performing any computation its not saying that value of x was changed by five percent so reduce it by five percent or something like that so  um  it it its not its not performing any computation its just re writing back on to disk  50  44  so how do you perform recovery  um  in single user environment using transact using immediate updates again like deferred updates we use two lists of transactions list of all committed transaction since the last check point and list of all active transaction since the last check point and first we start by undoing the the the activities of the set of all active transaction using the undo policy that we just saw in the previous slide and then we perform the redo operation of all the  um  transaction that have been committed since the last check point and then we we submit all the active transaction to the dbms so that they can execute once again  51  31  and how do we perform recovery in in multi user environments its its using immediate update techniques it s the same its its quite similar to that of the deferred update techniques where we use strict two phase locking so that none of the locks are released until the transaction is committed therefore there is no possibility of a cascading roll backs and  um  and as before use two list of transaction that is the this list of all committed transactions and the list of all active transactions since the last check point then undo the writes of all active transactions using the undo policy and redo the  um  write operations of all the committed transactions using the redo policy and then just release all the locks that are been held by the active transactions and  um  give the transactions back to the dbms that is resubmit the transactions  52  26  so what are the properties of immediate update recovery  um  as you can see database updates can happen as in when logs are written on to disk  um  that is  um  the the the operating rather the dbms should only keep track of when the buffer cache logs are written on to disks as in when the the buffer cache logs are written on to on to disks the appropriate data base update can also start happening so  um  because of this  um  data base updates the the load on data base updates is uniformly distributed they are not they are not burstive as in deferred update transaction deferred update techniques where  um  all updates happen  um  at the commit point and  um  but note that  um  the the any physical activities that is being performed by the application program like say dispensing money or or launching a missile or whatever so any physical activities that is to be performed by the upper application program can be performed only after the database is updated that is only after we know that  and the database is updated why this is because even if transaction is committed that commit the the the the  um  the fact that it has been committed might still be in the buffer cache it may not be written on to disk and the system could crash and once the system  um  comes back again it is treated as an un committed transaction and it is run once again  even if in memory it  um  the dbms knows that it has been committed it can not or it should not tell the application programs saying everything is ok everything is still not ok  um  it will be ok only when the the disk is updated that is  um  once they are flushed on to disk so its its commits can be performed only after either the database or the logs are updated that is force writing logs on to disk whenever a commit happens  54  34  so that brings us to to the end of this this session on  um  transaction recovery using log based recovery techniques here we saw  um  several different issues we started with several different issues concerned in recovery in data base systems we looked at the different kinds of failures that can happen and what it means to to recover from  um  from a system crash or or or some kind of a failure and there are two issues that affect recovery processes one is cache management that is  um  for the sake of efficiency disk blocks are usually cached into ram in the buffer cache and and because we are talking about recovery this can impact recovery process because we are not really sure whether something that has been written on to disk has actually been written on to disk so so some part of the buffer cache is usually controlled by the dbms and which is called the dbms cache and then we looked at the concept of check pointing which allows us to throw away safely throw away  um  historical information that is stored in logs and then we looked at two kinds of log based recovery techniques deferred updates and immediate updates both of which use  um  what are called as idempotent undo and redo operations there are no undo operations in deferred updates but there are undo and redo operations in immediate updates and and then we also categorized this log based recovery into two different kinds single user environments and multi user environments and in multi user environments we have a requirement that we have to use strict two phase locking in order to prevent cascading roll backs  um  in case of a  um  crash recovery so that brings us to the end of this session ../ lecture # 29 hello and welcome let in the previous session we have been talking about recovery techniques in data base management systems how do we recover a data base especially when the data base was running and the the the system was subjected to some kind of a failure and we saw that there are different varieties of recovery techniques each having their own advantages and disadvantages let us continue with this topic further today and bring it to a logical conclusion  um  by looking at all the other issues that that entails database recovery let us briefly summarize what we have studied now  um  when ever you are thinking   2  00  given computer system whether it is data bases or or otherwise  um  is is subjected to frequent or  um  frequent or non frequent kind of failures different kinds of failures and we could classify failures into different types like say system failures or media crashes or communication failures or transaction failures system failures and transaction failures are by far the most frequent or the most common kinds of failures what is a system failure system failure is something like suppose you are running your dbms on on a machine and lets say the the power goes off and your machine crashes the the characteristics of such a system failure is that all  um  data that are there in the volatile memory like ram are lost how ever data that are present on persistent storage like disk are still retained so so whatever has made it to the disk can still be retained however what ever was still in the ram is lost similarly there are  um  failures like transaction failures which are again quite common transaction failures entail failures where  um  for for a variety of reasons given transaction can not is is not able to complete its operation this reasons could be something like too many processes running in this system or  um  not enough privileges  um  for for for running this transaction or the transaction trying to do illegal transaction and and so on and so forth ok and these are again quite frequent kind of failures and here again  um  the the the the the characteristics is that whatever the transaction was doing  um  and whatever data that the transaction had in the main memory is lost however the data that are present on persistent storage like like disk are still retained then there are  um  communication failures especially if your data base is distributed and and you had to communicate between two or more  um  two or more different  um  geographically distributed data bases then you might encounter communication failures which which  um  which need to  um  abort a transaction or or  um  or or leave a transaction midway and there are  um  some what in frequent failures called media crashes like like a disk crash like a disk developing bad sectors or or some kind of a damage physical damage to the disk where not only data that are there in the physical memories in in the primary memory is lost but the data that are present on on the disk are also lost that is the the the you you   4  51  now what is the main property that we have to consider or that we have to maintain when we are talking about  um  recovery techniques as we all know transactions in data bases follow this  um  well known property of acid which stands for atomicity consistency isolation and durability rite  um  so  um  atomicity as essentially says that  um  either all updates that are to be performed by a transaction are performed or none of them are performed  um  this is obvious because suppose you are transferring money between two accounts let us say you you have a debit in one account and a credit in another account either both of them have to be performed or none of them have to be performed so either the transaction shou should run fully or it should not run at all and so on its not  um  its not sufficient if money is debited but credited or vice versa similarly consistency a transaction  um  when when it completes can not leave the data base in an inconsistent state we have seen some integrity constraints and  um  notions of triggers where  um  suppose  um  data data base consistency is violated automatically triggers are enforced which which will cause the transactions to roll in in many commercial data bases so  um  so if the data base starts from a consistent state at the end of a transaction it should remain in a consistent state and of course the third property is of isolation where when multiple transactions are executed simultaneously for efficiency reasons they are they are executed  um  in in a concurrent fashion that that is the updates made by each of the transaction are  um  are performed concurrently however the net effect that is not all possible concurrency is is is is ok essentially the the the  um  the kind of concurrency that is permitted is that the net effect of these  um  multiple transaction should be such that  um  it it should be as though the transactions are been executed in some kind of serial fashion and the the last property is about durability where once a transaction commits once a transaction says that i have committed than whatever changes that it has made is persistent after commit it should never be case that  um  let us say system crashes after a transaction has committed it should never be case that the transaction now is aborted or rolled back once the transaction is committed it means that it has performed a some kind of physical operation that is some  um  it it has performed some kind of a operation that is beyond the  um  the the  um  purview of the data base itself for example when the in  um  in in an atm transaction let us say in an automatic teller machine transaction once a transaction commits it means that it is safe for the atm to dispense money after the atm has dispensed money we decide to actually abort the transaction and then say ok it has been not committed and so on so so once  um  changes that are made by a transactions are committed they have to be durable that is it has to be persistent  8  07  and we also saw that transactions can can be in different states like active or partially committed or failed aborted  um  committed or terminated rite so an active state is is the initial state when the transaction is executing it is its performing some operation it feds some data elements and made some changes and so on and partially completed is a state when it has performed all its operations and it is ready to commit  um  its its not yet  um  committed and once it discovers that it is not able to  um  proceed further wheth whether its not able to commit whether its not able to calculate or perform its computation further we say that the transaction has failed once a transaction has failed it has to roll back any changes that it has made to the data base has to be  um  reverted has to be undone  um  and once a roll back is complete we say that the transaction has aborted and if the  um  on the other hand if the transaction successfully commits it s a committed transaction and a terminated transaction which has something either aborted or committed  9  15  so this slide  um  which is now familiar shows the different states of a transaction in in a in a in a schematic fashion that is an active transaction can become either partially committed or failed transaction depends depending on what it has whether it has been able to perform all its operations or its not able to perform all its operations and from a partially committed transaction you can still go to a failed state when you see that its not possible to commit the transaction but once you see that it is possible to commit the transaction then you go into a committed state  um  and once you come to a fail state you you perform a roll back operation where you go to the aborted state in the transaction  9  54  what is meant by transaction recovery once again a a transaction recovery is is a process by which we  um  recover the data base  um  system which is performing several different transactions into the most recent consistent state rite that is  um  at any given point in time  um  any  forming several transactions simultaneously now  um  during such a time when there are some kinds of system crashes let us say some kind of failures like say system crashes or  um  transaction failures or media crashes or whatever it it basically leaves the system in an consistent state half of some transactions have been performed some of them have been committed some of them have just started and some of them  um  were shown to be committed but but may not have been committed and so on and so forth ok so its essential that when we recover from this system crash we should recover to a state that is obviously consistent and it has to be not just any consistent state it has to be the most recent consistent state that was there before the crash and all active transactions around the time of the crash that is  um  when the crash happened there are let us say ten different active transactions that were running once we recover from the crash of all these active transactions there should be either  um  in a committed state or a aborted state after recovery that is either all the  um  operations that are performed by the transactions are persistently stored in the data base or they are rolled back so that they can be invoked once again buy the dbms system  11  46  we also  um  saw that there are certain pre-requisites for  um  recovery to happen that is there are certain kinds of schedules by which transaction activities are  um  are performed which are not recoverable at all  um  for the for example this slide shows two different transactions t eight and  following operations t eight performs a right operation and a element called a ok so it reads an element a and  um  and make some changes and writes it back to the element now let us say t nine reads the new element of a which has been return back by t a and commits that means let us say displace ok so so let us say something like your you have you have just updated the bank account in your in your bank or updated your stock prize or something like that and there is another transaction which has read the new prize or or the new balance account and displayed it or printed a statement let us say commit a some some kind of physical operation so let us say it printed a statement however  um  the the transaction t eight has not yet completed here and  um  while it is still going on it it encounters a system crash and  um  once we recover we say that t eight has to be rolled back or has to be aborted and restarted again now  um  but what what has happened here is that this transaction t nine has already said that  um  the the new value of a is so and so and and it has already printed the statement or displayed it or or  um  done something of that so such a kind of schedule is not a recoverable schedule that is we we will not be able to recover from such a schedule  13  27  there is also another kind of problem with recovery namely that of cascading roll backs this slide shows such a example here  um  that is even when a schedule is recoverable  um  sometimes whenever we roll back a transaction there might be several other transaction that are waiting on it that have to be rolled back and all of them have to  um  roll back one after the other which causes a cascade of different roll backs so here there are three different such transactions t t one and t two where  um  t has read element a and return something back to here now whatever has been return back by t a or or or by t is now read by t one and its return back and whatever has been return back by t one is now read by t two no  um  but transaction t has not yet committed here and its its still going on and then it crashes ok now after we recover from the crash suppose we decide that transaction t has to be aborted it means that transaction t one as well as transaction t two have to be aborted because they have read some changes or they have they have  um  they are basically depending upon the changes that are made by t itself so this causes the problem of cascading roll backs  14  49  so we  um  we saw how to prevent cascading roll backs and  um  how to prevent non recoverable schedules in in the previous session which i will not go into more detail here essentially  um  in order to prevent  um  in order to prevent non recoverable schedules  um  you need to ensure that no transaction reads a data element that is written by an another transaction unless it has committed unless there is  um  only after committing  um  a particular operation can another transaction read this transaction we also saw two kinds of log based recovery  um  what is a log based recovery log based recovery uses what are called as write ahead logging or what are also abbreviated as wla write ahead logging addresses wal rather write ahead logging ok now what is a write ahead logging write ahead logging essentially means that before performing any operation on the data base you perform a corresponding operation on to a log file and and then perform  um  the operation on to the data base now whenever the data base crashes you have to use your log file in order to be able to recover from your system failures we saw essentially two kinds of techniques based on write ahead logging what are called as  in deferred update techniques data base has not modified until after the transaction reaches its commit point that is whatever changes that a transaction makes  um  to the data base is first just return to the log and  um  only when the the transaction is ready to commit  um  we will the log be return on to the or or or  um  or the changes be return onto the data base itself so until after the commit point transaction or or  um  the the or the data base is not modified and and once the commit  um  succeeds you also enter a commit or committed entry into the log so essentially whenever the system crash you you just need to redo your transaction so that it reaches the point where it it has been and there  um  and there are other kinds of techniques called immediate update techniques where data bases updated as in when the transaction progresses however as as the name write ahead logging suggests that the the updates are first written onto the log before they are written onto the data base and immediate update techniques requires both undo and redo operation that is you need to undo all active transactions which have not committed and you have to redo all transactions which have committed actually  17  41  the general idea behind recovery using deferred updates is is shown in these slides and such kinds of that is there is no undo operation as part of this data base as a part of this recovery technique however there is a redo operations ok because why is there no undo operations because the data base is not  um  touched at all that is the the data base is not modified at all until  um  the the transaction is ready to  multi user system in a multi user data base system  um  u u u you have to perform lets say some kinds of two phase locking in order to  um  prevent  um  in order to enforce isolation  um  constraints and all locks that is whenever you are holding such a lock on a transact on a  um  on a data base item all such locks are released only after the commit therefore no transaction can read any other  um  can read the values written by any other transaction before it has committed therefore there is no possibility of cascading roll backs and also there is no possibility of a non recoverable schedule  um  occurring as part of this  um  recovery mechanism however the problem with deferred update techniques is that suppose a transaction performs let us say  um  some five hundred different operations now none of these five hundred different operations are written on to the data base until after the transaction is ready to commit therefore once a transaction is ready to commit suddenly there is a huge spurt of activity suddenly there is a all five hundred different operations have to be performed on to the data base at one instance of time rather than being distributed over the entire life span of the transaction therefore there could be potentially heavy disk operations during commits which could bring down performance or which could cause the the system to thrash or  um  lead to some other such problems  19  52  with immediate update techniques that is  um  where locks or where the data base is updated as in when the transactions are written onto logs we require both undo and redo operations so so such kinds of recovery techniques are also called undo slash redo transact or undo slash redo recovery techniques rite now what is the general idea behind immediate recovery techniques the general idea is that  um  as in when the the transaction is  um  executing the data base is updated even before committed which is which is very important here that is the data base is updated as in when the transaction is is is  um  is executing and the transaction may not commit at all the the transaction  um  may become a fail transaction it may abort it may have to roll back and so on however as in when the transaction is executing the data base is getting updated what it means is suppose the transaction can not commit suppose the transaction is not able to  um  successfully perform its commit operation then we need to roll back the transaction now because we are changing the data base as in when we are performing the  um  performing the transaction we have to perform the we have to perform corresponding undo operation that is we have to undo whatever the changes that the transaction has made with the data base so data base updates or perform after the updates are recorded in the log and the logs are written onto disks and of course it may not necessarily be force written onto disk that is you just write it onto log and then write it to the data base now once you know that the the log has been written to disks  um  its safe for you to write to the data base now suppose there is crash at any point in time and to do two different operations one is you have to undo all unfinished transactions since the last check point and you have to redo all committed transactions since  um  as as you know our different stages in in in transaction logs where we say that it is safe to throw away all other historical data that is that is there in the log because the the  um  the log is a monotonically increasing file which where every operation keeps on adding to the log suppose we don t truncate the log it could well be the case that the log becomes far more bigger than the data base itself and becomes far more difficult to manage than the data base itself therefore it is important for us to be able to conclusively to decide when at time what parts of the logs can be safely deleted or can be safely thrown away so that is where the notion of check points also coming to the picture  22  44  there is a third kind of recovery techniques which we have not seen in detail but i am including this here for the sake of completeness which are called undo based of transaction undo  um  undo variety of transaction recovery protocols we saw that that is undo slash no redo we saw that there was the the  um  the deferred update technique was no redo slash undo and the immediate tech update techniques were  um  undo slash redo and this is undo slash no redo the general idea behind undo based transactions is that  um  its its its  um  it s a recovery technique that is based on pure undo operation ok what this means is all committed transactions are flushed onto disks during check points that is  um  at every check point  um  in order to safely throw away certain transactions we flush all committed transactions  transactions are are  um  maintained that is the the chain of all activities by an active transaction is maintained by what is called as backward chaining in the log that is whenever i enter a entry for a particular transaction in the in the log i also maintain a pointer to its previous to the previous operation that it performed therefore what  um  and and and the way that these log entries are maintained or what are called as compensating log records  um  or also called as clr s where  um  for for every update operation you perform you store what is called as a compensating operation for example if you let us say  um  debit five thousand rupees from from an account a the compensating transaction or the compensating operation rather would be to credit thousand rupees or five thousand rupees to this this particular account rite so for every kind of update operation let us say you deleted a particular tuple the compensating transaction would be to insert the same tuple back to the data base the compensating operation so  um  so you maintain the log of compensating operations and at every check point you see whichever which are the transactions that are committed and flush them onto disk so that you don t have to  um  you don t have to undo them once again and at any time the system crashes or we encounter a system crash what we need to do is simply start from the state where the system has crashed and follow the backward chain of logs and perform all the compensating log records that is compensating operations that that are performed so this is the pure undo based operation where there is no need to redo  um  any transaction  um  or there is no need to redo  um  any transaction or  um  there is there is no need to redo any particular transaction activity the the redo is performed or the the flushing of transactions data are performed only at check points and and not after any system crashes  25  53  now let us look into the notion of check points itself in a little more detail as we said before a check point is some point in  um  in in in a log where we say that it is safe to throw away everything before this check point that is  um  all committed transactions until this check point have been are now durable that means they have been physically committed to the data base therefore the records of all committed transaction need no longer be stored as part of this  um  transaction log and it is safe to throw away all the  um  data about committed transactions and  um  the frequency at which we take a check point  um  decides  um  or some kind of a balancing factor between what is the amount of log that you are going to store versus what is the performance over head that you are going to incur because at every check you need to bring the data base to what is called as the quos on state that means you have to suspend all the activities all transactional  transaction that have committed and  um  ensure that they are physically modified in the data base and  restart the data base system that is  um  restart all the transaction activities therefore if check pointing is done at two frequent an interval if check pointing is done too frequently then  um  it it it impedes performance while if check pointing is done at too widen interval then it starts increasing the amount of log records that that you have to store  27  42  so what what is the general  um  algorithm for taking a check point we have to first suspend all transaction activities temporarily that is we need to take the dbms to a quos on state and then we have to force write or flush all main memory buffers that have been modified of committed transactions actually on to disk an then write a check point  um  record into the log and force write the log onto disk as well and then now it is safe to resume transaction activities  28  14  but then  um  as we saw that bringing a data base system to a quos on state is a costly operation just imagine  um  a large data base system like let us say the the data base system of of of  um  of for railway reservation let us say for for a given railway zone like say southern railway zone at any given point in time there are possibly hundreds or even thousands of transactions that are performing so many  um  requests coming from different places  um  for reservations and cancellations  um  or or some other kinds of activities not not just reservations and cancellations lets say rescheduling and so on there are several different transactions are running at any given point of time and  um  even if we stop the data base system let us say for  um  for a few seconds or even if or sometimes even a few minutes because the the size of the data base is is so huge it actually impedes  um  or or impairs the activities of of a large number of  um  requests that is the there could be a large number of people waiting in different reservation centers waiting to reserve their tickets and bringing the dbms to a quos on state will actually  um  stall all of them and so on so there is the  um  there are there are other technique that are used  um  which can which can try to obviate the need for bringing the data base on to a quos on state whenever a check point is being taken remember taking a checkpoint is quite important because otherwise the transaction log itself becomes too too difficult to handle but it doesn t he impedes the performance too much that is where the notion of fuzzy check point come into the picture that is fuzzy check points essentially  um  does not require normal check pointing requires suspension of all transaction activities but fuzzy check point does not require  um  in bringing the data base onto a quos on state essentially what we do in fuzzy check pointing is that we write the check point entry in to the log and we resume operations but we keep watching the disk itself that is at at the  um  at the physical layer for example note that i mean  um  note that when we write something onto disk the operating system may not actually physically write it on to disk it it would write it onto a buffer cache within the main memory so so its still volatile and it will write it onto disk at some later pointing time so so we wait until that point where  um  where the check point log actually comes onto the disk and then we start removing all the old entries that is what are has to be  um  can be safely removed are then removed then  31  17  a second kind of check pointing which  um  which does not required the data base to be brought to a quiescent state is also called a non quiescent check pointing operation  um  here  um  here unlike fuzzy check pointing there is also no need to watch the physical disk  um  sector or physical disk block for to to see whether the check point entry has actually appeared and but but but the idea here in non quiescent check pointing is that active and non active transactions at at the time of the check point or handled separately  31  56  let us look at how non quiescent check pointing actually performs there are  um  essentially three different steps in this non quiescent check pointing at any time if i have to take a check point what what i do is i first determine which are all the active transactions at this pointing time that is these are all the active uncommitted transactions that are currently running and then  um  make an entry called start check point with even to t k which are all the active transactions running at this point now this entry will now go into  um  the buffer cache and so on and so forth and finally at some pointing time it will reach the disk now wait until  um  and we are not concerned when they are going to reach the disk and we are not going to watch the disk at all we just wait until each of this transaction either commit or abort ok but we do not prevent other transaction from starting that is the other activities could be resuming by themselves but we simply have to wait until any of these  um  commit or abort when all of them are done that is they are finished or terminated we write a end check point entry into the into the log and then force write the log onto disk that is we flush the log onto disk now what is the use of such a check pointing that is how can we recover from such a check pointing  33  15  recovery from such a non quiescent check pointing is also quite quite simple let us say at some pointing time there is a crash and we are  um  and and we and we recover from such the crash and we are or we are started with the recovery process after the crash and we start to read the log backwards that is from the last entry onwards or suppose we meet a end check point entry first what does that mean that means that the previous check pointing operation has successfully completed rite that is we know all incomplete transactions that that were there at the at the time of the check point have been completed therefore there is no need to do anything that is it is safe we we just have to restart whatever  um  transactions were running  um  after this after this time ok but suppose we reach a start check point without an end check point what does this mean this means that the crash has occurred during the check pointing operation itself ok that is at the  um  these were the set of transactions that were running and all of them have not yet terminated they have not yet committed or aborted and the crash has happened so therefore what what is that we need to do we have to undo all these operations that is all the transactions all the operations that have been performed by this transactions there again we can use some kind of backward like like we saw in the  um  undo only kind of recovery algorithm by which we undo all of these operations and  um  until  um  how how far behind in the log we need to go we need to go just until the previous check point because we know that  um  the the previous check point would not have completed until all the active transactions at that pointing time have not yet completed ok so therefore we we never need to go beyond one check point that is beyond the previous checkpoint that is we can safely throw everything else beyond the previous check point rite and  um  we just have to trace them through the log till the till the previous check point and undo all these operations so so that s another kind of check pointing operations that that are used in addition to or or  um  in complement to the the the normal quiescent based check pointing where you need to suspend all activities of the data base system  35  41  there is another  um   which is called as shadow paging it was  part of sys in this but  um  where  um  where the the technique of recovery is is not based on logging but on maintaining different copies of data base pages that is the data base is is is managed and in terms of different pages or logical entities of of disk cooperation which pages could be just lets say disk blocks or disk sectors or set of sectors  um  and so on and  um  we basically make different t copies of these pages in order to maintain the log so there is no need for write ahead logging and all updated data are kept in a different location from the original data they are not kept on the on the main on the same data side and recovery basically entails that we discard the new page what whatever is the updated page and revert back to the shadow original page that is the the the  um  the shadow of the original page which which was still there that that we don t delete the original page until it is safe to do so which which we will see shortly shadow paging is  um  quite efficient in terms in the sense that there is no need to perform multiple write operations in in in in log based recovery you need to first do a write ahead logging and then write on to the data base system so so so the there is a need to perform multiple write operations for for every given multiple update operations for every update operation that is involved but in shadow paging there is no need to do that however shadow paging suffers from fragmentation of pages over  um  an extended period of time and usually we need to de fragment the  um  set of data base pages by an off line operation after a  um  a number of recovery techniques have been executed  37  54  so  um  how does the shadow paging technique work here is a small illustration there  um  we we usually keep a page table that is each page has a particular address  um  where each page can correspond to a set of tuples for for a given table for example ok and each page is given a particular address and there two different point of for each page address what is called as the current page pointer and the shadow page pointer initially we just have the current page pointer here and which is pointing to a particular page now let us say  um  some tuple in this page has to be modified so so there is a modification request for this page now we see that this page has no shadow page ok so therefore what you do is we create a copy of this page ok and  um  and point the current page pointer to the copy the original will will now become the shadow that that is the shadow of the original page that that existed and we are perform the modifications whatever update operations that we need to do will be done on this  um  page here now suppose somewhere down the line when we are using this this page there is a system crash now when is a system crash all that we need to do is to discard this page and go back to the shadow original page and then change that change the current page pointer to the shadow pointer here and said the shadow pointer to null so we just discard whatever was there and we have gone back to a consistent state now you might be wondering at what time are we going to start  um  how do we know that this consistent that is how do we know at what time do we start creating a shadow page  39  41  essentially shadow pages are updated with with current pages at every check point that is whenever there is any check point or check pointing operation not just crash recovery operation we perform this following operation that we we perform the following set of activities that is we we replace the current page with a shadow page make the shadow page pointer to null and the current page pointer to the previous shadow page pointer and  um  so so at every check point we are flushing  um  the the shadow page or we are updating the shadow page with the new current page and every first modification after a check point requires creation of a new current page and  um  and as this kind of evident here we we  um  the the check pointing operation that is that has to be used here should be quiescent mode check pointing operation that is we we need to suspend a data base activities for for performing such a update operation that is  um  replacing the shadow page with the new consistent current page  40  46  there also other issues during  um  that that we have contain with for crash recovery what happens that is  um  the the first issue the main issue is what happens if there is crash during the recovery process itself that is we are there is a system crash and we are trying to recover the data base system and then there is again a crash ok now  um  in order to be able to handle such issues that is so so that crashes during recovery do not affect the  um  affect the data base we need to be or we need to ensure that the undo and redo operations are idempotent remember even in the previous session we had used this word idem potent idempotent essentially means that no matter how many times you perform the operation the net effect would be as though you have performed the operation only once  um  and  um  and it doesn t matter how many times you perform this operation that is let us say  um  copying an element a to element b is idempotent no matter how many times you copy it it is equivalent to saying that we have copied it only once but  um  but there is a problem with idempotent operations as well that is if you have to make undo and redo operations idempotent then we have to ensure that multiple transactions or multiple  um  active transactions do not access the same element at the same that is one transaction has to wait until previous transaction has committed before it is for it to access a data element and  um  that actually impedes performance in terms of performance and of course  um  in in shadow paging technique there is other problem which which occurs when there is a crash during recovery especially when we are manipulating pointers that is suppose you have discarded the new current page and we are changing the pointers to the  um  to the old shadow page and then there is a system crash it may so happen that all pointers are lost and and and the page is just has just become garbage that is there is no way to access the page and and and figure  um  basically place it in the in the context of the larger data base  43  14  until we have essentially talked about system crashes or transaction failures where the the fundamental  um  criteria was that we can lose or we may lose data in the main memory but not in the persistent storage what happens if there are media failures that is there is a disk crash let us say  um  or development of bad sectors in this case we can not even rely upon the the data to be safe in the persistent storage as well that is even the data are there in the persistent storage are gone fortunately media failures are much more  disk crashes happen far more in frequent fashion than let us power offer  um  or operating system hanging and so on  44  06  and  um   crashes what are called as archiving or taking backups  um  and i am sure you would have  um  you you know the concept of taking backups its simply taking a copy of a entire set of data base on to another  um  off line media which which is stored physically in a different location and  um  however with when you are talking about large data base systems again when we talk about data base system always imagine a large data base system because most of the problems of data base management systems come from their size and not from anything else so so so imagine a large data base system like a like a railway reservation or a bank or whatever now the problem is  itself takes a huge amount of time suppose i have one giga byte of data  um  in my in my data base which is quite common in fact it could sometimes we we even have more than one giga byte of data we we have several giga bytes of data in in any operational data base and possibly even tera bytes of data now copying them to to a separate let us say take media or or optical storage takes huge amount of time in the order of hours or possibly sometimes even days to to copy the entire set of data elements onto backup storage  um  and there fore there is a need to perform back up operations in an online fashion that is as in when data base activities are going on we can not obviously suspend all railway reservations let us say for one day we can not say tomorrow there is you can not reserve any train tickets and so on it has to be on everyday twenty four hours a day or seven days a week so so everyday  um  every instance of time the the  um  reservation activities are are going on  um  you can  um  book train tickets over the net and you can book them over the counter so on and so forth  46  14  so there is a need to perform online backup operations so let us briefly see what what is the what is the basic idea behind online operations and which which can help us appreciate the the need and the complexities involved in a online backup operations before se do that we need to  um  we need to  um  take up certain definitions we distinguish between different levels of archivals so what to call as level zero archivals and level i archival level zero archival or a  um  what is called as a full dump is an entire archival or a copy of the entire data base from  um  onto the archival storage and a level incremental dump is essentially copying of only the changes  um  that the data base has under gone before the  um  rather after the last archival that is only changes since the last i minus one level i minus one are archived  47  25  now the the basic idea behind online archival is shown in this figure  um  between main memory and disk there is this check pointing operation as you you might have noticed the similarity between archival and check pointing check pointing requires that the dbms system to be brought in to a quiescent state an archival requires a suspension of all data base activities  um  in order to be in order to copy them onto the archive so check pointing  um  gets data from memory onto disk and whenever we need to recover we recover based on logs and the the archival process gets data from disk onto archive and because it is unrealistic to say that we suspend all dbms activities during archival we usually allow that the dbms to keep updating its data base operations as in when the archival process is going on however in addition to the data base we also store the log  um  from the from the previous check point until the previous check point  um  in addition to the  um  data base onto the archive therefore whenever they need to recover we need to recover from archive plus log to to bring back the data base onto the last consistent state before a media crash and the data base is being modified as in when the dump operation is running  48  47  so a simple algorithm for online archival is given here  um  we first begin by writing a begin dump  um  record to the log that is starting from the logging operation itself they they begin the archival process we then perform a check point usually quiescent check point but not necessarily and then we flush the log ok that is we we have begin the dump and began the check point that is we have we have flushed all  um  committed transactions onto the data base and we have a and we have a log of all the active transactions here then perform the dump ok full or incremental or whatever whatever kind of dump from the disk on to the archival data now once all data transferred onto archive including the check point that is there in the log then enter end dump in the log  49  47  so  um  what is the advantage the advantage is shown in this slide that is during recovery that is then we have to restore the data base now the the recovery from media crashes is called restore ra rather than recovery that is we are restoring the data base from the archive so when we restore the data base from the archive we find the most recent full dump and reconstruct the archive based on all the incremental dumps ok and then  um  we write it back on to the data base and then we take the data base log that is the surviving log entry that that was also archived in the  um  archives in in the archival storage and there we have a check point  um  which which shows what all what all has been done  um  to the data base and whatever was there after the check point we start redoing  um  redoing those data base that is whatever kind of  um  logging method is used that is suppose we have an undo log logs then we we perform the corresponding undo operations and suppose there were it was a redo log we perform the corresponding redo operations depending upon lets say it s a deferred updates or an immediate kind of logging operations  51  00  so  um  that  um  kinds of summarize there are number of other  um  other  um  topics in data base recovery which we have not touched upon esp especially perhaps most significantly the kind of recovery techniques that are used in many of the commercial dbms systems most of the commercial dbms systems today use a combination of undo and redo logging operation that is  um  this is to get the right trade of between performance and recovery over rates  um  because in in a pure redo operations you you have performance overhead in the sense that there is a huge spurt of activity during commits for for every transaction  um  the therefore they use a combination of these undo and redo operations and there there is a kind of a famous  um  series of protocols called aries a r i e s which which can possibly  um  search on on the internet which was proposed it ibm un verdant research centre which was kind of  um  used  um  quite which is kind of very popular and used in several different commercial data base systems  52  23  we have not covered those covered the details of the aries protocols here for the reasons for the reasons of brevity but most of that concepts used in aries depend upon or or based upon  um  several of the concepts that we have studied here like  um  deferred updates or immediate updates and check pointing and  um  undo and redo operations and idempotent operations and and so on and so forth so let us briefly summarize what we studied in data base recovery  um  in the under the topic of data base recovery we looked at we looked into the idea of transaction states and what are the different states that that a particular transaction can exist and the acid criteria for transactions which which determines  um  how or which determines how our recovery protocol should run and we looked at different kinds of failures like say system crashes media crashes transaction failures and so on and for the most frequent kind of failures like say system crashes or transaction failures we saw three different kinds of  um  recovery mechanisms rite that is no undo slash redo operations that is deferred update  um  recovery and undo slash redo which is immediate update and there is undo slash no redo operations which are based on compensating log records or clr s we also saw the notion of shadow paging where you don t have the need for a log therefore its its much more faster the the the recovery mechanisms or the dbms itself is much more faster because there is no there no multiple write operation  um  overheads that are involved in during every update but it suffers from a possible fragmentation that is  um  so many pages created  um  at different places in the disk or possibly when a suppose there is system crash during recovery itself there is a possibility of encountering garbage system pages as part of the disk itself so so so shadow paging has its own advantages and disadvantages with respect to write ahead logging we also saw the notion of archiving or the the idea of taking online backups for handling media failures where where we take a incremental backup and we recover or we we restore from from backup based on not just the archives but based also on the log that that that brings us to the end of the session ./ lecture # 30 hello and welcome in today s session we are going to be looking at a slightly different topic from the conventional idea of data bases and such a change in topic occurs simply because the kind of users that that the that the data base is going to be  um  is is meant for is going to change and until now we have a kind we have had a implicit assumptions that the users of who are going to be using the database are  um  in some sense if i can put it this way or in some sense clerical users in the sense that they are most interested in adding and retrieving data elements into the data base as efficiently as possible that means the users were using this data base are involved in the operational aspects of the larger system for example if you are thinking off let us say railway reservation we are talking about how best can we design a database system for reserving a ticket ok therefore what what it means is whenever a new ticket entry is made it has to be efficiently entered into the data base and suppose if there is any modifications for for a given ticket entry it has to be efficiently modified that means it should be able to efficiently search the the given entry and make modifications at at just one place if possible and not not many more places and keep that overall consistency of the database in time ok and that is what meant by operational  um  aspects of a database system or operational aspects of a system it s a day today operations somebody comes you you  um  gives gives a request for a reservation you enter the request you  um  reserve a seat for him and give him the ticket and or cancel it something or ask for concessions or whatever and so on  but there are other kinds of users who use the data base system as well and specifically we are talking about  um  users who take strategic decisions ok in addition to or in contrast to the tactical decisions that are taken by the operational people that is the the the  um  the folk sitting in front of a let us say ticket counter take very tactical decisions how efficiently can you perform your operations but then there are a variety of strategic decisions which which is the best way which is the best way which is the best location for me to  um  place my next railway reservation counter which part of the city has the most people by traveling  um  by trains or which part of the city has the most people traveling by first class ac or which part of which time of year is the best for me to offer concessions on second class sleeper something like that ok so these kinds of strategic decisions are of a qualitatively of a different nature than the tactical decisions that are taken for operational issues of any system and but but if you if you watch closely it is the same  or fed and retrieved in in operational situations is the same data that that is required for making strategic decisions as well if you have to make strategic decisions about let us say which is the best location for me to open my next open the next reservation  for that we need a lot of information about  um  what is the  um  from what addresses are people coming and booking tickets where rite if somebody comes from area a and books tickets in in area b and assuming that area b is the nearest reservation counter and  um  and the address information will will in turn show me that because there are lot of such people coming from here to there it probably makes more sense for me to open a next counter here and so on ok so we will see in  um  the next few sessions how  um  how the whole aspect of data base design changes when the usage scenario changes from an operational  um  data usage to a strategic decision making usage so that is that brings us to the topic of data warehousing data warehousing as you  um  as you might understand the term warehousing a warehouse is a  um  where you where you keep your inventory stocks rite that is where where you have  um  stocks from several different sources are going to several different sources and you  um  and you are  um  essentially talking about a large number of stocks that that are maintained in the warehouse and a warehouse is typically of strategic importance if you take us let us say some kind of a civil engineering project the location of your warehouse for example  um  is is of prime importance depending upon because there are  um  which depends upon several factors like say what is the cost of  um  transportation logistics coordination and so on suppose you are you are having you are handling a large infrastructure project some where lets say building a flyover something like that  um  having the location of your warehouse in a in a way that is easily accessible with with possible cost constraints is is probably one of the most crucial strategic decisions that can be taken in an analogous sense a data warehouse is is a warehouse of data elements that are been captured from different operational data sources so that you you can so that this this whole set of data elements becomes or  um  is of strategic importance you can you can take strategic decisions  um  based on the data elements that you have gathered and which leads us to the the next term in the slide here which is called the olap or online analytical processing that means we are we are looking for  um  requests or queries that are of an analytical nature rather than an operational nature or or what you call as a transactional nature we have talked about transactions quite a bit now and we are now looking into analytical queries where  um  where queries in turn help in strategic decision   7  49   by itself is a vast subject and that several  um  research papers that have been written in several commercial implementations that are  um  that have been implemented and  um  the huge amount of interest on in data warehouses however in this course we shall be we shall probably be touching  um  just a just a small part of this vast ocean of data warehouses and the way this  um  we have  um  partitioned this this our exploration into data warehouses are in three part as shown in the slide here  um  in the in the we shall be looking into  um  an important difference between olap that is online analytical processing versus the online transaction processing that that that traditional data bases are geared towards  in data warehousing namely that of data cleaning and data integration the next two parts deal with the deal with the warehouse core itself where we are talking about the data models that that go inside the data warehouse and what are some kinds of some of the thumb rules which go towards data warehouse design we will also look at some kind of index structures for for data warehouse based on the data models that that we studied and see that how they differ from  data bases like say b plus trees or b trees or  um  hash based indexes or something like that  9  25  when we are talking about data we can essentially divide data into two kinds of data  um  essentially i have called them as  um  operational data and the next slide shows  um  calls it as historical data that is operational data is  um  is can also be considered as data that works within quotes that is data that is involved in the operation of a particular system for example if you are if you want to withdraw money from your account you need data about you account your your your account number your pin number if you are using an atm  um  your account balance and so on all these data elements are all operational data elements because they are crucial they are necessary for performing the operation of withdrawal of money rite and what kind what are the characteristics of operational data operational data are subjected to frequent updates and queries you could be withdrawing money almost everyday or some kind of operations would be happening on your account almost on a daily basis and there should there would be some kind of queries or updates happening quite frequently to your to the database that that is maintaining your  um  account information therefore we saw in the sessions on normalizations and functional dependencies  um  we saw that in order to make the process of updates and queries efficient we need to normalize  um  the the the set of data elements  um  that is we need to normalize what is to prevent any kind of redundancies occurring in the in the data and also to prevent any kind of update anomalies suppose a update  um  records of  um  in in some place and there are redundant copies of the the the same record of of some fields then i will have to update all occurrences of this field so in order to minimize update anomalies we essentially normalize the set of data elements and  um  a set of tables that are normalized are are essentially fragmented because  um  given data elements let us say  um  your your account information  um  may require several information something like your account number name address branch location balance transaction history  um  and type of account so on and so forth and because they are normalized the set of all tables are fragmented in different phrases and the and the operational data is usually of local relevance  um  which is  um  which is kind of emphasized in the  um  which should kind of emphasized that means it is very unlikely to  um  to expect let us say let us say that you you have opened an account in  um  chennai and you go to delhi and you want to access your account its unlikely to expect the account information that you have opened in your bank in chennai to be also be present in delhi it has to be queried in chennai and your request has to be routed to through chennai and it has to be queried here and the result sent back to delhi so  um  operational data is usually of local relevance  um  wherever the data is its relevant there in in the geographic location and the kind of queries over operational data are also what are called as point queries what is a point query a point query is something like  um  asking something about  um  asking query about some individual tuple for example what is the balance in your account with account number so and so or where  um  what was the last what was the last set of ten transactions that your account had therefore  um  what what is happening here is that there is there is a particular point or a specific tuple that is the key where your account number in in this case where  um  which is being used to access  um  all relevant information for the operationalization  13  38  on the other hand there are  um  you can think of an other kind of data called as historical data or also called as archival data that that is data which are archived over a long period of time over a long period of  um  different operational  um  data sets its a it s a it s a long set of operational data sets and it is these kinds of data which tell us something that is its its also called as data that tells that tell us something about the the overall trends of operationalization that that is happening that is suppose i have a data collected over from all  um  railway reservation centers for the past ten years it can tell us about trends as to which are which are the peak times in which people travel  um  what kind of  um  what kind of people travel in what kind of  um  or make what kinds of railway reservations and so on i mean people having what kind of  um  salary range travel in what kind of  um  classes sleeper classes ac pr whatever and so on so its its that data that tells something about the larger system which is using the data set and as you can see here the the  um  kind of updates that that this data sets undergoes is quite infrequent in nature its not like everyday that that you are going to get historical data you have to get historical data over a period of time therefore you collect historical data let us say once in six months or once in a year or  um  something of that order so its its it s a very infrequent updates but what is what is more important here is that updates are not all that important that is they they happen  um  very infrequently so so we can take care of them when whenever they happen but it is the queries that are more important that is  um  analytical queries requires huge amounts of aggregation what is the average age of  um  the person traveling in second class sleeper if you if you want to calculate that it needs a huge amounts of aggregation over a large data set  um  based on all different  um  second class sleeper tickets that that have been sold rite and this is an integrated data set with a global relevance we are we are not  um  it usually it doesn t make sense to look at trends for trends in a given operational data source it doesn t make sense to say  um  what is the trend in this particular reservation center i mean its it usually would would be very small compared to the larger set of all different operational data sources and the performance issues in  um  managing historical data occurs mainly in query times and not in update times  um  because of course updates are far  um  infrequent or far more infrequent than queries and queries need to access a large amount of data and you have to perform a number of aggregate operations in a before being able to return the query and they have to return it in in in an online fashion that s why its called online analytical processing you have to return it in a  um  return query results in a  um  interactive response time that is the the the the user should should be sitting in front of a terminal and the interactive response time is usually of of the order of maximum a few seconds and you you cant expect the user to be waiting for  um  a large period of time after  um  giving the query  17  22  let us look at some examples of  um  a transactional data and historical data a transactional data or operational data are are those kinds of data limits that are handled by  um  systems that are called as oltp systems or online  um  online transactional processing systems that is  um  these systems which  um  which maintain transactions and handle different transactional activities in the system now some exam what are some examples of operational queries  um  something like say what is the salary of mr.misra about so it is some  um  it s a it s a point query as you can see here that is you you find out the employee number of mr.misra and then get the salary field and that s it or who is the what is the address and phone number of the person in charge of the supplies department again it is a point query that is  um  its its its just a question of following different references find the supplies department and look up the set of managers  um  given the suppliers department id look up who is the manager managers id and then given the managers id find the address and phone number of  um  of the manager and so on ok so so these are some kind of typical analytical produce  um  typical transactional queries which we have seen  um  quite often now that is we have  um  we have seen how to specify this queries in sql how to optimize this queries how to create transactions around them and and so on so forth  18  58   queries given this slide how is the employee attrition scene changing over the years across the company as you can see this this kind of query is qualitatively different its qualitatively  um  in in in a different caliber than the queries which which we saw in the previous slide  um  if you want to answer this query let us say how is the employee attrition scene changing over the years and across the company its its not a question of  um  accessing one particular record or one particular tuple you need to you need to look at  um  employee attrition information across the company and across the years and then look for trends saying ok it is increasing it is decreasing and so on that is you have to you have to find out some aggregate employee attrition information across the company and then plot it  um  against the years and see how it is changing or something like is it financially viable to continue our manufacturing unit in tai taiwan it s a it s a it s a pretty vague query rite what do we mean by financial viability  um  what do we how do we calculate financial viability profits against cost and so on so how do we know that we are we are incurring more profits than than costs or we are gaining more profits than the cost that we are incurring its its not its not quite easy to find that out that is you need to be able to aggregate a number of different information sources and then and then say ok  um  profits are generally more than the cost and so on rite so these are the kinds of queries which are of analytical nature and typically  um  for interest for strategic decision making  um  managers who who perform strategic decision making  20  54  a data warehouse is an infrastructure to manage such kinds of historical data and it is designed to support olap queries involving  um   um  involving very gratuitous use aggregations aggregated queries and so on and its not just a queries there is there is also a number of post retrieval processing also called as reporting which is just as complex or possibly more complex than the query retrieval itself that is i have i have gathered huge amount of information about all possible profits and costs from all possible centers and so on how do i project this information how do i give give out all these information to the to the decision maker which is which is again a quite  um  involved in itself  21  44  so this slide shows the the schematic diagram of of the oral architecture of a data warehouse a data warehouse as we saw in the previous slide is meant for managing historical data and  um  where do you get the historical data from or how do you first of all  um  come out with a historical data historical data you you essentially obtain from all the operational data sources or the oltp units rite so you have several oltp units for your organization when whenever your thinking about an organization here think of a large organization something like say the the  um  life insurance corporation of india or the indian railways or something like that where where they have a number of different units each of them having their own databases each of them handling their own accounts each of them handling their own reservations and  um  transactions and so on and  um  from each of these oltp units we obtain huge number of operational data elements everyday rite everyday there are  um  number of people traveling in trains  um  and from number of different and and they would have booked their tickets from from number of different sources now all of these data operational data are then subjected to a data cleaning and integration process we shall see shortly what is meant by data cleaning and integration  um  there are huge number of  um  possible inconsistencies or possible sources of what we call as dirty data that can exist in the oltp sources and we will see what are these sources of dirty data that can exist and how we can clean  um  what is also called as data scrubbing and so on before  um  we are able to present it as historical data now once operational data is subjected to data cleaning and are integrated across all of these oltp sources we present them as historical data into a data warehouse that is a data warehouse would  um  already exist and you you just integrate this set of data into the data warehouse you you you  um  just update the data warehouse with these set of data  24  04  there is also a notion of data marts which  um  is probably of interest  um  here at atleast to look at the definition whenever we talk about data warehouses we look at then as a collection of data marts  um  that is a data mart is a historical data about one specific kind of  um  one specific kind of segment or one specific oltp segment suppose you are  um  considering let us say again take the example of indian railways ok  um  let us say  um  we we have one segment called express train reservations ok that is one that is one data mart that is suppose we are interested in strategic decision making only about express trains or may be let us rajdhani express or something like that all the  um  super fast trains ok so all of the historical data that we gathered about rajdhani express would would go into that data mart called rajdhani express that is of of all rajdhani expresses across  um  across the  um  railways and over a period of time and so on so and the data warehouse is usually seen as the collection of this different data marts that feeds into the  um  data warehouse and data warehouse data marts are also seen as small warehouses that  um  where you can in in some way  um  you can support all activities that you support on a data warehouse also on a data mart and but these are within a given segment  25  45  now let us expand this system or this box and data cleaning and integration and see what happens inside as we  um  as we earlier mentioned  um  we obtain data from different oltp data sources operational data sources that feed into the data warehouse now  um  can we or what kind of  we are able to  um  before we are able to  um  integrate operational data sources or before we are able to populate the data warehouse from the operational data sources operational data sources as we have seen are mainly meant for the data that works that is  um  as long as that the as as for as as long as the data is sufficient for the operation to for the operation to be  um  performed it is good enough it is it is  um  characteristic of operational data is that operational data is of local relevance its its usually relevant that is data in a in an operational data sources is relevant only locally however when we are talking about historical data in a data warehouse there are two different things that that that you have to  um  is  um  one is that because we are using different kinds of  um  data sources we should have a uniform standard of data representation and semantics across all of these different data sources and secondly we should remove any kind of duplicate information that are that are present in these data sources suppose you have  um  suppose you have open suppose you have opened an account let us say let us again take the the the running example of the indian railways ok now suppose you have opened an account for booking tickets over the web ok now you have booked a few tickets over the web and then  um  you have not booked tickets for a quite quite a significant period of time  um  in which time you forgot your user name and password and you forgot that you you had ever opened an account here and then you go back and open another account for  um  for buying tickets over the web now as you can see if if these two accounts the the behavior in these two accounts are considered different are considered to be from different users then it will give us erroneous information or erroneous aggregate information for  um  for strategic decision making it is important that it is important for the  um  for historical data that these two accounts should be clubbed or we should recognize that these two accounts belong to the same user and then club them together whenever we are performing any kind of aggregate queries so let us see what what what does it entails  um  to to to do all these what are the kinds of complexities that that that we encounter in performing these kinds of data integration and  um  cleaning and so on and so forth the the general model of data integration and cleaning is shown in the slide here these are the different oltp data sources which feed into the  um  data cleaning and integration unit and usually the we have opened the box which was a which was a black box here now we have opened the box and you see that there are usually two different units here the the data cleaning unit and the data integration unit and  um  the the process by which data flows through this unit  um  is not a unidirectional process that is  um  you  um  you garner a data from this oltp data sources pass them through data cleaning pass them through data integration and also perform some kind of a back flushing that is  um   that you have back into the oltp data bases itself we will we will look at some examples which which  um  which tell us what that what it means actually and  um  once the data is integrated it is  um  fed into the data warehouse and  um  based on queries in the data warehouse or updates in the data warehouse you get a feed back from the warehouse itself which tells how you we said that you should perform your cleaning what is important for the data warehouse cleaning essentially means that  um  we have to change whatever is important for oltp to  um  we have to change representations from whatever is important for oltp to representations where whatever is important for the data warehouse  30  54  so let us look at data cleaning or or the dcu in a in a little bit more detail as the name suggests data cleaning  um  performs a cleaning operations on data sets that is data sets that are that contain dirty data what is meant by dirty data that is what are the different sources of  um  dirt  um  i am talking about dirt within quotes so that that one can encounter for given operational data source there are several different possible  um  possible  um  sources of dirt that requires  um  cleaning to be performed the the first kind of  um  the the first kind of dirt is what is called as lack of standardization for example  um  because you have different branches different reservation centers or different cities there could be multiple encodings one of them could be using ascii base systems one of them could be using unicode or one of them could be using edsidec or something or the other so so there could be multiple encodings multiple locales multiple languages  um  in which data is represented and you should be able to   onto standard  um  standard encoding or language or whatever there could be spurious abbreviations somewhere somebody writes mahatma gandhi road in in part of an address and somebody else writes m.g road now we need to be able to  um  we need to be able to recognize that m.g road is same as the mahatma gandhi road so when we are talking about when we are answering a query like how many  um  how many people  um  comes from in and around mahatma gandhi road or live in and around mahatma gandhi road who buy tickets we should be able to say that we should be able to search both m.g road and mahatma gandhi road further because they are the same and and similarly there could be semantic equivalence m.g road and mahatma gandhi road probably you can you can write an intelligent algorithm that will see  um  whether mahatma gandhi road is an expansion of m.g road and so on and you could kind a well fairly find out ok this is a spurious abbreviations and so on however what if there are semantic equivalence chennai and madras some people may use madras for  um  for the city name and some people may use chennai and because we are talking about historical data note that which is important here that is  um  at one point in time officially chennai was called as madras so even in official documents you would be using the name madras but then again it would it would have become chennai now now you should be able to query later or or say that these two are the same and  um  knowing though intelligent algorithm can do that you need extra knowledge in addition to the the the data sources you need knowledge about the overall environment the governmental policies the standards and so on in order to be able to identify this this duplications similarly there could be multiple standards there could be one data source which could be using the metric systems while somebody else could be using miles and feet and so on and so forth so so somebody might in some one data source might have said one point six  um  well one point six kilometers is the same as one one mile so  um  so one point six kilometers and somebody else might say one mile and so on  34  30  there could be other sources of dirty data like missing spurious or duplicate data for example  um  the age field of an employee could be missing and which will hamper my  um  query of what is the average age of an employee or something like that what is the avera what is the correlation between the age of an employee and and the attrition  um  and the probability of attrition and so on and so forth ok or there could be incorrectly entered sales values or incorrectly or some some typographical errors and so on or or there could be duplication of data sets  um  and duplication appearing in different forms for example take look at the last example  um  a person called l bala sudar  um  would would have registered in one  um  would have bought a ticket in one reservation center sighting his name as l bala sudar the same person could have gone to a different reservation center and bough another ticket at some other point in time sighting his name as b l sundar and  um  with the same address and the same phone numbers and so on and so forth and we should be able to detect such  um  such kind of semantic duplication that is  um  thee is duplicate data that is occurring in a different data sources but not in the same form that is  um  the the name has changed or or something has changed but semantically they are they are still duplicate  35  56  there could be other sources of dirty data like inconsistencies incorrect or in consistent use of codes let us say in one of  um  let us say in one reservation center  um  the the gender of the person who is reserving a ticket is maintained as a character either m or f or and in other reservation it is maintained as zero or one its just the different standards that that each each data base designer has designed for each center and this is especially true if this has evolved over time let us say  um  reservation center number one would be using historical data base that was designed which is not changed because of cost considerations and  um  the the new reservation centers using a new data base system that is that is been redesigned which which is termed to be more efficient and so on and so forth but which uses different codes for for the same data element and there could be codes with some outdated meaning let us say third class sleeper  um  because we are talking about historical data we we  um  it is its quite its quite  um  possible that we encounter some data about third class sleepers which which no longer exists in  um  in railways today and so on and there could be inconsistent duplicate data that is there could be two data sets that are found to belong to the same persons we have reasons to believe that it s the same person but he has given two different addresses in these two sources and so on how do we detect such things and other kinds of inconsistencies like inconsistent associations that is let us say  um  one department provides particular kinds of  um  says that says that the sales have been so on so on so on but it does not add up to the total sales figure that that is also been provided so  um  associations across different data sources may not may be inconsistent  um  and there could be semantic inconsistencies somebody might have typed february thirty-first in in a date  um  so so what is that referred to is it february thirty-first or march first or march thirty-first or what or  um  integrative violations in referential integrity like referential inconsistency there is let us ay ten lakh rupees sales reported from a unit that has actually closed down which is which is not even there and so on so several kinds of these  um  sources of dirty data crop up in practice and perhaps cleaning or data cleaning is perhaps the single biggest  um  research problem that is still kind of open in  um  in the in the field of data warehouses because there is no single thumb rule for  um  data cleaning we we cant just say all data is pass through one kind of data cleaner and it will be cleaned because there is several sources of  um  dirty data that that can occur and there is no single way of  um  of cleaning them  39  10  so what are the issues in data cleaning it  um  automated its quite difficult to to to automate the  um  for the for the computer to learn by itself that let us say chennai was called madras or burma was called myanmar and so on and so forth and  um  and the thing is we don t even know whether the the the data cleaner is is performing correctly which is which is what is called as gigo or garbage in or garbage out suppose you give some garbage and we don t have correct rules we do we just give some junk data and it just gives some junk outputs we don t know whether it has actually cleaned or whether the the output is correct or not and its its quite hard to verify the the the cleanliness of the output data and data cleaning requires considerable knowledge that is tacit for example one kilometer or or one point six kilometer is equal to one mile which which we know but its kind of a tacit knowledge its not exquisite  um  knowledge and which goes beyond the purview of the database like  um  chennai was called madras so so you should you should know data about governmental policies you should know data about geography should know some rather some knowledge about geography governmental policies metrics and so on and so forth which has to go into the data cleaning unit so its its not possible to design a data cleaning unit  um  in a way which which kind of paraphrase does one size fits all or  um  which says that  um  this is the data cleaning unit and this will work for any kind of data in any context what so ever and so on and the data cleaning complexity increases as we increase the  um  number of data sources or they increase the history span  um  that that we have taken up for cleaning  41  10  what are the some of the steps in data cleaning how how does typical data process looks like essentially there are five five different steps that that go into a data cleaning process firstly we will start with data analysis given the set of oltp data sources we start by analyzing them and look for certain kind of meta data that is  um  what can we learn about the data that we have and then give all those meta data back to the user or the data cleaning ok now the user in turn will now specify a set of transformation rules that is if this is the kind of dirty data this the this how we have to transform it into clean data and so on so  um  so so so the user specifies the set of transformation rules that that are performed either the scheme or data level which transforms dirty data into clean data we then verify the rule by by running them on test test data sets that is some kind of sample data sets and then we incorporate the transformation rules into the data cleaner and then perform the data cleaning process so once the data cleaning process is performed we usually also perform what is called as a back flow that is we repopulate the data sources with cleaned data for example  um  suppose we have seen that chennai and madras are used interchangeably in in the in the oltp data source we see to it that or we perform a back flow so that its all either chennai or madras in the in the in the  um  in the oltp sources that is theer is some kind of standard  um  in the in the oltp sources itself which is called as back flow of of data  42  55   um  let we we shall not be looking into great detail into how each of these transformation rules are performed but let us have a look at some  um  specific or some typical examples of how  um  dirty data can be cleaned for example how do we search for illegal values within what is the strategy for checking for illegal values we could use some kind of a mix min or mean deviation or cardinality criteria that is slide a window through your data source that contains a max min  um  limit for your  um  for the particular data value whenever  um  a given data value is lesser than the mean or greater than the max you know that it s a illegal value so similarly mean and deviation or or cardinality and so on and so forth and spelling mistakes how how would you  um  how would you look for spelling mistakes  um  there is there is  um  there is some techniques what are called what are called as n gram outliers an n gram is essentially  um  a collection or a sequence of n diff n  um  letters that form a different words for example suppose i take a three gram let us say hashing i i i perform a three gram transformation on hashing then i get several different three grams has is a three gram ash is a three gram and shi is a three gram and hin is a three gram and so on so i get different three grams then what we do is we cluster all these three grams based on their occurrences and we see if  layers in the clusters that is are there any n grams which which stand alone without belonging to any cluster  first slightly these  um  outliers are going to be spelling mistakes and that s that s the strategy that s that s generally used for  um  checking spelling mistakes similarly lack of standards that is compare values sets  um  from a given column across different tables and see whether they they are using the same standard or so on and  um  duplicate and missing values that is compare the number of rows with the cardinality of of this particular column so so you find out if the number rows does not match the cardinality  um  you see that there could be some kind of missing values or  um  null values say in this  45  31  ok so let us look at one or two algorithms in slightly more detail which can give you an appreciation of the kind of  um  the complexity that it takes or or the kind of techniques that that are used for data training  um  simple algorithm for duplicate elimination what is called as the hash merge algorithm if you remember the storage structures session that that we that we covered  um  you know what is meant by hashing rite that is  um  given a particular tuple run it through a hashing function so that it it is mapped on to a given bucket ok now what can we say about hashing  um  across different tuples duplicate tuples that is tuples with duplicate ok therefore elimination of duplicates now reduces to searching within a bucket for looking at any duplicate values and then eliminating them  46  36  rite so here is an illustration of the hash merge algorithm let us say there are  um  four different records like this and this is the hash key that is the name and address of the person that is we want to eliminate duplicates as for as person information goes so as you can see k j amit and fifty lvl road  um  both of them map onto the same bucket and these map onto different buckets and so on so  um  so so so you just compare within a bucket and then eliminate duplicates  47  07  there is another technique called sorted neighborhood techniques this is  um  mainly used for misspelling  um  or detecting misspelling in in in data sets and sorted neighborhood techniques is  um  is given by the following algorithm shown in this  um  slide here  um  first of all identify what is the key as in the previous case we identified that name and address is the key ok so identify what is the key within a given tuple ok then sort the table with  the the table based based on the key  um  you can see that  um  all duplicates will  um  cluster together rite similarly all misspellings theer is a high likelihood its not that there will be but theer is a high likelihood that all misspellings will also be clustered together that is  um  will will be quite close to the actual spelling that that has to exist rite that is for example if  um  k.j amit become let us say k.j aman or something like that a m a n so  um  it its aman is still quite close to amit that is  um  assuming that we are we are sorting  um  tens of thousands of records  um  misspellings would would be quite close to the real spelling then you slide a window that is take a set of  um  take a set of n rows and keeps sliding it through the through the  um  through the data base to see if there are any misspellings and then merge the misspellings and we have to make multiple passes until theer are no more merges of records  48  55  so  um  so so so here is an example again where this is used for duplicate elimination in addition to misspelling detection where  um  given this table here  um  it is sorted like this here and given a window of  um  size three we see that  um  we see that there are duplicates within this window and we start eliminating them  49  17  and the last algorithms that we are going to see for  um  duplicate elimination  um  and also kind of and also for misspelling detection and so on what is called as the graph based transitive closure for  um    um  this algorithm is more of  um  improvement over over the sorted neighborhood algorithm where it basically reduces the number of passes  um  the the idea behind the algorithm is based on the notion of transitivity  um  in an equivalence relation for example  um  if if let us say records r one and and r two  um  we we see that  um  we see that r one and r two are duplicates ok so and we establish a relationship that r one is a duplicate of r two and so on and then we find that r two is a duplicate of r three ok we can we can definitely infer that r one is a duplicate of r three as well rite because the duplicate  um  is duplicate of relationship is a transitive relationship if a is a duplicate of b and b is a duplicate of c the then a should be duplicate of c what this means is when sliding the window once we established  um  that a and b are duplicates theer is no need for us to compare a and c and so on ok so so that s the essential idea behind graph based  um  transitive closure  50  59  this slide here shows the schematic example where there are five different records and  um  in a naive sliding window  um  sorted neighborhood  um  protocol we start by comparing r one r two and r three then r two r three and r four and r three and r four r five we we are just sliding it with with a window size of three however when we encounter when we slide r one r two and r three suppose we see that r one r two and r three are duplicates ok so r one is a duplicate of r three and r two is a duplicate of r three and so on ok now there is no need to compare r one and r two with r four and r five its its its  um  it is sufficient if we just compare r three with with r four and r five so that that s the general idea behind  um  behind this algorithm which can reduce the number of passes in  um  eliminating duplicates  51  56  the next  um  topic that we come  um  in in the data cleaning and integration is  um  is the  um  issue of integration itself that is there are different oltp data sources from which we we are getting data most of each have different kinds of inconsistencies  um  sources of dirty data and so on and then we have to clean them and so on but after we have clean them we have to integrate them  um  under a under a common banner ok and this integration can  um  can occur at two different levels which are shown here as data integration and schema integration that is schema integration entails forming an integrated schematic structure  um  based on the the set of all desperate data sources that is you have data sources from counter a counter b counter  under one common schematic structure and data integration essentially entails cleaning and merging data from these different sources that is eliminating duplicates and  um  merging all of them under this schematic structure  53  10  let us briefly look at what what are the issues or what is the complexity in  um  in schema integration as well and its not as simple as it looks like consider the following schemata that that is shown here or theer there is a schema that that that let us say the the one of one of the oltp sources used which which is called which uses a table called cars ok and the cars table has different  um  different  um  fields like serial number model color stereo glass tint so on and so forth everything is in one  um  one table on the other hand another  um  another retail center let us say ok uses two different tables for the same information and look at how these tables could be different first of all there are two tables rather than just one table then the names are different  um  all these names are in german and these are in english so so cars cars are called autos and serial number are called as serien numeral or something color is called farbe and so on  um  so so so its not just the the structure is different the name of each of these fields could be different the the data types could be different and so on  54  27  there is several different challenges that that we have to  um  face when performing schema integration that is there are naming differences there are structural differences data type differences semantic differences  um  missing fields and so on and so forth  54  42  the generic architecture of schema integrator is shown in the slide here where  um  it basically consists of two  um  specific stages one is the the lower most stage is what is called a wrapper or an extractor that is  um  given a particular schema the extractor maps all given schema into a standard schema set  um  that is whether it is color or whether it is cars or autos or whatever it maps it onto a common schema set and then the mediator constructs the overall schema set  um  or or rather it looks like at the federation of different data sources having the same schema and then handles query based on  um  or constructs the the data warehouse or handles query or whatever and so on  55  35  so  um  the the the difference between a wrapper and an extractor  um  is that an extractor essentially physically extracts data crates a schema and physically extracts data from the data sources according to the schema where as the wrapper is just a logical wrapper similarly mediator is a logical mediator while a constructor physically constructs a data warehouses from all the extracted data sources  56  04  so theer are different tools for  um  data cleaning and integration and as i said theer is no there is no  um  common tool that can fit all possible requirements but there are several tools that can make the life easier for sink so so here are some examples d f power or eti star and ssa name and so on and so forth rite so  um  let us summarize what we studied in this  um  in this session we looked at the important differences between olap and oltp queries what are what is the difference between analytical queries and transactional queries what is the characteristics of  um  olap  of a data warehousing system where where you  um  take data from different oltp fields and  um  take them through data cleaning and integration  um  phases before  um  before populating the data warehouse and then of course there is a feed back and back flush and so on and this itself is a is a is a major issue with the machine so that brings us to the end of the session / transcription  shobana proof reading  vidya database management system prof d janakiram lecture # 1 introduction to database management system welcome to this course on introduction to database systems this is an introductory course um um usually taught at the first semester of  noise  third year under graduate course is also expected that you should have done um course on data structures file systems probably operating system to better appreciate the concepts covered in this course though the course material um may not assume completely that um um you have understood all the prerequisites it could also be done probably independently because the wherever the concepts are required they would have been covered and some difference to the background material would have been indicated in the appropriate sections it s a forty two lecture module of one hour duration um and expected to be covered in a semester long course what we will be doing is the two instructors in this course um when myself will be covering the transacting processing system at the later stage of the course and the other instructor dr srinath will be covering the initial sections on database design um er modeling and other basic concepts in the initial sections of the course what i am going to do in todays lecture is give a brief introduction to databases and show the importance of databases and also in the process introduce some of the basic concepts um that um we will be covering in much more depth as we go in to the course  noise  as the term databases indicates um ever since data has been digitized and we are able to store data in digital form um we see that um the mode of data that is being stored by corporates and other organizations as increased from a few kilobytes or a period of time to now terabytes now databases in that sense  noise  have become an integral part of our day today life in that um we actually um too lot of transactions our day today transactions whether it is railway reservation or it is an airline ticket reservation or withdrawal of money from a bank um in some sense we are actually working with the underlining databases that this organizations um have they actually store the costumer data and other information user information in the form of um in the form of databases in the in the form of databases where it is accessible by a number of other other entities so that our regular transactions like withdrawal of money from the bank or reserving a ticket for a train these are actually are um  noise  are business processes that are working with the underlining data that is stored in this organizations in that sense databases are an essential um thing in the in the business processing world and they become a key entity interms of developing these applications business process applications to show what kind of you know importance database have assumed in recent times i will just show you an example of a business transaction in how we actually work with these business transactions in our day today life i realized only yesterday that i actually have to pay a premium for one of my lic policies and in normal sense what we do is for paying this policy the lic actually sends us an reminder which is a paper copy posted through e through the postal mail and normally look at this remainder and then you will actually send somebody or you go to the lic um branch and try paying the premium across through your cheque or something bank cheque or other mechanism if you carry the cash you will use the cash to actually deposit and then take the receipt back you know all this requires that you physically now move from one place to the other place and sometimes stand in the queue if there are more people waiting to do this transactions and then the process you will really have to spend time in doing this transactions business transactions in your day today life but now with the digitization and the storing of this data into the digital form into the databases what you start realizing is that it is possible for you to actually access this data online and then and see that you have to pay a premium and then see you know if you know actually do this and get on to your bank  noise  you know and then pay the premium online through the bank and also get the receipt of this  inaudible  transaction back on your email in your email box which means the whole transaction can be completed sitting in your office without moving even an inch from your office and fully finish this entire business transaction  noise  and this is sometimes what we call as work flow you know to show how this work flow actually takes place let us see how the scenario which had explained earlier can be managed and i will show you online how this whole thing actually can be done you know so we will actually go to the lic s database so this is the lic s portal  refer slide time  00  08  46  which actually gives the user the ability to login to this lic portal  noise  now i basically  noise  try login in to this database ok so once i login  refer slide time  00  09  04  here i should be able to get all my data relating to what are the policies that i have with lic and what are the you know premiums that i paid earlier with lic and what are the due dates and if i actually want to calculate the lone information i should be able to do all this say for example you know you  inaudible  pay premium which have paid online so i can say that in this particular year i want to see what are the premium that i have actually paid which shows that  refer slide time  00  09  39  these are the premium that i have paid already online in this particular year so this database actually shows you know for example if you want to see the receipts you can see online so all this is being managed for you by the lic database so once you logged in and all those informations relating to you is available this becomes a one single point access for you and now we can say for example one of the transactions that i want to execute now is i actually want a i actually do a online premium payment  refer slide time  00  10  19  and once i want to do this online premium payment i have to choose the policies for which i want to pay the premium online and it suddenly gives me a lot of options here i can say this is the premium for which i want actually do it online  noise  so now what i will do is i will say submit so this should take me saying that  refer slide time  00  10  40  yes this the premium that i have to confirm which says that would you like to pay for this particular policy this much amount of premium online  noise  now if i say yes actually it will says you know i can only be paying the premium in my name and all that you know things which are indicated here ok and it also tells that i am going to get on to more secured way of paying this premium  refer slide time  00  11  05  ok this is the transaction i the automatically generated and it says how do we actually want to make this payment i can say since i have now the various ways in which these can be pay probably you can see that there are  noise  now the city bank debit card you know sbi let say that i have sbi account so i will now try to do this using the sbi net banking um  refer slide time  00  11  37  so this will take me to sbi database now it automatically pushing me to the um state bank of india s gateway payment gateway now this will put me into in to the um you know um database of um of state bank of india  refer slide time  00  12  03  now i will login into state bank of india 's database give my details and now say let me submit this which should say yes now it says i have logged into this ok  refer slide time  00  12  11  and it says now would you like to confirm paying this money to lic you say confirm  refer slide time  00  12  25  so it says um verify and confirm life insurance con transactions detail so it gives the client code it gives the indian rupees and date and you say service charge is zero and the branch is indian institute of technology chennai then i say confirm  refer slide time  00  12  44  and this should now it says that you know i have actually paid your payment request is being processed  refer slide time  00  12  50  so this will um basically underlying take into the sbi s database now i am gone to lic looked at the premium that i need to pay and i told this much premium i need to pay to lic and then i have gone to my sbi account now i said this much amount be given to lic for paying my premium and its automatically done  refer slide time  00  13  15  now you can see i come back to lic s portal and it say thank you for paying premium though the payment gate way where the site has been generated for the payment click here to view or print a confirmation mail with a copy of the receipt has been sent to the mail id provided at the time of registration so you can see that now it will give me the full registration the the receipt which is needed for me if i storing it for future requirements to store that i have paid this premium you know this is important because you know at a later point of time we need to say that yes we have paid this premium so we need this um this receipt which tells that this is the receipt  refer slide time  00  14  04  and interestingly if you basically look at it this receipt is electronically generated and is digitally signed so it it doesn t required any more signature of anybody else with the lic can in future conform that since it is digitally signed by lic it can be conform and saying that this receipt is generated by lic and you can see the whole details are available for me here and i can store for it my future reference this is what we mean by actually a business transaction and a work flow as you can see that i am moving from one database to the other database and in the process actually i am accessing data that is stored in this organizations databases and doing business transactions with them and please remember this is very important because these are business transactions at a later point of time the bank can not say that it has not paid this amount to lic and lic at the later point can not deny that i have not  noise  received this amount so there are lot of issues involved here like you know the properties of the transaction that we need to ensure saying that the costumer is protected again as double payment for example if he clicks twice you know the the he has to be protected saying that this transaction is actually carried out once and not twice and his money has has is available on the in his bank account is properly reflex this payment so  noise  as you can see this has become now an integral part this is one of the business transactions i could execute this online sitting right now here and you can imagine it can be done very late in the night when you realize that you have to pay this premium and with no intervention of the bank officials with the no intervention of the lic officials since this data is digitized and exposed you are able to actually as a customer access all these data and do this online um without any hassles and hence this become an integral part of our day today life and hence databases are an extremely important important subject for us interms of developing these applications and making them usable by um in our day today life by people you know lot of other examples also exists for example if you if you go to clear drip trip dot com you can similarly make payments and buy your tickets and do so many other things with respect to on a booking your reservations and things like that so in some sense this course is going to focus you and tell you the underlining examples or principles in developing these kind of database and what i am going to do in the next um huf um half an hour is to actually show you an example of how i could have developed this example and what are the different components that are available for you when you are actually developing these kind of applications you know for this what i am going to do is i am going to take a simple application that we have in iit madras and we are going to show how incrementally i can develop you know a database model and a database application for this um um business process within iit madras the business processes actually one of trying to allow people to purchase computer systems and peripherals by the faculty of iit madras where there are different vendors who can supply these peripherals at different prices so the idea is actually to allow the um  noise  faculty to be able to purchase these systems at a very competitive rate and what iit madras does is actually it enters into what is called the rate contract with various vendors for a supply of computer peripherals and computer systems now this processes done every every few months to actually um take care of the varying rates of this computer items and the vendors can bid at different points of times and then the best competitive vendor is chosen for a given computer system and then that is made available to all the faculty so that they can you know they can use that rate contract system to actually obtain their necessary computer items without going through any other purchase procedure you know and this is done you know often to actually allow the systems which keep changing interms of a configuration um to be brought in to the rate contract system now one of the requirements here is the vendor should be able to quote at regular intervals into for iit for various computer system configuration they should also be able to see what iit s requirement is and once these things has chosen um the systems are chosen then they should be made available to the faculty so that they they they are able to actually choose um whatever systems they need and are able to generate the required papers for purchasing those systems now when we initially had a paper based system all the vendors used to quote supplied their quotes on paper and then they used to be personal in our purchase department who use to enter all the things into a word processing system like microsoft word providing a comparative statement of different vendors for this computer configurations and so its basically file systems that we are using but then lot of processing manual processing of entering these data in that which is usually other problem because this involves a tds a typing of the details provided by the vendors and then circulation of this papers to varies members so that they can look at the details and then choose the  noise  systems that are most competitive now the problem in this particular case as we can understand is that um which used to take nearly three to four weeks minimum for the entire process starting from the vendors quoting to the selection it used to take three weeks to four weeks and used to involve lot of manual processing by various people and also circulation of large quantity of papers containing this data two various various members involved in choosing these things so obviously this is a very good case where one can think of how this whole processing can be improved as a business case and see how one can bring in an automated processing by choosing databases as an example of introducing data base systems for automating the entire business process or work flow from one end to the other end to just show what are the likely business processes or work flow elements that will be involved in this i will just take you to a um  noise  um a site which actually gives a um a first cut requirements of what was developed to show how this whole thing can be um can be you know sort of automated in a um  noise  using the database concepts in a nice way you know  refer slide time  00  22  53  so this actually gives an initial know initial flow of how we could probably introduce the database systems for automating this business process and what are the key things as i was explaining was to actually search through these rate contract details which were approved which could be used by various faculty members in choosing their requirements now as given earlier now i can actually quote through this which actually says that you can search through this rate contract details let us understand this um one business process of how exactly one can search through this rate contract this actually take you to the various other menus that we have  refer slide time  00  23  33  for example one can search through computer system configuration or add on parts or space for computer systems or printers and scanners and note book computers you know  refer slide time  00  23  54  so if i go into computer system configuration then i can search through this computer system configuration now i can search for example iit specification number wise where each specification will give the different specifications  noise  you know for the computer systems or you can have the search through vendor name or through brand name or processor name or a combination of this for an example let us say i actually click on specification number wise and submit this then i can actually see all the specifications iit has in this particular case  refer slide time  00  24  31  there are ten specifications in this case let us say i actually choose the fourth specification and submit that specification  refer slide time  00  24  42  then now i can see all the vendors who actually quoted for this specification right and the their prices for example i can see here there is a company um infotech limited which quoted for the specification four at twenty five thousand rupees you know for g thirty three chipset for p thirty three p thirty five chipset actually it is available at twenty five thousand five hundred so what this gives is actually all the vendors and whoever has quoted for the system and they prove to be in the rate contract and what are available in this rate contract can be searched by using this  refer slide time  00  23  54  one can infact do a little better search by saying for example if you want to actually take a specific processor or a cpu clock speed and also a vendor name then you can actually submit this quote here  refer slide time  00  25  47  now you can say a cannocial electronics private limited is a vendor name and i want clock speed two point eight and i want to see um all specifications matching these two requirements then i will basically submit this  refer slide time  00  26  02  i get one specification matching this requirement which says that this is the available system for me in the rate contract which i probably put order if that satisfies my requirement now to understand what is happening here as you can see here we have  noise  we need to get this information organized in the back end um database and have this information stored properly into our database system you know now this is where actually the data model becomes extremely important you know what is the data model and how do we store data into that system and how do we access that data as we need it for example in this particular case you can see that i am actually accessing the data by actually saying that i i actually need to retrieve the data which matches a particular specification number and a particular vendor number you know so i am actually going into the data and trying to match the required values and retrieving the data as they match that particular values that i have supplied its quiet possible that i actually navigate through data which means that i actually um retrieve at one level for example you can see when actually go with specification number i am actually retrieving the data at one level now we can see these are all data i have as for a specification number is concerned now i can say for five is what i am actually choosing which means i am actually drilling down i am navigating using into the database by saying for five now i need the information  refer slide time  00  27  54  so i get to the second level by saying please get the information on five for me you know this is what we understand as a navigational query you know where you are navigating through the data has a post to actually retrieving all the information that satisfies a particular criterion for example one can say i want to find out all the students who have registered my um database systems course or my paradigm programming course or my grid computing course so or i want to find out for example a further intersection of the sets by saying i want to get all those students who registered for my paradigm programming course and also my course on grid computing so in that sense we have various ways we retrieve this data and hence the data model becomes extremely important what is the kind of a data model we have um becomes extremely important and also another interesting fact that we should see when we see data models is that here is a case where the data is well organized as a table for example i have an iit serial number and have an iit specification have i have a chipset have a company name unit price unit price without monitor category thing like this this is more like a table organization of the data  noise  now lot of information if you see for our business purposes gets very easily organized as flat tables of this of this nature and that is one kind of a data model that is extensively used while storing the data you know their other data models that are possible for example you could have data models where you have an object um where the information could be unstructured um which could mean that you have a field name and you have a corresponding value but then one field in that particular object could actually point to a another object and that could point to a another object when um example for such a kind of data model or real world data model is for an example if you want to store the information about maruti eight hundred what are all the parts maruti eight hundred has you know it fits more as a object data model because you are going to specify now what kind of engine maruti eight hundred has and within that what kind of other parts that engine has so you are going to actually store the information more interms of objects and the objects linking each other and that becomes an object data model you know where you have a flat model like this flat table base model it is often also called the relational model and as you go into this course you will see um in detail how the relational model can be used for storing data and how the relational database system can be used for developing applications that fit very well into the relational model what i have shown here is a simple example of using the relational model for building this computer rate contract system you know i will further go down and show how the relational model fix in here interms of developing the application you know one of the important things that we should note down here is for example refer slide time  00  31  41  this also enables um the vendors to come in and give information of what they have directly once i have enabled this database system that can be shown here for example by saying that a vendor who wants to quotes to me can login here  refer slide time  00  32  06  for example you can see here it allows the vendor to login here and in this particular case i can give a guest name guest is the login and then with the password you can login now  refer slide time  00  32  21  which shows that now he is allow to actually enter my system and he can quote to my um various requirements for example in this particular case i have a computer system configuration  refer slide time  00  32  38  now for the computer system configuration there are different kinds of specifications which i need so he can say that he wants to quote for the specification one here he can also click here to see the specifications that i have  refer slide time  00  32  51  at the at the moment for the computer systems so this gives the list of specifications that he has you know for the computer systems now from this he can choose for which specification he wants to quote for the computer system configuration once he decides his specification which he is going to quote you can basically allow him to quote here  refer slide time  00  33  17  by saying that now he can give this information infact he can say that this particular specification one he wants to quote it let us say at twenty five thousand then you can say he can supply that information here unit price and he can say the unit price without monitor could be eighteen thousand in this case assuming that he has some calculation for the monitor so he basically can give this information and also put some comments saying that that extra something here you know if he wants to say that the the additional component cost component that is going to use in this once he is decided to quote for this he can say that this is the information that i can give it to you that for a specification one is concerned then he can say he is is actually quoting for this  refer slide time  00  34  19  which means that this information gets into my database now ok i will show you in a minute where exactly this gets in and gets stored for me and that s where basically the entire relational database system concept makes sense once he is doing this update this goes in and get stored in an appropriate table  noise  in my database system and i will be able to retrieve or use this information later to make my decisions or other business processes or transaction to access this particular data you know  refer slide time  00  34  58  now you can say that this confirms saying that you have logged in as a guest here and you have quoted from my computer system and the quotation is registered into the database this is extremely useful for various reasons because now the vendor doesn t need to actually come to iit madras he can be anywhere he can be situated in delhi he can be in chennai different parts of chennai all that he needs to do is he need to registered with me so that he has a id and the password once he has that he just logs into my system and able to quote into my system and once he quotes that quote is actually stored into my database system and other processes and transaction in my database system will be able to actually access and do the necessary processing here you know now let us understand what are the key challenges are conceptual thing that we should be understanding in building this you know now um for a minute let us understand this application what we have as different entities in these application and how do actually store this data or create my tables and my data and then  noise  how do actually make the transactions work with this particular data that i have underneath you know now the most important requirements in in this particular case is to actually see that underneath i have actually created a database system  refer slide time  00  36  36  in this particular case i am using a mysql relational database system which actually allows me to actually create tables and store the database um underneath server for me you know now what are the different ways actually i can now put my data into this system for example here is the you know key um database system that i am using for actually creating this rate contract system that is called rate contract rc underscore april underscore six zero six and this has actually got all these tables as you can see i have actually got several tables underneath which are all storing the information for me you know for example one can see that there is an approved quotations for add on systems this is one table then i have the approved quotation for computer there is another table and approved quotations for note books and approved quotation for printers now we can see these are all the tables in some sense they form the schema of the database and this is very critical in database design which you are going to study as part of your um your your course here how do i design my database system the starting point for design of the database systems is actually understanding the different entities that are their in your domain for example in this particular case there are vendors there are users there are people who select you know based on the what the vendors have quoted so you have actually got different entities and they have some relationship with each other for example vendors quote you know into your data and then there are people who actually pick information from this and choose so actually you you you have to understand what are the key entities in your domain and how exactly each entity interms of what what um attributes these entities have for example one of the things that you can see here which is um  noise  one of the um tables for example here shown here is the quotations for computers which is something that i have used here this is one of the tables now this table has different fields these are entities for example if you see quotation computers has some kind of a table which comes from my you know um entity ok  noise  which we will see in a minute what is the relationship and how do i actually break an entity in to a table you know now in this particular case you can see serial number chipset vendor name these are all the attributes or the you know fields that i have for this particular table you know if you take for example vendor name and they they all registered with iit madras then vendors will have a user name and a password and then their address these are all the attributes that a vendor entity will have you know and when actually i have this vendor as an entity one way i could possibly create um this information in to my database is by actually converting this entity into a table for example in this particular case as you can see a simple thing like users for example has um um to the users of the system and they have the information on company user name and password being available in this particular table so if you see user as an entity these are the fields that will be available as part of that entity so one easy thing when you are designing your database system is to actually um convert the entities into tables relational tables and store this information now there are lot of issues and you know interms of converting an entity in to a table because if this information is duplicated for example this company name and id is duplicated in other places for example it is available in other other other places the company name is available in other tables then what happens is this duplication of data will create problems when you actually a company says that now it has changed its name then you have to go and then start changing this name at multiple locations so one of the key things that we understand when we are designing database is that one um fact at one location so you don t want to store you know the same piece of fact at multiple locations because it causes lot of problems for you know so database design concepts will explain how exactly this process of design could be done and how one can come up given an entity relationship diagram how you can come up with tables like this for example the database design in this particular case has all these twenty three tables and this is the essential part when i design my system and this is the key of the whole concept and how do i actually arrive at these twenty three tables and each table for example has its own you know for example in the case of quotations you can see this is the schema that i have you know  noise  you can see the the table here has serial number chipset vendor name unit price bulk price bulk two price for example if they quoting for more then some number of systems whether there is a discount additional you know with monitor without monitor all these stuff and then you know whether the monitor is what kind of monitor brand that they are supplying with me and what are the comments for example whether he what s vat to be added or its included and all this you know so this is what we understand as the database designed  noise  now this schemas also change for example tomorrow i don t want this bulk two price to be quoted because it is no longer my requirement then i drop this particular field from my quotation for computer which means that i no longer need this particular field to be available  noise  this is what we understand as schema change in my system in my in my database you know and often these schemas change because as the systems evolve business processes evolved and systems evolved schema changes are natural you know and your system should be able to cope up with this schema changes that s other important issue when we actually deal with databases  noise  right now in this particular case we will go and see the quotations for computers and see what we have done earlier interms of these quotations whether that is available in this particular case as you can see here i am using the php admin and directly getting into the database for example in the particular case i can go and then browse for example um the values of the database  refer slide time  00  44  30  now you can see that these are the values that are available for this particular field so i can also probably browse the entire database so you can you can see that this information that s available in this particular table can be you can use the browse and you can see what is the information available for example quotations for computers you can see now here you have um um  noise  dev systems which quoted intel g thirty three chipset for this unit price this is a bulk price they didn t quote bulk two price then it s a category is its a local vendor and the monitor is the viewsonic and then a some comments for people to look at for both viewsonic or acer tft monitors you probably would like to supply both of them you know so you can see this this is what we have as the  noise  information right now one can even see how this data gets changed when something got updated for example you can go to the um as we saw earlier you can go to the computer system and once you have quoted for that you probably can see  refer slide time  00  45  58  based on here what i had given earlier as a guest has come up now here you can see that i have inserted earlier in the as a guest and quoted twenty five thousand and eighteen thousand as the unit price and without monitor i have quoted eighteen thousand so that information is reflected here as you can see here you know and the default samtel has been picked up and i said vat is extra and that is also available for me here you know now as far as the work flow is concerned i can use these this information whatever people have quoted and um pick the whatever quotations that i want put them into the approved quotations for examples for computers this will basically reflect the approved quotations for the computers now you can see all the approved data is available in this particular case and then this is the approved systems for computer systems and that s how the information can be organized in terms of your database tables i can pick the informations from one table and then you know it can be added on to the other table all this what basically are called transactions so you are going to look at in this course in detail how transactions operate on the database um system on the underlining data and change the state of the data as it goes  noise  for example each one of this can be seen to be a transaction for example um one can think of which are the things that are actually transactions in this particular case i am searching through the computer configuration as i have actually shown you earlier now in this particular case i am actually the first level search is not a transaction because it is just giving me all the um you know all the fields that i can use for searching now you can see once i come here this is the point where i choose a specification and once i choose a specification or a vendor name for example i am actually trying to make a query into the database it is at this stage  refer slide time  00  48  26  actually it goes and picks up this data from the database table that i have underneath for example these are all the vendors that i have in my system and now once i choose one more vendor here and i say let us say i am actually choosing the um one of the vendors that is available here and then submit that thing  refer slide time  00  48  53  it again goes and reads the data from the database these are actually called the read only transactions because all that they are doing is they are just going into the database and reading the information that i have and then giving me you know in a convenient you know form where i can look at it through my browser so last thing happening underneath but then ultimately all that it is required here is to go carry the database get the required information and show this information for somebody who is asking for that information so these are actually read only transaction they are just reading the values that are stored in the database but as apposed the this kind of transactions one can even think of the other kind of transactions which we saw here you know we actually said that one can login as a guest and once he logs in as a guest off you can sort of  noise  quote for the computer system now once he says that for a particular system three is quoting now and then he give some data here saying that please take my quote as let us say you know twelve thousand and then give the other thing as eight thousand then i think what we are doing in this processes we are actually updating our underneath database once he says quoted what is happening is you you will be seeing that the whole you know quote has been registered into the database which means that the data has been now been written in to the database so these are actually write transaction they are writing in to my database there are some transaction which can go and both do both read as well as write from my database to summarize whats happening here what we are trying to do is given an application like this we have been able to design the underneath tables which are the you know database models and store the data in that particular modal and have our transactions which are actually accessing this data and supporting various business processes that i have in this particular case searching through my rate contract allowing my vendors to quote into the database and allowing my you know um faculty to choose and then order the related computer systems so the entire business processing of the data is automated using an underlining database systems and hence database systems become extremely critical interms of supporting automating this processes and eliminating manual processing and manual you know production of the papers so that the whole work flow gets completely automated and we have a simple and elegant and nice way of actually handling our business processing systems now um what you are going to see in this course as part of this course is you are going to see how this database design can be done what is the meaning of transactions and how they operate and how do they actually maintain the consistency of the underlining data and provides certain properties when we develop these systems and make it available for day to day use for the people you know so these issues are going to be covered in depth in the next forty one lectures that we have in this course  noise  thank you transcription by  vidhya proof read by  shobana database management system lecture # 32 case study  mysql dr s srinath hello and welcome to another session in database management systems in um this lecture today we will be looking at um a case study of of a real world dbms we learned so much of concepts in database management systems um like um especially the relational database i mean how relational tables are stored what is the relational algebra and what is the um query processing engine do and um what are the different kinds of indexing methods and so on and so forth let us see how um it is actually implemented in some real world context in this lecture today we are taking up a pretty interesting real world database system namely mysql database system  refer slide time  01  24  what is very interesting about this database system is that um it is a phenomenon or it is um it is an outcome of a very remarkable phenomenon of of recent times namely the open source and and the free software phenomenon if it were not um for the the open source or the free software phenomenon um i would contain that several different innovations that that we see in the in the realm of algorithms and um and software design would not have been possible today it is possible for um for any one around the world to to be able to freely download um this this database here which which we are going to this dbms here which we are going to um see today and um um you can download it for free of charge that is um um its not only you can download it for free for charge its you can also have access to the source code that that made up the the mysql database and you can make your own changes to the source code of the um dbms and you you can um its its completely free um um in that sense free um as in freedom like um which is which is generally used which is the term generally used by the free software foundation um people who allocate free software um around the world so let us look at mysql in more detail mysql is a very popular open source database systems  refer slide time  03  37  and it is licensed under what is called as gpl or the gnu public license um of course  noise  looking in to the inter cases of gpl is definitely not within the scope of um our lecture here but essentially the the gist of gpl essentially says that um um the the the software provided is free in the sense that you are free as in as in freedom that you are free to um make any modifications to the source code you can free to redistribute the source code you can free to resell the source code um um if you wish to however you need to also give the same freedom to whoever is going to next use your software and so on and there are different varieties of gpl and you should check up what exactly um is the is the specific variety of gpl when you download down mysql and it is it s a widely used database management system and it is used in the wide variety of systems like embedded systems to large scale information systems and its largely written in c and c plus plus and its source code is available like i like i said and it has been ported to many different operating systems and operating platforms and the the website on which it is available is is shown here in the slide namely www dot mysql dot org  noise  let us have a brief look at the history of mysql how it how it came it to being and what are its different features mysql is a relatively recent phenomenon in database management systems in fact the the um genesis of mysql stems from another um small sql engine called msql small m and sql as as shown in the slide here  refer slide time  04  49  msql are are i guess it stood for many sql was was a simple sql engine that allowed users to um write um sql queries and maintain very simple databases with um with a small set of tables here now some um set of people um in sweden actually um who who try to um try to tweak msql msql also was um was free software and they try to tweak msql and out of this endeavor um arose a completely new set of um database system which is now called mysql now mysql has now um the people who form mysql has now um formed um form their own company mysql ab which is a swedish company and um run run essentially by by the three people who form this database system in the first phase  noise  and of course there are um there are also commercial versions of mysql which which are sold for a price and which come with lot more support um from the mysql team um who can actually install and tune your database system and so on and so forth but for the free version of the software there is there is no support and there is no warranty as as as such um mysql has several different features there are several apis or application programming interface um that are available for many languages like c c plus plus eiffel java perl php and so on and so forth this apis allows um um programs written in these languages to directly send sql commands to mysql so you can embed your database system or or rather your database client within another application program it uses b tree based um disk tables its and um um mysql team themselves developed new storage structure structure is called myisam and also performs index compression so that the the the shadow of the index on on the disk is is quite small and there are storage engines that are available which support both transactional and non transactional operations remember what is what is a requirement of a transactional operation especially essentially the acid semantics that it has to provide atomicity and isolation and consistency in durability and so on um um durability also in in a sense implies recovery that is um once a commit has happened you should be able to um it should be reflected in the database even if there is a crash later on and so on  noise  mysql um is is a is a multi threaded engine and um if your operating system supports threads at at the kernel level mysql uses that and if your operating system and your hardware of course supports multiple processors um mysql automatically uses multiple processors  refer slide time  08  44  so so it is um it is scalable to um many processors on on a on a multi processor machine and um um and is aware of the fact that there are there are many process processors that are running and um you you can utilize them the kind of memory allocation system that that it is uses is a is a thread based memory allocation system rather than a process based where each thread which is a which is a light weight process manages its own memory um it also has other um nice features like fast joins which um which is computed by by an optimized algorithm um we just uses one pass operation over over the tables when when performing the joins um we saw several different kinds of join techniques merge join hash join and so on um so which um um which essentially forms the gist of the kind of algorithms that that go into these um these algo these techniques here that that computes fast joints and whenever temporary tables are required um say um virtual tables or um tables as a result of a query in a in a nested sql query and so on they are implemented using in memory hash tables and they are extremely fast to access and so on mysql server can um um has um has these double features that it can run as a stand alone server as well as it can run as an embedded server that is server that is actually embedded within some other application so so your application actually runs as the server um and it looks as though your application is supporting um database system um in itself  noise   refer slide time  11  27  mysql supports several different data types um  noise  several different atomic data types um we we saw some kinds of data types in when we are talking about sql for example integers and date type and um character strings and characters and um enamor or the or the set data type where where you where you provide the enumerated set of values or or the set of all possible values that that a particular um data element can take and so on um similarly in mysql there are several different um atomic types that are supported signed and unsigned integers and you can there are even different lengths of integers they are called as um tiny int small int um integers and big int and so on and so forth so so there could be one byte or two bytes or four or eight bytes long and um there are um float um floating point numbers double precision numbers characters varchar essentially is a is a string which can um variable number of characters and text and blob is a is a binary large object um which could be some thing like a any media object or um um audio visual file um or anything like that um date time and so on it even supports um variable length records your record length need not be fixed you can you can have a combination of fixed and variable length records um within a single database  noise  and um mysql supports functions in both from and where clauses of an sql statement remember um an sql statement has three different um um three different parts right where you say select um something from something else where the condition right so you say select and and you say set of field names or or attribute names and which says what to select but here in fact you you can say um select and give a function so you sa you can say select average of um um the the values of the this lets say a total marks um taken by or scored by students and so on ok  refer slide time  17  17  so you can say select average of marks from student um um where marks equal to whatever and so on ok where marks is greater than sixty percent or whatever and so on ok so um you can have functions where average here is is a function that is that s actually in the from clause not exactly in the where clause we saw in the session on sql that um functions being supported in the in the from clause is only a recent addition to the sql standard and it was not primarily in the in the in the initial sql standards it also supports mysql also supports outer joins both left and right outer joins um remember what is an outer join um when you are joining two different tables um um um you you basically join lets say you are computing an equijoin so you basically join based on the the value of the certain attribute therefore um um join happens where um jo join happens by matching values um different values that this attributes take between the two tables ok now it might so happen that for a particular value in the first table there may be no corresponding value in the second table um such records are such records are generally ignored if um if you are computing an inner join or a or a usual um  noise  or or a usual conventional join um operator but in the outer join operator you um um you compute even those um you don t throw away such such records and embed them with nulls essentially right so um um this example essentially is an examples of left outer join and the the corresponding or or the dual operation of that were um were  noise  a a given attribute in the second table has no corresponding attribute in the first table would would form a right outer join it also um mysql also supports aliases on tables and columns so so you can refer to the same table and and column using different names and um  noise  in recent versions of mysql especially in mysql version three dot two dot two and and later um you can actually provide an sql query across different databases itself so where what is a database here databases um um essentially set of tables that um that are that are stored under one heading say saying this is the student database this is an employee database and this is the some other database and so on um um here you can actually provide you can actually give sql queries that can span across different databases um within a single query  noise  the there also many features like scalability and um um performance issues and so on mysql has been tested on very large databases um up to databases which have up to sixty thousand tables and five billion rows um um in in the table it can allow up to sixty four indexes per table that is um you you you can you can index up to sixty four attributes in a table and up to sixteen columns as part of an index essentially sixteen columns as part of the key that that makes of the index so so your key can be um at up to sixteen um um attributes long and um it also supports prefix based indexing what is a prefix based indexing essentially on on um  noise  data types like varchar which is a which is a string or or text you might want to ask queries like show me all all students whose name start with some um um pr a or whatever something like that right so um you you might want to um you might want to um search for similar strings or you might want to search for prefix match strings and so on so um such kinds of search can be efficiently supported using prefix based indexes rath rather than indexing on the complete string as such um mysql also supports user privileges and password based um authentication um by by users and also host based authentication so so um if the same user or or if an authenticated user logs in from a different host  noise  um here she would would have go through a different set of authentication privileges um here she may not be um um may not be allowed access from from a from different host and only from particular host as such  noise  now what kinds of standards thus mysql comply to um note that um when when we talk about standards we don t means something that is setting time  refer slide time  17  29  when we say sql standard um the the standard called sql itself has been evolving over time um um they um almost um every um several years or so new features are added to the sql standard and which becomes the new version of the standard in addition to this evolution and standards um mysql or or or any other database system would itself be evolving that is when when when the dbms is is implemented um um um the there would be a need that is felt for some more features to be included and so on so so there is a there is a different set of evolution that is happening as part of the database itself or as part of the dbms itself so so the two different evolutionary streams one of the evolution of the standard one of the evolution of the um dbms itself and when we say it supports or or it complies to a particular standard we should we should be clear about which version of the dbms supports which version of the standard because both are evolving in its own um um different trajectory right so um mysql actually um has um has several different modes or or it can operates in several different modes um that can um supports several different standards in in sql for example mysql versions greater than four dot one can apply different modes to different clients um itself that is lets say client one connects to mysql and says i want to use um the the ansi standard of sql and second client connects to the same mysql server that is running and and says i want to use the ibm db two standards um of of sql and not the ansi standard sql and and so on so the the the default mode of operation for um um for a for a client can be set with this option it is it is shown here  refer slide time  17  29  minus minus sql mode equal to a given mode when we are starting um mysqld that is for the server rather not for the client that is um you can set up this is the default standard or this is the default mode of operation for mysql and ansi sql can also be selected using the minus minus ansi option um when you are starting up mysqld that is in the command line itself and you can um you can set the mode for for any particular client um um at at any time using the set command that is set sql mode equal to some some particular mode  noise  so what are the different modes that that it supports um the the ansi mode for example um where um set mode equal to ansi ok so when when mode is set to ansi there are certain implications um in the way mysql treats given sql query for example um um real suppose um attribute is is named as real then it is by default treat it has double that is double precision real number by mysql  refer slide time  22  00  but in ansi standard sql it is treated as a floating point number the single precision floating point number and similarly um um you can use double cotes for for um for identifiers rather than strings that is um when when you when you are identifying um um column name you can actually use double cotes rather than when you are um repre representing the string data which is part of a column similarly this um double pipe operator that that is shown here is treated as a concatenation operator rather than the logical or operator um the the the two vertical lines that that you see here  refer slide time  21  14  and um  noise  um when i give a function um let us say average ok it ignores spaces between a function and the first parenthesis suppose this is the function here now um it ignores all the spaces between the function and the first parenthesis here  refer slide time  23  17  now what is the implication of this the implication of this is that i can not use the the same function name um um as part of a let us say a column name or a or a variable name or or something like that because this would clash with the function name because there is no way to um there is no way for the the sql server to distinguish whether this is a function call or or a variable name or anything or a or a column name during parsing ok so by default mysql does not ignore this thing that is you should not have a space um um between a function name and the first parentheses so that function names can um can be repeated or function names need not be unique that is you can use the same function name for for your column names or variable names similarly there are um other sql modes for example you can say mode equal to db two or mode equal to oracle which um which supports some some kinds of parsing um which has some kinds of parsing implications that are compatible with the way um ibm db two um um works or the way oracle database work for example both of this modes have more or less same um same implication for example you have to treat double pipes as a concatenation operator rather than a logical or operator and allow double quotes for um identifier quotes and um and ignore space between function name and parentheses that is there is um you can not use the same names between the function and um and a column name there also other sql modes we shall not going to the details here but let us just enumerate what are all the different sql modes  refer slide time  23  31  you can set mode equal to maxdb maxdb is the variant of mysql which is primarily oriented towards enterprise application so its it s a very it s a transactional it supports mainly transactional semantics and you can build large enterprise applications around the maxdb and then you can um set mode equal to mssql which is microsoft sql or postgres sql and mysql um different version like three dot two dot three or mysql forty dot zero and so on  noise  there are other s sql extensions that that mysql supports which are different from the ansi standard um these include the the following that are that are shown here  refer slid etime  24  12  for example you can um um insert a comment c like comment um c like comment means um um one which starts with slash star and ends with star slash um you can include a comment um within a sql query any anywhere within a sql query um similarly you can um um you can perform some kind of um um macro operations or or pre processing operations which which um um from which you can um bring about selective execution of an sql query depending on which version of mysql server this query is being executed on for example um look at this statement here  refer slide time  24  52  it says select and within comments it says exclamation mark or bank three two three distinct um name from employee now what is means is that um if my sql server or mysql server um which is executing this query is of version three dot two dot three then you um um then you execute whatever is there within this um um comment otherwise comment out this um that is remove this um commented part from the query therefore if i am running it on let us say um um mysql server four then um the the outcome would be select name from employee and and only in three dot two dot three dot two dot three it would it would say select distinct name from employee now um why would one want to do some thing like that because there are certain kinds of um um nuances or um certain kinds of changes in the sql semantics between different versions of mysql and you can write a single sql query that can work on any version of mysql by selectively um um masking or enabling certain parts of the query  noise  let us look at how data is organized on um on the disk in the um in in mysql  noise  mysql does not um create its own file system in fact it uses the file system whatever file system is provided by the operating system and each database is stored in a separate directory in an existing file system what are the implications of doing this that is what are the implications of using an existing file system and and the fact that you are using you are keeping um one directory per database  refer slide time  26  32  um some of the implications are shown here that is the the names of tables can be either case sensitive or case sen insensitive depending on the operating system for example um if you are using a unix based system or a linux based system it would be case sensitive so so if you say um name with with n as capital this would be different from name with with all small letters so so they would be two different um tables um having having these two different names however on an operating system like windows it is case insensitive so um um where both of them can be in the same um would would point to the same table and tables can be renamed and dropped using operating system commands um not necessarily with in mysql for example you just say move or rename a given file and then the table actually gets renamed so as shown here to rename a particular table you just have to rename files with the following extensions dot myd dot myi and dot frm for example if you have a table called employee um you will see that and and in a database called employees database um you will see that under the directory called employees database you will have employee dot myd employee dot myi and employee dot frm now suppose you want to change employee to to to something else say contractor whatever you just have to change um or you just have to move these files to some other files names using operating system commands and this change will be automatically reflected in the um within the database itself  noise  what kinds of storage engines does does does mysql use and what is the storage engine in the first place  refer slide time  28  39  a storage engine essentially is is the engine um um that that determines how tables are organized within files and and what is the updation policy and so on  refer slide time  29  33  and storage engines also defer with respect to their support for transaction semantics that is does it support atomic updates does it support isolated updates does it support recovery and so on and so forth the different storage engines that that mysql supports um the the um the the primary storage engine which many of the earlier versions of mysql where shift with um was the isam storage engine as we saw earlier there is um it is called the myisam storage engine isam essentially stands for index sequential access mode and provides um indexed sequential access um does not provide any transactional support ther are also other kinds of storage engines like like the heap storage engine which um um which is used for managing tables that are in memory that is temporary tables or virtual tables and so on um this is also called the memory storage engine there is another kind of storage engine called merge storage engine which um um which can treat several isam tables as a single merge table that is it can efficiently merge different tables that have been created using the myisam storage engine and in recent versions of mysql there are storage engines like ino db bdb which provides support for transaction safe updates that means it provide support for acid semantics um there available in versions only later than three point two three point three four a so um so to be very safe and and also um you should be um you should also be aware of the fact that in in these versions or if this is the first version in which this storage engine was was released  refer slide time  30  45  then it should be available only in source code form and not in pre compile binaries therefore if you want to use transaction semantics um on on mysql it would be um it would make um alot of sense to download a later version something like mysql four or mysql five which as which is the latest as of today  noise  and the the bdb storage engine is not supported on all operating platforms and um so so you have to um ensure that for whatever platform that you are using um the the bdb storage engine is actually supported there is also the ndb clusters storage engine where um um um where you can implement mysql over a computational cluster um what is a computational cluster it is it is it is a collection of different machines on a local area network which can um um which can provide what is called as the single system interface that is which can um um which can provide the um um combined um power or processing power of all the processors in in the of all the machines now ndb clusters is a storage engine that exploits this this capability of a cluster so it can support tables that are actually physically partitioned over different  noise  um machines in a cluster and of course ndb cluster is also very recent storage engine that is supported by mysql and it is sup it it is available in versions four dot one dot two and later on and like we had noted above that in four dot one dot two its only available in source form that is you have to compile um this engine yourself but in later um versions the binary forms or pre compiled versions are also available  refer slide time  32  34  what about transaction semantics in mysql um transaction semantics um have been supported in mysql only after um version three dot two two three max and and above and so on but before that um um mysql actually supported what are called as atomic updates we will we will look into atomic updates shortly and um what what are the relative advantages and disadvantages of atomic updates versus acid semantics and so on but if you want um specific acid semantics suppose you are using your mysql um engine for um u let to say performing some debit credit updates in in any kind of whether banking or um any kind of money transaction that thats going on which has to be recorded on this its better to use um mysql which supports the ino db or the bdb storage engines because they they have support for full transaction semantics and other non transactional storage engines like um like the earlier versions of um mysql and and so on support what are called as atomic updates or atomic operations and um um this can be simulated in a um in a ino db for example by setting auto commit equal to one that is um commit is automatic and its its its not explicitly stated in the sql query these non transactional atomic updates are actually faster than the transaction storage engines and because of course you don t have to  noise  worry about locking and isolation and other isolation based semantics nor do you have to worry about um logging and so on where the the first was locking and the second is logging and um logging in order to um support log based recovery mechanisms that is in order to be able to support durability aspect of um acid semantics now because all of this are not required and only atomic updates are required its its much faster and in many cases um this is all that is required that is um um suppose you now that there are no um concurrent operation that are happening or um suppose you are you are relatively sure of of the uptime of your dbms then it is um it makes much more sense to use non transactional storage engines which support only atomic updates because they are many times faster than um um acid um um i mean storage engines which support acid updates and um the the and a very interesting feature of mysql is that you can use storage engines on a per table basis so you are actually save um only for this table use use ino db and for every other tables use um use let us say myisam and so on so so that um um  noise  you use atomic updates and only for the critical table you say that i want full acid semantics  noise  the next topic that we are will see are stored procedures um um what are stored procedures stored procedures is essentially a set of sql statements that can be actually be stored on the server and um um some kinds of sql statements which are used um pretty often in different queries um so you don t have to keep writing it as part of the query again and again and you can actually refer to these stored procedures as part of a given query um um there are two kinds of um um two ways in which you can store um sql statements um um as what are called as stored procedures and stored functions a procedure essentially um um performs a set of operations and um um may or may not return a value or may even return multiple values um um when it is called  refer slide time  36  11  but a function has a specific semantics that is um um it can be embedded within any sql statement and it returns back exactly one value as part of the um as part of its execution so stored procedures support is um is a very recent phenomenon in in mysql and it is supported in mysql versions five dot and above and the the latest version is actually as of now is actually five dot zero um and um it and whatever later versions would would still support um stored procedures and um um stored procedures as you know can um increases the performance in terms of communication complexity especially if you are using a large number of stored procedures in your um query um um you you can save on a large amount of communication complexity between the client and the server so you don t have to send all the query from the client to the server for for execution but um um um but the flip side of stored procedure is that um if um several different clients are connecting to the server and calling several different stored procedures then then the sever gets a huge load in in trying to execute all this different stored procedures  noise  and mysql also allows um um libraries or of a functions to be used as a um stored procedures within the server that is these libraries need not actually be sql statements but um um they could be pre compiled libraries written in some other application specific language like like c or c plus plus and um um stored procedures are created using the create procedure um um um  noise  construct in in sql and functions can be created using the create function constructs and um procedures are invoked using the call constructs you can say call procedure one or procedure two and so on um  refer slide time  37  41  while functions can be embedded within any sql statement like like we saw um select average of whatever or select anything and so on ok so um so you can actually embed um um embed function within an sql statement however the function has to return one specific value so functions return a value while procedures need not um its not actually do not but it need not return a value in fact a procedure can actually return multiple values through the input parameters itself and um um a function um is distinguish from a procedure by um by this returns statement so or or the returns clause so um function says returns this thing returns a particular value when you are declaring a stored function and a stored procedure or a function is associated with the specific database um what are the implications of of saying that a stored procedure is is associated with a specific database see um um you can say for example i have an employee database and there is a particular kind of query that that is invoked on this database for example what is the um um what is the average um um um what is the average working hours per week in this month and so on ok now you can actually store um store that as a stored procedure in the employees database now let us say you are using some other database let us say salaries database or some some other completely different database itself not not some other table but you can still um use or invoke a stored procedure on the employees database note that in sql in mysql now um you can actually um um you can actually give an sql statement that spans over multiple databases  refer slide time  38  54  so um so so when when you invoke a store procedure on a given um database an implicit command called use databases called use database is basically um um something like connect to the database and start using tables in this database  refer slide time  41  31  and this use statement is terminated when the procedure terminates and um um  noise  the the procedure now has access to all tables of the database um within which it is written  noise  and of course you can you can call store procedure of a specific database by prep ending its its um its name with a database name for example here you say call employeerecords dot updatesalary where updatesalary is a procedure that that um performs updates on on salary an employee records is the database um within which this stored procedure is stored and another um important implication of the fact that um stored procedures are associated with the database um is that when a database is dropped ok or or it is deleted then all stored procedures and functions that are associated with it are also dropped  refer slide tiem  41  37  support for triggers um triggers as you know we we covered a session on constraints and triggers um um triggers as you know are um um are essentially um spec special kind of stored procedures that is um um they written using eca rules as we saw in one of the previous sessions that is they are triggered automatically when an event happens and a given conditions is true now um um support for triggers is not very um um um is not very comprehensive in mysql as of now at least that is there is only preliminary support for triggers that is available um in mysql version five dot zero and triggers are essentially stored procedures and um which are actually called automatically in response to certain events mysql also supports views um as we saw view can be created either as a virtual table or um or as the materialized view that is um view can be um stored as a query or can be actually physically materialized depending on what kind of application that you are using in um in um transactional applications views are usually um um virtual tables whereas let us say in data warehousing and um other um um analytical applications views are actually materialized um mysql supports views as virtual tables and are they are supported from um mysql version five dot zero and above  refer slide time  42  31  and they are treated as real tables for all practical purposes that means um um each table has what is called as an access privilege system that is which user has what privileges for a particular table for example you can say um this user has a read privilege but no write privilege or this user has a read privilege write privilege but no drop privilege that is you can read and write to the table but he can not drop the table he can not delete the table and so on  noise  and view updation is automatic because only virtual views or virtual tables are supported and views are um created using create view command or there is also a variant of this which says create or replace view so if the or replace construct is is is present then a view would be updated um if if another view of the same name exists and how does mysql handle constraints um and constraints and of course triggers and so on um um well um in mysql the notion of triggers are are calling um stored procedures is separate from that of um calling constrai or handling constraints the kinds of constraints for example um data specific constraints like say primary key constraint or unique constraint um um if they say violation on the primary key constraint ese essentially that primary key has to be unique um and it cant be null right  refer slide time  44  20  so if there is a violation then um mysql will roll back um if if a transactional storage engine is used that is um it just performs the transactional um transaction rollback and if a non transactional engine is used then um it just stops execution on the on the offending row that is it does update the row and um from there on it does not update any other row um in the in the engine so therefore it is not it is not an atomic operation that is it could have updated some rows prior to it but it will stop execution at that point in a non transactional storage engine and many of this insert or update um queries also support a keyword called ignore essentially that means that constraint violations are ignored  noise  similarly um if there are violations in not null constructs and um say default constructs um um mysql automatically inserts the default value for example um if you say that some some attribute should be not null and the default value is say zero um then whenever that that attribute value is null then um um zero is inserted automatically now the reason that mysql does not um stop in this in this case is that you will be um um able to catch such violations only after the query parsing stage is um um is has been finished so um um and it would be it would make it quite in efficient to stop execution at this point and and get back um to the user for a different query so um mysql automatically because it already has a default value it automatically inserts the default value and similarly for queries that try to insert invalid values for example if it tries to insert um string into a numerical field or some kind of a numerical overflow happens during insertion and so on  refer slide time  45  32  mysql automatically inserts the best value that is um zero for numbers or null string for for strings and so on and similarly um um in an enumerated data type what is an enumerated data type where you say this data type can um um or this column can take only these set of values ok so um one can say the column called gender  noise  can only take m or f or um column called grade let us say can only take values a b c or d and so on ok so um um if an invalid value is is entered in to an enumerated type then it is automatically replaced by the first element in the enumerated type that is the element number zero let us see now how you can um actually download and install um um mysql on a linux based environment and get it running ok now um  noise  of course in order to download mysql we we already saw saw what is the where to visit in the internet www dot mysql dot org and from that you can actually download mysql on to your machine now um there are several different sources and binary versions of mysql that are available um and there available there there pre complied binaries that are available for different um linux distribution so um there are pre complied binaries for say um suse linux or mandrax linux or red hat linux and dban and so on so forth  refer slide time  48  15  um um the the easiest way of installation is to is to use an appropriate binary rpm package um rpm essentially is the red hat package manager which is the the default way in which um the packages are managed across different linux distributions and binary versions of mysql are complied with um static option which uses static libraries so um so its its quite um stable that is there there is very little possibility or no possibility of um linking violations or run time error occurring after the mysql server has started running  noise   refer slide time  48  51  so when you install mysql on your system um you need to install at least mysql server and mysql client packages um in order to get a complete functional dbms on your system that is you should have a both server part and a client part um you should also install this um package called mysql shared compat for backward compatibility especially if you are upgrading from a to a newer version of mysql mysql servers are by default installed in to the in to this directory var lib mysql so so after you install you can see um  noise  you can actually visit the directory and see what all it is installed and a new user called mysql is also created and um which is the which is the owner of the mysql demon or the mysql server um that um that is running and its also possible to make um mysql server start up automatically whenever you boot up your machine  refer slide time  50  03  let us see how we can do all of these things um in order to install um um mysql on on your machine you have to use this rpm minus i command or the or the mysql or the or the install command which which installs the server and and the client and of the of the appropriate version number and um after installation of mysql it is necessary to allocate certain um areas where databases can be create and this is automatically perform by the mysql install db program and in some kinds of installation procedures this is this program is automatically called after installation so so you don t have to even um um worry about calling this program and mysql install db creates a directory structure under which databases are created where each database as you know is a directories in itself right  refer slide time  50  40  and by default it creates two databases mysql and test the mysql database is the database of databases that is its the database holding information about other databases in this installation and the test database is a sample database that is provided to the user so let us lok at um how um mysql database looks like once you start let us say um um um once you have run your mysql install db there is something called mysql show which will show the contents of a given database  refer slide time  51  16  so when you say mysql show mysql um um you see that mysql database has several different tables um columns underscore prev db func um host tables underscore priv user and so on so which holds different informations about this about all the databases that are stored on this machine for example this holds user information about um which are all the valid users and their passwords and their their privileges and so on and so forth and tables privileges that is per table privileges this table is authorized to um um which user for doing what and so on ok then there are host based privileges and functions that that are stored as part of this database and um db is information about the other databases that are that are stored on this um um machine and this is columns based privilege information ok  refer slide time  52  46  and um you can um like we had noted earlier you can make mysql to start and stop automatically um um  noise  or start automatically on boot up whenever you boot up your machine for that you need to um  noise  you need to go to slash etc slash rc dot star where um star is either um dot dot three or dot five or whatever where um um in which you start your computer by default and those files should be changed in order to start up mysql um mysql server by default and the mysql server can be stared at any time um using this this command mysql dot server start and it can be stopped at any time from the shell um by mysql dot server and then stop  refer slide time  53  22  there other options for mysql server that can be added in a global configuration file and this is available in slash etc slash my dot cnf and this is the typical global configuration file which which is saying which is the database directory um which is the socket under which um mysql um um is connected which is the port on which the mysql is listening and who is the user owning the mysql and so on  noise   refer slide time  53  41  and the mysql client is the is the client program using which you connect on to the mysql server so mysql client can be started by mysql minus h host minus u user that is um connect to the server on this host using this um user id and then it ask for a password and then you get in to the mysql prompt  refer slide time  54  17  and sql commands can be issued from the mysql prompt um um and um um um it works within a user session that is you can say um use particular database which is the same as connect to to a database in several other dbms system and then you can start um um issuing sql commands and you can quit the client when the um using the quit command  refer slide time  54  35  in addition to sql using the mysql client itself mysql provides support for several application programming interfaces that is you can embed your mysql client um within another application program like c c plus plus perl java and so on and so forth let us have a brief look at um what kinds of um um c apis tht does mysql provides inorder to use the c api in your c program you have to include the mysql client library and once you include the mysql client library there are several data structures that that that are available to your program  refer slide time  54  53  for example there is a data structure called mysql which is a structure representing um which is the struct struct structure is essentially the struct data type in c that that represents a handle to a db connection or to a database connection that that your that you have par presently opened similarly mysql underscore res represents the result of a query and so on and so forth so so there several different such data structures which which you can access similarly there are several different um functions that that you can also use as part of your program for example you can say mysql underscore init which gets the initializes the mysql structure that is it obtains the handle to a data base connection similarly you can say mysql real connect which which actually connects um to the server and you can issue a query using mysql underscore query and once you get the query results you can say mysql fetch row which fetches um um the results of this query in a row by row fashion so so so the first row and the the next row and so on so forth  refer slide time  55  47  so using this you can actually embed your sql semantics or database semantics in to your larger application program itself so let us summarize what we have learnt um about mysql today is a very interesting database it s it s a very popular open source dbms that is the source code is available to you and and as a result um mysql has or um um changes to mysql has been contributed by several people across the world  refer slide time  56  41  in addition to the mysql um ab people who are who are in mysql ab and it provides several different storage structures um and um its scalable to number of clients and and data sizes and it also has transaction support and limited support for um triggers and stored procedures and um um best of all there are there are several different it has been ported on several different platforms you have mysql for window mac and linux and so on and it has several different apis for application programming or embedding application programs in to mysql so that brings us to the end of this session transcription  shobana proof reading  vidya database management system dr s srinath lec  2 conceptual design greeting to you all we have been talking about conceptual modeling of databases in previous session so in this session let us continue with um this subject into some more detail and before we continue or before we start with today 's session let us have a brief review of what we looked into conceptual modeling in the previous session we um if you remember we first talked about what is a typical database process or or design and development process looks like we first start with analyzing the uod or the or the universe of discourse and the analysis of the uod revels us two different kinds of requirements one is the data requirements and the second is the set of process or or application requirements so so the database runs with in your application context like i gave the metafer um um in the previous session a database is like an engine and the application context is the overall body or the car or bus or whatever that that you build on around the engine so both are important when we are trying to analyze the uod ok so um coming to a database analysis or or design of the database system we first start with a high-level description of what the database should handle right and this high-level description should not include any dbms specific terms or dbms specific issues it is mainly meant for the end users its mainly meant for us to show the end user saying this is what we have analyzed or this is what um we we have understood by analyzing your universe of discourse and it is mainly meant as a as a conceptualization of what are all the data requirement that that exist in this domain ok so um one of the most popular models for perform performing this conceptualization or or building a conceptual schema is what is called as the er diagram or the er model the entity relationship model we saw that entity relationship modeling um are made up of essentially two kinds of building blocks entities and relationships right so we looked at different nuances of entities and relationships and several issues that affect them like constraints and attributes and so on so let us briefly describe some some notation this is not exhaustive review but review of some of the main points what we looked at in in the entity relationship diagram  refer slide time  00  03  46  an entity type is described by a rectangle like this a simple rectangle and and an entity type is something which represents a class of entities or objects that that have an independent existence like a customer is an entity or a staff is an entity account is an entity in in a bank or um or in a company a department could be an entity manager is an entity and so on any logical u-net that has an independent existence is called an entity and an entity type is is an intension or some kind of a schema for a class of entities or for a set of entities there could be a set of different managers but all of them share the same attributes or or the same properties of the entity type called manager similarly there could be several departments  noise  but all department share the same properties of the entity type called  noise  department right um and then entities are associated with attributes which describe the characteristic of the entity and um we saw that attributes are intern define by the domains  noise  for example the age of an um is define by a domain that it should be greater that are equal to eighteen years and less than are equal to sixty five years typically  noise  right so um a a domain represents a space within which an attribute lies ok and even within attributes there are we saw that there are several kinds of attributes you could have a normal attribute a simple attribute an attributes which takes just one value or you could have a multi-valued attribute we gave the example of the color of a peacock right it doesn t have one color it has it has many colors and so it could it could have different values for for the same attribute an attribute could be a composite attribute that is it could contain many sub attributes like first name last name middle name and so on it could be a derived attribute like the like the age of a person which can be derived if you know the date of birth of the person and the present date right so so so there are several different of attributes so all entities of a given type have the same set of attributes right now um we also saw what are called as key attributes or what are called as keys i have not depicted this here in these diagrams but key is some set of attributes that can uniquely identify an entity within an entity type right so if i have a set of employees and suppose employees are given an employee identification number the employee identification number becomes the key if they if they don t have an identification number we usually have something like the income tax per permanent account number called the called the pan number or something which which forms the key it uniquely identifies a person on the other hand we ca n't use something like name or age as the key two persons can have the same name and of course two or more people are very likely to have the same age right so it can not uniquely identify a particular entity whereas um something like the identification number identifies the entity uniquely right so um key attributes identifies this  noise  entity types uniquely and there need not be just the one key attribute that could be more than one key attributes and um um like i gave the example of i need not have just one key to my house i can have two keys one for the front door and one for the back door fine but we usually use just one of the keys which the default basis and sometimes use the other key frequent ok and then um we talked about what is meant by a weak entity type a weak entity type is is a kind of entity type which does not have a key attribute we also saw an example yesterday um the example of an employee and and and his or her insurance record right so an employee has a key attribute the employee number or pan number or whatever but the insurance record does not have any existence without a corresponding employee or without a corresponding person to to be more general before without being associated with a corresponding person now once an insurance record is associated with a particular person whatever way we whatever we use to identify the person becomes the key to identify the insurance record as well right so for example identify an insurance record with a person having a particular pan number i can as well identify the insurance record with the same pan number as i am identifying the person right and we saw the second building block which are called relationships between attributes so um a relationship basically um ties in or or brings in an association between two or more entity types right so um even within relationship there are several different um um kinds of relationships and we saw cer um certain constraints that identify some kinds of relationships one of the first thing we saw was the cardinality constraints a relationship which has a cardinality constraint says that says how many entities of a particular type can participate in a relationship for example we saw that department managed by manager ok so it could be um there could be a constraints that a department may have just one manager or one head and um one person may manage at most one department at a time ok so so there is a cardinality constraint that exactly one um one department may be managed by exactly one manager ok on the other hand if i have something like works in an employee works in department ok so it is its rather that a one is to one relationship it is a one is to n relationship one department may have n employees and there could be a constraints that an employee may be associated with just one department so that um for for n employees there are just one department so on  noise  and we also saw what is called as an identifying relationship an identifying relationship is the one is a relationship that um um identifies a weak entity type with a strong entity type right that is if i have an employee and says insurance um um has this insurance record using some kind of a relationship it means that this relationship is giving an identification or an identity for this weak entity type called insurance record right so so such relationships are called identifying relationships and we are also saw what is called as a total participation within a relationship ok so um the the same thing here an insurance record will have no existence without um its corresponding employee or without without a corresponding person ok so um this insurance record is set to totally participate in this identifying relationship however we also saw that um just because there is a total participation does not mean that the entity type is a weak entity type how do we what is an example for that we we saw an example of department managers project ok let us say that a project has to be associated with a department otherwise well it will not get funding it wo n't it wo n't get of the ground or something like that ok so um so so in this case this is a this is a total participation that is the the existence of project the the existence of projects will depend upon this relationship that is um the existence of some department that is willing to manage this part  noise  however a project by its need not be a weak entity type that means it may have a separate key by itself a project will have its own separate  noise  project identification number which may be which may have no relationship with the department number right not every um total participation implies identifying relationships but an identifying relationship implies total participation right so um we will be continuing further on today with some more um notations and which can give us greater expressiveness to to express what we um um what we perceive as relationships and associations between data elements in our uod now these kinds of these sets of notations that we are going to see um today are what are called as enhanced er notations  refer slide time  00  11  47  or um sometime also called as a extended er notations or um and  noise  abbreviated as eer notations  noise   refer slide time  00  11  56  one of the first um relationships are are associations that we are going to see today is is the notation of subclassing or inheritance ok subclassing essentially face what is called as an is-a relationship as you can see in the slide here ok an entity class b is said to be a subclass of another entity class a if it shares and is-a relationship with a what is meant by an is-a relationship have a look at these examples a car is-a vehicle right a monkey is-a primate and a primate is-a animal or is an animal whatever ok a manager is-a employee right so um um if you have noticed here an is-a relationship identifies a specialization of um some particular entity type a car is-a vehicle ok but not all vehicles are cars there could be trucks there could be bicycles there could be um scooters and so on so vehicle is a more general class of cars ok similarly if you say um a maruti eight hundred is a car ok but not all cars are maruti eight hundreds right so car is a generalization of maruti eight hundred and maruti eight hundred is a specialization of the entity type called car right similarly a monkey is-a primate and a primate is-a animal but not the other way right so um so so so the entity class b that is at the left hand side of the is a relationship is set to be a specialization of entity class a ok or on the other hand um entities of class a are set to be generalizations of entities of class b right so what are the properties of um generalization and specialization have a look at um this slide  refer slide time  00  13  46  and um there are some interesting properties when we are talking about a generalization and and a specialization relationship now suppose um i have an entity of type car ok so so let us say this um this entity with with a particular registration number is a car that exist in the database now in the database i also have some entities of type vehicles now should this car entity type belong in the vehicle entity type if you um think about it carefully you will say that the answer is yes right because the simple reason is that a car is a vehicle if i have an entity that exist um in in the set of cars the same entity should also exist in the set of vehicles right so an entity can not exist in the database that merely belongs to a subclass it also has to belong to the super class as well right  noise  and subclasses undergo type inheritance of the super class ok what is meant by type inheritance of the super class again notice um care carefully here now let us say that we are going to describe some properties of vehicles ok now what kinds of properties can can we think of vehicles um vehicles will have wheels ok vehicles will have um i mean depending on what you call vehicle i mean you cou you could also called a rocket as a vehicle ok so assuming that we are only looking at road vehicles suppose i say that a vehicle is represented by wheels and it should have some kind of controlling mechanism it should have a driver seat or something of that sort ok and it should move ok  noise  now um you see that all of these characteristics apply to all subclasses whether it is a car a bicycle or um or a truck or a um van or whatever all of these have to move they have to have wheels they have to have some kind of a control mechanism whether it whether its a handle or a  noise  steering wheel or anything like that and so on right so all attributes that describe a general the the general class has to be inherited by the special classes ok the the specialized class each member of a subclass has the same attributes as that of the super class entities and participates in the same relationship types the the second aspect is also um is also important the um if a general class entity participates in a particular relationship type a specialized class entity should also be able to participate in the same relationship type ok that is um if i can use a vehicle to go from point a to point b i should be able to use a car to go from point a to point b or um i should be able to use a truck or or a or a bicycle or whatever right so um if there is a relationship that exist between vehicle and um let us say an employee using that vehicle ok you should be able to replace this vehicle with with any of the subclasses and the semantics should not change the the semantics of the entire database system should not become incorrect that s why said we are we are looking at road vehicles i mean its its its it s a matter of naming the particular um entity um a car is a road vehicle obviously if i also consider rockets and airplanes as vehicles um this this this doesn t hold anymore using a rocket let us say go from here to the moon but i ca n't do that using a car right so um so you you want be able to replace a subclass subclass entity wherever a super class entity arise ok so when you are coming out with inheritances and special generalization and specializations in your in your database you should be aware of the fact that um this type of this kind of replacements of um of general class entities with with special class entities should be possible that is what establishes a correct inheritance relationship with a in incorrect inheritance relationship  noise   refer slide time  00  17  56  so um here is another example suppose i say that manager is an employee ok or or a manager could also be an employer so um as you can see here the the inheritance relationship is depicted by a u um in in the relationship type that is that there is a straight line with a with a u which which represents an an inheritance relationship or a specialization relationship if you note that if this is an employee here ok and an employee is um is uniquely identified by the pan number the same pan number can uniquely identify the manager as well right so so what is that mean it means that any key attributes that uniquely identifies um that that uniquely identify entities of a super class or a or a or a more general class can also identify attributes of the special classes ok however um a special classes may may have some more attributes in addition to the key attributes that the they are the general classes have ok for example a manager may also have one more identification which says um for which department is a manager or what is a scale or whatever ok now in order to be able to um um identify one manager from a from another from another uniquely you may have to combine that attribute with with a pan attributes here ok so um so there may be other attributes um that form the key for the special classes but all the key attributes of the general class has to um has to be retained in the special classes ok so  noise  another example of type inheritance in in in this case its its more of a key inheritance the the key has to be inherited directly  refer slide time  00  19  42  the process of creating subclasses out of a given entity type is called specialization that is suppose i have a particular entity type suppose i have identified that um the the uod here requires vehicles ok now out of these vehicles i identify the require vans the require cars the require trucks and so on um um this and then i also identify that ok van is a vehicle and a truck is a vehicle and a car is a vehicle and so on ok so um i should be able to form what is called as an inheritance tree ok so this process is called specialization on the other hand its also  noise  possible that that we go in the reverse fashion we first look at the uod we we first um go through the company talk to people and see what is happening and and then we identify different entities we we see that the company uses cars the company uses um buses the company uses motorbikes the company uses trucks and so on the company uses vans and then once we are once we have listed all of these we start seeing relationships among them we say that all ok of these are vehicles and all of these share the same attributes as far as the company is concern ok and then we put all of them in an inheritance tree and and this is what is called as a generalization ok now before we going to the go to the next slide let me inter checked here to to note that this is not such a straight forward process um which um which entity is a is a special class and which entity is a general class is not such a straight forward process sometimes depending on the the usage context um which becomes a general class and which becomes a special class a specialized class may may change from um from one context to the other let me give a particular example ok take two entity types an airplane and a glider ok now which is correct the first one which says a glider is a airplane ok without engines or whatever a glider is a airplane ok or an airplane is a glider which is correct ok so if you look at it carefully let us um let us go back to what are the properties of specialization and and generalization classes ok the the first property of specialization classes is that wherever i am planning to use um the the generalized class objects i should be able to use a specialized class objects ok so is that always true wherever i use airplanes can i use gliders maybe or maybe not i mean depends it depends on the context ok and secondly whatever attributes that that that the generalized class has has to be inherited by the specialized class ok again this seems to say that um um let let us say an airplane has several different attributes it has engines it has wings it has um um um wheels it has controls and so on a glider also has all of them except that it doesn t have an engine ok so it seems to suggest that a an airplane is a is a glider is a correct one ok so so glider is more general and an airplane is more specific ok so because a glider has a smaller number of attributes and and an airplane has a larger number of attributes however look at it in this um um in the context of learning how to fly a glider or learning how to fly an airplane ok now if you see that um let us say i have different paragraphs about or different kinds of skills that i have to learn for flying an airplane and for flying a glider ok it could well be the case that depending on the sophistication of the airplane there are um there are some airplanes here where you do n't have to do anything you just have to go and um plan your journey and push a button and it will take you there ok um um with with all auto pilots and so on and so forth and so on ok so um so so depending on the context you may actually have to learn more to fly a glider than to learn um than to fly a airplane ok so if if the number of attributes are the different kinds of scale i need to fly this ok you see that the opposite  noise  um inheritance tree is is valid that is an airplane that is a glider is an airplane that is an airplane requires smaller number of scale sets to to to fly um while a glider requires a larger number of scale sets to fly therefore a glider is a is a subclass of airplane ok so as you can see that it is not such a straight forward thing to to identify is-a relationship so it it depends on the application context and we should not ignore the application context like we um saw in the previous session um um a database um can not ignore the information system context within which it is going to be done ok is the application context about building an airplane or a glider or is a application context about flying an airplane or a glider now that that much change the inheritance semantics in in our conceptual schema ok so um  noise  so so coming back to specialization and generalization processes  refer slide time  00  24  57  let us take a small um generalization example and and see how um we go about it ok let us say we have identified two two entity types in our back to our company database ok so um let us say we have identified an entity type called secretary and and the secretary is identified by a pan and salary and the the kinds of skills the the secretary has typing short hand or whatever so on and so far similarly we have identified manager ok and we see that um manager also has a pan number um a manager also has a salary and there is an experience field saying what kind of experience the manager has now when we see um two or more entity types sharing the same kind of attributes for a large extent that is out of three attributes two are two are similar here it gives us reason to believe that probably these two are special cla special cases of the same general class ok so we can  noise  generalized them something like that ok  refer slide time  00  25  59  so so we can create a more general class let us say um called called employee and then say secretary is a employee ok and manager is a employee so so note that um the the the attributes that where first a part of secretary and manager have have gone here that is only the common attributes between secretary and manager have moved up the hierarchy to to go to the employee class ok and all those attributes which are specific to to this specialized classes remain in the specialized classes that is skills remain here and um experience remains here right  noise   refer slide time  00  26  41  now in some cases it maybe able to we maybe able to identify precisely how to distinguish one special class to specialized class to another specialized class have a look at this um slide here this this slide shows an entity type called employee which is defined by attributes called pan and salary and one more attribute called job type which is not actually shown here ok now um um there are two specialized classes secretary and professor ok now suppose we identify um a property that every professor has a job type has academy and every secretary has a job type called admin ok so um each professor belongs to belongs to a category of academic jobs and each secretary belongs to a a category of administrative jobs ok so um so so we know exactly how entities of one specialized classes can be distinguished form entities of another specialized case so so this is how we identify this this here we say job type ok and then we say admin um is secretary and academic is is professor ok so such kinds of um definitions are what are called as predicate-defined subclass the this subclasses are defined by the values of one are more predicates that that exist in the er schema next we go to an example where um um we see that in in some cases not all subclasses may maybe unique ok now let us take take back the example of secretary and professor ok you see here that while denoting this subclasses we have we have drawn a circle with with a small d here ok now what is this d denote this d denotes the fact that these two subclasses are disjoined ok um um what what is meant by disjoint here that is they they are mutually exclusive no secretary is a professor and no professor is a secretary ok because all secretaries have to have a job type as admin and all professors have to have a job type of academic right so um so so so the set of all secretary is dis is a disjoined set from the set of all professors ok but  refer slide time  00  28  59  this need not always be the case sometimes two or more specialized classes or specialized entity types may actually overlap they need not be mutually exclusive from one another for example um suppose in in some university there is there is there are notions of chair professors chair professors are usually supported from from external sources of um external funding sources but for all practical purposes there are has they they they work as as any other professors here ok now it could well be the case that some professors are chair professors and um some chair professors are normal normal professors that is they they need not be supported by a project but but they also work in other activities and so on ok so um such kinds of inheritance trees are are subclasses are set to be overlapping subclasses ok so these two subclasses need not be disjoined from one another and this overlap this kind of overlapping maybe either partial or or even total overlapping ok now um you you might have you might notice that every chair professor is a professor but not the other way around and so on ok so so it s a total overlap as far as chair professor is concern ok on the other hand if there if there are some chair professors or not um um who are not teaching let us  noise  say who are not doing doing the normal activities of of a professor here then the the the overlap is partial overlap between them  refer slide time  00  30  33  the next kind of um generalization technique that we are going to do um see is what is called as a union type or there is also called as a category ok now have a look at this slide um um a little more carefully now this slide shows um entity type called account holder in a banking scenario ok now when when you ask a banker who or  noise  what is an account holder he will probably tell you that the account holder is is just an abstraction it is an entity ok it does not it does not necessarily represent a person ok because it may actually represent an institution an institution maybe an account holder or an or an individual maybe an account holder or sometimes in some um um in some cases accounts maybe held by families or sometimes dynasties and so on ok so um as far as the bank is concern um all of them are just account holders and they are they are just abstractions ok but all of them share all of them um have their own set of attributes and have different sets of characteristics obviously an in individual is different from an from an institution institution has characteristics like number of employees and so on which an individual may not um which may not make sense for an individual entity type right so um so each of these entities here in the in the top most in in the top run here may have their own sets of attributes which may not be in common with one another but all of them are account holders here right so um so each individual may have its its own different key for example a pan number for an institution and address for a family or um some kind of registration number for for an institution ok but all of them are account holders as far as a bank is concern so such a kind of relationship is what is called as a union type if you  noise  are familiar with programming in in c programming you have this notion of unions which has very similar co-notations that is an account holder is either an individual or a family or or an institution ok a union type is also called a category  refer slide time  00  32  51  and just like we saw in the case of inheritance where um in the in the case of subclassing where where a subclassing subclass could be either disjoined or overlapping we can have what is called as a partial union or a complete union ok now for example here not every individual that exist in the database could be an account holder an individual is an entity and an entity is is something which has its own independent existence ok so we may be keep in track of individuals for our own purposes ok but some individuals in the database could be account holders similarly we maybe keep in track of institutions for some other purposes but some institutions in our database could be account holders ok so when only a part of the the entity set of individuals form or participate in this relationship we call this as a um partial union on the other hand if every individual that we hold in our database is is an account holder or or participates in this union relationship then its it s a full union relationship  noise  ok  refer slide time  00  34  04  so that was briefly about um generalizations and and specializations and infact this is a very crucial concept in in in being able to obtain the notion of abstraction that is to to be able to abstract away unnecessary details from a special class and and go to the general class so so if you are able say that we are we are going to use an entity type of a general class it means that it has just enough details that is necessary for this relationship to exist ok that means if i say that um i need a car for this particular activity ok i do n't need to worry about what kind of a car is that what color of the car is that or or what is the horse power of that car or whatever its all this um all this attributes are specific to particular kinds of cars but um um for this particular activity any car would would do ok so so so we are essentially abstracting a way or covering up all the unnecessary details and looking at only the necessary detail what what is required for a relationship to exist right the next um um concept that we are going to be looking at here is the concept of higher order relationships until now we have been considering relationships with a degree of two recall that um the the degree of a relationship is the number of entity types that participate in this relationship ok now this slide shown here shows um a relationship with a degree three have a look at this relationship carefully it it says that the relationship is called supplies and it relates three different entity types ok the supplier part and project what is the relationship say or what is a semantics of this um relationship again there are three different entity types supplier project and part so basically it means that the supplier supplies this particular part for this particular project ok now um um if you think carefully it is not possible to reduce this this ternary relationship or or a relationship of degree three to um um any number of  noise  relationships of degree two ok a supplier may supply some parts but not all parts maybe designated for this particular project ok a supplier may supply for a project but you may not supply all parts that that are required for the project ok now a project may use a certain parts but um but not all parts that are used by a project or may be supplied by just one supplier there could be any number suppliers ok so so we can not reduce it to three binary relationships without losing meaning ok let us try to do that and see what what happens  noise  ok  refer slide time  00  37  01  now the closest possible binary relationship um um that that that tries to simulate this this ternary rela relationship is something like this ok a supplier supplies to a project a supplier stocks some parts ok and a part is required by a project or a or a or a project requires certain kinds of parts and um to be fair to be sure we also note that a part is a weak entity type it has no existence by itself it has to be either associated with a supplier or with a project ok so um even when you do that this is probably the closest we can come to simulating the the entity relationship but not not quiet close as you can see its it it can still it just because suppler stocks certain parts doesn t mean that this part will be required by this project or or or vice versa ok  noise   refer slide time  00  37  57  let us take another example of of higher-order relationships and um and see whether we can reduce it to lower order relationships without losing meaning ok so um um this slide here shows a relationship which which is a ternary relationship called offers ok so um it says that instructor offers a course during a semester ok now there are also other relationships that we have identified in the database and um which says that um instructor taught_during certain semester ok or instructor can_teach a particular course ok or a courses offered_in a particular semester ok now note that um if i have an instance of this relationship that is um offers i s c ok what is i s c means um um if there is an instance of this relationship called offers for a particular instructor in a particular semester for a particular course ok this implies that the the instructor has taught_during this semester and the instructor can_teach this course and the the the courses offered_in this semester right that is taught_during i s and can_teach i c and offered_in um c s that is um um an existence of this ternary relationship implies the existence of all this binary relationships ok however the converse need not be true the the the converse that is um suppose i have instructor can_teach instructor i can_teach course c ok and instructor i taught_during semester s and course c was offered during semester s ok but that doesn t mean that the the the same instructor has offered this course during the semester is ok instructor i can_teach this course ok um but and the courses offered during a semester and the instructor taught_during that semester but that still doesn t say that the the instructor taught the same course during the semester he could have taught some other course he you have um um um um and and this course could have been taught by somebody else right so so so while this is true the the converse is not true the the reducing if instances of binary ration relationships exist we can not um we can not be sure that the the the instance of a ternary rela relationship also exist ok  refer slide time  00  40  26  cardinality constraints on higher higher-order relationships so what is it mean when we say um um when we put cardinality constraints on higher-order relationship here is an example um  noise  this examples shows again the the instructor semester course example so um it has put a one here and a n here and a n here so so that means that um at given course-semester combination should have only one instructor ok that is um um in a particular course for a particular semester there has to be only one instructor on the other hand a given instructor may have any number of course-semester relationship that is given instructor can teach in any number of semesters and any number of courses ok so um again if you think about this carefully if i have a set of all this relationship types ok instructor course-semester and so on how do i identify an instance of this relationship type uniquely the the the the key here is the course-semester pair ok so if i take an instructor an instructor may offer any number of courses in any number of semester however if i take particular semester and a course you see that it can uniquely identify an instructor that is because every course and semester pair should have um just one instructor associated with it  refer slide time  00  41  51  the last concept that we are going to um um look at in this  noise  in this session is the notion of aggregation this concept is usually used in what is called as knowledge management or or km in in the in the concept of ontology s and and and so on ok so so an aggregation basically um aggregates a particular er schema and makes into an entity of at at a higher level of abstraction ok um note the um note the certain difference or or and and very important difference between the the kind of abstraction introduced by aggregation and the kind of abstraction introduced by inheritance or or specialization ok aggregation um um brings about the concept of composition ok or contains relationships ok so here this um slide shows an aggregated entity called offering ok which contains um one or more instances of the the the relationship um called instructor course and semester ok so an instructor offering a particular um course in a particular sema semester is called a course offering or or or an entity type called offering ok so the relationship between um the offering entity type and and this um relationship called offers is that of contains ok offering contains offers ok on the other hand the relationship between generalized and specialized classes is that of is-a relationship ok or rather the the the the between specialized and generalized classes ok a car is-a vehicle ok and and a bus is-a vehicle ok um um or monkey is-a primate and so on but um aggregation offers the concept of containment this contains this this contains this and so on so even aggregation brings about a kind of abstraction that is you are um you are covering up unnecessary details if i am not really required to know what is the structure this offering i do n't need to really worry about that ok so so this slide here um shows that shows the relationship between offering and offering that is one course offering requires another course offering ok so let us say course number um a requires or or has a prerequisite that some other course lets it said has to be taken up by the student ok so a course offering of a requires a course offering of z so um so so without z being in the database i can not have a course offering of a ok so um here the abstraction basically throws away all details which um that talks about what exactly and an offering is about ok  refer slide time  00  44  44  so um so so so with that um we we have covered the major parts in the enhanced er notation or or eer notation so before we conclude the session let us take up a small example of a of a university database and see how or or in which kinds of situations do we get these um inheritance and gen generalization and specialization how how do we go about identifying that um we might probably generalize here or even probably specialize here and so on ok now take up a small university database now now this example here is by no means exhaustive i mean we we can not build a um build a complete database in the in the course of a session like this but um i am we just trying to see what what kinds of what kinds of typical problems or typical kinds of issues that that can arise here  refer slide time  00  45  38  so some basic entity types so each universities has a student and of course several other entity types i am again i am abstracting away unnecessarily details that is students faculty members and staff and so on and so on ok so so let us say we have identified a basic as entity type called student ok  refer slide time  00  45  57  now um we we then go about um adding some attributes for students we note that each student is given a roll number which uniquely identifies the students as long as the student is in the university ok each student has a name each student has a gender a date of birth address and and so on ok  refer slide time  00  46  16  and then we go about looking at um other entity types let us say um um um oh we say that oh faculty is an entity type ok and then we we we we talk about what are the attributes that that characterizes a faculty member ok then we come out with some um some more attributes like this lets say each faculty has an employee number each faculty has has a name um um and and a gender and date of birth and address and so on ok now if you see um if if you see student and faculty they look quiet similar all ready ok  noise   refer slide time  00  46  51  then we identify let us say some kind of non teaching staff and then we see that even they have the the same kinds of um um attributes that is employee number name gender date of birth address and so on ok so which um um um tells is that we are actually looking at different entities of the same generalized class ok so um so so what kinds of generalization can we make out of these three different classes if you see um staff and faculty there is hardly any difference between um  noise  between the two um entity types but between faculty staff and student um there are certain um um um certain differences ok so um so so how do we identify these these these differences here ok  refer slide time  00  47  41  so so um this this brings us to a generalization and specialization tree ok we we see that faculty and staff can both be categorized as employees ok and they have the same key called called employee number on the other hand the same the the key called employee number can not identify a student a student is given a roll number ok so um employee and student do not belong to the um same level as a faculty but they they belong to the same um entity type called person ok now what is the properties of this person entity type every everything else that was common between the three entity types what are the common attributes between the three entity types name gender date of birth address ok so um so so so the attributes that going to person would would be all of these attributes name gender date of birth address and so on which which all of them share whether it s a student or a faculty or staff all of them share ok  refer slide time  00  48  40  similarly we can um we start looking at certain association let us say um a faculty works in department ok and a faculty heads department and we and we identify certain kinds of um association constraints that says that n number of faculty member may work in a department while only one faculty member may head a department ok  noise   refer slide time  00  49  02  and um we also identify some more associations which says that um n number of students maybe registered in a particular department ok  noise   refer slide time  00  49  13  and um um we we can also find some aggregations which which says that um a project um involves particular department or project is headed by a faculty member and and a faculty member belongs to a particular department and we see that this whole thing can can be aggregated into an entity type called sponsored project ok so um a sponsored project means that there has to be a project entity which is involving a particular department and is headed by a particular faculty member and so on ok so um how well schema um is is aggregated into the sponsored project  noise  entity type ok we can also see a certain higher-order relationships for example  refer slide time  00  49  53  um let us say some foundation um some organization or non governmental organization or or or whatever supports a particular project ok um and a particular department so so again we we we see here that foundation supports department on this project ok and we can see that um we can not reduce it to binary relationships foundations may support sponsored project and may support department but but the ternary rela relationship says that for this project and for this department this foundation is give is supporting ok  noise   refer slide time  00  50  30  so um and and have a look at this higher-order relation relationship here um um let us say um i have a relationship that says a faculty member collaborates with some some other faculty member on a particular project ok so so note the double use of this faculty entity type that is um this faculty member collaborates with this faculty member on a particular project ok and so one faculty member probably the the the head of the project may collaborate with n other faculty members on n other um um on on n different projects ok  refer slide time  00  51  09  now  noise  um you can reduce it like this that is um um a faculty member collaborates with other faculty members and the same collaboration extends to project  refer slide time  00  51  20  however you ca n't reduce it like this that is a faculty member collaborates with another faculty member and works on a project because we are going to lose semantics  refer slide time  00  51  27  so so that brings us to the end of um the the second session of um enhanced entity relationship concepts so before we conclude let us briefly go through the different um concepts that we learn today the first concept that we learned was about generalization and specialization will where um you achieve abstraction using an is-a relationship ok so um um um in a in a generalization and specialization relationship for any entity of the general class can be replaced by any entity of the special class or or or the specialization without losing semantics only then will you be able to say that my generalization is correct ok it need not always be correct um um just because something um looks like is-a would hold doesn t mean that the the the the the generalization is is correct ok so um we we saw the notion of inheritance that is each specialized class or a or a subclass inherits all attributes including key attributes and constraints and and relationships from the generalized class and we also saw the notion of overlapping subclasses and disjoined subclasses and um and and how to build a entity type using union types or categories and and we say the notion of higher-order of relationships and how they can not be reduced to lower order relationship without losing semantics and the final concept that we saw today was the the notion of aggregation which is again a kind of abstraction relationship but however which um um which establishes the notion of containment rather than is-a that is um abstracted by the the the specialization um relationship so so that brings us to the to the end of this second session on enhanced entity relationship concepts transcription  shobana proof reading  vidhya database management system dr s srinath lec  3 relational model ok um hello everyone we have been looking to the process of database design and let us continue with this in this session as well as we saw in the previous sessions um a database design goes though several different phases and we have been mainly looking into the conceptual design of a database a conceptual design especi essentially means a high-level design of the database or the database system which is mainly meant for targeting the end users that is trying to explain your database model to the end users today we are going to look at um another model of data which is called the relational model and how do we place relational model with respect to the the entity relationship  refer slide time  00  02  04  model that we have been considering until now um in order to answer  refer slide time  00  02  10  this question let us revisit our typical database design process that we saw in one of the previous sessions as we had seen that a database design process um is contained with in a universe of discourse that is a universe essentially is the information system context within which a databases design whether it is a bank or whether it is railway reservation whether it is you and your mobile phones in many of these different application context da databases are usually embedded so it is this application context that makes up the universe of discourse now once we analyze the universe of discourse we um essentially um get two kinds of requirements one was what was called as the database requirements as shown in the slide here and the other is what is called as the functional requirements um so the the database requirements essentially meant what are the data elements that makeup the system and how are they interrelated and and um how should we make sense out of the data elements and functionality requirement or functional requirements or the application programming requirements which say what kinds of processes have to run on this databases and what are the semantics of these processing s ok  refer slide time  00  03  24  so these requirements intern gave to two gave rest to two kinds of um parallel processes right the the database requirements gave rest to the conceptual design of the database and from the conceptual design came the conceptual schema and we saw in the previous class that um the conceptual schema is usually um build using the er model that is an en an entity relationship diagram now let us follow this upper stream that you see in the slide here  noise  a little bit further and see what happens to the conceptual schema  refer slide time  00  3  57  now the conceptual schema which is typically meant for um or essentially meant for communication with the end user is inturn going to give rise to the physical schema what is a physical schema the physical schema is essentially the schema that is actually build on the database system on the computer and as you might have imagine while the conceptual schema is oriented towards human understanding that is communicating your schema or communicating your design with the end user the physical schema is oriented towards machine understanding or essentially efficiency in terms of storage and retrieval of data elements so so the physical schema is optimized towards quick updates quick inserts um easy searches and and so on so it is one of this physical schema or or the building blocks of of such a physical schema is um is what we are going to see today and the the model that we are going to see today the the relational data model is pro is is the most widely used data model in most databases today and whether it is any kinds of application context whether it is banks or railways or telephone exchanges or whatever more any of these application context typically used um relational model to store data in the as as part of the internal schema structure ok  refer slide time  00  05  22  so what is the background of the the relational data model the relational data model was introduced in the nineteen seventies the the early seventies by ted codd from ibm research and in fact there is um um you can do a web search for ted codd today and many of his seminar papers that where where the database or or the relational model was proposed are actually available over the internet and you can actually have a look at them to to see what was why it was so influential before the relation relational model was proposed there where another models like hierarchical and network model and so on which were not very amenable to internal storage which are which are not very efficient in terms of storage and retrieval complexity and so on and the relational model was extremely elegant in terms of storage and updates and retrieval searches and and all these the main concept of behind the relational model is the is the notion of a mathematical relation you might have studied in course on discrete mathematics and mathematical relation is just a is just a mapping between two or more sets right so where each set constitutes a domain and a mapping between each of these domains forms a mathematical relation in intuitive terms a mathematical relation is no different from what we understand as a relationship in um in in normal english a relationship is similar is is is essentially some kind of an association um or some kind of linkage between two or more  noise  different data elements an employee is associated with a department an employee has a name associated with him an employee has a id associated with him an employee has a salary that s associated with him and so on and so forth  noise  so so its this its this mathematical relation um um let us say the set of all employees ver verses a set of all salaries you you can establish a mathematical relation between these two sets it is this set that forms the  noise  underlining basis for the relational model so this is the standard database model for most um transactional databases today  noise   refer slide time  00  07  29  so um um let let us go step by step into the nuances of the relational model so um what exactly is um what exactly are the building blocks or or the co essential concepts that makeup the relation ok so what is a relation um intern a relation intuitively represents a table as you can see in the slide here or its also called a flatfile of records this slide shows here a small table which as um three different columns the the first column is named as roll number the second column is named as name and the last column is named as date of registration and there are several different rows and each row corresponds as you might have guessed by now each row corresponds to one particular record or one particular student in this case so so there is a there the the first row has a student by name adithya and his roll number is two thousand three zero one and there is a particular date of registration now this is one data element one set of data elements that are interrelated so so one schematic data element now this row is independent of the second row which um um talks about another student called ananth kumar and with his own roll number and with his own date of registration and this intern is an independent data element that is independent of both the first row and the third row ok so each row in a table represents a collection of such related data values or what is called as an instance of the relation the the relation in this case is um um or the or the schema in this case is a table comprising of three different columns roll number name and registration an instance of this relation is one of these rows one which says two thousand three zero one and name as adithya and a a date of registration as twelve eight two thousand three ok so um each row in a relation is is called a tuple and each column is called an attribute of the relation ok  refer slide time  00  09  32  so let us delve little bit deeper into the relational model and um before we do that we need to define some um certain crucial elements in the in the relational model now um the the first definition we are going to comeback to it again um the the first definition in the in the slide talks about the notion of an atomic data type ok now have a look at the slide once um once again a data type is said to be atomic if it can not be subdivided into further values um um remember in one of the previous sessions we talked about um attributes in an er schema being either a simple attribute or a composite attribute a composite attribute is a non atomic attribute that is for example name if if the name of a person intern comprises of other attributes like first name middle name last name title um initials and so on um this is not an atomic attribute on the other hand the age of a person is an atomic attribute because you can not sub divide this attribute into um further sub attributes ok now this is important because a relation is defined only over atomic attributes as we will see in the next slide and the the next definition that we are going to consider is the notion of a domain we have already seen the notion of a domain in the in the er model and even here the notion of a domain is no different um a a domain is basically a set of atomic values which defines the space within which an attribute might obtain a value for itself for example we had seen the examples of the age of an employee which which may not be lesser than eighteen years and greater than sixty five years ok so the the age of an employee is a set of all numbers um maybe even fractions between eighteen and sixty five so so this constitutes the domain for this attribute called age similarly um um the there are domains for names and dates and and so on and so far this slide actually sho shows some examples like the set of all integers the set of all valid student roll numbers the set of all indian cities with population above six million or whatever anything that defines a space of possible values is called a domain a relation schema or a or a relational schema wi this is a crucial definition the the relational schema is um as you can see in the slide has a very specific um notation a a relational schema is defined by the the name of the relation here it is shown as r and a list of attributes here its shown as a one a two extra to until an ok so um so so this is the this is the name of the relation r in the previous example the name of the relation was the was called student record and the attributes are roll number name and date date of registration ok so each ai shown in this shown in this definition here is the name of is is the name of certain some attribute ok so or as shown in the slide here is the is the name of the role played by some domain wh which is an other way of putting the same thing name of an attribute or the name of a role played by a particular domain now um for example in the in the previous case their roll number the domain of roll numbers was the set of all possible valid student roll numbers now what is the role that this domain plays in this relation is um um is the attribute which is the part of the relational schema  noise   refer slide time  00  13  08  so few more definitions just like we um had that degree of relationship in in the er model we define the degree of a relation in a in a pretty analogies fashion it is simply the number of attributes in the relation schema for example in the previous um schema that we took the the degree of the relation was three it had three attributes roll number name and date of registration ok so a relation r or or what what is even called a set of instances of of a of a relational schema ok a relation ok this is different from a relational schema ok the the relation of a relational schema um is a set of tuples which belong to the schema or which can form to the schema along with the schema itself ok so set of tuples of the form t one t two extra t m were each tuple is um is of the form of of n different attributes  noise  or n different values one for each of the attribute ok so again in the example that we saw earlier we are it had three different tuples um there were three different student records um um with with three different roll numbers so so the entire set that is the the schema com plus the the set of all tuples is called a relation ok so a relation as you can say it can also be defined as a subset of a cross product of all this domains right so um the the cross product of the set of all roll numbers um times the set of all student names times the set of all registration dates that that are possible now a subset of this cross product is is what is going to form a relation now what is some of the characteristics of um of a relation in the relational model ordering of tuples tha that is um that that is one of the first issue that we are going to look at  refer slide time  00  15  02  this slide shows here three different characteristics of um tuples ordering of tuples mathematically the the relational model does not have any ordering that is specified over the tuples ok now it does not matter as far as the the mathematical model of the relational schema is concerned whether the the the the um set of all tuples are ordered roll number wise or name wise or whatever it is just a set of tuples that are um um in conformance with the relational schema ok so but but in reality of course they do have some kind of an order um which which is specifically the order in which they are stored um um on on on the disk whether they are stored in sorted order or sorted order are not it doesn t matter but they have some kind of an order in reality ordering of attributes note that a tuple you might have studied in um course on discrete mathematics a list is is something where the order is important a list is different from a set ok so tuple is basically a list of um n different elements which means that the the order of these n different elements is actually important ok so so if the relational schema says that um my my relational scheme is is called student record and i have these attributes roll number name and date of registration and if i have a tuple having three different values the first value corresponds to roll number the second value corresponds to name and the third value corresponds to the date of registration so so so the ordering is important ok but that is mathematically speaking infact um um um in reality though we can do away with um ordering within a tuple as well i can as as well dereference a particular attributes by its name i can as well say what is the roll number value of this relation what is the name value of this relation what is the date of registration value of of this relation and so on ok so um and values of tuples which is the third characteristic that we are going to look at today so like i mention before each tuple or each value that makes up a tuple is assume to be atomic in in nature and this is what is called as a first normal form assumption infact we are going to see in one of later sessions um that the the first normal form is just the the first step in a series of different normal forms in which the the database can be optimized for enhanced maintenance of the data what do we mean by enhance maintenance easy addition of data elements um easy searching for data elements easy updation of data elements and so on and this is primarily the reason um why it is stipulated that or why it is required that each data value that that makes up a relation are to be atomic in nature and atomic as we had seen earlier is something that s not composite that is some value that can not be subdivided into further semantic values  refer slide time  00  18  16  so just like um entity relationship model a relational model also are specified by certain kinds of constraints on the data model the one of the first constraints that we are going to look at is pretty obvious which is what is called as the domain constraints what is the domain constraints say each value of an attribute within a relation has to have a value which lies within the domain which is pretty obvious right so so the roll number of a particular student has to be a valid roll number that is it has to be it has to belong to the set of all possible roll numbers if the set of all possible roll numbers range from um um one to one hundred and fifty i obviously can not have a roll number which which is two hundred or i obviously can not have a roll number which says a b c and so on ok so um so so so the domain constraint specification basically say states that each data value that makes up a relation um has to be or has to belong to the domain in which it is the the domain of the attribute in which utilizing ok the second constraint in the relational model is what is called as the key constraint and this is quiet similar to the key constraint that you saw in the er model as well um in any relation in any relation there could be a subset of attributes that have a property that for every tuple in which those attributes appear they have a unique value for for those tuples such kinds of attributes are called super keys now what is the use of a superkey obviously to to be able to uniquely identify every tuple in a relation right in the previous example where we took um the at a relation having three different attributes roll number name and date of joining you can see that roll number forms a superkey because roll number is an attribute whose value is unique for every tuple in the relation however um you can also combine roll numbers along with the names and you can you can see that its its always going to be unique if the roll number is unique roll number plus name is obviously going to be unique ok so roll number plus name or or in fact the entire record can be called the the superkey for  noise  for for each record ok which brings us to a which brings us to a another property of the relational model in the relational model each tuple is distinct that means um um in the worst case the entire tuple is the is the superkey ok the relation does not allow multi multiple tuples having the same value in the in the sense that it is a set of tuples and not a multi set of tuples or or what is typically called a bag of tuples ok so each tuple are to be different in value from the other ok so a key is a set of tuples in which is defined in a similar fashion as in the er model a key of a relation is a subset of the superkey such that no subset of of a key is a key in itself ok if you remember this was more or less um um very similar to the way we defined a key in the er model itself ok so roll number in the case of the the student record is a key and if you take away the roll number you you can not identify or you can not distinguish one tuple from the other so um however roll number plus name is not a key in itself why because you can still take out the name of um um the name attribute from the key and you can still uniquely identify each tuple using just the roll number because the roll number is sufficient to identify each tuple uniquely in the relation ok so there are also called minimal superkeys  refer slide time  00  22  10  so um this slide shows the talk talks about what we saw just now that in the student table roll number and name is a super key however it is not a key or its not a minimal super key because you can still take away name and still um be left out with the key that is you can still identify each tuple uniquely in the relation or in the table ok so just roll number is a minimal or a or a minimal super key now just like we had see in the example earlier that a house can have more that one keys the the key for the front door and key for the  noise  back door um um any relational scheme or any relation can have more than one keys or so on take take an employee record and usually employees are given employee identification number and they they also have a pan number which which is given by the government now using either of these two you can identify an employee uniquely because each of them are unique for for an employee ok so each such key or each such set of subset of attributes which can uniquely identify tuples in in a relation is called a candidate key so um so either the employee number or the pan number is a candidate key in itself but usually one of those candidate keys are used for identifying um um tuples in a relation in a um in a company contacts its usually the employee number we do not usually identify people with that pan number when you are talking about um their their performance records or salary statements or anything of that sort we usually talk about their employee number now which your candidate key is used for for the purpose of um retrieval in quires and insertions and so on is called the primary key of a relation ok so in the student relation that we saw earlier usually its the the the roll number is what we use for for students ok so the the roll number is a good primary key  refer slide time  00  24  14  now um um just like their entity constraints as part of the er model there are certain constraints that make up the relational model as well the first um constraint is what is called as the entity integrity constraint ok so what is the entity integrity constraint it essentially says that um um which ever entity the now look at what what we mean by an entity here it s a it s a slight change of nomenclature what we mean by an entity here is a tuple in that in the in the relation ok so the primary key of a tuple can never be null obviously because we can not we wo n't be able to identify um um um each tuples um um um each tuple uniquely ok the the second integrity constraint that that s important in the relation model is what is called as a referential integrity constraint now what is a referential integrity constraint mean sometimes um some set of attributes um in a in a relation may point to certain other tuples in another relation we had seen such an example in in one of the previous sessions as well when we said that a a a department is headed by an employee who is the manager or or the head of the department ok so um that um um the the headed by is usually contains an employee id of the of the person who is going to head this department ok so here what we are um um um and assure that we are going to get here is what is called as the referential integrity here there is a reference from the department entity to the employee entity now this referential integrity constraint basically says that whenever i make a reference from any tuple to any other tuple it should make a reference to an existing tuple that means i can appoint somebody um i can appoint some employee with some employee id as as manager of the department as longer as um or only as long as such an employee already exist in the database ok so such a reference is what is called as a foreign key so in any given relation set of attributes is set to be a foreign key if the following rules hold the the first rule basically says that the attribute in foreign key has to be the same as um as the primary key of the other relation this is fairly obvious if i am going to say that the department is headed by an employee and um i write an employee id here but use employee name as as the name of the person who heads it it obviously is not a foreign key um um it has to be either both the primary key in the employee record and the reference in the department record has to be names or they both have to be employee id s in in some sense ok and for every tuple in the referencing attribute or or in the referencing relation like like like department the attributes in its foreign key refer to existing tuples that is each department should have a manager who exists who already exists in the database or there should be null that means there should not have a manager um at all so so either of these two should should hold for the referential integrity constraint to hold  refer slide time  00  27  36  so referential integrity is usually depicted in a in a diagrammatic fashion as shown in this slide here this slide shows two different um um two different um schemes or relational schemes one called the employee schema and the and the second called the department schema ok so so the employee schema has employee id name works_in and reports_to ok so employee id would be the primary key here and the works_in actually is a foreign key which which refers to the department id and reports_to is another foreign key which refers to another employee id in in within the same relation ok so note that foreign keys can be from a relationship to itself but the the same referential integrity constraints hold that is um if an employee a is reporting to employee b employee b should already exist in the database by the time employee a is being added to the database  refer slide time  00  28  35  and the last kind of constraint over a relational model is as the semantic integrity constraint an semantic integrity constraints is usually more of an application specific constraints something like the the age of an employee can not be less than eighteen years and greater than sixty five years so this is not um per say part of the the relational model but usually this is important to be implemented within a database context and and automatically checked and and verified that that these integrity constraints are maintained  refer slide time  00  29  07  now let us quickly look at what are some of the basic relational algebra operations that that make up the relational model now essentially the operations of relational algebra can be categorized into one of two different kinds of operations namely retrieval of um data or updates to the database now updates to da to the database are usually handled by what are called as insert and delete operations we should not be looking into insert and delete operations in any detail in this in this session mainly because there um the they they do n't have many very many properties um that we we can explore at this moment right so right now we we will be looking mainly at the retrieval operations and retrieval operations um are handled by two basic operations called select select is denoted by a sigma as shown in the slide here and the project operation which is denoted by a pi which is also shown in the slide here  refer slide time  00  30  11  so so let us go to the select operation um the select operation is a is a is a very simple operation that is used to select a set of tuples from an existing relation so remember a relation is basically a set of different data tuples along with the schema ok now given this set of tuples and schema we can use the select operation to select a subset of those tuples now this slide here shows an example which says select salary greater than three thousand from employee ok so as it shows here sigma salary greater than three thousand as a as a subscript and employee has the parameter of the of this operation so as you can see there is a there is a operation here there is a condition and there is a domain or or there is a relation over which the operation is going to be performed so this is going to select the set of all records from the employee relation where the the value of the attributes salary is greater than three thousand  refer slide time  00  31  15  so so the general form of select is is shown in this slide its simply as like this select condition relation so where condition is a conditional expression i have written a slightly formal grammar of how a condition looks like essentially condition is a logical expression over attributes names something like select salary greater than three thousand and gender equal to male from employee ok so so which basically says give me all male employees in this relation whose salary is greater than three thousand and so on ok  refer slide time  00  31  48  now what is some of the properties of um of the select operation the first property which you might have noticed here is that the select operation is unary in nature ok what is a unary operator a unary operator is something which operates on just one operand the select operator operates on just one relation um even if my database has many different relations the select operator operates on just one relation and we have to some how make sure that when we are giving the select operator we we have just one relation at the as as the argument of this select relation and each selection criteria the the condition basically that s that s specified is applied to each tuple separately the the condition that we specified here was select salary greater than three thousand from employee now its going to um apply this condition separately to each tuple basically it also means again that each tuple in in um in a relation is independent of the other and the degree of the relation that um that emerges out of a select operation note that the output of a select operati operation is a relation in itself because it has just taken a subset of the tuples from the given relation and um and return then along with the schema so um um the the input to the select operator is a relation and the output is also a relation and the degree of the output relation is the same as the degree of the input relation that is if the employee table in this example here had four different attributes the output of the select operator also has four different attributes and in the same order as well however the number of tuples return by a select operator is bounded by the number tuples that are that already exist in the relation that is the the this slide shows this as a in a in a very compact fashion the the cardinality of of the of the select um output relation is less than or equal to the cardinality of the select input relation and the last properties shows that select is commutative which is again um um quiet interesting and and important the this slide shows here that um if i select based on condition c one an output of a select based on condition c two for r i can as well replace c one by c two and and c two by c one and it doesn t matter so so you can verify that for yourself that um the the the the select operator is commutative whether whether it doesn t matter in which order i am going to apply the conditions  refer slide time  00  34  33  the second operator that we are um that that we are going to be um looking at is what is called as a project operator now select operator if you have absorbed carefully are is going to return entire tuples it is not going to modify the the the the schema of the relation which is given as an input for example if the if the employee relation is given as an input and the employee relation has four attributes in some particular order the same set of attributes in the same order is what is going to be return by the select operator on the other hand wha um what if we can select over columns rather than select over rows and return a different relation with with a relation with possible different scheme in order to do that we are going to use the projector operator the project operator as showed as shown in the slide here um um is quiet similar to the select operator um in the sense that it um it it has it has first the command called project which is deter which is um denoted by pi and a list of attributes here it shows name comma salary from employee ok so so the output of this relation is again another relation however with a different structure from that of employee that means it is going to return just the name and salary attributes are are the name and salary columns of the employee table as part of its output so we can also say that it it has projected the employee relation on to the selected set of list of operation  refer slide time  00  36  13  so so the general forms of the project operation is simply of of the form project attribute list and relation so so i can just give a list of attributes and and the project operator returns um relation in the same order that is being presented in the attribute list hence if i gave salary before name it would also return salary before name in the tuple that that that is return as part of part of the project operation so what are the properties of the project operation so um the the first um property is that um um which which basically is one of the main properties of the relational model  refer slide time  00  36  53  that is a relation may not have many duplicates that means um um when i am projecting let us say i am just projecting name and salary attributes from from the employee relation it may so happen that there maybe two employees with the same name and the same salary but with of course different employee identification or employee numbers ok so so the project operator actually would would start forming duplicates in in the relation that that that emerges out of this however duplicates are not alone ok so so the project operations removes duplicates from from its results when it returns results ok and the number of tuples return by project is less than or equal to the number of tuples in the specified relation how can you verify that the that the number of tuples is less than or equal to why not equal to  noise  because i am just asking for certain columns in the relation the answer to this lies in the first point that is um um there there are no duplicates so when i selected just the name and salary attributes from from the employee table it may so happen that their their their maybe certain duplicates that that exist in the um um in the in the output relation now because the duplicates are removed the number of tuples um in the output relation is actually less than the number of tuples that form the database that that was in the relation in the first place ok so when the attribute list of project includes the superkey then the number of tuples is same as the number of tuples in the um um as in the database which is which again follow from the first two points and the last point is again important the project is not commutative ok so have a look at the slide once more it gives an example project l one and project l two out of r ok if this were the case then this would become this would be equivalent to just saying project l one from r if if and only if l one is a substring of l two ok so for example if l one is just name and l two is name comma salary then um um i could just say project name from r instead of saying project name project name comma salary from r ok on the other hand if i try to do it the other way around then it becomes an incorrect expression ok so i ca n't project name comma salary after projecting just name from from the relation composition now this is another property of um um um of of the relational model if you notice again carefully we have mentioned this point in passing in when we looked at both select and project but this is going to be very important now the the input for the relational operator select as well as project is a relation and the output is also a relation the the select operators returns a relation containing only a subset of the tuples of the input relation similarly the project operator returns a relation which contains only a subset of the attributes of the input relation however both select and project return a relation now this brings us to a to a very important property that a relation can be dynamically defined it need not actually statically exist in the in the database that means to say that um i can um i can put a project operator to the output of a select operator and it it would still make sense because the output is a relation and and the project operator also expects a relation ok so this is what is called as composability of um of of the relational operators ok so relational operators can be composed as shown in the slide here which shows that which which shows project name comma salary as the as the outermost operator and there is an innermost operator called select ok  refer slide time  00  41  01  so um if you can look at the slide again there is a project operator here the project operator is operating on um um on obviously on a relation now what is this relation the the relation doesn t exist when when the project operator is um um is performed in fact it exist only when the select operator is performed that is once the select operator finish finishes it basically um um brings out a relation which is the set of all tuples where the salary field is greater than three thousand from the employee record or the employee relation and this set of um um tuples which is dynamically created forms the input for the project operator which is going to be another relation now because this is this is going to be another relation it can be very well assigned to um to to a relation called salary statements for example here ok so which says salary statement equal to project name comma salary from um where select um salary greater than three thousand from the the set of all employees ok now um before we conclude today um we will we will just look at one um um one one major question which i am sure you would be asking yourself now the the question is both project and select operators expect only one relation now does it mean to say that i can not ask any queries that that span more than one relation should i ask every query the over just the employee record over over just um the the department record i mean the the department relation or employee relation or so on can i not ask any question that um um that spans employee and department um relations together and so on so so the answer to this  refer slide time  00  42  56  is the cartesian join obviously ok so um um the um essentially what we have to do here is because both select and project operators require just one relation we have to some how ensure that even if you have more than one relation they they they all fall back or they they all combine to form just one relation ok so we are going to look at one such very rudimentary operator to to make um just one relation which is namely the cartesian join in fact there are other um um um much more efficient ways of combining um combining two or more relations which we are going to explore in the next session so the cartesian join as you might have imagine is very similar to the cartesian product between two sets what is a cartesian product between two sets you just take each element of one set and combine it with each other elements of the other set ok so in um um in the case of relations you just take each tuple of once one relation and combine it with every possible tuple of the other relation so the um slide here shows such an example there are two tables here one table is called student and the other table is called lab ok now student has um um student table has three different um um attributes roll number name and lab ok and lab attribute lab table it self has three different attributes that is name faculty and department that is the name of the lab the faculty heading the lab and the department in which the lab belongs to ok now consider the relational query here shown below the tables select student dot lab equal to lab dot name ok so that means to say that lab attribute from the student table equal to name attribute from the lab table from a cartesian product of student and lab that is combine student and lab to get it so so what is the output of output of this um um relation or or how how is this relation or how is this query evaluating  refer slide time  00  45  16  let us most straight to compute the cartesian join or the cartesian product of the two relations student and lab now what i have done here is um i have take the student um um relation that is the the student relation had um um three um four different students ok so for each student um i have i have combined it with each possible lab um um tuple to form one big relation ok so here it says um note how the attributes names are changed that is it becomes student dot roll number student dot name student dot lab lab dot name lab dot faculty and lab dot department ok now the the the query that we require to mach was student dot lab equal to lab dot name ok now all these um um quires that match are shown here in in in pink ok now it is these tuples that are going to be return ok  refer slide time  00  46  14  so so the result of the query is would be something like this ok that is were the student um working in a particular lab is it same as the name of the lab for whose record that we are we are maintaining ok so so so essentially we have what we have done here is that from two different um um attributes or two different relations student and lab we have made just one relation by by their cartesian product and then given it as um um um just um as just any other input to a select operator ok so so it has become just one relation as as seen in this slide here ok  refer slide time  00  46  59  the last slide that should be looking here today would be what what exactly are the properties of um of this cartesian cartesian join operator just like you saw the properties of project and select now um the cartesian join represents what might be termed as a canonical join between two um relations that is it just joins every relations from the first tuple to every other relation in this second tuple in the example here it just joined every student record with every lab record whether it made sense or not it was if if a human being read the two tables he would have um um he would have noted that many of these joints do not make sense that is um even though it says that the student works in a particular lab um um that record is um um is joined with um some other lab which has no relationship with with with with what the student is doing ok so um so essentially if um um the the number of tuples that come out of a cartesian joint is actually the product of the number of tuples that that exist in each of the relations that make up the join so hence cartesian join as you might have imagine is actually two inefficient for um um for for joining tables especially if you note that one table has ten thousand records and the other table has one million records and the the cartesian product would be one million times ten thousand records and and probably the output would be something which maybe ten records or so which which is not clearly worth it ok so um cartesian join is mainly of um theoretical interest in the sense that this is a canonical form of join operator by which we can um um we can join two or more relation to form just one relation because each um um um each operator requires just one relation as as its input ok so um in the next session we are going to be looking at um um several other join operators especially what is called as the theta join operator and see how it is how it is actually computed and how its going to be more efficient than than the cartesian join operator we are also going to look at certain more certain other relational algebra constructs and um um and and see how we can express the several different quires using the relational algebra so so to summarize what we saw today the the relational model is a data model for the the the internal schema of a database and and the internal schema is something which is oriented towards optimized performance on a computer rather than human consumption that is something that is meant for human beings to see and understand and so on ok and um the relational algebra an algebra is is some kind of formalism um over which we can we we can we can build sound software that is something that s based on a mathematical formulism can can be used to build sound software so comprises of um um operators that very elegantly take relations as input and produce relations as output and then you can start combining relations from one another and so on and we also just started to see how we can combine relations so that we can give just one relation as as an input that is required by each of the relational operators in the next session we are going to be looking at certain more sophisticated forms of combining relations which are much more efficient both in terms of space and and time require to compute um these these joins so this brings us to the end of today 's session transcription  shobana proof reading  vidhya database management system dr s srinath lecture  4 relational model hello and greetings in today 's session we shall be exploring relational algebra into some more depth we started with relational algebra in the in a previous session with the definition that relational algebra is a data model that is based around the mathematical concept of a relation let us briefly review what we have learned about relational algebra before going further on into some more concepts in the relational algebra um so just to revise what we have already seen  refer slide time  00  01  48  the relational algebra is based up upon the notion of a mathematical relation a mathematical relation as we saw before represents a mapping between two or more domains essentially one relation represents a mapping between two domains that relates one each element of um particular domain to some other element of another domain so for example i might have the set of all student names being related to the set of all roll numbers and a mathematical relation is a subset of this mapping that is the set of all valid assignments between um um between students and roll numbers so um this forms under underpinnings or mathematical underpinnings bet behind which relational model is based upon and as we also saw yesterday the relational model is mainly meant for for the design of the internal schema of a database an internal schema is meant or um is is meant or optimized towards machine consumption that is its optimized towards efficient storage retrieval and queries of of the data rather that human consumption that is trying to look at the data model and trying to understand what the data modal does or what the schema is all about so the building block of a relational model as we saw yesterday and its also shown in the slide here is a set of relations that that contains a set of attributes and each attributes belongs in a certain domain for example the set of all students belongs to the domain of the set of all valid student names similarly the set of all age um age attributes belongs to the set of all valid age for a given employee or student or whatever so um we also saw the notion of a relational schema which which describes how this relation looks like that is what are all the attributes that makes up these relation and what are the domains of the attributes that that that make up these relation an each relation is de referenced by a relation relation name and every instance of a relation that is every data element that conforms to this relation is called a tuple and each tuple in a relation is independent of every other tuple in the relation hence for example if i have one record of a student compressing of the roll number name date of registration date of birth and so on that constitutes a tuple in this in the relation called the student relation and each tuple that is each record about student is independent of the other records of of of other students and a relation by itself is um um is a combination of tuples plus schema that is given a particular schema and a set of tuples that conform to this particular schema is what is called as a relation as you saw in the previous class a relation is the input and the output for most of the relational algebra expressions like select and project which we had seen and what are the properties of relations the first property is about the ordering or the lack of it that is tuples in a relational schema or or in a relation need not have any order they need not be placed in any particular order there ja a relation is just a set of tuples that conform to a particular schema and um the second property was about duplicates traditionally or or the pure relational model does not does not allow for um duplicates of a tuple which means to say that the each tuple is unique when when you take a tuple in its entirety so we also made a statement that by default the entire tuples founds the super key for a tuple that is using the using the contents of a tuple you can identify each tuple uniquely in in the relation we are going to actually generalized on this concept today to see how things would change if you can allow for duplicates of the tuples and whether it is required or whether it is desirable to allow for duplicates to exist in a relation we also saw that the relational model is defined by certain kinds of constraints and the the the one of the first of which is is the key constraint so we defined a super key as as something which can identify a tuple uniquely and we also define a minimal super key or a key which which which says that no subset of which is also a super key in itself and we also saw the notion of a candidate key where two or more sets of minimal super keys can be used as keys and and the notion of the primary key and a foreign key when it comes to referential integrity constraints we also looked at entity constraints which says that the primary key of of a relation may never be null so you can not have a tuple in which the primary key is null because null is not a valid value for for an attribute it it basically says that the the the attribute is not applicable that the that there is no value associated with with null and we also saw basic retrieval operators select and project operators represented by the the greek letter sigma and pi and select is an operator which selects a subset of tuples from a given relation without changing the schema of the relation that is the the input relation and the output relation from a select operator have the same schema while a project operator um selects in a sense specific columns of of the input relation that is it changes the schema of the relation without changing the data in the in the tuple it does it or it may change the number of tuples if we are if we mandate the fact that the the output of the project relation has to be a set that is it can not be a bag or a or a multi-set that is each tuple from the output relation has to be unique so in which case the number of tuples that are return would be less than the number of tuples that already exist in the relation we also saw that both select and project operators require specific relation that is exactly one relation has input and provide exactly one relation as output now whatever it do when we have more than one relations on which we have to answer a query so we saw one possible solution to this that is to use the cartesian join or the cartesian product now how do we defined the cartesian product over relations remember what is meant by the cartesian product over sets if you have two sets a and b a cartesian product a times b or a cross b is is the set of all mappings from all elements of a to all elements  of b  this is the same definition for a cartesian product in for relations as well when we consider relations as simply set of tuples so um a cartesian join between two or more relations is the combination of set of all tuples from um from every relation to every other relation so a cartesian join as we saw yesterday is unnecessarily expensive in the sense that if i have m tuples in the first relation and n tuples in the second relation it needs to first compute m times n or m m n number of tuples to to generate a table which in turn goes as input to the select and project operators and from where selection has to be made clearly this is very inefficient for most operations on involving two or more tables so let us move on today to look at other forms of join operators which generate for lesser number of tuples than than the canonical join that that is represented by the cartesian join  refer slide time  00  09  26  the first join operator that  noise  we are going to see today is what is called as theta join operator this as shown in the slide here a theta join operator shows a join symbol which has essentially something like cartesian um product symbol with with two parallel lines and which which has a subscript called theta which is again shown in the title here so a theta join combines two or more relations or combines the tuples of two or more relations in a way that is specified by a join condition and the the join condition is um is um is specified by the operator theta so the the slide here shows a specific example where the the the same relation that we took yesterday the student and lab relation is computed using a theta join operator that is the the slide shows student in in a join operations with the lab condition that is the the student relation um theta join lab such that student dot lab equal to lab dot name ok so um student dot lab equal to lab dot name is um is the joint condition and the theta join operator is is the operator that is going to combine student and lab relations ok so so the this is the relation that that is shown here and as you can see the the output of this relation is the same as the select operator that we saw in the in the previous session that is select student dot lab equal to lab dot name from student times lab that is the cartesian join between student and lab ok the the only difference here is that the condition for this cartesian join is specified as part of the join con join operator itself now how many tuples does this generate  refer slide time  00  11  20  let us have a look at the student and lab tables from from the previous example the student table has four different tuples and the lab table has three different tuples now the cartesian join operator um um initially  noise  generated four times three that is um  noise   refer slide time  00  11  39  twelve different tuples as as input to the select operator on the other hand the um the the theta join operator starts with the condition that student dot lab equal to lab dot name is a prerequisite for computing the join between the the two tables so in this slide here such tuples where student dot lab equal to lab dot name is shown in a different color um um are shown in pink so as you can see there are only four such tuples that that match this condition  refer slide time  00  12  13  hence the number of tuples that are um generated as input to this um um select condition is just four instead of twelve ok so um how is the the theta join computed well the general answer is it depends it would be note that in this case it would be most efficient to compute this theta join operator if lab dot name were to be a primary key of lab that is the the lab is being referenced by the the name and student dot lab is a foreign key in to the lab relation and of course referential integrity is maintained ok so because we have to compute the equality student dot lab equal to lab dot name all i need to do is take up each student dot lab attribute and search for the corresponding tuple in the in the lab relation because it is a for it is a foreign key and lab dot name is a primary key this search can can be uniquely done efficiently using several techniques which we are going to see later like indexing or hashing and then you you you compute the join between the two relations  noise   refer slide time  00  13  25  so the general form of the theta join relation is shown in this table um or is shown in this slide its its simply two relations with the the join operator with a subscript join conditions so the the join condition is is simply a logical expression over the attributes of r and s which um which in the previous case we we saw was the equality condition that is student dot lab equal to lab dot name it need not necessarily be equality condition it could it could actually be um um any other logical condition thy like less than or greater than or so on now um it may so happen that in some cases the join attributes may be null which is also true even if there is referential integrity that is there there may be a lab dot name which is missing from from a student record now in such cases those tuples do not appear in the result that is um um whenever i can not combine two or more relations or whenever a particular tuple can not be combine with a corresponding tuple from the other relation such tuples do not appear in the final result  refer slide time  00  14  34  so some more properties of the the theta join operator we saw a join condition now which which sets student dot lab equal to lab dot name now the condition here or the logical operator here is the equality condition or the equal to condition now theta join operators in which the only comparison operator that is use is a equality condition is called an an equijoin operator or or an equijoin condition so what we saw actually was was an equijoin and theta join is more general in the sense that it could it could mean any other kind of attributes now a a special kind of equijoin is of particular interest and this is what is called as natural join between two or more relations so natural join is denoted by star as shown in the in the slide here it is an equijoin where some of the attribute names between the relations that are participating in this join are the same ok now um consider this example consider again the student and lab example now let the student relation be modified like this that is the the student relation has has attributes name roll number name and lab name instead of instead of saying just lab ok similarly we have modified the lab relation as lab name faculty and department instead of just name now as you can see here the lab name attribute or the the name of this attribute is the same between the student and lab relations now if i say student star lab which which denotes a natural join between student and lab relations it returns me a relation which is of this um this structure that is the first three are the the attributes of student and the last three are the attributes of lab and and the middle attribute here lab name is the common attribute between student and lab now it it is going to join only those tuples which where this common attributes match so this is a special kind of equijoin operator where not only equality condition is assume but its also assume which are the attributes on which the equality condition is operated upon so the natural join simply takes um um attribute names which are which are the same in the in the two relations and then computes an equijoin over them  noise   refer slide time  00  17  13  so the next operator that we are going to be looking at is what is called as a renaming operator we just saw here now that um suppose we modify student as um so and so or suppose we modify lab or so as so and so we can just use natural join now can we um formalize this notion of modification into the relational algebra itself ok can we introduce a notion into relational algebra by which we can we can say this relation is modified as this relation and then used in this expression so on now one way to achieve modification is is by assignment yesterday we saw that the input and output of select and project operators are both relations therefore the output of these operators can be assigned to a new relation name and then this becomes a relation by itself ok now this assignment can be further generalized such that we not only assign to a new relation name we also assign the attribute names of the new relation so this slide shows the idea here um the first expression here projects the following statements that is it it takes the student relation and then projects roll number and lab attributes of the relation now once you get a relation as an output it is in turn assign to to to another relation called ta or may be teaching assistant with the roll number attribute name replaced by id and lab replaced by lab name ok so so the output of this relation is is another relation with its own name in this case ta and it with its own attribute name that are that are different from the attributes names of the the incoming relation now um this renaming operator or this kind of renaming can be implicitly achieved without an assignment statement by using the rename operator which is identified by the greek symbol row which is shown below in in the slide here so the statement here the are the relational expression here shows that shows the same thing that um as the as the assignment statement above that is it projects roll number and lab from the student relation and then computes a rename or gives them as part of a rename expression that that says rename it as ta id and lab name ok so how does um the rename operator work in general  refer slide time  00  19  51  in general the the rename operator contains or may be defined by the following properties this is shown in this in this slide here the first form of the rename operator shows that the entire relation the input for the rename operator is of course um a relation and of and of course the the output is also a relation so the entire relation is being renamed that is it is given a name s and the the attributes are given names b one b two extra until bn the second kind of rename expression that is row under row subscript s operated upon relation r we will rename only the name of the relation that is the output relation is called s in this case and in the third case where row has the subscript of b one to bn within braces or or rather within parenthesis and the input is is the same relation r the renaming happens only on the attribute names and not on the table name itself that is the name of the table or the name of the relation remains the same and the attributes are renamed to to be called as b one to bn in this um um in this case  noise   refer slide time  00  21  08  the next set of operators that we we will be looking here are the set theoretic operators again the set theoretic operators here operate on relations rather than specific sets um as such now a relation is also a set but um um it could be multi-set or it it could have certain differences when we are talking about joins and so on and so far ok so the the set theoretic operations like unions intersection set difference they have to be they they can be apply to apply in the relational model as well now this set theoretic operations can be applied only to what are called compatible relations now what is meant by compatible relation between when when we are considering two or more relations that is can i compute a union operator between let us say a student relation and an employee relation or a student relation and a project relation now um some of these union operators may make sense and some of these may not make sense now that is why we we formally define the notion of compatibility or union compatibility between um two or more relations now what do we mean by compatibility the the the formal definition is shown in this in this slide here suppose we consider two relations a and b or in this case r and s and suppose they have a set of attributes let us say a one to an and b one to bn they are compatible if and only if first of all you might have already noticed that the number of attributes are the same that is the r has set of attributes a one to an and um s has a set of attributes b one to bn ok so firstly the the number of attributes are the same if there are have to if they have to be compatible and the domain of every corresponding attribute is also the same that is the domain of a one is the same as the domain of b one if if a one um um can span the set of all valid roll numbers b one should also span the set of all valid roll numbers and so on ok similarly for a two to until an b two until bn ok so so the set of all corresponding domains are the same note that there is nothing about there is nothing here about the names of each operators roll number could be called roll number in um in in relation a and could be called id in relation b it doesn t matter as long as the domains of each of these attributes are the same we we should be able to compute the set theoretic operations like union intersection and set difference now um assuming that we have two or more relations whose um which are compatible how do we compute set theoretic um um expressions the union operator r union s which is shown in the slide here it simply returns the set of all tuples that are present in either r or s or both and of course without any duplicates that is any tuple um note here that that entire tuples are are compared between r and s the entire tuples where the corresponding elements have to be same or or compared that is suppose i have one stu student relation or one record about a student in r and another record about a student in s it just combines both of them that is the output of r union s is is the set of tuples that lie either in r or in s or both and of course without duplicates similarly the intersection operator returns the set of all tuples that are present in both r and s which is the which is same as this intersection operator on sets and similarly the the set difference operator r minus s returns a set of all tuples that are in r but not in s so the set of all tuples that are unique to r but and and not present in s would be the output of r minus s so we can note that the this this standard properties of set theoretic operations also apply here that is union and intersection are commutative r union s equal to s union r and r intersection s equal to s intersection r however the set difference is not commutative s minus r is not the same as r minus s so we shall be coming back to these set theoretic operators again when we relax the fact that a relation may not contain duplicates now what happens if you allow for duplicates in the tuples  noise   refer slide time  00  25  43  the next operator that we are going to be looking at is the division operator the division is a slightly unintuitive operator in the sense that it it it needs a little bit of explanation to understand what are where a division is going to be used a division operator is essentially used in cases where we may have to identify data elements that are associated with some other data element whenever the other data element occur that is for for all properties of the other data elements have a look at this slide here which which shows a particular example now firstly there there is there is the definition here the division operator is used to denote conditions where a given relation r is to be split based on its association with every tuple in another relation s ok let me go straight to the example here and then go back to the explanation of division the example shows two relations r and s the r relation has two attributes a and b and the s relation has just one attribute a ok firstly the division of r and s is going to return the attribute b ok that is um the r divided by s is the the set of all attributes b such that there is some relation between the attributes in a so what is that relation the set of all attributes b contained in r that are associated with all values of attribute a of s that is suppose let let us take the the example of b one is b one associated with a one in r yes it is a one b one is here is b one associated with a two yes it is a two b one is also here and these are the only two values in s hence b one is a valid result in t div t equal to r divide by s that is every data element here that is associated with every other data element in um in the in the other relation is what is going to be returned as part of this relation consider an example something like um which employee has worked with some other employee on all projects that that he has work let us say which employee has worked with worked with some employee named arun or something on all projects that arun has work so suppose these were all the project that arun has worked ok and there is this employee b one who has also worked in all these projects you are going to get b one as as an output consider the case of b two here now b two is associated with a one but it is not associated with a two there is some a three which with which it is associated hence b two is not part of the result but b three which is again associated with a one and a two it is part of the result so the division operator in some sense divides the the first relation based on which data elements in the second relation um in in some way completely divides that  noise  that is is associated with all data elements with of the first relation we are going to look at an example later on with where where division operator is going to be used and and what is the power of the division operator  refer slide time  00  29  12  there are also other relational operators um um which are also called additional relational operators which which we shall briefly mentioned and and have have a look at their properties um we are we are going to mainly look at two such operators are namely the outer join and the outer union operator now going back to the notion of join recall that join or a theta join operator takes a join condition as one of its input now suppose any of the attributes which which match condition is is null then such tuples are not for um for the processed at all that is they are just thrown away from from the relation however in some cases it may be required to compute all possible joints even when the the join attributes are null and this is what is called as outer join so consider two relations r and s now suppose i i have let us say for the sake of simplicity we have defined um a natural join between r and s ok now every attribute in r which is participating in the joint should um should should point to or should refer to an existing attribute in s that is it should not be null and the attribute that it refers to should exist in s ok only then the the the natural join can be processed however there can be two possible scenarios the left outer join that is um suppose the attributes that are participating in join the attributes of r that are participating in join are null the left outer join includes such tuples even when the attributes that are are the referencing attributes are null similarly the right outer join includes tuples even when the referenced attributes does not exist ok so it basically replaces them with nulls and then just includes that part of the tuples that is that part of the tuple from r which which has data in it and and the rest of the data elements would be null so it is some kind of a union or a canonical um well i shouldn t say canonical but some some kind of a union operator where you you include every tuples any way its some kind of an inclusive operator or inclusive join operator where where it includes tuples any way even when the the corresponding attributes are null on the lines of outer join we can also define outer union operator now we saw that um the the union operator that is um um union between um um or an union or intersection or any set theoretic operators between two or more relations can be performed only when they are compatible now what did we defined as compatibility as the compatibility was was that both of these relations should have the same number of attributes and the domains of the corresponding attributes should be the same in both these relations now the outer union is basically a relaxation of this of this constraint and the the example shows two relations r x and s z that is x is a set of all um or the list of all um attributes of r and z is the list of all attributes of s ok now suppose x and z are not compatible however a subset of x and z are compatible that is a w are subset of x and y are subset of z are compatible now a union or or an outer union operator computes the union based on this compatible subset of these relations and then simply includes all other relations or all other attributes as they are in the in the relation so so so similarly the the outer union is the the outer union operator is um um um is simply some kind of a um um um some kind of an inclusive union operator that that includes um um tuples or in or works on relations even when they are not union compatible or even when they are not perfectly compatible between them so so so um theoretically speaking there are um do we need all these operators or or um can we express one operators from other or has a basis of um using other operators the this slide shows what is called as the complete set of relational operators now as you can see in the slide here  refer slide time  00  33  49  there are the following operators select project union set difference and the cartesian product now this set of operators is called the complete set of relational operators because every other kind of relational operator can can be expressed as a sequence of the above operators i am just giving two examples here but um you can verify that for for yourself taking each operators each operator and trying to ex express it as a sequence of the other  noise  one of the complete set of relational operators for example r intersection s can be or or is equivalent to the r union s and the difference of r minus s union s minus r ok so um um let us not go into the the set theoretic operation to to prove this um um equality but i am sure this is quiet obvious that you you can be able um you you can express the intersection using union and set difference similarly um joins can be express theoretically using using a select condition over um cartesian products we we already saw that in the example there were student dot lab equal to lab dot name or in this case here r join condition s is the same as select condition over r times the the first one here should be should be the join symbol and r join condition s is equivalent to select condition over r times s  refer slide time  00  35  25  now we are going to look at um  noise  making one generalization over or trying to relaxes particular constraint on relational expression or or on relations now until now we have been saying that relational operators should take relations that are sets and not multi-sets and return relations that are also sets and not multi-sets however in some cases it might be necessary or even desirable to allow for the existence of duplicate or duplicate tuples in um in relations both in the input and output relations now um first of all such relations or such sets where elements can occur more than once is called a bag or its also called a multi-set so bag or a multi-set is a set that may have multiple occurrences of a given element as we can say it s a generalization over the the the the present notion of a set that is every set is a multi-set in which um each element occurs exactly once however a mul multi-set is something where an element can occur more than once now in some cases bags are actually necessary in in um in the relational models not only desirable but also necessary when are these conditions consider the case that we are querying the database to compute the average marks obtain by all students in a particular semester ok so um so so so we have a student relation and um in which there is one of the attributes which is called as marks now we project this um this attributes saying project marks based on student ok now um based on the the the set of all marks that that that it has projected we compute the average mark by computing the sum of all these marks and divided by the total number of entries that are there now in this case if duplicates were actually to be removed we can not compute the the average in a um in a correct fashion we will actually be losing information when when we when we um when we change the multi-set or when we remove duplicates um and make it into a normal set ok so when we are computing aggregate relations like sum and average we actually need multi-sets or duplicates in in the relation the duplicate should not be removed from the output relation similarly um if we can tolerate duplicates in the relations somewhere while while evaluating relational expressions it may make things much faster for example every time i return the output of a project operations um i may have to spend considerable amounts of time um trying to um looking for each tuple and see and seeing whether there are duplicates of this tuple in the in the output relation so especially when computing projects and unions duplicate removal may take a significant amount of time now um if i have a relational query that has several um projects project operations and union operations and and and are embedded um somewhere deep in the in the query it may be it it may be terrible inefficient or to to be computing or to to be eliminating duplicates every time we compute project and union operator so some times it may be necessary or it may be desirable to tolerate bags or to to tolerate multi-sets as part of a relation as part of an intermediate output in a relation expression now how does this generalization from sets to multi-sets affect relational operator  refer slide time  00  39  09  now um consider two bags r and s ok now let in the first bag let a tuple t or occur n number of times and the same tuple t occur m number of times in r ok in in s ok so how do we define set-theoretic operations based on these bags the first operation that we define is called the union of bags or its also called the disjoined union of sets and it is denoted by r plus s you can also denote it by r union s when when we are sure that r and s are bags and not um and not sets so its simply denoted by r plus s and and its simply contains r plus s that is the cardinality of r plus the cardinality of s number of tuples that is um both bags are simply combined um every every tuple in in s is combined with every tuple in r and that s it that is it has m plus n occurrences of this this tuple t which which has been repeating now when you are compute the intersection of bags r intersection s what happens to tuples that occur multiple times so tuple t which occurs n number of times in r and m number of times in s occurs only a minimum of m and n number of times in r intersection s ok so as you can see the generalization here that is um in sets a tuple will appear in r intersection s only if it appears in both r and s here it will appear the minimum number of times it appears in both r and s ok similarly the set difference between bags the the set difference between r and s r minus s is where the tuple t occurs n minus m times that is n number of times it had occurred in r and m number of times it had occurred in s so so the number of tuples that is that is going to occur in in r minus s is n minus m if and only if n is greater than or equal to m if n is less then m then the number of times is going to appear is zero of course it ca n't appear negative number of times ok so so so tuple appears does not appear if the number of tuples in r does not out number the number of tuples in s you can think of it has something like canceling out tuples from from r and s so for every tuple in r we cancel out ever tuple in s and then see how many tuple are remaining in r after we have cancelled out all tuples in s and that is the number of tuples that we are going to take in r minus s  refer slide time  00  41  45  now what is the other operators on on bags the select operator does not change the select operator operates the same whether it is on sets or bags um um you just take a select condition and apply it to each tuple in the in the input relation regardless of whether the relation is a set or a bag similarly the project operator becomes simpler it it does change but it becomes simpler that is um a project operator simply takes the requested columns or or requested attributes and gives them out there is not need to eliminate duplicates similarly the the cartesian product of bags um its also the same thing it is if a tuple t occurs m times in r and n times in s as before then the tuple t s that is t combined when with s occurs mn number of times that is every tuple in r is combined with every tuple in s regardless of how many times they appear in r m in each of these relations  refer slide time  00  42  45  however there are certain algebraic expression on on bags that do not hold when the relations are tuples have a look at these algebraic expressions that that are shown in the slide here the the first algebraic expression is the distributivity over the the set difference operator that is r union s minus of t is is is the same answer is equivalent to r minus t union s minus t you can easily verify that this expression is true when r s and t are sets however the the this expression is not true or does not hold when r s and t are bags and not sets when they are multi-sets ok now um  noise  why is it not true let us take an example this is called the one one one principle ok now consider that a tuple t occurs exactly once in r s and t there there are some particular tuple or some particular data element that that occurs exactly once in r s and t ok now when we compute r union s the in the resulting set this tuple occurs twice if it is a multi-set or if it is a bag ok now once we consider um and then once we compute r union s minus t this tuple which we are considering now would have occurred once that is two minus one number of times once it could have occurred on the other hand look at the right hand side here when we compute r minus t this common tuple which had occurred exactly once is is not going to occur in r minus t its going to occur zero number of times similarly in s minus t this tuple vanishes it occur zero number of times so in the union between r minus t and s minus t this tuple does not exist at all whereas there is one occurrence of this tuple in the left hand side of this of this expression so so this expression does not work when r s and t are bags and not tuples and and not sets take this second expression the the the distributivity over intersection and union that is r intersection s union t is equal to r intersection s union r intersection t ok this is of course um easily verified when r s and t are sets however the the this does not hold when r s and t are bags and this can again be easily verified um by considering a specific counter example which is called the two one two principle i will not be going into detail into the two one two principle you can um use the same argument as we have used um um in the first case where we took the one one one principle that is consider a tuple that occurs exactly once in r s and t here consider a tuple that occurs exactly twice in r once in s and twice in t and see what happens and see um if um if the left hand side of this expression is equal to the right hand side of the expression as far as this tuple is concern and you can see why this relation does not exist does not hold or this equality does not hold the third um um expression is also significant when we are considering bags and not sets expression gives a select operator that is select c or d c and d are some conditions over attributes of over r so i am selecting c or d over r r that is select any tuple where either c or d or both holds and and give me all those tuples now if it where a set that is if r where to be a set i can rewrite this as select c union select d ok that is select c over r and union it with select d over r but the same thing does not work when r r is a bag and and not tuples ok now again you can take a very simple counter example to show the to show that this is the case now consider a particular tuple where both c and d um um are true ok now that tuple is going to be return only once in the left hand side of this relation but this tuple is going to occur twice in the right hand side of this relation because its going to be once from c and once from d and when when we are taking a union or disjoint union we are going to just add up both of them and its going occur twice in this relation hence this does not work when r is a bag so um tolerating bags is um is not only desirable but sometimes also necessary however bags pose their own unique problems when are um unique issues when we are considering set theoretic operations and algebraic expressions over over bags and which we have to keep in mind when um when we say that when we either decide to tolerate bags are are not tolerate bags in a nut shell we have covered quiet a few of quiet a significant um part of what constitutes relational algebra expressions and and um um what constitutes or how to write queries and relational algebra in the next three slides let me give a small example of relational algebra queries and um um and how queries can be composed from from one another  refer slide time  00  48  18  so this slides shows a very small um database schema comprising of five different relations employee department department locations projects and works_on ok so employee is a relation that talks about details of an employee it has the first name middle initials last name the employees pan number date of birth address gender salary the supervisor of that employee and the department number where the employee works and the the department contains department name department number which is the key here all primary keys are shown underline and the the pan number of the manager and the the start date of the manager department location um only shows each department numb number and the the location where it is um um where it is located and project shows project name project number location and and the department number where where the project is working and similarly works_on talks about this employee works on this project for this number of hours and so on ok  refer slide time  00  49  24  so let us take some typical queries very quickly and um and go through how we can answer these queries query one um the the first query which um which we are going to consider says that retrieve the name and address of all employees who work for the research department ok so how do we answer this query first of all we we we we take the set of all tuples that that form the research department that is select dname equal to research from department the set of all tuples which are the research department now um compute which are the set of all employees who works in the research department how do we compute that compute a join between research department and employee where the department number is the dnumber recall that in the employee record there was a dnumber here which showed which is the department number where the employee work ok  noise  so so compute a join an equijoin where this is this now from this we have got all details of of employees who work in the research department from that we need only the first name last name and address because that s what the query asked that is the name and address for employees so project as a last query  refer slide time  00  50  39  query two find the names of all employees who work on all projects controlled by department number five ok so have a look at this query again we we want the names of employees who work on all projects that are handled by this department ok so how do we go about answering this first of all let us find out what are all the projects that are being handled by department number five ok so department five project is the is the name of the relation which says project um the the project number um um and from and select from project where department number equal to five and project only the project number ok then which are all projects that employees work on take the pan number and the project number um and then work on this ok now what we have to do is that we just have to compute a division between employee project and department five project which is which which basically gives us the set of all pan numbers of employees who work on all projects or who are associated with all projects of department five which is the result that is we we we we in turn use that to to and comb combine it with the employee record to to return the first name and last name of of employees so um um in this way we can um um as you can see here for for any given query we ac we usually need to perform a series of operation series of relational algebra operations before we get to the final result  refer slide time  00  52  13  so let us summarize what we have um um learnt today in in a in a brief fashion so we saw the definition of um um um relational schema rela um um the the notion of a relation domains and attributes and the characteristics of relations especially with considering duplicates and um ordering of tuples and so on we also saw the the basic relational algebra um retrieval operations that is um select and project and so on and um set theoretic operations and relations and and also how this set theoretic operations change when um when when we relax the notion of the relation from being a set of tuples to a bag of tuples when when um when we can allow for duplicates we also saw why in some cases its not only desirable but also necessary to use bags so and and we also saw how we can given a particular user requirement how we can go about formulating a relational algebra query in a step by step fashion so that brings us to the end of um this session on relational algebra thank you transcription  shobana proof reading  vidhya database management system dr s srinath lecture  5 structured query language hello and greetings in the on going saga of management of data we have covered the aspects of high level schema design using entity relationship modeling and one ten of a data model called called the relational model which is mainly used for designing of low level schemas today we are going to look at something more concrete that is something which you are likely it be using on a day today basis if ever you are going to be working in um in in databases especially if you are a database user or database administrator this is the the sql language  refer slide time  00  01  54  or or the structured query language the the structured query language is the the standard query language that is used by most database management systems that are available today and and its its this is a language that is definitely required to be known by anybody who is going to be using a database management system so um let us move into structured query language today  refer slide time  00  2  19  now what is meant by a standard for query languages why do we need a standard for query languages i just mentioned that the structured query language is the de facto standard or or its rather these standard for relational databases that all databases or most of the database users today use sql in some form or the other or the other why do we a standard query language for um um database management now today at at this point in time in the in the database arena there are large number of database management systems that are currently available some of them are commercially available some of them are freely available and some of them have been implemented in academic institution some of them are have been implemented in companies industries and so on we have oracle we have ibm db two we have sybase microsoft sequel server mysql postgres and what not there are so many different varieties of databases now um suppose we where to um have a different query language being adopted by each different database management system it becomes extremely difficult if not impossible to be able to port application program from from one database to the other we will we will have to write software that specifically uses let us say mysql or software that specifically uses oracle and so on however with with a common query language like like sql all that the application programs need to know is sql they should be able to speak sql and um using sql we should be able to connect or use any um database management system so so this is what is depicted in this in this slide here suppose you have several different database systems in fact in in some application context you may have several dbms within the same application context a huge company for example may use um several different kinds of dbms systems in in in their different branches while all of them have to deal with the same application same work flow procedures  noise  or same um um business lo logic or business processes and so on so suppose we have several different databases as or dbms as as shown in the slide here um and suppose all of them are able to speak um um a query language like like sql all that the applications need to do when there are different application says that they should be able to speak sql and they should um they they it doesn t matter which dbms they are they are going to be working but reality obviously is is much more complex than these and um um while this was the reason why a standard for database query was introduced it is nowhere near to achieving its objectives that is um in reality however we still um see applications that say this this application is meant for mysql this is meant for oracle database this is meant for db two and so on because there are there are number of additions are there are number of other features in addition to sql that are unique to each different applications and so federating or um um or  noise  working across different connecting different database system is um a completely different question all together now and data integration is a is a completely different question which we will be um which we would be exploring in much more detail in in a later session  refer slide time  00  05  57  so coming back to sql sql stands for structured query language and this was introduced the the geneses of sql was at ibm research for database system that they had build called system r um just like seminal paper for cord you can give an internet search for a system r and there are of course some very good papers that are been that are available on the net that talks about earlier days of sql as you will see today sql contains a lot of constructs like select and set operators and cartesian products which are quite similar to relational algebra that we saw in in an earlier session and sql also works on a relational data model um very similar to relational algebra however sql has no relation to relational algebra or very little relation to relational algebra in itself the mathematical foundations for sql is is an other data model called the tuple relational calculus which is also another data model based on the the relational data modal that is the the notion of mathematical relations um but there are some similarities bet between in terms of terminology between what we have seen in relational algebra and and sql and sql is a database independent language and in this session we would be looking at construct from the sql two standard and sql three standard has has some more um facilities that that are that are provided and sql is both a data definition language and a data manipulation language what is it mean a data definition language or a ddl essentially defines data elements that means what are the data elements in a relational model of course a relation that is um um relation attributes domains values constraints um um keys and and so on so all of these can be defined using sql using sql you can define a relation you can define its attributes you can define its domains you can define constraints across attribute relationships key constraints and entity constraints and so on and its also data manipulation language in the sense that you should be able to add more data into a database you should be search for some data you should be able to retrieve some data you should be able to modify data elements and so on so so it s the both ddl and a dml  noise   refer slide time  00  8  30  so um in terms of terminology um while we use the terms relation tuple and attribute in relational algebra usually the terms table rows and columns are used for the corresponding terms we shall be using um either of these terms interchangeably as they they mean the same thing whether it is a relation or a table we saw in in a in the session on relational algebra that relation can be represented in the form of a table where each attribute of of the relation is a column which is what is return by the project operator and and each tuple in the relation is actually a row which is what is return by the select operator  refer slide time  00  09  15  the sql two standard defines um defines um methodologies for um for introducing schematic structures into our databases  noise  a schema is um is created by what is shown here as the create schema command so the slide here shows a small example which says create schema univ authorization dean ok so it creates a schema called univ which is owned by the user with a user id called dean we shall be looking into this authorization in much more detail in when we are talking about security and authorization in in databases where different users of of database management system are usually given certain privileges and certain authorizations by which um which authorizes them to do certain kinds of data manipulation operations on the database so hence if um if the owner of this schema is is this user id called dean then a dean is given certain kinds of authorization that is defined in a um by by some default values which which can also change here um um which which would typically include adding a table deleting a table adding rows deleting and and so on i mean any kind of activities that that a typical owner of a database would do a catalog in in sql terminology is a named collection of schemas so suppose i have a collection of different relations and combine them within um um within a particular name this is called a catalog and a catalog is required or it is required for different tables or different relations to be within the same catalog if i have to be able to enforce referential integrity on on my schema so using a scale you should you are able to enforce referential integrity only within tables that lie within the same catalog  noise   refer slide time  00  11  19  creation of a table table as you know is another term for relation and a table can be created with with the command create table command just like create schema for um for schema so the the slide here shows an example create table command of the form create table table name and some kind of column descriptions um the parts of the syntax enclosed within box braces are optional structures that means you can um um refer to the table name when the context is clear or um otherwise you you need to identify the schema within which the table belongs so so you sh you you can say something like create table univ dot department that means create table called department under the schema called univ and there are also certain column descriptions which we are going to see in in in a um um shortly which can say what are the different attributes that form the table and what are the domains of each of these attributes and what kinds of constraints that um each of these attributes have  noise   refer slide time  00  12  30  so before we look into column descriptions let us say what kinds of data types are also domains that sql two supports there are several different kinds of um um domains that sql two supports and some of the most commonly used domains are as shown here there are there is the numeric domain which is um identified by different data types called int small int float real and double precision each of these um refer to different sizes that are of of the um of the data word that is being stored as part of this domain there are formatted numbers like decimal i comma j which which basically means that this this number has i number of digits of which j number of digits occur after the decimal point so um hence if i say a decimal eight comma two it means that there are a total of eight digit in its in this decimal of which two digits the last two digits occur after the decimal point you can also define character strings of either fixed length which is shown by char of n or of varying length which is defined by varchar of n so i can say that this attribute name is a varchar of of say sixty so that means that this um attribute can store a string that can whose length can vary anywhere up to a maximum of sixty characters then there are then there are also big strings that you can store either a fixed length or or varying length and similarly a special data types like date time and time stamp and text and several other data types like binary objects and and and so on but typically we would we would generally be working with numeric or character string in a in a typical um um transactional database system like like employee records or railway reservation or whatever the the standard transactional databases that that we be considering here  refer slide time  00  14  34  you can also create your own domain name domain by using the create domain construct the slide here shows an example which says create domain roll number type as int of six if i know that the the roll numbers that i give to to students in our institute would would be an integer having six digits i can as well use roll number type instead of int of six wherever i need i need to store student roll numbers the advantage with this is that tomorrow i should if this um if this domain definition is changed i need to change it at only one place where um where i am defining the domain called roll number type  noise   refer slide time  00  15  17  there are also certain constraints and and default values that you can specify in using the create table command which which you can specify on an attributes for example the not null constraint the the first constraint is that we are going to take up is a not null constraint which says that this attribute name for which i am placing this constraint can never have a value called null um so it it disallows null as a valid value for um for this constraint for example i can specify that the age of an employee may never be null that means i need to always have a valid age for an employee record that is entered in this data value if i have to insert a particular row into the database similarly i can use um the default construct the default construct shows what would be the default value for a particular data item so if um if um if the not null is specified and i also specify a default value whenever i um let us say i i specify that age of an employee not null and default eighteen so whenever the um whenever the age of an employee is not known the default value namely eighteen in this example is um um is put in its place then there is the primary key constructs the the third constraint in this um um in this slide which specifies one or more attributes of of this tables as the primary key of this record similarly there is um um the the constraint called unique i can say for one or more attributes i can say unique which essentially says that this attribute has to have unique values or distinct values in each um um for for each row of this of this table in other words it means that it is an alternate key or a secondary key note that a key is something which can uniquely identify every tuple hence the key has to have um um a unique value for each tuple in the relation similarly i can also specify a foreign key construct that ensures referential integrity especially when some part of a table is actually referring to some other parts of some other parts of another table i can specify them as foreign key in order to ensure that they they refer to existing aspects of the other table so um just to brush up what is referential integrity if i have some um aspects of my table referring to another table it should either refer to an existing tuple or existing row in the other table or it should be null that is i should not refer to any row in the other table or i should refer to an existing row in in in formal terms this is what is meant by referential integrity  noise   refer slide time  00  18  16  so here is an example of table creation this slide shows um um an example table which says a create table employee the the name of the table or the name of the relation is called employee and then there is a set of attribute descriptions the first attribute called pan number the the the the permanent account number of of an employee is given a varchar that is variable character string of sixteen letters and it it is shown as not null and it is shown as unique that is this pan number may never be null and it has to be unique across different um um across each employee that is each employee has to have a distinct pan number if you go down the slide um um you can see that there is another construct called primary key um um and emp number that is the the second attribute employee number which is also shown as not null is um um um is termed as a primary key hence the pan number in this case because its unique can form a secondary key or or an alternate key of this relation then there are other attributes like name gender date of birth address salary and reportsto um which which basically shows the employee number of the the manager or the person to which those employee reports to so you um you also say another constraint called foreign key which says that reportsto the the field called re reportsto is actually a reference to another um record belonging to the same relation called employee um um and referring to the employee number in this record therefore whenever i i enter um a table and i enter a particular employee number for reportsto suppose i enter details of an employee and enter the details of the manager by specifying the employee number of the manager who to whom these employee reports to then because of the foreign key constrain that is specified here the database management system will verify whether a a record for the manager already exists if the manager record does not exist that is the employee with that um um with that reportsto employee number does not exist then addition of this employee record is will fail as part of the database management deletion of tables tables can be deleted the the terminology used for table deletion is called drop so tables can be dropped using the drop table command this is shown in this um um in this slide here you can use um um um the the syntax is something like this  refer slide time  00  21  05  drop table and name of the table and um um there there are certain optional attributes which says dependent cascade or restrict ok so the the first three terms are are obvious um drop table name that is um um the the table name by the given name should be drop what is the dependent clause or what what is it what is it do so if dependent is termed as cascade if i say drop table employee dependent cascade then um any foreign key constraints um that the table holds um or views that reference the table well we have not come to views as yet but the for for the moment let us let us just um um let us just not consider that but um but when i drop a table let us say employee any foreign key constrains will also be dropped so so so the dropping is in some way a cascading process because of table employee any foreign keys that that that the table references to or all going to be dropped in a cascading fashion on the other hand if if the restrict option is specified then a table is dropped only if it does not have any  noise  references incoming references that is only if nobody references the table only then will a table be allow to drop just like um table deletion you can also delete an entire schema using the drop schema command this is shown in the slide here um the the drop schema command also has a very similar syntax it it is like drop schema name and either cascade or restrict so drop schema and name is obvious that is you have to you have to drop a schema by the given name and if the cascade option is provided then all tables that that are there in the schema will be automatically dropped on the other hand  refer slide time  00  23  04  if restrich restrict option is specified then a schema is dropped only if it does not have any table or if if it does not have any elements  refer slide time  00  23  12  how do we modify tables how do we al alter an existing table definition ok so table definitions can be modified using what is called as a alter table command note that alter table or modification of a table is modification of the schema of the table not the data in the table that is we are we are not modifying existing data elements or or adding or deleting data elements from and to the table we are actually modifying the table definition or the table schema the table schema is simply a definition of the set of all attributes that form the table and and their domains so the slide here shows an example which which says alter table company dot employee add job varchar twenty so it essentially adds a new column to the to the employee table with the name called job and domain varchar that is a variable character string with a maximum size of twenty characters now um um suppose the table that i am altering has already containing um um is already containing some data that is i have created a table i have added certain data elements and i have used the database for sometime and then suddenly i give an alter table command and say add a new column like like job now what happens to um what value should should job get in in all of these tables because i have not specified any value as part of the alter table command you might have guessed it that it is going to be given a given a default value of null so so um um a new column called job is going to be created with a value of null but what happens if i specify a constraint called not null suppose the the the um slide here reads alter table company dot employee add job varchar of twenty not null ok now what should be the um um what should be the value that that um that has to be filled in for this new column  refer slide time  00  25  28  the answer to this is in the next slide here now unless a default condition is specified unless a default value is specified you can not use the not null constraint as simple as that that is if i do n't use a not null constraint um and the and and the query is is just like this that is which says alter table command company dot employee add job varchar twenty it just adds this new column with with all null attributes on the other hand if i wanted to specify not null than i should also use a default value which is what is going to be um filled for for all the data elements in this new column so i can say default employee or default um um shop floor or something like this so um so the the the default value called the shop floor is going to be filled for all of the elements in this in this new column which should later be change for for specific rows using using some other command which we are going to see later on  refer slide time  00  26  31  now we just saw how to modify a table by adding a column what do what if we need to delete a specific column the the the syntax is again quiet similar to that of deleting tables we use the key word called drop so this table here shows an example alter table company dot employee and as it to drop the column called pan number and there is an option called cascade now drop pan number is um um is is obvious that is the the column called pan number is is going to be dropped if the cascade option is used then all constraints that refer to this column are also dropped automatically that is if some other column refers to this column as in the in the form of a foreign key or or a or a view or so on then they are all going to be dropped automatically its its it s a cascading process similarly if restrict is used then a column is dropped only if there no incoming references to this particular column  refer slide time  00  27  41  it is also possible to alter a column definition um um rather than just adding and deleting new columns or column also has a particular domain and a particular constraint set set of constraints that that are associated with it now it is also possible to add and drop these domains or constraints that um that that refer to a column and this slide here gives um um certain examples the the first example shows alter table company dot employee and in turn says alter reportsto that is um alter the column called reportsto set default as zero zero seven so so what is this do this basically says that wherever the reportsto um um wherever the the reportsto column is null set it with the default value called zero zero seven so um um whoever whichever employee does not have any manager um assign him to the manager called zero zero seven which is which is what is is being set by this command the second command shows alter table company dot dot employee alter reportsto drop default which which is basically the other way around that is suppose it already has a default value then all those rows where this column has the default value are set to null and um and and the default value is going to be dropped from from this for this column  refer slide time  00  29  11  we now come to the main um operation in in sql the the most frequently used operation for for retrieval of data elements from um um from table from tables which is called the select operation we have not really seen how to add data into a table as yet but let us first um see how to retrieve data from a table and then we are going to consider how to add or modify data elements to and from a table so so the select operation is a most detailed um operation in in sql and is the most frequently used operation um um in in and it has the variety of forms which we are going to see in a step by step fashion so the the sql operation is the basic retrieval operation in um um the select operation is the basic retrieval operation in sql it has no relationship with the select operation in relational algebra just to reemphasis the point that sql is actually based on tuple relational calculus so um and we are going to see here that the select operation of sql can perform both select and project that that are are defined in relational algebra so and sql select one major difference between sql select and that of relational algebra is that it can um um it considers relations as a bag we saw in the session on relational algebra that by default relational algebra expects relation to be sets and when we convert them to bags we we have to take care of certain algebraic conditions with with sets which does not necessarily hold for bags but um sql by default considers tables to be bags and not um and not sets there there may be multiple occurrences of the same tuple  refer slide time  00  31  05  the basic um um syntax of a select operation is shown in this slide here um um it is very simple it says select attribute list from table list where a given condition so so there is an example which shows here select employee number comma name that is the the list of attributes emp number comma name from employee which is the name of the table where reportsto equal to zero zero seven which essentially means that show me all employees that is give me the employee numbers and names of all the employees who reports to a manager whose employee number is zero zero seven the select from where is um um is the is the basic operation that that we saw here and um um it can also act on multiple tables it need not act on a single table until now we have being considering one single table called called the employee table  refer slide time  00  32  03  now let us work with two tables just to show that select can act on multiple tables now let us first define a new table called department so this slide shows the definition of department that is create table department and which says where the first attribute is called dnumber which is also the primary key the the department number which is int of six and not null and name address and head that is name of the department address of the department and the head the the employee number of the person who heads the department now it also retrace the fact that head is a foreign key that refers to employee number from the employee database or or the from from the employee table  refer slide time  00  32  51  consider the following query what is the name of the person who heads the supply department ok so um um if you look back at the definition of department um we have seen that the the department contains department name address and head which is the employee employee number it does not contain the employee name um so so but but the query here requires the name of the person who heads the supplies department so so the name of the department is supplies and we require the name of the person so this can be specified by small um sql statement like this select employee dot name from employee and department where employee number equal to head and department name equal to supplies as you might have imagine this is quiet similar to performing a relational algebra select on a cartesian product of two tables in this um case there is a cartesian product of two tables employee and department and we are stipulating the fact that employee number equal to head in this cartesian product that is consider only those tuples where um where the head of the department corresponds to the employee number of record in in an employee and the department name equal to supplies ok and also note the use of that table name in order to disambiguate attributes having the same name now even the employee table um um in in the definition of the employee table the name of the employee is um um is specified by an attribute called name n a m e name in this in but in the same way in the department record as well in the in the department table the name of the department is also de referenced by an attribute called name now when we say select name which do we mean do we mean the employee name or the department name in order to disambiguate this we can  noise  we can we can preppened the the name of the attribute with the name of the table so so so the query here says select employee dot name rather than saying just name and then also in the where condition there is where department dot name equal to supplies so for some strange reason if some employee is called supplies that should not be matched its only the department name which which should be matched against supplies however this um disambiguating um um um attributes by prepending them with um with the table name is not always sufficient  refer slide time  00  35  40  consider the next query here now the query here um um says what is the name of the person to whom arvind kulkarni reports to now here is an employee called with name called arvind kulkarni and he reports to some some person now we need to know the name of the person um note that in a data in the in the employee table we only have the employee number of the person to whom each employee reports to ok so um um so so obviously we need to have a join of the employee table on the employee table itself that is you have to you you have to have a self join between um um for for the employee table um so um suppose we write a query like this that is suppose we we try to write or we try to disambiguate attribute names by putting the table names before them so so such a query shown here that is select employee dot name so so so we need employee dot name from employee comma employee because both employee and manager are both employees ok so it s a select employee dot name from employee comma employee where employee dot name equal to arvind kulkarni and employee dot reportsto equal to employee dot emp number obviously you see that there is something there there is quiet a bit that s wrong here you do n't know which employee um um which employee table are you referring to is it the first employee table or the second employee table  refer slide time  00  37  14  so this is still ambiguous um so in order to disambiguate this um um um attributes names in such a situation sql provides as with the opportunity of using what are called as aliasing so aliasing can be used as fo as follows now consider the same query shown in this slide here  refer slide time  00  37  36  so um um this slide for the for the time being concentrate only on the second line of this slide where it says from employee comma employee as boss ok so the the entire query is like select boss dot name from employee and employee as boss ok so essentially what it saying here is that take the first table employee and the second table employee however use an alias called boss for the second table so we know whether we are talking about an employee or his boss ok and then we say employee dot name where employee dot name equal to arvind kulkarni and boss report employee number is the same as the emp employee reportsto number ok so so employee dot reportsto equal to boss dot employee number so in this case we we will be able to identify which um um um  noise  which name are we referring to from from which relation  refer slide time  00  38  44  suppose we omit the where clause in the in the select from and where syntax and we just give a query of of the form that is shown here that is um um select name comma pan number from employee what is going to be the um output of these as you might have imagine this um um such a select statement is similar to the project operation in re relation logy so um um what this statement does is it returns all rows in the table called employee however only the columns name and pan number so it is similar to saying um project name and pan number from employee and what happens when the where clauses omitted and instead of saying just one table name we we actually specify more than one table name this is shown in the query here it says select employee dot name comma department dot name from employee comma department that s it  refer slide time  00  39  43  now what happens here in this case of course we we get only two columns as output that is employee dot name and department dot name however we get all possible combinations of employee dot name and department dot name in other words we have computed a cartesian join or a cartesian product between employee and department with this with this operation  refer slide time  00  40  07  suppose we want to select all columns of um of a particular table um that that is similar to the select operation in relational algebra we want to select all or or or entire tuples um and based on certain conditions in such a case you can use the um the term called star as shown in this query here it says select star from employee where name equal to bhadriah so um so essentially the the this query is similar to the relational algebra expression which says select or sigma name equal to bhadriah from employee that means the entire row or the set of all attributes of relations where the name attribute is called bhadriah is going to be return similarly if i say select star from employee comma department in the second query that is shown in the slide here um it computes the complete cartesian product between employee and department  refer slide time  00  41  13  we now come to the fact that how um um um tables are treated in sql in relational algebra we have seen that by default relations or tables are considered to be sets on the other hand in in sql tables are considered to be multi-sets or bags that is multiple tuples having the same values are tolerated we have also seen um why this is sometimes not only desirable but also necessary it is desirable because um um it is expensive to remove duplicates suppose i um i return a query with with ten thousand records of which um the their their may be hundreds of duplicates i need to perform i need to first sort each of these um um the this whole set of records and then remove duplicates and then reorder the records in whatever order that that the user has asked for therefore it is very difficult or its a its an unnecessary over head to to remove duplicates therefore it is desirable in many cases to to tolerate duplicates and in some cases its actually necessary to tolerate duplicates we have also seen examples of these suppose i wan t to compute the average marks of of all students in a particular course it is not only desirable but it is actually necessary that i retain the duplicates because the  noise  the duplicates all contribute to the um total number of marks um which which have to divide by the total number of occurrences to to find out the average marks so for computing any aggregate properties i need to have duplicates however if i in some cases if i want to remove duplicates explicitly from  noise  the output of a query in sql you can give the clause called distinct as part of your select statement the um the the table the slide here shows such an example it says select distinct name from employee so which simply says that show me the set of all distinct names that the employees have so if two or more employees have the same name then they are shown only once in this um um as as part of this query  refer slide time  00  43  32  similarly um one can perform several set theoretic operations like union intersection and set difference using sql so union is um um is operated by by using the clause called union and intersection by the clause called intersection and set difference by the clause called except as shown in the slide here now by default union intersection and except assume that the the sets that they are operating upon are are actually sets and not multi-sets so um so note the use of distinct in this in this examples the example here says that select distinct name from employee where salary is greater than three lakh union select distinct name from employee where salary is less then twenty four thousand so essentially what its doing is that  noise  it is selecting the set of all names of um um employees who are earning more than three lakh and combining them with the set of all names of employees who are earning less than twenty four thousand and there um um and duplicates are removed in these sets of name so the union operator assumes that um duplicates are removed when it is performed in the union of these two sets  refer slide time  00  44  55  on the other hand if i want to tolerate duplicates or if i want to specify that the the the sets are actually bags and not and not pure sets therefore i need to perform a disjoint union or a disjoint intersection remember what is the disjoint intersection of two sets if a tuple or a or a if the data item occurs multiple times in an intersection for an example it has to occur the minimum of the two number of times so um if i have to specify that i am actually working on bags and not sets i need to specify that with the with the key word called all which is shown in the slide here so um so if i have if i have not specified the distinct construct in my select statements as as in my previous examples i should use the term union all for disjoint union and intersection all for um um um bag intersection and except all for difference or set difference between bags  refer slide time  00  45  53  we can also perform comparisons over character attributes especially string attributes by by comparing um partial strings or comparing wild cards so this slide here shows two such examples the first examples says select star from employee where name like percent arun percent ok so so note the use use of firstly the key word like and secondly the the use of the percentage symbol ok so a a percent symbol matches any number of characters um um wherever it it occurs therefore um  noise  this this query here matches employee name where the employee name contains arun as a substring a r u n as a substring and where it may be preceded by any number of characters and and succeeded by any number of characters so while the percent symbol matches any number of characters a single character can be matched with the underscore symbol so um so suppose i if i had said where name like underscore arun underscore underscore ok so um so it essentially looks for one character before arun and two characters after arun so um any kind of character any kind of name which where arun occurs as a substring with exactly one character before it and two characters after it  refer slide time  00  47  22  one can also specify arithmetic operators like addition subtraction multiplication and division so i can say where salary plus perks not greater than fifty thousand or or so on and i can also use these um um arithmetic operator not only in the where clause but also in the select clause so have a look at the example shown in the slide this slide shows um um a query which says select one point one times salary that is one point one into salary from employee where salary greater then three lakh so which basically says that um show me what would be the figures salary figures of um of employees if salaries where to be raised by ten percent effectively that is i am um i am multiplying existing values of salary by one point one and showing them showing that as the result  refer slide time  00  48  21  so this brings us to the end of the first session on um um the structured query language where we have looked in to the basics of um what um what makes up the structured query language and um we we have seen how to create a schema using sql and um um what is a catalog that is a that is a collection of tables and how to specify the the um structure of a table by by specifying the name the the the the attribute names the attribute domains the constrains on the attributes like not null unique default values and so on and the the key constraints like like the primary key which which identifies what is the primary key in this and and also the and also referential integrity constraints like using the foreign key constraint we have also seen um um how to alter schemas and table constructs or or table structures and what are the implications of these constraints on um on these modifications that is what happens if i drop a particular um column name but that column name is actually referred to as a foreign key from some other table now in such cases i can also um um specify whether to drop all foreign key references or um to drop this column only if there is no foreign key reference coming into the table so using either the notation of cascade or restrict we also saw the most widely used operation in the in relational algebra namely the select operation however we have not finished um um looking into the different forms of select um operators in the next session we would be looking at some more features of select as as an how to nest um select operators and how do we dereference  noise  or how do we disambiguate attribute names in in the nested select operators however as we have seen because select is the most widely used um um um sql operator or sql statement it has um varied number of forms and and several different notations and one of the main properties that we have to remember about the select operator is that the select operator treats tables as bags or as multi-sets rather than as sets that is it tolerates duplicates in the sets unless we specify explicitly that we do not want duplicates and this is specified by the distinct clause so um so so to summarize the the slide here shows the summary of what we have seen today we have seen an introduction to the sql standard tables attributes and and values and we we saw how um schemata schemata are created and tables can be created and constraints and and essentially the select operation in its different forms that brings us to end the of this session thank you transcription  shobana proof reading  vidhya database management system dr s srinath lecture  6 structured query language ii hello and greetings um in our on going explorations of databases we had started out exploring the sta um the the default or the standard for database querying the namely the structured query language or sql um so let us continue with sql um  refer slide time  00  01  36  in this session today and look at some of the more advanced aspects and um what what kinds of queries that that we can express within simple sql statements so before we begin let us summarize what we have studied until now in um in terms of sql  refer slide time  00  01  53  now we looked into the an introduction to the sql standard and what are the building blocks of um sql namely tables attributes and values which or tables rows and um um and columns and we also saw how to create schema in um in a in sql and what what is meant by a schema and how do we create tables and what kinds of column descriptions can we give when we create tables and what kinds of constraints can be spe can we specify when um when creating tables and so on we also saw how to specify entity constraints specifying what is a primary key and what are the secondary keys in a um in a relation or a table we also saw how to specify constraints like something should not be null or something should should have a default value and we also saw how to specify foreign key constraints um so that referential integrity is maintained and um um we also started by looking into  refer slide time  00  03  01  the different forms of the select operation which is perhaps the most frequently used sql operation for for retrieving  noise  tuples from the database so so the most generic form of select operation is the select from where clause um where you say select attribute list and say from table list where a given condition holds true we also saw how we can disambiguate attribute names especially when we would using multiple tables and two or more tables have attributes with the same name one simple way of disambiguating ar attributes names is to prepened the name of the table before the attribute name so so we saw something like instead of saying name we say employee dot name or department dot name and so on so on the other hand we can also use aliasing for disambiguating names main mainly because when  noise  um um when when table names are also the same when when we saw the example of a join between employee and employee tables so we we we basically use the notion of aliasing where you say use the term or use the table employee and call it as some other name like say e or boss or whatever then we also saw how to how we can select from multiple tables the the cartesian products of multiple tables or um um and um  noise  what happens when we omit the where clause and so on and we also saw the the different kinds of set operations on tables by default in sql and very unlike relational algebra in sql tables are treated as multi-sets or bags that means um um it can tolerate or it is valid to have different tuples of the same um having the same data in a in a sql table which is not valid in or which is not valid by default in relational algebra so um we can um we can make a table into a set by using the keyword called distinct so when we say select distinct some  noise  query a query is return with all duplicates removed from the query we also saw how to perform set operations on um um on tables and sets by default these operations by default assume that table are sets um um like union intersection and except for um for set difference if we have to specify that the tables are not sets but in instead they are bags we have to specify you have we have to use the um qualifier called all so we we say union all whatever so so we say employee union all department and so on or employee union all managers and so on and we also saw how to compare substrings using the the the percentage symbol or or the underscore symbol with with which which matches a single character and we also saw some arithmetic operators like addition subtraction um multiplication and so on which need not just be used with in the where clause but they can also be used um right after the select clause itself  noise   refer slide time  00  06  21  so let us move on further today and um um in today 's talk i will be using two example tables in order to illustrate several features of sql so let us look into these tables once again the first table that we are going to be considering is the employee table so um the employee table contains employee number which is the second field shown in the slide here as the primary key it also has a secondary key called pan number of the employee which obviously has to be unique and non null and employee is given a name gender date of birth address salary and reportsto which is a foreign key that is reportsto contains another employee number which um which refers to the manager to which the this particular employee reportsto if reportsto is null then it means that this employee is the head of the company or that is he doesn t report to any other um employee then there is a department number or the dnumber which which shows where the employee works in the second table that  noise  we are going to be considering today for our examples is the department table the department table as shown in the slide here  refer slide time  00  07  39  um is indexed by um or uses as primary key the the field called department number or dnumber which is which is the same as the dnumber used in the employee relation  noise  that that means the same domain department also has the name address and head which which is again a foreign key  noise  which points to the employee number of the employee who heads a particular department so here are the specific declarations which says primary keys is is dnumber and head is a foreign key which references employee number in the employee table  refer slide time  00  08  17  so we had started out with looking at arithmetic operator so so let us continue from from that point on and we had seen how um you can use plus minus star and slashes to to perform arithmetic operations namely addition subtraction multiplication and division similarly you can use the operator called or you can use the keyword called between to check for um a range that is to to to check for to check whether um a given parameter lies within a given range the slide here shows an example which is of the form select star from employee where salary between three lakh and four lakh therefore this query returns all employee records select star in this case note that the star here is not is not the multiplication operator but star here refers to the entire tuple in this when when star occurs by by itself therefore this query selects the entire set of tuples the the entire set of columns for all tuples where the salary field or the salary attribute is between three lakhs and four lakhs it is also possible to sort the output of of a given query using one or more parameters  refer slide time  00  09  41  the sorting is achieved by um um a new construct called order by this construct is shown in this slide here this slide shows another small query which says select star from employee where salary is greater than three lakhs and order by name and employee number therefore this query returns the same result as the previous query that is the set of all tuples the the complete tuples for all employees well not exactly the same of course um um for all employees whose salary is greater than three lakh but not necessarily less than four lakhs and the output here is is ordered first by name and next by employee number so that means that um um it is first ordered the the records are first ordered by name and wherever there is a tie that is two or more employees having the same name um um it is then it then orders those tuples using employee number as as the ordering attribute  refer slide time  00  10  39  we now come to the next aspect of sql querying perhaps what gives it um um um what gives it compositionality so so to say that is by which you can compose um um queries bigger queries from from small queries this is what is called as the nested query  noise  note that just like in relational algebra the output of an sql statement is a table especially the select statement the output of a select statement is a table and the input is also a table it can be multiple tables which which are treated as a cartesian product in in which case therefore the output of a particular query can be used um um um as part of another query to to to to perform further searches so the slide here shows a shows such an example where um um a query has two different parts in it for the sake of convenience they are shown in two different colors white and yellow there is there is an outer queries so called outer quer query which says select name from department that is select the name names of departments where dnumber in so in is a new um um keyword that we are also introducing here which um which essentially stands for set membership that is where the dnumber or the department number belongs to the set of all tuples that are returned by the query which which shown in yellow that is um where dnumber in the query called select dnumber from employee where salary is greater than three lakhs so let us analyze this query wha what does um  noise  this what does this query do firstly um taka a look at the inner query the inner query says select department numbers of all employees whose salary is is greater than three lakhs and the outer query is saying select the names of those departments which are returned by the inner query so um essentially this query is asking the database show me all departments which pays greater than um three lakhs as a salary so the highlighted query the the yellow part of the query here is called the nested query and the non-highlighted query is called the outer query in this in this case  refer slide time  00  13  11  so um let us continue with um with nested queries and note that a nested query contains two different queries and the inner query or the or or the nested query in this case returns a complete relation or returns a complete table for all practical purposes we can um consider the table as sets or bags co comprising of different tuples um and um the the in clause that we used in the previous slide um performed exactly that that is did exactly that that is it considered the table that is returned by the inner query as as set and um and essentially performed a test for set membership that is whether the dnumbers specified in the outer query belongs to the set that is that is run in the inner query similarly um one can um one one can do a check for a particular attribute against all elements in a set using the keyword called all this is shown in the slide here um um the outer query says select name from employee where salary is greater than and then the inner query begins this is shown in the slide here so the inner query here um says select salary from employee where supervisor equal to zero zero seven ok so let us try to analyze the query step by step first take the nested query or the inner query here it says select salary from employee that is give me the salaries of all employees where well it should actually be reportsto there is a there is a small bug in this in this slide where reports to equal to zero zero seven that is give me the salaries of all employees who report to zero zero seven ok so um um we just want to know what is this employee number zero zero seven or or or agent zero zero seven pays all of his um um subordinates and then the outer query says select name from employee that is give me the names of all employees where salary greater than all of this inner query that is whose salary is greater than the all of the salary that is um um that that is paid by zero zero seven that is um it is greater than the maximum salary that is paid by zero zero seven so it returns the names of all employees whose salaries are more than the salaries of all those who report to zero zero seven  noise   refer slide time  00  15  45  now um what happens to um disambiguation of um attribute names when when we when we are considering nested queries so this slide here shows such an example firstly the rule which says that any unqualified attribute name in a nested query applies to the inner most block of the query so what does it mean have a look at the query here there there are again two levels to this query the outer query says select e dot name and then i say from employee as e that is i am using i am i am aliasing employee table as e where e dot employee number in and the inner query begins which says selects reportsto from employee where e dot name note that i am using the alias called e in inside the inner query as well um while the alias is actually defined in the outer query and then i say e dot name equal to name ok so what is this query do actually so um um if you notice closely the outer query as well as the inner query works on the same table called employee the outer query calls itself as e and the inner query does not change the the name of the table um um it is it is still called employee therefore in the inner query when i say um select reportsto um that is give me the set of all employee ids of of the bosses of all employees where the name of the employee is the same as the name of the boss that is e dot name here would be the boss here because um i am looking for e dot employee number to occur within this this set of all um um employee numbers who are bosses ok so um the the query essentially returns all employee names who have the same name as their boss  refer slide time  00  17  49  so let us look at few more definitions pertaining to nested queries the kind of query which we just saw now that is where a particular alias is defined in an outer query and then used inside an inner query such kinds of queries are called correlated nested queries um that is there is there is a greater correlation other than the fact that um um an inner query or or a nested query occurs within a condition there is a greater correlation between these two queries now how do we understand or or how do we um analyze the behavior of a nested query now to to understand how a nested query begins um it should be noted that or it is it is sufficient if you just note that every select query is performed or every select condition is performed exactly once on each tuple that is specified in the table that um for the table specified in the query therefore let us have a look at the previous slide once again in the previous slide for the time being consider just the outer query assume that there is just one select statement which is the outer query ok so so so this statement says select some attribute that is e dot name from the set of all tuples in employee where some condition that is e dot employee number in whatever ok so so let us not worry about what the condition is let us just assume that there is some condition called  see  now this condition that is in in this case the the nested query and and the set membership function that is e dot employee number in that nested query the this whole condition is checked once for every tuple that forms the employee record that is um um that that forms the employee table so so for every employee this particular condition is checked that is for for every employee we are checking whether his employee number occurs in the set of all employee ids of people who have the same name as one of the subordinates ok so so in that way as long as we remember this fact that select query or the select condition is checked once for each tuple its its easy to understand nested queries in um in in a in a in some kind of recursive fashion that is um um you you have to understand this for each level in a given nested query  refer slide time  00  20  25  so continuing further with with next nested queries we can use the a term or or the keyword called exists or of course not exists to check whether the output of a given nested query is empty or not so um if the output is not empty then exists returns true that is there exists some results from this from this query and if the output is empty then exist returns false or not exists returns true so um there is an example given in this slide um regarding this um um um which just says select d dot head where um from department as d of course that is alias department as d where not exists ok where the following condition does not exist what is the following condition the which is the inner query select employee number from employee where reportsto equal to d dot employee number ok so so that means um um essentially i am looking at the the head of all departments that is um i am looking at d dot head ok so so the employee numbers of of of the heads of all departments where um this does not exist what is this select employee number from employee where reportsto equal to um its its this should actually a there is an again another bug here this should actually be employee dot employee number that is um um  noise  um or or d dot head rather so where where reportsto equal to this head that means give me the set of all heads of departments who do not have anything who do not have anyone working under them because this condition should not exist is what we are checking for  refer slide time  00  22  15  we can also specify um um explicit sets until now we have been looking at implicitly defined sets we have looked at set membership in the form of in condition we have looked at set comparison in the form of all condition and we have also checked for empty sets using the exists condition  refer slide time  00  22  33  but all of these sets where actually specified in the form of a query the that is the nested query we can actually specify sets in an in a more explicit fashion using just parentheses that is we just parenthesize and um enumerate all elements of the set explicitly the slide here shows one such example which says select name from employee where reportsto in zero zero seven zero zero eight and zero zero nine therefore what it says is that give me the names of all employees who report to either zero zero seven zero zero eight or zero zero nine or that is whether the reportsto field or the reportsto attribute holds one of these values in this  refer slide time  00  23  18  one can check for null values in in an sql table um um use using an sql command called null or or a sql keyword called called null cap capital n u l l and you can also um you you can also check for whether something is null or is not null so remember what what constitutes a null value for a particular attribute a null value is a a value which is not applicable or an attribute which has no value or no semantic value associated with um for a particular tuple this is different from saying the the value the attribute is zero or unknown ok so um the example here shows um a query which says select e dot name e dot salary from employee has e ok so that is select the names and salaries of of employees where something exists ok what is that that exists or that is where the following set is not null what is the set which should not be null um the the set which says select employee number from employee where e dot reports to eq equal to employee number and reportsto is null ok so let us analyze the um um interior query or or the inner query again ok it says select or give me the set of all employee numbers from the employee table where e dot reportsto that is the the employee from the outer query um um is a subordinate of um of me that is the the the employee in the inner query and reportsto is null that is i do n't report to anybody else ok so um um essentially the the query returns the names and salaries of all people who report directly to the head of the company that is um um this the name and salary of all people where the following condition exists the the following condition here is the set of all um there bosses who happens who also happens to be the head of the company that is who doesn t have any other supervisor himself or other self ok so um note that here um the the use of the condition is null that is whether reportsto is null this is different from saying um reportsto is um is zero zero zero suppose zero zero zero is the valid employee number this is different from null that it means that this employee has a boss but in this case this is looking for the fact where this employee does not have a boss or does not have anyone to report to  refer slide time  00  26  04  just like we have been using aliasing for um for renaming table names the as key word or the as clause can be used to rename attribute names as well so the slide here shows such an example um it says select name as employee underscore name from employee so what is it do it it simply selects the set of all employee attributes from from this table however while returning it when when the table is return the name of this attribute is changed from name to employee underscore name now this in turn can um may may probably be as part of a larger query were were this would matter or even if it is just printed out on the on the output the the name of the attribute would have changed from name to employee name or employee underscore name  refer slide time  00  26  58  note that when a select query is um run over multiple tables that is the the from clause contains multiple tables select name comma salary from um employee comma department by default it assumes that we are having a cartesian product between the tables that are specified therefore um if we have to compute a join between employee and department for example um we have to identify we have to explicitly identify which is the um attribute and which um um which has to be compared therefore in a um simple select operation um let us say we have to join um um we have to compute a join between employee and department we would say something like select star from employee comma department where employee dot dnumber dno remember what what the employee record look like the the employee record had one of the fields as the dnumber or the department number of the employee where he is working in so where employee dot dnumber equal to department dot dnumber ok so we have to explicitly equate these two attributes to perform a join otherwise it is constitute as a cartesian join between employee and department on the other hand we can um specify a join um that that is rather than a cartesian product using the keyword called join the slide here shows such an example the slide shows a query which says select name comma address from employee join department now when we just say join without any  noise  further qualifier there is still not enough information to um um to um identify which attributes to use for join condition what is the theta or what is the join condition here therefore we specify that condition explicitly so we say as shown in the slide here select name comma address from employee join department on dno equal to dnumber note that dno was um um is an attribute of employee and dnumber is an attribute of department so um so this is specifying in equijoin condition that is um it is equating dnumber dno to dnumber and of course there is a where condition which says department dot name equal to research therefore it is computing the join between employee and department and selecting only those tuples um where department name is called research and then printing the set of all names and addresses of employees who work in this department  refer slide time  00  29  53  it is also possible to um specify natural join directly um without having to specify the equality condition note that in a natural join if we have um two tables at least one of the attribute names should be the same or should be common between the two tables so natural join which in relational algebra was depicted by the star operator just performs in equijoin between two um relations where the set of all where where it has some subsets of attributes having the same name ok so um the same condition also holds true in in sql that is if you are performing a natural join you have to have at least one um um attribute name which is the same you can always change the name of a table and its attributes names using the as clause which which we have already seen ok so a natural join may be specified using the natural join constrict it automatically finds attributes having the same names for performing the join and of course relations can be rename relations as well as attribute names can be rename in order to accommodate natural join so there is a um um there is there is a specific example here which says select employee dot name comma department dot name from employee natural join department in our specific example thought the the only um attribute name which was common between um these two was the name clause itself therefore for our particular example schemas that we have taken this um query may not make sense but the the syntax of the query is illustrated the the main idea behind the example is to illustrate the syntax of the query wi which just says select employee dot name comma department dot name from a natural join between employee and department that is identify all attributes which are which have common names between employee and department we compute a natural join and then project the set of required set of attributes from them  refer slide time  00  32  04  similarly other kinds of joins in which we saw in in the session on relational algebra can also be specified um using the appropriate keywords in sql just like you have natural joins you can also specify left outer join right outer join and full outer join um as part of any sql statement  refer slide time  00  32  26  we next come to um the notion of aggregate functions in sql until now we have been working on generating a set of tuples or the set of all tuples that match match a particular um um criteria some times we may need aggregate properties of um um of a query rather than the set of all query results so there are number of aggregate function in in sql some of them are shown here there um um for example count which counts the number of tuples in the in the query result sum which computes the sum of the set of all values in the query result in which case the the query result should return single numeric attribute average which computes the average of all values in the query result again the query should return numeric attributes max and min which computes the maximum and minimum values of all um um of all the query results and of course max and min will work if if the query results are numeric um or um or they are ordinal which basically means that they have some kinds of total ordering that is specified among them for example date um date has a total ordering you can always compare two or more dates so you can um the the query can result the maximum and minimum among dates and of course char you can always compare characters using their ascii equivalent so um the the maximum and minimum can be return here is a small example query shown in the slide which just says select count of salary min of salary max of salary sum of salary and average of salary from employee which is obvious it it just counts the number of um um salary elements as you can see i could have as well set count star it does not make any difference because it just counts the number of tuples um in um in in the relation and min of salary min of the salary field max of the salary um salary attributes sum and average of all the salaries  refer slide time  00  34  40  since aggregate functions return a single value and not a table they can actually used as part of they can actually be used as part of a logical operator within a where clause until now when we have been talking about nested queries we have been treating the nested query as the set and we have only been applying set theoretic operators like in and all and exists and not exists and and so on ok but once we use um aggregation function and reduce our query result to a single value i might as well use it as part of a logical operator the example in this slide shows such a case which says the outer query says select e dot name from employee as e where the inner query says select count star that is select the count of all the the the set of all tuples that are return by this query and what is the query from department as d where department dot head equal to e dot employee number that is it is selecting the set of all um employee numbers who heads departments ok and um um counting the set of all um um counting the set of all such queries that is it is it is count um it is um for each employee that that is search in the outer query it is seen it is first searching the set of all departments that the employee heads and return in their count and this count is compared with two here that is greater than or equal to two therefore the the semantics of this query is return the names note that we are finally retuning e dot name ok return the names of all employees who head two or more departments  refer slide time  00  36  32  in some cases when we are talking about aggregate functions um it is not really desirable to apply the aggregate function to the set of all query results um um set set of all um tuples that have been return by the query it may be um more um desirable to apply this the the query are are the aggregation functions two different sub groups of of the query results now you can specify such sub groups using what is called as the group by clause the group by clause is is shown in this query here in this slide the slide shows a small example select dnumber d dno comma count star comma average salary from employee group by dnumber what it does is for each department that is it first say it it first checks the or it first scans the set of set of all tuples in the employee table and then groups the set of tuples according to dnumber or dno ok so for each department um that um that are contained in the employee table count the number of people working in it that is because we are counting the set of all um employee records which have this dno ok so count the number of people working in this department and um return also count the average salary of of the people who are working in this department  refer slide time  00  38  04  but note that the group by function is computed after computing the results of a query that is the query first scans the set of all employee records as in the previous example and then groups those records or groups those tuples um based on dno and then performs the select operation however if you want to select certain tuples based on some aggregate property not an individual property note note the important difference here suppose they want to select a set of tuples from a a table based on some aggregate property we can use um different keyword called having clause ok the um slide here shows such an example it says select dno that is select the department number count and average salary from employee group by dnumber that is it s the same query as as earlier that is for each department return me the count and number of people working in it and average of salary having count greater than twenty that is there is a further constraint here for all departments having more than twenty people working show me the department number the number of people that are working their and their average salaries note the difference between having and where clause you might you might be wondering we could have just used the where clause select from where where actually specifies a particular condition and having is also specifying a particular condition now what is the difference between having and where  noise   refer slide time  00  39  49  having versus where where conditions apply to individual tuples independently that is the where condition where i where i say something like salary greater than three lakhs is applied to each tuple independently however the having condition applies to groups of tuples its an aggregate property that is having count greater than twenty that is the number of tuples is greater than twenty or the average of salary is greater than something else and so on so um having is specifies a condition that applies to a group of tuples whereas where specifies conditions that apply only to individual tuples  refer slide time  00  40  30  so um let us um um look back at select now how does select look like after um going through all this different um um different variants of select now um  noise  once i look back looking at all this different variants we see that the select condition has the following constructs  noise  select attribute and or function list i can also give a function remember i can always say select one ten times ten star salary that is what happens if my salary increases by ten times and so on ok so select um attribute list or function list from table list and the rest are all optional the where condition is an option if you do n't specify where then every tuple is is checked ok group by grouping attributes how should the tuples or how should the um um how should the output be group having group condition that is some kind of aggregate properties that we can check then finally order by which is the sorting condition for for the output  refer slide time  00  41  44  we now move into other operations within sql um um select obviously is the is the most widely used operation and and hence the most um um detailed in terms of some its syntax but we still need operations for inserting or adding data into um into tables and modifying tables deleting data from tables and so on by modifying tables here i mean modifying data in tables not not modifying the structure of tables which can be done using the alter table command ok so insertion of data into tables can be can be performed using the insert command the the insert command is shown in the slide um just like we have select we have insert into so we say insert into employee values and i give the entire record for the employee within parenthesis so i am inserting a complete record where the the first one zero zero two stands for the pan number the second zero zero two stands for employee id the third field bharath kumar stands for the name then m stands for the gender nine five nineteen seventy three stands for date of birth and so on ok so we can specify the entire um um tuple  noise  in line into the um um in to the query itself and corresponding fields and the corresponding attributes are matched so inserts an em entire employee record with corresponding values  refer slide time  00  43  16  on the other hand i can insert or i can specify only partial set of attributes within the within the tuple so this slide shows such an example which says select into employee or rather insert into employee name address and dnumber values arun k ysehwanthpur and five therefore it returns or it it inserts only those fields or only those attributes called name address and department number or dno what happens to the other attributes the oth the other attributes will either get a null value or the default value if a default is specified therefore you can see that um there is an implicit constraint in this select operation that is i can not leave out when i am inserting a tuple i can not leave out any attribute name which contains um  noise  which contains a non null constraints not null constraint and does not have a default value if it does not have a default value and it is not allowed to be null then i have to specify a value during insertion or else insertion is going to fail  refer slide time  00  44  26  an insert or insertion of more than one tuples within to to a table can also be performed using a select operator select operation note that select actually returns a table now if i return and table is nothing but a set of tuples now if a select operation returns a set of tuples and these tuples are in a format that a ready to be inserted into to another table i can directly specify the select command within the insert command this is shown in this example here first i just create a table called emd which contains just three attributes employee manager and dno that is the department  number   refer slide time  00  45  08  then um um i give an insert command from the employee and department table using the following um um syntax i just say insert into emd that is insert into the new table name and what should i insert the output of the following select operation i then i just give the select command that is select e dot employee number as a employee e dot reports to as manager and d dot dnumber as dno and then from employee as e join department and and so on so so its basically i am getting a set of employee number manager number and department number which is what is going into the new table  refer slide time  00  45  49  so what are um the properties of the insert command let us summarize the insert command once again so um insert can insert tuples that are specified as part of the command and all attributes which are not null and do not have a default value have to be specified as part of the insert statement and what happens if i give the same insert statement twice that is the same insert statement with the same set of data elements and um um it is actually perform twice it it doesn t insert does not do any checks that is um remember that tables are treated as multi-sets rather than sets therefore insert just goes and inserts the tuple again that is second occurrence of the same tuple this is done as long as the second insertion does not violate any unique constructs that is if i um if i give a set of data values in the first tuple and give the same set of data values in the second and one of the attributes has to be unique then the condition then the constraint fails and in turn insert also fails so as long as the unique construct is not violated insert will just insert multiple tuples into the table insert also fails if referential integrity is violated this if your dbms supports it of course that is if i try to insert an employee number as manager where which refers to a manager entity which does not exist referent referential integrity fails multiple tuples can be inserted within a single insert command um first of course by using the select statement or by just giving multiple tuples one after the other separated by commas and each tuple is enclosed within parenthesis  refer slide time  00  47  42  deletion of tuples how do we delete  noise  tuples from um from from a table deletion of tuples is very similar to the select statement and is has the same structure  noise  which is of the form delete from where it says delete from employee where some condition that is um employee number equal to zero zero seven the the first one which just deletes one tuple note that employee number is the primary key therefore it is unique and therefore it it just deletes one tuple on the other hand the second tuple says delete from employee where department number in the the the set of all departments that are headed by zero zero seven so it can delete possibly more than one tuple in this statement on the other hand if i remove the condition and i just say delete from employee it deletes all tuples note that deleting all tuples is different from dropping the table here deleting all tuples corresponds to truncating the table that is the the table exists but it has no tuples in it whereas if i drop the table the table itself does not exist in the database  refer slide time  00  48  54  updation of tuples how do we modify tuples um um you can update tuples um by using the update operation and the update operation also has a very simple syntax um which have which is of the form update set where that is update employee as shown in this record um um example here so update employee set whatever updation i need to make that is set salary equal to salary times um one point one where reportsto equal to zero zero seven therefore what i am doing here is that for all people working under zero zero seven i am updating their salary by or i am increasing their salary by ten percent i am giving them a ten percent rise by by by this update statement here  refer slide time  00  49  37  we now come to the last um um leg of this um this session where we take up the notion of views or virtual tables this is again um an important concept in um when when um designing large databases view as you can typically or intuitively understand is one particular view of the database that is one particular um um um um one particular projection of of the database which is suitable for certain certain kinds of work that is um um let us say a hr manager needs to know only the the employee that is the the hr related details of an employee he doesn t need to know the technical details of an employee or or the project related details of an employee therefore we have we have basically created a view of the employee record for the hr manager which is different from the view created for let us say the project manager so table um or or a or a view is is also called a virtual table the this is a table that is derived from other tables in contrast a table that exists in the database is called a base table so um a view can be derived from either other base tables or other views views are need not be stored its not that they are not stored but views need not be stored in the database that is the data contained in the views need not be stored in the database but they may be they they are typically stored as queries and not as tables and update operations for views are limited this is as the result of storing them as queries but querying is not you can you can query but updation is limited in terms of views  refer slide time  00  51  24  so views can be created in sql using the create view command this is again pretty simple construct which is shown in the slide here which says create view and emd which is the name of the view and there are three fields empl mgr and um um department so um so so this is the structure of the view that is the name and and the attribute list and this view is created as this query that is whenever i need to to compute this view i i just need to run this or execute this query this query is going to return a table which will populate the um the the data that that is required by this view  refer slide time  00  52  06  what are some of the properties of views because a view is stored as a query a view is always up to date i do n't need to modify a view when i modify some data elements in my table because um the the there is no data element that is stored in a view hence characterization of a view is not done during view definition time that is when i define a view i just leave it like this when i define a view like this it it is just left like that it is not characterized it is not computed but this computation is done during the time of query that is when i give a query on a view this characterization is done views can be deleted using the drop view command just like we we can delete a table the efficient implementation of a view is is a is a pretty tricky problem and we shall be addressing this view maintenance as a separate session in itself um because how do we efficiently maintain a view um um um and um um execute queries over  refer slide time  00  53  14  so that brings us to to the end of this this session so let us briefly summarize or look at the titles of all the different topics that we have studied we looked into a sql two standards tables attributes values and um constraints entity constraints foreign key constraints and so on  refer slide time  00  03  01  we looked at several different kinds of select operations select from where and um disambiguation aliasing selecting from multiple tables set operations multi-set operations substring operations arithmetic operations  refer slide time  00  53  40  existence checks and null checks nested queries and aliasing and scope in nested queries and um group by constructs and having constructs and so on and then we also looked at um um sql statements for insertion deletion and updation of um tuples from database finally we we looked at the notion of views or virtual tables or rather i should say that we have just scratch the surface of views or virtual tables and saw how we can um specify a view in in sql using the create view command so that bring us to the end of this session thank you transcription  shobana proof reading  vidhya database management system dr s srinath lecture # 7 er model to relational mapping hello and welcome to the next lecture in the dbms course until now we have seen two main kinds of data models  refer slide time  00  01  18  when for representing data or managing data in several different application context like i say whether it is insurance or banking or railway reservations or company databases or so on um these two models where the the first was the entity relationship model or what is called as the er model and the second was the relational data model we also saw a typical database design um process and placed these models into appropriate positions in the process  refer slide time  00  01  46  the entity relationship model or the er model is essentially meant for human comprehension it it basically is meant for um creating a conceptual database or conceptual schema or or what is termed as a logical schema of the database system and the relational data model is used for the physical schema or something that is implemented on in the dbms so um and both are dbms independent models that is no matter which companies database that you are going to use you can still use the same er model for representing your data and even the same relational model for representing the schema that goes onto your dbms now um um we um in in this session we are going to address one important issue now are these um we we are going to ask a question are these two different data models er and relational model completely independent of each other or are they the same or is it is there some way i can map between the the er schema and the relation schema without having to um break my head too much essentially that means to say or um to to put it to to take it to its logical extreme can i design some tools or can i design some kinds of software that takes an er diagram of a given system and generates appropriate relational schemata for for the system and we have also seen in the session on functional dependences we have seen how we can optimize a given relational schema up to a certain extent using some kinds of automated techniques we have saw we have seen how to take a relation to a bcnf or third normal form or or the fourth normal form and so on so um um suppose we want to build a tool to automate this process um we need to be able to first map between an er dat database schema and a relation schema and and then use techniques from um functional dependencies to optimize this relational schema so that we can build a database um application around it so um what we are going to study today form the underpinnings of what are called as lifecycle tools or database lifecycle tools there are several different lifecycle tools which provide support for the entire lifecycle of a database systems starting from the conceptualization of the problem to the actual implementation of the application and um and maintenance of the database and so on so before we begin um  refer slide time  00  04  25  let us briefly summarize what we have learned about er models and relational models the er models as um we already know is used for conceptual modeling of the database schema conceptual modeling or um to to create the logical database schemata this is meant for human comprehension this is essentially used to show end users what you have understood about their problem domain this is a high-level database design and their no implementation details about that that are included as part of the um er model and of course the er model is a dbms independent and um um it is made of building blocks like entities relationships and attributes which which can be attributed to both entities and relationships  noise   refer slide time  00  05  10  in contrast um a relational data model is um um the data model that is most popularly used for physical schema design a physical schema is is the schema that is actually implemented on the computer therefore the the relational data model is meant for or optimized towards machine conception that is um how do we efficiently store data in um in in my database how do i efficiently search for a given data element how do i efficiently update a given data element so that it does not create anomalies how do i efficiently delete data elements again without creating any anomalies and and and so on and of course the db the the relational data model is also dbms independent that is no matter what kind of database that you use you can still use the same data model as far as the the database that you are using is a is a relational database you can use the same data model to to represent your data on the dbms um of course um reality is quiet different from from from the concept of dbms independent and some dbms systems may include more features than traditionally what is supported by the relational model the relational model also um supports some kinds of automated optimization techniques which we have seen in the session on functional dependencies where you can optimize a given relational schema you can reason whether a given relational schema is is optimal or not whether its going to create redundant data in in its dbms or whether its going to create some kind of anomalies during updation and deletion or so on and how you can systematically change the database schema without changing the correctness but increase in the overall efficiency in terms of retrieval and appearance  noise  and what are the building blocks of the relational model we have relations um which um which which comprise of several different attributes and the notion of keys forms um a very crucial role or place a very crucial role in the relational database model  noise   refer slide time  00  07  23  let us come back to the the er model and look at some of the notations which we will require if we have to um if we have to study translation into er models the entities are  noise  represented using rectangles and a strong entity type that is um an entity type which has its own key attribute and which represents a physical or which represents some kind of a logical entity of the of the real life is represented by a rectangle with solid lines surrounded for example the slide shows this entity type called employee which depicts all objects of type employee which which are present in the present um current system on the other hand we also have what are called as weak entities weak entities are those which do not have an existence of their own or without being associated with a strong entity type um the slide shows the example of an insurance record an insurance record doesn t mean anything unless it is associated with some person um in a in a company for example an employee therefore um we have to when we talk about insurance record we have to say who s insurance record and so on ok so that that is the general um idea and um more specifically the insurance record entity type does not have any key attribute it has to be associated with with a strong entity type called employee which in turn has a key attribute therefore such weak entity types are depicted using dashed lines or dotted lines for the rectangle we then have the relationship type for example a relationship called handles so employee handles project or something like that um which which is represented by a diamond and a normal relationship type is represented by a diamond using solid lines whereas um what are called as identifying relationship types that is the relationship types that identify a weak entity or provide an identity for weak entities by associating them with strong entities they are shown with double lines in the diamond  refer slide time  00  09  36  entities and attributes um are associated or entities and relationships are associated with attributes which are some values in a given domain attributes are depicted using ovals and normal attribute or a simple attribute is depicted by a oval with um um with a solid line and key attributes in this example um um an attribute called pan or pan number which uniquely identifies each income tax payer is shown as a key attribute and it is shown underlined saying that this this attribute is a key attribute for this entity type and then there are multi-valued attributes which which can have several values for the same attribute we took an example of the color of a peacock now the color of a peacock is um is actually given by several different colors and all of which in combination form form the color of um of the bird such kinds of attributes are depicted using double lines as shown in the slide here and then there are derived attributes that is attributes whose values can be derived from other attributes and these are shown using dotted lines we took an example of the age of an employee that is if you know the date of date of birth of an employee and the current date we can derive the age of an employee  refer slide time  00  10  58  let us are also look at some definitions from the relational um um model the relational model is based around um um the the the notion of a mathematical relation now a mathematical relation is set to comprise of atomic values or atomic data values and what is atomic data value a data value is called atomic if it can not be sub divided into smaller values for example the age of a person similarly each data value is set to reside in a domain in the er model a domain is also called a value set which is term that generally used by several um um people and in the relational model usually the term domain is used which is going to um um which is going to specify the range of values that a particular attribute can take similarly a relation schema or a relational schema is is denoted by a schema name that is um um in this example is shown by the name r and a set of attributes in this example shown by a one a two until an and each attribute has a specific value that lies within the domain specified as domain of ai  noise   refer slide time  00  12  13  we have also um um define what is known as the degree of a particular relationship the the degree of the relationship is is simply the number of attributes in its relation schema if you remember the the same definition of the degree of a relationship also applied to the er model that is a relationship diamond can be a binary relationship or a ternary relationship unary relationship or a n-ary relationship that is it can um it can be associated with one two three or any number of entity types  noise  and um the the slide shows that the relation is actually um um is actually a subset of the cartesian product of all of the domains that that form the attributes  refer slide time  00  12  59  in the relational model the notion of keys plays play a very crucial role especially we saw in the notion in in the in the process of decomposing relational um um schema in order to make them normalized or um conform into let us say bcnf or third normal form or fourth normal form and so on so let us revisit the notion of keys in in a little more detail and keys are again very important when um when we translate from an er model to a relational model we have to be aware which attributes are the key and which attributes are the foreign key and so on so a key constraint um in the relational modal es es essentially defines the notion of a superkey which is a set of attributes of a relation which can uniquely identify each um each tuple in the relation that is each instance of the the relation and a key or a or a minimal superkey is something which um um which is minimal in a sense that if you if you remove any element of the minimal superkey it seizes to be a superkey anymore and there is also the well known entity integrity constraint in in the relational model which says that the primary key of of a given tuple may never be null the primary key is is the minimal superkey that is going to be used to uniquely identify a a given tuple in the relation  noise   refer slide time  00  14  30  and we also saw the notion of referential integrity which is again a an important issue in in the relational data model and the referential integrity constraint says that if um a tuple of one relation refers to another tuple of another um um relation it should refer to an existing tuple that means foreign keys that is keys of primary keys of another relation embedded into the re into the tuple of yet another relation should refer to tuples that already exist in the um um in in the first relation so the foreign key constraints are shown in the slide here that is first of all the attribute of the foreign key or or the domain um of the foreign should be the same as the domain of the primary key of the other relation and they have to refer to existing tuples in the other relation  refer slide time  00  15  27  and we also saw that relations can be um um or or popularly viewed as tables and which is what is a notion used in sql that is a relation of the form student with three attributes roll number name and lab can be re specified in the form of a table with the name student and three kinds of columns called roll number name and lab  noise   refer slide time  00  15  53  so um so so let us now come to the the issue of mapping between a given er model and um and a relational database model now i um i had i had said in the beginning of this session why this such a such a mapping is important um there are several different commercial tools that are available which are called as lifecycle tools of dbms um um design a lifecycle tool provides support in several or or in most of the phases of a typical database lifecycle that means the tool should be able to or using the tool you should be able to create um a logical schema talk to your end users saying this is what i have understood by your requirements of um of your of your system these are the different data elements these are the different functionality requirements that that form your system and so on and then um um using the same tool you should be able to create physical schema at a from the logical schema by automatically translating them to to whatever extent possible in practice it its not possible to completely automate this process that is automatically generate a relational model and um optimize it sometimes some kind of human intervention is necessary um in order when the human knows some domain knowledge can not be captured into the er model but there are several such tools an example is is the tool called erwin from from computer associates which um which provide such a support for automatically translating between er and the the relational model  refer slide time  00  17  34  so let us see how we can go about such a translation the first case that we are going to take is the case of a simple relation or or a simple entity ok so the slide here shows a simple entity type called department and it has three different attributes department name department id and manager and the department id obviously is the key or the key attribute of this department now given such a relation it is fairly obvious to given such an entity type it is fairly obvious to see that it can be translated into a relation of the type department that is also shown in the slide here just below the figure so this er um model can be um translated into such a relation um where the name is called department and um which which has three different attributes department id department name and manager and also note that the key attribute is retained that is the department id which is the key attribute of this entity here becomes the primary key of the relation that s found so this is straight forward that is as long as we have a simple entity type with simple attributes note that the attributes are also simple there are no multi-valued attributes or composite attributes and so on and um it can be translated in a straight forward fashion to the relational model  noise   refer slide time  00  18  57  what happens if we have a composite attribute remember that a composite attribute is something that is made up of sub attributes a composite attributes is different from a multi-valued attribute that is a multi-valued attribute is something which can have many values for the same attribute the color of a bird can can have several different colors on the other hand a composite attribute is made up of two or more other attributes each having its own domain for example the slide shows a composite attribute called department id for the same example of a department entity type so the department id is a attribute here which has which is a composite attribute which in turn is made up of two other attributes called location id and region number so on ok now a location id could could have a sev could have a different domain let us say um um let us say location ids are given alphabets like a b c d and so on and region number are given numbers one two three four and so on ok so both of them may have different domains and um the um they combine to form the attribute called department id which in turn is is also the primary key of this department therefore we are considering two different um um aspects here one is how to deal with um  noise  composite attributes and the second is what happens if the composite attribute is the key attribute of um um of the entity type um so the slide here shows um shows an exa shows the example of um and shows how we can translate this into relational model firstly the the name department of the entity type becomes the name of the relation called department and all the other simple attributes are retained department name is retained as department name manager is ret retained as manager and only here for the composite attribute the department id never app appears here its just that all the simple components of the com of the composite attribute are are straight straight away loaded into this relation that is location id com comes here and region number comes here and in fact if either of these two let us say location id or region number is again a composite attribute and it has some more attributes just take all the simple components of the com composite attributes so so do n't take region number and just take whatever is the simple component of of this region number and add it to the relation here and all of these simple components which form um the the the the composite attribute which is the key becomes the primary key that is the the the the primary key here is a composite key in made up of two or two or more different attributes which combinely um um identify or help in identifying a tuple of the relation  noise   refer slide time  00  21  51  the next example that we are going to see is the example of how to map relationships first of all let us look at the following relationship that is shown in this slide here um what characteristics can we um um um can we ascribe to the relationship that is shown here firstly um we notice that the relationship here um is a one is to one relationship that is one employee is associated with one insurance record ok or rather the other way around in this case that is one insurance record is associated with one employee and have a look at the association as well the association or or the relationship type um is um is is an identifying relationship type that means the relationship type called insurance details shown in the slide here is used to identify or provide an identity for the insurance record by associating it with an employee and also the the insurance record has a total participation in this relationship that is insurance record has no um existence without this relationship ok now how do we translate um um translate such entity types ok now um so so essentially the the the idea here is what how do we translate um um insurance record into the relational model ok so um  noise  so so for weak entity types the um um the translation is shown um here in the in the slide below just create a relationship or or a just create a relation of the same name as the entity type but since it does not have um um a a key to because weak entity types do not have key attributes so since it does not have a key attribute use the key attribute from employee with which it is associated with and take that key attribute and make it into the key attribute of um of insurance record however note that since pan number here is also the primary key for employee this has to be made as a foreign key of insurance record that means um whenever we are we are updating or altering the table we have to use the cascade option um  noise  whenever let us say the the employee type is updated or deleted that means to say that if if the employee relation is delated deleted from the database this um has has to in turn delete all the insurance record relations from the database itself because insurance records do not have any existence without the the employee records ok so um so so so the three step here in order in order to translate a weak entity type is to first identify or is to find locate the identifying relationship and see which entity type is this weak entity type associated with and um use the primary key of that entity type as the key for um the the the the weak entity type or or the record of or the relation for the weak entity type and make it into a foreign key of um of this um entity type and use cascade options whenever updations or deletions or performed on the strong entity type um entity type  noise   refer slide time  00  25  20  let us um move on with um  noise  translating relationships um so how do we translate um let let us take the simplest form of relationship again the one is to one relationship we saw what happens um or how do how do we translate one is to one relationships when weak entity types are considered ok now let us consider an entity type which is not weak but still invo is involved in a total participation that this is shown in the figure here the figure shows a a relationship type called managedby which relates two different entity types that is a department and manager and there are attributes relevant attributes are shown for each of them that is um the department has a key attribute called department id and a manager has a key attribute called employee number and the relationship itself um um has a key attribute called secretary that is a secretary is assigned for a department that is managed by a manager that is the the secretary attribute does not have an existence without an existence of this relationship that is if if a department is not managed by a manager then there is not secretary that is associated with um with this ok now how do we translate this the translation is again shown in the slide below  noise  so first create um create an entity type or or create a relationship um  noise  create a relation called manager let me re repeat this again for this for this relationship create a relation um um in the in the rdbms model called manager with the following um attributes now you might be wondering why should we create the the why should we create a relation called manager why not department ok now let us um think about it a little further see in this slide here that manager is um is a strong entity type it is not a weak entity type however it is involved in a total participation in this relationship type what is a total participation the total participation is that um the entity type does not have any existence without um being participating in a relationship type of this kind ok so what it essentially means that means here is that a manager has no existence that is a manager would probably be just an employee so so a manager would pro would have no existence unless here she is associated or is given a department to manage ok so um so it is the it is the entity type that is involved in the um um um in the total participation is taken as a primary entity type or or the base entity type um um primary relation called manager and then employee number becomes the key here that is the the key for manager ok and the department id which is the primary key for department becomes a foreign key in manager and whatever um um attributes um are associated with the relationship itself become attributes of this relation here that is of the manager relation here ok so therefore the manager relation has a primary called primary key called employee number and a foreign key called department id note that this makes um this makes sense when we note that manager is a um does not have any existence without this relationship that is um um refer to the problem of um referential integrity in relational data model what is the referential integrity stipulate whenever a foreign key refers to a tuple in another relation the tuple should exist that is it should refer to an existing tuple in the other relation now if we had made department as the base entity or or the base relation here we can not use employee number as um as the um um as a foreign key because the manager relation wo n't even exist um before this this relation that is managed by is formed ok on the other hand department has an independent existence without whether or not a manager is associated with it therefore um um that forms a rational behind why we choose the entity type which entity type which um which is involved in total participation as a base entity type for the translation  noise   refer slide time  00  30  03  so um let us summarize um this um the the previous slide once again so in any one is to one binary relationship between types s and t choose one of them as a base relation in case one of them is involved in a total participation choose that as the base if neither department nor manager where to be involved in total participation it doesn t matter which you are going to choose as the base um relation include the primary key of primary key of the other entity type as a foreign key in the base relation and include any relationship attributes as attributes of the base relation  noise   refer slide time  00  30  42  consider the um um example shown in the slide here what happens if um in a one is to one relationship both entity types um that is um  noise  both entity types that are participating in this one is to one binary relationship are involved in a total participation take a look at the slide here the slide shows two entity types project and consultant ok and each project is uniquely identified by a project id and each consultant is uniquely identified by his or her pan number and there is a relationship called re relationship type called consultation which has its own attribute called secretary and it s a one is to one  noise  relationship and both of them are involved in a total participation that is a project has no existence unless um um it it is being consulted by a consultant and a consultant has no existence unless here she is associated with a project so neither of them will have independent existence without the other ok in such cases um um we we can not identify any relation as the base relation if we identify project as a base relation and try to use pan number of the consultant as the foreign key then referential integrity could be violated it s the same in the other way around as well if we use consultant as the base relation and try to use project id as the foreign key again there is a chance of violating the referential integrity in such cases the the simplest way is um to take the relationship type in  noise  this case the consultation as the base relation that is form a relation um called consultation and use project id and pan as um um as the primary key of consultation and then all of the attributes from both of them will become attributes of this relation  refer slide time  00  32  40  so in case both entities in a one is to one binary relationship are both are are are both in total participation then we merge both of the entity types into one usually in the name of the relationship that is um um in the name of the relationship called  noise  consultation in the previous example  noise   refer slide time  00  33  02  now let us see how do we map um one to n relationships what is a one is to n relationship that is um n entities of one of the entity types could be associated with one entity of the other entity type that is it it form some kind of a tree relationship that is one entity being associated with n different entities of the other type so the slide here shows such an example that is employees works in department that is employee is an em entity type so n different employees can work in one um department that is one department may have um several employees but each employee is associated with only one department and of course there are keys called employee number for um employee and department id for department the slide also shows how we can um reduce this to a um to a um relation the the simplest way is to take the um entity type on the n side of a relationship ok so in this case the employee so  noise  take this as the base that is um translate it into a relation called employee and the the primary key of employee becomes the key of the employee re entity type here and the department id becomes a foreign key here ok  noise   refer slide time  00  34  19  so this is um as simple as that that is for for each binary one is to n relationships identify the relation um um s that represents the entity type on the n side why why is this so because each entity type on the on the n side uniquely identifies a department that is uniquely identifies the entity type of on the other side ok therefore we can use the primary key on the um of the entity type on the other side as a foreign key in the in the base relation therefore use this as the base relation and create a relation um um including the key of the other entity type as the foreign key  noise   refer slide time  00  35  03  how do we map m n relationships now what is an m n relationship an m n relationship essentially says that m different entities of the first type can be associated with n different entities of the second type therefore there is no unique identification that is given an employee in the previous case um one could uniquely identify the department with which the employee is working in because each employee can can work in at most one department on the other hand here let us say um um a relation called relationship called deputedto so um an employee could be deputedto several departments let us say ok so um so m different employees can be deputedto n different departments and of course employee has has employee number as the key and department has department um id as the key and so on ok and deputedto also has um um has a has an attribute called record number which maintains a record of which employee is deputed where and so on ok so in order to um um translate such um relationships note um um note the steps that are shown in the slide here there are three different relations that are that are formed um one is the employee relation ok that is one of the entity types of this relationship type so so the employee relation is formed with employee number as the primary key and and all the other attributes that that form the employee entity type similarly department relationship um um relation is found with department id as the um as the primary key and all other relations and then a separate relation is created for the relationship type itself so deputedto becomes a separate relation by itself and then uses employee number and department ids as foreign keys and also as the primary keys of this of this relation and whatever attributes that belong to the  noise  that belong to the relation um becomes um um part of or belong to the relationship type becomes part of the relation that is created here that is um deput the the record number attributes become one of the attributes of deputedto relation ok note that we can not move this recorded record number that is the attribute of this relation to either the employee or um a department because it does not uniquely identify either employee or department each employee could be associated with n different record numbers because they are they are asso they could be associated with n different departments and similarly each department could be associated with um m different record numbers because m different employees could be working in that um in the department  noise   refer slide time  00  37  57  so this slide shows summarizes how m is to n relationships are translated in a m is to n binary relationship it is not possible to collapse the relationship into one of the entity types because neither of the entity types uniquely identifies the other entity type therefore a separate relation is required and usually this is in the name of the re relationship type itself ok so a separate relation is required in order to complete the mapping and of course the cascade option should be used whenever updates are performed on any of the um relations pertaining to the entity types that is whenever the  noise  whenever the entity type called employee or department is updated or or deleted then these um changes should be cascaded so that they are reflected in the deputedto relationship as well that is if employee is employee entity type is is deleted then of course the deputed to relationship should also be deleted  noise   refer slide time  00  39  06  now one might ask the question um um is it possible to um um is it possible to use the strategy that is use the relationship type as the base relation rather than any of the participating entity types can we use this strategy for mapping one is to one and one is to n relations as well because m is to n is simply a generalization of one is to one and one is to n relationships right so of course it is possible that is um um take the example of employee works in department that is n different employees working in um one department we can still create a separate relation called works in where it can use the employee number and the department id as the foreign keys of of this relation however um um it it it just creates an extra relationship or extra relation in the in the database that is that is totally unnecessary but the this is sometimes actually attractive to use than than than collapsing the relationship into one of the relations especially where um um especially if we have to avoid null values that is especially if we have to um especially if we have cases where there are some employees who do not work in any department if n different if an employee can be associated with at most one department it means that an employee can also be associated with zero departments so in that case the the department id um field of of employee would be null it does not violate referential integrity because remember that referential integrity says that a foreign key should refer to an existing tuple or else should be null therefore um it does not violate referential integrity but creates a lot of null values in the database schema in in the database itself so if we have to avoid null values it is actually preferable to use the relationship type as the base relation when performing the translation  noise   refer slide time  00  41  11  how do we map multi-valued attributes we have seen how to map composite attributes and a simple attributes and keys and so on but what happens if there are multi-valued attributes composite attributes um um are different from multi-valued attributes in the in the sense that each of them can have several different domains that is it is just a combination of several simple attributes so we just open up the combination when we are when we are translating a composite attribute and then include all the simple attributes that that form part of the composite attribute on the other hand a multi-valued attribute is not um um is not composition of several um several sub attributes instead it is an attribute that can take on several values instead of one value and the example the slide shows is that of a bird ok so a bird has a multi-valued attribute called color ok so what is the color of this bird a bird could have several colors it it need not have just one color and of course there is a primary key called species which which identifies each bird uniquely so in order to translate multi-valued attributes um take a look at the lower half of the slide which shows um two different relations which which make up this translation the first relation shows um um shows um a relation called bird with species as a primary key and all other attributes except the color attribute ok all other attributes of um of the entity entity type called bird and then a separate relation is created called bird colors where species and color are both um um are both included and are both um of part of the key that is um um combinely define the key of this bird color therefore we say that birds species eggs has color y eggs has color z eggs has color a and and and so on ok so the the color attribute may be repeated in in in several or several tuples or rather the species attribute may be repeated in several tuples one once for each different color that the bird can take and both of them that is species and color become the primary key for bird colors  noise   refer slide time  00  43  34  so for each multi-valued attributes um um of a given entity type we have to create a separate relation that has a primary key of s paired with all possible values that the multi-valued attribute can take and of course the cascade options should be used for referential integrity um um for um on the bird um um relations that is whenever the bird relation is deleted or updated the corresponding changes has to be made in bird colors as well  noise   refer slide time  00  44  05  how do we map n-ary relation ships until now we have been looking at binary relationships what happens when there are n-ary relationships and um um different entities forming part of the relationship the slide here shows such an example that is the the standard example of suppliers supplies part to project ok so there is a suppler who is uniquely identified by the the supplier or or the sales tax registration number or something streg number and there is a project that is uniquely identified with project id and there is a part that is uniquely identified by part id and then the supplies relation which relates all of these three different entity types so um the the simple way of translating this is to use a separate relation called supplies  noise  as the base and of course separate relations each for supplier project and part with their corresponding um primary keys and the supplies relation which which has the primary keys of um um of each of these relations has the foreign key of this um um of this relation  noise   refer slide time  00  45  15  so for each for each n-ary relationship of of any type r where where n is greater than two we have to create a new relation s to to represent this relationship um to to represent this relationship type r and of course the the primary keys of of the participating relations um become foreign keys in in this new relation and the cascade option should be used for all of the um relations that correspond to the entity types that um participate in this relationship type what happens if um one of the relations in in an n-ary relationship type is a weak entity type that is um let us say part is a weak entity type there is no existence for part unless it is associated with a supplier supplying it to some project in that case we have to identify um um we have to we have to first it um we have we have to first identify the um entity type which gives an existence to part ok and the part relation here has only foreign keys it does not have any primary keys but the supplies relationship does not change that is it doesn t have any part id it it just has a it simply as the um supplier um primary key and the project primary key without the part primary key there therefore um um therefore we get two different um relations that have foreign keys and um and they do n't have their own primary keys  noise   refer slide time  00  46  56  so um so that that bring us to the end of the um this um this session that talked about that gave a um that that talked about how we can um map er models into um um in into um relation um relational database models using several different rules and of course this is not a comprehensive set of rules because there are um several other sets of rules used for example in um derived attributes or um  noise  or enhanced er models like generalization and specialization which which are not covered here but all of them um in totality are used um to to to to create the basis for any kind of a um tool software tool that can translate between given er schema and its corresponding relational schema so um this slide shows a summary of um each of this mappings that is it gives the set of thumb rules saying if this is what is given in the er model then what happens in the relational model ok so in an er model if an entity type is given then a corresponding entity relation that is a relation in the name of the entity entity type is created if a one is to one or a one is to n binary relationship is given then um we create corresponding foreign keys from from the n side to the one side or or from the weak entity type to do to the to the strong entity entity type in this relation ok so we create a relation and create appropriate foreign keys from them if a m is to n relationship type is given as shown in the slide here then we create um um um um then then we create a relation with the name of the relationship type ok so so so there is the um within quotes which shown as relationship relation with two foreign keys that is one for each um um entity entity type that participates in this relation  noise   refer slide time  00  46  57  if an n-ary relationship type is given it is still the same strategy that is we create a relationship relation with n different foreign keys that is one for each um um entity type or or rather um n different foreign keys as long as these en entity types are strong entity types if there is a simple attribute in a in a er model that that simply becomes an attributes in one of the relations in the relational model if it is a composite attribute in the er model then it becomes the set of simple attributes that is you take the simple part of all component composite attributes that is just um um go on finding um the the the simple attributes that that form the composite attribute and then make all of them as part of the this relation  noise   refer slide time  00  49  47  if it is a multi-valued attribute then we need to create a separate relation and a foreign key that is you have to associate the primary key of the base relation um with each possible value of the multi-valued attribute if it is a value set it becomes a domain um value set in the in the er model it becomes the domain and a key attribute in the in in the primary um in in the er model will become either a primary or a secondary key in the um  noise  in in the relational model so that brings us to the um um to the end of this session transcription  shobana proof reading  vidhya database management system dr s srinath lecture # 8 functional dependencies and normal forms um two different kinds of database models now when we are talking about um how to design a database system the first data model that we saw was the entity relationship model which is es especially used for building conceptual schema  noise   refer slide time  00  01  31  so um as we have defined a conceptual schema is something that is meant for human consumption um that is um it is it is meant for um communication with end users so that um they can understand what what the database designers have really understood about their domain um what kinds of entities exist what kinds of relationship exist and so on but an entity relationship schema is not really the best data model for storing data in a computer and for storing um um data that is the the internal schema or the physical schema it is usually the relational model that is used and the relational model um has in some sense has some kind of common terms with the entity schema like we have seen um the concept of keys foreign keys and referential integrity and so on which which hold both for entity schema and um and the relational schema however the relational schema is is mainly meant for the physical schema or the internal schema while the conceptual schema is meant for human um consumption while the physical schema is meant for the the computer now usually what happens in a database design process is that um we use some kind of um tools or some kind of case tools to build um um um build an er model or of of the information system that we are trying to design and there are number of automated techniques by which or number of automated tools which can take a er representation of a schema and then convert it into relational schema however while this is automatic this need not always result in the most optimal form of the relational schema and what do we mean by optimal we are going to try to formulize this notion today and try to see which kinds of relational schema or schemas are better than which other kinds and are there some kind of um techniques formal techniques that um um that that i can use by which without trying to understand the semantics just given a relational schema no mat doesn t matter wh in what context can i try to optimize it into a fashion that um that can help in efficient storage and and retrieval of data so today in the in in this session we are going to look at an important concept of relational database design um namely the the idea of functional dependencies the the notion of functional dependencies is is fundamental to the the design of different normal forms which we are going to see in in a relational database schema  noise   refer slide time  00  04  20  so what are functional dependencies functional dependencies are a frame work for systematic design and optimization of re relational schemas of course there are um many more um non systematic options for optimization for example if we know something extra about the operational domain or the specific database in use we can up always optimize something more um than what we can do with the relational um with with functional dependencies however functional dependencies are um um are a um are some kinds of techniques that are database independent and can be used in a formal fashion in order to achieve some level of optimization or some level of efficiency in terms of database design we are going to see how um we can define keys um we we have looked at the notion of super keys primary keys and so on how we can defined the notion of keys using functional dependencies in fact functional dependencies are are more generalization or is a generalization over the concept of keys and they are crucial in obtaining the correct normalized schemas when we are trying to design database systems  noise   refer slide time  00  05  33  so let us first define the notion of a functional dependency let me first take the definition here the the formal definition here and try to explain it in in formal terms the the definition reads in any relation r if there exists a set of attributes a one to an ok and an attribute b such that if any tuples in the relation have the same value for a one to an then they also have the same value for b ok so essentially what it means this is a formal way of saying that um these set of attributes a one to an uniquely determine the value of b that means if two or more tuples have the same value of a one to an its not possible essentially that means to say that is they should have the same value of b they ca n't they ca n't have different values of b because these sets of attributes a one to an uniquely determine the value of b so this is called a functional dependency and a functional dependences is written as shown in the slide here it is written as a one a two extra an and there is a right arrow to b so um um an important thing to note here is that functional dependencies define properties of the schema and not of the tuples in the schema what is that mean um essentially this kind of dependency that exist between um um a one to an and this attribute b should be a property of the entire set of tuples that are there in the schema for example um i might um it might be um i might say that the employee number uniquely determines the name of the employee ok in in any relational database schema however um we can not say that um the let us say the employee name uniquely determines the age of an employee because there there could be two or more employees having the same name but different age but it could well be possible that there is um certain employee with very rare name um and there is nobody else who has that name and therefore for this particular employee the the name uniquely determines the age uniquely determines his or her age but this is not a property of the schema in itself this is a property of that particular tuple of this particular employee who has this rare name so that there there is nobody else with that name therefore we can uniquely identify the person and the the age of that person so when we are talking about functional dependencies we are talking about properties of the schema which which holds for um um which holds for all relations or all tuples in the schema  noise   refer slide time  00  08  21  in any given schema there could be many functional dependencies um um let us say the the set of attributes a one to an can um uniquely define um its also called uniquely determine b or defines b or so on so um a one to an can uniquely determine b one or b two or b three until bm so this is written as um as shown in the slide here that is a one to an and right arrow b one to bm ok now why is this called a functional dependencies what is fu what is so functional about functional dependency if you if you are familiar with the mathematical notion of a function which is a a special kind of mapping or a special kind of relation between sets you can see here that the notation that is used for example a one to an or um uniquely determining some um some attribute b is precisely the um the the notation used for a function it basically determines because a one to an uniquely determines a particular um attribute b it acts as a as a mapping or um it act as a function which um maps the the this set of um attributes a one to an uniquely to each b that it defines um this is um um this this precisely acts as the mathematical notion of a function however um there need not be any computation that that determines this um this unique definition for example the employee number um may uniquely determine name however just given the employee number i mean i wo n't be able to compute i wo n't be able to determine what is the name its its just that um each employee number has a different um or identifies an employee with a different name um or identifies uniquely ea each employee however it is not possible to determine what is the name of the employee given just the employee number we still have to access a database to do that  noise   refer slide time  00  10  24  so let us revisit the notion of keys now when um when when we say um um functional dependencies as you might have noticed the notion of functional dependency is actually a generalization over the notion of keys that is in the previous slide here if a one to an uniquely determine the set of all attributes um um which which form the relation let us say a one to an combined with b one bm forms the entire relation then we we we see that a one to an is nothing but a key for this relation that is for the entire relation comprising of a one to an and b one to bm  noise  and similarly minimal super keys are um candidate keys can be defined analogously  noise   refer slide time  00  11  11  let us take some examples now consider um um a relation of the form that is shown in the slide here the slide shows a relation called movies which has the following set of attributes um um it it has title year length filmtype studio and star so um going by the standard definitions of what is a movie and what what are title and what is the year of its release the the length of the movie and so on we can from common sense reasoning identify some kinds of functional dependencies for example given the title of a movie and the year in which it was released i can probably um uniquely determine what is the length of that movie ok and similarly um given um um oh there is a bug in the second um um slide here second functional dependency here given a title and the year of of release of a particular movie i can probably determine what is the film type given a movie of a particular type um um of a particular name and the year in which it was released i can um pro probably say well this is an art movie this is a commercial movie this is um social documentary this is a comedy tragedy whatever similarly i can probably also um say title comma year uniquely determines um say film studio ok however um title comma year may not uniquely determine the star that is the actors who who who star in that movie because there could be more than one actor who have stared in that movie and you can not uniquely determine given the the title and the year you can not uniquely say well this is the movie star who acted in that movie because um um it need not um it need not be complete there could be other stars as well ok  noise   refer slide time  00  13  03  so let us see what kinds of um um properties we can identify about functional dependencies and what we can do with these properties um note that if if in any given relation r if i have a functional dependency of the form a to b ok where a is a set of attributes now i am using the term a that is without a subscript um um to determine actually a set of attributes ok so um if um um the set of attributes a defines a set of attributes b and the set of attributes b uniquely define another set of attribute c then we can um easily identify the functional dependency saying a also defines c so there is an example here if if the employee number is given in such a way that it also defines the job of the employee let us say all um um employees who um um who who do supplies work are given numbers between hundred to two hundred and all employee who do um administrative works are um um are a clerical job is given numbers between two two not one to three hundred and so on ok so um employee number um suppose uniquely defines job and let us say job also uniquely define salary that is every employee of a particular job has the same salary then we can um easily say that the employee number actually defines salary that is given just the employee number i will be able to determine what is the salary for for that employee  noise   refer slide time  00  14  44  some more definitions um suppose we have two sets of functional dependencies a and b ok now again a and b are sets here ok and um we have two sets of functional dependencies a and b and that is a defining b and c defining d now these functional dependencies are set to be equivalent if the set of all relations that satisfy the first functional dependency is the same as the set of all relations satisfying the second functional dependencies ok similarly um um um a a small generalization over this definition we say that um a functional dependency s follows another fd called t if the set of all relation instances satisfying t also satisfies s ok so um we will take up examples here the the follows um um property of a functional dependency is extremely important by which we will be able to compute the compute what is called as the closure of a functional dependencies that is suppose i give some kind of functional dependencies saying um um a defines b what else can i say about that um what else can i infer from this functional dependencies is what we are going to take up  noise   refer slide time  00  16  07  before we move on to um um inferring functional dependencies we need to um um complete a few more definitions the next definition that we are going to take up is the notion of a trivial functional dependency ok now a trivial functional dependency may look of course trivial but it is quiet important when we are trying to define let us say normal forms now have a look at the example in the slide here this slide shows a functional dependency of the form title comma year def defines title which is obviously true um every attributes defines itself that is um every attribute uniquely identifies itself therefore um um any functional dependency in which the right hand side is contained within the left hand side that is the um antecedent or the or the right hand side of a rule is a subset of the left hand side of a rule then it is said to be a trivial functional dependency if there is at least one element in the right hand side which is not part of the left hand side then it is called a non trivial functional dependency and usually we are interested in what are called as completely non-trivial functional dependencies which says that none of the elements in the right hand side belong to the left hand side we will see that this um this notion of triviality is is not so simple because um essentially um when there are circular dependencies it becomes quiet tricky to um to handle functional dependencies which are are are removed triviality from from functional dependencies  noise   refer slide time  00  17  42  we now come to an important aspect of um functional dependencies the the notion of closure of fds what do we um intuitively understand by the term closure um if we have  noise  if we are familiar with the notion of closure and say discrete mathematics you would um um you would probably recall the definition that the closure we we say that a particular algebra is closed when i say when i take certain elements from or certain operand operands from um um a set of from a set that defines the the universe and then perform an operator that is performs some kind of a function on um on these operands and the result of this function also goes back into this set for example if i take two integers and add them the result is again an integer if i take two integers and subtract them the result is again an integer now um this is what is called as closure that is i take certain um um um operands or take certain elements from a universe perform a functional on them and the result of that function also belongs to the same universe ok so um let us define the function the the notion of closure on functional dependencies in an analogies fashion the closure of functional dependencies defined by a that is a is a set of attributes in any given relation r is the set of all attributes that are eventually defined by a that is how we can eventually define or uniquely did um um determine what all attributes can be uniquely determine eventually by a for example let um in a given relation let the set of attributes a uniquely define a set of attributes b similarly let the set of attributes b uniquely define two other sets of attributes c and d now let um the the combine sets of attributes b and d uniquely determine another set of attributes called c then we say that the closure of a contains all of these attributes that is a union b union c union d union e that is all of that and actually um um be because all of these attributes can be eventually define from a from a we can eventually define b from b we can um um uniquely define d and combining b and d we can uniquely define e so all of these attributes contained in all of these sets can be eventually uniquely defined by a what about elements that that lie within a can they be uniquely define by a the answer is yes this is because of the trivial property of functional dependencies that is every attribute in a uniquely defines itself um um which is a trivial triviality rule however closure also um um just a union b union c union d union e that that is shown in the slide does not complete the closure of of a  noise   refer slide time  00  20  52  if for example there is a subset of a or subset of any element of closure of a ok and if that subset of attributes defines some other sets of attributes f such that f is als f is not part of the closure of a then we also add f to the closure of a ok  noise   refer slide time  00  21  13  so um let us um first um  noise  see look at an algorithm by which we can compute the closure of of a functional dependencies then we look at a few examples of of closures so um given a relation r and a set of attributes a how do we compute closure of a so initially we start with um um the set saying the closure of a equal to a that is equal to all elements of a this is true because of the trivial functional dependencies every element of a is trivially defines itself now for every a prime that is a subset of a if their exists a functional dependency of the form a prime defines b and b does not belong to a that is b is a set of attributes which um which does not belong to a for um um um as of now then just add b to the set of um um set of elements in the closure of a and repeat step two until no more attributes can be added to closure of a ok so um um the closure of a set of attributes a um is also denoted by a with a su superscript of plus that is a plus and also note that suppose we run this algorithm and um um a plus contains the set of all attributes of r what can we say about um the the the set of elements in a a is of course a super key of r because using a we can eventually um uniquely define all elements of r  noise   refer slide time  00  22  53  so we now come to the um um next property of functional dependencies the the notion of inferring functional dependencies or following that is how do we find out functional dependencies which follow from one another given a set of functional dependencies and given no more knowledge about the the domain in which a relation exists what can we infer about um any more functional dependencies that exist in r ok so um um we we have already seen one rule for inferring which is which is a transitivity rule which we will revisit again now and before that note another rule that is that is shown here suppose a b c and d are um um sets of attributes of r such that the following functional dependencies exist that is a uniquely defines b b defines c and c defines d ok now um um based on the closure property and transitivity we can easily say that a defines um d ok however there could be some elements of d which are actually contained in a ok that is d need not be completely disjoined from a so let d subscript a that is d a be the set of all attributes in d such that they belong to a that is they are they are actually a subset of a now take away these attributes that is let let d prime be defined as d minus d a that is shown in the slide here once we do this we see that we have actually inferred a non-trivial functional dependency ok that is um um a defines d prime that is um um we um we started by saying that because of the transitivity rule a defines d however there is some triviality that that that is there in this um in this definition and once we remove that triviality we have actually um encountered or we have actually inferred a new functional dependencies which is non-trivial that is um which is of the form a defines d prime so functional dependencies which are specified um which are given to begin with are called stated functional dependencies and fds which are derived are are called inferred functional dependencies  noise   refer slide time  00  25  18  now given a set of relation um suppose we have a set of functional dependencies that are stated um this set of functional dependencies is called the bases of the relation for example in the movie relation that we saw where a movie is defined by title year film type um um star and so on the the functional dependency is the title and year define the length of a movie was given similarly the the the title and year define the film type was also given so all these functional dependencies that are given um are called the basis of the relation now if the basis of functional dependencies are such that no subset of this basis um is also a basis that is um none of these functional dependencies that form the basis can be derived from one another then it is said to be a minimal basis for the relation  noise   refer slide time  00  26  18  so um how do we go about inferring um um functional dependencies that is computing the closure or um um getting inferred functional dependencies and so on how do we go about um inferring functional dependencies given a basis that is given a set of functional dependencies for this there are set of axioms um or set of rules that define how functional dependencies behave these are called armstrong 's axiom which are quiet useful when we are talking about properties of functional dependencies the first property is that um the is the notion of reflexivity which is um um which is like saying which which is um easily obvious based on the tri triviality rule that is um if set of attributes b is a subset of the attributes a then a functionally defines b for example theater and year or or rather the the title and year of a movie functionally defines title that is if i have a set of attributes um these sets of attributes functionally define every possible subset of this attributes ok similarly the the second role is that of argumentation that is if i know that there exist a functional dependency of the form a defines b that is theatre comma year defines length then i can add a set of um um attributes to to both sides of the functional dependencies without violating the dependency for example i can say that if um um ti title and year of a movie determines the length of the movie then i can add um an attribute called star or um studio to to both sides of the functional dependencies without altering the semantics that is i can say title year and studio of um of a movie uniquely determines the studio and um the length of the movie ok also note that i need not add studio to the right hand side because um um it um it it forms a triviality rule that is there there there is some triviality that that s um that entails from adding the same attribute in both sides i can as well add um um i can just add a new attribute just to the left hand side and still the the functional dependency holds take the same example again suppose i say title and year uniquely determines the length of a movie then i can as well say that the title of the movie the year of the movie and the studio in which the movie was shot uniquely determines the um the the length of the movie which is fine because adding new information to um some set of attributes that that uniquely determine something does not alter the dependency similarly the last rule is that of transitivity which we have already seen that is um if a uniquely defines b and b uniquely defines c then um we can infer that a defines c  noise   refer slide time  00  29  23  the next um concept that um we learn here is the notion of projecting functional dependencies what happens if i compute a project operation on a relation um you know what a project operation is in standard um um relational algebra a project operation takes certain columns or um certain attributes of a relation and produces a new relation out of these attributes that means it actually throws away certain attributes from the original relation ok now suppose let r be a relation and f of r be the set of all functional dependencies in r now suppose i project another relation s from from this new relation r by throwing away certain attributes now um what can we say about f of s what kinds of functional dependencies exist in f of s obviously for example if i say that um um um um from the movie um um relation if i throw away the the the year attribute of um of this relation then obviously i can not have the functional dependencies which says title comma year uniquely determines length because the year doesn t exist in the new relation at all therefore at this point the the at the first step we can um um say that functional dependencies in the new relation s should satisfy the following properties what are these properties firstly they should follow from the functional dependencies in r that is we should be able to infer them from the functional dependencies that existed in the earlier relation secondly and which is also quiet obvious that they should involve only attributes of s obviously they can not involve attributes of r which do n't exist in s a functional dependency only has to involve attributes that exist in the current relation  noise   refer slide time  00  31  24  so how do we compute um the the functional dependencies on a projection now consider this um this example rather than going through a formal set of rules let me explain it by an example now given a relation r containing the following attributes a b c and d and let us say the the following functional dependencies that is a defines b b defines c and c defines d now suppose s is projected from r and b is thrown away um um from um um from r in in getting s ok so s has just three attributes a c and d now what shou what should be f of s that is what should be the set of functional dependencies that lie in s obviously we can not have um a defines b because b doesn t exist at all in in in this relation and we also can not have b define c because again b doesn t um exists in this relation so um um but um can we say anything else about these dependencies ok now um going back to r let us see what happens when we compute the closure of a let us compute the closures of each attributes ok so um i have i have not included the trivial functional dependencies in in computing the closures that is firstly we start from um a defines a obviously then um we we have a given rule which says a define b a basis rule from which we can infer that a define c because there is another basis rule which says b define c and from which we can still infer a defines d because there is another basis rule called c defines d ok so  noise  in in r the closure of a is a defines b a defines c and a defines d in s all we need to do is take away all these or throw away all this functional dependencies that contain um attributes which do not exist in s so um um here a defines b is one such attribute is one such functional dependencies it it contains b which does not um which is not part of s therefore the set of um um the closure of a in in s  noise  is essentially a defines c and a defines d or a c d ok similarly the the closure of c in in s is c defines d ok that is c comma d um in in in this case and the closure of d is just d ok now um since um the the closure of a contains all attributes of s that is a c and d we do n't need to compute anymore closures that is um um we do n't need to compute the closures of a c or a d or a c d because we already have all attributes of a we already um know um how to uniquely determine each attributes of a ok therefore the functional dependencies that that exist in um um in s are a defines c a defines d and c defines d and of course there are trivial functional dependencies like d defines d and c c defines c and a defines a which which are not shown as part of this um set here ok so um um so so um a simple way to compute the closure or compute functional dependencies in um in a projection is to first compute the closure of  noise  the set of all  noise  functional dependencies in the original relation and then throw away everything which which contains attributes that do not belong to the new relation  noise   refer slide time  00  35  10  so what is a use of um all this functional dependencies and closure and transitivity and axioms and so on um um these sets of underpinnings are used for um normalizing or optimizing relational schemas for better performance what do we mean by optimizing or or what are we optimizing against what is the um um what is the property that we are trying to remove um what is the undesirable property that we are trying to remove this undesirable property is um the property of redundancies in relational schemas if a relational schema is badly designed then um um it is going to contain um several redundant information which result in a number of anomalies what are these kinds of anomalies and what kinds of redundancies um are these let us have a  noise  look at them redundancy of of course means that some kind of information is repeated across different tuples the the same information saying um um the the title is this one or the year is this one or the star is that one and so on so if the same information is repeated across several different tuples then we encounter two kinds of anomalies what are these anomalies the the first anomalies the notion is is that of updation suppose i need to update a data element using the using let us say the the update clause of of sql um i can not update in just one tuple especially if this particular information is repeated across several different tuples if the title of a movie is repeated across several different different tuples in the database and later on i see that the title of the movie is entered incorrectly there is a spelling mistake ok so um i need to change the title of the movie in every tuple there which um in which it occurs similarly the notion of deletion anomalies suppose i delete a tuple  noise  which um um um suppose i need to delete a tuple um um about a particular movie given a particular title i need to delete all tuples which contain um um this title in its um um as its attributes so i need to essentially search the entire database and delete it in several different places otherwise it again creates anomalies  noise   refer slide time  00  37  37  so how do we design relational schema so that um um we can remove redundancies and remove these kinds of anomalies now have a look at the the movie relation once again um um the movie relation um has the following attributes that is title year length studio and star and we also have the functional dependencies which says title and year uniquely determine length and title and year of a movie uniquely determine the studio in which the movie was um shot however title and year does not uniquely determines star because there could be more than one actors who have stared in the movie ok therefore for every actor who has stared in this particular movie you have to um you have to repeat the information of length and studio across these tuples let us say some movie has um um has has two or three different um actor say say shahrukh khan hrithik roshan or whatever ok now um um um in a given movie let us say some movie x y z um that is shot in the same year in the same studio and has the same length um its only the last field the the star which which changes from say shahrukh khan to hrithik roshan and whatever so um so for each actor who was stared in the movie i need to repeat all the other information that forms part of this tuple which is what is the notion of redundancy and suppose after doing all this i find that um i have entered the length of this movie incorrectly and i need to change the length of the movie i need to change it in all of these different tuples where where this is stored and similarly the um the the problem of deletion  noise   refer slide time  00  39  22  so anomalies are um removed from a relation by the process of decomposition now what is meant by decomposition given a relation r are containing a set of attributes a you decompose the set of um relations into two different um relations s and t such that um um the the sets of attributes in um of s and t are a subsets of the attributes of a and there are no anomalies in s and t ok now um um a decomposition that does not contain any anomalies is said to be in what is called as boyce-codd normal form or its also abbreviated as bcnf so a bcnf has the following property suppose given a relation um r with the set of attributes a it is said to be in bcnf if there is a non-trivial note the emphasis on the word non-trivial here if there is any non-trivial functional dependency of the form a prime defines a double prime that means a double prime is not a subset of a prime ok so if such a non-trivial functional dependency exist then it means that a prime is a super key of r that is there is no functional dependency of the form a prime defines a double prime in which a prime is not the key if if that is the case then the relation is not said to be in bcnf  noise   refer slide time  00  40  51  so um um we will we will take up examples of this one once we co co complete the notion of decomposition into bcnf and um um when when it becomes more clearer now how do we decompose a relation um such that it it it forms or it it becomes comply into bcnf ok now suppose in a given relation r let there be a functional dependency of the form a prime defines a double prime which violates bcnf what do we meant by violating bcnf it means that a prime defines a double prime is firstly non-trivial that is a double prime is not a subset of a prime and a prime is not the super key of r that is um um it is some other sets of attribute in order to bring r into bcnf we decompose r as follows first um take the set of all attributes that are defined by a prime ok now a prime defines a double prime it may define something else and and so on now let b be the set of all attributes that that lie in the right hand side of any functional dependencies that are defined by a prime now remove the set of all attributes a prime along with b and form a separate relation and retain the the the remaining set of attributes along with a prime to form the other part of the decomposed relation r let us have  noise  an example which um which makes this very clear  refer slide time  00  42  22  consider the movies example once again let us have a look at this um um relation movies having the following um attributes title year length studio and star now here the following functional dependency hold um holds that is um title and year uniquely determines length title and year uniquely determines studio and title and year uniquely detr det determines um it it does not determines star actually so um this is actually a bcnf violating functional dependency because um um title and year obviously can not be the key for this relation because it does not uniquely determine star ok so um title and year is not a super key as  noise  as the star attribute is not in the the the closure of title and year so to decompose this relation just remove title and year along with length and studio ok so so so there is no star here um just remove title and year along with length and studio that that it define and put them in a separate relation and retain title and year along with star to form the other relation therefore we get um  noise  two kinds of relations  refer slide time  00  43  33  that is movies is divided into title year length and studio because title and year define length and title and year define studio so so so we have um separated them from movies and and what ever is left out that is star is combined with a prime which is title and year and retained as it is so so movies one and movies two are are decomposed um um forms of movies and it is also easy to verify that movies one and movies two are bcnf complaint  noise   refer slide time  00  44  08  um there is one property of bcnf relations the the the notion of two attribute relations suppose i have any relation that has just two attributes um we do n't need to um do anything it is always bcnf complaint how do we ensure or how do we prove that a two attribute relation is always bcnf complaint consider these four cases let us say there is a a relation r which contains just two attributes a and b ok now there are four possible scenarios the first one is there is no functional dependencies between a and b that is their exist no non-trivial functional dependencies only trivial functional dependencies are there a defines a and b defines b in which case r is definitely in bcnf ok secondly there is a non-trivial functional dependency that is r defines a defines b but there is no functional dependency of the form b defines a which is also no problem in because in this case a becomes the key because there are just two re two attributes and a is defining b and b dos is not defining a so a is the key of the relation um and the the relation is in bcnf similarly if b defines a and a does not define b then b becomes the key and suppose a defines b and b defines a which is also no problem because both a and b are keys and we can um both a and b are candidate keys and we can use one of them as are super key or or the primary keys which which is fine and and the relation is also in bcnf  noise   refer slide time  00  45  46  we now come to um another um form um um of um um normalization which is called the third normal form of a relation um the the third normal form is useful because in some cases it is not possible to decompose the relation um such that the the decomposed relations are bcnf complaint it is not possible to decompose without losing some information now um what is an let let us let us understand this by an example now consider another relation um of um of the form which is shown in the slide here let us say we have a relation called drama ok and it has the attributes title theater and city that is there is a particular drama troop that is performing um a drama having a particular title in a particular city um um and in a particular theater in the city ok so therefore we can um identify the following functional dependencies the first one is title and city the title and of the drama and the city in which it is being played will probably determine the theater as well ok so we can say that in this drama played in this city is being played in this theater only ok suppose there exist a functional dependencies of this form ok and let us also assume for the um um for the sake of argument in this case that um um given the name of a theater we can uniquely identify where the theater is um given the name of a drama theater lets say something like kalamandira or um or guru nanak bhavan or whatever some kind of theater name we we immediately know which city in which um which contains the theater ok now fd two violates bcnf as you can see here because theater is uniquely defining city but theater is not the key in fact title and city form the key title and city define theater so so given a title drama title and a city we can um identify theater therefore um this is a bcnf violation in um violating fd  refer slide time  00  48  03  however based on the um based on the decomposition rule if we decompose drama into two relations um um title comma theater and theater comma city base based on this rule um here because theater um defines city ok now basecs on based on this rule if we decompose drama like this into drama one and drama two it will actually be incorrect why because once we perform the join um once when we decompose relations um um and we join back the relations we should retain all the properties of the original relation now when we perform the join between drama one and drama two then the the key constraint will no longer hold let us look at this by by an example  refer slide time  00  48  46  let us say drama one contains um title and theater entries like this there there there is a particular drama with with a title say yugant and it is um um in a particular theater now there is another um drama with with a different title but in the same theater maybe at some other time ok now this theater theater uniquely identify city let us say given a theater name i can uniquely identify city now if i join if i perform a natural join between theater that is drama one and drama two based on the attribute theater um what are we going to get  refer slide time  00  49  23  we will get a table like this that is um a title theater city and title theater city now um if you um see given a title and a theater the there is no unique um title and th um title and city um is no longer the key that is we we we do n't know um um we can have um we we can have possibly different um um let us say um we we can possibly have different sets of um attributes that are defined by the same title and city combination ok  refer slide time  00  50  05  so it dose not uniquely determine the title that is theater and city does not uniquely determine the the title of the drama such dependencies or such discrepancies occur because of a particular property and which is what we are going to see in this slide here ok so we are going to define the notion of third normal form which is um um which which is essentially um um relaxation of the second normal of the rather the bcnf um or or the boyce-codd normal form assumption what is a relaxation that that that we are um um doing here if you notice in the bcnf violating constraint that is ti ti theater defines um city the the the right hand side of the relation that is right hand side of the functional dependency called city is actually part of the primary key that is title and city was the primary key ok so in in order to accommodate such cases we use a third normal form assumption which says that any relation r is said to be in third normal form if there exist any non-trivial fd of the form a defines b either a is a super key which was all the the the condition we need for bcnf and here we have an extra condition which says or b is a member of some key ok so an attribute that is a member of a key is called a prime attribute so therefore um either b should be a prime attribute or a should be a super key which which is what makes um um um makes the relation into a third normal form relation  refer slide time  00  51  33  we shall not be going into details of how to prove um properties of third normal form relations however we will su suffice it to note that it is a um um it is a slightly general form of the bcnf or the boyce-codd normal form let us quickly um um visit the last um kinds of dependencies and the and the next normal form that that results from it which is known as the multi-valued dependencies now um even in bcnf um bcnf is a strict um form of um um decomposition so even in bcnf we we have not fully removed all possible redundancies consider this following example here the the slide shows a example relation called drama which has the following attributes title theater director and genre of the drama ok and we note that um drama is in bcnf because um let us say title is the key ok let us say each drama is played in um in precisely um um one theater ok for the sake of argument therefore title uniquely det determines theater and director and the genre of um of the drama ok however um it may well be possible that a given title may be classified into two or more genres that is a given um um drama could be classified as comedy and it could it could also be classified as a social commentary ok now because it is classified as um um under two or more categories there exists what is called as this um one to many relationship between title and genre ok so every time um um every time we identify the the set of all comedy dramas the theater and director has to be repeated similarly every time we identify social commentaries we um the theater and director has to be repeated because it is the same drama  noise   refer slide time  00  53  37  so um this is what is called as a multi-valued dependencies now what is multi-valued dependencies in a in a more formal fashion now suppose in a given relation a um there is um um there is a non-trivial functional dependency of the form a prime defines b ok and and suppose a prime is also also a key for because its in bcnf now suppose if b is completely independent of all other attributes of the relation then we say that there is a multi-valued dependency in this case the the um um the attribute theater and director are completely independent of the category or the genre of the of the drama it has no relationship between a theater may may play any kinds of drama and and a director may direct any kinds of drama himself so so its completely independent of that so we say that there is a multi-valued dependency ok  refer slide time  00  54  37  so we we defined the notion of a non-trivial multi-valued dependency in order to remove them ok so firstly what is the notion of a non-trivial multi-valued dependency um um a multi-valued dependency of the form a um define a prime defines b is non-trivial if um b does not b is not a subset of a prime which is the non-trivial property for bcnf as well and there exist certain other attributes in addition to a prime and b that is a prime union b is a proper subset of a the the set of all attributes that is there exist some more attributes in addition to b so a relation r of a is said to be in fourth normal form if for every non-trivial um functional dependency of of this form that is a multi-valued dependency b a prime is the super key  noise   refer slide time  00  55  31  so um we shall not be going into um more details of the fourth normal form and um for for this session we will suffice it to say that um fourth normal form is is an even most stringent um criterion for removing duplicates or removing redundancy in um um in in relations so essentially the normal forms can be um um categorized like this third normal form is um is the most lenient among the three and next is bcnf and finally the most stringent is the fourth normal form so if any relation that that is complaint to fourth normal form is automatically complaint to bcnf which is in turn automatically complaint to third normal form  noise   refer slide time  00  56  15  so um we now come to the end of this session and let us briefly um summarize what all we have studied here so we um um we studied the notion of functional dependencies which is a generalization over keys and properties of functional dependencies like transitivity reflexivity and augmentation extra we also um saw the notions of trivial and non-trivial functional dependencies and how they affect um bcnf and and how we can decompose relations into bcnf so that we can remove redundancies however we see that not all relations can be decompose into bcnf without losing information um because um of which we we have also we also need the concept of a third normal form and we finally saw the notion of multi-valued dependencies which can remove redundancies that exist in bcnf and the fourth normal form which um forms which remove multi-valued dependencies so that brings us to the end of this session transcription  shobana proof reading  vidhya database management system dr s srinath lecture # 9 er model to relational model mapping entity relationship model are what is called as the er model and the second was the relational data model we also saw a typical database design um process and placed these models into appropriate possessions in the process  refer slide time  00  01  31  the entity relationship model or the er model is essentially meant for human comprehension it it basically is meant for um creating a conceptual database or conceptual schema or a or what is termed as a logical schema of the database system and the relational data model is used for the physical schema or something that is implemented on um in the dbms so and both or dbms independent models that is no matter which companies database that you are going to use you can still use the same er model for representing your data and even the same relational model for representing the schema that goes on to your dbms now um um we in in this session we are going to address one important issue now are these um we we are going to ask the question are these two different data models er and relational model completely um um independent of each other or are they the same or is it is there some way i can map between the the er schema and the relational schema without having to um break my head too much essentially that means to say or um to put it to to take it to its logical extreme can i design some tools or can i design some kinds of software that um takes an er diagram of a given system and generates appropriate relational schemata for for the system and we have also seen in the session on functional dependencies we have seen how we can optimize a given relational schema up to a certain extent using some kinds of automated techniques we have saw we have seen how to take a relation to bcnf or third normal form or or the forth normal form and so on so um um suppose we want to build a tool to automate this process um we need to be able to first map between an er um um data database schema and a relational schema and um and then use techniques from um um functional dependencies to optimize this relational schema so that we can build a database um application around it so what we are going to study today form the underpinnings of what are called as um lifecycle tools or database life cycle tools there are several different lifecycle tools which provide support for the entire life cycle of a database systems starting from the conceptualization of the problem to the actual implementation of the application and um and maintenance of the database and so on so before we begin um let us briefly summarize  refer slide time  00  04  11  um what we have learned about er models and relational models the er models as um you already know is used for conceptual modeling of the database schema conceptual modeling or um um to to create the logical database schema this is meant for human comprehension this is essentially used to show end users what you have understood about their problem domain this is a high level database design and their no implementation details about that that are included as part of the um er model and of course the er model is a dbms independent and um it is made of building blocks like entities relationships and attributes which which can be attribute at both entities and relationships  noise   refer slide time  00  04  56  in contrast um a relational data model is um um the data model that is most popularly used for physical schema design a physical schema is is the schema that is actually implemented on the computer therefore the the relational data model is meant for or optimized towards machine consumption that is um how do we efficiently store data in um in in my database how do i efficiently search for a given data element how do i efficiently update a given data element so that it does not create anomalies how do i efficiently delete data elements again without creating any anomalies and and and so on and of course the db the relational data model is also dbms independent that is um no matter what kind of database that you use you can still use the same data model as far as the the database that you are using is a is a relational database you can use the same data model to to represent your data on the dbms um of course um reality is quiet different from um from from the concept of dbms independent and some dbms systems may include more features than traditionally what is supported by the relational model um the relational model also um supports some kinds of automated optimization techniques which we have seen in the session on functional dependencies where um you can optimize a given relational schema you can reason whether a given relational schema is is optimal or not whether its going to create redundant um data in um in its dbms or whether its going to create some kind of anomalies during updation and deletion or so on and how we can systematically change the database schema without changing the correctness but increasing the overall efficiency in terms of retrieval and updates wha and what are the building blocks of the relational model we have relations which um um which which comprise of several different attributes and the notion of keys forms um a a very crucial role or place a very crucial role in the relational database model  noise   refer slide time  00  07  09  let us come back to the the er model and look at some of the notations which will require if we have to um um if i have to study translation into er models the entities are um  noise  represented using rectangles and a strong entity type um that is um an entity type which has its own key attribute and which represents a physical or which represents some kind of um um logical um entity of the of the real life is represented by a rectangle with solid line surrounded for example the slide shows this entity type called employee which depicts all objects of type employee which um which are present in the present current system on the other hand we also have what are called as weak entities weak entities are those which do not have an existence of their own or um without being associated with a strong entity type um the slide shows the example of an insurance record an insurance record doesn t mean anything unless it is associated with some person um in a in a company for example an employee therefore um we have to when we talk about insurance record we have to say whose insurance record and so on ok so that that is the general idea um um more specifically the insurance record entity type does not have any key attribute it has to be um associated with with a strong entity type called employee which in turn has the key attribute therefore such weak entity types are depicted using dashed lines or dotted lines for the rectangle we then have the relationship type for example a relationship called handles so employee handles project or something like that um um which which is represented by a diamond and um are are normal relation type is represented by a diamond using solid lines whereas um what are called as identifying relationship types that is the relationship types that identify um a weak entity or provide an identity for weak entities by associating them with strong entities they are shown with double lines in the diamond  refer slide time  00  09  22  entities and attributes are associated or entities and relationships are associated with attributes which are some values in a given domain attributes are depicted using ovals and a normal attribute or a simple attribute is depicted by a oval with um um with with a solid line and key attributes in this example um um um an attribute called pan or pan number which uniquely identifies each income tax payer is shown as a key attribute and it is shown underlined saying that this this attribute is a key attribute for this entity type and then there are multi-valued attributes which um which can have several values for the same attributes we took an example of the color of a peacock now the color of a peacock is um is actually given by several different colors and all of which um in combination form form the color of of the bird such kinds of attributes are depicted using double lines as shown in the slide here and then there are derived attributes that is attributes who s values can be derived from other um attributes and these are shown using dotted lines we took an example of the age of an employee that is if you know the date of date of birth of an employee and the current date we can derive the age of an employee  refer slide time  00  10  44  let us are also look at some definitions from the relational um um um model the relational model is based around um um the the the notion of a mathematical relation now a mathematical relation is said to comprise of atomic values or atomic data values and what is atomic data value a data value is called atomic if it can not be subdivided into smaller values for example the age of a person similarly each data value is said to reside in a domain in the er model a domain is also called a value set which is a term that is generally used by several um people and in the relational model usually the term domain is used which is going to um um which which is going to specify the range of values that a particular attribute can take similarly a relation schema or a relational schema is is denoted by a schema name that is um um in this example shown by the name r and a set of attributes in this example shown by a one a two until an and each attribute has a specific value that lies within the domain specified as domain of ai  noise   refer slide time  00  12  00  we have also um um define what is known as the degree of a particular relationship  refer slide time  00  12  07  the the degree of the relationship is is simply the number of attributes in its relation schema if you remember the the same definition of the degree of a relationship also apply to the er model that is a relationship diamond can be a binary relationship or a ternary relationship unary relationship or a n-ary relationship that is it can um it can be associated with one two three or any number of entity types  noise  and um the the slide shows that the relation is actually um um is actually a subset of the cartesian product of all of the domains that that form the attributes  refer slide time  00  12  44  in the relational model the notion of keys plays play a very crucial role especially we saw in the notion in in the in the process of decomposing relational um um schema in order to make them normalized or conform into let us say bcnf or third normal form or forth normal form and so on so let us revisit the notion of keys in a in a little more detail and keys are again very important when um when we translate from an er model to a relational model we have to be um aware which attributes are the key and which attributes are the foreign key and so on so a key constraint um um in the relational model es es essentially defines the notion of the a superkey which is a set of attributes of a relation which can uniquely identify each um um each tuple in the relation that is um each instance of the the relation and a key or a or a minimal superkey is something um which um um which is minimal in a sense that if you if you remove any element of the minimal superkey it seizes to be a superkey anymore and um there is also the well known entity integrity constraint in in the relational model which says that the primary key um of um of a given tuple may never be null the primary keys is is the minimal superkey that is going to be used to uniquely identify um a a given tuple in the relation and we also saw the notion of referential integrity which is again a um um an important issue in um in the relational data model and the referential integrity constraint says that um if a tuple of one relation refers to um another tuple of another um um relation it should refer to an existing tuple that means foreign keys that is keys um of primary keys of another relation embedded into the re um into the tuple of yet another relation should refer to tuples that already exist in the um um in in the first relation  refer slide time  00  14  54  so the foreign key constraints are shown in the slide here that is first of all the attribute of the foreign key or or the domain of the foreign key should be the same as the domain of the primary key of the other relation and they have to refer to existing tuples in the other relation  refer slide time  00  15  13  and we also saw that relations can be um um or or popularly viewed as tables and which is what is a notion used in sql that is a relation of the form student with three attributes roll number name and lab can be ref specified in the form of a table with the name student um and three kinds of columns called roll number name and lab  refer slide time  00  15  39  so um so so let us not come to the the issue of mapping between given er model and um um um and a relational database model now um um i i had i had said in the beginning of this session um why this such a such a mapping is important there are several different commercial tools that are available which are called as lifecycle tools of um a dbms um design a lifecycle tool provides support in several or um or in most of the phases of a typical database lifecycle that means the tool should be able to or using the tool you should be able to create a logical schema talk to your end user saying this is what i have understood by your requirements of um um of your um of your systems these are the different data elements these are the different functionality requirements that that form er system and so on and then um um using the same tool you should be able to create physical schemata from the logical schema by automatically translating them to to whatever extent possible in practice it its not possible to completely automate this process that is automatically generate a relational model and um optimize it sometimes some kind of human intervention is necessary um um in ord when when the human knows some domain knowledge that can not be captured into the er model but there are several such tools an example is is the tool called erwin from from computer associates which um um which provide such a support for automatically translating between er and um the the relational model so let us see how we can go about  refer slide time  00  17  20  such a translation the first case that we are going to take is the case of a simple relation or or a simple entity ok so the slide here shows a simple entity type called department and it has three different attributes department name department id and manager and the department id obviously is the key um or the key attribute of this department now given such a relation it is fairly obvious to se given such an entity type it is fairly obvious to see that it can be translated into a relation of the type department that is also shown in the slide here just below the figure so um this er um model can be um translated into such a relation where the name is called department and which which has three different attributes department id department name and manager and also note that the key attribute is retained that is the department id which is the key attribute of um this entity here becomes the primary key of the relation that s formed so this is straight forward that is as long as we have a simple um entity type with simple attributes note that the attributes are also simple there no multi-valued attributes or composite attributes and so on and um it can be translated in a straight forward fashion to the relational model  noise   refer slide time  00  18  41  what happens if we have a composite attribute remember that a composite attribute is something that is made up of sub attributes a composite attributes is different from a multi-valued attributes that is a multi-valued attribute is something which can have many values for the same attribute the color of a bird can can have several different colors on the other hand a composite attribute is made up of two or more other attributes each having its own domain for example the slide shows a composite attribute called department id um um for the same example of a department entity type so the department id is the attribute here which has which is a composite attribute which in turn is made up of two other attributes called location id and region number so on ok now a location id could could have a sev could have a different domain let us say um um let us say location ids are given alphabets like a b c d and so on and region number are given numbers one two three four and so on ok so both of them may have different domains and um the they combine to form the attribute called department id which in turn is is also the primary key of this department therefore we are considering two different um um um aspects here one is how to deal with  noise  composite attributes and the second is what happens if the composite attribute is the key attribute of um um of the entity type so um the slide here shows um um shows an ex um shows the examples of and um shows how we can translate this into relational model firstly the the name department of the entity type becomes the name of the relation called department and all the other simple attributes are retained department name is retained as department name manager is ret retained as manager and only here for the composite attribute the department id never app appears here its just that all the simple components of the comp of the composite attribute or or straight straight away loaded into this relation that is location id con comes here and region number comes here and in fact if either of these two let us say location id or region number is again a composite attribute and it has some more attributes just take all the simple components of the com composite attributes so so do n't take region number and just take whatever is the simple component of of this region number and add it to the relation here and all of these simple components which form um the the um the the composite attribute which is the key becomes the primary key that is the the the the primary key here is a composite key made up of um two or two or more different attributes which combinedly identify or help in identifying a tuple of the relation  noise   refer slide time  00  21  37  the next example that we are going to see is the example of how to map relationships um first of all let us look at the following relationship that is shown in the slide here what characteristics can we um um um can we ascribe to the relationship that is shown here firstly um we notice that the relationship here um is a one is to one relationship that is one employee is associated with one insurance record ok or rather the other way around in this case that is one insurance record is associated with one employee and have a look at the association as well the association um or or the relationship type um is um is is an identifying relationship type that means the relationship type called insurance details shown in the slide here um is used to identify or provide an identity for the insurance record by associating it with an employee and also the um the insurance record has a total participation in this relationship that is insurance record has no existence without um this relationship ok now um how do we translate um um um translate such entity types ok now um so so essentially the the the idea here is what how do we translate um um um insurance record into the relational model ok so um  noise  so so for weak entity types the um um the translation is shown here in the in the slide below just create a relationship um or a or a just create a relation of the same name as the entity type but since it does not have um um a um a key to um because weak entity types do not have key attributes so since it does not have a key attribute use the key attribute from employee with which it is associated with and take that key attribute and make it into the key attribute of um um of insurance record however note that since pan number here is also the primary key for employee this has to be made as a foreign key of insurance record that means um um whenever we are we are updating or altering the table we have to use the cascade option um um  noise  whenever let let us say the the the employee type is updated or deleted that means to say that if if the employee relation is delated deleted from the database um um this ha has to in turn delete all the insurance record relations from the database itself because insurance records do not have any existence without the the employee records ok so um so so so the three steps here in ord in order to translate a weak entity type is to first identify or is to first to locate the identifying relationship and see which entity type is this weak entity type associated with and um use the primary key of that entity type has the key for um the the um the the weak entity type or the or the record of or the relation for the weak entity type and make it into a foreign key of um of this um entity type and use cascade options whenever updations or um deletion are performed on the strong entity um entity type  noise   refer slide time  00  25  05  let us move on with um  noise  translating relationships um so how do we translate um let let us take the simplest form of relationship again the one is to one relationship we saw um what happens or how do how do we translate one is to one relationships when um weak entity types are considered ok now let us consider an entity type which is not weak but still invol is involved in a total participation that this is shown in the figure here the figure shows a a relationship type called managedby which relates two different um entity types that is department and manager and there are attributes relevant attributes are shown for each of them that is the department has a key attribute called department id and a manager has a key attribute called employee number and the relationship itself um has a key attribute called secretary that is a secretary is assigned for a department that is managed by a manager that is um um the the the secretary attribute does not have um an existence without an existence of this relationship that is if if a department is not managed by a manager then there is no secretary that is associated with with this ok now how do we translate this the translation is again shown in the slide below  noise  so first create um um create an entity type um or create a relationship um  noise  create a relation called manager le let me re repeat this again for this for this relationship create a relation um in the in the rdbms model called manager with the following um um attributes now you might be wondering why should we create the the why should we create a relation called manager why not department ok now let us um um um think about it a little further see in this slide here that manager is um is a strong entity type it is not a weak entity type however it is involved in a total participation in this relationship type what is the total participation the total participation is that um the entity type does not have any existence without um um being participating in a relationship type of this kind ok so what it essentially means that means here is that a manager has no existence that is a manager would probably be just an employee um so so a manager would would have no existence unless here she is associated or is given a department to manage ok so um so it is the um it is the entity type that is involved in the um um um in the total participation is taken as the primary entity type or or the base entity type um um um primary relation called manager and then employee number becomes the key here that is the the key for um manager ok and the department id which is the primary key for department becomes a foreign key in manager and whatever um um attributes um are associated with the relationship itself become attributes of this relation here that is of the manager relation here ok so therefore the manager relation has a primary called primary key called employee number and um a foreign key called department id note that this makes um um this makes sense when we note that manager is a um does not have any existence without this relationship that is um um refer to the problem of referential integrity in um relational data model what is the referential integrity stipulate whenever a foreign key refers to a tuple in another relation the tuple should exist that is it should refer to an existing tuple in the other relation now um if we had made department as the base entity or or the base relation here we can not use employee number as um um as the um as a foreign key because the manager relation wo n't even exist before this this relation that is managedby is formed ok on the other hand department has an independent existence without whether or not a manager is associated within therefore um um um that forms the rational behind um why we choose the entity type which entity type which um um which is involved in total participation as a base entity type for the translation  noise   refer slide time  00  29  49  so um let us summarize um um this the the previous slide once again so in any one is to one binary relationship between types s and t choose one of them as a base relation incase one of them is involved in a total participation choose that as the base if um neither department nor manager where to be involved in total participation it doesn t matter which um you are going to choose as the base um relation include the primary key of primary key of the other entity type as a foreign key in the base relation and include any relationship attributes as attributes of the base relation  noise   refer slide time  00  30  29  consider the um um example shown in the slide here what happens if um um in a one is to one relationship both entity types um um that is  noise  both entity types that are participating in this one is to one binary relationship are involved in a total participation take a look at the slide here um the slide shows two entity types project and consultant ok and each project is uniquely identified by a project id and each consultant is uniquely identified by his or her pan number and there is a relationship called re relationship type called consultation which has its own attribute called secretary and its a one is to one  noise  relationship and both of them are involved in a total participation that is a project has no existence unless um um it um it is being consulted by a consultant and a consultant has no existence unless he or she is associated with a project so neither of them will have independent existence without the other ok in such cases um um um we we can not identify any relation as the base relation if we identify project as a base relation and try to use pan number of the consultant has the foreign key then referential integrity could be violated it s the same in the other way around as well if we use consultant as the base relation and try to use project id as the foreign key again there is a chance of violating the referential integrity in such cases the the simplest way is to take the relationship type in  noise  this case the consultation has the base relation that is um um form a relation um um called consultation and use project id and pan as um as the primary key of consultation and then all of the attributes from both of them will become attributes of this relation  refer slide time  00  32  27  so um incase both entities in a one is to one binary relationship or both or um or or both in total participation then we merge um both of the entity types into one usually in the name of the relationship that is um um in the name of the relationship called consultation in the previous example  refer slide time  00  32  48  now let us see how do we map one to n relationship what is a one is to n relationship that is um n entities of one of the entity types could be associated with one entity of the other entity types that is it it form some kind of a tree relationship that is one entity being associated with n different entities of the other type so the slide here shows such an example that is employee works in department that is employee is an um entity type so n different employees can work in one department that is one department may have um um several employees but each employee is associated with only one department and of course there are keys called employee number for um employee and department id for department the slide also shows how we can um reduce this to a um to a um um relation the the simplest way is to take the um entity type on the n side of a relationship ok so in this case the employee so  noise  take this as the base that is um translate it into a relation called employee and the the primary key of employee becomes the key of the employee rela entity type here and the department id becomes a foreign key here ok  noise   refer slide time  00  34  05  so this is um as simple as that that is for for each binary one is to n relationships identify the relation um um s that represents the entity type on the n side why why is this so because each entity type on the on the n side uniquely identifies a department that is uniquely identifies the entity type of on the other side ok therefore we can use the primary key on the um um of the entity type on the other side as a foreign key in the in the base relation  refer slide time  00  34  36  therefore use this as the base relation and create a relation um um including the key of the other entity type as the foreign key  refer slide time  00  34  50  how do we map m n relationships now what is an m n relationship an m n relationship essentially says that m different entities of the first type can be associated with n different entities of the second type therefore there is no unique identification that is given an employee in the previous case um one could uniquely identify the department with which the employee is working in because each employee can can work in at most one department on the other hand here let us say um um a relation called relationship called deputedto so um an employee could be deputedto several departments let us say ok so um so m different employees can be deputedto n different departments and of course um employee has has employee number has the key and department has department um id has the key and so on ok and deputedto also has um has a um has an attribute called record number which maintains a record of which employee is deputed where and so on ok so in order to um um translate such um relationships note um um note the steps that are shown in the slide here there are three different relations that are that are formed um one is the employee relation ok that is one of the entity types of this relationship type so so the employee relation is formed with employee number as the primary key and and all the other attributes that that form the employee entity type similarly a department relationship um relation is found with department id as the um as the primary key and all other um um relations and then a separate relation is created for the relationship type itself so deputedto becomes a separate relation by itself and then uses employee number and department ids as foreign keys and also has the primary keys of this of this relation and um whatever attributes that belong to the  noise  that belong to the relation um becomes um um part of um or belong to the relationship type becomes part of the relations that is created here that is um um deput the the record number attribute becomes one of the attributes of deputedto relation ok note that we can not move this recorded record number that is the attribute of this relation to either the employee or um a department because it does not uniquely identify either employee or department um each employee could be associated with um n different record numbers because they are they are they could be associated with n different departments and similarly each department could be associated with m different record number because m different employees could be working in that in that department  refer slide time  00  37  43  so this slide shows summarizes how um m is to n relationships are translated in a m is to n binary relationship it is not possible to collapse the relationship into one of the entity types because neither of the entity types uniquely identifies the other entity type therefore a separate relation is required and usually this in the name of the rela relationship type itself ok so separate relation is required um um in order to complete the mapping and of course the cascade option should be used whenever updates are performed on any of the um relations pertaining to the entity types that is whenever the  noise  whenever the entity type called employee or department is updated or or deleted then these um um changes should be cascaded so that they are reflected in the deputedto relationship as well that is if employees employee entity type is um is deleted then of course the deputed to relationship should also be deleted  noise   refer slide time  00  38  53  now one might ask the question um um is it possible to um um is it possible to use the strategy that is use the relationship type as the base relation rather than any of the participating entity types can we use the strategy for mapping one is to one and one is to n relations as well because m is to n is simply a generalization of one is to one and one is to n relationships right so of course it is possible that is um um take the example of employee works in department that is n different employees working in um one department we can still create um a separate relation called works in where it can use the employee number and the department id as the foreign keys of of this relation however um um it it it just creates an extra relationship or extra relation in the in the database that is that is totally unnecessary but um this this is sometimes um actually attractive to use than than than collapsing the relationship into one of the relations especially where um um especially if you have to avoid null values that is especially if we have to um um especially if we have cases where there are some employees who do not work in any department if n different if an employee can be associated with at most one department it means that um an employee can also be associated with zero department so in that case the the department id field of um of employee would be null it does not violate referential integrity because remember that referential integrity says that a foreign key should refer to an existing tuples or else should be null therefore um it does not violate um referential integrity but creates a lot of null values in the database schema in in the database itself so if we have to avoid null values it is actually preferable to use the relationship type as the base relation when performing the translation  refer slide time  00  40  57  how do we map multi-valued attributes we have seen how to map composite attributes and simple attributes and keys and so on but what happens if there are multi-valued attributes composite attributes um um um are different from multi-valued attributes in the in the sense that each of them can have several different domains that is it is just a combination of several simple attributes so we just open up the combination when we are um when we are translating a composite attribute and then include all the simple attributes that that form part of the composite attribute on the other hand a multi-valued attribute is not a um um um is is not a composition of several um um several sub attributes instead it is an attribute that can take on several values instead of one value um and the example the slide show is that of a bird ok so a bird has a multi-valued attribute called color ok so what is the color of this bird a bird could have several colors it it need not have just one color and of course there is a primary key called species which which identifies each bird um um uniquely so in order to translate multi-valued attributes um um take a look at the lower half of the slide which shows um um two different relations which which make up this translation the first relation shows um um shows um um a relation called bird with species as a primary key and all other attributes except the color attribute ok all other attributes of um of the entity entity type called bird and then a separate relation is created called bird colors where species and color are both um um are both included and are both part of the key that is um um combinely defined the key of this bird color therefore we say that bird species eggs has color y eggs has color z eggs has color a and and and so on ok so the the color attribute may be repeated in in um in several or several tuples or rather the species attribute may be repeated in several tuples one once for each different color that the bird can take and both of them that is species and color become the primary key for bird colors  refer slide time  00  43  20  so for each multi-valued attributes um um of a given entity type we have to create a separate relation that has a primary key of s paired with all possible values that the multi-valued attribute can take and of course the cascade option should be used for referential integrity um um for um on the bird um um relation that is whenever the bird relation is deleted or updated the corresponding changes has to be made in bird colors as well  noise   refer slide time  00  43  51  how do we map n-ary relationships until now we have been looking at binary relationships what happens when there are n-ary relationships and um um different entities forming part of the relationship the slide here shows such an example that is um the the standard example of supplier supplies part to project ok so um there is a supplier who is uniquely identified by the the supplier or or the sales tax registration number or something streg number and there is a project that is uniquely identified with project id and there is a part that is uniquely identified by part id and then the supplies relation which um relates all of these three different entity types so the the simple way of translating this is to to use a separate relation called supplies  noise  as the base and of course separate relations each for supplier project and part with their corresponding um um primary keys and the supplies relation which um which has the primary keys of um um of of each of these relations has the foreign key of this um um of this relation  refer slide time  00  45  02  so for each for each n-ary relationship of um of any type r where where n is greater than two we have to create a new relation s to to represent this relationship um um to to represent this relationship type r and of course the the primary keys of of the participating relations um become foreign keys in in this new relation and the cascade option should be used for all of the relations that correspond to the entity types that um um participate in this relationship type what happens if one of the relations in um in an n-ary relationship type is a weak entity type that is um let us say part is a weak entity type there is no existence for part unless it is associated with a supplier supplying it to some project in that case we have to identify um um um we have we have to first um it um we have we have to first identify the um entity type which gives an existence to part ok and the part um um relation here has only foreign keys it does not have any primary keys but the supplies relationship does not change that is it doesn t have any um part id it it just has a it it simply has the supplier primary key and the project primary key without the part primary key there therefore um um therefore we get two different um relations that have foreign keys and um and they do n't have their own primary keys  refer slide time  00  46  42  so um so that um that brings us to the end of the um this um this session that talked about that give a um that that talked about how we can um map er models into um um in into um relation relational database models using several different rules and of course this is not a comprehensive set of rules because there are several other sets of rules used for example in um um derived attributes or um  noise  or enhanced er models like um generalization and specialization which which are not covered here but all of them in totality are used um um to to to to create the basis for any kind of a um tool software tool that can translate between given er schema and its corresponding relational schema so um this slide shows a summary of um each of this mappings that is it gives a set of thumb rules saying if this is what is given in the er model then what happens in the relational model ok so in an er model if an entity type is given then a corresponding entity relation that is a relation in the name of the entity ty entity type is created if a one is to one or a one is to n binary relationship is given then um we create corresponding foreign keys from from the n side to the one side or or from the weak entity type to do to the um to the strong entity entity type in this relation ok so we create a relation and create appropriate foreign keys from them if m is to n relationship type is given as shown in the slide here then we create um um um um then then we create a relation with the name of the relationship type ok so so so there is a um um within quotes which is shown as relationship relation with two foreign keys that is one for each um um entity entity type that participates in this relation  noise   refer slide time  00  48  44  if an n-ary relationship type is given um it is still the same strategy that is we create a relationship relation with n different foreign keys that is one for each um um um entity type or or rather um um n different foreign keys as long as these en entity types are strong entity types if there is a simple attribute in a in a er model that that simply becomes an attribute in one of the relations in the relational model if it is a composite attribute in the er model then it becomes a set of simple attributes that is you take the simple part of all component composite attributes that is just um um um um go on finding um um the the the simple attributes that that forms the composite attributes and then make all of them as part of the this relation  noise   refer slide time  00  49  33  if it is a multi-valued attribute then we need to create a separate relation and a foreign key that is you have to associate the primary key of the base relation um um with each possible value of the multi-valued attribute if it is a value set it becomes a domain um um value set in the in the er model it becomes a domain and a key attribute in the in the primary key in in the er model will become either a primary or a secondary key in the  noise  in the in the relational model so that brings us to the um um to the end of this session transcription  shobana proof reading  vidhya database management system dr s srinath lecture # 10 storage structures looking at what might be termed as the logical aspects of database design we looked at different data models how data can be represented and how the relationships between them can be represented we saw the er schema which is meant primarily for human comprehension and we also saw the relational schema which we um claimed is meant for the the physical schema that is meant for machine comprehension  refer slide time  00  01  40  but when we say physical schema it is still a kind of misnomer because the the relational data model does not say anything about how data is actually stored on computers or or storage device like disks or um whatever wherever databases are implemented so in this session and the next few sessions we are going to actually ripe a part in in in a sense look inside a dbms or inside an implementation of a dbms and see how are these data elements actually stored on computers and um what is it mean when we say that we have stored um um a table how is a table ac actually residing on um on disks or or any other kind of storage device so that is what we will we are going to be concerned with in the next few sessions so um let us begin the session on storage structures  noise  in order to be understanding storage structures one of the first things we need to understand  refer slide time  00  02  39  is the the what what might be termed as the memory hierarchy when we talk about storage or storing data the first question we need to ask is where is data stored there are different kinds of devices um um wh which which are capable of storing data you might um um you might have obviously come across um hard disks i mean data that are storing stored in hard disks floppy disks cdrom and even the computer ram that is the random access memory in the computer the cache memory within um um within the machine and even the registers are within the cpu all of them are meant for storing data and all of these can be organized this kind of um these kinds of different kinds of memory devices can be organized in the form of a hierarchy which is um um termed as a memory hierarchy this slide shows such a memory hierarchy but it um divides this hierarchy into two kinds of storage devices what are called as primary storage and what are called as secondary storage if we were to draw a hierarchy primary storages would storage devices would would would appear up in the hierarchy and below them would be the secondary storage devices what is a differences between the primary and and secondary storage devices primary storage devices um some examples are shown in the slide here like cpu registers cache memory ram dram sram and and and so on all of them are extremely fast memory devices you can address or retrieve data elements extremely fast from from these devices however all of these are volatile memory devices that means once the power is switched off they no longer can hold data data in that are stored in primary storage devices can not be persistent in in nature on the other hand secondary storage devices which of which some examples are magnetic disks like your hard disk on your pc magnetic tape which is primarily used in in many locations for for um archiving data or or taking backups of data then there are cdroms there are read-only cdroms there are um write-once cdroms there are even some kinds of read-write cdroms that that are being available today and there are also what what is a more recent phenomenon which what is called as the flash memory flash memory is a is a kind of is made of what are called as eeproms that is electrically erasable programmable read only memories which can store data persistently um um even after power is switched off and they can perform or they they can perform data transfer um in in a rate that is much faster than existing storage devices like say magnetic disks or tapes and and so on so the common theme in the in secondary storage devices is that data can be stored in these devices in a persistent fashion and usually secondary storage devices are much cheaper than primary storage devices and they can store much more data than can be stored in a primary storage device however usually secondary storage devices are much slower to access they they inquire much more over heads in during access than accessing a primary storage device like say ram or cache memory or so on  noise   refer slide time  00  06  06  now for the most part when implementing a database management system we shall be concerned um mainly with secondary storage devices we do not concern hassles with what are called as main memory database which um which are which are databases that are completely held in main memory there are there are not many implementations of main memory databases simply because its  noise  much much more expensive um um to to to have large main memories which which can implement databases of of sizable um of of of a pretty large size so um secondary storage devices have certain kinds certain characteristics which which are important or which influence the kind of storage structure that we are going to use to to access and store and access data in these devices we can either categorize secondary storage devices either random access device or a sequential device something like the pc hard disk or magnetic disk or random access devices that is you can access any given block of data in in a um in a magnetic disk they are called sectors so you can ac you can access any particular sector um directly it is not purely random access because it does performs some kind of um um sequential searches however for um for most practical purposes um or mag magnetic disk is a random access device where um you can you can address any um block directly and move to that block read or write to that block directly on the other hand something like a magnetic tape is a sequential access device if you have to access the hundredth block and and the the tape is rewound you have to run through the first ninty nine block before being able to access the hundredth block therefore um it is its important um wh what what kinds of how efficient can can data access be um when when when we are using a device which is either random access or sequential for example we can not implement a storage structure that that has to perform a lot of pauses on on the data um um mainly because um especially when we are using a sequential device similarly we can ha um classified devices as either read-write devices write-once devices or read-only devices read-write devices are those where you can read and write data any number of times hard disks floppy disks um magnetic tapes and so on are examples on the other hand there are write-once devices with where you can write data once but you can not erase it once it is written it becomes a read-only device so um  noise  um so such kinds of um um devices can not incorporate data structures which need to be modified during runtime for example similarly there are read-only devices which um where data is stored during the manufacture of the device itself and it can never be altered and um and so um new data can not be stored on such devices again so um there are um devices especially in in embedded systems which which stores small databases within read-only devices and it s extremely important that um such storage is um um is is performed correctly the the first time because there is no scope for any kind of modifications once data is written on to these devices then there are um devices that can be classified as either character devices or block devices character devices are those where you have to read um um data character by character which can be extremely inefficient when we are dealing with large amounts of data some kinds of tape devices um and so on are character devices on the other hand there are block data access devices where um usually the the unit of data transfer is um a set of characters called a block so um every time any read request is given a read request reads an entire block of data into memory and writes back an entire block of data on to the device and usually there block access is coupled with what are called as read aheads that means its not just one block it s a set of blocks that are read into memory at a given point in time in order to um increase data transfer efficiency  noise   refer slide time  00  10  40  what are the requirements for storing databases what kinds of storage requirements to to to databases pose firstly we note that um almost all databases required data to be stored permanently or what is called as persistently for longer periods of time it should not be the case that once the computer is switched off the all the data in the database is lost it has to be stored permanently or in a persistent fashion usually databases are far to big to fit in main memory it is not realistic to to be able to search a database by loading the entire database into memory and then um um use such techniques on that that are mainly useful for main memory we have to store the database on disk and involve strategies that that can search data on on to the disk and use memory for for this purpose  noise  um over the years the the cost of um um storage has dropped drastically in fact there are many cla claims that um the the um  noise  progress in storage has beaten what is called as moore s law that is the the amount of um um storage that can be packed into um a given cost for a for a given cost or or or on a or a on a on a given square area of of physical dimensions today we can um we can have gigabytes of data stores in a very small device that one can store within your um in in pockets or on in a very small um area we have gigabytes of data stores that that are embedded within watches for example and within pens and and so on so um the the amount of um the data store that that are the are the storage available per person has increased dramatically over the years and one of the byproduct of this is the changing definition of what might be termed as very large databases in fact um the the term very large databases when it was coined um was meant to refer to databases which are of hundreds of megabytes large slowly very large databases came to mean several gigabytes of a data and now we have databases that are several hundreds of gigabytes or terabytes which are ten to the power of twelve bytes and then even petabytes of data petabytes are ten power fifteen bytes of data where especially databases that work on web related data like like google or altavista or or search engines which which collect data from all over the web actually work on petabytes of data so um  noise  so the definition of very large databases has been changing continuously to include more and more data storage requirements  noise  therefore what is being what has become imperative today is to design extremely agile data structures that can um that that can store and manage data between main memory and secondary storage devices in a um in an efficient fashion  noise  when when we store data on to secondary memory we usually distinguish between two kinds of data storage the one is what is called as a primary data storage the primary data storage talks about how we store the data itself um how data itself is organized on on to disks or or any storage device and how are they accessed secondary file structures are those are are also called as auxiliary or augmenting file structures are those sets of files that are used to speedup access to these data elements and this is especially important secondary um um file organizations become especially important when um the size of the database starts growing by leaps and bounds when we have terabytes or petabytes of data it is um um the the the role of auxiliary files or secondary files that that provide pointers to um that help us in locating the required data element becomes more and more important  noise   refer slide time  00  15  04  usually when we talk about data storage on um um on secondary devices we are um we are talking about what are what is termed as file organizations data is stored in logical structures called files on on disks the way files are organized on disk is called the file organizations usually files are stored as a sequence of records and a record is um um is analogs to the notion of a tuple in a relation um um um in in relational algebra or a row in a um in in sql parlance so a file is um um stored as a sequence of these physical um or logical records and and are stored in terms of what are called as physical blocks as we as we saw before in previous slide the block is the unit of data transfer between main memory and the storage device so um so there are two different um um things here within a file one is the logical ordering of data which is in the form of records and then the physical um storage of data which is in the term terms of blocks um there could be one is to one correspondence between records and blocks which is very rare which means to say that each block is one record or um um there could be many records per block or many blocks per record depending on um how um we def define or record the structure the fi the term file organization refers to the way in which records are stored in terms of blocks and in the way blocks are um placed on the storage medium and are interlinked so that they can be access from wherever there are three different kinds of file organizations that we are going to see um um in this session today the first one is what is called as the unsorted or the pile file or organization and the second kind of organization is what is called as the sorted file organization and lastly we are going to look at hashing file organization  refer slide time  0017  09  let us um um  noise  briefly look at the notion of record and blocks which is important for a for us to understand these different kinds of file organizations a record like um um we mentioned earlier represents a tuple in any relation it is a logical unit of of data um which um of inter related data which is of interest to the user or the database management system a file is defined as a sequence of records and records could either be fixed length or variable length remember in sql for example you can use variable length strings and variable length integers and so on so um so the length of a given record may um cou could also be variable or fixed and records comprise of a sequence of different fields and fields is the as same as a field is the same as a column in sql parlance or um or in attribute in relational parlance  refer slide time  00  18  09  blocks so um blocks like we mentioned earlier is the physical unit of data transfer or data storage in in storage devices they correspond to for example sectors in in hard disk or page in virtual memory systems and so on they store usually um um a block stores records from a single file but it need not necessarily be such a case it it depends on wh depends on the file system structure blocks are usually of fixed length blocks can not be of varying length unlike records and the the length of um um a block is dependent upon physical characteristics of of the storage device and also of the operating system and the ram and so on so um many times the the database management system itself does not have much control over the size of a block a storage device is termed to be either defragmented or fragmented depending on whether contiguous sets of blocks on the storage device belong to the same file or or to different files  refer slide time  00  19  19  we now define a term called the blocking factor which is important to um to to determine how records are packed within blocks the blocking factor is the number of records that are stored in a block um on on a given storage device it is constant across blocks the the um this this blocking factor is constant across blocks if record length is fixed on the other hand if record length is variable then the blocking factor is also variable because the number of records per block may vary from block to block blocking factor is simply defined as the number of blocks divided by the um or or or the size of block divided by the size of the total number of records so bfr as shown in the slide is is b divided by r where b is block size and r is record size and this is a floor function that is um um which takes a lower integer value of this division since um um the record size may not exactly divide block size there would be some amount of wastage which is given by this formula that is um um each block in each block there is a wastage of this much amount of bytes that is blocking factor times the record size number of bytes  noise   refer slide time  00  20  41  so how is this wastage um um wastage manage there are two kinds of approaches to managing this wasted block area when records are stored within blocks the first is um to do nothing that is do n't use um um let wasted spaces be such kinds of techniques are are used in what are called as un spanned records that is a record may not span multiple blocks this slide shows such an example the slide shows one block which um spans from here to here comprising of three different records um and there there is no space for a fourth record however there is some extra space that is left in the block which is left unused on the other hand there are what are called as spanned records  refer slide time  00  21  28  a span record is something that can span across different blocks this slide shows such an example um um this block contains three records and there is not enough space for the fourth record however part of the fourth record is placed in the remaining space leaving a small amount of space for a pointer to point to the next block in the in the logical sequence of record and wh wherever the next block begins the remaining part of the fourth record is stored and then um um the the next records are stored here so um such kinds of record organizations are are term to be spanning organizations where a record can span across multiple blocks i am sure you would have notice that if record size is bigger than the block size we have to necessarily use spanning organization for storing records because we can not store records into blocks otherwise  refer slide time  00  22  30  so this is what this slide says that is when when record size is greater than the block size that is r is grater than b then usage of spanned records is compulsory  refer slide time  00  22  41  when we have variable um um um record sizes or when we use spanned  noise  um record allocation we can um we can term what is called as the average number of blocks that are required per um or we can compute what is called as the average number of blocks required for storing a collection of records  noise  so this can be um um in order to compute this we first simply compute the blocking factor that is the block size divided by the average size of each record then the the following formula where  noise  where r is the number of records divided by the blocking factor will give the number of blocks that is required for storing a particular database that is particular set of records  refer slide time  00  23  30  let us now start with the the the different kinds of file organization techniques the first kind of file organization technique that we are going to see today is what is called as the unordered file organization this is also termed as a pile file where um wh the the the term meaning that records are just stored as a pile in inside the file this slide shows such an example that is um records are coming in um um into the system and they just being appended to the file record one record two record three record four without regards to what data that are contained in these records and how they are going to be searched and of course these data these these records eventually go into different blocks and they are they are managed in in some fashion by the operating system underneath in the machine  refer slide time  00  24  23  a pile file is the simplest form of file organization we do n't have to do anything for um um for organizing this file and um insertion of records is the simplest that is records are inserted in the order of their arrival and um um on the other hand in if we have to search this file um we usually need some kinds of auxiliary files or we we we we require some kind of help in order to efficiently search these files for for a given data element therefore insertion is very easy um however searching is extremely expensive um um because we will have we will have to do a linear search we have to just search through the entire file in order to um find the data element that we require  noise   refer slide time  00  25  07  what about deletion in pile files deletion posses yet another tricky problem in in pile files and um um which which can create certain kinds of fragmentation problems have a look at the slide here the slide shows a pile file containing three records and some part of the block is um is empty and um there are other blocks as well in the in the file now suppose that record two has to be deleted now once we delete record two and empty the space we can not reclaim back this space because the insertion algorithm for for a pile file is not cognizant of this of this extra space that it can use in the the insertion algorithm simply inserts records int at the end of the file it just appends records to the to the file therefore um such a kind of deletion strategy is inefficient in terms of space use usage  refer slide time  00  26  06  when we are using variable length records in a pile file we encounter another unique problem um and this is of record modification whenever some data is modified in a record as long as the record is of fixed length it does not matter we can make the modification in place and write it back into the file however if we allow for records to have variable size and the modification results in um um in in the size of the record to grow there may not be enough space to to write back the record this slide shows such a example there are three records in this file or rather four records in the file record one record two record three and record four and record two is modified now um um the modification is such that the the size of record two increases now we can not write back this record at the same place where it was earlier therefore we will have to mark this um um mark the earlier record as deleted or or unused or something like that and write back record two at the end of the file this is shown in the figure here and of course we need to update any kind of auxiliary data structures that um that point to record two so that it points to the new location in the file  refer slide time  00  27  27  the second kind of file organization that we are going to consider are what are called as sorted files sorted files are those are those files which are physically sorted on on the disk based on some field called the ordering field so the the file is actually the or rather physically sorted on the disk so when you read the file um on disk in a particular order it it provides or it returns back records which are sorted based on the ordering field ordering field should be a key field or i should or it is um it is recommended that the ordering key ordering field should be a key field that is it should be unique for each record and should belong to an ordinal domain what is an ordinal domain an ordinal domain is something where you can establish a total order among the um um among elements of the domain for example the set of all integers is an is an ordinal domain um the set of all names for example is not an ordinal domain we can not place um one name with respect to the other unless of course we impose some kind of an ordering like say lexical ordering we say um we order the names as per the lexical um rule that is a comes before b b comes before c and so on um  noise  in sorted files insertion and deletion are both expensive because we have to ensure that the file remains sorted at all times um especially when a new record is inserted with um with um ordering field which has to go somewhere in the middle of the file rather than at the end of the file and updation of a record may actually involve physical migration of the record especially if the ordering field is um modified however searching in a sorted file is is made simpler because of the because we can use what is termed as binary search what is binary search  refer slide time  00  29  34  we shall not go into too much details of binary search let me just give a small algorithm of what a binary search um um looks like essentially the the binary search um um technique is a technique where we divide the search space by half that into um in each iteration of the set that is we reduce the search space into to half of the um previous search space in each iteration so this slide shows a simple algorithm for binary searches we see in the first step here that we start with two bounds left and or lower and upper bound ok the lower bound is at one that is the the first element or the first record and the upper bound is said to b that is where b is the number of blocks in the file um um or the last block in the file and in each iteration we are going to compute the mid point of these bounds that is l plus u divided by two is a is a is the midpoint now suppose we have to we have to search um for a given key value k we say we we we read the records from form block i that is a midpoint and then see if um um um and compare that with the required key attribute now there could be three different options that is one is the the key attribute is equal to the key that that is read from the block in which case we have found the record therefore return we we return a success on the other hand a key attribute could be less than the midpoint in which case um we have to search the lower half of our search space that is we have to search between um um one and um i minus one that is l and i minus one um on the other hand if key attribute is greater than the midpoint we have to search in the upper half of the database that is we have to search between i plus one and u so um this um um this series of steps is performed until until and unless or as long as um u is greater than or equal to l that is the the upper bound is greater than or equal to lower bound whenever they they cross that is whenever we um um whenever the upper and lower bounds cross without having found the given record we um we are able to conclude that the record does not exist in the file and then we we say it is not found so binary search um we are not going to going to um going to detailed analysis of binary search here however one can verify that a binary search technique requires an order of what is termed as log n where where n is the number of blocks in the in the file um um a binary search requires an order of log n number of disk accesses where as a linear search which actually searches through the file um requires an order of n number of block accesses for um for searching that is on an average n by two number of block access have to done for a linear search whereas only log n to base two number of access need to be perform for a binary search  noise   refer slide time  00  32  45  let us now look at um um one more technique um um by which sorted files can be made more efficient in terms of insertion and updation note that whenever um um a sorted file has whenever a new record has to be inserted into a sorted file um it is it is always a problem because um the the file has to be always sorted physically on the disk its not a logical sorting um that that is being performed here therefore whenever a new record is being inserted as shown in the um um slide here let us say um record with key one is um um is already in the in the file next um key value of three is already in the file and key value with seven is already in the file and now we um um we receive a record whose key value is four now what do we do with this record in fact this record has to appear between record three and record seven in the file physically on the file that means that we have to physically move record seven below and then insert record four here  noise  so that the file remain sorted this is an extremely expensive operation especially if lot of um  noise  insertion operations are taking place in order to mitigate this problem  noise  another technique what is called as the overflow file is used an overflow file is um um is a secondary file or or another file where  noise  records are stored in an unsorted fashion whenever a new record is being added to the um to the database um it is just added to the record or the overflow file in the form of a pile file that is its just appended to the overflow file and periodically um um that is in a less frequent fashion let us say once in a month or um once in a weak or a on a weekends or something like that the the overflow file is merged into the master file or or the actual sorted file that is the the overflow file is first sorted and there are a number of merge algorithms that can take two sorted files and merge them together in an efficient fashion and using this the the over flow file is merged back into the master file um when such a technique is is used for sorted files searching also um becomes a little bit different from a pure binary search that is um whenever a key has to be searched we can perform a binary search and the master file and incase the key is not found in the master file we have to perform a linear search in the overflow file so there are two kinds of searching that that has be done when overflow files are used in sorted file organizations  refer slide time  00  35  35   noise  so um to summarize sorted files we see that sorted files are more efficient than pile files especially for key base searches however they are suitable mainly for random access devices note that if we have to perform a binary search  noise  um um and binary search over a tape device we may have to keep moving back and forth in the in the tape device quiet often which makes it more which makes it terrible inefficient therefore binary searches are more suitable for random access devices rather than sequential devices it s a better choice where a database is mostly read only because insertion is always a problem insertion and updation is is a problem we have to use overflow files or physically move records and merge and so on um and mostly queries are key word key based retrievals  noise   refer slide time  00  36  28  the third kind of file organization that we are going to see today the third and the last kind of file organization that we are seeing today is what is called as hashing file organization what is meant by hashing hashing is a means of providing very fast access to records on certain search conditions and um what are these search conditions these search conditions are usually the equality condition based on a key field that is whenever i what to search a record having a particular key attribute it is not note that it is not something like whose keys are less than a particular key attribute are greater than a particular key attribute and so on this is useful only when we are searching for records whose key attributes are equal to the attributes attribute that is given in the query hashing techniques uses what are called as hashing functions or um which are also termed as randomizing functions that map particular keys into buckets for hosting records and just like sorted file techniques even hashing techniques are primarily suited for random access devices  noise   refer slide time  00  37  39  before we go into how hashing is performed on disks let us have a look at what is called as internal hashing internal hashing is is hashing that is performed entirely in main memory and um um in most database management systems hashing is used extensively um um in main memory in order to quickly access a given data element among a set of data elements that have been loaded onto memory so usually such kinds of hashing techniques uses an internal data structure um that has something like static array of m different buckets and they are indexed from zero to m minus one and they are several candidates for for such a hashing function a simple candidate is um is to just compute the mod or or the remainder of um of the key with m that is the m where m is the number of buckets in the um for the hashing function there are also other um hashing algorithms called folding the key where um a given key attribute is twisted and folded in different ways in order to come out with a number that um um that is uniformly randomly distributed across the the set of all buckets that form the array um um static array of hash buckets there are also other techniques like sampling the key and and so on which um we we are not going to be seeing here  refer slide time  00  39  02   noise  external hashing is um um is the um hashing technique that is used for managing data on disks rather than in memory external hashing um um comprises of blocks on the disks which act as buckets and in turn are augmented by one or more blocks that hold the the the hashing array itself or or the set of buckets that that form the hashing array itself so the figure here shows um um um a typical hashing process given a particular record with a key attribute k it is first put through a hashing function um which is called hash of k here and this hashing function maps it onto a particular index entry in um in an array of buckets each index entry here in turn has an address of one or more blocks which form this bucket usually um one block can be allocated to one bucket or it could also be more than one blocks that are allocated to one bucket this once the block is identified this record is just appended to this block so whenever um um um a search has to be made on this key um we have to make a sequential search within the block however we can reduce a search space drastically especially when there are large number of records on to a single block or a set of blocks that form a given hash bucket  refer slide time  00  40  31  so um to summarize external hashing again um external hashing uses two levels of indirection that is hashing into buckets and searching within buckets a bucket is usually one disk block or a set of contiguous blocks which is also important here that is there is no point having non-contiguous blocks as as part of hashing because we again need to store some information on how to access these um um blocks from from one another a bucket can hold more than one record obviously um um um and also this depends on records size and we have to perform a sequential search within a bucket  noise   refer slide time  00  41  10  hashing has to um um has to content with a with a contentious issue of what is called as overflows what happens if we choose a hashing function that tries to hash every key onto a very small number of buckets um  noise  um it especially the this can especially happen when the dataset itself could be could be skewed even if the hashing function that we have chosen is a reasonable one that is um um given a set of keys that are uniformly distributed over a given range this randomizing function uniformly distributes it over the set of all buckets however if a dataset itself is skewed the um um we have a large number of key values um um near a particular value rather than um all across the range then the hashing function would also be correspondingly skewed in such cases what happens um um is that buckets could get overflowed that is um um they they could um um the the number of records stored in a bucket could could go beyond the capacity of the bucket itself in such cases there are several techniques that are used for overflow management there are three different techniques that are that are primarily used the first one is what is called as open addressing open addressing simply says that if this bucket is full just use the next available bucket which has um which has some space in it ok so um um so so um um once um hashing function hashes on to a particular bucket and then we find that it is full we start a sequential scan or a linear scan of the bucket space um for the next available bucket in which we can store the record the second kind of technique that is used for managing overflows is is what was called chaining chaining is um um the um technique of maintaining a link list of different buckets so that when a particular bucket is full it maintains a pointer to another disk block or another set of disk blocks acting as another bucket which can hold some more data in them and so on and when that becomes full there is another chain and so on however because um um hashing has to perform sequential searches within buckets um if we encounter or if we end up with a long chain of buckets it becomes terribly inefficient in terms of searching then the third kinds of kind of technique is what is called as rehashing where um we try to use another hashing function if the first hashing function doesn t work that is maps to an overflow bucket we we use hashing function two and then see if it works and then hashing function three or whatever and and then combine it with um something lok something like open addressing or chaining in order to manage overflows  refer slide time  00  43  57  so this slide here shows the um um shows the concept of chaining where there are main buckets here which in turn have um um have pointers to overflow buckets and um these pointers point point to exact records in these overflow buckets and each record here has a next record pointer which points to the next um overflow record that are um that are managed in the by this overflow buckets  noise   refer slide time  00  44  25  until now we have been looking at um um kinds of hashing where the bucket space or the number of buckets is fixed such kinds of hashing techniques are what are called as static hashing techniques and we have already seen what is the limitation of a static hashing technique a given hashing technique might work generally that is a given hashing function might be good enough so that um if the sets of keys are uniformly distributed the hashing is also more or less uniformly distributed however when the set of keys are skewed when the data itself is skewed um um um using a static hashing might be terrible inefficient because some amounts of buckets could be overflowing um um while a large number of other buckets could be more or less empty in order to obviate this need we um use what is called as dynamic hashing dynamic hashing is um um um is the process where um um the the number of buckets can change dynamically can grow or shrink with time um as and when keys are being added or deleted from the database  noise  the overall strategy in dynamic hashing is is quiet simple and it is shown in the following three steps the first is we just start with a single bucket to begin with and um um and we hash everything on to this bucket once the bucket is full we split the bucket into two separate buckets and then continue with the hashing process this is um and then we redistribute the the records or the data that is stored within a bucket such that um they are more or less uniformly distributed across the two different buckets this process continuous um um the the process of splitting continuous whenever there is an overflow and then there is the process of merging that happens whenever there is the underflow that that happens that is when a when a bucket becomes empty  refer slide time  00  46  17   refer slide time  00  46  19  we now look at a simple dynamic hashing technique um um where we ca which was how buckets are can be split and merge the the slide shows um  noise  one such technique here we see that um we we have a small diagram here which shows two different kinds of nodes or or data structures this kind of data structure the circle here is what is called as an internal bucket and the square or the or the rectangle here is what is called as the leaf bucket or or an external bucket the leaf buckets or those which actually store the data initially all data is stored in a given in in a single bucket that is this is the bucket for all records and um um assume that now we um our keys are made of binary strings and then we are storing all of our data with these keys in these records now suppose there is an overflow that happens here in this bucket ok now what happens when when there is an overflow  noise   refer slide time  00  47  19  this slide shows um um shows such a technique that is when when there is an overflow the bucket is split into two different buckets and um um you can notice the labels here um for for the edges joining these buckets the first bucket is the set of all records whose keys start with zero and the second bucket is the set of all records whose key start with one assuming that our um our keys are made of binary strings now suppose there is again again an overflow in this in this bucket and there is no overflow in in the um upper bucket here what happens to the um hash table then  refer slide time  00  47  56  the hash table then changes to to this following data structure where the the overflow bucket is split and a new internal node is created and two different um buckets are um are then created so this bucket now is the set of all um um holds the set of all records whose key start with one zero whereas this bucket holds the set of all records whose key start with one one so we can trace that starting from the start um start node here so one zero takes us to this bucket and one one takes us to this bucket what happens now if one of this bucket um encounters an under flow that is it becomes empty by um when um um when when records are deleted from um from the database we just have to merge this bucket with with its partner so to say that is um um see in the diagram here that this is the bucket who whose edge is labeled as one we have to just merge it with its partner whose edge is labeled zero  refer slide time  00  48  57  so this takes us back to the previous configuration where um where we had only two buckets in the hash table  noise   refer slide time  00  49  03  there is another kind of dynamic hashing what is called as extensible hashing which also uses a similar kind of um um hashing technique in order to grow and shrink buckets extensible hashing uses what is termed as a global directory of two power n number of bucket addresses where where n is called the global depth of the directory and then um each bucket is uniquely identified by some set of higher order bits um d number of bits which is less than or equal to n which can uniquely identify each bucket and of course buckets are split and merged whenever um they are um they they overflow or underflow and correspondingly n is changed that is the global depth is either increased or decreased in a corresponding fashion  refer slide time  00  49  50  this slide shows such an example um um here we have a global depth of three that is n equal to three and there are several different um um bucket pointers that shows zero zero zero zero zero one and so and so on and there are several diffent different buckets each with differing capacity where um um here this bucket says that d equal to two that means this bucket can be uniquely identified with just the top two bits that is um all um keys starting with zero zero can go into this bucket here for example d equal to one that is all keys starting with one go into this bucket but here these two buckets have a large number of data elements that is where d equal to three that is this this bucket contains all keys starting with zero one zero and this one contains all keys starting with zero one one now suppose what happens if the  refer slide time  00  50  43  last bucket overflows that is where d equal to one this bucket um um is then split so that d becomes two and then instead of um um instead of just one bit we have to use two bits in order to uniquely identify this bucket therefore the the top bucket here the upper bucket here is the set of all um um keys which start with one zero and the lower bucket here is the set of all keys that start with one one  refer slide time  00  51  12  so that brings us to the to the end of this session where we have looked at several kinds of file organizations and um for for physically managing um records um on on storage devices let us quickly summarize what we have learnt in this in this session we first looked at different kinds of storage media and what are their characteristics we can classify storage media into different kinds volatile non-volatile primary secondary and so on in fact they can be placed in a hierarchy and then there are different characteristics like um random access sequential access or um um read-only versus read-write or write-once and so on and then there could be either character devices or block devices and so on each of them each of these characteristics impact the kind of data structure that we can use for um um storing records we then looked at the concepts of records blocks and files which are the terminology we use for um um dealing with data that are physically stored on on to disks records are the logical unit of data that that are stored while blocks are the physical unit of data that that is that that is used for data transfer and file is the set of records or or a sequential records in which typically a relation is stored we also saw the notion of spanning and un spanning of of records um um in terms of how they affect the the blocking factor or the number of blocks that are required to store a particular file of records we then saw three different fi kinds of file organizations the first one was the pile file organization which is the simplest where we just um um append records into a file however which poses problems with insertion or rather with um um deletion and updation and um also with search we then also saw sorted files which are files that are physically sorted on on disks based on some um ordering attribute sorted files are much more efficient for search because we we can use binary search on sorted files however they are um um they pose very tricky problems in terms of insertion and updation of records and they are especially um um um they they become especially tricky when records can can can be of varying lengths and updations can change the the key value of on which it is sorted we also saw the last kind of file organization called hash files where hashing function is used in order to identify the block number or the bucket in which particular record is stored we also saw how um or what are the um challenges that that are faced by hash functions especially when we use static hashing techniques because static hashing techniques can not um um can not work efficiently when the dataset is skewed in which case we have to use dynamic hashing where the number of buckets in the hash table can  noise  dynamically grow or  noise  shrink as and when data is inserted or deleted so that brings us to the end of this session lecture # 11 indexing techniques single level hello and welcome to another session in um database management systems in our on going exploration of dbms we have a kind of graduated from looking at a data management from a logical perspective to looking at a data management from physical um perspective we have look at the how we can represent data conceptually using say the er schema or um in in a more physical form that is in in a way that is um friendly to the computer as um as in the relational data form and so on however all of these models were basically mathematical models that um that gave us a kind of formalism which told how we can represent data elements and how we can represent relationships among data elements and so on we graduated from there to um seeing how data is actually stored on the computer what kinds of overheads to we incur when we use one kinds one kind of storage method towards another which kind of storage method is um easier in terms of lets us say insertion easier in terms of updation easier in terms of maintenance um easier in terms of searching and so on um we looked at three um basic kinds of file organizations and compared them in terms of a um their complexity of insertion updation um search and so on and in our session um storage structures we have also mentioned that when we talk about storage files there are basically two kinds of files that we are concerned about what is called as a primary file which are the data file contains the actual data that  noise  that is stored in the database system and then set of one ore more secondary files are what are called the auxiliary files which contain metadata which help us in accessing data elements as efficiently as possible in today s session we are going to be concentrating on these secondary files or these secondary files are called as index files which provide one or more index structures that can help us in accessing whichever data element we need as efficient as  3.19  possible so let us look into index methods by firstly briefly surveying or briefly summarizing what we have learnt about storage structures  refer slide  03.27  first of all what are the storage requirements of databases what kinds of um what kinds of requirements we are looking at here databases need data to be stored in a permanent fashion or um what is termed as persistent fashion for longs period of time it shouldn t be the case that once power is switched off um all your data is lost it should be there for much longer period of time then the typical user session on a computer and usually the amount of data that we are talking about is too large to fit in memory we saw how the definition of very large databases has been changing over the years initially the term very large databases was used to mean hundreds of mega bytes of data and now we are talking about peta bytes of data which is actually ten ten to power fifteen bytes of data um this is especially true in databases like um web log i mean web databases as as in web search engines like google altavista or so on they routinely deal with peta bytes of data and infact the um we also saw how the how the storage technology has been has been beating moore s law in the sense that larger and larger amounts of storage is now possible in a smaller and smaller surface area and also in at lower and lower prices therefore storage bytes is not a big problem storage is cheap secondary storage is especially is is cheap quite easily available however the bigger problem now is to search for databases we have we have anyway stored peta bytes of data but how do we search the the relevant data items or whatever data that we need in as efficient a fashion as possible imagine um how would it be if you used a search engine like say goggle and you gave a web search and it gave you a request to come back after two days to to look at your search results it is a it is unthinkable we are looking at um we are looking at response time that is interactive in nature at most a few seconds before which the user gets bored um the use can not wait beyond that therefore um a search engine like this has to search potentially a data size of peta byte of data for before giving results for a given request so in order to um efficiently handle these data structure or this vast amounts of data what is getting more important now is the set of secondary or auxiliary file structures using which we can try to efficiently access um the data that is stored in primary databases  refer slide 06.25  let us briefly go through some of the definitions that we studied in storage structures let us review some of them because they are again important when we are talking about indexing methods first the notation of a record a record is the physical counter part of what could be termed a tuple in a relation or a row in sql parallels6.49 and um  noise  on disk a data is stored in terms of files and file is treated as a sequence of record so one might um analogously say that a files stores a given relation or a given table all that need not all um exactly be the case because a file sometimes files are used to store more than one relations or relation is sometimes spread across different files due to some physical considerations like maximum file length that is allowed by operating system and so on however in a general sense we can consider a file to be representing or to be storing um are to be the physical counter part of um of a relation records could be either fixed length or variable of length fixed length records are easier to handle in terms of um quickly finding their location in a file for example finding the offset of a given record in in a in a file however not all data elements can be amenable to fixed length records especially when we have to store data elements in form of text where a text can range from few words to thousands of words um so if we allocate a large amount of memory for um for the text field it would be unduly wasteful when when we are using fixed length records in which cases we use variable length records and records themselves comprise of a sequence of fields a field is analogous to a column in in sql par lens  8.19  or in attribute in the relational algebra     8.21  refer slide 08.25  within the concept of blocks a block is a physical unit of storage in um in storage devices for example sectors in in hard disk or page in virtual memory and so on um they are they are the smallest unit of data that are been transferred between the storage device and the computer and usually we deal with block storage devices um when we are talking about databases we rarely deal with character devices where the the unit of information transferred is a single character blocks are usually um are almost always of fixed length they are not of variable length and the length of a block is based on several characteristics that are beyond the scope of a typical dbms for example they are based on a considerations that that deal with what is the storage capacity of the of the storage device what we are talking about what is the operating system that we are using what is the size of the data bus and um in the in the machine and so on therefore a um the database management system has little or no control over the size of a block and a contiguous blocks in a storage device may or may not correspond to the same file if they correspond to the same data file then it is well and good um in the sense that there is lesser overheads in in accessing a file um we don t need to many um seeks seek is the set of operations that is performed on on any storage device like disk in order to find the correct block that we are that we want from the device so um if contiguous blocks belong to same file we do not require too many seek operations when um accessing a data file on other hand if they do not belong to the same file then we may incur some overheads in in the seek time of the um of the storage device so a storage device is said to be defragmented if um contiguous blocks belong to the same file and it said to be fragmented if there are if the blocks are distributed all across the storage device  refer slide 10.30  when we are taking about blocks there is an important term that we used the the notion of a blocking factor the blocking factor is simply the number of blocks per record that um  noise  that are stored in the database that is if i have a record size of r and i have block size of b blocking factor is simply b divided by r the the the floor of this function b divided by r um if b is greater than r then blocking factor is greater than one that means there can be more than one records per block however if r does not divide b that is um if b is not a pure multiple of r then there is some extra space that is wasted um that is the which is given by the reminder of this division so how do you deal with this extra space  refer slide 11.23  there are two varieties of dealing with this extra space um we saw that records could be either of the kind of unspanned records or they could be spanned records unspanned records are those which do not spam across across different blocks in this in such cases if we have some extra spaces as shown in the slide here they just are unused we we can t do anything about it that is um we just leave that extra space unused which results in certain amount of wastage of space however it is it helps in easy accessing of records from blocks on the other hand  refer slide 12.35  we can we could think of spanned records were records can be split so that they are stored across stored across different blocks this slide show such a um diagram where three records are stored um in their entirety in in a given block m and the fourth record is split between block m and block p and of course at the end of each block we should have some kind of a pointer that points to the next logical block um next block in logical sequence in the disk so so that we can know which block to access next in order to find the the remaining part of the fourth record  refer slide 12.38  and of course whenever b is less that r that is records size is greater than the block size then we have to use unspanned um  noise  unspanned record storage  refer slide 12.54  we also saw three different kinds of file organizations and a file organization is simply a organization mechanism by which data is stored in files so that they can be um efficiently accessed we saw three different kinds of file organizations which which we termed as unsorted files or pile files sorted files and hashing files unsorted files are are those files where um you just input records into the file or just append new records into at the end of the file without any consideration to the data that is present in the data um in the record or in the file unsorted files are very efficient when it comes to inserting new records you don t have to do any kind of searching you don t have to do any kind of reorganization you just have to append it to the end of file however it is very inefficient when it comes to either deletion or modification of a data or searching especially in searching of data in the worst case we may have to search the data file in a sequential fashion and um  noise  this can be a tremendous overhead when the file is extremely large in size when when it is giga bytes or tera bytes of data stored in one unsorted file a sorted file on the other hand is a file organization where the file is physically sorted on the disk based on the some field in a record which is called the ordering field  noise  therefore a physical sorting of um records helps us in easy access of of the file we can use a search technique called binary search that can reduce our search space by half in each iteration so that um usually what is termed as log n order of time we can find whatever record we are looking at however the sorted file organization incurs a lot of overhead whenever insertion or deletion happens in the file whenever a new record is inserted we should ensure that the file remains physically sorted that means we have to may records in order to accommodate the the new records in its place and similarly the the case for deletion in order to remove any kind of fragmentation we have may to move records so that the the overall file remains sorted the last kind of file organization we we saw was the hash files a hash file uses a hashing function which hashes um or that is which is which is a function that can take a value of a key field in in a record and transform it into one of several bucket addresses we saw two different kinds of hashing static hashing and dynamic hashing where in static hashing the amount the number of buckets are fixed and we have to deal with um we have the problem of um containing with overflows especially if the data is is queued that is if the data set or the distribution of keys is queued we may have a small number of buckets um overflowing and i have to be dealt with techniques like open addressing or chaining and so on which which     16.19 terrrible overheads during searching and we could have a large number of buckets that are empty because the none of the keys were hashed to those functions we also saw a remedy to this problem namely the notion of dynamic hashing where the number of buckets could actually grow or shrink in size whenever records are added deleted from the database  refer slide 16.40  let us move on to auxiliary file structures the the main topic of concern today the the notion of indexes let us um first go through a few definitions before we look at actual index structures firstly the notion of an index file an index file is a secondary or auxiliary file that contains meta data or data that helps us in accessing the required data elements from the database an index or or an access structure is the data structure that is used um inside the auxiliary files that helps us in searching for our data elements as efficiently as possible and of course data structure are augmented with their corresponding search methods in order to search for our data um for our data record we can think of two kinds of index um indexes what are called as single level indexes and multi level indexes single level indexes are um have just one level of index structures that is in addition to the primary file there is just one secondary file and the index file maps directly to block or record addresses in the primary file a multi level index on the other hand has multiple levels of indirection where one level of index structure may may point to another level of index structure and so on and finally the the last level would point to block addresses or record addresses in the primary file  refer slide 18.15  let us look at a few more definitions regarding um pertaining to index structures the notion of an indexing field or or or a indexing attribute is the field on which the index structure is built that is um searching is efficient whenever a search a search is given on this field that is on whichever field an index is maintained um for example we saw the notion of the ordering field which is a field based on which  noise  records are ordered on disk in sorted file um organizations an indexing field is analogous in the sense that this is the field on which an index structure is build usually um the ordering field and the indexing field are the same and um usually they are also the primary key that is the primary key is usually the ordering filed and by default an index structures is built on the primary key that is used um that is used in the records a primary index is um is an index structure that is defined on the ordering key field that is the the field that is used to physically order records on um on file in sorted file organizations and in many cases the ordering field is the same as the primary key  refer slide 19.36  we also define a notion of clustering index where a um where these are index structures that are defined on an ordering key field however um in cases where the ordering field is not the primary key or not even a key field that means the the ordering filed need not be unique remember that all key fields have have the unique constraint um that are posed on them and um whenever ordering is performed on a non key field um it is not unique so an index structure on such a non unique um field is called a clustering index we also define the notion of a secondary index which is an index structure that is defined on a non-ordering field um not necessary the key field  refer slide 20.27  let us have a look at primary index to begin with like we mentioned before a primary index is an index structure that is defined on the ordering field of records and that to when um the the ordering field is a key field usually it is a primary key um that on which records are ordered on disk a primary index comprises of um an ordered file um note that even a primary index is a sorted file and um it comprises of fixed length records and has two fields these two fields are shown in the slide as k of i a of i as you can see here that is there is a pair called k of i which is the key for the i th record and a of i is the block address containing the i th record  refer slide 21.21  the figure in this slide shows an example of a primary index here on the right hand side of the figure um the the primary data file is shown where records are divided into several blocks so each block begins with a particular field number and of course the the this is a sorted file organization in the sense that records are physically sorted in these blocks now if you can notice here the first record or or the first field in each block for example the first field in the first block is two thousand three zero one zero one um the the roll number of a student and of the second block is two thousand three zero one two one and of some other block here is two thousand three zero two two one and so on the first field in these um in these um blocks are indexed in the index file that is they appear in the index file here and um so these the first field in the index file is k of i which we saw earlier that is the the key value that is maintained in the index the second field contains the pointer to the block or or the block address that contains this record  refer slide 22.39  what is some of the properties of primary indexes if you have noticed the number of entries in the index is equal to the number of disk blocks that um that  noise  that comprise of the primary file that is the primary or of data file the first record in each block of the file is indexed that is the first record in each block of the file appears as one of k of i s in the index file these records are called anchor records because this is the anchor by which other records in the um or other key values in the block or accessed therefore if you are searching for a key value of let us say um two thousand three zero one two zero as as shown in the slide here um we we perform a binary search on um on the index file and um we come to a point were we realize that it has to lie in the first block itself because two thousand three zero one two zero is greater than two thousand three zero one zero one and lesser than two thousand three zero one two one so we have to find that block address which um or that anchor address which is lesser than um  noise  um or lesser that or equal to the given key and the next block address could be greater than the given key such an index structure in which not all um ordering or key attributes are indexed is called a sparse index the primary index that we saw here is a sparse index a sparse index essentially means that not all attributes or not all possible key values are indexed um more specifically in a primary indexed we are index we are only indexing um one um key value per block on the other hand a dense index or is is an index structure where each key or each search key value that appears in the primary data file is indexed in the indexed file  refer slide 24.50  how do we search using a primary index search is easy we saw um just a few moments ago that we can search using binary search so we just have to perform a binary search whenever we we have search using the key value and then find such a record or or a key value that is less than or equal to the key value that we are looking for um such that the next key value appear that appears in the index is greater than the key value that we are looking for what about insertion and deletion insertion and deletion is a easy if records are of course one fixed length and they are statically allocated to blocks without block spanning what is this mean this means that suppose i allocate suppose i know the set of all um the the entire range of values of a given set of keys and i statically allocate a given sub range or subset of keys to to particular blocks for example let us consider that a student roll number can range from zero one zero one to zero one five zero let us say there are one hundred fifty students who can range from zero one zero one to zero one five zero and then we say that each block contains exactly twenty records therefore we store zero one zero one to zero one one nine or or zero one two zero rather um in the first block and zero one two one to zero one four zero in in the second block and so on therefore we statically allocate  noise  each record to block address if we do that insertion and deletion are easy however they may result in wasted space especially if not all records of um of these of these data set may be available at any given point in time for example if we if we have only let us say zero one zero one and zero one five zero we still have to have a number of blocks wasted um because each address is stored in in a particular block we can not store any other address in other blocks on the other hand if we don t want this waste wastage of space to contain with um  noise  re-computation of um blocks that is moving records between blocks and also re-ordering of the index structure whenever insertion and deletion takes place that is um this is because um primary index is based on a sorted file that is a file on which the the entire data set is sorted and it has to remain sorted whenever insertion and deletion take place  refer slide 27.37  the next kind of index structure that we look at is the clustering index remember the definition of clustering index a clustering index is a index structure that is um  noise  that is used when um a sorted file organization is used and the ordering key or the key on which a file is sorted is not the key field that is ordering field is not a key field what is the implication of saying that um the ordering field is not a key field the implication is that when a field is not a key field um it means that there is no unique constraint on the field that means there could be repetitions that is the same key value in the in the k of i a of i model the the given k of i value may point to multiple addresses or multiple block addresses which which store  noise  which store data values pertaining to the same key the structure of a clustering index file is similar to that of a primary index file in the sense that um even this stores k of i and a of i addresses except that except to this small change that  noise  only distinct values of the um of the ordering field are stored that is suppose we are ordering um a primary data file based on the student names rather than roll numbers we just store one index entry for each distinct student name and not every occurrence of student name and we store um of the block address of the first occurrence of any given student that is the first occurrence of any given um of the particular ordering field is what we are storing rather than all addresses of a given field  refer slide 29.38  let us look at an example the slide in this example shows a data file um where which is a sorted data file of course and where the ordering field is the department number and there is no unique constraint on the department number that is department number may repeat over several records we see here in this example that there are three records pertaining to department number one and three records pertaining to department number two and two records pertaining number three and so on and we also see that the record pertaining to department number two has spanned or um or this key or this ordering field department number two has spanned over two different blocks that is the first block and second block the left hand side of the slide shows the clustering index which  noise  which contains the usual k of i and a of i fields where k of i is the value of the ordering field or or the indexed field and a of i is the block address note the first two entries in in the indexed files the first entry says that um for the value one department number one look up block number one the second record says that or the second indexing record says that for the value of department number two look up block number one again why is this so this is because the the value two appears for the first time in block number one what about the other values of two how do we search for the other values of two note that the file or the primary data file is a sorted file um that means to say that if we know the the first occurrence if we know the block number of the first occurrence of a given um of a given ordering field it is sufficient because the the file is sorted and we can start looking at the next logical blocks in sequence until we finish all the set of records having the same ordering key therefore if we are searching for a particular data record having department number two we have to first start from block number one and proceed in the next um logical block in sequence which in this case is is the block number two until we exhaust all records having department has two  refer slide 32.07  a clustering index is also a sparse index why is this so this is because only distinct values that are appearing in the ordering field are indexed in the previous slide even though department number one appeared three times and and the same thing would be department number number two only a single entry existed in the clustering index field for cluster indexing file for both of these um records insertion and deletion in clustering index may cause problems because the um because of the well known problem of sorted files that is the files have to be sorted and um when we have to retain this sorted mechanism of or sorted form of files this may impact on the clustering um clustering index in the previous example suppose we inserted a new record having department number as one then we have to move all the records having department number two that is the last record in block number one to the next block that changes the corresponding address um the corresponding a of i value in the clustering index field that is um a of i can not can no longer point to the first block but instead it has to point to the second block therefore we need re-orderings whenever we um whenever we perform insertions and deletions there is an alternative solution for um handling this problem of insertion insertions and deletions that is to allocate um  noise  blocks for each distinct value of the clustering field this is shown in the example in the next slide  refer slide 33.53  this slide shows an example where um a clustering index is used on a um on a primary file that is ordered on a non key attribute however there are there is a very specific organization of the non key of the primary file in the sense that each distinct value of the non key attribute of the ordering field is allocated a separate block have a look at the right hand side of the figure more carefully in the first block there are three  noise  there are three records having record number having department number one in case more um more records are inserted having department number one they are not allocated to next logical block in sequence however they are allocated to separate blocks and a separate pointer is maintained to these blocks so that we can access more records having department number one have a look at the third block that is seen in this slide here there are two records having department number eighty and um um and the block can um can accommodate three records of a particular um of this particular size however even though other records exist for example department number eighty nine they are not put into to this block um they are given a separate block by themselves therefore um this kind of block organization results in certain kinds of wastage space because especially since especially if um  noise  if there are not many repetitions of the non key attribute however insertion and deletion are much more simpler in such a organization this is the um because we don t have to um we don t have to worry about any changes in the in either the block structures or in the index structure itself  refer slide 36.02  the next kind of index that we are um  noise  exploring today is what is called as the secondary index a secondary structure or a secondary index file that is used to index fields that are neither ordering fields  noise  nor key fields that is um there is no um there is no assurance from the primary file that the the file is organized or ordered along these fields and they are also not key fields that is there is no assurance that these um the the values in these fields are unique there could be many secondary indexes possibly on a single file that is um depending on the how many fields that are there in a given record a secondary index  noise  maintains one index entry for each record in the file that is if you remember the definition of a dense index a secondary index is a dense index um in contrast to a sparse or a non dense index where not all values of the the indexed field are indexed  refer slide 37.10  this slide shows a shows an example of um a secondary index assume that roll number is no longer the key field and its it not not even be unique so the left hand side of this this slide shows a dense index where each and every um field or each and every value of the roll number field that appears in the primary data file also appears in the index file and there is a corresponding pointer from each um from each these key values to the particular record addresses directly note that we don t have to maintain block addresses here because this is dense index we can directly dereference or directly refer to the the record address that is the block address and the offset within a block where the record begins and note that the file also need not to be ordered based on this secondary index and the and the pointers or the record pointers are arbitrarily shaped when they point to the primary data file  refer slide 38.22  what are the properties of some what are some of the properties of this secondary index since  noise  um if i am performing a secondary index on a key field let us look at some properties of secondary index having um maintaining secondary index on a key field on in previous example um roll number could be a key field in the sense that it need not be an ordering field but it could be a key field the the data file could be ordered on some some other field let us say name or grade something like that but the index is maintained on the secondary or on the key field which is unique that means that since each field or each key field is unique there are as many secondary index entries as there are records in the data file because it is dense index and each um index entry or each key field has to be indexed however the data file need not to be sorted on to the disk that is it need not have um there is no need for that because we are directly referencing is a dense index and directly pointing to the particular record address and because it is key field um and because key fields are unique we can maintain or we can um be sure that we can use fixed length records for the secondary index because we know the length of the key field and we know the length of the address um of a given record therefore the length of a key field plus the address of the the given um record in a primary file forms a length of the cluster of the secondary indexes  refer slide 40.11  what happens if the seconday index is maintained on a non key field that is um where um  noise  where the field that is being indexed may have duplicate values that is it need not be unique um and um it it can have many number of value what is the implication of having duplicate values as we saw in the clustering um index a given value of the indexed field may point to multiple records or multiple records in the primary file there are three different techniques for handling the duplicates in in secondary index note that note the un difficulty that that we encounter in secondary index that is not there in a clustering index in a clustering index we have the assurance that even though the the indexing field that we um using is not a key field the primary file is ordered or is physically sorted according to this ordering field or on this non key field therefore if we just know the first occurrence of a given particular value of of this field it is it is sufficient using which we can access all other values that are present in the database on the other hand when we are using a secondary index and a secondary index um can can be used on fields which need not be ordering fields we don t have any assurance of that sort that is a this is a non key field so therefore there could be duplicates and there is no assurance as to how these records are distributed in the primary file itself so um handling duplicates becomes much more difficult in a secondary index rather than in a clustering index there are three different varieties of handling secondary index duplicates one is to use duplicate index entries duplicate index entries means that we use fixed length records however and of course use a sorted file or physically sorted file for for the secondary index file and maintain as many  noise  index records as there are different values of a given um of a given non key field the second one the second approach is to use variable length records that is have one value of k of i and many values of a of i that is um more than one values of a of i that that point that that um that in turn point to different record addresses or the third approach is to use extra re direction levels which will see in more detail shortly where we can where the first level points to a block of record addresses and so on  refer slide 43.04  the first option is to use duplicate index entries that is index entries are repeated for each duplicate occurrence of the non key attribute the example shows here um that the the term two thousand three zero one zero two is repeated four times and two thousand one zero two thousand three zero zero one zero three is repeated twice in the secondary index file the advantage of such a scheme is that we can still use fixed length records however the searching of the of this data file becomes a bit more complicated binary search becomes more complicated why is this so because remember how binary search works binary search starts with the um starts with the entire um space or the entire set of indexes as the search space to begin with that is the lower bound for the binary index is um to begin with the first record and the upper bound is the last record and then we compute a mid point of of the lower bound and the upper bound compare our key the key that we are searching with the mid point now what happens if we compute a mid point in a data file or in a index file where um where key values could be repeating when it means when we compute a mid point that is when you compute lower plus upper divided by two we are not sure that um there are um we can not say that all records having this particular key will appear either to the left or to the right that is there could be repetitions or there could be duplicate entries of the particular key at the midpoint on either sides of the midpoint so we have to search both sides in order to retrieve our particular or in order to make the next decision about the next iteration and of course insertion of records um would require restructuring of the index table because the index table is always sorted and maintained in a sorted order  refer slide 45.10  the second approach to handling secondary indexes was to use variable length records um  noise  in in a variable length recording record schemes we use um a given k of i value that is if you look at the previous slide here for a given k of i value the size of the a of i field is not fixed that is there could be one or several address fields for the given key index what are the um what are the problems are um are advantages of this approach one advantage is that um binary search while it is still little bit complicated however it does not suffer from the complications of repeating multiple keys that is if we know exactly the block addresses and block addresses are stored in a way that that we can um that we can um find find out the next block address very quickly we don t have to worry about whether a given entry appears on both either side of the mid point however it becomes a complicated in the sense that if variable length records are stored in a single index file we can not um the midpoint may not pertain to or may to point to um a valid block address or a valid block address of the index file and insertion and deletion of the records may require restructuring of the index table so um restructuring in the sense that we may have to add more fields in the in the a of i or we may have add more addresses add more values a of i field which in turn may affect the next fields that appear in the database and of course this also um there is also this problem of spanning and non spanning of of records that is we may have to use spanning records in order to um in order to allocate them into blocks and um associated problems of um a searching and retrieval which we saw in the session on storage structures also um hold for this kind of indexing  noise  scheme  refer slide 47.41  the third kind of um scheme secondary index scheme in order to handle duplicates or um in non key attributes is to use extra redirection levels have a look at the slide shown in this figure this slide shows three different levels of um files that is um the the the right most part of this slide shows the primary data file which contains blocks um which in turn holds one or more records there is a label id or a lab id um field in this in this file which is the um record or which is the field which is being indexed by the secondary index file the lab id field is neither in sorted form in the um in in the primary data file nor is it a key field that is it need not have unique addresses it can have repetitions because it can have repetitions each repetition um that is each distinct record address for a given value is um is given a separate block have a look at the left most part of um the the slide here the left most part is the is the is the usual secondary index file comprising of k of i and a of i fields where k of i contains distinct values of each of the um um distinct values that that appear for the um um for the indexed fields which is a non key field now because this distinct value can um can pertain to several records or several records in the database it is first um the the a of i address first points to a block that is reserved for this um for for storing addresses of this key value or of this index value this block is a fixed length um note that um note this is shown in the second field here note that even though a particular is not full it is not used up um for the second address this is the the block for the second value is a completely different block from the block for the first value even if it is not full that is a separate block is allocated for each distinct value of the um index field and this block contains the set of addresses by which we can um um using which we can perform a sequential search or um on the on the particular address that we require or um retrieve the set of all um records pertaining to the given key value that that we are searching for so um  noise  in this session we looked at um three kinds of indexed files or three kinds of single level index structures namely the primary index the clustering index and the secondary index let us briefly summarize before we end this session um and and look at what are the different kinds of index structures that we saw in in this session today  refer slide 50.59  first of all let us um let us revisit the the extra um indirection levels to and look at some more properties of extra indirection levels um before we sum summarize the extra indirection level is is the most frequently technique for handling duplicate record and the advantage of this is that the index records are of fixed length and it doesn t suffer from complications binary search complications that we discussed earlier and we can use the usual binary search in order to search for a particular given address however um there could be wastage of spaces because um because a complete block is allocated for a given value even though if it is not um it is not completely full and what happens when block blocks overflow that is there are large number of records of the given index in such cases block overflows are handled by chaining which is the same technique that we saw in um  noise  in the hashing technique and retrieval requires sequential search within blocks however insertion and deletion of records are straight forward we don t have worry about restructuring the index or restructuring the the data file we just have to insert the corresponding entries in the in the block file whenever insertion and deletion take place so let us come back and and summarize the the different kinds of um index structures that we saw today  refer slide 52.31   noise  firstly the type of indexes if the index structure is on a key field and the the ordering field is um  noise  or rather the the if the index structure is on um key field and the ordering field which is also an ordering field then an index such and index structure is called a primary index that is usually on the primary key and the keys order so primary index can can afford to be sparse and um and point to um and store and index only the anchor records in each blocks on the other hand if the index is on a non key field however which is a ordering field then it is called a clustering index that means the the data file or the primary data file is physically sorted based on the ordering field which is not a which is not a key field that means there is no unique constraint for this field in which case we can use the clustering index where we just store um we store um index structures for each distinct value of of this field and store the address of the first occurrence of this distinct value because it is ordered we don t have to worry about accessing the other values that that exist in the database because they they appear in in logical sequence starting from the first occurrence if the key field is um or if the indexing field is a key field however it is not an ordering field then we use a secondary index for a key field we saw that a secondary index on a key field is a dense index with fixed length records um on on which binary search can be used efficiently if the field on which indexing is performed is not a key field and is also not a ordering field then we have to use secondary index of the non key variety that means we have to deal with duplicates in one of three different fashions either use duplicate index entries or use variable length records or use the most commonly used technique of extra indirection levels in order to handle duplicate address  refer slide 54.48  and um quickly what are some of the properties of the different index structures that that we saw the primary index um has what are the number of index entries the the number of index entries for a primary index is the number of disk blocks in the primary file or in the data file it is a non dense index or a sparse index a clustering index is also a sparse index which stores the number of distinct index field values that is the number of distinct values that appear in the index file and a secondary index on a key attribute um contains as many number of index records as there are number of records in the data file itself it is a dense index and a secondary index on a non key field may or may not be dense may be either dense or sparse depending on weather the non key field is unique or not that is weather the repetition in the non key field or not and the number of records contained in the index file is equal to the number of distinct values that appear in the indexing attribute so this brings us to the to the end of this session where we saw a different kinds of um single level indexes in the next session we shall be looking at multi level indexes where a index structure can have several different auxiliary files lecture # 12 indexing techniques multi-level and dynamic indexes hello and welcome in the previous session we had looked into different varieties of index structures index structures um are auxiliary files that are used in the database storage that are used to help in accessing data that are which are stored in the primary files index structures are extremely important especially when database sizes have been growing exponentially in the recent past and the the value of index structure is also more important because the main problem in database is today not the storage of the large amounts of data but retrieval of them in rather in searching data elements based on some certain criteria key values and so on we saw different levels of different kinds of index structures namely primary indexes clustering indexes secondary indexes on key attributes and secondary indexes on non-key attributes and so on let us briefly summarize them today um in this session before we move on to more complex index structures  noise   refer slide 02.28  some key definitions in um in index structures are shown here an index file is a secondary or auxiliary file that that is used to help speed up data access that that is contained in the primary files or the data file an index or an access structure is the data structure that is used in these secondary files which um help in this um retrieval process and of course the data structure is associated with its corresponding search methods and algorithms using which we can um  noise  access these data elements as quickly as possible we said that there are two kinds of index structures primarily namely this single level index and the multi level index structures however until now we have just covered this single level index structures a single level index structure is a single auxiliary file that directly maps to addresses in the primary file and these addresses could be either block addresses which which stores physical blocks on disk or any other storage medium or they could be recorded results where where the address of a record is directly stored or a record can be directly accessed within a block that is the block address is augmented with the offset value which gives which gives us the record address  refer slide 03.57  some more definitions which are again important for um looking into multi level indexes which we are going to be um exploring in this session um the indexing field is the field or the attribute on which the index is maintained and usually um the field could be either an ordering field or a non ordering field it could either be a key field or an non key field and the corresponding index structure for each of these fields changes depending on  noise  what kind of fields or what is the characteristics of the field that we are indexing a primary index is um an index that is defined on the ordering key field of the data element that is the field should not only be an ordering field it should also be a key field of the data element what are the properties of an ordering field and and a key field key field has a property that it is unique that it has a uniqueness constraint or that no two fields in the in the database no two key fields in the database have the same value similarly if the field is an ordering field we can be  noise  we can have an assurance that the primary data file is physically sorted based on this field therefore whenever we have um a key value of of key of a given value i we know that for all key values greater than i we have to search forward that is we have to search in the forward direction of the file we don t have to search the um reverse of the file or or all the key values until now and so there therefore these properties the the property that the file is ordered based on the ordering field and the field is a key field helps us in building a primary index there is an sparse index which can which which which can help access data in the primary file as efficiently as possible we also look into clustering index which is the index structure that is used when the field that is that is to be indexed is an ordering field but not a key field if the um  noise  if the clustering field is not a key field then it is no longer constrained by the uniqueness constraints that is there is no longer requirement that each of these each element in a in this key field has to be unique this poses a particular problem in the sense that a given key value may correspond to more than one addresses the last kind of index that we saw was the secondary index a secondary index is an index that is um that is defined over some non ordering field  refer slide 6.45  that is there is no assurance that the primary data file is ordered based on this field if the primary data file is not ordered based on this field then it is not possible for us to store a sparse index this is because we don t know way to search the next record record from therefore the index has to be dense index structure however there is still there is a further dimension to the secondary index data structure that is um is the key field or is the indexing field on which the secondary index is based upon is it a key field or a non key field if it is a key field then we have a particular kind of index structure and if it is a non key field then the index structure changes  refer slide 07.27  let us briefly look at some illustrations of the three kinds of index structures that we are covered so far so that it helps us in understanding the more complex index structures that we are going to cover in this session the primary index structure is shown in this slide here this slide shown an index file um pointing to a different blocks in the data file the data file comprises of different blocks that is records structure organized into different blocks and the blocks are or the records are sorted within the blocks that is the the indexing attribute is not only a key attribute that is it is not only unique um it is also the the set of data records are also sorted based on this indexing field when this is the case it is enough for us or it is sufficient if we are able to store or if are able to index just the first just the first attribute or or the key value within a given block this is called the anchoring record if you remember so we just store the key value of the anchoring record in the index file and maintain a sparse index the number of entries in this index file is equal to the number of blocks the physical blocks that that make up the primary file or the data file and this index file is a sparse index because it does not store all um attributes or all values of the key and this index file can afford to use fixed length records because the the value of the record is value of the key is known and the value of the address block address is also known and we don t need anything else therefore the primary index file can afford to use fixed length records  refer slide 09.15  this slide shows an illustration of the clustering index indexing structure in a clustering index the file or the primary data file is ordered based on the clustering field however this clustering field is a non key field if it is a non key field then there is no guarantee or there is no requirement for the the field to be  noise  for the field to be unique so in this slide there are um there are some records that are shown ordered on the field called department number and there are repetitions in the department number that is there are three number of ones um for for a given department number three number of twos and two number of threes and so on however since the um since the primary file are the data file is ordered based on this um based on this field it is sufficient for us to know where does the first or where is a first occurrence of a given value and that is what we store in the index file that is the index file stores  noise  a unique values that the um ordering field takes up that is shown in the left hand side of the slide that is values like one two three and so on and the a pointer that points the first occurrence that is um to the block that contains the first occurrence of this particular value because the um primary file is sorted this is sufficient for us however there is a problem with insertion of records which needs which may need a clustering indexes to be altered and which can be rectified by assigning separate blocks for each distinct value of the ordering field  refer slide 11.03  we also know that secondary index on on key field attributes um if it is a key field note that secondary index or index structures that are that are maintained on non ordering fields that is a fields on which um fields which do not contribute to the physical ordering of data records in the primary data file because they do not contribute to the physical order of the records in the primary file this has to be a dense index because we have to for each record we need to know where this index or where this is actually stored therefore all values of the the index attribute has to be reflected in the indexed file in the slide here there are two files shown the the index file shown in the left side contains each roll number which is the indexing attribute and which is also the key attribute that is um the the roll number is unique because the roll number is unique that is because the attribute is a key attribute we don t have to worry about duplicates we don t have to worry about repetitions and because we don t repetition is not a problem we can afford to use  noise  fixed length record sizes for the indexed records that is um we know the length of the key attribute and we know the length of a record address and therefore we can afford to use fixed length records in the indexing file however the indexing file is densed and it contains as many records as there are records in the data file itself if secondary index is um is maintained on a non key attribute then we no longer have the luxury of the uniqueness constraint on the attribute that means this attribute not only does not contribute to the physical ordering of records on on um in the primary file it also is not constraint by the uniqueness constraint that is there may be repetitions there may be several different records having the same key value if this is the case a given key value k of i may correspond to multiple addresses in clustering index this was not a problem because um the because the indexing attribute was was an ordering attribute that is the um physically the um data records were ordered based on this based on this attribute therefore it was sufficient for us to know where is the first occurrence of this record we do not have such a luxury in um in secondary indexes because this indexing field is not a ordering attribure in such a case we use extra levels of indirection or extra level levels of redirection in order to reach the data record in the slide here there are three different kinds of files that are shown  refer slide 14.02  the left most file is the secondary index file which shows k of i and a of i attributes that is key values each every distinct key value and every and a block address for each key value disk block address is not the block address of the data file but in fact a block of addresses a block address containing a block of addresses many different addresses so this block contains um several addresses one each for each um record having this particular value and they are stored block wise in  14.42  and block overflows are handled by chaining so that each different key value occurs in a separate block by itself  refer slide 14.53  so let us briefly summarize the different characteristics of single um  noise  a single level indexes and see um an motivate a need for multilevel indexes um  noise  if the field or the indexing field is a key field and an ordering field we can use a primary index as shown in the slide here if the indexing field is a key field and not an ordering field we can use a secondary index secondary index on keys that is a dense index with fixed length records if the indexing field is a non key field and but it is an ordering field then we use the clustering index where we can store just the address of the first occurrence of every unique data value if the on the other hand if the indexing attribute is neither a key field nor an ordering field then we have to resort to secondary indexes of non key varieties that is we have to use an extra level of indirection there also other properties of the index structures that is primary index the sparse index where the number of records in the index file is equal to the number of blocks in the primary file  refer slide 16.06  similarly a clustering index is also a sparse index where the number of records is equal to the number of distinct values that are present for the attribute a secondary key index is a dense index which contains as many number of records as there are um records in the database itself while the secondary non key index is a is either a dense or a sparse index depending on weather there are repetitions in the um in the non key attribute and the number of records in the key attribute is simply the number of distinct values that are present in the indexing attribute in a data file  noise   refer slide 16.46  the one of the main advantages of index structures like say primary index or dense secondary indexes and so on is that index files are ordered files that is their their they are ordered on their key values because they are ordered on their key values we it is possibly to search them based on binary search a binary search is a search technique which reduces the search space by half in each iteration therefore in the average case in or in a ideal case one can reach the particular key or the address in log n number of times log n to base two where n is the number of records in the um file on the other hand a linear search requires a requires times of the order of n um  noise  or n by two rather so n different um memory accesses or n different record accesses however we can note that um there are three different um entities that we are concerned with during physical storage these are the file itself which is the which is a sequence of logical records a record which is a logical equivalent of tuple in in a relational schema and the block which is purely of physical nature that is um which is meant for efficient data transfer between the storage media and the computer and whose size is determined by physical characteristics now between block and records we we have defined a notion of the term blocking factor where blocking factor is the number of records per block or how many number of records can we store in the block now if the blocking factor of um of an index file that is of blocks storing an index file is greater than two that means if a block if a disk block can store more than two addresses or more than two index records then we can actually come out with a even better method of searching where the the method of searching is of the um reduces by the order of blocking factor rather than by the order of two which is which is constant in binary searches these are explored in multi level indexes  refer slide 19.20  a multi level index is a um is a index file which contains several different levels as the name suggest and each index block at a given level has a factor called the fan out  noise  a fan out is a is typically derived from a blocking factor that which which depends on the number of records that that one can store in a block so a fan out is the the number of different records that are or the number of different entries that a given index entry can point to and  noise  in a good implementation block accesses can be reduced from um log log n or log b to base two where b is the number of blocks in the in the index file so it can be reduced from log b to base two to log b to base fan out so this is useful if fan out is obviously greater than two if fan out is greater than two we can reduce the number of block access by um by tremendous factor  refer slide 20.31  the figure in this slide shows a two level index structure or where index structures are categorized into first level and second level the first level is called the base level and the second level is called the top level and for the sake of clarity i have also shown um how index records are divided into blocks um at at the base level it is divided into um two here the blocking factor is two but usually the blocking factor is more than two if you can notice the slide carefully the way the slide or um the way the index um index stores information is that there are different different levels in which information is stored at the top level um there are just two entries k of i entries two and ten this entry says that everything between two and the next number can be found in this index file that is every key value between two that is greater than or equal to two and less than ten can be found in this index file similarly every um key value that is greater than ten and there is nothing below therefore which is just greater than ten can be found in the index file that is pointed to by this pointer um the the other the second level index file is also a replica of the top level index file in the sense that um everything greater than or equal to the key value that is specified here and less than the key value of the next record is pointed to by the the present address that is this is an indexing scheme that is um that that indexes an ordering field and and also a key field therefore all records starting from two to less than five can be found here and all records starting at five and less than ten can be found here and so on  refer slide 22.39  the first level or the base level is a usual primary index that is maintained on a sorted file the second level is a actually a primary index on the primary index because the index file itself is a sorted file and it is sorted based on the key attribute we can store another primary index on this index file and we can continue this process to any number of levels depending on the size of our database so we could have a third level that stores an index or that stores a primary index on the second level and so on  noise  and at each level the number of entries in the next level is determined by the fan out or the blocking factor here the fan out was two therefore um what we ended up seeing was a binary tree structure however in general the fan out would is usually much more a block could contain many number of records many number of index entries much more than two therefore um at at each level from level two to level three for example um fan out number of records that is a blocking factor number of records can be indexed using just one index structure therefore the number of entries um starts reducing exponentially by a factor of fan out so this again um depicts the same thing that is um at each level the number of index entries is getting reduced by half  refer slide 24.19  in order for a multi level index to be efficient enough um to be more efficient than primary index there is an important consideration of the multi level index structure if you are familiar with with data structures you might have come across a data structure called tree which is um which is a data structure used for representing hierarchies a multi level index structure is useful only when the tree structure that is um that is um specified by the multi level index structure is balanced what is meant by a balance tree it is well possible for us to to have a multi level index structure of the kind that is shown in the left hand side of this this slide shown in this figure in the left hand side as you can see there are several levels to the um to the index structure and um  noise  and in the worst case one has to make four different one two three and four different traversal s of of different index um files in order to find a given record on the other hand the right hand side of the figure has the same number of nodes or here each node or each circle here represents an index file and the the right side of the um figure has the same number of index files however a smaller number of levels and the load or or the number of index files is more or less evenly balanced across all this across the entire tree in such a case in the average the average behavior of the um such a tree or even the worst case behavior of a balance tree is only log n log n to base fan out while the worst case behavior of an unbalanced tree is is of the order of n different block accesses and which is no different from performing a linear search over these different index files therefore an order for multi level indexes to be useful they have to they have to form a balanced tree  refer slide 26.42  so um whenever insertions and deletions happen in a data file containing a multi level index the balanced property of the index trees should be maintained and um this is especially problematic in multi level indexes because all index files are physically sorted files and we need to make a number of different um number of different adjustments at number of different levels if you have if you have just storing several different primary index structures in order to um in order to maintain the balance tree property of the index structure an approach to overcome this is what is called as dynamic multi level indexes that is an index structure changes itself dynamically by as a little a number of operations as possible so that the balanced property of the index tree is maintained  refer slide 27.41  the most commonly used index um dynamic index structures are what are called b-trees and b plus trees let us have a look at these two index structures in a little more detail here a b tree is a is an index tree which is of course as the name suggest a tree data structure where each node has a pre determined maximum fan out given by p which is of course when we are implementing it which would be related to bfr that is the blocking factor there are several terminologies we use when we are talking about b tree a given block that is allocated to a b tree is called a node in the b tree as we will see later um a block corresponds to tree node in the logical tree structure that the b tree forms a special kind of node called the root node forms the access to the b tree that is it is the top most node in the tree and each node has a maximum of p children that is the fan out number of children and of course each node has a maximum of one parent and the the um i am saying the maximum of one parent because the root node will not have any parent node so it is either zero or one parent depending on whether it is a root node or a non root node and there are what are called as leave nodes that are the lowest level nodes that is nodes which do not have any children and any node in the tree that is not a root node um that is neither a root node and nor a leaf node is called an internal node and of course we have already defined the notion of parent and children that is when when a node points to some other node then it is said to be a parent of the other node and the other node is said to be the child of the node which which is pointing to it  refer slide 29.41  this slide shows a typical structure of a b tree node this node is nothing but a block when it is implemented on disk as you can see here there are several um  noise  there are several aspects or several fields to this node in a b tree one can easily notice that there are several pointers um in fact there are precisely maximum of p pointers in the b tree each pointer which um which points to a triangle represents a sub tree that that can be pointed to by the by this node of course a sub tree can be null if this is a leaf node  noise  or if a pointer does not exist and um  noise  sub trees are filed in such a way that um there are filled leftmost that is you can you can not have a left most sub tree as null but some internal sub trees to be filled up and and left most sub tree being null and in addition to the sub tree pointers or pointers to other nodes there are several different  noise  key and data pointers that is there are several blocks here that contain a key value under pointer to the record containing this key value similarly there is another key value and and pointer to that record and and so on so so there are several key values that that are present and pointers to where the key values are present in the multiple in the primary file in addition to um maximum of p pointers to other nodes in the tree what are the properties of these what are the properties of such nodes or what are the constraints that a b tree has to adjure to  refer slide 31.30  for a node containing p minus one keys note that if a node can can point a maximum of p sub trees it can contain a maximum of p minus one keys because um in as shown in the previous slide um their keys embed or or this pointers embed the keys that is their one pointer um on the left most side of the left most key and one at the right most side of the right most key and one pointer between every two keys in the node the keys are always stored in a sorted sequence that is if there are p minus one keys then k one is less than k two is less than etcetera until k of p minus one for any data element in a sub tree that is pointed to one of the pointed to the by one of the sub tree pointers let us say in some sub tree p i for any data element in that sub tree p i that data element should be less than the data element of of the left most key that is k of i minus one and or rather it should be greater than the the left most key um the right most left key that is k of i minus one and should be less than the the leftmost right key that is k of i  refer slide 33.02  so more constraints of a b tree nodes each node can have at most p tree pointers of course and each node except the root node and the leaf nodes should have at least sealing of p by two tree pointers what is sealing of p by two um divide p by two and take the upper integer value of the this division so they they must have at least more than half of their um pointers to be filled up as part of this um as part of the tree building procedure the root node should have at least two tree pointers unless it is the only node in the tree that is um unless root node is also a leaf node it should have at least two children as part of the tree and all leaf nodes are the same level what is the the level of a node in a tree the level is simply the distance in terms of the number of hops from the root node all leaf nodes are maintained at the same level in the tree so this is the constraint that has to be maintained and suitable algorithm have to be um or suitable algorithms are created so that these constraints are maintained  refer slide 34.22  we shall be looking at insertion and deletion algorithm for b trees after we have a look at the b plus trees in fact the insertion and deletion algorithms for for both of these tree structure are the same except that in b plus tree has greater expressiveness or b plus trees allows for different kinds of accessing different varieties of accessing um the data elements in addition to b trees therefore let us have a look at some definitions of b plus trees and their constraints before we look at insertion algorithms b plus trees is a most common index structure that is found in many of the commercial rdbms it is very similar to a b tree except that the leaf and non leaf nodes have different structures in a in a b tree there is no difference between a leaf node and a non leaf node both of them have a same structure that is they have a set of address pointers and the set of key values and data pointers  noise  the the leaf nodes form a a separate kind of index containing um containing each different key value in a sorted form and pointers to the corresponding data elements um that is leaf nodes are linked together so has to provide ordered access to the data file records  refer slide35.48  a non leaf node of a b plus tree is depicted in the following figure and as you can see it is quite similar to a non leaf node in a or it is quite similar to a node of a b tree that is it contains a two kinds of entities that is sub tree pointers or block pointers and key values but um the the only difference is that there are no data values here that is there are just key values k one k two etcetera there are no data pointers as part of this node and of course the same set of constraints hold that is for any x between k one and k two the the value of all the keys in that sub tree x should be should be greater than k one that is the right most left key and less than k two which is the left most right key  refer slide 36.41  a leaf node in a b plus tree is shown in the following figure where um there is a there are no um sub tree pointers because there are no children for the leaf nodes and there are only a set of keys and data pointers that is there is key one and data pointer to the record containing key one there is key two and that the data pointer containing to record pointing to record containing key two and so on and at the end of this block there is a um pointer pointing to next logical leaf node or or the left logical block in this sequence therefore starting from the left most leaf node we can access the entire database in a sorted form just by following the leaf nodes and and the links to the left next leaf node  refer slide 37.37  what are the properties of leaf nodes keys in a leaf node have to be ordered just like the property that we saw in b trees and where where keys have to be ordered within a leaf node we should be able to access um each leaf node in in key sequence that is k one is less than k two less than k n if there if there are n different keys in a leaf node and just like the nodes in a b tree each leaf node should have at least half of its keys filled up that is a sealing of p by two number of keys should be filled up and all leaf nodes should be at the same level as far as the overall b plus trees is concerned  refer slide 38.23  let us first look at how we perform search in b trees and b plus trees um we are going to be looking at searches and additions and deletions in b plus trees and um the corresponding algorithm for b tree can be derived with um in a analogous fashion in fact that um algorithm for b trees are little simpler than that of b plus trees the searching algorithm over b plus trees is a generalization of binary search here it is a peary search in the sense that um where where p is a fan out of each block so just like binary search we we go about um with a given key and starting from the root node of the tree that is given a search key k start form the root node if the key is present in the root node in the current node itself then um then we are successful that is the key corresponds to um  noise  sorry the the from the key we we are able to um end up find the corresponding leaf node from where we can find a corresponding data pointer however if the if the current node is a leaf node in the b plus tree and key is not present then we can be sure that the key is not available in the in the database itself then then we return not available or else what we do is we search for the the different pointers such that the key value that we are looking for is embedded between the left and the right most keys that is um if we are searching for um if we are searching for the first left um first sub tree that is p one then our key pointer should be lying between k one and k two if we are searching um if we want to search um any p i then our well um then our um then we have to search then our value key value should lie between k i and k i plus one it is it is a matter of terminology the slide shows k i minus one and k i it is a matter of terminology whether how we use i minus one and i that is we can either say k i minus one and k i or k i and k i plus one and we continue this this search in the left sub tree in in a recursive fashion by by going back to step two and searching in that node and searching in a sub tree and and so on  refer slide 41.06  what about insertions this is the main contribution of b plus trees in the in the sense that we we will be able to insert records while maintaining the balanced property of the trees now we should be illustrating the process of insertion with an example and we shall not be going into the exact algorithm of of insertion which can be referred to in in um in any standard text books however the the illustration serves to help us clear the or help us to clarify the notion of how insertion happens within a b plus tree um the logic behind insertion in a b plus tree to begin with b plus tree starts with a single node which is the root node and which is also leaf node it has no parents and it has no children and um the the first key that that is inserted into the database goes into the um goes into the root node and because it is a leaf node it just points to the the corresponding data pointer as in when nodes will up that is as in well more and more record are inserted and more and more keys have to be incorporated into the tree nodes get filled up as isn t when they their they are filled up nodes are split um and and this split nodes are made into children of a newly created node and the key values are split or also split correspondingly across these these two nodes and the and and the new parent node is updated accordingly and this split operation is cascaded to levels above so that we um end up with just one tree starting from one root node following um following until the leaf nodes and in the balanced fashion  refer slide 43.11  let us take an example um to illustrate our point let p equal to two that is the fan out factor for just for the sake of simplicity we shall be assuming that the fan out factor is two the um that is each node has only two has at most two children each internal node or root node has at most two children and let us consider a sequence in which records are different keys are inserted and a possible sequence is shown in the slide here that is um keys are inserted in sequence five eight three seven two nine and so on so um they appear in some arbitrary sequence that the sequence need not be ordered and we we can not we can not ascribe any particular property to which in which keys are inserted initially um one when we insert five we just have one root node and one key node and data pointer and nothing else in the tree when we insert eight it is still just one root node that is shown in the figure here and with two key pointers five and eight and two data pointers two corresponding data pointers however when then next key is inserted that is key value three is inserted this node overflows that because p equal to two we can not accommodate any more keys in this node therefore we require a spilt in this key node how do we split this node  refer slide 44.47  note how nodes have been split in this u in this slide here initially we started with a root node which was also leaf node when you spit this node we get one more leaf node that is which is shown in right most side of this slide here and another intermediate node that is a node that points to two leaf nodes in a b plus tree node that the intermediate node has a different um structure than the leaf nodes this has to be incorporated so um five and eight were present in the in the tree and now um key value three has to be inserted because three is less than five and note that the keys always within any leaf nodes the keys always have to be in sorted form therefore the the key three has to be has to be inserted to the left of five to the left of this um pointer called five therefore we get a um we get two different leaf nodes one containing three and other containing five and eight there is there is a small bug in there is a small error in this slide that is the left most node contains just the um just the key three and the right most node contains the um pointers five and eight and the non leaf node or um or  noise  or the intermediate node is suitably updated so that three appears here that is everything less than or equal to three appears in this node and everything greater than three appears in this pointer there is nothing else to be placed here because we don t have any other keys um to to begin therefore assume that we have got keys in the sequence three five and eight we would end up with a tree as shown in this figure here now suppose seven has to be inserted now seven can be inserted into this leaf node without any problem that is um seven gets inserted here and and the nodes and the key values are reordered earlier we had just eight in this node and one seven was inserted the key values were reordered so that the keys are always sorted and there is no overflow however the next um key that is key value two causes another overflow that is key value two has to be inserted at the left most left most side here this causes causes an overflow and this overflow has to be cascaded up or has to go up the level in the insertion or in the b tree  refer slide 47.32  therefore um we get a b tree of the of the following form here that is um both key values in the intermediate node now get filled and the intermediate node now points to three different leaf nodes the insertion of nine that is the next key value again forms a overflow because nine has to be inserted beyond the last block here um it try the algorithm tries to insert nine into the last block which fails and then a new node is created  refer slide 48.10  now because a new node is created and um we encounter another overflow this overflow happens in the the um at the level above that is because a new node is created we need four different pointers however a node can accommodate only three different pointers here therefore we need to we need to spit even the the node above and and introduce a new level into the tree therefore the corresponding tree that that gets formed is shown in this figure here that is the next level node is also split so that a third level is created and then um the keys are more or less uniformly distributed across the entire tree as you can notice here the property or the balance property of the tree is maintained as in when the insertion is happening and all leaf nodes are at the same level that is the height from the from the root of the tree is is maintained at the same level for every leaf node in the index structure  refer slide 49.16  so that was a brief illustration of of how keys are inserted into a b plus tree and um we shall not be going into the exact algorithm in order to in order to insert keys in a b plus tree deletion of keys have to contend with an analogous problem that is the problem of underflow in insertion we had the problem of overflow and in deletion we have the problem of underflow an underflow happens when a node contains less than p by two floor of p by two keys less than or equal to floor of p by two keys that is note that there was a constraint that alteast half of the keys in a node has to be filled up that is more than half rather so if it contains less than half then um of keys the um in in a node then there is a underflow whenever there is a underflow a node is merged with its sibling in order to bring down number of um levels in a tree we shall not be going into deletion algorithms also in in detail here let us um move on to the last aspect of indexing structures namely um how do we deal with index structures on multiple attributes until now we have um we have assumed that indexing attributes are are the fields on which are indexes are maintained or simple indexes that is simple attributes however um the indexes could some times be maintained on composite attributes that is a um a set of attributes forming a key attribute that is um for example department id and employee number combine to combine um to form a key attribute is is a composite attribute how do we maintain indexes on a composite attribute there are several different strategies that are variations or extensions of um existing indexing structures and we shall briefly summarize some of the main techniques used for indexing multiple attributes  refer slide 51.23  one simple way is to index multiple attributes that is maintain a sorted file a primary index of multiple attributes is to have an ordered index on multiple attributes that is instead of sorting the file on just one attribute we sort the file on two attributes that is sorted based on first attribute and among them sort based on second attribute um which is which is the simplest solution possible but which is which can give us only a primary index as as we have seen earlier that is it has to be a um ordering attribute and a key attribute the second um  noise  the second strategy for dealing with composite attributes is to use what is called as partition hashing  refer slide 52.13  partition hashing is a hashing technique which takes a composite attribute that is n different key elements um pertaining to this attribute and um and returns n different bucket numbers we then um we then transform this into a single bucket address by concatenating all these bucket numbers to form the bucket address so that is another technique for dealing with composite attribute the third technique that  noise  is used especially in um in applications like data warehouses is to use the notion of grid files  refer slide 52.50  a grid file is a file that that that establishes that establishes grid structure what is a grid structure we can use a grid structure when we know the entire range in which a key value can be um can um can be spread upon for example if we know that the roll number of a student ranges form zero one to one fifty we know the entire range of the set of all possible roll number and if you are fairly sure that the distribution of key values in this range is fairly uniform then we can split this range into several different buckets now such splitting of of key ranges into buckets and combining this different buckets forms a grid structure and these grid structures form a forms a matrix or or a um or a hypercube which can be stored within a single file using um several techniques called row major techniques column major techniques and and other techniques called space filling curves and so on um using which they can be stored within a single file this slide shows such a such an illustration  refer slide 54.03  that is there are two different key values roll number and grade roll numbers are ranging from zero zero one to one twenty five and they are divided into five different ranges similar five different buckets similarly grades are divided into five different ranges a b c and d and the combinations of this forms a grid and each cell in the grid corresponds to a set of bucket address or or a set of block addresses which contain records um that satisfy both this constraints of keys so so each pool or each cell corresponds to bucket pool  refer slide 54.42  this brings us to the end of this session let us quickly summarize the main topics that we have covered in this session we covered several kinds of multi level indexes and multi level index has index structures has several different levels under usually organized in the form of a tree a tree contains a root node which which is the entry to the multi level index structure um several leaf nodes and many internal internal nodes that form the tree structure and for a tree structure to be efficient it has to be balanced and um balancing or self balancing tree structures are what are implemented in dynamic multi level indexes in which we saw b trees and b plus trees and we also insertion how insertion and deletions are handled in b plus trees lastly we looked at some strategies by um using which we can um we can maintain index structures on multiple attributes that brings us to the end of this session lecture # 13 constraints & triggers hello and welcome to another session in database managements systems we have covered a quite a bit of ground in the explorations of dbms already we have looked into logical models of data data management from the conceptual perspective from the physical perspective and so on we also looked at physical requirements of data storage although we have only looked at very specific kinds of physical data storage and and retrieval strategies for databases we also looked at few kinds of index structures over which are using which we can efficiently access or efficiently search and retrieve for the relevant tuple or or the relevant record that we are looking for in a database management system um usually when we are taking about databases there is some times a physiological question that is raised as to whether any collection of data is a database especially the the um perhaps in the late ninety s there was this ranching debate about whether the web the world wide web is is just a large database now um of course one might um one might be tempted to believe that any collection of data is a database or um and and some some more argue that no that is that is not really true collection of data is just a collection of data it is something like the the difference between you know um a set a bag and a relation and so on all of them are just collections of something s um a set of tuples or a bag of tuples or relation of tuples all of them are just a collection of tuples however um some of them for example sets or bags impose what are called as additional constraints over this collection a bag of tuple is simply a collection of tuples and there is there is no constraint what so ever on what should be the tuple um what kinds of tuple should be there what should be the size of each tuples what should go into um or what should define the attributes that make up the tuples and what about duplicates in the tuples how many different kinds of the same how many different numbers of the same kinds of tuples can be there and so on there absolutely no kinds of constraints that are imposed on a bag of tuples however when we consider a set of tuples there is an implicit constraint that is um imposed that a set is a collection of a things of the same kind although um not not always so but usually we talk about a set as a collection of things of the same kind therefore when we say a set of tuples we tend to believe that it is a collection of things or collection of tuples of the same kind that is all of the tuples have the same number of attributes and um each attribute or each corresponding attribute of each tuple has  the same domain and so on and there is also an implicit constraint that there are no repetitions in sets that is two or more tuples can not have the same value for every corresponding attribute that they contain therefore there is a implicit constraint of no um no duplicates that is imposed on a set of tuples and um and of course in order to implement this that is in order to implement a bag of tupels um and to implement a set of tuples the the kinds of programming that we have to do um the kinds of logical checks that we have to do changes implementing a bag of tuples is the easiest we just um we just maintain one collection of tuples using whatever data structure that we can use for maintaining collections link list trees whatever arrays and so on similarly in order to implement a set of tuples we can use the same kind of data structure in order to um implement a collection however we have to keep making checks so that the set property of um this this tuples are maintained that is we have to um maintain checks that all of the tuples are of the same um kind and there are no duplicates that are there in the tuples um moving on if we consider a relation of tuples relation of tuples is also a collection of tuples just like a bag or a set however it it imposes even more constraints on the set of um the the set of tuples that is we have seen some kinds of relational constrains like like the entity constraint the key constraints um and referential constraints no duplicates and and so on so a relational a relation does not allow for duplicates in tuples it also it explicitly states that all tuples have to be of the same kind um that is they have adhere to the same schema that is defined by the relation and it explicitly forbids any tuple that does not adhere to the schema and there are strong constraints of about how the values of each attribute should be that is what should be the domains and how um each what what is the domain of each um attribute and where can a value lie and so on and there are other constraints like key constraints which which can uniquely define a tuple and because there are no um there are no repetitions in in a relation by default the the set of all attributes of a tuple form its super key and so on and so fourth in this session we are going to look at these constraints in much more detail we are going to be concentrating on the concept of constraints and an associated concept of triggers in database systems triggers are a concept that are quite prevalent in what are called as active database systems  refer slide 07.14  and we are going to be looking at um triggers in um in this context in this session so so let us look at what are the different kinds of constraints that that are imposed or either implicitly or explicitly in a typical database management system let us define the term integrity constraints um as the term um  noise  as the term implies integrity constraints are constraints that strive to enforce the integrity of the database system what is um what is an integrity of a database system a the integrity of a database system or or a system of data elements essentially states that um the the set of all data elements that are stored in this system is a valid set  refer slide 08.01  note that a valid set or validity is different from correctness validity simply states that um or validity is is some kind of correctness that is um that is independent of what the user or what the application really considers semantically correct from um from the usage perspective for example if i state that the marks obtained by a student can range from zero to hundred then a value of two hundred for the the attribute called mark is an invalid number it is not at all it is not valid so um so this pertains to validity however if i um enter a value called let us say fifty for a particular student it may be valid however it need not be correct it could be an incorrect um entry for that um for that field the student may have actually obtained ninety marks where where as i would have entered fifty for for the student therefore it is an incorrect um value however it is a valid value um integrity constraints as part of the dbms are independent of the application programs they have no idea about what is the application context or what is the usage scenario in which this particular data element is being used therefore they can only talk about or they can only enforce validity of the database um values or of the data elements stored in the database they can not obviously ensure correctness of the data that is stored in the database  refer slide 09.47  one of the first form of key constraints in the um in the relational model um one of one of the first form of constraints in the relational model is the key constraints we have seen key constraints when we talked about the the relational model and also in sql but let us revisit key constraints here again for the sake of um completeness when we are talking about constraints a key constraints is is a very fundamental form of relational constraint and it manages um entity existence and of referential integrity what is meant by entity existence entity existence essentially means that we should be able to deference that is we should be able to identify each entity or each tuple in our database uniquely so um that that essentially means that um each entity should have at least one super key using which it can be uniquely identified it can be uniquely distinguished from the rest of tuples in the database in the relational model um or in the relational algebra since a relation forbids um duplicate tuples the entire tuple in in the worst case itself is the um is the super key of the tuple  refer slide 11.03  so the key constraints therefore states that key attributes should be unique across the relation that is there there should be um no two tuples having the same um key attribute that is um if if i identify a subset of attributes as a key attribute there should be no two tuples um such that the key attributes are the same that is such that they are indistinguishable as far as the key attribute is concerned the entity constraint stipulates that the key attribute can never be null because all null attributes are the same no matter where they occur and there are there is no difference between a null attribute in the name field verses a null attribute in the age field all null attributes are the same that means that are it means there is no value associated with it um the the entity constraints therefore states that the primary key relation that is the the relation the the subset of attributes that can uniquely identify tuples in a relation may never be null and the third kind of key constraint is a referential integrity constraint the referential integrity constraint essentially is a constraint over the foreign keys um remember what a foreign is a foreign key is a a set of attributes or subset of attributes within a tuple that refer to another tuple in a different relation um mainly the primary key or or any key attribute of another tuple in a different relation a foreign key or or the referential key integrity constraint states that a foreign key should either be null that is it should not refer to any other tuple or if it refers that is if it is not null then it should refer to an existing tuple we can not refer to a tuple that does not exist um we can not assign a manger to a department for example um which which is non existent we can not assign a manager to a non existent department on the other hand we can say that a manager is not assigned to any department at all that is set foreign key as null which is quite um which is quite acceptable as far as a referential integrity constraint is concerned  refer slide 13.20  how do we um how do we specify key constraints using sql we have seen this in the um in the session on sql let us briefly summarize it here again note the creation of a table um in sql the following slide shows a table called employee which has two key constraints one is the employee number which is the primary key which is marked as a primary key and there is a pan number for each employee which is a secondary key that is it is um it is it is given some constraints called non null and unique and so on and there is also a foreign key the that is the the field called reportsto which um which states who is the supervisor or who is the manager to which this employee reports to so so the foreign key um references another um employee tuple um and references the field called or references the attribute called employee number in in another employee tuple or in another employee um of an other tuple in employee relation so this is um these are the the the circles that are shown in this slide depict how um the key constraints are identified the first circle shows that when we um when we stipulate that pan number is not null then we have identified that has the key constraint that is it is it is one of the keys um by which can uniquely identify tuples in this relation the second circle shows that the employee number is primary key is a primary key which implicitly states that employee number may not be null so um therefore the the not null constraint in the um for for the employee number is actually spurious but um never the less it is it is states that employee number is is another key which is used as the primary key that is which is used to um which is actually used to deference every tuple in this relation finally we have we identify foreign keys using the foreign key constructs that is we identify which first of all which field in this tuple is the foreign key and it refers to which field in which other relation that is it references the relation called employee and the field called or the attribute called employee number  refer slide 15.53  how can um key constraints be enforced now that we have seen some physical aspects of data storage or data management let us go inside the dbms to see how um or what could be a possible means by which um constrains or key constraints can be enforced a simple way of enforcing constraints is to create um an index structure over the field that form the keys for example if we create um any kind of index structure let us say primary index on the um on the declared primary key attribute then we can easily enforce the unique constraint note that um primary key has to be unique all keys have to be unique therefore whenever a new tuple is is been inserted into the database system we just use the index structure to verify whether um key attribute of this value already exist in the database system or not if it already exists then um then inserting a tuple with the same value is going to violate the uniqueness constraint um so in which case this can be rejected and the integrity constraint can be enforced similarly um  noise  for all other attributes that is all secondary key attributes we um we can maintain secondary index indexes such that um we can always verify whether before insertion of a tuple whether the corresponding value for that key actually exist in the database or not referential integrity similarly can be verified by using a primary index over the other table that is the over the table that is being referenced therefore whenever i insert a tuple in a relation that contains a foreign key i first verify um whether this this tuple in fact really exists in other table weather the foreign key um weather the foreign key references to a tuple that actually exists in the reference table only if this is so then um then i have to allow the insertion of the tuple in the referencing table  refer slide 18.03  so let us um probe a little further into enforcing referential integrity in sql  noise  sql automatically rejects whenever a tuple is being inserted into a relation containing a foreign key such that the foreign key is not null and it refers to a non existent tuple and um this rejection is performed using corresponding index structures and um um and by default sql uses the restrict option for managing um alterations or or updates in tables um for example if um if a tuple is deleted or if a tuple is um asked to be deleted in the um  noise  in the referenced table that is um let us take an example of a manager um working in a department if the department tuple of the corresponding department table is is to be deleted um then sql rejects such a deletion because there is a foreign key constraints that is from the table called manager which is referencing this tuple unless of course we use the cascade option in which case even the corresponding referencing tuple um would would be deleted that is the corresponding manager tuple which which is referencing the department tuple would also be deleted when the um department is deleted  noise  similarly sql rejects um any updates to the um to to the part to tuples that can affect the foreign key constraints for example if i try to um update the um the department id in the manager table such that it now points to a non existent department sql rejects this update on the other hand if i try to update the department um table and change the department number such that it violates some some foreign key constraints that are pointing to it that is um it makes some um some foreign keys dangling um then sql would reject such an update  refer slide 20.21  of course updates and delete deletes can um we we can force the um dbms to go ahead with the updates and deletes by using the cascade option in which case the the overall referential integrity is still maintained however um updates and deletes are cascaded that is every referencing tuple is also updates whenever the reference tuple is being updated  refer slide 20.48  there are also other constructs in sql that um that um  noise  that instructs the dbms to perform in the different fashion to to act in a different fashion than mere cascade a cascade simply says that um whatever changes is being made in the reference tuple make the corresponding changes in the referencing tuple that is if i change my department id from fifty to one not two change all referencing tuples that that are referencing to department number fifty to department number one not two um that is simple cascade however we may want to do some other operations other than simple cascade the other options that that we can use um to to instruct the dbms to to perform differently one of such option is a set null option have a look at the relation shown in the slide here the slide um creates or the relation creates a table called employee where employee number is the primary key and there is a field called name and there is a foreign key called reportsto which which is the employee number of the manager or the other employee to which this particular employee reports to therefore foreign key references employee relation and the employee number field and then in the foreign key relation um or in the foreign key specification there are two other constructs the first construct says that on delete set null and then the second relation says or the second construct says that on update cascade what do these things mean the first thing um the first construct um says that on delete set null essentially it means that if the if the tuple that i am referencing to is deleted then set null that is set this set this field to be null note that a foreign key can be null without violating referential integrity therefore um this semantics of this um statement means that if the person whom i am reporting to is for some reason deleted from the set of employees then set the field as null that is i am not reporting to anyone um on the other hand if the person to whom i am reporting to changes his employee number let us say i am reporting to a um manager with with employee number one not two and um employee number is changed to one fifty then then it says on update cascade that is um cascade this new employee number to and make corresponding modifications in the foreign key foreign key constructs so that referential integrity is maintained  refer slide 23.43  sql also um supports what are called as deferred constraint checks a deferred constraint check essentially state says or um tell the dbms that when inserting a tuple that don t make a constraint check right now why do we require a deferred constraint checks deferred constraints checks are um especially useful when we have circular constraints um or circular referential integrity have a look at the figure shown in the slide here the figure show two tables with corresponding referential integrity um the table essentially states that the first table is a manager table which contains a primary key called employee number and the name of the manager and the department id that is managed by the manager the department field is a foreign key into the department relation the department relation in turn is um having a primary key called department id it has a department name and a field called manager which in turn is a foreign key onto the employee number um consider that um the this company which which is having a schema like this has has recently upgraded its database system and now they have installed a new dbms and they are now porting the set of all data that they have from their old database system to the new database system now when they are porting um which essentially means that they are adding these tuples in in a batch mode um adding a set of tuples in a batch mode there is no guarantee that or it is very difficult to guarantee that um the  noise  tuples would be added in the order in which they are required that is whenever i am adding a department id the the employee the corresponding employee id for the manager already exist and whenever i am adding a manager tuple the corresponding department id for which um he is a manager already exist this is um very difficult if not impossible to sustain or or um maintain so in such situations it is useful to have a deferred integrity check so if we if we use the deferred or deferrable option in sql um then then  noise  the dbms defers integrity checks till the present transaction is um completed we have not looked into the concept of transaction has yet um however let me um give a brief intrusion about the notion of transaction a transaction simply is a logical unit of database um operations um for example if i um if i talk about um debiting and crediting between two accounts let us say let us say i am i am performing um wired transfer between my account and my friend s account so um a transaction in this case is the set of operations that debits um a set of um an amount of money from my account and credits it to my friend s account this entire set of um entire set of operations belongs to one semantic entity or semantic um transaction so to say so and either both have them have to be performed or none of them um should be performed that is um either both debit and credit should happen or none of them should happen it shouldn t be the case that my um account is debited but my friend s account is not credited or the case that my friend s account is credit credited with some money from somewhere but my account is not debited either both of them should be um should happen or none of them should happen um um the deferrable option essentially um defers integrity constraints that is until the end of the transaction in the middle of the transaction we we might encounter a situation where the integrity is violated however um that is okay as far as um we have at at the end of the transaction we have perfect integrity that is we have maintained the integrity of the overall database system this slide shows how um or what is the syntax of the deferrable constructs  noise  have a look at the slide here this slide creates a table called manager um which is the which is shown in the previous slide or which was shown in the previous slide and manager has an employee number attribute which is a primary key and name attribute and a department id which is the foreign key and um  refer slide 28.32  of course there is no need to separately specify foreign key if we directly say that department id references some other um tuple that is department department id and we say that it is deferrable and also that initially deferred that that is it is initially deferred that means to say that um deferrable essentially means that it is possible or it is okay if um integrity check on this um on this referential integrity is deferred till the end of the transaction and the this second one says that initially deferred means explicitly tells the dbms engine that as soon as a tuple is tuple of type manager is um is inserted initially defer the um the referential integrity checks let us to move onto other kinds of constraints um from from key constraints the next kind of constraints that we are going to look at are constraints on attributes of course we have already seen a few constraints on attributes as um some examples are shown in the slide here  refer slide 29.42  the first constraint that we have seen was the not null constraint which essentially states that null value for for this particular um for this particular attribute is not a valid value and the second kind of constraint which is also key constraint but um some sense also a attribute constraint um is the unique constraint it essentially checks against duplicate values of um of the attribute being inserted into the relation  refer slide 30.13  a more general form of attribute constraints um in sql is what is called as the check option we can specify any general kind of attribute constraints using the check option on on a relation there is an example or there are two examples shown in this slide here the first example defines a field or or an attribute called gender um and says that it is it comprises of a single character char one and then imposes an integrity check on the value of this field or the value of this attribute it says that check gender in m or f that is those are the only two valid characters that can that can be assigned for gender even though it is a single character it is not just a single character but um it is a single character in this set of two characters m and f similarly the second um example shows a field called age that contains an integer of two digits um which which can contain an integer having two digits and there is a check that states that check that age is greater than eighteen especially if you are talking about employee records and so on there might be a legal um integrity constraint that is a legal um minimum age for an employee of eighteen years therefore that can be directly included as part of um as part of the check constraint that is check age greater than eighteen note that the first constraint um that is um check gender in m or f is a physical constraint that is um it is a constraint that is given as part of the physical world around us and the second kind of check that is used that is check age greater than eighteen is the normative constraint it is a constraint that is imposed by the local laws or or the set of norms which define the the the the the set of um correct behavior within this system and it can change from system to system in some places probably the minimum working age is um let us say sixteen or in some other places it could be twenty one and so on so that is a normative constraint while the the the formal constraint is a physical constraint  refer slide 32.38  when you ask a kind sort of interesting question can the attribute constraint that is can the check um  noise  constraint on on values of attributes be used to um enforce referential integrity referential integrity as you know is a key constraint that is it is a integrity constraint across different relations however note um have a look at the small declaration here which might which might seen to say that we can actually use a check as um check for enforcing um referential integrity the example shows that shows the declaration of a field called department id and department id shown as um integer having six digits and then there is a check here the check says that check department id in select department id from department now this is the department id of the manager um tuple and um when a manager tuple is being inserted there is a check that is made to see that the department id um that is being inserted for the manager actually exists in the department um relation which looks like or which seems to suggest that we can enforce a relation referential integrity using um  noise  referential integrity using um check statement that is um if it try to insert a tuple or if we try to um insert or or even modify a tuple of the manager relation such that it tries to reference to a non existent department id this check fails and the updates are insertion is rejected which seems to suggest that referential integrity can be um enforced using check however look at the other way around what happens if um a department um existed when the manager tuple was inserted and then at a later point in time the corresponding department tuple was deleted there is no way that this check constraint is is now enforced because the check doesn t even know that the corresponding tuple from that is being referenced in the department table is being deleted therefore because of this um because such um  noise  situations can not be handled by check we say that we can not use the the check condition for enforcing referential integrity the check constraint can not only be used um on just an attribute basis it can be used on a tuple wide basis that means  refer slide 35.28  a check constraint can actually um perform checks on several attributes on on a given tuple um the following example shows such a situation where it says age int of two that is the the age of an employee let us say um is a is declared as an integer having two digits and a check is performed to see whether age is greater than eighteen that is um the legal age for a particular employee in in a given um in a given company setting should be greater than eighteen or um there there could be children employed by the company as long as the the job type is based on talent that is job type is based on um  noise  um is based on encouraging the child s talent therefore um you can either perform a check on the age field which says age should be greater than eighteen or um if age is not greater than eighteen you can check a separate field or a separate attribute called job type to see whether it is um it is set as talent in this particular tuple  refer slide 36.39  it is possible to um give names to a constraints to declare in sql so that they can be referenced by their names rather than by their actual conditions um this particular slide shows such an example where the check called the the legal um age of an employee or the minimum age of an employee that constraint on this is given a name called legal employee that is age int of two um and and name is given called legal employee which is the constraint called check um age greater than eighteen or job type equal to talent  refer slide 37.20  once we have named tuples we can um alter constraints that is once we have named um constraints we can alter constraints um that is we can add or delete constraints by referring to them by their names for example the following slide shows um two different um alter commands the first command says that alter table employee drop constraint legal employee if the particular constraint is no longer um valid we can refer to a constraint by name and then say drop it so so that constraint is no longer enforced in future additions of um or future updates to the table similarly the um this second alter statement shows that shows how a new name constraint can be added to the to the table at any later point in time that is alter table um manager and add constraints allworks um that is the um constraint name is called allworks and check the department id is not null that is um um check to see that um for all the employee manager tuples in this manager relation the department id that is the corresponding department which they have to manage is not null that is there is no such manager who is not managing a department now what happens if there are already some fields or um already some tuples in the relation um whose um whose department id s are null in such a case the the addition of this constraint fails that is the the constraint can not be added because it can not be enforced on this table because there are already some managers who is department id s are null  refer slide 39.11   noise  a more powerful kind of constraint um on um in sql is um are what are called as schema level constraints until now we have been looking at constraints that acted at the tuple level um or or at the key level tuple or attribute or keys or so on that is which which acted on specific instances of a relation a schema level constraint on the other hand acts at the level of a schema that is um which which is enforced for all tuples are are um on a database wide basis one such powerful constraint a powerful general purpose constraint is what is called as assertions assertions are general purpose checks that can be performed on the entire database um and can be enforced on the entire database um for for the entire time um in which the database is in operation that is if i um if i make a specific assertion the assertion must be true when it is made otherwise the assertion fails of course and the the assertion must be true when it when it is made and it must be true through out the um through out the life time of the database or until the assertion is dropped that is um once an assertion is made it will be enforced that is every um modifications to the database in whatever table in whatever tuple um would be um checked to see that the particular or this specific assertion is not been violated  refer slide 40.50  this slide shows um an example of an assertion um an assertion can be created using the create assertion command um this slide shows create assertion command that creates an assertion called nobench um nobench is a name of an assertion and um the the assertion us essentially checks that is an assertion is a check statement which checks to see that they does not exist any manager whose department id is null that is there is there is there is no manager who is on the bench so to say that means who is not assigned any department to manage  noise  and um this assertion should be true um when this is first asserted that is um when i make an when i execute the create assertion statement um all managers in the um in all managers in the database um that are listed in the database should be associated with um a department and if there are any managers who are not associated with a department then the create assertion fails the assertion can be created only when either the the managers who are not assigned any departments are modified um to to assign departments are um or those tuples are deleted from the database and once an assertion is created that is once such an assertion is created it is enforced or it is maintained whenever there is a database update that is happened and any update or any um addition of a manager tuple whose department id is null is rejected and this set of assertions holds for the life time of the database or until the assertion is dropped or the um until the assertion is deleted um assertions can be dropped using the drop assertion command so we just say drop assertion nobench and then the the assertion no longer holds and um further on tuples um violating this assertion can can be inserted into the database the second kind of schema level constraints that that we are going to look at are what are called as eca rules or trigles triggers an eca rule expands to a even condition action rule this is a integral part of an specific kinds of databases is called active databases however eca rules have been incorporated into most commercial databases um as of now that is um let us say db ibm db two oracle nine i or ten g which is the latest all of them incorporate some form of eca rules an eca rule or a trigger um is again a database wide constraint but which differs from an assertion in the sense that there are not always active that is they are not always enforced and eca rule is enforced or an eca rule is awakened only when certain specified events occur  refer slide 44.08  that is note that eca stands for event condition and action so when an event occurs a corresponding eca rule is awakened the rule then performs a um or the rule then performs a condition check to check whether the condition hold that is when the event condition when the event occurs if the condition holds then the rule performs a given set of actions that is the the the the a part in the eca rule and the the action could be anything it could be like preventing the event from um from proceeding or undoing the event or any other set of database update operations that that could be totally unrelated to the event it could actually um be something like um intimating the user about about the event and so on and um the the action in turn may may generate more events which in turn could trigger more eca rules and so on there are several options that sql provides um to handle eca rules we can either specify that the action part of a eca rule um be executed either before the event occurs or after the event occurs that is um before a particular event is going to happen make a check for any eca rule that is waiting on this event or one could check for um performing the action after the um after the event occurs an action the action part of an eca rule can refer to both old and new values of data elements that are modified by the event  refer slide 45.34  that is um sql or um several  noise  or um most commercial dbms systems would maintain the older value of um older values of data elements that were modified by certain update events um in case they trigger certain eca rules and the eca rule would would want to refer to the older um value or of the the data element for example on might um specify an eca rule that says alert the user whenever the the current um stock price let us say falls below or falls by greater than five percent therefore we need to know the older and newer value of of this particular stock price in order to see whether the change in this in one um in one update is greater than five percent if that is so then the user is alerted  noise  and the action part of an eca rule can be either specified to be performed once for each modified tuple or once um for all tuples that are modified um during the update event that is the um during the database operation  reger slide 46.49  this slide shows a particular example of an eca rule um it says um all the keywords are shown here in in a highlighted form um the keyword to specify an eca rule is the term called create trigger so um so this statement creates a trigger called overdraft um which has to be performed after update that is the action part of this trigger has to be performed after the update note that um actions could be performed either before or after the updates so this has to be performed after update on um a relation called pre-paid and referencing each new row that is the the new row that were modified has nrow and for each row that was modified um it checks whenever that is when balance that is nrow dot balance attribute is less than or equal to zero then update um update that row and set block or set the attribute block equal to true that is for every pre-paid account um whose balance is less than or equal to zero the account is automatically marked as blocked as part of this trigger operation  refer slide 48.11  what are some of the properties of eca rules eca rules are automatically triggered by the dbms um the application program need not even be aware of the corresponding eca rules that were triggered it is the rule writers responsibility that is the the who ever enforces these rules or whoever wrote these rules to enforce these integrity constraint it is the rule writers responsibility to ensure that the rules terminate and um it is also the rule writers responsibility to prevent cascading triggers that is um this action would would in turn um create another cascading action which which in turn would trigger some more rules which in turn would create um create another cascading um action um triggering more rules and so on so it is a rule writers responsibility to prevent such um conditions from happening a rule write can also write a cyclic rule that is um rule a triggers rule b and the action of rule b triggers rule a that is um rule a tries to undo something um undo an update and which in turn triggers rule b which tries to redo it back and it it  noise  can obviously go of into infinite rules in most dbms systems termination and infinite cycles are are simply handled by um a seeing or or just maintaining a count of of the nesting level that is how many iterations have been made and if this crosses a maximum threshold then the entire set of operations including the update that triggered this infinite rules are rolled back that is they are all undone and nothing is changed and the database state is taken back to the state before the event that that created this infine looping of rules  refer slide 50.09  the last um um  noise  kind of integrity checks that we are going to be looking at in in an sql based database system are the notion of authorization and privileges authorization and privileges talk about which user is authorized to do what or which user enjoys what kind of privileges users in an sql based system can enjoy certain privileges that grants them certain authorization what what do we mean by authorizations um an this could mean an authorization to read a particular tuple for example is a a particular or is a given user authorized to um let us say look at the account of account details of some other user or um is a a given employee is authorized to look at salary details of some other employee and so on okay so um  noise  there could be a read authorization there could be inset authorization that is um is a user allowed to insert tuples into the database there is an update authorization delete authorization index authorization that is index essentially means that can a user um be allowed to create and delete indexes on a particular table why is this security constraint why is um why is index important that is why is index important that is why should i use authorize to create and delete indexes um this is because what happens if i use a um weather um intentionally or unintentionally deletes the um primary key or or the primary um index structure on a tuple then the entire database or or the entire data file becomes extremely inefficient to access therefore um i um a user used should be authorized to to create or delete indexes then alternation and drop authorizations  refer slide 52.01  what about views what kinds of authorizations can we impose on views most database systems allow authorizations to be um specified on views without obtaining the specific authorizations on the base table that is um let us say a hr manager would would have a read authorization on a hr based view of the employee records that is which which contains employee salary details perks and so on however there may not be um or the manager may not enjoy read authorization on the actual employee tuple which may contain some more information like his like his or her personal details and so on which the manager may not be authorized to access therefore even if the corresponding authorization does not exist on the base table it is possible to obtain certain authorizations on the view and since views are derived table um whenever a query is given through a view authorization checks should be performed before the the query is answered  refer side 53.09  the general form of providing authorizations is to is to use a grant command the the grant command simply says that grant privilege list on a particular relation or view name to a set of users  refer slide 53.26  this slids shows such a example which says grant insert update and select privileges on the manager table to public public means that all users that is everybody every user in the database system  refer slide 53.42  the privilege names used in the grant command um or or the same names as um the the sql command that is used for for this privilege that is a select privilege gives an authorization for reads an insert privilege gives an authorization for inserts and so on an update privilege gives an authorization for modification and so on and all privileges the the keyword called all privileges suppose we say grant all privileges it means all the above privileges can can be granted to the specific user in question  refer slide 54.16  it is possible to revoke privileges um that is um  noise  by using the revoke command that is revoke privilege list on relation or view name from user list and of course we can use the um  noise  now familiar terms called either restrict or cascade restrict basically means that if i revoke a particular privilege from a from from a set of from a given user all the privileges that were granted by the user to other users will not be revoked they they stay in place on the other hand a cascade privilege says that if i revoke privilege x from a user and if that privilege x has been passed on or granted by the user to other users they are also invoked in in a cascading fashion  refer slide 55.04  so that brings us to the um to the end of this session where we have um looked into the notion of constraints in the database system essentially we have seen in different kinds of constraints that a typical dbms um system provides and they can be classified as either um key constraints that that act on the key attributes of a particular relation or they could be attribute constraints which act on any attribute um which checks domains or validity of of the attribute value with reference to its domain um in any given relation and there are tuple constraints which act on the on the complete tuple itself that is um where the value of certain attribute may depend on the value of other attributes on a tuple wide basis and so on and then there are schema level constraints especially assertions which are a general purpose um schema level constraint which um maintain certain integrity um over the entire database for the lifetime of the database or until they are until the assertions are dropped and finally we looked at triggers or eca rules that that are not valid always but um are awakened whenever certain events happen and um a certain condition holds which in turn prompts them to perform certain actions we finally then looked into the notion of privileges on authorization that different users in a database system can hold and they could be granted certain privileges and privileges could be revoked from them and this can happen in a um in a restricted fashion or in a cascading fashion so that brings us to the end of this session lecture # 14 query processing and optimization hello and welcome to another session in database management systems we have  noise  in one of a previous sessions especially when we talking about a storage structures and index structures we talked about what is a biggest challenge facing a databases such as yesterday the challenges is no longer the problem of storing data or of designing that can store larger and larger amount of data in fact we have very small devices today that can um that can store huge amounts of data devices that you can probably put in your pockets um are something which which you can wear inside your watches and so on um which can store something like two giga bytes of um of data therefore the the problem today is not not primarily of storage of data storage of data has become much more um the surface area required for storing data has become has shrunk in tremendous proportions and the cost of storing data is also fallen tremendously over the years however this fall in cost or this affordability of massive amounts of data storage has resulted in um a new problem or a new challenge the challenge is that are of retrieval of data how efficiently can we retrieve data um among the huge amounts of data that we have stored we also saw how the um definition of very large databases has been changing over the years today in um ten years ago very large databases would probably have meant hundreds of mega bytes of data probably of few giga bytes of data however today when we talk about very large databases its we are talking about databases that are easily into peta bytes of data ten power fifteen bytes of data so we saw um we had mentioned these things when we trying to motivate the um the use of index structures or the auxiliary files those are files containing metadata that can quickly point to that can help the application program or um the query to quickly retrieve the required data elements form the database there is however um another aspect of the story um using index structures alone does not help in efficient retrieval the other crucial element in effective retrieval of data and making a data or making the difference between usability and um unusablity of a database is the is a query execution strategies or the query execution and optimization strategies this is the topic of this session and the next few sessions that we are going to consider query execution and optimization um  noise  without it goes without saying is an extremely important aspect of database management and um this is what is going to determine whether a particular query is going to be useful at all or not if a particular query execution um strategy um takes um takes enormous amounts of time to to retrieve a particular um to retrieve a data element that can make the difference between whether the query is interactive in nature or whether the query is is batch in nature that is whether you have to whether the database will have to say that please come back after two days for for your query results and so on  noise  so let us look into what makes up a query execution and query processing and optimization  refer slide  05.07  query processing as we said before effective query processing or efficient query processing is crucial for good or even effective operations of database a database can be rendered unusable if a good query execution strategies are not used let us do a quick calculation suppose i have one giga bytes of data um  noise  and one giga byte as you know is thousand mega bytes of data even it is safe to assume that for for most of the server class pc s today we have data transfer rates of something like one mega bit one mega bytes per second um well it is usually one mega bit per second let let us consider that it is one mega byte per second or eight mega bits per second so this if i have to access or if i have to scan through a relation let us say i have a um i have a query that select query which which requires me to scan through the entire relation of one giga bytes of data that means it would take about thousand seconds for me to scan the entire relation because it is of a one giga bytes of data now consider a query which um which is given on two different tables each of one giga byte of data so there are two giga bytes of data that is there in the database now a bad query execution plan would would actually try to compute a cartesian product of the two tables before trying to return the um return the results that we required now if i have to um if i have to compute the cartesian product of one giga bytes of data times one giga bytes of data where each axis of the table is going to take me thousand seconds then it is going to easily take me ten power six that is one million seconds just to compute this cartesian product which is clearly unusable and which is clearly um ineffective as far as an interactive response time is concerned therefore efficient query execution or um trying to rehash a given query in in a more effective form for example in this in the example that we just took up the query might um might be able to figure out that what the user really wants is a natural join for example or or some kind of an equi join rather than a cartesian product if it is able to figure that out um then then probably you um you would get a response in a little more than a thousand seconds which which is much better than a million seconds um for for query execution and query processing depends on a variety of factors and not all of these factors are under the control of dbms what are this factors that um that can affect query execution let us take a few examples one of the factors that affect query execution is for example whether the storage media is fragmented or defragmented if the storage media is let us say fragmented um if you remember what is meant by fragmented storage of in a fragmented storage contiguous block on on the storage media belong to different files now if i have to access a relation if i have to scan through a relation let us say um in response to a select statement um and these blocks are divided or distributed all across the storage medium then the the response time would be increased considerably for this query so not all query execution or not all factors that affect queries or query execution times or under the control of the dbms um the the example that we took right now is not purely outside the control of the dbms also this is because several different several kinds of database management systems would override the operating system um override the operating system mechanisms and then start to deal with devices directly for example they they create their own file system that can ensure that um the file system is never defragmented is never fragmented at all and so on therefore um some um high end database management systems would try to overrule the the underlying operating system and try to access the hardware directly in order to speed up query execution and um and speed up or decrease response times and um as we saw earlier insufficient or incorrect information about factors that can that um that affect query plans can lead to vastly ineffective queries for example if a query execution plan estimates that the size of a table um is let us say few kilobytes but the size of the table is actually a few giga bytes then whatever execution plan that it uses for um for a few kilobytes will not work for for the few giga bytes table because the entire strategy changes when the scale of of the problem changes um a few mega bytes can probably be or few kilobytes can probably be stored in main memory whereas a few giga bytes can not be stored in main memory also and query executions usually use what is called as catalogs we shall be looking into catalogs in more detail in a later session um they use what is called as query cataloging that help in estimating several factors or several infor several kinds of information about the database this this can include the the size of um the size in terms of bytes of a particular table size in terms of um tuples or the number of tuples in a in a table and estimate of the number of tuples and um and estimate of the number of distinct values in a tuple that can help in building indexes for example query catalogs hence play a very crucial roll in deciding the query execution plan that is ultimately used on the database management system how does a typical um process or query execution process or query processing um process look like the typical steps in a query execution um process is quite similar to the execution steps in typical compiler um the way a compiler complies a given high level language construct into machine language and executes it  refer slide  12.08  we start with the user description of the sql query the sql query is is then read and parsed that is the sql query is then read by a query complier that performs scanning where um where lexical analysis is performed that is the the sql query is read character by character and then tokens out of the characters are recognized and then a token stream is given to the parser which in turn parses the query constructs a syntax tree and then the the um and then the tree is validated for semantic checks for for types and interoptability and and so on and once this is done an intermediate form of the query is is generated and which is called the logical query plan tree this intermediate form of the query is usually a relational algebra representation of the sql query now once this intermediate query tree is generated this um a series of heuristics and cost based searches are used to rewrite this tree or rewrite this query execution tree in order to make it more optimal or in order to make it um in order to use a better equivalent query for whatever execution whatever query has been requested by the user  refer slide  13.33  this intermediate query representation is then given to the query optimizer which in turn generates the query execution plan that is it rewrites the tree in order to um in order to reorder few of the operations and then um a final physical query plan tree is created we shall be looking into a query optimization strategies in a later session however there are optimization strategies can be broadly divided into either a heuristics based optimization strategy where which are essentially some kinds of thumb rules which which have been known to yield better strategies for for query execution and then there are what are called as cost based optimization strategies where an estimate of the cost that is required to execute one query plan against the other is used in order to um in order to um utilize the best potentially the best query execution plan the physically query execution plan is written in a separate language not necessarily the machine language but um there is a separate language that uses it own construct or its own um algebra for  noise  for representing um what are called as internal queries that is the the query that are actually performed on the storage structures on the physical files that are stored onto disks the query execution plan and then given to the query code generator which either executes the query as it is that is um starts giving results directly which which is called the interpreted mode of query execution or it generates machine code um which is called the complied mode of query execution which can then be used to actually perform the um physical operations required to answer the query  refer slide  15.31  the code um the machine code that is generated in a in a compiled um mode of query execution is then given to the run time database processor which executes the code and returns the query results  refer slide  15.49  in these steps of query executions two aspects are important and interesting these are the intermediate form of the query and the physical query plan the intermediate form of the query as we mentioned before is usually a relation algebra equivalent of the sql query that the user has given the intermediate form of the query is in the form of tree structure which um which is also called as an expression tree where relational algebra operators are are on the non leaf nodes and and the actual domains form the leaf nodes that is the actual relations on which query is are executed from the leaf nodes the tree is the then rewritten based on a set of rules that are derived from either heuristics or cost based optimization to to um generate generate an equivalent tree which produces the same query but preferably in or hopefully in much lesser time with with much lesser overheads the physical query plan is written in a separate language which has it own construct and that is either interpreted or complied into machine code let us have a look at what constitutes the physical um query execution plans and what are the constructs that make up this physical query execution plan the logical form or or the logical query execution plan or the intermediate form we shall be looking into greater detail in later session  refer slide  17.24  the physical query plan comprises of a basic set of operators that define the language of physical query execution now what should this operators be obviously the operators at the lower level or or the inner query or the internal query should contain all the operators of relational algebra itself that is if the relational algebra says select on this condition the internal um language should also be able to support um an operator that that can select a particular tuple based on a set of tuples on this condition however on addition to the relational algebra operators there are several other operators which talk about a physically accessing tables and um iterating through them um or or several other physical operations note that um relational algebra itself does not concern itself with the physical implementation of the database  refer slide  18.23  let us have a look at a few candidate physical query plan operators and see how they work that gives us a flavor of how does the physical code actually look like or or the query execution code look like um the first operator that you are going to see is the table scan operator a table scan operator has the name suggests just simply scans a particular given table that is it scans and returns an entire relation r or um the operator can be parameterized in the sense that you you can give certain conditions to the table scan operator that scans a given relation r and returns only those tuples that satisfy the given condition or the given predicate  noise  the main operations that are performed by table scan is to read all blocks note that blocks is the physical um component of um in terms of which the records are stored so a table scan contains a code or or the operator for table scan contains code by which blocks belonging to a particular file or or a particular table are read in sequence um and the block is and the table is returned there is also an index scan operator that makes use of an index file in order to read all blocks of a data file in sequence  refer slide  19.50  another operator that is used usually in the physical query plan language is a sort scan operator the sort scan operator scans a relation r and sorts this results before retuning it to the higher level whichever called it um if um if the relation is already stored in a sorted form and the sorting is also required on the same ordering attribute no sorting needs to be done separately by the sort scan operator and if the relation is small enough to fit in memory then sorting can be sorting can be done directly in memory however if the relation is too big to fit in memory then external sorting and external sort merge techniques have to be used in order to sort the given record  refer slide  20.41  the next um physical query plan operator that um that is quite frequently used is what is known as the iterator iterator is an important concept in in managing um or in the physical management of records if you have probably let us say programmed in um a done programming c plus  plus let us say using the standard template libraries on on unix environments or or the active template libraries on microsoft environments you would have come across the the term iterator in several places um what is iterator do the iterator is um is a operator that that functions on an operand which is a composite operand for example an iterator operates on a hash table or a linked list or a tree or something like that um so the iterator operates in a way that iterates through each element that form up forms the composite operand that is it starts from the first element and it comprises of um getnext function which can get you the next element until you reach the end of the um end of the operator or the data element so iterators typically contain three different functions as shown in this slide here the first function open um would will open the iterator object that is the the composite object on which the iterator function has to be performed the next function called getnext is going to get the next logical block that or or the next logical record or next logical node or whatever it is in this composite object um that has to be return and then the last operator called close closes control on the object  refer slide  22.35  this slide shown an example of the iterator function um example of a table scan iterator that is um how we can implement the table scan operator using an iterator um as the slide shows there are three different functions that have to be implemented open getnext and close which is shown in the next slide here the open function let us say given um relation or or a given file um let let us consider relation to be stored in a file or a table to be stored in a file  noise  the open construct initializes two variables um a variable called b which which points to the first block of the relation and a variable called t which points to the first tuple in inside b the getnext function just iterates through this variables that is um before we are go in to getnext function let us try to um ask ourselves what should the getnext function do ultimately for the programm which is calling the table scan iterator all that is required is the the set of tuples one after the other the getnext function um however has to deal with two different um two different things the blocks that is the physical data stores and the tuples that is the logical data stores now tuples can be iterated within blocks but when a block is exhausted the blocks themselves have to be iterated across the files that is the next logical um block in the file has to be chosen um so the getnext function um performs precisely this um set of function that is um if t is beyond the last tuple in b that is if the current um block b has been exhausted then we increment b that is um point b to next logical block in the sequence in in the file and if b is beyond the last block in the file then you return no more data that is um that is it is um it is exhausted in the record or or the file or else um the else condition he has states is um basically t is beyond the last tuple in the but b is not beyond the last block in the file that means set t to the next um or set t to the first tuple in b that is the next um b is been incremented and set t to the first tuple in b or or the new block  refer slide  25.10  and then increment t and return the old value of t which was which t was pointing to so so return that is we we first assign oldt equal to t and then increment t and then return oldt for close we don t have do anything because we have already returned we already return from the getnext operator if we have reached end of the file therefore close in this um in this example is um is redundant however usually the close um function performs some kinds of a clean up operation where if some data structure were opened during the um during the course of the iterator function these data structures are closed and and the corresponding memories freed and so on let us implement um or let us look at an example of the iterator function we shall implement the table scan operator that we saw earlier using the iterator function  refer slide  26.16  as we saw before an iterator has three different functions the open getnext and close assume that um the table is implemented or um is contained within a file and the file is organized as a sequence of blocks that is there are several blocks that make up the file so when we open the iterator that is open the table scan iterator um we need to initialize a few things um this is shown in the slide here that is the slide um the open function um initializes two different variables b and t where b is points to the first block um in the file or of the record and t points to the first tuple in the block the getnext function note what the getnext function should return here the table scan operator should return um tuples after tuples that is the first tuple second tuple and so on however at the physical level we are concerned not only with tuples but also with blocks that is we actually read read and write in terms of blocks and in not in terms of tuples therefore um  noise  initially what we do is we first check to see if if the um if the tuple is beyond the last tuple in b if this is the case we have to increment b and if is beyond the last block in the file then we just return no more data that is we we say that is all there is um there are no more tuples to return or else that is the the the else is for the inner if so else t is then set to the first tuple in the next block that is we have incremented b and then we we just set t to the first tuple in the next block and if none of these is the case then we just return the next tuple that means we fist copy the um corresponding tuple to be return in the into a new variable which is called oldt in this example and then we increment um t and then return oldt  refer slide  28.10  for the close function in this example we don t have to do anything because we have already returned in the getnext function when we have reached the end of the file but usually in a close function um we we use the close function to clean up whatever um whatever mess we have created so to say that is um whatever data structures that we are opened whatever memory we have allocated which which are not useful anymore have to freed up and so on so um so the close operator or the close function is called at the end of the iterator which performs all this clean up operations let us look at another example using iterators on how to compute the bag union of two  noise  of two relations um what is the bag union  refer slide  28.58  remember we have talked about um a considering relation as bags rather than sets a bag is just a collection of um tuples or collection of elements um  noise  without regard to whether there are duplicates in the collection so it is it is it is also called a multi set a multi set union or a bag union is simply a bag that is made of two different bags that is you just empty contents of one bag into the other and you have got a multi um multi bag union or union over bags so um um this is denoted by the operator plus or or the disjoint union operator or plus plus so an iterator for performing the disjoint union here um we are considering that both r and s which represent relations for us are now in the form of iterators themselves that is we are abstracted away a relation form being a file to being an iterator that is we know it is just a data structure which we can open and um call the getnext function and then close once we are done with the data structures so as far as we are concerned um both relations are just iterators so in the open function of our um disjoint union iterator  refer slide  30.25  we just open one of the relations we we say r dot open and then we point the current relation to be r in the getnext function we say that if um current relation equal to r then we have to call getnext on the on the current relation that is we just say current relation dot getnext and if getnext returns no more data that is um if if there is no more data that that is return then  noise  um  refer slide  30.53  start or or set current relation as s and then call s dot open and then in the um in the sub sequent getnext operations you just call s dot getnext instead of r dot getnext so so what we effectively done is um we have exhausted one of the records by calling getnext as as many times as possible that is whenever getnext is called on us we call getnext on the the current rel that is the currel relation so once we exhausted one of the relations we open the other relation and start um and start calling getnext on that function so when s is exhausted it returns no more data which is what should be returned by the getnext as well and in the close function we just close both of these um iterators that is we we called r dot close and s dot close when r dot close and s dot close don t do anything which you saw in the previous example but in case they they do certain clean up operation it is always a good measure to um or it is always a good programming practice to call um the corresponding close operators in our close operator  refer slide  32.13  so um we just um went through some of the um some of the elements of the physical query plan program programming language that is um it contains elements of table scan index scans and iterators and and so on now let us have a look at some algorithms that are build around these data structures or around this constructs of the physical query plan that can help us in understanding how a given relational algebra operator let us say like select or project or um something of that sort are is actually executed inside the database system um we can broadly divide algorithms for a data access into one of the three following categories we call them sorting based methods hash based methods and index based methods um these methods has has you can see here are typically meant are oriented towards um increasing the effectiveness of search in a sorting based method the the relations that are um scanned are scanned using the sort scan operator that is they are sorted as in when they are scanned and because they are sorted um the the property that the the relations are sorted would help in um in in performing certain other relational operators like say join um in a efficient fashion similarly hash based methods um use some kind of a hash function to quickly search for whatever tuple or um data element that that is being asked for within this relations an index base methods resort to index structures like um trees or balance trees and so on um for for searching the the required data element the um  noise  we can also divide algorithms for data access based on what kinds of data access requirements that the course we can divide the kinds of data access requirements into one of these three kinds of requirements the first requirement is what is called as a tuple at a time unary operator that means um the the query requires or requires to contend with one tuple at at time for example select and project operator um every time select is called select has to be um or the condition for select has to be checked against each tuple in the relation that is tuple after tuple so at a time one tuple is being accessed and this is a unary operator that is um it is just one relation on which a particular tuple is being accessed then there are full relation unary operators where the entire relation has to be searched for example if i have to return um something based on or return the value of some relation or if i have to compute let us say set theoretic operations like um a like not of something and and so on um or any kind of set theoretic operation that that that are unary in nature and um um and the last kind of operations are full relation binary operators these are operators that again have to um compute or that again requires a complete relation for their for as their query result and they are not just unary they are binary that means they have two um two relations to contend with  refer slide  35.52  that is some examples or something like set theortic operators like union intersection and so on which require two relations and the entire relation has to be scanned the entire relation has to be returned  refer slide  36.10  let us see how um each of these kinds of um query execution requirements can be meet using some algorithmic strategies in this session we are be going to be looking at um a kind of strategies what are called as one pass algorithms what is a one pass algorithm a one pass algorithm is an algorithm that um that that performs at most one pass over the entire database that is um over over the entire relation um of interest it does not access the relation multiple times um very important and many times limiting assumption in most of the one pass algorithms that we are going to see here is that it assumes that the relation that we are looking for is small enough to fit in main memory in many um cases this is the reasonable assumptions but in many other cases it is not reasonable assumption that is um even a single relation could be so huge that it it may not fit into memory into main memory so how does um how can we use a single pass or a one pass algorithm to perform a tuple at a time unary operation let us take some example like select or project as shown in the slide here let us say select some condition over r or project some condition over r all we have to do is scan through this r that is use the use the table scan iterator for um for scanning through this relation tuple after tuple and store this relations or store this store this tuples that are been scanned in a in a input buffer perform a unary operator and output it to the output buffer so this is schematically shown in the diagram here that is this is the relation iterator and this relation iterator returns tuple after tuple which goes into the input buffer and in  noise  in this case this input buffer can be as small as one tuple long that is we can allocate just enough memory in the input buffer to store just one tuple so each tuple after tuple is is um put into the input buffer and checked against the unary operator and either discarded or sent to the output buffer so as simple as it this is this is quite simple that is within a simple single pass we have been able to answer answer a tuple at a time unary operation  refer slide  38.49  what about relation at a time unary operations what are some examples of relation at a time unary operators one example is that of let us say the unique function in the in the select in the sql statement suppose i say select name from employee um or select unique name from employee that means given me the set of all unique employee name s without repetitions this as you can see it is a unary operator that means it it operates on just one relation however it is a relation at a time operator that is it requires to have the entire relation knowledge about the entire relation um before being able to return the required value so  noise  the general strategy or a general one pass algorithmic strategy for um  noise  now for relation at a time unary operators is shown in the figure in the slide here r is r is the now familiar um table scan iterator which returns to a tuple after tuple which which goes into the input buffer now the input buffer um is is then read into the unary operator whatever whatever be the unary operator whether it s unique or group by for example group by is another relation at a time unique operator now this unique operator um will either output a this tuple into the output buffer if it if it is safe to do so or otherwise will put the tuple into a data structure holding the history of um of whatever relation has been read until now for example in the unique operator all we need to do here is have a hash table that contains one entry each for each unique entry that we have found until now in the database so whenever i read a new name um let us say whenever we read a new tuple into the input buffer and check out the name attribute we just check the hash table here the the data structure holding history um we just check the has table to see if this name name was already encountered if you are already encountered this thing if you have already encountered this name then we just discard this new tuple otherwise we add this new name into this um hash table here and then output the tuple so um a single pass algorithm um for a relation at a time is also quite simple accept that we need to have an augmenting data structure in the form of usually a index tree or a hash table or something like that that can hold um the the history that is required now um one more thing that is to be noted here is that suppose the unary operator that we are um concerned with is the groupby operator now the groupby operator can not return any output until the entire set of relation is um or the entire set of tuples in this relation is read and the performing grouping is formed using this data structure that means this space allocated to this data structure should be large enough to hold the entire relation therefore a such a algorithm can not be used for relations that are too large to fit in memory um because we are concerned only with one pass algorithms in this session here we assume that the relation can be held in memory so that um the entire relation or the entire history of what we have read can be held in the data structure  refer slide  42.35  let us look at one pass algorithms for relation at a time binary operators now one pass algorithmic strategy is vary depending upon on what is the binary operator that we are looking and almost all of the algorithms that um for binary operators require that at least one of the relation be read completely into memory before we start reading the other relation and obviously um if we have two relations and one is much smaller than the other it is it makes much more sense to read the smaller relation into memory and iterate over the larger relation so let us look at a few examples and um which will make this clear  refer slide  43.19  let us see what is the strategy um what is a one pass algorithmic strategy for computing the set union of two relations r union s i have explicitly use the word set union here instead of just union that means this is not a bag union that means to say that we have to compute r union s without um without returning any duplicate entries in the result that means we have to remove all duplicates while returning r union s assuming that among r and s r is the bigger relation here is a very simple strategy to compute r union s first read s into memory completely using the iterator on s um retrieve all tuples from s and and place it into memory and place it in a place it in some kind of data structure like an index or hash table by which we can access each tuple in um of s in as efficient of as possible now um as an when we are s keep outputting the tuples of s because anyway r union s should contain all tuples of s then once s is completely read into to memory and indexed in a data structure start reading r that is the the next relation and for each tuple of r that is read into memory check whether it already exist in s if the tuple already exist in s then just discard the tuple because we we do not want duplicates in the output result otherwise if it does not exist in um in r then or if does not exist um in already in the in the relation then just output the tuple now here we are also making another implicit assumption that r and s are sets themselves and they are not multi set that means um there are no duplicate tuples in r itself therefore it is sufficient for us to check for duplicates um against s um otherwise we need to also store tuples in r so as to check the duplicates within r itself if we assume that r and s are sets the set union operator can be performed using the strategy that we outline just now the next binary operator that we are going to look at is the intersection operator the strategy for the set intersection operator is also quite similar to that of the union operator  refer slide  45.56  assume that we have to perform the set intersection between r intersection s and assuming that r is the bigger relation we first read s into memory and then store tuples of s in a in an in-memory data structure or in memory index or hash tables structure that that can help us access the data elements of s quite efficiently then start reading r into the memory tuple by tuple using the iterator for r then for each tuple um of r if and only if the tuple also exist in s output the tuple of r in to the output buffer otherwise discard the tuple  refer slide  46.38  what about set difference set difference if you see um differs depending on whether we are computing r minus s or s minus r because set difference is not a commutative operation now suppose let us say without loss of generality let us say we are computing r minus s and that r is the bigger relation okay so we are computing r minus s and the first relation r is the bigger among the two relations that means we read s into memory as usual that is um read the relation s using the s iterator into memory and put s into a in-memory index structure or a hash structure and for each tuple of r check to see whether um it whether it already exists in s if it already exist in s then discard the tuple or if does not exist in s then output the tuple as simple as that but what happens if um if we compute s minus r that is r is the bigger relation and and um it is right hand side of the difference that is instead of computing r minus s we are computing s minus r because r is the bigger relation um it is always more efficient to read s into memory rather than r now if we read s into memory how does the algorithm change let us have a look at that so um this slide shows set difference s minus r instead of r minus s and um and and assuming that r is the bigger relation now if r is the bigger relation have a look at the steps closely for this slide here if r is a bigger relation all we have to do is first read s into memory completely that is um read the complete use a s iterator and read all tuples of s into memory and place them into a index structure or a hash table now for each tuple of r okay what should we do that is we are we are computing s minus r that is s minus r is the set of all tuples in s that are not in r okay so for each tuple of r check to see if it already exists in s and now what happens if it already exists in s if it already exist in s then these tuples should not be their in the final output okay and though the tuples that should be in the final output or those tuples of s that are not in r so um so what we do here is um for each tuple of r um for which we found a we find a matching tuple in s we cancel them out that is we delete the tuple in s from the index structure that is we we deleted from the index structure of the hash table that we have been using now once r is exhausted that is once we have finished reading through the um relation r and we have deleted all common tuples whatever is left in um in the s data structure that that we have read into memory is the output that is that we can push them onto the output buffer  refer slide  49.47  what about cross product r times s cross product is simple as far as the algorithm is concerned and quite expensive as far as the performance is concerned that is again assuming that r is the bigger relation just read s into memory and um we don t we don t need any data structure here we we can just store s in it is contiguous um sequence of memory locations and for each tuple of r combined with every tuple of s and start returning it as simple as that  refer slide  50.18   noise  the last one that we are going to be looking at um is the is a one pass algorithm for natural join what is a natural join a natural join is an equi join on two relations that equates attributes having the same name and domain now assume that r x,y and s y,z are being natural are being subjected to a natural join that means y is the common set of attributes or subset of attributes between r and s now assuming again r is the bigger relation read s completely into memory and then index um or place x in a hash table or an index so that it can be searched efficiently now for each tuple of r what we have to do is search through the hash now this hash table or the indexing structure should be um should be done based on the um common attributes that is based on y here even if y is not the key attribute so so we perform the indexing or the hashing based on the based on the common set of attributes y and then using which we can search for every tuple of r that is read whether there is a matching tuple in s if there is a matching tuple match the two tuples and output it to the output buffer um if not just discard the tuple of r  refer slide  51.44  so um that was a brief overview of the one pass algorithms that um that can be used um by by using the the physical query plan operators like say iterator and sort um table scan sort scan etcetera using which we can develop a strategy for performing relational algebra operations like r union s r intersection s select project unique groupby and so on but what are the constraints of one pass algorithms one pass algorithms are applicable especially for binary relation at a time binary operators they are applicable only when one of relation can fit completely into memory and it is not just it fits completely into memory we should also have extra blocks of memory for the other relation that is let us say if i have m memory blocks available for me then the the smaller relation can be at most m minus one in size m minus one blocks in size it cant be m in size because we need at least one more block to to perform book keeping for the other other um relation that we are reading from disc  refer slide  53.00  and one pass algorithms rely to a great extend on correctly estimating the size of relation if a query execution plan in a dbms engine decides to use a one pass algorithm for performing a particular query um relational query relational algebra operation then it depends for a large part large part on the estimate of the size of the of the relation that it has now if the size is wrongly estimated then it can um for example if we if we allocate two few buffers thinking that the size would be small then the one pass algorithms will be unusable the query execution plan is unusable we can t use the one pass algorithms at all on the other hand if we allocate too many buffers um thinking that the relation size is big then we may end up in a possibility of thrashing where um memory blocks have to be swapped on to disk and so on so it is quite crucial to obtain a good estimation in order to use one pass um query execution plans  refer slide  54.15  so let us summarize what we have learnt in this session we have kind of a scratch the surface of an important and crucial area of data base management systems called a query processing and optimization we we have seen that we have seen the different stages in query processing um and and two important intermediate steps in query processing namely the logical query plan and the physical query plan and in this session we have concentrated more on the physical query plan that is um a physical query plan is is a set of language constructs that perform low level operations using um that performs low level operations directly on the storage media that can physically access um files into memory for for reading for performing any data base related operations we also saw what a physical query plan language would look like or rather what kinds of constructs it would have it would obviously have relational constructs in addition it would have constructs like iterators tables scans sort scans etcetera and we have looked at the variety of one pass algorithms using these physical query plan constructors using which we can perform a variety of relational algebra operations like select project unique groupby and many other set theoretic operations in the next session we shall be looking into the the big question of handling joins in in query execution plans and also have a look at the logical um query execution plans so this brings us to the end of this session thank you lecture # 15 query processing and optimization ii hello and welcome to yet another session in database management systems in the previous session we started at looking at a very crucial aspect of dbms design note that i am using the term dbms design and not database design um we we will look at um we will address this issue in more detail a little later what is dbms design verses what is database design we started by looking into a very crucial aspect of dbms design namely that of query processing we saw that nowadays um or in today s world the size of the database is no longer a problem we can have huge amounts of data stored in very small amounts of space and um and being available at pretty cheap cost infact um i have with me a small device which i can show it for you this is the small device which is a which can hold a database of size two hundred and fifty six mega bytes of of store and this costs um roughly about three thousand rupees so you can store um data of the order which was not even envisaged before in very small devices at pretty affordable cost and um and which occupies very small amount of space therefore storage of data is no longer a problem the problem today is retrieval of data retrieval in the sense it is not just reterving any data it is retrieving whatever data that is required by the user therefore we saw the the the crucial elements today or or the crucial technologies of that is going to impact database in the forth coming years are those um are those techniques that can help in retrieving data elements as quickly as possible from databases we saw that auxiliary files in the in the physical storage world which comprises of index structures and several kinds of hash structures and so on um is one crucial element in making this happen that is making fast data retrieval happen the second crucial element is query processing given a users query can we process the query in such a way as to return the results extremely fast there are several techniques that are used for um for making query processing interactive in nature that is remember the example of google if suppose google were to say that when you give a web search query suppose it would say come back after two days for you answer it becomes unusable nobody would use it if if the other alternatives that is but um a search engine like google can search peta bytes of data or or the search space of data is peta bytes of data and it can return you um relevant results within a few seconds now this is possible um one of the techniques for this is to use very intelligent query processing techniques of course there are um if you throw more hardware into your database if you throw faster processors it it becomes faster and so on however a crucial element is is still the algorithm that is used to retrieve data it can a bad algorithm can make the difference between a database that is very efficient to a database that is unusable for all practical purposes um let us briefly review what we learnt in the previous session in query processing and move on further today to to look at some more aspects of query processing efficient query processing is is crucial for good dbms design um and in fact it can make the difference between a database that that is operational or effective and database that is ineffective  refer slide  05.14  a bad query processor can can render a database all but useless when the size of the database grows beyond a certain size beyond a certain limit query processing depends on a variety of factors um unfortunately um and not everything is under the control of the dbms we saw yesterday that query um query processing could depend on let us say the um the the memory size or the size of the um tuples or or the size of the relations that that are present in the database and and the kind of file system that are stored um whether and whether the file system is fragmented or defragmanted and so on so there are there are variety of factors that impact query performance um and not all of these factors or within the preview or within the control of the dbms therefore query processing is um oriented towards obtaining the best out of whatever is available um at the biggest or at the control of the dbms  refer slide  06.20  what are the typical steps that are um that are handled or that are executed in a query execution process given a query given an sql query the the query is fist scanned using a lexical analyzer which in turn return um it takes the string which is the which forms a query and returns a set of tokens and then these token streams is then passed to build a um syntactic tree um or what is called as a parse tree and then syntax analysis and so on um semantics checks and etcetera are all are all performed which comprises the validating phase of the query complier and once this is done an intermediate query representation is generated this intermediate query representation is is also termed as a logical query plan tree now this logical query plan tree is usually a tree data structure that represents the relational algebra equivalent of the sql query um using this tree data structure there are several rules that are employed in order to systematically um optimize the tree for better performance the intermediate query representation or the query tree is then given to the query optimizer which generates a query execution plan the the execution plan is also called the physical query plan tree and um in in contrast to the logical query plan tree that means a physical query plan tree comprises of um a query plan written in a language that is usually executed by by a dbms interpreter or would be complied into machine code for execution and um this physical query plan language comprises of its own constructs um some of which we saw in the in the previous sessions something like table scan constructs and sort scan constructs and iterators and so on in addition to um all relational algebra constructs like select project set union difference and so on  refer slide  08.30  the query execution plan if it is an interpreted dbms would be executed directly and the results would be returned to the user if it is a complied dbms um engine the the query execution plan is then given to the query code generator and then appropriate machine code is generated from the execution plan  refer slide 08.53  of course the the last step would be the the code um if it is generated in in a complied mode it is then given to the operating system run time um or or the database runtime and then query results are given  refer slide  09.10  now we are primary primarily interested when we are taking about query processing in two of these stages in the um query execution stages the the first stage that we are interested in is the intermediate form of query representation in fact that is something that we have deferred to a later section we in fact went straight onto the second stage that is a physical query plan representation we saw some constructs of the language that that is used to um that is used to describe the physical query execution plan and um constructs that are or the physical execution plan is then um either fed through a interpreter for this language which executes the query or is complied onto machine now what are some of the operators that we saw yesterday the um physical query execution plan operators are those set of operators that defined the basic um physical operations that need to be performed in order to answer a query this are also sometimes called the internal query operators or or the operators that make up the internal query language sql or relational algebra on other hand is a is an external query language that is it is the query language used by the users or application programs that are using the database but the database itself for the dbms itself uses a internal language for answering queries that accesses the file system or or the or the storage structure used by the dbms in order to in order to retrieve tuples in response to a query um the this of course comprises of the set of all relational operators plus some additional operators that um that talk about physical characteristics of um retrieval some example operators that we saw yesterday was table scan operator which um which can be implemented using an iterator um operator that is an iterator objects so to say um where which opens a table and returns a tuple by tuple that is an iterator if you remember has three different functions um an open function a getnext function and a close function a table scan operator opens a table when the open function is called and returns the next or or the top most tuple um on every getindex or getnext invocation and starts incrementing a pointer so that the next tuple is returned and so on so so the i th getnext invocation would return um us the i th tuple in the table and then the close function would close the table123 and then there was index scan that can that would use an index to get a particular or get the required tuple then there is a sort scan operator which not only um returns all tuples in a table but also sorts them before returning them we then saw some some set of one parse algorithms that that are built on top of these physical query plan um constructs or data structures that perform various operations  refer slide 12.22  we divided one pass algorithms into into three different kinds of categories the first category was tuple at a time algorithm a tuple at a time algorithm is something like select or project which needs to concern or which needs to be concerned only with one tuple at a at any given point in time um tuple at a time algorithms can can can easily be executed with a single parse regardless of the size of the relation because we assume that the tuple is never large enough so that it can not fit into main memory are all we always assume that one tuple of um of the database can always fit into main memories therefore since we are only concerned with single tuple at a time we can always use a single parse algorithm for tuple at a time functions and on the other hand there are relation at a time um algorithms that is there are functions that need to be concerned with the entire relation rather than each tuples independently for example the function called unique in sql needs to look at the entire relation in its entirety of course rather than each individual tuple in isolation relation at a time functions can use one parse algorithms only if the relation can fit completely into memory um  noise  and its um that s not just that s not enough in fact the relation should not only fit completely into memory but especially for binary relation at a at a time functions there should be at least one block left over to read data from the other relation hence if um if a relation requires m memory buffers um or or rather if m memory buffers are are present for for the dbms then the maximum size of one of the relations of the smaller relations can be um at most m minus one it can t be m because we need at least one more block for the for data from the other relations  refer slide 14.36  and of course that is the limitation of the one parse algorithm in the sense that you can use then only when um you know that at least one of the relations can fit completely into memory now how how do you know that a relation completely fits into memory what if you don t know the size of the relation in such cases you need to estimate the size of whatever data relation or whatever table that that you are using hence one parse algorithms rely to a very large extent on procuring good estimates of relation sizes in terms of the number of tuples and um if if the estimation algorithm is wrong that is if too many buffers are allocated um that is if the if the estimation is um is too high then there is a possibility of thrashing on the other hand if the estimate is too low then we may not be able to use one parse algorithms at all because the the relation wont fit into memory and um and the strategy is that we studied for one pass algorithms will no longer be applicable whatever we do in such cases that is what do we do in cases where um one or both of the relations or too too big to fit in memory bytes for these we use what are called as multi pass algorithms um  refer slide  16.03  in multi pass algorithms we shall be studying today mainly about two pass algorithms generally many multi pass algorithms are generalizations over two pass algorithms and if you know the general strategy that is used behind two pass algorithms it is sufficient or it would not be too difficult to generalize it to multi pass algorithms and um multi pass algorithms has the slide shows are used when relations can not be read into memory in their entirety only a part of um each relation can be read into memory and they they usually have an alternation between reading um reading part of a relation into memory writing it back onto disk that is an alternative alternation between computation and intermediate result generation and retrieval of intermediate results  refer slide  16.57  the first kind of two pass algorithms that we are going to be looking at or what are called as sorting based two pass algorithms we are going to be looking at three different paradigms or three different strategies of two pass algorithms sorting hashing and indexed base two parse algorithms the first one that we are going to be looking at is the simplest um which is called the sorting based two pass algorithms the basic idea behind sorting based two pass algorithms is shown in the slide here suppose we have given a relation r of course um that is um such that the relation r is too big to fit in memory and um and of course for the dbms we are given a maximum of m blocks of um memory that is m blocks of memory elements can be um are are allocated for the dbms therefore m blocks of data can be read from the relation r by the dbms at any point of time the sorting based two pass algorithms have the following basic structure  refer slide  18.05  this slide shows the basic or the skeletal algorithms for a for a two pass sorting algorithms um essentially as we noted before the algorithm um or any multi pass algorithm alternates between reading intermediate results and um writing them back that is reading part of data from the disk and performing computations and writing them back okay so the basic idea behind two pass algorithms are based on sorting is as follows um first start by reading the relations one or two relations that is we are we are going to concerned only with either unary or binary operators for the time being in um of course any nary operators can usually be um be reduced into one of these these forms and it its generalization of considering either unary or binary operators so let us consider that um the the relation or the pair of relations that we are going to be using are are first read um block by block okay so because we have m blocks that that are allocated to us we can read these relations m blocks at a time now um m blocks of relations from m blocks of tuples from from the relation or pair of relations are read into memory and then they are sorted once they are sorted they are they are written back into disk this is the alternating phase that is you read part of data from from disk sort them in this case which is which in this case is the computation and then write them back to disk now continue steps one and two until the relation is exhausted i am just assuming here that we are dealing with unary operator that is until relation r is exhausted and um it it can of course mean until the pair of relation is exhausted or until the set of all relations that are um that this query handles are exhausted and then once these um intermediate results are written on to disk that is um sets of different m blocks of sorted tuples are written onto disk use a variety of query merge techniques remember what is a merge technique if a if we have taken a course on let us say data structures you would have come across an algorithm for merging which which is usually um seen in relation with sorting a merging basically means that um given two or more lists that are sorted can i obtain a single list that is also sorted in of course in as efficient of as soon as possible um we have there are there exist very efficient algorithms for merging especially when the results or um especially when the lists are sorted that can produce results in a linear order of time so use a variety of these query merge techniques note here that um in the in the two parse algorithms it is no longer just merge techniques it is query merge technique that is um as in when you do the merging perform your query or answer your query as in when you are doing the merging on this intermediate results so use a variety of query merge techniques to extract relevant results from all the sorted m blocks of disk  refer slide  21.51  now let us see um some examples of um two phase or um two pass sortings two pass sorting based methods for different answering different kinds of queries um and you can notice that all of these examples follow the same skeleton of the basic idea that we presented in the previous slide so let us take first the example of duplicate elimination now as you can see duplicate elimination is or relation at a time in unary operator um we don t have to worry about tuple at a time operators for um for two pass algorithms why because all tuple at a time operators can can be answered using a one pass algorithm because all that we need to um be concerned about at any point in time is just a tuple and each tuple can be checked in isolation um with the other tuples so let us take the first relation at a time unary operator namely duplicate elimination that is the the unique construct how do we implement the sql unique construct um let us assume that it is relation r on which we have to eliminate duplicates and of course we have to also assume that relation r is too big to fit in memory now as we know that because we are given m blocks of memory available to us start reading r in terms of m blocks of data that is read m blocks of data into memory and sort them and um and the third step is store the sorted set of m blocks back onto disk now continue from step two that is read the next m blocks of um data from r sort them and put them back on to disk in in a separate file name okay so keep doing this until r is exhausted r becomes empty then what is the next phase now now um now we have read our relation sorted them and put them back into memory or rather put them back into disk the next step now is to use one of these so called query merge technique on these intermediate results that we have generated so what is a query here the query here is um is the elimination of duplicates that is we do not want any duplicate tuples to to appear in the relation once once it is output to the output buffer so how to we eliminate duplicates from these set of sorted intermediate results so the query merge technique for eliminating duplicates is quite simple  refer slide  24.36  take one block from each sorted sub list um there are several sub list of data that have been generated and each list of data is a maximum of m blocks in size that because that is because we have we have read data in terms of m blocks except the last set of data that we wrote every other data would um or every other intermediate result would be of size m blocks the last data intermediate element that we wrote would have a maximum size of m blocks it could also be smaller than m blocks in size now take um take the top most block in each set of these intermediate results that we have generated and um for each tuple that is take the smallest tuple among this for each tuple um when a which um just go pass the the repetitions of this tuple in the other blocks um let me illustrate this with an example in the next slide which makes it much more clearer it is as simple as taking one tuple and then moving or rolling forward all other blocks so that they move pass the present tuple that i have taken that is if they contain duplicate tuples if they don t contain duplicate tuples you don t have to move them so um and and of course once they are all moved put the put the first tuple into the output buffer now this is possible um or this is possible to be done in an efficient fashion because the tuples are all sorted that is if a tuple t appears at at one particular stage then all tuples that are um in some way greater than t should appear below them therefore i just need to search each block until i find a tuple that is greater then t when i am trying to remove duplicates this such a kind of merge elimination takes an order of m n b time where um m is the number of blocks in a sorted block set that is one of the intermediate result block set that that we have stored and b is the number of tuples in a block n is the number of such block sets um if you if you remember um merging algorithms a merging algorithm between two list of size m and n would take an order of m plus n time because we have n different lists of um of blocks of m different blocks that is each list comprising of m blocks of data and each block containing of b tuples of data we we just multiply all three of them together so that we get the total um time that that it takes for eliminating duplicates from let us look at duplicate elimination with with an example that makes the process much more clearer  refer slide  27.50  now let us assume that as shown in the slide here this is the first set of block set that is first set of m blocks of data that is written that is one two is in the first block two two is in the second block and two five is in the third block and now this whole thing um is a set of three blocks that were written onto disk and as you can see here this this entire thing is sorted they are in sorted order that is one is smaller than two um is less than or equal two and so on until five this is the second set of um blocks that were written onto disk two three four four four five and so on and this is the third set of blocks that were written onto disk we now start by taking just the first blocks of each block set into memory and then considering just the first tuple in the first such block that was read okay now the first tuple says that it is um or reads as one when when i read one here all i have do to eliminate duplicates is to eliminate one from all other block sets okay now if one has to appear in all the other set of blocks they have to appear at the top they they can not appear somewhere at the bottom this is because it is sorted um in order okay therefore one has to appear in the top so all we need to do is start from the top of each block set and start moving forward until we find a tuple which has a value or whose key value is greater than one the so um so therefore once we read one here we we look at the second block set and we read two to begin with and we know that one does not exist in this block set when we read the third block set we read a one and cancel it out read the next tuple and this is also one we cancel it out and then go on to the next tuple which is a two um so we know that we have exhausted all ones that have occurred in this block set and we can stop this process here and output one so after one is output um the the set of blocks become like this that is two is in um at the front one all the ones have gone and the um the block sets have been left shifted so to say in an appropriate fashion now the next tuple that needs to go out is two so um you see that the the first tuple here reads two and you have to eliminate all two s from everywhere else that is return this two um to the output buffer and start moving forward until you have eliminated all two s okay so therefore you cancel the next three two s here and end up at five in the second block you cancel the first two and end up at three and the same thing in the third block set as well that is you cancel the first two and end up at three  refer slide  31.06  this is um depicted in the second slide that is um after the second tuple is put into the output buffer that is after two is put into the output buffer our set sets of blocks looks as it is shown here that is um all the two s have been eliminated here and five has come to the top and all three s um that is all two s are eliminated here and three has to come to the top and all two s are eliminated here and three has come to the top here now if i start to read five here then there is a problem because i have not read three has has yet three and four still exist before five okay so so i can not start rolling this block set until i read until i go beyond five the simple answer to obviating this problem is um is to note that the first set of blocks from all the block sets are in memory so we just choose the least element from the first set of blocks and then start rolling the blocks therefore we we now choose three um as as the tuple to be output to to the output buffer and then start rolling each of the block beyond three therefore three would be output um from from the second block set and we we will roll it until we find four here and same thing three would be output here and we will roll it until we find five that is we we roll the block set until we end up at five and so on so in this way we can eliminate duplicates by um by taking the least element of the top most set of buffers of each block set and then rolling um blocks until we move beyond the least element  refer slide  33.00  how do we perform set union using sorting remember this kinds of algorithms that we looked at yesterday um for um relation at a time operators we looked at duplicate elimination we looked at different kinds of set union set theoretic operations like union intersection set difference and so on okay so set union is a binary operation it is a relation at a time binary operator okay and because it is a set union  noise  that is um i have emphasized the word called set it means that no duplicates in the result as you might have imagine the strategy for computing set union is very similar to duplicate elimination when we have to compute the set union between two relations r and r and s let us say r union s we just have to output all tuples from r and s without duplicates as simple as that therefore the the set union algorithm is quite similar to that of the previous algorithm where you read blocks from r and s rather than just from single relation sort them and store them onto disk now it does not matter for us whether um any given tuple that we have read belongs to r or to s we can just um consider r and s to be a single relation and and then start reading tuples from them sort them and store them back to disk and use the duplicate elimination merging technique that we just um looked into in the previous example for eliminating duplicates so the output of this would um would be it is clear to see that the output of this would be r union s  refer slide  34.49  what about set intersection using sort um using sorting techniques set intersection between two um relations r and s essentially has to return the set of tuples that that are common between r and s this set intersection is slightly different from the previous two algorithms um why is this so this is because here we need to be concerned about or we need to distinguish between tuples that belong to r and tuples that belong to s in duplicate elimination as well as in union all that we were concerned about is eliminating duplicates that is whenever there are duplicate tuples you just return it once and then cancel out all others but here you have to return um tuples if and only if they they appear in both relation r and relation s therefore we need to have a mechanism of tagging each tuple as to where it belong does it belong to the relation r or does it belong to relation s so the simple strategy for set intersection is shown in this slide here given two relations r and s read them in terms of m blocks rather than and store the blocks on um on disk in sorted order that is read relations r and s um and sort them and store then back on disk now when reading it um ensure that the intermediate results of r are stored in a separate block set um or in a separate buffer pool than the relations of s so that we can easily distinguish between buffers that belong to r and buffers that belongs to s and and in order to compute the common elements between r and s we simply do the following we take each block of r that is from the block set and take the first block in each block set and um move them into memory and read the smallest tuple the the first tuple in this um in this means the smallest tuple in the block set that we have just read into memory and um and um and try to roll s that is try to roll relation s beyond this tuple that is beyond this first tuple if any um that is um if if this tuple t existed in s then there should be at least one block which gets rolled that is which gets left shifted if there is at least one left shift in s then it means that the the tuple t is common between r and s therefore it can be written to the buffer pool or or to the output pool if no such um left shift happens in s then the the tuple t is not there in or is not present in s and therefore can be discarded and note that here we we are assuming that r and s are sets that is there are no duplicates in r and s for this algorithm if there are duplicates then what we have to do is we first have to eliminate duplicates from r before shifting s that is take the smallest tuple from r roll all other um block list in r until you eliminate um duplicates of this tuple and then start looking for for occurrences of this tuple in relation s  refer slide  38.27  the last algorithm that would be looking at in the sort um sorting strategy is the natural join function the natural join um um as you know is an equijoin that operates on two relations it it is a binary operator um and it is an equijoin on attributes having the same name and domain um that has that in in the two relations so assume that we have two relations r x comma y and s y comma z where x and y x y and z um attribute list and y is the common component between r and s and assume that we are computing natural join between r and s now a simple way of computing natural join is a um is shown in this algorithm here we first read blocks of r into memory that is um we first read r in terms of m block and sort them on on the y attribute now we can not sort them entire tuple contents we have sort them on the y attribute this because this is the y attribute is is the one that is common between r and s now sort them on the y attribute and store them back on to the disk and then secondly read um relation s block by that is m blocks by m blocks and again sort it on the y attribute and store them back on to disk now what is that we need to compute the natural join now for each um relation or each tuple in r that is stored on to disk we have to find all other tuples of s that are that are stored that are also stored onto disk that can be combined with this relation kept now um we we however encounter a small problem here what is a problem um there could be situations where a given tuple in r can be joined with every tuple in s okay um now if that is the case especially if we are talking about um outer joins where we where we can tolerate null values for y then we may have to take a tuple from r and join it with every tuple in s and we not have um of course we do not have enough memory to store every tuple of s in um into memory and then start outputting it so we have to we have to do something else now that is um there could be worst case conditions where even combining the intermediate results um can not be done in memory that is there is there isn t enough um ram to to perform the intermediate operations in order to tackle such issues what we do is we um we start with a second level of um second level of what may be termed as external merging what we do is for each value of y that appears at the top of the the first blocks of r that is read the first blocks from each block set of r into memory take the value of y that appears in the first block of r okay and for each value of i that appears in the first block of r note that here we can not remove duplicates we um we should have duplicates um in the result unless of course it is specified then there should be no duplicates okay so for each value of i that are at the at the top of the first block of r start rolling s that is um um start reading the first blocks of s into memory and start rolling them um and beyond um until they go beyond the value of y and take all these tuples and store them separately that is let us say you take tuple t one from r and a set of tuples um t s from s and put all of them into another file okay now once you have put all of them into another file um you just you just start combining um the tuple that you have found in r with each tuple that you have found in s okay so of course as you can um as you can imagine this is a pretty um this can be a pretty slow operation because there are several amounts of disk accesses or disk writes that are happening here the first set of disk writes are reading um or the first set of disk write happens when we write the intermediate sorted relations back onto disk the second set of disk um accesses happen when we read read each um of the blocks from the block sets of of r into memory and then for each tuple we have to read each set of blocks in in s and for all matching tuples we have to write them onto some other intermediate file where they can be combined note that we can not delete the tuples from s after we have done this operation because there could be another um tuple in r having the same value of y which can be which can also be joined with all these tuples in s that is for example if we are um joining let us say employee and department and let us say um the join attribute is um  noise  is the department number that is um for each department number attribute in the employee relation find the department number attribute in the department relation and then join them together now there could be a case where two or more employees have are working in a same department and and have the same department number so when we have found the first employee with some department number let us say d and we are and um assume that a employee relation is r and and the department relation is s once we have found the first employee um who works in a given department d in s um we take the tuple d is s and write them onto disks so that they can be combined however this tuple can not be deleted from s because there could be another employee who is also working in the same department and which whose tuple also has to be um combined with the tuple of the department so so therefore um natural join using sort is is a is a slightly expensive technique  refer slide  45.35  the next set of algorithms that that we are going to see are the next paradigm of algorithms that we are going to see are the hash based algorithms as the name suggests um hash based algorithms use a hash table at their underlying um as their underlying data structure um and um use hashing as a overall paradigm or the overall strategy in which the algoithms are based the basic idea behind um hash based algorithms are shown in this slide here um we first given a relation r that is too big to fit in memory of course we read relation r block by block we don t even have to read it m blocks at a time if if it is possible to read it m blocks at a time fine it it makes it even more faster okay now for after we read relation r block by block take each tuple in in a block and hash it um move it through a hashing function so that it is hashed onto um onto a bucket in a hash file note that what is a um um remember what is a bucket in a hash file a bucket is a set of buffers is a chain of buffers that contain all tuples that are um that are hashed in the same value for a given key that is all tuples having the same key would be hashed onto the same bucket there could be tuples having different key being hashed on to the same bucket but um and which which could well be possible but what we can definitely say is that all tuples that have the same key will will be hashed onto the same bucket so all similar tuples should hash to the same bucket then we examine each bucket in isolation we don t have to work across buckets anymore in sort based techniques note that we had to work across the buffer list that were sorted buffer list that were generated in hash based technique we can examine each bucket in isolation of the other bucket in order to produce the final result  refer slide 47.41  so this slide is is a reminder of um what a hash file organization looks like given a tuple with a particular value key on which hashing is performed the key then maps onto to a set of buckets this one shows static hashing and of course there are also dynamic hashing techniques okay so um the hashing function hashes a key onto a bucket number the bucket number in turn points to a chain of blocks that form the bucket or where data or tuples that um that are that should lie in the bucket are stored let us look quickly at our usual algorithms for um for duplicate elimination and set theoretic operations like union intersection and and so on um using hash based techniques how can they be performed using hash based techniques let us first look at duplicate elimination using hash based techniques um this is quite simple has shown in the slide here um  refer slide  48.57  given a relation r which contains duplicate tuples and of course which is too big to fit in memory read the relation block by block or m blocks by m blocks and take each tuple in the blocks um that are read and hash it to um hash it to a set of buckets of a hash file and um let me reiterate the fact that um if there are duplicate tuples in r and important property that that we have to note here is that because we are using the same hash function if there are duplicate tuples they will necessarily be hashed on to the same bucket so um we have already cornered in a sense all duplicates into the same bucket and um and now our our job is much simpler that is visit each bucket and eliminate duplicates using either a one pass algorithm for duplicate elimination if the bucket can fit into memory remember how i did um one pass algorithm for for duplicate elimination that is we have to maintain a internal hash or a internal index structure in memory and then eliminate all duplicates as in when we read the relation so if the bucket can fit into memory you can use a internal data structure or in memory data structure to to eliminate duplicates or if the bucket is too big to fit in memory then we can um consider this bucket as as as a relation um as small relation and use um our previous algorithm the two pass sort based algorithm in order for removing the duplicates in the buckets  refer slide  50.31  what about set theoretic operations union intersection set difference and so on um for set theoretic operations involving two relations r and s we maintain two separate hash files okay hash based file organization okay however these two hash files will use the same hash function um hash of k and the buckets are also labeled analogously that is bucket number zero in the first hash file corresponds or is analogous to bucket number zero in the second hash file and so on okay so um if tuple t appears in some bucket n in in the hash file of r then if it is present in s it should also appear in bucket number s of the of the hash file of s okay and in addition to that um the the standard property that all duplicate tuples are always hashed to the same bucket in a given hash file  refer slide  51.34  so using this it is quite simple to perform set theoretic operations using hash based techniques um for example if you have to um if you have to perform union all we have do is hash each tuple of r into r hash file and hash each tuple of s into s s hash file and take each corresponding buckets that is bucket zero of r and bucket zero of s and eliminate duplicates and then output the results similarly if you want to perform intersection we just examine corresponding buckets bucket zero of r and bucket zero of s that is bucket i of r and bucket i of s and then return only the common elements and the same thing for set difference so um it is that um that is quiet straight forward the last technique in hashing that we are going to see is what is called as the hash join algorithm  refer slide  52.29  this is perhaps the um most widely used technique for performing natural joins um because of it is simplicity and efficiency um  noise  natural join over let us compute natural join over relations r of x comma y and y s of y comma z where y is the um common component between r and s now all tuples now we are going to first read um r and s and use the y component as the key to perform the hashing rather than the entire tuple now all tuples having the same value of the y component should um should hash onto the same bucket right if we if we are using the same hash function okay so if the same hash function is used for r and s we just take the corresponding um corresponding buckets that is um just take bucket number zero of r s hash file and for all the tuples that you find if they can be combined or if they can be joined with any tuple then this tuple has to necessarily exist in bucket number zero of the s s hash file and so on so so given this property we can um it is enough for us to examine just the corresponding buckets in order to perform the join operation this makes the join operation extremely efficient because we don t have to perform too much of look ups when we are performing the um we don t have to too many look up s when we are performing the join such an algorithm is also called the partition hash join algorithm because the the pure hash join algorithm is is is a slightly different algorithm from this but this is um widely used algorithm for computing natural joins in many dbms systems so um let us summarize what we have um learned today um in the two pass um  noise  algorithms  refer slide  54.31  so um we um let us summarize query processing in general and and then go back to um the two pass algorithms that we have studied today um the we looked into the different stages of query execution to begin with and then we saw that there are two stages um namely that of the the logical query execution plan and the physical query query execution plan that are that are of um that are of that are especially of interest and then we saw what are some of the um language constructs that make up the physical query execution plans and then we looked into algorithms that can be built to perform this internal queries or physical um to answer this physical queries using this physical query execution plan construct and these algorithms could be either single pass algorithms if the um which can be used if the relations are small enough or if the query is tuple at a time query and there are multi pass algorithms which can be used if the the relations are big among multi pass algorithms are are rather in two pass algorithms we looked at sort based algorithms or sorting based strategies for performing several of these operations and hash based strategies for performing several of these operations we have not looked at index base strategies um which is quite analogous to sorting and hashing base strategies we shall briefly um visit index base strategies in the next session before we take up the the logical query execution plans and ways by which a queries can be optimized so that brings us to the end of this session thank you lecture # 16 query processing and optimization iii hello and welcome in today s session we shall be continuing with whatever we have being exploring in past two sessions namely query processing and optimization issues as we have seen query processing is a very crucial element in dbms design that is um this is not about database design like i had mentioned in previous session that is database design is the term is used for used to denote um activities like schema design normalization and and so on that is how to design a data database such that a dbms can be used to handle handle this um the data in the database efficiently as possible on the other hand query processing issues concern design issues of the dbms itself that is how can we built a dbms that can  noise  efficiently process a given user query even if in many cases even if the query is not formulated in a form that is the slightly to be the most efficient can the database or can the dbms detect it and rewrite the query in such a way that that the query becomes much more efficient and we saw that a query processing is um is so crucial that it can make the difference between  noise  usability and un-usability of the dbms let us briefly have an overview of um the different topics that we have studied in query processing before we move on to today s topic that is of query optimization  refer slide  02.47  a typical query processing um a typical process of query processing takes several different steps when the user gives an sql query it is um first passed through a scanning and parsing phase where um where the query is first scanned so that the the query becomes um or query is divided into stream of tokens and these sets of tokens are then parsed to build a parse tree or a query parse tree that gives the syntactic structure formulated query that is given by the user now from this parse tree a logical query plan is generated that is the parse tree is um rewritten based on certain rules heuristic rules and um several different rewritings of this parse tree or possible and one of them is chosen based on some  noise  based on some criteria like the cost estimation for this particular query tree and so on so using this the logical query plan is generated the logical query plan of course is is re written and we optimize the the dbms optimizes the query automatically to to certain extent based on rewriting the query then from this the physical query plan is generated the physical query plan is um is a plan written in an intermediate language that is either interpreted executed directly by dbms or is complied into machine code and in the last two sessions we saw what would be the or what are the typical building blocks of this physical query plan language there are the physical plan of course should support all kinds of logical query operations like select project and so on in addition it should also support some physical aspects of query processing like like how to iterate through different tuples in a given relation um or things like sort scan and index scan and so on how do we um how do we retrieve based on a particular index and so on we also saw some algorithms that implement the internal query that is if you remember the internal query are um are formulated um or or all those queries that are formulated by the physical query plan language that is it is the query that the dbms uses to access data from the file system the external query is the query that the users use or the application program uses to um to access data using the dbms we saw different kinds of of access algorithm based on the physical query plan um constructs we saw different constructs um we basically  noise  divided this algorithms into one parse algorithms and multi parse algorithms one pass algorithms are those algorithms which um which make um atmost or exactly one parse over the required relation but one parse algorithms have a limitation  refer slide  06.10  in the sense that if we are using a one parse algorithm for a relation at a time operator what is a relation at a time operator an operator that requires the entire relation to be present in order to answer the question that is being posed by the query for example operators like um removing duplicate that is the unique operator in in sql or order by or group by in in sql and so on so all this required the entire relation to be available before the query is answered unless the entire relation is processed the the first um even the first tuple of of answer can not be returned even before answering the first or outputting the first tuple of the result the entire relation must be processed at least once in single parse algorithms um we can apply single parse algorithms only if or if we use let us say tuple at a time queries where we need to be concerned only with one tuple at a time rather than the entire relation at a time or we can use it for relation at a time queries as long as atleast one of the relations can be fit into memory completely that is we can it not only fit into memory there should be more memory space left over for at least one block of the other relation if we are using any binary operator in the previous session we saw what are what what are called as two parse algorithms and we also said that multi pass algorithms are basically generalizations of the two pass algorithms two pass algorithms are used when um  noise  of course they they are primarily used in relation at a time query is because tuple at a time query do not need two pass algorithms we can we can use a one pass algorithm so a two pass algorithms are used for relation at a time queries where the relation size is too big to be able to fit into memory and we saw the basic structure of a two pass algorithm the basic structure of a two pass algorithm the um as as an alternating um computation and intermediate storage handling phases that is um take one set of block from relation perform some computation on them like sorting hashing or indexing which we are going to see today and then write them back into disk and then read back intermediate results before producing the final result so we divide two pass algorithms into three different strategies or three different paradigms so to say what are called sorting based methods which we saw in the previous session where the computation that is done when a chunk of blocks is read from um is read from a relation is the sorting function that is a chunk of blocks is read from each relation and they are sorted and they are placed on to disk the next kind of algorithms that we saw about the hash based methods where a read a chunk of blocks from from the relation and start hashing each tuples based on certain criteria if it is a natural join for example we hash it on the um common attribute or if it is the group by or if it is a unique function then we hash based on the entire tuple and so on today we are going to look at the last method or we are going to see just an overview of the last method namely the index based methods that is where the computation that is used um is um an indexing function that is adding or searching on a index and for our purposes we are going to assume a sorted index like a b plus tree a sorted index is is something like where we can use the index structure not only to retrieve um retrieve a tuple based on a key value but we can also retrieve tuples based on a sorted order based on a sorted order of all the key values that is present in the relation  refer slide  10.24  so um what are index based algorithms index based algorithms um can be contrasted from sorting and hash based algorithms in the sense that the user index instead of sorting and hashing for the computation phase and their useful when tuples are to be extracted on attributes that are been indexed um and they are especially useful for selection functions especially of course the um that is when the attributes that are being searched for index and they are fairly effective for joins and other binary operation as you will see there is another join function called zig zag join which is also quit popular as as hash join and in terms of efficiently computing joins between two or more between two relation  refer slide  11.19  let us look at the first um algorithm in um index based method how do we perform select or how do we perform a select operation based on using an index um we are of course assuming here that the select operation involves a condition that that is over an attribute that has been indexed of course if the the select is over a condition which is not been indexed then we can not use index base selection we should either use one of the other methods that we have seen now it is fairly straight forward if the selection condition is an equality condition on an attribute so that is what indexes are meant for let us say searching a b tree based on the value of a key is simple saying that select key equal to this value from the relation which is on which the b tree is maintained um so um the simple algorithm is to search for the index the required set of tuple or tuples in case of inequality conditions um that is something like a less than or equal to ten we have to we have to retrieve a sub tree from the b plus tree that is we can not um we can not retrieve just one node of the index and we are also making another assumption here the equality and inequality condition involves the key under constant and not another attribute that is we are saying a equal to ten or a less than or equal to ten we are not saying something like a equal to b where b is another attribute which is which is not been indexed but um we have to search for all um tuples in such a case where the the index indexing indexed attribute is equal to some other attribute that is not been indexed  refer slide  13.15  we shall not be covering all the other algorithms say say set theory algorithms um for index based methods once after we have seen the general pattern of set base operators like union intersection and set difference are performed using sorting and hashing it is fairly straight forward to design algorithms using index based methods also let us look at the last um query that we looked at at the other algorithms namely joins or natural joins how do we perform natural joins using index based algorithms now consider a natural join as usual between two relations r x comma y and um s y comma z where y is a set of attributes that are common between r and s that is y is the attribute over which the natural join is going to be performed now assume that um suppose that is um suppose y is not only um indexed that is the um y attribute not only indexed it is also the primary key in s okay assume that um s has as index over it over its y component okay if it is a primary key then it becomes more simpler but even if um it is not it is just an indexing attribute or set of attributes we can use the following algorithm we just start reading r um that is the relation um in this chunk of blocks that is um chunk of m blocks so read r block by block and for each tuple that we are read from r extract its y component and search relation s based on y component that is we start sequentially reading one of the relation and for each block that we read we do a index search and because index search is much faster we find the set of all relations let us say if it is a primary key then we we find exactly one relation and not a primary key we have to use some kind of clustering index and we find a set of tuples um a set of tuples that that that that may contain this value of y that we are looking for now all that we have to do is join this tuple with how many ever tuples we have found in s and we have computed the natural join over r and s so if a corresponding tuple is found then perform the join and push this to the output buffer using index based methods we can also perform another kind of natural join which is which is called a zig zag join  refer slide  16.05  the zig zag join can be performed when there is a sorting that is um that is available for both r and s that is assume that we are performing a natural join over two relations r and s where r is x comma y and s is y comma z where y is the common set of attributes between r and s and now also suppose that not only that y is indexed over s it is also indexed over r and this index is a sorting index that is um it is a something like a b plus tree where i can access all keys available in the relation in a sorted form now all that we have to do here in um for for performing such a join is is shown in the set of three steps in this slide we just use or we just keep calling getnext function note that we can represent relation r and s as iterators so we just open both relations in in iterators we and let us say we use r and start calling getnext function on the iterator or iterators now this get next function get the next logical key or the next sorted key in this um in r and for each key it is found in um in r we start searching for corresponding tuples or matching tuples in s and if matching tuples or found perform the join and append the output to buffer such an algorithm is called zig zag join this is because even though logically we are going in a sorted form physically um it the control would actually going in a zig zag fashion over on on a storage disk that is we don t know or we are not sure that the sorted form we are accessing the keys is indeed the same form in which records or tuples are stored on disk so such a algorithm is called a zig zag join however it is it is quiet an efficient algorithm because we are using index on both set of attributes that is both are in r and s  refer slide  18.24  let us move on to the main topic of today that of query optimization until now we have been looking at the last step in a query execution process that of managing the physical query um plan query plan languages that is how do we um how do we perform internal query answering using physical query plan languages let us move on to the next or the upper level before this that is of logical query plans that to primarily that of parse trees that are generated after a query is scanned and parsed by the query complier we are going to see um weather or how how or weather we can re write a parse tree in in a sense that we can make the query um answer in a much more efficient fashion than it was formulated by the user or the application program so query optimization techniques that are independent of the semantics of the query or the semantics of the application is what we are going to be looking at here and these techniques are based on rewriting the parse tree representing a relational algebra expression of the query that is parse tree would actually convert a given sql query into a relational algebra and then represent in the form of expression tree which can be optimized  refer slide  19.54  and there are two kinds of optimization what are what might be termed as heuristics based optimization and cost based optimization a heuristics based optimization is essentially a set of thumb rules using which we um we make assertions that the resulting um that the resulting parse tree is a better or more efficient than the original parse tree however given a parse tree usually we might there might be more than one parse trees that could be generated based on which heuristics are applied and what order they are applied now  noise  among these different parse trees um we might have to choose one of the parse trees for a particular query instance and we can perform this chossing by assigning trying to estimate cost that each of the parse tree incurs so um su usually in practice a combination of heuristics and cost based optimization are used before a query execution plan is finally chosen for execution  refer slide  21.04  now we have been talking about parse trees and rewriting parse trees so so what are parse trees a parse trees has you might have um come across um especially in a in the context of compliers or syntactic structures that most programming languages that can be expressed in the form of a tree structure or tree data structure a tree data structure represents a hierarchical structure this is also called a syntax tree or a parse tree and execution of um a syntactic structure is usually um done by what is called as a post order traversal of the parse tree that is a post order traversal simply says that given a tree having a root or two or more sub trees or one or more sub trees execute the sub tree first and then execute the root and and the same rule recursively applies to all of the sub trees and  noise  we shall not be going into details of how to build a parse tree and a traversals and so on but we shall be concentrating on how to modify a parse tree that is in the context of relational algebra that is context of a queries that are expressed relational algebra  refer slide  22.20  this slide shows an example of a parse tree there is um the left hand side of the slide shows a small relational algebra query which says project department name from select salary greater than three lakhs from again manager join department where the join condition is manager dot d number equal to department dot d number that is it is a natural join between the manager and department relations and from this natural join we were selecting set of tuples um where the salary field is greater than three lakhs and then projecting this set projecting the department name that is we are looking um we are querying all departments who are which pay their managers a salary greater than three lakhs the  noise  the corresponding query can be represented in the form of a tree data structure that is shown in the right hand side of the slide as the tree shows the top most operation project becomes a root node of the tree now the project is a unary operations therefore it has one um one child which is the select query so um the select query becomes the child of the project query the child the select again is unary operations so which which is being performed on the join operator here so the join operator becomes a child of select um and the join operator is a binary operator where it has two children and these two children are its two arguments that is manager and department relations so logically as you can see um the queries expressed the hierarchical structure that is inherit in the query is made explicitly by using the parse tree once a parse tree is generated by a query complier there are several kinds of checks that are performed on the parse tree before it is optimized some of these checks are in this slide here  refer slide  24.31  syntactic check the um syntactic checks simply says that is the syntax of every operator correct for example we can not have two children on on a project operator project and select are unary operators and similarly we can not have a single child on a join operator it is a binary operator and so on and entity checks entity checks basically check weather every relation name that is specified in this query actually exists that is um is it um is it there on disk that is is there a relation called manager and department on disk and so on in some cases a relations may not a relation that is named in a query may not actually exist on disk but it could be derived that is it may not be a base table but it could be a derived view that is it could be a view that is there in the schema in such a case um the  noise  the the complier would expand the view remember a view is again another query so um and the contents of the view or not actually on the um database so the view is again expanded and the parse tree for the view is joined or is hooked to the parse tree of of the overall query at wherever the the view name appears so this is called view expansion and then there are attributes checks that is does every attribute name refer to valid attributes and then there are type checks that is does each attribute participating in an expression have a proper type that is we can not say something like salary less than cats and so on i mean it has to be um if salary is um numeric then the other attribute should also be numeric and so on so so there should be type um compatibility between attributes within a expression once these checks are performed parse trees are rewritten based on certain heuristics so um these heuristics are also called as rewrite rules and which specify several condition and corresponding actions that need to be performed when these conditions hold and a parse tree should be expanded to its maximum extent before rewriting that is for example view should be um view should be replaced by um the relevant parse trees  noise  when when the parse trees is being um before the parse tree can be optimized and some rewrite rules could be situation specific and we are not going to be looking at such rewrite rules here anyway and they work only if certain conditions hold on the data set that is the next certain assumption about the dataset um based on which the parse tree can be rewritten  refer slide  27.35  the first rewrite rule that we are going to see today is what is called as pushing selects the pushing selects basically says that um try to push a select operation as low down in a tree as possible without altering the symatics of the parse tree or without altering the correctness of the parse tree why is this important or why is this beneficial remember a select or the cardinality of a select operation is less than or equal to the cardinality of the input relation for the select that is a select basically removes certain tuples from um from a relation before returning um the output and um one thing to note here is that whenver we talk about relations always think of very relations that is mega bytes or probally giga bytes of tuples present in a relation and if a select operation um is such that it requires only a small fraction of the tuples there is a great amount of um optimization that is performed already that is huge number of data need not be handled after the select is over and we can start working with a much smaller dataset so select based optimizations are the most common optimizations that are used in query rewriting so the slide here shows an example of um of pushing selects um based optimization let us look back at the parse tree that we just generated previously that is a project department name from select salary greater than three lakhs from natural join between manager and department now here you can see that the select operator here that is salary greater than three thousand is being performed after all possible managers or join with all possible departments now if we are going to anyway look at a managers whose salary is greater than three lakhs we don t have to consider joining all possible manager tuple with all possible department tuples you might as well say that we are going to join tuples of of manager which where the salary field is greater than three lakh so that is what is is performed in this tree here  refer slide  30.06  the select operation is pushed down um in in the tree so that first we select the set of all manager tuples where salary is greater than three thousand and then join this with um a greater than three lakhs and join this with the corresponding tuples of the department relation and then project just the department um so if um if let to say if there are about thousand managers in a company and only about fifty mangers have a salary greater than three lakhs then um instead of joining or instead of um looking at um  noise  thousand different manager tuples we need to content ourselves with only fifty manager tuples  refer slide  30.58  the second form of um rewriting rules is also related to pushing selects and which is called the cascading select or conjunctive selects that is um a select which as a conjunction can be split and cascaded into several different select operations and which can progressively start reducing the certs space as we go along this slide shows a small example which illustrates this point suppose we have a select operation that says select c and d over r that is select where c and d here are are some logical condition now we are selecting those sets of tuples from r where both condition c and condition d holds um if this is just the relation here now this could be um the conjunction should be could be even more that is it could be c and d and e and f and so on there could be many more and conditions here now if we leave this as it is then the entire set of relations are searched for um and for this condition that is the the entire set of um conditions are matched on the entire set of relations on the other hand we can see that the right hand side of the relation or the right hand side of this equivalence condition says that um select c from select d from r that is it is a cascading select that is shown in the slide here where instead of using c and d over r we say we first say select d over r that is um this is um this could be much more efficient if d is an indexed attribute over r so if d is an indexed attribute over r or rather d is an condition over an indexed attribute over r we can quickly or efficiently retrieve all those tuples that match the condition d from this um from this much smaller set or hopefully much smaller set of tuples that have been extracted we then perform a select c um we then look for the um condition c so the search space c is much smaller than the search space of d and if d is a condition over an indexed attribute um the the search space of d or or the inner select can also be very fast that is um because we are reterving based on index searches  refer slide  33.41  um there is an exception however to the pushing select um thumb rule that is push selects has far down a relation as possible thumb rule and this exception occurs whenever there are views that are there in a relation when a view is expanded it might be necessary sometimes that the selects are actually pushed up beyond the view before they are um they can be pushed down let us look at an example to illustrate this this slide shows an example having two relations movie and starsin movie is a relation that has the following attributes the title of the movie the year of the release of the movie the director of the movie and the language in which the movie is made and then stars in um is an attribute that says which film stars um acted in which movie okay it says it um it has an attribute title which is the title of the movie the year which is the year in which the movie was released the name of the flim actor and the language of the movie and now of course we can see that among the two relations title year and language are common that is we can perform a natural join using the three different attributes and now we create view called bengalimovies okay where which just says select star from movie where language equal to bengali that is we are we are interested only in those tuples of the movie where language equal to bengali now let us see if um we give a particular query and what happens  refer slide  35.27  let us say we have a query which says which filmstar worked under which director in bengali movies that is we have to pair each film star with director as long as language of the movie is bengali okay so um so how do we go about are the how do we express this query this is quite simple from an sql point of view that is we just say select starname and director from bengalimovies natural join starsin that is we we join we perform a natural join between bengalimovies and starsin note that bengali movies also have the same structure as the relation called movies that is you can join based on title year and language attributes okay so the corresponding parse tree for this is also shown in the slide here that is we are projecting starname and director from a natural join between bengali movies and starsin however this this relation bengali movies is a view so we have also expanded the view here the view for bengalimovies is basically select language equal to bengali from movie okay so this view is expanded and starsin is a base tuple which is kept as it is in the parse tree now if you see here we are even though the left hand side of this um tree um that is the parse tree that that um that talks about bengali movies contains tuples where the language is field is bengali okay it does not contain tuples having any other language field however for while performing the natural join we are still considering all tuples in starsin even if um it does not contain the the language called bengali that is we take a particular actor and we look at the set of all his or her movies regardless of weather the the language is bengali or not this is clearly wasteful because after all we are only interested in which star worked under which director in bengali movies okay so one way to  noise  optimize this is to put a select operator here that is we need to select those tuples from starsin where language equal to bengali okay therefore this this select operation here which which is in a view or which defines the view should first be taken up before it is brought down this is what shown in the um next slide here  refer slide  38.08  that is all tuples of starsin are selected even if they are joined with tuples having language equal to bengali so um so here we we just select all tuples which is not really necessary therefore we need to take this select tuple that is the left hand side of the relation selects a particular um or performs a particular selection condition which is also applicable to the right hand side of the this sub tree okay so we move up we move this selection relation up as much as possible before moving it down and to bring it here that is um so this would be the rewritten parse tree that is there are two separate select statements one for movie and one for starsin which finally forms the query  refer slide  38.58   noise  some more thumb rules involving selects some times in a join um function we perform a join blindly and then select um a set of tuples from this joined set of tuples based on um a condition that applies to only one of the relations we have actually seen such a example earlier but let us revisit again in order to make this bring out this rule we we didn t we didn t apply this rule however because we applied some other rule okay now consider the following query over the movies database again the the same movies database comprising of movie starsin and bengali movie relations now the query is  noise  we are only looking for um which actors or which stars acted under the director satyajit ray in bengali movies okay we are not just pairing up all sets of actors with all sets of directors under bengali movies we are just looking for who all acted under the direction of satyajit ray in bengali movies so the sql statement for this again is quite simple we just say select starname from bengalimovies natural join starsin where select director equal to satyajit ray okay  refer slide  40.30  the corresponding parse tree if you build the corresponding parse tree this would look like this um at at the lowest level is the join operator between bengalimovies which is expanded here that is shown in yellow and and the starsin relation now after this after these two relation are join we select for director equal to satyajit ray and then project starname and director name of course we need we need to only star name there is no need to project director okay so um what we are doing here is that um we are taking we are joining two relations okay even if let us say that um even if we move this select up and bring it here we are still joining two relations based on just the factor that language equal to bengali however we can note that um in the second um relation here we are finally interested here we are finally interested in those tuples where the director field equal to satyajit ray we don t we are not interested in any other directors um who whose records are available in this relation here so we can as well move this director equal to satyajit ray select below the join here that is we have we have which is above the join here is now below the join function and of course in addition to this we can also perform the previous um optimization which is not shown here which is move this language equal to bengali up and bring it down so that it comes before starsin okay so there are two different optimizations that are possible in this particular query  refer slide  42.17  the next kind of thumb rule we are um going to be that we going to consider is what is called as inserting projects now um note that if the um if the output of a query involves projection of just a few attributes over a large relation containing let us say tens of attributes or different or tens or even hundreds of attributes um a large tuple containing many different attributes there is no need to work with so many tuples or so many attributes when all we require is just what is um what is given by the overall projects and the conditions that um of of select operations that is that are given in the query have a look at the examples shown in the slide here the left hand side of the slide shows a query tree which says going back to our manager and department project department name from um that is select salary greater than three lakhs between from natural join between manager and department that is this is also optimized that is um the select operator which is here is now brought here so that it is optimized okay now if we notice the output of this tuple is just the department name um the output of this query is just the department name the um user or the application program is not concerned about any other field of um of the database that is um anything like the department id or the number of people in the department the location or whatever else that is there in the department relation that is not um really important however we can not just throw away all other attributes other than whatever is requested because these attributes may be required for some conditions which are the other attributes that are required for some conditions here manager that is department dot dnumber equal to manager dot dnumber is another condition that is required for the natural join that is shown here okay so um um the um only set of attributes from department that we are concerned about here are the department name and dnumber that is the department number similarly the only set of attributes that we are concerned about for um the manager relation is just the salary and the dept department number we are not concerned about the name of the manager the age of the manager the date of birth employee number the address nothing okay so um what we can do is we might as well insert extra projects down in the query tree so that um the output um relation becomes smaller and more focused to what is required for as part of this query that is we have inserted a project dnumber and salary before manager because those are all the fields that we require above and similarly inserted a project dnumber and department name because those are all the attributes that are required for these query from the department relation  refer slide  45.42  so that was the brief overview of some of the thumb rules that can be used to rewrite um query parse trees in order to make them work better let us look at the other aspect of um query optimization that of cost based optimization a cost based optimization is essentially used um to choose between one or more or choose between two or more different candidate parse trees um given a query parse tree if i have used the thumb rules and generated several other candidate parse trees each of which claim to be more efficient which is the one that i have to use in order to do that we assign a cost for each of the different components or or a cost estimate for each of the different components of a parse tree and the overall cost of of a parse tree is a is a algebraic expression over each of these cost estimates it is either a sum of all these costs um cost components or multiplication whatever depending on what exactly is the tree now what are the factors that affect execution cost this slide shows some possible candidates that affect execution cost say access cost is second storage how costly it is to access data from disk or any other storage medium that we are using then what is the storage cost how much storage do we need especially for storing the intermediate files and what is the computation cost how much of processor time that that we require and so on what is the memory usage cost how much memory primary memory or ram does it take and how what is a communication cost especially between the dbms server and the client if they are situated on different machines and communication cost becomes very profound if we are considering distributed database that is how many communication um sequences are required between the different dbms servers that that form this databases so all of these factors affect the overall query execution cost  refer slide  47.49  a database catalog is a set of um value estimates or some kind of meta data or meta information that that are stored in a dbms that are useful for cost estimation and catalogs are metadata that that could be either say table specific metadata like an estimate of a number of tupels in table the size of the table and the number of blocks that are occupied by table and so on and they could be field specific like the number of distinct values of a particular field a an estimate of those number of different distinct values of a field and so on are they could be database wide tools are they could be index specific information and so on s let us look at some typical kinds of information that is stored in a catalog and see how we can estimate the cost of different operator  refer slide  48.57  some typical information that are stored or um depicted here let us say b of r is the the notation used for the number of blocks that are taken up by a relation r similarly t of r the number of tuples that exist in a relation r and similarly v of r comma a is a field specific attribute that is it is a estimate of the number of distincts values that attributes a has in relation r and um of course for example if if attribute a is the gender then the number of distinct values is only two on the other hand other hand if if attribute a is the is something like distance between something and something else okay it could take on a range of several other values and of course v of r comma a one till a n is the set of is the number of distinct values of the combined tuple or or that is formed by a one to a n  refer slide  49.57  let us look at some simple cost estimation techniques and there are many more cost estimation techniques other than this but we are only looking at um examples that are um that could that should the bigger picture behind this the representative example okay so suppose we have an equality selection that is select um s equal to c from r where a is an attribute name and c is a constant okay now a is an attribute name and c is a given constant and um we know the  noise  we we have an estimate of the number of distinct values that that the attribute can take which is given by v of r comma a then the probability of a equal to c is simply one over v of r comma a and because the number of tuples is t of r the slide here shows the estimate that is t of a is equal to t of r divided by v of r comma a which which which is a estimate for the number of tuples that that are there in s and this is a good estimate if all values of a have fairly uniform probability or that is in the selection query if the if the dataset is cude then it may not really be good enough estimate  refer slide  51.27  consider the inequality condition that is select a not equal to c from r which is assigned to s and c is again a constant now this is again um quite simple because suppose the number of distinct values of a is given by v of r comma a then the probability that um a is not equal to c is simply v of r comma a minus one divided by v of r comma a that is one over one minus one over v of r comma a multiply this with t of r which gives us a estimate of the number of tuples that that could be in s what about um what about a composite condition something like select c or d that is condition c or condition d over r and assign it to s okay now let us um first um suppose we have estimated that um p tuples in the relation satisfy condition d condition c and q tuples satisfy condition d and there are n number of tuples in the relation now the probability that a given tuple will match c ord is shown here that is one minus um the the multiplication of one minus p over n and one minus q over n um that is this is the probability of um a tuple not satisfying c and this is the probability of tuple not satisfying d and the multiplication of this is the probability of tuple not satisfying both c and d and one over this or one minus this is the complement of this um this is the typically the demorgans law of um which says the which gives us the probability of tuples that satisfy c or d now multiply this with n um or the number of tuples which gives us the size estimate of the the query  refer slide  53.29  the last um estimate that we are going to look at is trying to estimate the size of a natural join consider a natural join between r of x comma y and s of y comma z okay initially for simplicity let us assume that y is a single attribute all though x and z could be composite attributes that is their sets okay now um if also let us assume that um v of r comma y that is the number of distinct values of y that are in r is less than the number of distinct values of y y that are in s now each tuple in um r can be combined with with corresponding tuple in s probability of one over v s comma r because that is the number of values that um that is s comma y rather than there is a small bug here v of one over s comma y now that is the probability of finding given value of y okay now since there are t of s tuples combining with t of r tuples the the size estimate of the total of this join would be t of r times t of s divided by v of s comma y here okay now this is true if um if the smaller relation is r that is the v of r comma y y is less than v of s comma y  refer slide  55.04  if the opposite is true then v of r comma y appears in the denominator therefore we just take the maximum of v of r comma y and v of x comma y in the denominator and the numberator remains the same t of r times t of s which gives us the good enough estimate of the number of tuples in the natural join that is if y is a simole attribute if y is a composite attribute that is it contains of many different attributes then we have to consider the match of max of each corresponding attribute that is common between s and um r and s we shall not be looking into this in more detain here  refer slide  55.40  so that brings us to the end of this session and also the end of the the um the topic on query processing and optimization that we have studied we have in some sense um just scratch the surface of this vast and crucial aspect of dbms design especially a namely query processing and optimization so in this session we looked at index based algorithms for um for physical query plans and we also looked at different query optimization techniques based on pushing selects cascading selects and pulling selects out of views and inserting extra project operations we also looked at several kinds of cost estimation techniques which can be used in combination with heuristics and um of query relating in order to select the best or in order to select um what is considered to be the best query execution plan that brings us to the end of this session thank you lecture # 18 transaction processing and database manager in the previous lectures we have looked at the basic properties of transactions namely the acid properties atomicity concurrency isolation durability in today s lecture we are going to see how these properties will be realized by the transaction processing system within the database manager  refer slide  01.49  we will take a few simple examples and through this examples we will illustrate how the transaction processing system will ensure the acid properties of the transaction  refer slide  02.04  here is a very simple example shown in the slides here there are two transactions here which are shown t one and t two t one is a account transfer transaction transfer of money from money from one account to other account now thousand rupees had been transferred from account a to account b by transaction t one transaction two is a interest payment transaction so it is actually crediting into each account a five percent interest into each of the accounts now what is shown here is t one and t two operating simultaneously on the banking system now what will do is we will try to understand these two transactions in terms of various operations performed by these transactions what i am going to do here is i will write t one as performing several transactions or several operations now the first operation that is performed by t one is to actually take the account a and read the value of the balance that is there in the account so it is basically a read operation of the account the second operation is essentially to add thousand rupees into this account and the third operation is going to be writing the value back the same thing is going to be done for the account b so i will actually show that as the fourth operation but the subsequent operations will not be shown here they they are self explanatory one can understand after that the other operations now suffix here shows that this is the first operation and this gives the transaction id one is the transaction id and the one here indicates that it is the first operation so operation one one indicates that this is the first instruction of transaction one this indicates that it is the second instruction of transaction one like that it is shown here now if you basically take a operation o i j it indicates that this is j th operation of i the transaction this is the notation that we will be using okay as you can see here this is basically a read operation on a and this is basically a write operation on a okay so we have between the processing we have the reads and the writes happening on the data items now we can also understand the transaction t two also has shown in the slide as trying to do the following operations o two one is going to be a read of a and then the o two two operation is going to be an update on the value of the data item and then o two o two three is going to be an item again a the rest of the operations as shown in the earlier case for the b operation b okay  refer slide  06.17  now what we are going to show you is what happens when these transactions are executed simultaneously on the database now a list of actions form a set of transactions as seen by the dbms as you can see here o one one o one two o one three o one four are set of transaction set of operation constituting t one similarly we have o two one o two two o three three constituting the set of instructions constituting transaction t two  refer slide  06.52  now it is possible for these operations to get interleaved in the sense that possible for this transactions operations of the transactions to execute in the interleaved fashion now when this get executed in the interleaved fashion we basically call that as schedules okay a schedule is nothing but a series of operations as executed by the database management system now you can see here it is possible for these two transactions to execute concurrently as a interleavings as shown as the operation t one are constituting a equals to a plus hundred is executed here then t two is executed which is equivalent to say a equals to this operation is executed here followed by the t two of b and t one again b equals to b minus this is one schedule which is called a possible schedule we can call this is as sc one as one possible schedule as you can see here these two constitute transaction t one these two operations these two operations constitute t two  refer slide  08.29  now there is another possible schedule also shown in the slide there in which case t one and t two are executed as shown here but the other two operations are interchanged this is executed before the other operation now we actually have the schedules one of the important criterion for this schedules to be valid is to see that these schedules produce proper consistent results at the end of the execution  refer slide  09.07  now this schedule one can be seen as shown in the slide as read and the writes on the various data items t one is basically is reading a and then writing a it is reading the old value of the bank account balance of account a and writing the new value for the balance here t two is also doing a read of a and write of a because you are computing the new value of a by calculating the interest that is payable for this account similar way you can also write for write b and then read b and write b and read b and then again write b okay what we are going to show you through this example is what happens when these reads and writes are interleaved from the consistency point of view now we can see here the notion of what is corrupt from the execution point of view is shown here suddenly we don t want transactions to execute one after the other because then the throughput of the system will come down drastically you want as many operations as possible should be executed in a concurrent fashion to actually increase the throughput of the system now when concurrently executed transactions at the end of it whether they can be translated into the voice called a serial execution  refer slide  10.51  now in this particular case what you would like to say is t one executed completely after t two this is one possibility or t two executed after t one as long as this is possible for you say we call this kind of schedule as a serial schedule and that is what is actually shown in the diagram here a schedule is which is equivalent to one of the serial schedules is equivalent to saying that the t one executed before t two or t two executed before t one that is one of these should be possible now to see what really happens when the execution is not serial that means when finally you are not able to detect saying that the execution of the transactions is not as per the serial schedule  refer slide  11.51  i will take a very simple example and show how exactly will reduce the serial schedules let us take a case of a transaction t one with operation x okay and then one two which is actually a write of y now we take another transaction t two where basically it is a read of a y and then a write of x okay now no matter how these operations are executed as long as it possible for you to say that all the operations at t one have been executed before t two which is equivalent to saying that if there is a schedule that says o one one x o one two y has actually finish before o two one y and o two x are executed by the database manger this is equivalent to saying that t one finished before t two that is what actually we mean by a serial execution of the transactions  refer slide  13.26  that is t one finished execution before transaction t two started executing now it is also possible for you two also have the reverse order where all the operations of t two have been executed before t one this is a very simple and straight forward case where we can easily save on all the operations of t one and t two are executed in this particular fashion very easily to see that t one is actually finished all the operations before t two stated execution the only case where you will have problems is when some operations of t one have been executed in such a way that they are interleaved with the execution of the operations of t two  refer slide  14.12  and the problem of deciding whether the schedule is equivalent to a serial schedule that all the operations of one transactions finish before the other becomes a important requirement and that is what actually we are going to look at how that can be done in this particular case what we are going to say is all that that will be required for us as a criterion whereas schedule is produced is two operations are said to conflict we say the notion of a conflicting operation is when one of them is a write operation okay one of the operation is a write operation  refer slide  15.15  now to give you the little more simplistic view let us say t one as trying to read a data item x and t two is actually write the data item x these two operations o one of some i o two of some j are said to conflicting because they are operating on the same data item and they are conflicting with each other right here as you can see here t one is reading the data item x t two is writing the same data item x since both transactions are reading the same data item and one of these operations is a write operation we say that these two operations are called conflicting the whenever we have conflicting operations like this the first inference is here o one i and o two j are conflicting now in transactions if there is conflicting operations and there is a way this conflicting operations have been executed let us say the conflicting operation in this particular case is executed is such a way that this is the order in a schedule now this order actually determines that t one actually preceded t two because it is conflicting on data item x and t one has been executed before t two  refer slide  17.02  and now this order should be preserved no matter what happens with respect to other operations and as long as you preserve that order with respect to all other operations we say that the operations have been executed in a serial fashion or the schedule is reducable to a serial schedule this is the concept of serializibilty now this is the important notion here is when transactions are executing concurrently we need to ensure that the conflicting operations are serializable all the conflicting operations are serializable now here is a very simple case shown in the slide where it shows that where it is not possible to serialize we need to actually abort the transaction now in this particular case it is shown that t one actually is reading as we go to the earlier case  refer slide  18.00  t one is actually reading the data item a and then and writing data item a now if you look at the t two is also reading data item a and then writing data item a now the other part of t two is read b and write b now as you can see here with respect to data item a the order between t one and t two is t one is before t two now if you look at the data item b it is coming in the revere direction which actually means that ad you can see here n the data item b as far as the conflicting data item a is concerned t one is before t two as far as b is concerned it is t two before t one as you can see this is on data item b this is on data item a okay  refer slide  19.18  now from this it is not possible for us to say whether t one actually is finished before t two or t two is finished before t one since we cant no decipher which one has actually finishing before the other this schedule is not serializable schedule and this is what should be avoided a non serialzable schedule shows that the execution of the operations we lead to inconsistency the database will be in a inconsistent state when we have the operations executed in a non serlizable way now in this particular case it is shown here that t two can commit but t one has to abort that s what was shown in the slide saying that only b can um transaction t two can commit but t one has to abort because t one trying to commit here will produce a non serial serialzable schedule this is also shown as if you go back to the slide you can see that this is shown as dirty reads that is write and read conflicts as you can see b has been t one has read a and b values but it is reading the writing the value at a much later stage and now the one of the later updates will be lost if you allow t one and t two to execute in this particular way one of the operations will be lost and that s the reason why the schedule is not allowed okay we look at other kinds of conflicts that can arise when transactions are executing first kind of conflict that we saw is a write read conflict  refer slide  21.08  we can also have a read write conflict and a write write conflict what a read write conflict shows is the example shown here is transaction t one is reading the data item a and if its value is more than zero then it decrements two is actually reading a and it is decrementing the value of a now when t one and t two are operating concurrently t one is actually reading data item whereas t two is actually writing on to the data item this is what we mean by a read write conflict on the data item a with respect to data item a both transaction t one and t two are conflicting in terms of t one reading the data item and t two modifying the data item and this is what we mean by a read write conflict again you can see that in this particular case the value read by t one if it is before the value is changed will be inconsistent let us say right the value of a is zero and t one reads it as zero then it is unlikely to decrement that because it has read the value as zero now if t two also read the value at the same time as zero at decrement as reads it as zero then it decrements it as minus one so that value is actually in conflict actual value that is read by t one and t two are not correct another example for conflict is the write write conflict where two transactions both of them access the data items and tries to modify the value of the data item this is a case where it is shown here as two people simultaneously trying to credit into the same account is shown as a example of a write write conflict as can be seen here t one is actually writing the value of um value into the account t is also writing a value in the account a as well as into b and t one at a later stage is trying to write the value into b now as you can see in this particular case the writes on the same data item will be conflicting leading to non serializable schedule so typically this is what we mean by a write write conflict to summarize what we actually seeing i will give a simple example of an operation and show how exactly is the conflicts serializability is to be achieved if you take a transaction t i and say t i as a operation o ip and data item x and you have transaction t j which as an operation q on x now we say that these operations are conflicting if one them is right now when you say this is right and this is a read you have between o ip x and o j q basically have a read write conflict the x data item is read by transaction t i and it is been modified by t j so this is basically a read write conflict now if you say that this is a write write typically what you see is between operation o i p x and o j q x you see a write write conflict  refer slide  25.01  now when there is a conflict between the two operations whether it is a read write or a write write conflict you need to actually serilize the operations by actually saying that they are executed in a serial order which is what we mean by conflict serializablitiy now whenever there is a conflicting operations we need to actually serilize the two operations which is known as the conflict serializability in this particular case o i p x and o j q x are the conflicting operations and they need to be serialize in a particular fashion and this is what we mean by conflict serializability we are going to see in later lectures how the transactions are executed by the transaction manager to ensure that conflicting operations are serialized or serial schedules are produced by transaction manager one of the simple technique that is used is what we see as a two phase locking and we will study that two phase locking technique for achieving conflict serailizability later in our lectures now here is actually what is shown as how exactly transaction manager achieves some of the properties that we have been discussing  refer slide  26.52  dbms ensure that a transaction either completes and its results are permanently written this is what we mean by committing a transaction or no effect at all on the database this is equivalent to saying that the transaction has been aborted so we have two states for the transaction either a commit state or an abort state in the case of commit state all the operations of the transaction or executed in full and then committed in case of abort no effect at all on the database as far as that transaction is concerned now the idea of transaction manager is it controls the execution of the transactions as we saw in this particular case it controls the execution of the transactions in such a way that the operations of the transactions are serilizable similarly if you take the recovery manager recovery manager is responsible for undoing the actions of transaction which do not commit this is a equivalent to saying that the recovery manger is responsible for ensuring the property of atomicity all actions of the committed transactions survive any kind of hardware or software failures this is actually known as writing the committed transactions on to a stable storage what we are going to do is we are going to look at little further into how the recovery manager ensures that properties of atomicity and durability  refer slide  28.28  now what are issues involved in ensuring atomicity and durability the following errors can occur when a transaction is executing first is it could relate to logical errors for example you are trying to withdraw some money from a bank account it is possible that the account itself doesn t exist or the account doesn t has sufficient funds in all this cases transaction can not proceed any further this is what we mean by logical errors the transaction may have to abort because of logical errors there could be simple address for example it is possible that there are problems of network there are problems of system failures temporary failure of power failure in which case when the power comes back you need to know what really happened for your transaction with respect to already started transactions a simple example could be you go to a atm and try to withdraw money from the atm and the power when you actually press the button for withdrawing the money the atm stops functioning atm failed due to various reasons you would like to know whether their system is actually debited the amount from the bank from your balance or not that is basically system errors there could be crash system crashes there could be a hard disk failure the disk could have been corrupted so there could be various reasons why the system didt perform it could be a system crash the all these errors are possible when the system actually goes into any of these errors you want to understand how exactly the atomicity and durability properties can be maintained a simple example trying to illustrate this point will be something like a file which all of us open on a windows machine or any of our unix machines now here when you open a file in a editor mode and tries to edit your file there is no guarantee in terms of what happens when a power fails because the file could be in a very corrupted state there is no guarantee fot you in terms of state of the file which all of us know we keep related saving the file when we actually entering um some document we try actually saving the document as many times as possible so that when the power goes off or something else happens we still save the portion of the work we have actually done we don t lose the file because of power failure all the work that we have done now the same thing can not happen in the case of database systems because here the the more critical data that is been in saved in the file so we need to ensure that whatever happens when any of these failures happen the system is still in a predictable state that is the difference between ordinary file systems implemented by an operating system and a database implemented by commercial systems they ensure that the whenever these things happen still the properties the safe properties for the transaction the acid properties of the transactions are retained  refer slide  32.11  to explain how the these properties are are retained by the system we need to also understand the different storage types that are available in a computer system a simple volatile storage we basically look at a simple volatile storage this dose not survive system crashes that means when the system actually crashes the storage is lost the storage is volatile it is lost the minute the system crashes if you when you talk about non volatile storage the system actually survives these crashes that means there the storage is the the whatever you write into the storage is not lost when the system crash occurs a simple case is whatever is there in the main memory is lost when the power goes whereas if you have written it onto your hard disk it survives a power failure because it is written into a more non volatile storage now we also have a concept of as table storage which is an abstraction of maintaining replicated copies in multiple non volatile storage media so that whenever higher diasters occur we still have a way of getting our data back and that is we mean by the concept of a stable storage now what we are going to see is how this concepts are used for actually achieving the atomicity and durab durability properties in the transaction manager  refer slide  33.55  what we are going to show here in this particular case is what really happens when transactions have to roll back the rolling of the transaction has to happen mainly because of logical errors or the system crashes and hence it has to be restored back to a previous state transaction abort requires roll back means undoing all the action of that particular transaction now to ensure that roll back of the transaction occurs properly what we have to do is all the writes of the transaction have to be properly recorded in what is called as a log file the log file retains all the information relating to the writes of the transaction and this will be used when the roll back has to occur now if the system crashes all active transactions during the crash need to be aborted when the system comes back this is equivalent to saying that they they will all be rolled back and the information that is there in the system in the log file will be used to properly undo the transaction activities whatever the transactions are being doing what we are going to do is again in this particular case we will take a very simple example and see how exactly this happens  refer slide  35.22  now here is the case where case where the logs are maintained how this logs are helpful in ensuring the atomicity property or the how the atomicity property will be realized by the database manager what is shown here is a simple case of writing the logs before the transactions starts executing and making sure that it is carried over whenever the data item is being written or a new value for the data item is being written what you can see here is the first log that will be written as far as the transaction t one is concerned is what is called is the begin log transacting begin log now what we have is basically a relating to a transaction every transaction is preceded by the begin transaction as the key word now this begin transaction actually tells the database manager to write what we see as a log this is the transaction begin log okay now this is a the begin log has to be written on to the transaction now in between the transaction does various operations has we have seen there its possible that there are several operations which are done by the transaction in between and then we have and end transaction now this is actually the last instruction that is executed by this transaction so we have a what is called a commit log indicating that the transaction has actually committed which is equivalent to saying that all these operation have been successfully executed so between begin and end at any point of time when there is a crash we need to actually recover back to the starting point and this is what we means by actually roll back what we are calling as roll back is basically rolling back all the things that a transaction is done to the beginning this is what is meant as a roll back now what we do is we actually ensure that whatever the transaction is doing is written on to this log and this log will be used for rolling back the transaction whenever a crash occurs for example you can see that in the slide it is shown that t i writes a data item now as you can see the first one transaction starts there is a transaction log t i start now the second thing that you notice is t i writes a data item now there is a old value and a new value old value is the old value of the data item and the new value is the new value of the data item x so there is a log that is written there which shows that t i x old value and a new value is shown here this is how actually the log is written whenever there is a change in the data item we basically write the log and now this this log shows what was the old value and what is the new value now when you come to the last transaction basically you have a commit log that is a t i commit log so as shown here we have a begin log and a commit log and in between whatever is happening is being recorded there as shown in the slide there okay so one of the things we are going to look at now is how this logs can be used for recovery purposes how exactly this logs can be used by the database manager to ensure that whenever those kind of failures that we are talking earlier occurs  refer slide  39.54  how the system will recover back from those failures now one of the things when writing this logs is one of the things that one should remember is the id s of the transactions are stored so that we know to this logs pertain to which transactions so transaction id s are appended when the logs records to identify the transactions for which this have logs been produced logs are replicated and stored in a stable storage this is also very important because in the logs themselves are lost then there is no way you can recover back logs only assuming that the logs are written on to a stable storage you can ensure that the transaction can be made to recover but if the logs themselves are subjected to failure then you will not be able to recover back and hence logs are replicated one of the assumption we make it logs are replicated and they are stored in a stable storage so when we say a log is written we assume that the values relating to the log have been on a stable storage and it is possible for us to recover this information at any point of time  refer slide  41.10  now as you can see here we are also showing how this log entries will also be ordered okay ordering of the log entries we say a transaction t are can commit only if the log entry relating to that is same on a stable storage this is equivalent to saying as you can see here when you write this commit log this commit log is actually written on to a stable storage then we say the transaction is completed this is the um point where it is possible for the transaction now to say that it is committed now before this is actually written this log is written all the entries before this pertaining to to this transaction should have also been written onto this stable storage you should never the commit log before all the other log entries relating to this transaction have been saved on the stable storage  refer slide  42.23  now only after the writing the entries relating to the logs you should write the data items themselves after this point onto the stable storage this is very important know these steps are very important because if you perform them in any other order you will have problems in terms of recovering back first requirement is all the log entries relating to this transaction should have written onto the stable storage in the first instance before you are writing the commit log only after writing the commit log the data item values pertaining to the transaction can themselves can be written onto the stable storage the reason for this is simple if you if you don t write the log values first under the stable storage there is now way if something happens to recover from that particular failure for example if you have written the data value onto the stable storage now something happens there is now way of finding out what is the state in which the transaction is unless the logs are written properly so logs are the bases for the database manager to find out what is the state in which the transaction is when a failure is occurred and hence it is important for you to first write all the logs relating to the transaction then write the commit log and then write all the data items onto the stable storage this is how one needs to order or write the various things relating to the transaction  refer slide  44.09  take a very simple example a simple example in this case to see how the two transactions can really execute writing their logs in this particular case it is shown t one and t two t one is actually reading certain data items and writing certain data items similarly t two is also reading and writing certain data items for completeness sake we also have shown the4 initial values that are their in the database when this transactions t one and t two start executing okay as can be seen in the slide the initial values of a are a is hundred b is three hundred c is five d is sixty and e is eighty now t one when starts executing it is going to read the values of a will increment by fifty then read the value of b increment it by hundred then read the value of b back into the system then will read the value of c now c is value is inc is double then the value of c is again written at the end of it a is recomputed as a plus b plus c and the value of a is written if you look at the transaction t two t two is actually reading the value of a it is incrementing it by ten there it is actually reading the value of d it is decrementing by ten then actually reading the value of e and reading the value of b then e value is computed by adding e plus b and then writing the value of e then d is recomputed as d plus e them finally the value of e is written what we wish to covey with this is the two transactions are simultaneously reading several data items and trying to modify it is equivalent to saying that there is set of operations which are going simultaneously in terms of reads and writes between these two transactions  refer slide  46.21  now let us understand what really would have been the logs that would have been produced when this transaction t one and t two start executing now when t one starts executing you can see that there is an initial log given there as t one start this actually shows that t one started executing that is the start log for the transaction t one t one is the id of the transaction now when actually t one tried writing the value of a the old vale and the new value are actually stored in the database in the log as you can see in this particular case this log shows that a s old value is hundred and the new computed value is one fifty similarity when b has actually been recomputed we have a old value for b as three hundred and the new value has four hundred similarly for the value of c it is five and ten and finally when the last computation for a actually took place a is one fifty and then old value of a is one fifiy and then the new value is five sixty it is at this point of time the actually the t one finished executing all its instructions and it is ready for commit and that is the time the commit log is written t one commit log is produced at that point of time a similar thing is shown for t two as you can see there is a start log for t two and then we have various new values and old values for the data item that are done by t two are also shown here and finally commit log is shown for t two  refer slide  48.08  now what really happens is once a failure is occurred undo all transactions for which there is a start entry in the log but there is no corresponding commit entry this is equivalent to saying that i will just go back to the previous slide to show what really we are we are saying now we can say t one when you actually a system crashes occur you can see there is a start log for t one but corresponding commit log is not present then all that we have to do is you have to actually undo whatever has been done by the transaction for example in this particular case if t one has actually modified the values of a b c then those values have to be reset back to the old values from the new values that is equivalent to actually saying that we have undone the transaction because it has not reached the commit state now the other case is redo all transactions for for which there are both start and commit entries because these transactions have already gone to the finish stage we are going to redo the transaction for all those which commit a start and commit entries are there  refer slide  49.26  and also if typically we have if the data items are not yet written this is what we mean by actually looking at whether the not just the log entries but the data item entries are not still written then we need to redo but if the data item entries are also been stored on the stable storage there would have been along with commit log there would have been a complete log and the log book showing that the transaction has completed all the operations relating to it in which case the redo need not be done and this is typically achieved by what is called checkpoint record  refer slide  50.04  if the checkpoint record is entered into the log whenever the system writes the data item values onto the database the affect of the write operations are all committed on the transactions now all transactions whose commit log entry is before the check point entry need not be redone in case of a system crash so the checkpoint is a place where you can decide whether a commit log exists whether you have redo or need not redo those transactions this is how exactly the database manager will ensure the transactions are executed atomically and they satisfy the property of durability what we are going to see in next class is how the concurrency control properties of the transactions are um are realized by the transaction manager okay lecture # 19 foundation for concurrency control in the last class we have been looking at the basics of transaction processing systems um what we have done is two look at the basic properties of transactions when they execute in the database system we looked at the essential properties of transactions in database systems and we have been looking at the four properties that are essential for programs transactions  refer slide  1.56  atomicity consistency isolation and durability atomicity property is required to ensure that all the instruction in the transaction is either executed or none of them are executed so the property of atomicity ensures that all the instructions either get executed or none of them will be executed the property of consistency ensures that when more than one transaction operates on the database um the consistent state of the database is maintained that is they don t really malign the values written on to the database the property of isolation provides that the affects in one transaction or not visible to other transaction till it has committed all its values to the transaction the property of durability ensure that the transaction is returned value on the database it will be stored permanently on the database that s the property in durability we have been looking at the this four properties and trying to understand more on how the consistency property is realized in database systems what we are going to do in today s class is to look at the foundations for the consistency management which is also known as the concurrency control mechanisms in database systems what we are going to look at it is the basic concepts of transactions from the view of concurrency control and this is what we call as foundations of a concurrency control in databases what i am going to do in today s class is show the basic properties of basic foundations for concurrency control in databases by first introducing the notion of a schedule what is a schedule and what are the different things that we can understand by the concept of a schedule the second thing that we are going to look at as part of the schedule is we look at how exactly we can produce what are called serializable schedules this is basically that basic notion of ensuring consistency in databases so we are going to use these two things one we are going to look at what is a schedule what is a meaning of a schedule and how exactly serilable serializable schedules can be produced after understanding these two things what we are going to do is in the next class we are going to see some protocols that are their in the database systems that ensure that this schedules produced by the database or serializable schedules so we look at the protocols in the next class but in today s class we will understand what we are what we mean by a schedule and what we mean by a serializable schedule  refer slide  05.28  now let us go and understand the meaning of what we mean by scheduling now the meaning of a schedule in  noise  database transaction system is it is essentially a set of operations performed by the transactions on the database we will consider a very simple schedule let us say s a and then here what will do is we will write a series of operations performed by the database by the transactions on the database we are going to understand first the notion of of what are these different operation before we write the actual schedule what i am going to do is i will say essentially the database operations could be either a read operation which means that the read an item from the database read x shows that this is an operation trying to read data item read an item x from the database what this means is if this item is not in the main memory this item will be fetched from the disc and will be transferred into the main memory of the database and after that this will be used as the program variable by the program to do some operations that means the value is available now it can perform some computation using the variable the other operation that the database transaction can do is what we see has write x write x essentially means that write value of x back to database which means that the updated value of the transaction has to be now returned onto the database we will also have two extra operations which are performed either a commit or an abort at the end of the transaction execution commit means we are essentially committing all the things you have done this is at the end of the execution of the operations a transaction can issue what is called a commit command commit actually means do all the changes okay and then abort means essentially discard all the changes okay so typically the transaction decides at the end of the execution whether to store the values back on to the database or discard them these are essentially the operations that we are interested when we are dealing with a database okay a schedule essentially consists of read operations write operations a commit and abort command  refer slide  08.44  now what will do is we will further define the schedule much more clearly by taking a set of transactions that are operating on the database simultaneously for example take the case where we have the reservation system where passengers are trying to book their ticket simultaneously they could be one passenger who is trying to travel to delhi by rajdhani express and he will try to book his ticket for that particular train there could be another passenger simultaneously trying to book for the same train for going from chennai to delhi which actually means again both of them both these operations simultaneously operating on the database now both of them should get different seat numbers when this operates simultaneously if both of them get the same seats we actually leaving the database in the inconsistent state because two passengers can not get the same berth to travel from chennai to delhi so that is what we mean by looking at um i know when transactions are operating simultaneously on the database how exactly we can produce consistent results on the database in this particular case what we can see is there could be first transaction t one which reads the current reservation information from the database we will just abbreviate as read as an r symbol so and we will give an prefix r one here indicate this is being done by transaction one this is looking at the rajdhani express availability which is given by r one x now after actually checking this it will try to write the value back onto the database saying that it wants the um reservation for this in some cases the passenger finds that he doesn t have the money at this point of time he can discard the changes which means that the transaction can get into a abort state after reading the availability but nor booking the final one it is possible for various reasons the passenger didn t have a lower berth as he desires so in all this cases the transaction after started would abort the operation of writing this values back on the system which means that essentially the passenger is not interested in booking the berth for himself after reading the values now let us say this basically transaction commits at the end of it which means that you have got the berth and your writing this value back this is what all the operations that could be performed by the transaction tone now if you take basically another transaction t two it could also be reading the value of this radhani express availability for a berth and then writing the value back and then later committing now if you basically look at these are all the operations of the second transaction its possible that these two operations of the transactions can be interleaved in any order when they are executed on the database what does that mean it means that its possible that i could execute r one x of t one then r two x of t two then w one of x and w two of x then say i commit c one and c two this could be one possible execution of sequence because these can be interleaved in whatever fashion that is possible this is basically what we mean by a schedule s a or some schedule a schedule is nothing but a sequence of operations that we are performing on the database from transaction t one to t n som en transactions now the transactions keep continuously executing on the database which means that as the transactions are coming we are executing the read and write operations relating to this transactions and either committing or aborting the transactions at the end of what they perform and this process continues now as its happening we need to ensure that whatever operations are performed by the database is actually leaves the database in a consistent fashion for example we can look at the schedule that we have just produced and see what would have happen if the sequence of instructions are executed as shown in a schedule s s a in this particular case now as you can see here the value of x let us say at this point of time is basically ten at the start of the execution so read x read one of x would have resulted in reading the value of x as ten read two x will also result in reading the value of x as ten then a write would actually resulted whatever computations that was done and writing the value let us say actually after this computation i actually right a value of x equals to five i subtract five form ten and let us say the w to actually adds five to ten which means that it results in fifteen and after that c one commits which means the value of x will be written as five when c two commits value of x will be written as fifteen  refer slide  14.38  as you can see here if the transactions have been executed one after the other the end result would have been different x the end value of x would have been different from what was actually produced here if you carefully notice the affect of the first transaction are last on the database if because the second transaction also read the same value of x not produced by the first transaction and hence this will result in an inconsistent operation of writing the values onto the database we will characterize this consistency more carefully by looking at schedule and then trying to characterize the schedules in terms of how they basically whether they are consistent schedules or whether they produce what we see as consistent results on the database now a simple case is where the transaction t one is written assuming that this is the only ony program that is operating on the database let us assume that t two is also written assuming that this is the only transaction that is executing on the database this requires that t one is consistent as long as it is executed from start to finish consistent from start that is start to end all the instructions are executed without any other transaction seeing the values used by t one similarly the same thing is true with actually t two which actually means that it will also assume that the start to end is executed as far as t two is concerned without being interpreted what this means is either you execute t one completely before t two or you execute t two before t one this is a very important notion here of saying that i have what is called a serial schedule a serial schedule is one where the transactions are executed in such a way that new transactions are executed only after finishing the earlier transaction so in this particular case um if you say a serial schedule all the transactions should be executed one after the other for example if i have n transactions their should be a mechanism by which i categorize t i less than t j less than t n like this which actually produces a serial schedule the only problem with the serial schedule is this is very limiting because its possible that these transactions can be executed concurrently simultaneously till actually producing correct results for example let us assume that t one is booking for rajdhani express and then let us say t two is trying to book the reservation for let us say trivandrum mail now there is no conflict between these two which actually means that even when these two execute concurrently there is not going to be any problem in terms of the end results because they are not conflicting with each other  refer slide  18.20  so by unduly restraining that i know the transactions should be executing one after the other would only affect the database performance we can when they are not conflicting suddenly we can execute them in a parallel way and get better performance for the database rather than enforcing a serial order this is the first important concept of trying to look at a serial schedule now what we will try to do is how exactly one can think of a serial schedule and produce a serial equivalence schedule not exactly serial schedule but equivalent schedule now what we do is for this we will define the notion of equivalence schedules what this means is two schedules can be seen to be equivalent under certain conditions for example let us take a schedule s a and a schedule s b and define what we mean by an equivalent schedule two schedules are equivalent if basically all the operations which appear in s a also appear in s b for example for transactions t one to t n all the operations will define all the operations appear in both schedules now after this point do define equivalent schedule we need the property of saying what kind of equivalence is this between the two schedules one is two say as i have actually shown in the last slide that when they are actually not conflicting doesn t really matter how the operations actually appear in the schedule s a and s b for this what we define is what we call as conflict serial ability which actually means that only one transactions are conflicting with each other those operations alone need to taken care other operations need not be no they can be executed in any possible order what this actually means that you need to focus between the two schedules s a and s b on what we see is the conflicting operations and ensure that this two conflicting operations are done in the same order in s a and s b  refer slide  21.22  now for this i will define what it means to say two operations in transactions conflict okay um conflicting operations are the following now one of the operations of the transaction is basically a read operation let us say read one of x okay and the other operation is essentially a write operation oaky this is conflicting because the transaction um t one and t two are operating on the same data item x and one of the operations is a write in which case we say these two operations are conflicting with each other okay there are also other probability where the first operation is a write and the second operation is also a write which essentially means that t one and t two again will be conflicting with each other with respecting to this write operation  refer slide  22.39  so from this we can infer that if one of the operation is a write and two transactions are operating on the same data item in this case x and one of the operations is a write then we say that the operations are conflicting now all the executions that a transaction does need to worry about how these conflicting operations are executed in a schedule okay now to give this notion what we say is all that we will be worried about is conflict serializability that means you don t need to really concerned yourself about serializing all the operations but you have to actually do what is called the conflict serializability that means when operations are conflicting you have to do what is called the conflicts serialabitlity now in this particular case let us say two operations r one w as shown in the last slide which means that r one x and w two x if they have performed this operations one after the other it essentially means that t one has executed before t two for data item x is concerned okay now it could be the other way around also depends on how this is done in the schedule how exactly this conflicting operation is performed but suddenly if r one x is performed before w two x this is the order as far as data item x is concerned now if the transaction are also conflicting on another data item let us say y and on y database has actually performed operations such that w t w two y occurred before let us say w one y on the database item y if you actually want to order this transactions then it is going to be t two before t one as far as y is concerned okay now if these two operation occur in this order in a schedule it essentially means that there is no fixed order as far as t one and t two is concerned because as far as x is concerned t one is before t two as far as y is concerned it is it is the other way around t two is before t one which exactly means that i no longer can infer from this t one occurred before t two or t two occurred before t one  refer slide  25.35  which essentially means that on the conflicting operations there is no way to actually serialize the transactions by saying t one before t two or t two before t one in which case such an execution is not obeying conflict serializability because the conflicting operations are not serializable in the schedule now to actually give the notion how exactly this can be further looked at two schedules s a and s b can be seen to be equivalent the conflicting operations appear in the same order between this two schedules which means let us say i have a schedule where there is a set of operation that are performed in this particular fashion on schedule a if they actually are performed in the same order in s b there could be some other operations interleaved but then as long as the final order i see between these two conflicting orders is the same then i say these two are equivalent schedules now this wont be equivalent if the order in which they appear here is different from each other for example if w x in the other schedule comes before r one x then they are not equivalent schedules if the operations are not conflicting it doesn t really matter in what order they are appearing for example let us say there is a data item here in this schedule x on which actually transaction one reads this here and then there is another data item which transaction two is actually writing which is     27.32 in this particular case now these are not conflicting operations now even if you change the order of this operations it still doesn t matter because they are not conflicting operations which is equivalent to saying that even if you now transfer w two to before to r one x its still okay for me because this operations are not conflicting and hence we don t really care to actually worry about order of non conflicting operations since in the first one r one x we just recap what we are trying to do here two schedules s a and s b are equivalent as long as the conflicting operations appear in the same order between the two schedules in that sense the two schedules one and two as shown here three cases where s a is you know some random order where r one x appears before w two x r one x indicates that this is a read of transaction one on x this is read write of transaction on two of x since these two are conflicting they way in which the schedule a there appearing is the read x is before the write x if it appears in the same order in s b also then we say that equivalence schedules that all conflicting operations appear in the same order in the two schedules we say they are conflict equivalent conflict um it terms of equivalence they are um in terms of operations they are equivalent schedules whereas you can see they are not conflicting it doesn t really matter in what order they appear  refer slide  29.22  now to introduce the notion of how we use this conflict equivalence in actually deciphering whether schedules are schedules can be use to decipher produce consistent results we say a serial schedule is always a consistent schedule okay this is the benchmark to say that i produce serial schedule then it is consistent the simple reason here is t one all operations of tone executed before all operations of t two i see this as consistent execution because i am able to execute all operations of t one before t two and hence the database is consistent what it is doing is consistent okay when i have a schedule which is not equivalent to a serial schedule i will try to change the operations which appear in the schedule s a okay i transform now this to s a dash but this is an equivalent schedule as long as conflicting operations are same between s a and s a dash this is still a equivalent schedule this can further be transformed to s a double dash and finally this s a double dash becomes a serial schedule which actually means that i have been able to transform a schedule s a into a serial schedule now we use this notion to say that s a is a serializable schedule not serial schedule but it is a serializable schedule okay in terms of the conflicting operations the way i see executed this conflicting operations is same as s a double dash a and hence s a is basically a serializable schedule now what we are interested in is producing the serializable schedule because serializable schedules can be reduced by swapping the non-conflicting operations in whatever way you want into a serial schedule  refer slide  31.36  this is in effect conflict serializable schedules conflict serializable schedules what we are as long as a schedule is conflict serializable and you at the end of the um execution you are able to show that this schedule is equivalent to a serial schedule s a is a consistent schedule or the operation of s a is consistent an that is what we are interested as far as serializability is concerned this is a very important notion of actually being able to serialize um the transactions produce serializable schedule we will try to look at now is how exactly the notion of serializablity we will be used by database transactions to produce consistent schedules now for that what i will show is to start with um as the database is operating okay transactions will be coming in at any given point of time into the database imagine for example this is a railway reservation system which means that the um passengers keep coming and keep reserving the tickets at any given point of time which means that there is this set of schedules um this set of transactions t one t two t n which keep generated at different points of time now as they keep arriving into the database some operations of t one t two t n will be executed here which actually means that to just produce this we will say o one x is the operation of transaction one o i j is a general operation on a data item y on the database and this is how this um operations are executed a far as the database is concerned this is what we actually mean by a schedule okay now since the database will be operating continuously this transactions keep coming regularly into the database um it is not possible at any given point of time to actually um close the transaction close the schedule and say i have actually looking at a particular schedule what this require is at a given point of time if you want to analyze you need to put a break point and say i will actually take what is called a projection of this for the complete schedule which means that all those transactions which have actually committed or aborted as far as the schedule is concerned whose operations are all performed that will be included in this complete schedule let us say up to t some r i have been able to now do the all the transaction execution then i will say i will execute from t one probably i will do a slightly change here i say t r we just make sure that we get the thing right t r comes later which actually means that up to t one to t n these are completed transactions that means the complete projection of s will include up to t one to t n which means that my schedule s which is a on partial schedule of all the transaction coming in which includes the t r here this t r actually goes into the this schedule here and then this from this actually i am actually projecting from this set here to complete s  refer slide  35.30  now which actually means that t one at a given point of time the t one to t r are the total set of transactions that i can consider but t one to t n is a completed set and i am actually looking at complete schedule which means that all the operations of these transactions have been included in the complete schedule now when you actually take the c of s as the projection then we want to apply the notion of whether this schedule to produce is a correct schedule or not this is when we are going to apply the equivalence this schedule let us say is called s a now i apply on s a and then see whether this s a is reducible to some serial schedule okay a simple check of seeing whether a serial schedule is being produced or not um what we can see is a simple algorithm which constructs a graph showing how the transactions are executed in you system  refer slide  36.33  assume that actually t one came into the transaction system now you try to actually see t one put it as a point in the graph now let us say another transaction actually t two comes into the system now assume that t one and t two actually conflict on a data item x and then this conflicting operations on data item x are executed in such a way that t one appears the operation of t one appears t two then provide arc showing that there is a precedence relationship between t one and t two showing that t one comes before t two as far as this operation is concerned now we assume that there is another transaction which came t three and this is conflicting on let us say an item z and this is the operation as far as this is conflict is concerned let us assume now between t three and t one there is a relationship in terms of conflict on y and if in this case t three is executed before t one we have essentially a cycle in this graph which means that the conflicting operations actually terms of the graph of forming a cycle what is the meaning of this cycle assume now t one is less than t two as far as first arc is concerned t two is less than t three as far as second order second arc is concerned as far as the third is concerned t three before t one which shows that it is not possible form this to actually say any particular order in which these three transactions have executed the last one will be wrong because by a transitive relationship t one should have finished before t three  refer slide  38.45  and this is what exactly is done to show whether a schedule is a serial schedule or not there should be any cycle if there is a cycle it shows that it is a non serializable schedule the presence of the cycle in the transaction graph is shown to produce non serializable schedule because a cycle prevents you from coming with order in which the transactions are put in the particular order of one being finished before the other and hence this will not produce a serial schedule i think this is important as far as the concept is concerned because we use the transaction graph to understand whether a protocol is basically produces serial schedule or not as a later lecture towards the next lecture what we are going to build is several protocols for actually building the or executing the transactions essentially these protocols will try to construct the transaction graph in such a way that and incoming transaction is put in the correct order as far as the sequence is concerned all that you want to do is you don t want a cycle in the graph you are not trying to produce a cycle in the graph and the protocol has to ensure that there is no cycle i this particular graph and that s what exactly we can understand for example imagine that there is currently um a current transaction graph looks something like this let us say i have three transactions active in my database and this is the current sequence as far as the conflicting operation are concerned let us say t four now comes into the okay into the database at this point of time i have several options of where i can put this to avoid a cycle from coming in into the this particular graph it is possible for me where the protocol always allow the transaction graph to grow only in the forward direction which actually means that it is possible for me to keep this t four here that is one possibility or i can put t four here i can put t four here which actually means that the graph goes only in the forward direction and when it goes in the forward direction it prevents any cycle from occurring because you are not going to but a backward r as long as you don t put a backward r you let the transaction graph only in forward direction it is possible for you to avoid a cycle in the graph the other possibility is it is possible for the protocol to decide to put it even before which actually means that if it can be made to read the value let us say there is a write x here and i let this transaction read the value of x before this is modified then it is possible for t four to be put before t one even when it is coming after the graph in that sense it is possible that t four before t one also doesn t produce a cycle and hence this is also a correct schedule so it depends on how exactly the transaction graph can be allowed grow by these protocols essentially the concept is a graph is constructed and a cycle is prevented from happening in the graph so i think we understood now the basics concept of how exactly the serializable schedules is used by the transaction system now what i am going to show in the next few minutes is to look at other kinds of consistency requirements an interesting thing to understand as far as consistency is concerned is to look at basically t on executing before t two is a correct thing to happen but this need not be the case for example if you look at debit and a credit transaction it is possible that any number of debits and credits can be interleaved as long as the debit occurs as one unit and credit occurs as one unit now this is very interesting because we can start looking at what is a operational semantics and try looking at whether the way the transaction execute is consistent or not to give a more deeper treatment of this we will take a simple example and then see what exactly we mean by this understanding the semantics and seeing whether the execution is right or wrong as far as the database transaction execution is concerned for example imagine i am actually having account and i do a debit on my transaction which actually means that this is basically withdrawal and i actually add some numbers after that means i credit into my account which is equivalent to saying that i read the value of x here and then i basically add some number here as long as the read of x is consistent with respect to the write it does produce consistent results what this means is there is basically write that is happening on x before the read is happening on x now this write can be by any transaction let us say this is by the transaction i and this is by another transaction j this is the relationship between the two transaction in terms of i am actually reading the value let us say the j is reading the value produced by i that means t i has actually produced the value of x which is being read by t j now as long as this relationship is maintained between transactions in terms of how exactly they read the values of the previous transactions and this is actually maintained between the two schedules we say that two schedules are equivalent in terms of views okay this is called the as appose to conflict serializability this is called the view equivalence of schedules now what this means is two schedules s a and s b are view equivalent as appose to conflict operations equivalent schedules they are view equivalent schedules if the way actually read operations and the write operations are related is they actually read between the two schedules the operations are the same in terms of they the way it has been produced and read okay if this order is changed between the two schedules then it is basically not they are not view equivalent the final writes between the two schedules also have to be this is first requirement the second requirement is the final write operations are same between the two schedules are same in both schedules these are the two conditions that need to be satisfied for two schedules to be view equivalent  refer slide  47.06  it is possible that view equivalence can also be seen as producing consistent results for example you look at typical case of of what we considered as debit and credit transactions you know occurring simultaneously is possible to see that view equivalence is will produce correct results as appose to the conflict serailizability now this is interesting because we will start realizing that it is possible to enforce connect ness by understanding what is happening with the transaction semantics for example it is possible to look at semantics of operations finally we can look at a semantics of an operation and then see whether a particular execution of this operation can be correct okay what i am going to do is i will take a very simple example to show how semantics can be applied for understanding the consistency criterion i is possible to say that i have i will take as slightly different example here to show what is semantics of an operation we can take a simple queue as shown in this particular figure now the queue will have what we say as a front pointer and a rear pointer and it will basically have two methods which can be executed which is basically an add and a delete now if you can carefully look at how exactly the queue can be left in a consistent condition when adds and deletes are happing simultaneously now you can see that basically an add will will happen at the rear end and a delete will happen at the front end now adds and deletes suddenly be concurrent assuming that the queue is not full the queue is not empty under those conditions adds and deletes can occur concurrently because add is actually trying to manipulate the rear um pointer delete is actually manipulating the front pointer this is very important to look at little more deeper for example let us say there is t one here and you are actually saying q dot add okay now there is a t two which is actually saying q dot delete now we know that from the semantics of add and delete t one and two can happen concurrently and still produce correct results this is basically semantics knowing the semantics of the operations i am able to say that these two produce consistent results now if you basically further that two adds can also happen simultaneously and i have a mechanism for producing i know two adds working simultaneously it is possible because all that you need is lock the rear pointer an if you allow the rear pointer to be obtained by each add separately then you need to lock only the rear pointer and ensure that this two adds at the add level can be concurrent but at the rear level they will be blocking each other that means the access to rear will be may consistent but at the add level they can be still working parallely or concurrently  refer slide  50.56  and this is basically understanding the operation of the transaction and applying what we call as semantic consistency since you know what is semantic actually means that you know the meaning of the operation and apply the meaning of the operation to decide whether something is consistent or not and that is very interesting because it is possible to apply a much greater level of consistency criterions by understanding the meaning of operations to just recap what we have done in this particular class and then give you some indicators of how exactly the to go on further reading this particular subject i typically covered the idea of what is basically a schedule in this particular class and what i have also done in this particular case is i have actually produced equivalence schedules and this equivalence schedules are from different aspects two schedules are shown to be equivalent from a conflict operation point of view by saying that if the conflicting operation are executed between the two schedules in a particular way the same order is maintained between the schedules we call that as conflict equivalence we also showed view equivalence which actually means that the writes produced by one schedule the writes and the reads the way they occur on on operations are same between the two schedules we call that as a view equivalent schedules finally we also showed what is called semantics and based on semantics how the schedules can be seemed to be equivalent you can do the commutative operations as long as they are parallel whatever order they appear still the schedule is write as long as you commute the commutative operations are performed in any order is still will be producing consistent schedules and so we actually shown  refer slide  53.15  how exactly we look look at equivalence and what we have further shown in this particular class is typically how simple case of conflict serializability can be achieved by constructing a transaction graph a transaction graph is constructed by producing before and after relationships on the various transactions and that is how actually conflict serializability is achieved by constructing the transaction graph finally we have actually shown how exactly the protocols various protocols will be used will be designed to produce the serializable schedule the criterion for this is will be designed to produce conflict serailizability what i am going to do is in the next class i am going to discuss a series of protocols which actually produce conflict serializability okay we are going to look at a set of protocols we start with a most popular protocol of two phase locking and show how two phase locking will produce conflict serializability and also go on to show other kinds of protocols that exploit the property of the constructing the transaction graph without any cycle that is the essential property is here there should not any cycle as far as the transaction graph is concerned and the protocols exploit the property of trying to construct they cycle and we essentially can divide the protocols as being optimistic or pessimistic on how actually they construct the transaction graph 58.15 we are going to take this in the next class of looking at the protocols and seeing how different protocols can be constructed for producing serializable schedules as a thing of further reading on this you can typically look at there is a book by burnstien on actually concurrency control in databases this is a excellent burnstien and others basically a book on concurrency control and you can have a look at this the book as a further reference i have also used um basic foundation thing was used by the book on fundamentals of database systems by elmasri and navathe elmasri and sham navathe i have used the chapter from this book while u doing this particular foundation on concurrency control what i am going to also do as part of next lecture is while while doing the protocols at the end of the next lecture i am going to introduce a few problems and try solving them at the end of the next lecture okay we will stop here for this lecture lecture # 20 concurrency control part i foundations for concurrency control in particular what we did was we looked at the problem of serializability when transactions are executing concurrently for example there are a syrup transactions and all these transactions are execution concurrently we were looking at how this can be interrupted as a serial execution of the transactions for example some order in which they could be seen to be executed is what we looked at the theory for correct execution for transactions  refer slide  02.21  we also looked at two kinds of a serializibilty the conflict serializibility where the transactions which are executing on the same data item when they conflict how they can be reduce to serialzible transactions that is what we meant by conflict serializability we also looked at other forms of serializability like view serializability what we are going to in this today class is we are going to look at specific concurrency control protocols and to begin with what we going to do is we are going to look at what are the different types of algorithms that will allow concurrent execution of transactions to be serializable that means which produce correct execution of the transaction in a database so what we are going to do now is we are going to look at what are the algorithms that we use for achieving serializability and these algorithms are called concurrency control algorithms now there are two broad classes of these concurrency control algorithms based on how they actually view the system some view the system in an optimistic fashion and some view the system in pessimistic fashion what we mean by optimistic and pessimistic i just explain with a simple example now if you basically look at a narrow pass bridge where vehicles are crossing this bridge and this is relatively a very very know low traffic bridge we don t really we know look at the vehicles will hit each other when they are passing through this narrow bridge which enough at means that there are actually vehicles coming from opposite ends so you actually assume that relatively conflicts are very low which actually means that at a scenario where the conflicts are extremely low is one situation there could be another situation where at the conflicts could be extremely high which means that this is a very high traffic bridge and hence you assume the possibility of a of two vehicles finding themselves passing through this narrow bridge at the same time is lightly to be high in either case what we have to see is is the two two scenario s are different system configuration so what we will do is we will broadly classify conflicts being extremely low and conflicts being extremely high now if you typically look at the way we would see when the conflict are extremely high ace we basically will put some kind of a traffic signal here and we will ensure only one of the vehicles is into the narrow pass bridge that means we enough at will ensure that only one of them is going to be inside if we don t put the traffic lights here very often what will happen is two vehicles will get into the bridge and realize they are in conflict and one of them has to back track for example if we assume that one vehicle v one has passed up to this point and v two has passed up to this point and you find there is a conflict here one of them has to backtrack and this basically involves lot of hard work and hence when the conflicts are very high it is probably not advisable to actually allow the vehicles to get in without any kind of control the situation where the conflicts are low is seem to optimistic that means you actually believing that they conflict are less which is an optimistic approach or or you view the system in a optimistic way the second one where the conflicts are very high you look at it has pessimistic okay so this is broadly classified in the two scenarios optimistic and pessimistic scenarios  refer slide  06.47  now what will do is we will see algorithms that operate on the database system assuming that the system is in optimistic scenario and those algorithm which actually operate assuming that there in the pessimistic scenario for example if you know chennai for example in the mount road if you assume optimistically no vehicles are going to hit each other and remove the traffic lights everything is going to be chaos there but on the other hand if you know iit madras the gagendra circle you are not going to install a traffic light because we don t have that much traffic it is going to be cumbersome so in some sense if the conflicts are rare if you apply the other algorithm it is going to be overkill so typically we have to understand what is the scenario in which the system is in weather it is in optimistic scenario or in the pessimistic scenario and based on that you should be applying this algorithms now broadly what we are going to look is these two classes of algorithms which are actually categorized two classes of cc algorithms but it is possible that there are more algorithms then these two classes now the one class class which we are going to look at is the lock based algorithms which all assume pessimistic scenario which assume the transactions are going to conflict with each other often the other basically a time stamp based algorithms the time stamp based algorithms assume that the system is in the optimistic fashion and hence conflicts are there what we are going to look at is one lock based algorithm the most popular algorithm called the two phase locking algorithm and we are going to look at a basic time stamping algorithm for the time stamp basic time for the time stamp based algorithms we look at the basic time stamp algorithm  refer slide  09.14  both these classes we tell us that one belongs to the pessimistic class and other belongs to the optimistic class now what i am going to do in the rest of the lecture is look at the lock based algorithm called the two phase algorithm in detail and explain what are the properties of the two phase algorithm and we will study this algorithm in detail and this is the most popular algorithm implemented in practice as well so we are going to spend some time in looking at two phase locking algorithm use for concurrency control in database systems as the name suggests this actually is a locking based algorithm the lock is very important here because what we are trying to do here is when transactions conflict on data items we are in essential going to allow these transactions to lock the data items this is equivalent to saying that once the transactions is locked the data item it is not it is not accessible for other transactions and enough at that protects the data item from being wrongly manipulated by other transactions so locking is a concept that is used in this particular case now to explain the concepts of looking in more detail we have a problem in which we are also familiar in operating systems call the mutual exclusion problem what actually the mutual exclusion problem will try to do is it actually tell that if there is one process p one which is trying to be in a critical section the critical section is nothing but a piece of code that is trying to address a share data structure between p one and p two let us say there is a share data structure here between p one and p two and whenever p one is in the critical section it means that it is trying to manipulate the share data structure here okay now only one of them can be allowed to be in the um in the critical section for manipulating this is basically the read write operations that they might perform on the shared data structure so what in a sense the mutual exclusion problem does is it allows only one of the processes in the critical section it excludes the other process from being in the critical section this is what we understand in operating systems as a mutual exclusion problem now this is achieved in the operating system by using two operators called p and the v operators p is essentially a lock operator so when you actually apply p operation the process actually uses the p to lock the critical section and uses v to unlock the critical section this is like i know um two people can not be in the room simultaneously then what the use is they use a lock and once somebody acquires this lock gets into the room unless he actually unlocks it and comes out of it the other person can not enter into the room that is what actually prevented by using this p and v operator in the operating system context  refer slide  13.00  now in the case of databases the it is slightly different than being just mutually exclusion problem because what we are actually doing in the database context is we are actually assuming that there are different types of locks they could be read lock whereas in the case of a mutual exclusion problem we have only one kind a of lock that ere different types of locks that we introduce read lock actually says that transaction is trying to read the data item is trying to read this is very important that means it is not going to right on the data item is only going to read the data item okay and um if you say there is a right lock the right lock means that it is trying to right the transaction wants to write onto the data item transaction is trying to write on the data item okay is also you can say read lock i will abbreviate here afterwards read lock with an rl an rl shows that the transaction only wants to read the data item and this is also called a shared lock because its possible that more more number of transactions can start reading the item at the same time it is shared that means there wont be any violation if more than one transaction tries to read the same data item at any given point of time only one transaction can write on the data item so when we abbreviate write lock as wl this also is called exclusive lock that means this lock will be granted only to one transaction whereas the shared lock can be given to multiple transactions at the same time  refer slide  15.50  this by actually distinguishing the kind of lock we will in the sense that try to increase the concurrency because whenever a transaction acquires a read lock it is possible for other transactions also to acquire this lock but whereas if one transaction is given the right lock the other transaction can not be given um an extra write lock on the same data item unless the work was finished by the transaction so in other words as appose to looking at the operating system context where we have only one block we actually distinguish the semantics of the lock here whether it is read lock or whether it is write lock based on the database systems tries to optimize the concurrency that is possible when transactions are simultaneously executing to also understand the granularity of the lock this is also important issue when you actually locking algorithms the gradunality actually means that how um the size of the data items that are locked by um by the data base by the transaction now it is possible that the transitions only locks a simple um data item which actually means that in a database table one single data item for example in student in the case of a student record it is possible that we are locking only a um we are only a student name or w are the students ccpa we are not locking any of the other information of the student record the other one could be the entire tuple tuple means all the things relating to the students one particular student getting locked that is called the tuple this is equivalent to also a row in a database okay the entire row is being locked the other one is the table which actually means that the all student records are getting locked that means that the entire um table is getting locked this is very important to understand the granularity as you reduce granularity transactions only locks that small item but if you start increasing granularity it starts locking a large number of items so consequently as you increase a granularity of the lock you correspondingly reduce the concurrency that is available in the system for transactions to execute on simple example is for example if you provide for the entire iit access at the in gate where there is there is going to be a lock there is going to be only one person entering into the um entire into the campus at any given point of time and after he goes out the lock is given to the next person it drastically reduces the number of people who can get into the campus at any given point of time but on the other hand if the access is controlled at a room level where when they enter into the iit when they reach the particular room you want only one person to be entering into the room at aty given point of time and that is the point where the room is locked a particular room is locked then the amount of concurrency that is available in the system is extremely high but it depends on where the concurrency is to be provided but we suddenly should lock as small as a grandnulauty item as possible keeping the consistent criteria into account  refer slide  19.01  now given the understanding of the lock let us go on to see how a simple two phase locking protocol will be working in the database context as the term two phase locking explains there are two phases in these particular algorithm the first phase is what we called as a growing phase of the transaction in which the transaction tries to acquire all the locks and there is a second phase which is called a shrinking phase where the transaction tries to release the locks so if you basically look at the execution in terms of time let us say this is the time um access now the transaction starts executing at some point t one okay and what it actually does is as it starts executing let us say it starts to actually reach execution points where it needs some data items for example let us take a simple transaction where it is accessing bank card database as shown in the earlier examples i will basically try to read the balance in a bank account now the first thing that will happen in this case is i try to acquire a read lock on the balance data item okay which could mean that i might actually lock the entire account of a particular person and that shows that i enough at reach a point where i have locked i have asked for a lock okay now it is up to the database manager to see that if nobody else is actually has a lock on this at this point of time or if the other transactions own only the read blocks on this data item i can be granted the lock request okay now as i proceed like this i keep asking for a locks on other items as well so this is basically the growing phase where the transaction is actually asking for locks okay this is the first phase of the transaction execution which is called the growing phase the growing phase the transaction is trying to get all the locks it need to do the work that it has to do for example imagine that you have to move some items form one place room to another item okay another room then you first step before moving the data items from one room to the other room or manipulating the items in a particular room you try to acquire locks on all the rooms that is the first phase of the growing which is called the growing phase after you have got all the locks you do the um operation that you have supposed to do and after you basically start releasing the locks this is basically called the shrinking phase okay in the shrinking phase the transaction is actually releasing the locks okay this is lock acquisition and this is lock release okay in this side it is basically releasing the locks okay now the important condition of two phase locking is once a transaction is released one lock it can not ask for any more locks this is a very important condition and a subtle condition that enforces serializability we are going to examine this little more deeply little later but right now what we understand is the condition between these two phases is once the transaction has reached this point which is called the lock point okay this is called the lock point whereas the lock point has been reached let is say at time t the transaction is not going to ask for any more locks and it only releases the locks okay this we will put it so the first phase is the growing phase and the second phase is the shrinking phase now in terms of the two phases the condition is you are not going to this lock point you are not going to ask for any more locks after you have reached actually the lock point this is a very important condition that is enforced okay now let us examine how exactly the two phase protocol works for a transaction and understand deeply what are the consequences of applying a two phase locking protocol okay  refer slide  23.58  now i will take a simple transaction and show what exactly would have happened if i had actually applied a two phase locking algorithm i will take a simple case of a transaction one which is actually reading an item r x and actually writing an item data item x and then again it is basically trying to acquire or read an item y and then write an item y okay there is basically another transaction t two which is also doing exactly the same work of reading a data item x writing a data item x reading a data item y and writing a data item y now if you basically look at how two phase locking protocol would have work in this particular case when both these transaction at some point of time come into the system now let us assume that actually t two has actually coming to the system at some point of time the first thing it will start doing is as its starts it requests for a read lock on x okay read lock on x x now when the read lock on x is granted it is going to ask for an up gradation for this read lock to a write lock okay now it still can not release this lock on x though it knows that at this point of time it is actually finished working with x now it is going to work with y but still it can not release the lock on x because if it has released the lock on x it can not ask for the lock on y this is the condition for two phase locking if you have released one lock you can not ask for any more locks so the transaction t two will hold the lock x and ask for now read lock on y and it will upgrade it to the write lock on y and it this point of time it will release all the locks  refer slide  26.05  now only when it releases the locks on x and y can the transaction t one start executing which actually means that the locking will ensure if t two has acquired a lock on x it effectively prevents t one from acquiring the same lock on x and hence when there are two conflicting transactions on a data item they will be serialized based on who has acquired the lock first this is how exactly two phase locking protocol will work now the meaning of acquiring a lock is its subsequently prevents any other transactions from getting the lock on the data item as long as this transaction is using it this is typically what is achieved as part of the execution what you are achieving here is what is called conflict serializability here in fact making sure that the transactions execute when they are conflicting on data items in a serializable order i am going to show a small proof to show that a two phase locking in affect produces serializable order of transactions before i do that i am going to look at some more properties of two phase locking in terms what it can be doing since the first thing that is going to happen here is um transaction has to wait if the transaction has been locked by some other um transaction if a data item is locked by some other transaction it is possible that transactions could be waiting for each other now this could be result in what we call as the problem of dead locks this is one of the problems of two phase locking what we mean by dead lock is let us say t one has actually acquired a lock on t one has acquired a lock on data item x okay and t two has actually acquired now a lock on data item y it is possible that now requesting a lock on y and this is requesting a lock on x both can not proceed any further because they have reached this point but they are not going to release this x or y till they reach the point of lock point where this is the lock point so both will they are this point right now and there is now way both these transactions can reach their point but unless they reach the lock point there is now way they are going to release these locks so it is possible in which case t one is actually waiting for t two to release a lock and t two is actually waiting for t one to release a lock and both will keep on waiting for each other because they are in a continuous loop here this in affect means that there is a dead lock because neither of them will be able to proceed any further and they will be waiting for each other in this particular context this is what we mean by dead lock this is one of the problems of using a pessimistic kind of an algorithm because the algorithms which make the which makes the system wait for each other will transactions to wait for each other can result in this dead locks the dead locks will basically bring bring the system performance and the throughput of the system drastically okay which means the system um time the system throughput the number of transaction that are executed by the system can drastically get effect when there is a dead lock condition problems of deadlocks is they need to be detected when they happen and the system has to recover back from this deadlocks in this particular case um either of this transactions have to be aborted when a deadlock occurs and make sure they release the locks so that the other transaction can proceed for example to break the dead lock one of the transaction has to be aborted to make sure that t one can proceed so deadlocks needs detection and subsequently resolving this deadlock require that you abort  refer slide  30.06  one of the transactions that is involved in the deadlocks so that the system can proceed further this one of the consequences of two phase locking algorithm two phase locking algorithm also doesn t produce optimal schedules and we are going to look at the problem of what is an optimal schedule at a later point of time because the reason for this is it basically ensures two phases locking ensures that a transaction always proceeds after it gets acquires locks to the finish it never grants a lock and later actually aborts the transaction at a later point of time because there is a conflict whereas in the case of optimistic algorithms it is the other way let the algorithm um let the transaction proceed to execute um up till some point of time and resolve the deadlocks by um resolve the conflicts at a later point of time by looking at what they operated upon and seeing at a later point of time if the operate on a on a data items in a conflicting fashion they will get aborted at a later point of time so in that sense optimal schedules may not be possible in the case of two phase locking we area also going to see a small example and show how time stamping produces optimal schedules compare to two phase locking algorithm i explain this concept in the slightly different way by taking what we are actually done in the earlier class of looking at a serialization graph for example if you look at as the transactions t one comes into the system in affect you actually creating this graph which shows how the transaction have executed in the system one before the other for example in this case let us say there are three transactions and then after that basically one more transaction has been executing now we are basically what i am doing here is i am saying that t one executed before t two t two executed before t four this is required only when there is a conflict for example they conflict on a data item x i need to know how exactly they have executed one before the other okay otherwise it doesn t make in this particular case t two and t three doesn t have conflict directly and hence there is no need for me to actually put a arc here because they are not operating on common data items now let us say there is an incoming transaction t i into the system at this point of time now what the two phase locking in affect does is if there is a lock for example let us say there is a z item on which t three is locked in now the only way t i can come into the system is only after t three it can never be for example if it is conflicting on this z data item with respect to any other existing transactions the only way you can allow t i to execute is by being after t three there is no way this transaction which arrived now after t three has locked to come before this is prevented okay which means that the transaction graph the tg graph here can grow only in the forward direction that is the only way the two phase locking allows the graph to grow  refer slide  34.00  it doesn t let the graph to grow in any other direction but let us understand this problem of this graph growing in other directions as well this is very interesting for various reasons because that allows a better schedules optimal schedules to be created when you are actually executing the transactions now to explain this i will take a simple example of two transactions t one and t two executing on two data items x and y now it is possible that i actually allow the read and the write things to proceed in a slightly different way and ensure that as the transactions are operating on this either they read a pre copy or a later version which is modified by t one now as we saw earlier there is a read x and write x that is happening on x and there is a read y and write y happening on the now let us say t two exactly does the other way of reading y writing y and then reading x and then writing x now its possible that when the transaction is actually modified the value of x you can allow still this transaction to read a pre value which actually means that it is still has not written the value of its modification on to the database this is the database storage since it has not yet modified the value on the database storage it is possible to allow the t two to read a value before modification which means that there are three possibilities depending upon what you allow t two to do after t one has actually started executing if you allow the before value that is prex to be read then the transaction t two is actually coming before t one because it is actually reading a value that is actually modified or that is not modified by t one if you allow t two to read a modified value then in affect it is coming after that is this is this is write x this is after written t one has actually written the value on x you allow t to read the value  refer slide  36.40  now two phase locking will not allow the transactions to read pre x which in affect means that it will prevent this from happening more advanced algorithms allow allow the transactions to be placed anywhere in the transaction graph even they can come before a particular transaction after the transaction graph in effect there is algorithm called the five color protocol which allows transaction graph to grow in all directions these are actually five different the five colors here denote five different kinds of locks five kinds of locks that a transaction can can acquire besides just acquiring a read lock a write lock the other kinds of lock with a transaction can be granted when it tries to acquire a um read or data items which were before or after depending upon the pre and no post whether the transactions data items have been modified before or after is possible for the transaction to acquire the locks and this allows the transaction graphs to grow in all directions and this produces sort of it produces more optimal schedules then what we see in the case of um two phase locking  refer slide  38.12  the other interesting problem that you see if actually look at two phase locking is presence of long live transactions which actually means that the transactions are executing um for a longer time for example the example for a long live transaction is a transaction that is trying to compute annual um interest for every account in the bank what this does is um every every no year ending time the savings is taken and the interest need to paid for each account is calculated now what this long live transaction it runs for a very long time normal transactions run only for a few milliseconds this runs for a few hours okay and then it tries to acquire locks on almost all the data items acquires locks on a large number of data items okay now what this means is this is property one and this is property two large number of data items this in affect introduces lot of problems because this blocks a large number of short transactions what we actually have is in in case short live transactions now in the presence of long live transactions short transactions will have problems of execution because they don t be able to run they their response time is going to get drastically effected when there are long live transactions if you apply a two phase locking algorithm because the long live transaction will in affect lock all the data items and will not release the locks on other data items till it finishes because that is one of the conditions of the two phase locking  refer slide  40.09  so if you apply a two phase locking for long live transactions which are also there along with short live transactions the performance of the short live transactions will drastically get effected they wont be able to execute the response time is going to be really bad when you when you actually apply two phase locking this is another very important issue when you actually look at two phase locking algorithm there are lot of protocols which modify the two phase locking there is not not strictly two phase locking they modify the two phase locking condition to actually allow long live transactions to execute along with short live transactions and there is a whole large number of protocols available for making long live transactions execute along with short live transactions we will not go into details of that but then there is i will i am going to give reference at the end of it pointing out a papers which actually gives this um algorithms a host of this algorithms now what i am going to now touch up on is the other aspect of how a concurrency control algorithm has to be integrated with a commit protocol okay now what we understand by commit protocol and concurrency control algorithm has to be made little more clear now as you can see here it is only after the transactions commits if you remember the way we actually explain earlier um there is transaction begins execution by actually saying begin transaction when it actually comes to the end of the transaction you are actually um that is the time when you are writing all the values that the transaction is modified back onto the database it is still that point the values modified by the transaction are not actually written onto the database that is what we mean by the commit protocol there is inter relation ship between concurrency and commit in the sense that as the transaction modifies the values this is this the place where transaction is actually modifying the values okay transaction modifies and this is the point only after commit point it is actually visible the modifications are visible only after this point so there is going to be some kind of an interaction between concurrency protocols and the commit protocols and what we need to understand is how exactly the concurrent control algorithms get integrated with commit protocols because both together can only provide correctness and if you basically see that the transaction has it modified it is visible that is going to create difficulties in terms of the other transactions reading the values which are not yet committed by the transaction on the database so both concurrency and commit protocols have to work together and we are going to explain in the next few minutes how exactly the commit protocols works along with concurrency control algorithms what i am going to look at it is how the two pl algorithm gets integrated with a commit protocol  refer slide  43.42  we are going to look at several ways this can be done with two phase i will take this simple graph that we have actually taken earlier to see how the commit protocol gets integrated with this this is a familiar thing that we have actually now the t one is actually start of the execution of the transaction this is basically the end of the transaction and this is the lock point of the transaction okay now what we are actually looking at here is it is possible once a transaction is reached the commit point it is possible as it is releasing the locks they are immediately made available for some other transaction i will take only a simple case of data items x on which the transaction has a right lock now let us say it actually finished and releases the lock release right lock on x okay which actually means that it is possible that this x is available for some other transaction t two now to actually start working now when t two actually is trying to now acquire lock on this same data item x remember that this transaction is till not reached the commit because the commit point is here the transaction is actually committing itself here not before this so it actually means that we are allowing the transactions to be executed transactions to release locks before they actually reached the commit points this is where the interface between the locking protocols there is the concurrency control protocols and the commit protocols come into picture because the commit protocols start operating at this point whereas the before that we have applied the concurrency control protocol now if this protocol releases this lock then the data item is visible for other transactions because they effectively can acquire the lock but the value that they are going to read is not the value that actually is produced by this transaction because it is still not committed but if it is reading the value produced by but but our understanding is since it is released the lock any transaction t two acquiring the lock is after and hence there is a relationship between these two other t one executed before t two and hence this is to be preserved because the value t two should read is now the value modified by t one now what is going to happen in this case is if the lock has been released by a transaction before it is actually committed it s possible at a later point of time at the commit stage the the commit protocol issues an abort which actually means that this transaction t one affectively has actually aborted okay  refer slide  46.40  now this requires that t two is also actually aborted this is what we mean by if you if you allow the transactions to release the locks before you basically result in cascading aborts t one actually modified a value on x t two has actually read this modified value but now t one actually aborted for various reasons now t two should also abort and this is what really will happen if you apply the transactions to release the locks before they actually committed now we are modification to the two phase locking algorithms taking this into the account to avoid cascading aborts will require that the transactions actually acquiring the locks this is actually the lock point but actually none of them effectively will release their locks till they reach the commit point which actually means that the key point holding on to the locks till they reach the commit point and all the locks are released only after the commit actually happens which actually means that the transactions commit that means they write their values whatever values they have actually got they they will write these values back on to the system and then they allow or they release all their logs the logs are not released before committing okay this basically shows that the transaction affectively starts holding starts holding the logs till it actually reach reaches the commit point this ensures that the concurrency control and the commit protocols work correctly by actually integrating the concurrency control protocols with the commit protocols now what we are going to look at it is the all the protocols will require some kind of a modifications when we req when we look at  refer slide  48.25  how they integrate with the concurrency control protocols i will show you typically the three properties of if you if you remember for a transaction we are basically looking at three properties of atomicity which actually ensures that all our none of the actions of the transactions are written then we are basically looking at concurrency control typically this is consistency when they basically operate in okay together they basically produce consistent results then we are actually looking at the property of isolation isolation means that one transaction results are not visible for other transaction till the transaction has committed then we are talking about durability this durability is the transaction values are permanently written on the database so this together what constitutes what we actually called as the acid properties of the transactions now they together have to hold for every transaction what we are looking at here is basically the concurrency control aspects now they have to get integrated with the isolation properties this is where the commit protocols are coming into to picture this is where the integration has to take place between concurrency control protocols and the commit protocols when we discuss the time stamping algorithms also which operate in the slightly different way of actually ensuring that properties of um of serializability are actually enforced at the end of the transaction execution not before we have to see how that actually integrates with the commits protocols it is going to be interesting to see how time stamping protocols ensure commit protocols integrate together in a proper way when we discuss the time stamping algorithms we are going to look at how commit and concurrency control protocols integrate with each other in that particular context typically the atomicity properties and durability properties are achieved by what we say as recoverability properties which are actually ensured using the logs typically the logs are maintained to make sure at any point of time the transaction can redo or undo its actions and that is achieved using the logs  refer slide  51.24  logs are plus the concurrency and commit protocols together ensure that the transaction acid properties are realized and to just give you complete picture what we are basically looking at the lowest tier is the database items these are nothing but the tables that are stored in the database now at the other end this basically the applications which are trying to modify the database items now all the properties algorithms that we are talking about now come in the tier which is which sits between the applications and the database tables and this is what we mean by the database management system now in this case the dbms has various other things this has to ensure among other things um a part of the subsystem has to deal with the transactions and that is basically what we mean by the transaction manager now the transaction manager is part of the dbms and this transaction manager is the one which actually ensures that the as the applications are executing they serializiblity conidition is actually enforced using the transaction manager now to give an idea of what exactly happens  refer slide  52.44  when a transaction t one starts executing when it is actually putting a lock request let us say it is actually requesting a read lock this is actually given to the transaction manager it is up to the transaction manager now to grant this request or disallow this request at this point of time so all the transactions in effect will make the request to the transaction manager the transaction manager when it locks the data items for example there is a table here the student record table now typically it allows a particular tuple to be locked then this lock will be granted based on the request that is basically um requested by the transactions  refer slide  53.43  so all the lock requests are coordinated by the transaction manager and it knows which transaction holds what lock at the given point of time and ensures that the lock request are properly co coordinated among the various transaction that are executing it also ensure that along with the for example if there is a commit protocol that needs to be operated before the lock is released the transaction manager ensures that the transaction locks are not released till the commit point of the transaction is reached this is how exactly the concurrency control protocols work in the case of um two phase locking what we are going to do in the next few minutes is sum up and lead to the next set of algorithms which are typically time time stamp based algorithms now what i am going to do show in this particular case is a simple execution of a transaction and show how are two phase locking may not be a best way of executing the transactions and how one can think of in effect producing more optimal way of executing the transactions this is the very simple example i will um with this example i will lead lead to the next set of protocols that i will be talking in the next lecture which i called the time stamp based protocols in affect looking at the execution of two transaction in t one and t two if you typically look at a read x write x and a read y and a write y by transaction x and t one and t two let us say just repeats the same kind of execution you can in affect see that it s possible for t one and t two to execute in different possible directions now one way the two phase locking ensures that t one executes after t two is both locks of t one let us say the right lock on x and write lock on y will be granted for t one which in affect prevents t two from start executing till t one has actually finished which means that the schedule that will be possible in this particular case is read x write x of one read y write y of the other this is the point where the transaction would have committed t one and all the execution of t two proceeds after this point  refer slide  56.31  but it is possible for if you carefully notice what is possible here is this is not the best possible execution of the transactions it is possible for you to say ones t one has actually finished working on data item one data item x it is possible at the point of time for t two to start executing on the data item x because t one no longer needs that data item on x okay but the whole set of problems will come if you let this happen first thing as we discussed we will be scarifying on the isolation property because you are letting transaction t two read at the values before t one has actually committed so this will sacrifice isolation property of the transitions so how exactly if you actually optimize the execution of the transactions probably two phase locking is not the best possible way of executing but two phase locking is by far the best way of a simple way of executing the transactions i thing i will stop here and going to back in the next lecture transcription by  vidhya proof read by  shobana database management system lecture # 33 case study oracle & microsoft access dr s.srinath hello and welcome in this lecture today let us look at one more case study um of a commercial dbms this time  refer slide time  01  19  in fact we are going to look at case studies the well known oracle dbms and the microsoft access dbms which um which is shipped with microsoft office suite of a products whenever you buy them and um in the previous session we talked about mysql which is um um which is a open source or free software while the the once that we are talking about or that we are reviewing here are both commercial software that that are proprietary and and you have to buy it off and which can be bought of the self and so on now um let let us have a look at the the main structures of this  refer slide time  02  06  firstly the oracle dbms um in this  noise  um lecture we are not looking at any specific version of oracle oracle is the the dbms oracle is manufactured by oracle corporation headed by of course larry wall and  noise  and who has been a pretty early player in the um um in in the whole field of commercial database management systems and there have been several versions of oracle that have been evolved over the years and the latest version at least as of today is oracle 10g which is um the the g here stands for data grids where where um where the database can um um can  noise  can sense that it is part of a larger data grid and operate accordingly that is a grid is a is a collection of different databases um which could be distributed widely probably across the world and while functioning as a single data source or or a database and its been touted or its been kind of um targeted towards the data warehousing and business intelligence kinds of applications what are the basic elements of or or the building blocks of an oracle database when we are talking about oracle databases there some terminologies that that we need to be aware of um many of these terminologies um as we noted earlier oracle has been an early player in the um um in the field of commercial dbms systems and while terminologies have been invented and evolved over time um in the um um in in the academic and research community for for database management systems a similar kinds of terminologies and um and evolutions have happened in the commercial field as well  refer slide time  03  18  so um um so some times it it may so happen that the terminology used that that we have used for example in this class might differ from um the the terminology used in oracle or or lets say db two or some other commercial um database engine and so so it it it makes lot of sense to um um to to clear what each of these terminology mean or what kinds of terminology mismatches can we have between um um between what we have been using and what oracle for example uses when we talk about an oracle server in the in the database management system the oracle server server is actually the dbms server that that we know very well that is it s the dbms engine that serves one or more clients and when you talk about the oracle database um it is more than just the set of data files that that form the database but um oracle considers um its database as a collection of all its stored data including log files and control files and other such auxiliary files that that that make up the database and um usually for in in the um um in the academic community when you say database we usually talk about only the data files and the index files when when we talk about for example the storage structure then when you say an oracle instance um this refers to the set of processes that is the set of all processes including oracle system processes which with oracle server runs and user processes created for a specific instance of the database operation  noise  what kinds of languages do oracle support does oracle support oracle of course supports the ansi sql standard um with some extensions of itself um um that that was specific to oracle  refer slide time  05  43  and in addition to the um um well known ansi sql oracle has its own query language called pl/sql pl/sql is a procedural query language um um on top of sql that is say for example sql language is a is in some sense a pure query language in the sense that it does not have other programmatic constructs like say um using variables or well it has certain um um um you can use certain kinds of variables by aliasing and so on but um generally it doesn t have the generic structure of any procedural language like c or um um perl or some some other kinds of language pl/sql on the other hand um is a complete procedural language where you can actually define variables you can define control flow if then else constructs and looping constructs and so on and so forth and embed sql statement within um um within these procedural constructs when we talk about the database structure in oracle um we usually refer to two different kinds of structures what are called as the physical structure and the logical structure of the dbms the physical structure refers to the storage structure that is a physical organization of the um data on the database  refer slide time  07  13  while the logical structure corresponds to what is a conceptual schema of the um um of the database that that oracle is maintaining i mean um um logical structure here is the logical structure of the way oracle mange s data not that of a particular database itself so the way oracle manages data itself can be treated as a database by which which which comprises of several other databases and which comprises of a larger logical structure and update rules and associations and so on and so forth let us look at the physical structure in a lit in a little bit more detail we shall also briefly look at the the logical structure as well um um um in the when it when it comes to the physical structure of the database the way oracle stores  noise  data in on on disk the um it it basically stores data in several different files that um um um on disk and very unlike lets say mysql oracle uses its own um um buffer management policies for writing data into files what are the what is meant by buffer management policies for for writing data um um um dbms like mysql uses for example if you are if you are taking a unix based implementation or linux based implementation of and we are comparing mysql and oracle mysql uses what are called as high level system calls for writing data into disk that means high level system call essentially says um gives a block of data to to the operating system kernel and lets the kernel take care of writing the data on to the disk by itself now what the kernel actually does is um um in order to in order to speed up the process it doesn t immediately write your data block onto disk instead um it keeps it um in its um own memory its um in its own um um cache that that is in main memory and then flushes it on to the disk at at some point in time  refer slide time  08  08  this cache is called the buffer cache um we saw buffer cache and and how buffer caches are um come into play or or affect dbms operations when we are talking about database recovery processes right so when we write something onto disk there is and and we use a high level system call there is no guarantee that whatever you have written on to um whatever you have written has actually been recorded on to disk now if there is suddenly a system failure um then um even though our our write has returned us um um um returned us a successful operation and and we said we have committed that committed some transaction it may not actually have been reflected on to disk on the other hand oracle uses low level system calls that means oracle manages its own buffer cache and oracle decides for itself when it has to flush the buffer cache and when it has to write to files and so on however it still uses files that are visible from the file system interface of the operating system so let us look at the the files that oracle uses so when we are talking when we are looking at the files here that that that oracle uses we have to keep in mind that the way that files are updated in oracle is very different from the way that files are updated in say mysql so these are some of the files  refer slide time  10  45  that that oracle uses for for maintaining its databases firstly the data files of course the data files are the files that act that contain the actual data um and  noise  in addition to the data files a databases is associated with a set of redo log files remember what is a redo log files a redo log file is um are are the set of log files that are used for redo based recovery um we we looked at a set of log based recovery mechanisms where um you first write data on to the log file and then um um um after flushing the log file you you write data on to the actual database now whenever um um there is a system crash you you just have to redo based starting from the last check point in your in your log file all the operations that have been um um performed until the system crash happens so um oracle does something like that um so it it maintains the set of redo log files as part of database itself then there are a set of control files which contain different kinds of control information like um the the database name or or the different file names and and the locations of this different files and so on these control files are also use in the process of recovery some times they they are they are useful in the in the um in the process of recovery and in addition to control files there are um other files like what are called as trace files and alert logs trace files are essentially um um files that that track certain background processes that that oracles oracle runs oracle basically runs several different background processes that that for example the the server is a is a background is a processes and there are several system monitors that that oracle runs to to monitor events and um um and call triggers or enable triggers and so on and so  noise  forth now trace files essentially track this ba background files and logs them on to um um logs them on to files and alert logs maintains um um a log of the major events that have happened in um the from these background processes and and from anywhere else in the system  noise  the log files and control files as you can see are are the are the files that are essential for um the the process of database recovery therefore oracle also supports mechanism by which log files and control files can be multiplexed onto different devices  refer slide time  13  15  so when you write a log file um you can configure oracle so that it it actually writes it in parallel to multiple devices the the the advantages of this is even if there is um in a in a system crash even if there is um let us say a media failure and and the log file itself is lost we can use these back up copies of the log files from from actual um devices the from from different devices to recover back so so the resilience of the um recovery process is increased as as part of this  noise  so let us look at the logical database organization now that is um how is database organized in in oracle what are the building blocks that that make up a database in a logical sense and um um what are the different roles of these different building blocks and so on when we are talking about an oracle database um at at in the physical sense there only files every thing is a file and um um and file is managed in um by by low level um um system call operation but um from a logical sense each of these files um means certain different things and they are looked at as objects rather than files ok now what are the different kinds of objects that um that that an oracle database contains there are what are called as schema objects a schema object um um contains um definitions of definitions of some um relevant entity in the databases for example it might contain definitions of tables it can contain definitions of views or sequences or stored procedures um indexes clusters or or links to other database and so on  refer slide time  14  09  so everything is treated as an object here um and the the object is essentially in a sense serialized and and stored on to disk so um an object for um um using the object orientation methodology comes with its own set of um procedures or methods by which which which handles or which provides a set of services um based on the objects definition so when we say  noise  when we say a schema object represents a table it also provides methods by which we can access or perform different kinds of table accesses access a table row wise access um or perform any perform any index search on table and so on and um in addition to the schema object there is what is called as a data dictionary which um which is essentially the system catalog and it maintains different kinds of cataloging information for um um for oracle so it contains the information like user names and um um security information that is access privileges and what kinds of um users or what kinds of privileges on on which schema object and information about schema objects themselves um the the modification date and the the creation date and any other relevant information for for any given schema object then integrity constraints um um which which in turn trigger all the stored procedures stored procedures or which in turn trigger certain schema objects as um um which are stored procedures and then certain kinds of statistics um statistics essentially for um example that that help in  noise  performing good query execution plans we saw the the role of cataloging information when when we where looking into how do we manage or how do we um process a query and and optimize a query cataloging information contain certain i mean the the statistics here in the in the cataloging information contain certain estimates on let us say how many rows are there in a um in a table how many number of distinct values of are there for a particular column and um which um rows is accessed how many number of times or whatever all of these information go into um um performing a or forming a good query execution plan well  noise  while um um while making the query and then there are audit trails that is um auditing information about the the different aspects of the database the the next um aspect of the logical schema or or logical organization of the um of the oracle databases is is what is termed as the table space  refer slide time  17  46  a table space again an important concept in um the in oracle which basically describes um the physical storage structures of um of a given set of tables um in a in a database and um um it basically governs how the physical space of a given database is used we look into how the tables space is organized there there bas they basically contains different segments and which in turn manages the pages or or the disk blocks or that that make up a particular database so earlier we mentioned the concept of an oracle instance um an oracle instance um like we said has a specific definition that is um um it is in some sense it is a snapshot of the entire oracle system that is it s a the it is the set comprising of all processes that is oracle processes and um user processes and all of which comprises one instance of a servers operation  refer slide time  18  41  now an instance itself can be logically seen as comprising of several different parts um so an instance is logically divided into what what is called as a system global area what is the system global area the the system global area is um essentially the the area which um in some sense to to give an analogy to operating systems um it is the um um memory area that is used by a kernel for example in in addition um in contrast to that of um areas used by user processes that is um all the resources and and memory locations that that are used by the kernel is a system wide and um globally relevant so the the system global area is in some sense the kernel of the oracle um system it contains the database buffer cache um which which we saw is is maintained by oracle itself in order to flush buffers on to um on to disk and to in order to basically control um when buffers are flushed on to disk then there is the redo log buffer where um um which is the which is the buffer cache for the redo log so so so whenever a redo log is returned um it is actually first return into the redo log buffer and which in turn is flushed on to the disk at um um at at periodic intervals then there is shared pool of um other resources which which we will see then in addition to the system global area the when we talk about an oracle instance we also talk about the user processes  refer slide time  21  23  an oracle instance again is like i said is is something like a snapshot of the entire system so um you you might also um take an analogy to operating systems where we say an instance of the operating system is the set of all processes comprising of the kernel processes and all user processes at any given um instance of time in addition to user processes there is what is called as a program global area in contrast to the system global area a program global area is a is a memory buffer that contains a data and control information for server processes that is um  noise  where um this is the global area again there there is no specific analogy as such to to operating systems here but you can think of it as a global area um which is code specific that is for the um um data and control information for the the for the global processes that are that are happening in the database in addition there are um certain system processes itself which are the oracle processes um so oracle itself runs certain um runs several processes in the background which um um which comprises the server instance or the um or the oracle instance at any given point in time so oracle processes may comprise of the the server process itself or the server processes depending upon whether it is single threaded server or its concurrent server where it can um um where it can actually serve several different clients concurrently and then there are other background processes like audit trails and um um system monitors and and so on which um um which which belong to oracle so here is a schematic diagram of um the the system global area of course this um of an oracle instance rather and of course this diagram is um is in some sense um leaves out a lot of things that is for example the the program global area is not is not explicitly shown here are and the the system processes is not explicitly shown but generally um um all of this form the oracle instance  refer slide time  22  54  so um the the system global area which which comprises of the buffer cache and the redo log buffer um they they interact directly with with the database files and um user processes and and of course the server processes they they interact with the database actually through the sgr or through this system global area so so there are user processes which in turn which interact with the with the oracle server processes and um um oracle processes which could be server processes and possibly other processes ok and these in turn um rather than interacting directly with the database file um to be more correct this this diagram should be the the arrow here should should come through this thing that is it interacts with with the database um or with a disk through the system global area on the other hand if we are talking about database files um this this logical arrow is correct which which says that um logically the the server processes deal with the database files so let us look at oracle processes again in little bit more detail oracle processes can be um um can be categorized into different forms namely server processes server processes are those which actually handle request from user processes or um or oracle client or other application programs and so on which which send sql um queries to the server processes  refer slide time  24  24  there are two kinds of ways in which you can configure your oracle server you can either configure it as a dedicated server or as a multithreaded server and dedicated server essentially um um is a server that is dedicated to a particular client so so it can take one um handle one client at a time whereas a multithreaded server is concurrent in the sense that it can it can handle many client connections at the same time by by spawning different threads or um or processes if your kernel doesn t support threads and so on in addition to server processes there are what are called as background processes and these there are different functionalities for for each of these background processes um and there created for each instance of of oracle and one of the main functionalities of these background processes are to perform these asynchronous i/o that is there tho there these processes is that that mediate between the buffer cache in the system global area and the physical disk so um um it um these processes essentially perform what are called as read ahead or delayed write operation so so when you write on to a disk um um your your data goes into the buffer cache and in some point time the background process wakes up um and um asynchronous late at some point in time the background processes wakes up and then flushes all the buffer cache onto the disk similarly when um um um when when you give a read command um um your your data is read the the relevant data blocks are read from the disk but at the same time a background process is awakened which in turn reads several other data blocks also from the disk into the buffer cache again um for performance um considerations so so so the main functionality or the main um requirement of the background process is to provide parallelism for better performance and reliability  refer slide time  27  25  so there are different kinds of background processes which which take on different roles sum um that that perform these asynchronous operations and um usually it is it is good to distinguish between these these kinds of background process for example the database writer process so this is the this is the process that writes the buffer cache of the of the of the data blocks um from the buffer cache to the data files on on to the disk then there is the log writer background process which is which mediates between the log or the redo log buffer cache and um the um and the log files on the disk so so at data processes are are are managed separately from the log flushing process and then there is the check point background process a check point as as we saw in um in sessions on recovery based techniques this basically refers to a event in which all modified buffers are returned to the data files that is any log data or any data about transactions that have been committed before the check points can now be safely discarded so so we saw what are check points so this check points as we saw in the in the session on database recovery have to be taken at some points in time and there is a trade off between um um between um the speed or or the overhead introduced by the check pointing process versus how much um how much background data or how much historical data that you need to store in order to perf perform a recovery in the case of failures right  refer slide time  28  55  so um this check point background process is a is a process that that runs once in a while to um to perform this check pointing operation and as we saw in in our sessions on database recovery check pointing is a costly operation because you need to bring the database onto a quiescent state when you are performing a check point but there are other techniques by which there are other check pointing techniques which can obviate this problem so that you don t you don t have to stop all database operation when taking a check point then there are other process like the system monitor um which performs recovery of an instance that is it it identifies that when the when the system comes up it identifies that the system had crashed and um performs redo based log recovery then it manages storage areas and it also manages any kinds of recovers transactions that were kept during the recovery process then there is the process monitor background process it performs a recovery operations whenever a user process fails note that um um failure of a user process can also  noise  leave the database in an inconsistent state especially when um when it is in the midst of the transaction and especially even more when transaction data return directly onto disk so if the user process um um fails to complete the transaction then the database is still in the inconsisten inconsistent state and then you need to recover but then this is different from a system monitor um um in the sense that the entire system has not crashed so so other transactions are running um so um this recovery process has to be handled separately there is also a process called the archival process um which archives the online log files on to archival storage especially essentially for back up purposes um we we also some mechanisms of archival when we are talking about the database recovery techniques then there are recover recoverer processes which um um which essentially are useful when when oracle is used in a distributed database setting we have not um we have not as yet look into issues pertaining to the distributed databases but there are several issues when database is distributed across several different clusters especially to to um to detect that a particular transaction that that is spanning across different um machines as actually crashed or it is spending or um something of that sort  refer slide time  31  38  then there are other processes like dispatchers and lock processes where where dispatchers in multithreaded configurations the the route request from um from user processes to the appropriate or or appropriate or the available um server processes and then there are locking process which um which are in some sense again monitors that um that that monitors the locks between across different instances when when oracle runs in parallel server mode now let us look at sql in oracle what kinds of sql statements are handled um as we noted earlier oracle supports ansi sql in addition to ansi sql it has its own um extension to sql namely the pl/sql which um which helps the the user to embed sql statement into procedural um language  refer slide time  31  47  but as for as the support for the support for sql itself is concerned oracle supports all kinds of standards data definition statements and data manipulation statements and um it it supports transactional semantics um full acid semantics then um other kinds of um constructs like session control semantics and and even embedded sql statements in addition to support for sql oracle comes with an elaborate um mechanism for methods and triggers and these utilities where added to oracle as part of its object relational extension so oracle essentially is an object relational database where we have not again as yet seen it to what um constitutes an object relational database but um essentially the um um um a database that that uses object oriented semantics like method calls triggers and triggers are also some kinds of method calls and um inheritances associations and so on so a method um is actually a function that is part of the definition of a user defined data type um and um its its slightly different from the stored procedure um in the sense that  noise  um program essentially that that s a user program invokes the method by referring to an object of its associated type  refer slide time  34  03  but stored procedures are um are actually called from from sql statements um independent of any particular object as such its only the database um um whose context is necessary for stored procedures and oracle methods have access to attributes of their associated object and um um information about their types so um a method for example an oracle method for example would be a method in one of the schema object let us say schema object represents a table a method would be um um some code which says how do we access the next row in a table or how do we get the primary key field of a table and so on so get primary key or get next row or um or query or something of that sort which um um which abstracts away the the implementation or the storage structure of the table from the database engine as well so so this is different slightly different from the stored procedure concept because stored procedures are um um visible to to to um user programs or sql constructs while methods are essentially i mean the methods are visible to user programs only in an object relational context and not in a pure relational context and triggers are those methods which which have active rule capability that is which are called automatically in response to events and conditions let us have some more um um deeper look into the storage organization of um of the oracle database um we we noted earlier that storage is managed by what is called as the table space now table space is is the is the space that um manages the physical um um organization or physical allocation of um memory or disk blocks for a given database so a databases is divide divided into several different table spaces and um among these we can distinguish between um what are called what are called as system table space and the user table spaces  refer slide time  35  33  a system  noise  table space is is maintain for um maintains essentially all data that that are visible only to the oracle system while the user table space maintains data that that are accessible by user programs and data files are stored or or managed by the table space itself every oracle database contains um um unique system table space and this is labeled as system the in caps here and um the the data dictionary objects and and other system wide or information are stored in this system table space or managed by this system table space now the physical storage itself i mean how does the table space manages the physical storage the physical storage itself is divided into three different kind of kinds of storage what are called as data block or and extents and segments a data block um um corresponds to what is called as a page in in or data page in several other database systems it it is a smallest level of granularity in which um um in which data is stored  refer slide time  37  22  data blocks is is usually um data blocks usually have a one to one correspondence with um with a disk blocks or the granularity with which your um your disk accepts data most of the at least all of the disks accept data in terms of one sector size of the of the disk but there are some um disks with enhanced capability that can actually read and write multiple contiguous sectors at the same time now this multiple contiguous sector forms one data block and then extent the term extents is use to um refer to a specific number of contiguous data blocks so so um where where data block is the minimum unit of storage now a set of segments that are allocated to a specific data structure like like a table or an index or some thing like that is called as a segment so data blocks again each of these um lets look at each of these different um um elements in a little bit more detail a data block whenever um a particular data block is stored it contains the following information it contains the um some header and table directory information which talks about um which table it belong to and pointers to other um um um um tables and so on and there is a row directory information which talks about which are the specific rows that that are stored in this data block and the data that that pertains to a row and any um  refer slide time  38  34  free space information if if there is any free space information if there is any free space left out in the um in these data blocks and an extent as we said earlier is um is a set of contiguous data blocks so when whenever you create a table oracle allocates an extent to it and an extent um is extended in a sense that is um an extent is um um more extents are added to the table as an when extents become full and um  noise   refer slide time  39  10  and this extents are managed by segment that is we saw that a segment is a is a set of different extents and extents allocated in um extents can be allocated for any given data structure like like tables or indexes and so on so if i allocate an index um an extent to an index the extent remains allocated as long as the index exists and and is automatically freed when when the index is deleted  noise  and a collection of extent is what is called as the segments so there are different kinds of  noise  segments what might be termed as data segments and index segments and data segments um are are those which which store data or which which handle elements that that are stored in the data files  refer slide time  40  04  and data segments belongs to each what is called as a non cluster table and to each cluster what is a non cluster table um as we noted earlier a cluster um um is is a collection of machines on a lan which gives us the single system interface now a cluster table is some thing that is spread across the cluster that is a part of my table is in one machine and part of it is in a is in a another machine and so on and so forth now for every such part in a in a given machine a given data segment is allocated and index segments are are allocated for every index structures thats thats been created on this tables so every create index command will will allocate a index segment there are also other segments called as temporary segments which um which are used as temporary work areas um um especially to store intermediate results or or or to allocate data for virtual tables or or results of an intermediate query and so on  refer slide time  41  17  and then there are rollback segments that are used for undoing transactions  noise  so um um like mysql or or any other um dbms oracle um servers are usually um usually expose several different apis um so so that user processes or application programs can directly talk to the oracle dbms  refer slide time  41  53  so um the sql support or embedded sql support is provided for different languages like cobol c pascal and so on and in addition oracle has its own procedural language like called the pl/sql  refer slide time  42  47  let us have a brief look at how pl/sql looks like um and um what kinds of operations does it provide over and above um plane vennila sql ok pl/sql um as we noted earlier provides procedural constructs within which sql constructs can be embedded a pl/sql block can be can be divided into three different parts what are called as the declaration part executable part and the exception part um as you as you know if you notice correctly this is very similar to how um um a typical procedural language is also ordered except for this exception part and um out of these three parts its only the executable part that is mandatory so so the declaration part is where you declare variables or um and any other characteristics or functions that that that pl/sql uses and the um um um and the the set of statements or or the set of executions um is embedded within a begin and a end construct here where where you have the set of statements as well as the exception handlers um that is the executable part as well as the exception part written within this block that is within this begin and end construct  refer slide time  43  23  so the declaration part like we noted is used to declare variables and this could be variables that are used in both sql and pl/sql data types and the sql and the executable part contains both sql commands and pl/sql constructs like if then else or for loop and while loop and so on and so  refer slide time  43  41  so you can say if some condition then send this sql statement else send this other sql statement and so on and the exception part handles error conditions whenever an exception is raised and ex errors could be either system errors um um that are flat by the database itself or they could be user defined errors that are um um that are flat by the violation of for example some integrity constraint so let us summarize what what we have seen in oracle before having a brief look at ms access as well  refer slide time  44  54  oracle is a is a very popular commercial database management systems um its a it s a commercial one and its um in contrast to the mysql which is an open source dbms and the latest version 10g supports data grids um as part of oracle and oracle actually supports um not just relational it also supports object relational databases spatial databases and um database of sequences and so on so so different kinds of data can be can be stored in oracle rather than um rather than the pure relational database and it supports procedural constructs over sql in the form of pl/sql where you can write control flow constructs if then else and while loop and for loop and so on um and um oracle supports transactions and log based recovery it also supports clustering where where you can have a data file that is clustered over or a or a table that that is distributed over several machines in a cluster and several other features let us now briefly look into the second commercial database um whose case study that we are seen which is the microsoft access microsoft access we have we have included microsoft access as part of this case study mainly because it s a commercial data um um which which is kind of tailor made for ease of use for a for a non technical user and where every thing can be can be performed using graphical interfaces so um the the set up time and and the learning curve for for this database is much smaller than any other commercial database like oracle or other um database like db two or and even probably mysql  refer slid etime  46  08  so microsoft access provides the database engine and um and and a gui based thats the graphical user interface based mechanism for almost anything that is um for data definition data manipulation um um queries and reporting and almost anything to do with um um um to do with a database can be performed using graphical user interfaces  refer slide time  47  18  and but its not that its not that its only through the gui that that ms access can be can be accessed you can also access the database using basic that is um visual basic and and several other macros that um that you can write to to form of your own procedures and so on and one um one feature of ms access is it it provides hyper links that is url as a native data type which is generally not supported by other databases where where we can treat a url as a data type by itself and not as a string  noise  so what are the different components of the um microsoft access database its its called the engine of the of the microsoft access database all data that um for a given application are stored in a single file call which is um which has a suffix called dot mdb and um  noise  but but even then you can you can access these mdb files through through different through different ways using the open database connectivity mechanisms we have not talked about odbc as yet but generally odbc is essentially is a is a common interface by which you can access several different um databases as though they were um um data sources using a single common interface and ms access provides the support for data validation and concurrency control using logs but not um full acid semantics and it also provides some amount of query optimization what kinds of guis are available we have some screen shorts which will which will also see um of these guis later on  refer slide time  47  47  there are guis that are um available for specifying the structure of a table um formatting of the of the field layout any kinds of masks which talks about invalid inputs and validation rules default values data types index structures and anything to to do with database design can be specified using graphical user interfaces  refer slide time  49  07  you can also use guis for defining foreign key relationships and um you can also enable um what is called as an automatic inference of relationships that is whenever we use the the primary key field of of one table as an attribute of another table there is there is automatically ms access would infer that there is a foreign key relationship  refer slide time  49  43  so so the so the the it also provides this facility of an automatic inference of relationships between different fields then queries can be graphically posed um through a what is called as a query by example interface that is you can you can formulated you can graphically show how your query should look like that is um i should have these tables and these tables and these tables with these associations and so on as part of my query result and then say now give me the query ok  refer slide time  50  19  so um the the there is a qbe interface or query by example interface and as on when you if when you build your query as part of the qbe interface programmatically there is there is an um there is an sql statement that that is being created to which also you can switch to and change your query whenever um whenever you can and you can perform joins by um by drag and drop operations between tables and you can um form addition or and deletion of attributes to to tables using drag and drop then um there are what are called as expression builders where in which you can specify the the constructs of the where clause of your of your sql query um reports generation again can be performed in a gui fashion that is its an integral part of the access database  refer slide time  51  50  the access database doesn t just return a query and leave it at that you can actually specify how the return data should look like in the form of a report let us say in a page where um different elements of your query go into different parts of the page so so it it becomes a reporting mechanism and the the reporting um um whatever reporting um mechanisms that that you have created are tightly bound to the underlying database tables and and queries so there are different report generation wizards which help you in um in showing how a database report can be created and different styles in which um report can be created so um um many other additional features um what are called as cross tab queries which where in you can you can perform group by on specific um um values within a column and aggregating within within the within the group that that you perform the group by on and the the tables that you have generated or the or your database um um are available as ole objects ole is the is the well known object linking and embedding mechanism that are used across different microsoft applications therefore you can for example you can actually use your um um um database table as an embedded table in a in a word document for example or or in a power point presentation and so on  refer slide time  52  08  and there is a um um user level security based on login and password that is based around the nt server model and it also performs multi user it also um supports multi user operations and concurrent clients and so on so um to to summarize what we learnt on um ms access um ms access is is a dbms system that is meant primarily for the non technical end user where the the learning curve is much smaller than in using say oracle or mysql or or any other larger commercial database system it is shipped with the ms office with so so whenever you buy the ms office um suite of packages um ms access usually comes shipped with it and the main feature of ms access is the gui or the graphical user interface um which can help you um um perform almost any database operation um whether it is specifying your database or um formulating queries or reporting um um or any other kind of database operations so so it perform it gives support for graphical query specification and and also report generation and um these database are available as dsns or or data source names through an odbc interface um um um that is any database client that is compatible with odbc can can access ms access databases um as um as easily as it can access other odbc comply databases oracle also um um is also odbc comply and several most widely available databases are odbc comply  refer slide time  53  02  and these tables are are um database objects in in ms access are available as ole objects that can be linked or embedded within other ms office or or within other microsoft applications like say ms word or excel or power point or so on and so  refer slide time  55  27  so let us have a look at some of these screen shots here that that that show how a typical ms access or working with ms access look like so when you start a database um um this is what you get um that is you can you can start a database where um um where you can  noise  specify your database design using different views there what is called as a design view or you can create tables using a wizard where where it can where it will automatically fill up cer certain well known fields in um um in in a table or or suggest you for for certain um structures for for your database and or you can even create tables by actually entering data um using a form  refer slide time  56  03   noise  this screen shot here shows um um shows an actual form in which by which data can be populated so here there are two different forms for two different tables there is there is a table called mailing list and there is a table called assets and these are the different fields that that go into this this list so so you basically fill up this form and and the data is actually stored on to the database the last screen shot here um um shows how you can graphically build um a query using the qbe interface or the query by interface the the window here essentially says that or or the user who is formulating the query has said that the result of this query should contain these four different tables ok out of these four different tables these are the these are the fields that are required and um the user can specify and basically there there is a show um which can be clicked or unclicked which will say what to show in the in the table and the criteria can be specified here um which will say how to formulate the query so um um that is the beauty of this um ms access databases so with that we come to the end of the second case study  refer slide time  56  21  transcription by  vidhya proof read by  shobana database management system lecture # 34 data mining and knowledge discovery dr s.srinath hello and welcome in this session today we are going to look at um very interesting aspect of um or interesting application in which database technologies are used namely the um um the field of data mining and knowledge discovery in fact in recent years data mining has become um an extremely um um or field of um fields that eliciting an extremely large number amount of interest not just from researchers but also from um commercial domain um i mean the the commercial utility of data mining is probably um um is is probably of more interest than or or at least as much interest as as as the research interest that lies in data mining  refer slide time  01  25  and in addition to commercial um interest there is also number of public debates that um that that data mining has started which range from um which range from topics like legalities and um ethics and um the rights to certain um information and the rights to non disclosure of information or the rights to privacy and so on and so forth so so data mining actually is um in some sense has has opened a um has opened a pan door as box in um um and it is its only um it only time will tell whether um um whether the technology has has has given has been on an over all sense completely beneficial or or destructive in in nature but then um there is nothing beneficial or destructive about technology per say its how we use it um how we use technology which is what matters so so any way in this session we shall be concentrating mostly on the technical aspects of data mining obviously  refer slide time  03  59  and we shall look at the basic algorithms and concepts that that make up data mining and um um what exactly is meant by data mining and how does it differ from the traditional um traditional operations of databases or traditional way in which databases are used so the overview of this um or um this set of two sessions would be as follows let us first motivate the need for data mining that is why data mining and what are some of the um basic underlying concepts in data mining what what are the building blocks of of data mining concepts then we look at data mining algorithms um and several classes of this data mining algorithms um we will start with tabular mining as as in mining relational tables and we will look at classification and clustering approaches and we will also look at mining of other kinds of data like sequence data mining or mining of streaming data um and so on and um data warehousing concepts would would be covered as a different session all together first of all why data mining um from a managerial perspective lets first look at what a data mining has for the commercial world first before we go in to um looking at the technical aspects of data mining  refer slide time  04  33  if you um um where to let us say give an internet search or or talk to a manager let us say about um um about why he or she would invest in data mining um you would encounter a variety of answers one would say something like strategic decision making that is um um i look for some kinds of um um some ways or some patterns in um um or mind for certain um nuggets of knowledge um to to um understand some thing about um um something about strategic decision making or to help in strategic decision making um some body would say well it is um very useful for um something called wealth generation also although there is no precise definition of the term wealth generation um and you would say that oh data mining would would help me in um um understanding or making the right decisions that that can help me increase my financial portfolio or whatever some body would say well i would use data mining for analyzing trends analyzing how my customers behave or analyzing how um how um particular um market is behaving and so on and so forth and more recently data mining has been used extensively for security purposes especially mining network logs or network streaming data in order to in order to look for um abnormal behavioral patterns or um patterns that may be potentially um link to abnormal activity in the network or or in the system and so on so security is now um um um relatively recent and very important um very important application area of data mining so what is this data mining all about and why is this um so controversial and why is it um so interesting from a technical perspective at the same time data mining um is the the generic term um used to um look for hidden patterns in in data or hidden pattern in trends in data that are not immediately apparent um by just summarizing the data so if i want to look for certain patterns um let us say um if i have set of all students and their grades if i want to look for certain patterns um on how are the students performing over time or what is the um um is there a um is there a some kind of relation between subject a and subject b i mean if a student um does well in subject a he or she does badly in subject b or so on and so forth  refer slide time  06  33  such things can not be discovered um by just aggregating the data um by just saying what is the average or what is the summation or whatever and besides such things also can not be discovered by um i mean such things in a sense can not be within quotes discovered if we have to give queries that that finds out these aggregations that is um  noise  if we if we already knew what it is that that we are looking for then um its its not a hidden pattern any more we know um we know that that such a pattern exists that is students performing in subject a will not perform well in subject b we know that such a correlation exists and um and there is there is nothing hidden in the pattern anyway so um data mining um essentially has no query that is if you are performing a data mining on a on a database um we do not talk of a any data mining query in fact it is the um it is the mining algorithm that that should gives us something which we don t know now um how do um how do we say something which we don t know which is um which is um putting it in a very broad sense i mean which is which is making things so vague so data mining is actually controlled by what are called as interestingness criteria and um we we we just specify to the database that this is what we understand by um by an interesting pattern let us say correlation between performances in subject a and subject b or um some kinds of trends over a period of time this is what is interesting for us now find me something or find me everything which um i don t know about or which um um which are interesting according to this criteria  refer slide time  09  24  so when we talk about data mining um we have a set of data to begin with that is we we have a database and then um we give one or more interestingness criteria and the the output of which will be one or more hidden patterns which we didn t know exists in the first place now um  noise  given this model um we should say now um when we say patterns then then the obvious question to ask is what type of patterns what do you mean by patterns or what do you mean that this is or when do you say that some thing is a pattern and something is not a pattern if we have answer that we have to um ask two further questions that is um what is the type of data that we are looking at ok what kind of data set is it that that we are looking at and what is the type of interestingness criteria that we are looking um what do we mean by interestingness is it um correlation between something what exactly do we mean by interestingness so let us look at um the the different type of data that that that we encounter in in different situations the most common kind of data is the tabular data or the relational database right which is in the form of set of tables or um um now um slightly different multi dimensional form of database um so and its its very common that is any kind of transaction data that is let us say data array coming out from the database at um from an atm for example or the data coming out from the the transactional database at a at a um um at a railway reservation counter or at a bank or any place like that are all tabular in nature so um it s a it s a most common form of data and which is a rich source of data for um to be in mine  refer slide time  10  16  in addition to tabular data there are um spatial data for example where um data is represented in the form of either points or regions um which um which have been encoded with certain coordinates x y z coordinates so um each point in addition to having certain attributes also has certain coordinates and mining um in in this context also requires us to know what is the um importance of the coordinates system in addition to spatial data there are other kinds of data like say temporal data temporal data in the sense that where each data element has a time tag associated um with it so temporal data could be um um for example streaming data where um um network traffic or um set of all packets that that are flowing through a network um forms streaming data which which just flows fast and um where where each packet can be allocated some kind of a time stamp or something like activity logs your your database activity log is a is a temporal data there could be also be spatio temporal data that is um data that are tagged both by time and coordinates and other kinds of data like tree data which um for example xml databases or graph data where especially um bio molecular data or or volvoid web is a is a big graph data and so on then there are sequence data like um um data about genes and dnas and so on um and again activity i mean sequence is a kind of temporal data where um um where timestamp need not be explicit in in this um in sequence then text data the arbitrary text or multi media and so on and so forth  refer slide time  13  24  so so the several different kinds of data that we can um um that that can be the source which which from which we can extract or mine for unknown nuggets of knowledge similarly when we talk about interestingness criteria um several things could be interesting if cer certain pattern of events or certain patterns of data keep occurring frequently um then it might be of interest towards something that happens very frequently ok so frequency by itself is a interestingness criteria or or or an interestingness or or a criteria on which interestingness can be based similarly rarity if something happens very rarely um and and we don t know about it or let us say rarity is again a very um um very interesting pattern to be searched for when we are looking at um um say abnormal behavior of of any system or abnormal behavior of network traffic and so on so something that happens um um rarely that is that away from the norm um um is um is again an interestingness pattern correlation between two or more um um elements and if the correlation being more than a threshold is again interesting or length of occurrence in the in the case of sequence or temporal data and so on and consistent occurrence um consistency that is um consistency is a is different from frequency in the sense that overall um um in um in the set of all databases in the in the overall for the entire database a given pattern may not be frequent enough for example the the there could be a there could be one particular behavior pattern let us say one particular um um um customer comes to a bank every month at at the tenth of each month right so if we are looking for frequently um banking customers let us say this customer would not fi figure out in um in this algorithm because this customer comes only once a month whereas other customers could be coming many times a month however if we are looking for consistency in behavior then this customers behavior is for more consistent than someone who comes let us say um arbitrarily ten times the first month and once the second month and fifty times the third month and and so on and so forth so in terms of consistency in in his behavioral pattern across different months um um this pattern is is interesting even though its not frequent then repeating or periodicity um is is slightly similar to consistency except that a periodicity is um um i mean consistency is is across the entire set of um um across the across the entire set of months if you have if you have divided our database into months but periodicity the the time interval could could vary in in a periodicity of a pattern if a customer comes let us say a five times to the bank every six month um we may not be able to catch it as part of a consistent pattern anal analysis but if we use a if we use an algorithm that that detects periodicity of of several occurrence of events we will be able to detect it and similarly there are there is several other patterns of interestingness that um which one could think of now um when we talk about data mining usually there is a um um um sometimes a misconception and not completely but um usually there is a contention that data mining is the same as statistical inference um for a for many cases it is yes the the answer is true that is um um several concepts from statistics have been incorporated in to data mining and um um data mining software uses statistical concepts or many kinds of statistical algorithms comprehensively however um um there is a there is a fundamental difference between statistical inference and and data mining which um um which is perhaps the which is perhaps the reason for the renewed interest in in data mining algorithms um um and here is the um um general idea behind the data mining versus statistical inference um what do we do when we talk about statistical inference um statistical inference in techniques essentially have the the following three steps as is shown in this slide here  refer slide time  17  30  in statistical inference we start out with the conceptual model or what is called as the null hypothesis that is um we we first of all present ourselves or or perform a hypothesis about the system in concern that is we make a hypothesis that um um if um some something to the effect that um um if exams are held in the month of march then there would be um i mean then the turn out would be higher than if it is held in the month of june or something like that now based on this hypothesis um we perform what is called as sampling of the data set or or of the system now sampling is a very important um step in a statistical inferencing process there are there is huge amount of literature in to what is meant by correct sampling or what is called as a representative sample and so on now based on the sampling of of data set from the system we either prove or refute or hypothesis that is we um we show a proof saying yes this this hypothesis is true because statistical sampling of the system has has shown that this is true otherwise it is its false now um when we sample for example if you are if you are performing a statistical inference about user preferences or or lets say some kind of market analysis we um we present questioner to to different users based on our null hypothesis or based on our conceptual model now it is this set of questioner now this questioner has been created by our conceptual model so so this questioner already knows what to look for and the the proof or or the answers will either prove or refute the hypothesis but data mining on the other hand is a completely different process um or rather its it s the it s the opposite process  refer slide time  19  57  in data mining we have we just have a huge data set um and we don t know what is it that we are looking for um we are we are not we don t have any hypothesis we don t have any um um um null hypothesis to to begin with we just have a huge data set and um um we we just have some notions of interestingness now we use this interestingness criteria to to mine this data set and usually there is no sampling that is performed on the data set that is the the entire data set is is um scanned at least once by the by the data mining algorithm in order to look for patterns so there there is no question of sampling and there is no null hypothesis to to begin with so we just have a um weighed notion of an interestingness based on which we um um um we present an algorithm data mining algorithm over the data set out of this comes out certain patterns um um certain interesting patterns which form the basis for um for for forming a hypothesis so its some times also called hypothesis discovery obviously um um of course we can not discover complete hypothesis using just data mining but we we too discover patterns using which we can formulate a hypothesis so its in a sense its it s a opposite process of statistical inference let us look at some data mining concepts um um two fundamental concepts are of interest um um in in data mining especially in the in the core algorithms of of data mining especially the apriori based algorithms these are um what are called as associations and items sets um an association when we say an association it is a rule of the form if x then y as shown in this slide here and its denoted as x right arrow y ok  refer slide time  21  23  for example if india wins in cricket sales of sweets goes up ok if india wins in cricket then sales of sweets goes up ok so here x is india wins in cricket and y is the um is the um is the predicate that sales of sweets go up ok so um um  noise  we say that we discover such a rule if we are able to conclusively say based on analyzing the data that whenever india wins in cricket the sales of sweets go up ok um and um on other hand suppose if there is any rule of this form that is if x then y um then i can imply that if y then x ok  refer slide time  21  27  that is the the ordering of this rules is not important ok if india wins in cricket then sales of sweets go up if sales of sweets go up then india has won in cricket and so on which may be true or may not be true but but if that is the case then it is called an interesting item set that is its its just a set of item for example people buying school uniforms in june also buy school bags or you can also say people buying school bags in june also buy school uniforms so so its just a item set that is um um school uniforms and school bags are are a set of items which are interesting by themselves once we define the notion of a association rule and and an item set we now come to the concept of support and confidence that is how do we discover um um a rule to be interesting we say that a rule is is interesting in the in the sense of frequent occurrences of of a particular rule if the support for that rule is high enough that is the the support for a given rule r is the ratio of the number of occurrences of r given all occurrences of all rules ok so um we we look into the the exact or we will illustrate the notion of support in the in the next slide with an example where where it will become more clear  refer slide time  22  41  and when we say the confidence of a rule ok suppose i have a rule if x then y then the confidence of the rule is suppose i know that x is true ok the ratio um of all occurrences when y is also true versus when um um for for all other occurrences when when x is true and something else is here ok  refer slide time  23  46  so that is um um it s a ratio of the number of occurrences of y given x among all other occurrences given x ok so if if i know that x is true with what confidence with what percentage of confidence can i say that y is also going to be true let us look at some examples here  refer slide time  24  04  um let us say um um these are some item sets let us say these are data that have been distilled from um um purchases of different um um consumers over a period of time over in in a given month let us say ok so um the um the first consumer has bought a bag a uniform and a set of crayons the second consumer has bought books and bag and uniform the third one has bought bag uniform and pencil and so on and so forth ok now um um  noise  suppose i take the item set bag and uniform ok bag comma uniform what is the support for this item set now the support for this item set is look at all the um um transactions or or the rows here in which bag and uniform occur ok one two three four and um five uniform and bag ok um um out of a total of ten rows five of them have bag and uniform occurring in that  refer slide time  24  34  therefore the support for bag and uniform um is five divided by ten which is point point five that is with a this dataset supports the assertion that bag and uniform will be will be bought together um um with fifty percent support that is point five as a as its support ok what is the confidence um that um what is the confidence for the rule if bag then uniform that is what is the confidence by which we say sup whenever somebody buys a bag they also buy uniform ok for this we have to look at the set of all item sets or the set of all transactions or rows here in which bag and uniform bag occurs rather not just uniform ok in which bag occurs so bag occurs in one two three four five six seven eight different rows ok out of which bag and uniform have occurred in five different rows therefore the confidence um for for this assertion or this association rule is five divided by eight which is um  noise  which is about sixty two percent that means if some if some consumer has bought a bag then with sixty two percent of confidence or sixty two point five percentage of confidence we can say that the the consumer will also buy um um a uniform a school uniform along with this so the question now is how do we mine or or how do we find out the the set of all interesting item sets and the set of all interesting association tools now look at it um um look at let have a look at this um um previous slide  refer slide time  26  50  once again now the association rule when we talk about association rules um we have we have just rather when we talk about item sets first we we just saw a single item set having two different elements here ok but that that need not be the case bag by itself could be an item set a single element item set uniform by itself could be a single element item set crayons could be a single element item set or let us say bag uniform and crayons could be a three element item set and so on so item sets could be of any size size one size two size three size size n any um um set of elements now we have to find the set of all um um um item sets that is the set of all items that are bought together and and that have that have been together frequently as part of this transaction log here  refer slide time  26  48  now um now how do we do that now um there is a very famous algorithm called the apriori algorithm um which um which which performs such a discovery process that is um a discovery process for all frequent item sets in a very efficient manner the simple idea behind apriori algorithm it is shown in this slide here however let us not go through the slide in in in a lot of detail um um since um it will be um more easier to explain apriori through an example the the idea behind apriori algorithm is that um the essential idea behind an apriori algorithm is that suppose i have any n element item set let us say suppose i have any five element item set that is interesting or that is frequent ok so if this five element item set is frequent then it shou then all sub sets of this um item should also be frequent this this seems obvious but this is a very important conclusion um um or it s a very important observation in the apriori algorithm that is if i discover the set of all one frequent item sets that is the the set of all item sets of size one which are frequent then um there is no need for me to look at other item sets when i am looking for um um two frequent item sets that is um the the set of all item sets of size two which are frequent will be made up of combinations of set of all item sets of size one which are frequent so let us illustrate the the process of apriori with an example let us take our um  noise  um um let us take our consumer database again um the the previous consumer database again where we have um um consumers buying several school utilities like bags and school bags and school uniforms and crayons and pencils and books and so on and so forth  refer slide time  29  22  now um suppose we set um um when we say or when we ask the apriori algo miner to to mine for all interesting item sets we have to um the interestingness criteria here is frequency that is frequent occurrence now frequency is um um or interestingness here is parameterized by um a a threshold parameter which is called the minimum support ok or min sup ok so so let us say minimum support is point three that is we term um um an item set to be interesting if its support is at least point three or greater right now given this what are all the interesting one element item sets what is that mean to say what are all the one element item set which one element item sets occur at least um at a rate of thirty percent or more ok now this database here or this data set here has a total of ten rows therefore we have to look at all one element item sets which occur three or more times ok so given this we see that all of these are interesting that is bag uniform crayons pencil and books bag occurs much more than three times uniform also occurs more than three times crayons also occur um um more than three times and so on ok so all of these um um elements here occur more than thrice which therefore all of this one element item sets have  noise  have a have a minimum support of thirty percent or more ok now from this suppose we have to look at the set of all interesting two element item sets ok now um um um how do we  noise  how do we build the set of all interesting two element item sets we um just look at all possible combinations between one element item sets therefore we have bag uniform bag crayons bag pencil bag books uniform crayons uniform pencil uniform books and so on and so forth right now out of this for each such two element item set that have been created we have to see how many times they occur in this um um in this data set now we see that its only these set of um um combinations which have a minimum support of point three or more ok so for example bag uniform bag crayons bag pencil and bag books all of them um um along with bag are interesting however um let us say uniform and books is not interesting that is it it it doesn t occur more than thrice um so so let us see how many times uniform and book occur uniform and books occur once ok and um second one twice here ok so so they occur only twice but but we need a minimum support of three times ok so um so that s not interesting similarly  noise  um a pencil and uniform so uniform and pencil is agai is again is not interested ok so therefore um um we have um filtered away or or we have thrown away certain item sets from our um um from our exploration here and identified only a smaller subset of um um the set of all possible combinations of one element item set now from this um if we have to look for all three element item sets we have to um generate the set of all candidate three element item sets what are the candidate three element item sets um perform a union across all possible um combinations of of these interesting two element item sets to create all possible um distinct three element item sets ok and then look for those three element item sets which occur at least three times or more in this database given that we see that there is only one three element item set that is bag uniform and crayons that is interesting that is that that occur at least three times or more or that has at least that has um um um support of at least thirty percent in this um in this data set right  refer slide time  33  56  so as you can see the the apriori algorithm um um you can visualize the apriori algorithm in the form of um um let us say an an iceberg ok such queries are also called as iceberg queries when when when given on to databases that is um at at the at the base there are large number of one element item sets but once we start combining them together um we start getting smaller and smaller um numbers of of combinations and um we we peak out at at a very small of um large item sets which are frequent so the the the beauty of the apriori algorithm is that the for for every pass it does not need to need to go through the entire data set um um it does it does not have to parse through the entire data set it only needs to consult results of the previous iteration or results of the um um um or or item sets that are of one um element one lesser than the present iteration in order to construct candidates for the present iteration so um given this algorithm here let us go back  refer slide tiem  35  30  and um look at the apriori algorithm given the explanation here um with an example let us go back and look at the apriori algorithm which which will now be a little more easier to understand initially um we start with a given minimum required support s as the as the interestingness criteria ok now given um minimum support s as the interestingness criterion first we all we search for all individual elements that is one element item sets that have a minimum support of um s ok now um we start we go into a loop where we start um um looking for item sets of sizes higher um greater than one ok so from the results of the previous search for i element item sets ok um um search for all i plus one element item sets that have a minimum support of s this in turn is is done by first generating a candidate set of of i plus one item sets and then choosing only those among them which have a minimum support of s ok now this becomes the set of all frequent i plus one  noise  element item sets that are interesting so this loop is repeated until the item set size reaches the maximum that is there no more candidate elements to be generated um for the next item set or there no more frequent um um item sets in the current iteration  noise  now that was about item sets um the the a property of item sets is that there is no um um i mean you you basically consider item sets um as as one entity that is there is no ordering between the item sets that is it does not matter if um if um somebody buys a bag first or a uniform first or a or a crayon first or whatever as as long as um the the only thing that we are going that we infer from this is that um um the item set bags uniforms and crayons are are quite lightly to be bought together in in in one piece therefore if i am let us say a super market um vendor i mean um um someone um having a super market then it would make sense for me to place bags and school uniforms and crayons um next to each other so because there is the there is a higher probability that all three of them are are bought together but when we are looking for association rules um we are we are also concerned about he direction of association that is um um there there is a sense of direction saying if a then b is different from if b then a ok so association rule mining requires two different threshold the minimum support as in the item sets and the minimum confidence um um with which we can talk about a with which we can say um or determine that um that a given association rule is interesting  refer slide time  37  22  so how do we mine association rules using apriori again um um we shall do the same thing like we did in the past um we shall come back to this algorithm or or the general procedure after we have illustrated an example by which we can mine um apriori um using apriori algorithm by which we can mine association rules  refer slide time  38  47  now um  noise  the the main idea is the following now um um use the apriori algorithm and generate the set of all frequent item sets ok so um so let us say we have generated the frequent item set of um um size three which is namely bag uniform and crayons with with a min sup or or of point three that is a minimum support threshold of thirty percent now um  noise  this bag uniform and crayons can be divided into the following rules ok if bag then uniform and crayons or if bag and uniform then crayons or if bag and crayons then uniform and so on so forth right  refer slide time  39  38  now um um what is this thing mean this thing means that when a customer buys a bag ok then the customer also buys uniform and crayons and this rule means that if a customer has bought a bag and a school uniform then the the the the customer will also buy a set of crayons or if a customer has bought a bag and a set of crayons then the customer will also buy a school uniform and so on ok now we have got all of these different association rules now um each of these association rule has a certain confidence um um in this um um based on this data set now um what is the confidence for for each of these rules what is the confidence for the rule um if bag then uniform and crayon that is if a customer buys a school bag the then here she is um um will will also buy a school uniform and a set of crayons um in order to calculate the confidence of this we have to first look at which are all the item sets here that have bags that is where where the customer has bought a bag ok so there are one two three four five six seven eight different um entries where customer has bought a school bag ok now among these eight entries um in how many different  noise  entries did the customer also buy uniform and crayons one and two three ok so so there are three different entries um um  noise  three different instances out of eight instances where um um where this rule holds therefore whenever a customer buys a bag one can say with um three by eight or or thirty seven point five um percent of confidence that the customer is also going to buy a set of school uniform and crayons similarly  noise  we can calculate the confidence for each of these other association rules like this is point six point seven five point four two eight and so on and so forth now given a minimum confidence as our um as a second threshold then um um and suppose we we say that the minimum confidence is point seven then whichever the rules that we have discovered every rule that has confidence of at least seventy percent or more  refer slide time  40  47  that means we have discovered the the following three rules bag if bag crayons then uniform uniform crayons then bag and crayons then bag and uniform ok what is that mean in in plain english it means that um people who buy a school bag and a set of crayons are likely to buy a school uniform as well right that is  noise  bag and crayons implies uniform ok similarly people who buy a school uniform and a set of crayons are likely to buy a school bag ok that is um here somebody buys uniform and a set of crayons then they are they are also likely to buy um a a school bag similarly if somebody buys a set of crayons then they are very likely to buy a school bag and a school uniform as well right  refer slide time  43  09  so um the that is here that is somebody buys crayons then with with seventy five percent confidence one can say that they also buy bags and um um school uniforms so again its a it s a question of direct marketing or whatever if somebody is interested in crayons then um um you might be reasonably sure that they are also interested in a bag and a school uniforms so on ok now  noise  so so let us look at look back at the algorithm here  refer slide time  43  41  for um for for mining association rules simple um mechanism for mining association rules is first of all use apriori to to generate different item sets of of different sizes and at each iteration we can divide each item sets in to two parts an an lhs part and an rhs part the the left hand side part and the antecedent and precedent that is and the right hand side part so this represents a rule of the form lhs implies rhs then the confidence of such a rule is support of lhs divided by that is um support of the entire thing divided by the support of lhs  noise  that is support of um um lhs implies rhs div divided by support of lhs will gives us confidence of this rule so and then we discard all rules whose confidence is less than minconf  noise  so um now let us look in to um the question of um um  noise  how do we generate or how do we prepare um a tabular data for association rule mining or um um or let us say um um item set mining and so on now um because we use let us say relational data set relational database you might have observed that um or you might have um um got a little doubt when we have been considering a data set like this  refer slide time  45  19  there is something peculiar about this data set um what is peculiar about this data set here the the peculiarity is that um it looks like every um consumer coming to this um store is buying exactly three items which is very unlikely in fact what is more um what is more um um practical is that um this set this data set um um contains records of variable length that is one customer may may have bought just um um two different items where whereas some other customer may have bought ten different items whereas a third customer may have bought only five different items and fourth customer may have bought only one item and and so on and so forth ok  refer slide time  46  34  so so it is not possible to um represent this item set like a table like like like a well form table like this because because it basically is a um um is a is a set of all items of different lengths in fact the best way to represent this would be um in a in a normalized form let us say in  noise  in a in a database where um for example the same bill number here one five five six three one five five six three  noise  both of this refer to the same customer that is its the same customer who has bought books and crayons ok and this is not completely normalized because um date is not really necessary here but um but never the less um here all of these records are of are of uniform length um um they they are um um if you if you order this based on the set of bill numbers then we get the the set of all different transactions now um depending on what we are looking for this this ordering might um might make a difference how does this ordering make a difference here um when we are looking at data set like this  refer slide time  00  47  19  suppose given a dataset like this here performing group bys on different um fields will yield as um different kinds of behavior um data sets ok so what what does it mean suppose let us say we we perform a group by based on the bill number ok  refer slide time  47  37  so um suppose we perform a group by on the bill number on on this table then um each group will represent the behavior of one particular customer that is one bill represents one or one bill number represents one particular customer or one particular transaction right so um  noise  suppose we group by based on bill numbers and then perform apriori across these different groups then we would be getting frequent patterns across different customers on the other hand suppose we group by um over date ok so rather than bill number ok so um all um transactions happening on a given date will come in to one group and all transactions happening on another date will will come in to another group but um a given date may have transactions from several different customers but all of them or now group by grouped in to one single group ok so um and suppose we run apriori over this set over this different groups then um um we would um  noise  um then we would actually be looking for frequent patterns across different days that is across the different dates ok so um um we have to interpret what um what we mean by something that is frequent based on how we have ordered the data if um if we have ordered the data over um um different customers then it would show aggregate behavior over the set of all consumers who are um um with whom you are interacting with on the other hand if you have um if you are running apriori or or if you have performed group by over um over dates then it would show you aggregated behavior over a given time period rather than over the set of all um over the set of all um customers well it also includes the set of all customers but what is more important here is that um um how does the behavior um or how has the behavior changed over time so so if something is frequent over time um it means um that it is uniformly or in some sense consistent over over this entire period of time so um  noise  let us summarize what we have learnt in this session we started with the notion of data mining and like i said in the beginning data mining is is a is a is a very interesting um sub field of um of databases which um which has elucidated a lot of interest not just from researchers or and not just from the technology perspective but from several other perspectives like um defense perspective or um security perspective commerce that is business perspective and so on so and there there are several debate that that um that have raged on whether it it is right to use data mining to to to look for certain behavior pattern for example would it be right if um if a government uses data mining um over um over let us say um the the set of all different activities of um of people and find out the the the behavior pattern of of any particular individual and so on so um um um and um there pros and cons of on on both sides of the debate one one would say for security reasons it is it is right to look for um behavior patterns and one would say well for privacy reasons its not right to look for behavioral patterns and so on and so forth so it s a it s a it is a topic which is very much pertinent and has spond um spond a huge amount of interest in from several different areas and data mining is um is in some sense i called it as sub field of databases but that s not entirely true in a sense that data mining and knowledge discovery many would claim um is is a field in itself that is um it relays on database um concepts as well as several other concepts like learning theory or statistical inference and um and several other concepts in order to perform data mine so don t be really surprised if one would say that um a data mining is a is a is a complete field in itself and its only associated with databases not really sub field of databases but anyway um data mining as we said is the um is the process of discovery of previously unknown patterns in the sense that we have not really sure what is it that that that database is going to give us or what new um pattern or what new nugget of knowledge so so to say is we we are going to learn as part of the data mining process as a result there is there is no query as part of a data mining process that is um um a data mining algorithm is based around um one or more interestingness criteria rather than a given query  refer slide time  50  11  and we saw that um in um conceptually it is um it it it is in some way the opposite of statistical inference where um where we start with a null hypothesis and either refute or prove or hypothesis by sampling statistical sampling of of the population while here we we don t start with a hypothesis but the end result of of the dan data mining process is the set of um patterns which can help us in formulating a hypothesis we also saw the notion of association rules and item sets as well and and the concepts of support and confidence so um and two two different algorithms the apriori algorithm for for for mining frequent item sets and from which we we um also um saw the apriori algorithm for mining association rules in the next session on data mining we are we are going to look at several other algorithms like say classification or um discovery so that s brings us to the end of this session thank you transcription by  vidhya proof read by  shobana database management system lecture # 35 data mining & knowledge discovery part ii dr s srinath hello and welcome to this second session in data mining in the previous session we um um saw what this concept of data mining was all about and um we saw some very fundamental concepts of um um item sets and association rules and how do you discover particular patterns in an item set that is how do you discover something that you don t know from from a data set um using the concept of support and confidence and so on  refer slide time  02  06  so so so essentially you you you give a particular interestingness criteria and then you start um distilling out certain patterns from the data set let us move on um further in this session where we will briefly look into um some fundamental algorithms or some very simple algorithms on um on different kinds of data mining activities namely in the um um namely in discovering classification trees or or discovering clusters of of properties of of data and and mining sequence data the data of different sequences or or stream data mining and so on let us briefly summarize what um data mining was all about  refer slide time  02  23  data mining essentially is the concept of or is the idea be of looking for hidden patterns and and trends in data that s not immediately apparent by just summarizing the data right so um um when when we say hidden patterns its ess essentially means that something that we don t know about um there is nothing hidden if you already knew such such a pattern existed um in the data base so um um so um so in in a data mining setting there is no query right but um we use um we use the concept of an interestingness criteria that is we use let us say frequency or consistency or rarity or whatever be be the interestingness criteria and certain parameters um um define each of these interestingness criteria like frequencies um um is parameterized by support and confidence for association rules and um and just support for um um item sets and so on  noise  and again there are there are different kinds of um data we we can think of tabular data spatial data temporal data tree data graph data and so on and so forth so so today or in this session um we shall look at specifically at sequence data mining and um streaming um or mining streaming data and in addition to other mining algorithms  refer slide time  03  24  and of course type of interestingness itself could be varied that it could we could talk of frequency as um um frequent patterns as being interesting or rare patterns are as being interesting and so on  refer slide time  03  48  now let us move further um um from here and look at the concept of classification and clustering that is um um  noise  discovering classification tree and discovering clusters um within a given data set  refer slide time  04  03  now what is the difference between classification and clustering intuitively they they both seem do the same thing that is when you classify a given data set into different classes or whether you cluster a given data set into different clusters but essentially few um observe closely classification maps data elements to to one of a different set of pre determine classes based on the difference differences between data elements that is um um if um data element a and data element b belong to different classes if they are different enough on the other hand clustering groups data elements into different groups based on similarity between elements within a single group and some times its um its also the case that in a in a classification we um we have we know the classes apriori um we know what are all the different classi classes into which data can be classified into um and some times in clustering we don t know how many clusters we are going to get um um before the clustering process begins  refer slide time  05  19  let us look at um um mining with rela um in relation to classification techniques rather we are not interested here in the idea of um classification itself but um um we are interested in the idea of discovering classification what is it meant by discovering classification um discovering a decision tree the how how um or which which decides how to classify data sets into different classes let us take a small example um its discovering this cla um this algorithm is best represented by an example so let us take a small example and um um and see how we can discover a classification tree let us say that we have data about different cricket matches that have been um played over the last several years now we have a um um let us say in a given city ok now the question is this city is notorious for um for it rains for its rains and um um its unpredictable weather now in the past several times um um play had to be abandon that is um play was as to be over continued as was or were or was abandon and so on ok now we have data like this from from different um data sets when it was sunny and the temperature was thirty degrees play was continued when it was overcast and the temperature was fifteen degrees play wasn t continued when it was sunny and temperature was sixteen degrees play was still continued um and so on ok so so in some time um play was continued and some times um um play was discontinued it its no now what is the the the classification problem is can i um um can i classify weather conditions which is a combination of the outlook and the temperature into one of two classification classes that is whether we are going to play um um or play is going to be discontinued that is um what is the criteria when um um when play was discontinued and what was the weather criteria when play was continued  refer slide time  07  32  so there is a um um um well known algorithm called hunt s method for for identification of decision trees and um um like before let us first look at an example of um how we identify a decision tree before um looking at the algorithm itself  refer side time  07  53  the the  noise  the way of identifying decision tree is is quite simple first of all because this temperature field here  refer slide time  08  04  um is is a numeric value it could it could take um several different values and which might be of no interest to us so let us um um let us perform a hand classification of this um of this numerical values into different classes ok  refer slide time  08  19  so what we have done here is that temperature is now classified into three different um um classes warm chilly and pleasant ok so whether whether the temperature was warm whether the temperature was chilly and or whether the temperature was pleasant based on um um dividing the the set of temperatures into different classes now um  noise  first of all because there are two values here that is there are two fields here outlook and temperature both of them um both of them will affect the decision on whether we are going to play ok so how do we know what is the best or how do each um um how do each parameter affects the decision whether to play or not um let us start by looking at one parameter after another ok first let us look at sunny ok now um um if you see here that whenever the the outlook was sunny um the the cricket match was played it was not um abandoned ok it is sunny only twice here and in in both cases cricket match was played therefore we can directly conclude that if the weather is sunny ok regardless of whether um the the temperature is warm or whether the temperature when the whether the temperature is chilly or whatever we can conclude that play will continue ok the the play is not going to be stop ok on the other hand let us look at cloudy here ok now when it is cloudy here play was um continued in one case or rather in two cases and when it was cloudy here play was discontinued in one case ok so from cloudy we are still in a what is called as a bivalence state that is um it is still yes or no may be or whatever may be yes may be no we still don t know ok similarly when the outlook was um was was overcast let us say here um um it has it was overcast and they didn t play here um once when it was overcast they actually played and once more when it was overcast then they didn t played ok so from overcast we we still say yes or no we we don t know um um whether they are going to continue play or not  refer slide time  10  38  so um what we can do now is we can um um safely remove the first rule from our um um from our process that is this is a rule that we have we have already discovered that is when it is sunny they are going to play so so now let us remove this rule from our um from consideration and take these two rules ok now because from cloudy and over cast we are still in a bivalence state um um  noise  we are um we have to ultimately reach to a state where we can remove this bivalence that is we can either conclude yes or no conclusively right so um we will try to um um we will we will try to now introduce the second parameter temperature into this um into this state here to see whether we can remove this uncertainty ab about yes or no ok the first case the uncertainty is already removed so so there is nothing we need to do any more ok  refer slide time  11  32  so we have introduced let us say here  refer slide time  11  41  for for cloudy we have introduced all three possible cases warm chilly and pleasant similarly for overcast warm chilly and pleasant ok so let us take cloudy and warm ok so whenever it was cloudy and warm there is only one case here play was continued yes so so so basically we have um removed the bivalency that is we we have conclusively stated that whenever it is cloudy but the temperature is warm um play is going to continue we are not going to abandon play on the other hand um um whenever it was cloudy and chilly ok um um there is only one case here where where play was discontinued so again there is the the bivalency is removed that is cloudy and chilly means no ok so um we can again conclusively state that the play is is going to be abandon if um if the outlook is cloudy and and the temperature is chilly ok similarly um um when it is cloudy and pleasant ok cloudy and pleasant is here and um um ya there is there is only one case here cloudy and pleasant um um is yes so so when when the outlook is cloudy but the temperature is pleasant we can still conclude that they are going to continue play similarly um um overcast and warm there is no entry at all so so so we don t know the there we cant decide anything ok so overcast and warm decide remains as it is and overcast and chilly um gives us no that is play is going to be abandon similarly overcast and pleasant gives us yes  refer slide time  12  53  so um so effectively we have removed this bivalency that that existed um um here  refer slide time  13  20  when it was cloudy and overcast and um decided or um or came to know when under what conditions um play is going to be continued when it is cloudy and under what conditions play is going to be discontinued um when it is cloudy or and the same thing for overcast right so therefore what we have actually done is we have discovered this um decision tree right so initially we were in a bivalent state that is we don t know um um play is going to be continued or discontinued ok now in this bivalent state we were told that the the the outlook is sunny then we can immediately conclude um yes we are going to play today ok  refer slide time  13  53  on the other hand if you are um in this bivalent state here if you are tol if you are told that the outlook is cloudy we will still be in a bivalent state we still don t know whether they are going to um whether the play is going to be continued or not ok so um um so so we ask for more information ok and then when you find out that the temperature is pleasant let us say for example then we say that yes the the play is going to continue on the other hand if the temperature is chilly then then we have reasons to believe that um um play is not continued that is um the the data set tells us that play is going to be abandon and so on so what we have got here is a tree data structure um um where from a bivalent state we we eventually go into a univalent state that is um um a state were the uncertainty is removed and then we have concluded or we have classified um um this um this play into two different classes that is um yes or no that is play is going to be continued or play is going to be abandon so let us look back  refer slide time  15  11  at the um um algorithm little bit as how to how to go about this suppose we are given n different elements in our case um in the example that we right now saw n n was equal to two that is outlook and temperature right so suppose the suppose we are given n different element types and m different decision classes in this case again m was two that is yes and no ok so um what we do in this loop here for each of the different element types we keep progressively adding element i to the i minus oneth um um element item sets from the previous iteration and then whenever and then we see whether we can decide ok identify the set of all decision classes for each such item set if the item set has only one decision class that means we have already decided right so um this is done remove that item set from subsequent iterations otherwise keep continuing until you finish all your element types and of course it could well be the case that um even after finishing all my n different item sets i may not be able to reach a conclusive decision  refer slide time  16  07  so it might well be the case that um um when it is over cast and um and chilly um some times they actually played and sometimes they didn t play and so on so that again the there are several methods to deal with such kinds of um um indecisiveness for example to to to use probabilities that is um um this is going to or or some kind of fuzzy classification where we say that outlook is overcast and temperature is pleasant then they are going to play with a probability of ninety percent or something like that  noise  so let us look further into um um what are some clustering techniques  refer slide time  17  03  now um  noise  what is meant by clustering or or how does it differ from classification um we saw earlier that um there is a um philosophical difference between classification and clustering um probably not in the n result but but philosophically there is um um there is a there is a different of course even in the end result there are differences but um the the most marked difference is is philosophically that is um  noise  classification is based on amplifying the differences between um um different elements so as to make them belong to different classes on the other hand clustering is based on amplifying the similarities between elements so as to form them into different clusters  refer slide time  17  48  so um clustering essentially partitions the data sets into um several clusters one or more clusters or or equivalence classes and um  noise  um the what is the property of a cluster or an equivalence class um essentially the property here is that the similarity among members of a given class in a cluster is much more than similarity among members um across clusters ok so members belonging to the same cluster are much more similar to one another than um they are to some members belonging to some other clusters and there are several measures of um similarities and most of which are reduced um to geometric similarity by um by projecting these data sets into into hyper cubes or or n dimensional spaces and then use some kind of euclidian distance or other kinds of distance measures like manhattan distance and so on several um um distance measures to to compute the similarity  noise  um let us look at the first kind of um um clustering algorithm which is called the nearest neighbor clustering algorithm  refer slide time  18  58  its um this is quite simple that is this this clustering algorithm takes a parameter um called threshold or or the minimum um distance or the maximum distance t between members um of a given cluster so given n elements that is x one x two to x n and a giv and given a threshold t which which is a maximum um um distance that can exist between elements of a cluster um we can  noise  we can um find clusters in a very simple process initially the the set of clusters is is a null set ok then  noise  for for each element um let us say j equal to one here and j goes to until j j plus one for each element find the nearest neighbor of xj ok now let the nearest neighbor be in clust in some cluster if it is already in a cluster if it is not in a cluster then then fine you can just create another cluster by yourself ok so um suppose the nearest neighbor is in cluster m ok now if the distance to nearest neighbor is greater than t that is if it is greater than threshold then we know that there is no other element that is nearer to me um with with a distance less than t therefore i should belong to a new cluster so so then create a new cluster and increment the number of clusters else assign um it to the cluster m were the nearest neighbor of it existive so as simple as that that is given a small threshold you you basically start partitioning your um um set of elements into different clusters um based on which is the nearest neighbor to to a given element if the nearest neighbor is within a um within this threshold distance then i join the cluster otherwise i belong to a new cluster there is another kind of clustering techniques which is again quite popular which is called as the iterative partitional clustering  refer slide time  21  03  this is another clustering technique where um um um this this differs from the the um nearest neighbor technique in the sense that here the number of clusters are um um fixed apriori um in the nearest neighbor technique or in the nearest neighbor clustering techniques the number of clusters are not fixed apriori that means you don t know how many clusters you are going to get given a particular threshold and a data set right so um um this is very much unlike classification where where we know the classification um where we know the classes under which data can be classified into in iterative partitional clustering um the the number of um clusters are already known apriori and then um um we are trying to rearrange the clusters that is but but that is we don t know um how many or what elements belong to which clusters ok so given n different elements and k different clusters ok um each with a center ok what do we mean by a center here um its it s the centroid in the in the statistical sense for example um um it it could be the first centroid that means um if if a cluster has um um has several features the average of of of all these features along all different dimensions will form the centroid of of a of a given data set right so let us say we have k clusters each with a center ok now um um assign for each element assign it to the closest cluster center ok so each clusters as a cluster or or a centroid for each element find out which is its closest cluster center ok and assign it to that cluster after all assignments have been made compute the cluster centroids for for each of the cluster that is compute the average of all the points that that um that um um that made up this cluster and possibly this will shift the centroid to a different um um to a different location ok so um once this centroid is shifted to a different location the the nearest centroid or or the nearest cluster center will now differ for for each um um for each element therefore we keep repeating these two steps until the new centroid i mean with um with a new centroids that that are formed until the algorithm converges that is the until the algorithm stabilizes so the so that the centroids will stop shifting and then the we know we know that we have we have found the exact or we have found the um um best centroids for each of the clusters each of the k clusters so iterative partitional clustering essentially um um is a technique were um some thing like saying suppose i have a data set um um and and i say that suppose i want to create ten different clusters out of this out of this data set where would these clusters lie and so on ok on the other hand um um a nearest neighbor clustering technique would would say suppose i have this data set and suppose i have a maximum distance um um a threshold distance um of five between um um between elements that can lie within a data set then how many clusters will i find ok so um um um and whereas in the in the iterative clustering algorithm we are we are interested in were the clusters are going to be where are the cluster centroids of these ten different clusters that that are going to be formed let us now move on further and look at different other kinds of um um data sets we have been  noise  looking into um um until now we have been looking into  refer slide time  24  52  let us say the um um tabular data as in um as in apripri or um um um or association rule mining or some kind of multi dimensional data tabular data can be treated as multi dimensional data as long as there are they belong to certain ordinal classes which is of course beyond the scope of um um this session here that is how do we convert um um um a tabular data into multi dimensional data but any way as long as the data can be um um um data can be um um converted to to multi dimensional form um we can use clustering techniques for for clustering them into different um um clusters similarly um um tabular data can be use to also um um infer classification trees let us now move on to different kind of data what is called as sequence data what do we understand by the term sequence sequence is essentially a collection of data elements um um where wherein its not just the collection it is its an ordered collection where that is where where in the ordering matters that is in a sequence each item in a sequence has an index associated with it that is some kind of a subscripted um um um element each element is a subscripted element so so is a this is the first element this is the second element and so on so when we say um we we have a k sequence it means that we have a sequence of length k that is there are k different elements in a particular order in this now  noise  um and um um um there are different kinds of sequence data like for example um any kind of transaction log over a period of time ok um or let us say some kind of web browsing um logs um http logs or dna sequences or um the the patient history the the medical history of a patient um over time that is how is the history changing or what kinds of events happened and and so on so all of these are sequence data so let us look at some definitions um in mining sequence data and which which help us in formulating algorithm for looking at patterns in in sequence data  refer slide time  27  13  now um when first of all when we talk of a sequence a sequence is essentially a list of of item sets of finite length that is each element in a sequence need not be atomic it could actually be a set it could actually be a different um um set of items so for example this is the sequence the first element here is pencil pen ink or pen pencil ink ok the second element here is pencil ink the third element is eraser ink and so on and the fourth element is ruler pencil and so on right so this sequence essentially could um um for example could um um could be denoting the purchases of single customer over time in this particular store or whatever ok so let us say the the customer came in the first month and purchase these three things the second month you purchase these two and the third month you purchase these two and so on in some stationary store ok now um the order of items within an item set here does not matter but the order of item sets itself matters that is this is the first month this is the second month this is the third month so the the position of this item set matters but the position of items within an item set doesn t matters so so whether i read this as pencil ink or ink pencil it doesn t matter and we um we define the term sub sequence as any sequence with some item sets deleted from it so um some more definitions suppose i take a seq sequence a one a two until a m ok this is actually a sequence its not a set so so this curly braces should actually be a um um it it should not be there ok  refer slide time  28  49  so um suppose i take a sequence s prime um a one a two until a m we say that s prime is set to be contained within another sequence s if s contains a sub sequence of the form b one b two extra b m that is m different elements such that each corresponding element is a subset ok a one subset of b one um um um subset equal to rather and a two subset equal to b two and so on so hence for example um um this sequence pen pencil and ruler pencil is contained in this sequence that is pen is a subset of this pencil is a subset of this and suppose you take this out ok so so and create this sub sequence pen um these case is a subsequence then ruler pencil is a um subset of this one so let us look at the apriori algorithm um its i think all the apriori gen algorithm or whatever where um um apriori all algorithm where um it is um um where it is um applied for sequence data rather than um um item sets or or association rules  refer slide time  30  06  the apriori algorithm for sequences looks very similar to the apriori algorithm for item sets as well um how does the apriori algorithm look first of all we set we we generate l one that is the set of all interesting one sequences what is the one sequence a sequence containing just one element ok and then um um when l k is not empty where when k equal to one we generate all um candidate k plus one sequences and out of these um we take only the set of all interesting k plus one sequences what is interesting k plus one sequence here it is simply the set of all k plus one sequences which which have at least the minimum support um that that we have specified and so on now the now the main question here lies um in this statement here three point one that is how do we generate um um or what is the candidate generation algorithm how do we generate all candidate k plus one sequences  refer slide time  31  14  so how do we generate all candidate algorithms now given um um let us say um um different interesting sequences that is l one l two until l k candidate sequences of l k plus one are generated um simply by concatenating all sequences in in l k with all new one sequences found while generating l k minus one what is this mean let us um um let us illustrate this with an example let us say this is my data set  refer slide time  31  46  ok and this data set let us say um um um denotes let us say i have a website and this data set denotes um um which are all the different pages that have been visited by um um users um um in in different um usage sessions ok so one user um a went from one user went from page a to b to c to d to e and so on another user came from b and went to d and a and e and so on like this ok so we we have different um um sequences and of course as you can see here that um um um an element can repeat in a sequence that is this user um um um has requested for um the page a four times um um um and one after the other and same thing here  refer slide time  32  55  that is after b a is requested three times and so on for whatever reason ok now from here in order to look at in order to mind for all interesting sub sequences that is um what will be visited before what and um um um in in this data set let us start with the set of all interesting one sequences ok now we have set a min minsub as point five that is at least fifty percent of support ok now let us look at the set of all interesting one sequences what is it mean to say interesting one sequences essentially um it means that um which all sequence of length one have appeared at least five times or more ok so a has appeared one two three four five six seven eight times in in eight different sequences ok um b has appeared one two three four five six seven eight nine um different times and so on ok so a b d and e are interesting one sequences c for example um has appeared just once here right so therefore it is not interesting at all um um  noise  as as a one sequence now we generate all possible candidate two sequences that is um um it is now rather than a combination it s a permutation ok that is where where the order matters so so aa and um rather um its its not a permutation it is it is a concatenation rather that is concatenation of um all possible concatenations that that are poss um um that are possible between elements of this one so um um ab is different from ba and ad is different from da and so on ok so these are the set of all candidates two sequences now um we just see which of these candidate two sequences have minimum support ok  refer slide time  34  34  now among these you see that only ab and bd have a minimum support of point five right that is um um all others aa for example has the minimum support of um um one two three that s it ok no um um not point five that is um one is here rather four um one two three and four ok ab also has minimum support less than five and so on ok so the the only set of interesting um um um two sequences are ab and bd in this case ok so so so we have got the set of all interesting two sequences now how do we generate the set of all interesting um three sequences that is um candidate three sequences we concatenate um ab and bd with all the interesting one sequences found in the previous iteration ok so the previous iteration here is still the um is still the one sequence here ab d and e therefore we concatenate um um  noise  both of this with ab d and e like this ok and then we see that there are no interesting three sequences at all ok and then the process stops otherwise um um we would have filtered out few more elements here and then out of these again we would have um um um we would have concatenated with all possible interesting one sequences that we found in the previous iteration ok so  noise  so here the interesting one sequences that we have found in the second iterations are a b and d so for um for level four there is no need to concatenate it with let us say e ok so its its enough if we just concatenate with a b and d with sequence data there is another kind of interesting um um mining problem that that that occurs um when when we look at a sequence data as a behavioral pattern  refer slide time  36  34  see for example when when we say this is the way that users behave in a data um um user behave in a website ok the the the user here comes to page numb page a then goes to page b then goes to page c d and e and so on ok now um um we um we are encountered with a question as to can we model the behavior of the user ok what is um what would be a model that would explain me how users behave on my website ok so what this means is that um we have to find out suppose these are all the different strings um um of of a given hypothetical machine we have to gen we have to find out some some machine which can generate all of these strings and possibly other strings that that belong to the same class in in whatever sense that is ok so um  noise  so so the question here is that given different sequences treat this different sequences as um um um um as strings that are generated by a particular machine the simplest kind of machine that that we can generate is the state machine or or the deterministic finite automate or the or the finite state machine or whatever now um but that doesn t mean that um um everything can be modeled by a finite state machine but its its purely because of um um complexity considerations or practical considerations that that we assume that um the the model representing user behavior um is given by a finite state machine so given a set of input sequences we have to find out what is the finite state machine that recognizes this class of input sequences ok this also called as language inference that is um given the strings of a language you are trying to infer the grammar of the language or you are trying to infer um um um the the structure of the language now um what is the problem in language inference ok what is the um um what is the big um um where is the tricky um trickiest problem that that occurs in language inference  refer slide time  38  39  take a look at these strings let us say i have these four strings abc aabc aabbc aa abbc so on ok now if i want to give you these four strings and and tell you that create a state machine that will recognize this four strings it is quite obvious that one would come out with the state machine like this ok which says um which accepts these fours strings and exactly these four strings ok so abbc abc and aabbc and so on ok so which which accepts exactly these four strings ok on the other hand um um one can also write a machine like this um um comprising of a single state which leads on to itself and accepts all strings like this ok so this is a this is a most general state machine that is um um this state machine is also correct in a sense that um it accepts these these four strings but it also accepts anything else um um um made of a b and c in addition to these four strings while um this is a most specific state machine that is this is a state machine that accepts these four strings and these four strings only and nothing else now the now the challenge or now the trickiest problem um in language inference is to find the right kind of generalization that is um if we make some thing into a most specific state machine it will be of no use while we make something into a most general state machine it will be useless um as well so um um so so when we discover or when we try to discover a model of of user behavior we should discover a model which is not too specific and is neither too general it has to have the right kind of generalization how do we do that there are several different algorithms that um that that try to generalize a little bit and not too much and and not be too specific and so on  refer slide time  40  34  we will just look at one um um specific algorithm um which might be termed as the shortest run generalization that is um um generalize based on behaviors um um by using what is called as a shortest run technique of this thing now as um um  noise  um as we did in the previous for the previous algorithms let us first look at the example and then come back to the algorithm now um the the way shortest run generalization works is shown in this state machine here now let us say that we encountered different um um strings ok  refer slide time  41  21  now  noise  let us say this is the first string that that we encounter aabcb ok now um um there is no other string therefore we just build a state machine like this which accepts only aabcb and we haven t seen anything else so so we cant generalize anything else ok now second we encounter the string aac right so um what this means is this the state machine should accept not only aabcb but also accept aac ok what this what doe what does this mean this means that start from aa ok and from after aa if i get a c i can go directly to the end state right so so it has to accept um um not this aabcb but but also aac ok now let us say that i get the third string even here i wont be able to generalize anything this is the state machine that that accepts aabcb or aac so so we still haven t generalize anything now let us say i um um i encounter um one more string of the form aabc ok now what um what is this mean this means that um aabc that is this this string that is this is a prefix of this thing right that is this is the substring of this thing this is the prefix of string of the first one so aabc this state itself should be a end state ok so so basically we come like this here and abc this becomes the end state now what we do is we merge both of these end states so so b comes back like this ok when we merge these end states um note that we have performed a specific particular generalization here now what is this machine recognize this machine recognizes aabc b star that that means any number of b s after aabc ok so essentially what it sees is that or any number of b s after aac as well ok that means it has seen a b appear after aac that is this substring aa and c um um with or without b included it has seen that b may appear or not appear ok and it generalize to the fact that any number of b s may appear including zero number of b s ok which may or may not be right that that means to say that there might be an implicit um there might be some more hidden variables that that says that at most um three b s can appear let us say zero one two or three b s can appear not four b s ok but we don t have that information here as such ok so um so basically the the state machine generalize to to the fact that um um after aabc or after aac um zero or more b s can appear and we um we still lie in the end state but then um we also see that when we look at the end state here we we look at look at the tails of of all the edges coming into the end state ok so there is a tail here which says c and there is a tail here which says c ok now when whenever from the end state it it finds that there are two or more tails having the same suffix ok these two um um the the corresponding states are also merged ok so what we finally get is aa b star c b star ok so so that means what it is what what the machine generally is is actually saying is that um um this language has to have two a s to begin with so so it has two a s and it can have zero or more b s in um um um following two a s and then it should have a c and um um and then it can have zero or more b s and so on ok so because it has found zero or one b s between a and c and it has found zero or one b s after this c it has perform this generalization so so so this is one way of performing um um or trying to discover the the the behavior that is that is exemplified by a set of sequences let us look at the last kind of data set um um um for for this session namely streaming data streaming data um um has been um um of relatively newer interest among the data mining community and um um especially since the streaming data or mining on streaming data has several interesting applications now what is the characteristic of streaming data what you understand by streaming data you have let us say streaming audio streaming video network traffic um and sever several other such such um data sets which is um which are essentially um large data sequences possibly infinite data sequences in practice of course there are there are finite but um um possibly infinite data sequences and there is no or very little storage that is its not um um it is not practical to say that i am going to store the entire um um streaming data into a file and then start mining the file  refer slide time  46  53  because this if it is infinite or if it is extremely large it will be impractical it could be tera bytes or even um um or even more bytes of data that that could eventually accumulate um into the file ok  refer slide time  48  19  so um some examples are stock market quotes or streaming audio or video or network traffic and so on  refer slide time  47  12  so in order to um um mine streaming data or rather um um in a in a even in order to let us say query streaming data um um we um there is a notion of what is called as running queries or also what are called as standing queries ok that means um um in a traditional database the the data is standing the the data is there and the query actually um slides through the data set in order to um ret return you the answer but in a streaming data set it is the query that is standing and the data streams through the query and and then the query keeps returning you answers as on when the data streams through it ok so um how do we write some standing queries or how do we find some aggregate behaviors based on some standing queries let us look at some um simple standing queries um computing the running mean of um of of a data stream that is suppose i am getting a stream of different numbers and i have to calculate the average of these numbers as in when i read a new numbers so so it s a running mean ok so a simple way to calculate this this running mean is um is like this let us say let i i jus i just need to maintain two variables here one is the number of items that have read so far or the number of numbers that have read so far and the the running average that have calculated so far ok so whenever i read the next number all i need to do is  noise  first um um um first compute n times average that is um average times the the number of number that have read so far add number to it and divide it by n plus one and then increment your um um um increment the number of numbers that you have read or the number of items that you have read that is n equal to n plus one so as simple as that that is as soon as a um um as soon as a new number comes you you generate the sum see n times average is basically the sum of all the numbers that um that that have come so far so so generate the sum here add the new number and divide it by the the new that is number plus one n plus one um um as the new set of numbers that have come and then um increment your set of numbers similarly this slide shows um how to um  noise  how to um write a running query that computes the running variance  refer slide time  49  34  variance as you know is the square of the standard deviation of a um of a given um um  noise  of a given data set how do you compute standard deviation that is um it is um um for for every element x compute um x minus x bar that is number minus average um whole square and um um  noise  and and compute the sigma or or compute the sum over all of them all of these different um um differences ok um um  noise  so mean square distances essentially ok so in order to compute the running variance um um we we look at this formula little more carefully ok variance equal to sigma of number minus average whole square where where number ranges from i equal to one to n or whatever ok now when you um um expand this um um you can expand this into number square minus two time number times average um um um plus average square right so um um so essentially what this means is um we have to maintain certain variables ok one is sigma of number square ok so every time you read a number square the number and add it to the previous sum that that you have maintained ok of course you also you also have to maintain the number of numbers that have been read so far ok then  noise  you also have to maintain two times number star average of of all numbers that that have been read so far ok so you know how to compute the running average ok so every time you get um new number compute the running average that is we saw how to compute the running average um in in the previous slide and then compute um um  noise  two times um um number times average and and add it to this ok so so essentially um you can take out average from out of this and sigma of number um um um or or two times average out of this and you just basically have to maintain sigma of numbers that is the the sum of all the numbers that um we have calculated until now and um um multiplied to the new average that that we have found and then we have to um maintain um there is no sigma that is necessary here because average is a single number and we have to just maintain the square of the average of all the numbers that we have read so far ok and um um we know how to maintain the average now um by maintaining all this we can easily calculate the running variance that is you you just compute each of them put each of them in their corresponding places and compute the running variance therefore even if i have a long let us say stock quotes from from the stock market giving me um um how the quotes of um how the stock price of a particular stock is changing i can um i can maintain what is the mean stock price that that um that it has recorded so far and what has been the variance and i can easily calculate standard deviation at any point in time by computing the square root of the variance so so i know how much it has varied over time and what has been the mean um um behavior of this stock over um over the entire time that i have read so far so um so this slide essentially shows  refer slide time  53  10  how um um you can calculate the running um variance that is whenever you read the next number first compute the average we know how to compute the average then each of these is computed like this that is a equal to a plus num square b equal to b plus two times average star um um num and c equal to c plus average square and variance is a plus b plus c we we shall also look at one more um algorithm for for streaming data um essentially what what is called as a gamma consistency or um looking for events that have what are called as gamma consistency  refer slide time  53  49  what is meant by this gamma consistency essentially the the idea behind this is as follows suppose an event happens ok at some point in time the interestingness of that event will be high in the vicinity of the event that is write after the event happens let us say um um stock market crashes ok the interesting that event will be high in the in the next few days but over a period of time the the interesting that event starts going down unless of course the stock market crashes again right so so that is the essential idea behind gamma consistency that is um um first consider this streaming data to be in the form of frames where where each frame um comprises of one or more data elements ok then we look for some interesting events within a frame essentially um um let us say support based interestingness ok so by let us say number of occurrences of k divided by number of elements in frame ok and then we see um which of these events have sustained support ok over all frames rate so far with a leakage of one minus gamma ok that means in in every frame let us say um every day or every week or whatever ok we look at events that have that are interesting with with a support of k ok  refer slide time  55  02  and as if  noise  if the if this event keeps on occurring with at least this much um support um then you can consider this to be some kind of beaker where um where you where you are poring in the events which are um which are coming in with with with some kind of support and this beaker has a small hole underneath where in um it leaks at a rate of one minus gamma ok so over a period of time if you take it over a period of time um um if and only if the this um this event has a sustained support over time this beaker is going to be full or this beaker is going to have a particular level ok um um and if this um if the event does not sustain over time eventually the the beaker is going to empty itself so so this the the level in this beaker is an indication of two things one is how sustained is the support for this event and second second could also be how recent was this event ok so so the more recent the event is the the higher the the level is going to be similarly the more sustained the support for for an event is again the higher the level is going to be so so you can calculate the level like this and um um and then you can again put a threshold for this level and look at all events which which have a particular level or so or level are higher at any given point in time so um we now come to the end of this second session on um data mining we have just crashed the surface of what is a vast area of um knowledge discovery from databases and we have kind of scratch let in a breadth first fashion that is we we looked at several um um several representative algorithms of for different kinds of data mining problems whether it was a um apriori or whether it was classification or clustering or um or sequence data or or um  noise  or um um something like language inference and streaming data and so on ok but um this is just still the tip of the iceberg so um um anyway that brings us to the end of this session  refer slide time  57  04  transcription by  vidhya proof read by  shobana database management system lecture # 36 object oriented databases dr s srinath hello and welcome we were looking at on the on going saga of um trying to understand data management in terms of database management systems we have looked at several different topics in database management however there is a um um there is some kind of a common theme or an implicit theme in a database management in whatever topic that we are looking at namely that um database or databases or primarily or essentially represented using the relational data model right so what is a relational data model that is um the the data pertaining to the uod or the or the universe of discourse is um maintained as a set of tuples or as a set of rows in a table right and um the the main assumption here is that every possible kind of data can be reduced to a set of tuples  refer slide time  02  34  in um this lecture and in the following lecture um we will kind of generalize on this assumption or will um we wont accept this assumption and look at other kinds of database management um requirements where data can not be in a sense  noise  easily mapped on to the relational model and um so in in this context specifically we shall be looking at the object oriented databases that is shown in the slide here so let us look at um what kind of um um data that that we are talking about when we are looking into object oriented databases um um but before we begin um in fact object oriented databases have been um very popular in the um in the last decade of the twentieth century in the sense that in the in the nineteen nineties um and but however um um they were are not as widely successful as say the the relational model database namely mainly because um the object oriented databases do not have a sound theory that is they they they do not have a nice little mathematical model that describes the complete data model um as oppose to the relational data model where you have the relational algebra or the tuple relational calculus and so on where the entire data model is um is is amenable to a nice theoretical framework so um its because of one of these reasons whi which is probably cited as as the reason why object oriented databases um did not sustain lot of interest but however um the contraries also true to some extent that is object oriented databases have been in use or have been put to use in in several different applications mainly cad application computer aided design of um say electrical circuits or mechanical circuit mechanical um um um design and so on and they continue to be used and there are quite a few commercial implementations of um um object oriented databases so um um in in this lecture and the next when we are talking about object oriented databases we kind of implicitly assume an example application of a cad um um that is computer aided design um scenario where um where where users would be using um um computers to um perform electronic design or electrical design and where an electronic design comprises of several different components um i might have an icm i might have a capacitor a resistor a transistor and so on each component having its own characteristics and having its own behavior and so on ok  refer slide time  05  07  but um um that s um that s the implicit assumption that we are going to make but that doesn t really necessarily mean that um object oriented databases are suitable only for cad applications of course there could be several other applications as well so let us come back and look into what kind of complex data objects that we are talking about um that when we say that we are going to generalize or we are going to move away from the relational model and look into other kinds of models um have a look at the first example here say it s a multimedia databases what what do you understand by the term multimedia databases databases that store multimedia objects what do we mean by multimedia objects you might have en you would have encountered several kinds of multimedia objects le um if you have um let us say work with any um um gui based um operating systems like say windows um um windows xp and so on where um um you encounter objects like menus scroll bars drawing areas then then something like um ss when you click something that there is a sound that appears and there is a for for specific kinds of events specific kinds of um sound and light um so to say um um um a messages are are thrown to the user and so on and there is flash animation and so on and look at the second kind of application that we have talked about that is namely the cad where um what would what would the what would a typical cad application for electronic design comprise of um um a typical cad application in a typical cad application the user should be able to um um let us say  noise  um select a pcb a printed circuit board or select a resistor or a capacitor or a transistor or um or a particular kind of ic and voltage source and control sou grounds and current sources and so on and so forth right now the the main theme or the or the common theme between um these two applications is that um both of these applications are made up of fundamental objects which form the building blocks of this applications so multimedia applications are built from several of these um different objects they could be menus or scroll bars and so on  refer slide time  07  31  similarly cad applications or cad projects in a sense um or multimedia projects and cad projects are in a sense built using this um fundamental objects cad projects would have a pcb or ic and so on and so forth ok so um the um  noise  what are the characteristics of these complex data objects it is quite apparent that um these data objects are not easily amenable to reduction to a tuple that is we cant really reduce all of this to one set of one tuple having a set of characteristics because there there is much more to um um an object than a set of different attributes  noise  so um wha what is this what comprises this much more um essentially the the idea of behavior of an object um um a transistor for example behaves as his is is represented not only by um a set of attributes saying what kind of a transistor is that or um is it pnp or npn or so on and so forth what whatever else um um um goes into describing the attributes of the transistor in addition um the transistor also has a particular kind of behavior you can apply voltage um at at one of the pins and measure the voltage at one of the other pins and so on and so forth right so um um an object does not simply represent a set of attributes but it also abstract um um a a rather um an object is not um merely um structural abstraction but it is also a behavioral abstraction that is when i say transistor the the kind of behavior that that the transistor um um emulates is also abstracted by the object and um in an object um there there are several instances of an object that can belong to the same class so i could have several different instances of the same transistor um and um and each of this different instances may have different set of attributes at any given point in time that is um the the instance variables of of each of these different transistors could be different that means um each of the transistors um belonging to this particular class has different states at any given point in time  refer slide time  09  56  so let us briefly um um take an overview of object orientation concepts um object orientation concepts here um i am um  noise  essentially looking at from object oriented programming point of view object oriented the the idea of object orientation came from programming and there say several kinds of um oopls or object oriented programming languages um which were um started right from the early seventies and and so on so let us look at object orientation concepts from a programming point of you and then we we shall look look into how each of these changes when we um um when we consider object oriented database systems now um um the the the fundamental building block in an object oriented system is of course the object but a but then an object represents an instance and an object belongs to a particular type and here this is called a class so an object can belong to a particular class or rather we we define specific classes of objects and then we instantiate different objects of specific classes for example we could define a class of objects called cars and we can instantiate an object um of of type car which which specifically points to one specific car rather than the the type of all possible cars then um the the idea of um um an object is to provide an abstraction to the user so um um depending on what um what the application is when um um a type of when when an object of type car is created it represents an abstraction called car that is each car is suppose to have a have certain properties not just structural properties in terms of what attributes they they have but also behavioral properties what can you do with the car and so on for example take take something like menu in in multimedia database so menu is is an abstraction that is it not only says what are the attributes that make up a menu but what is the behavior of the menu as well that is um um the the menu should provide a list of items and there is a default selection by the menu and um um the the user should be able to scroll up or scroll down the menu and so on and so forth ok so an object  noise  an object provides an abstraction or an object  noise  emulates an abstraction of not just structure but also of behavior and how is this abstraction provided through the notion of encapsulation that is an object encapsulates structure and behavior within its um fold that is an object is defined by a set of attributes which define the structure that is um um that is um um described by the object and a set of methods or function calls that that operate on these variables which define the behavior um that is abstracted by this object and of course there is the notion of an interface that is um interface is also called the signature of an object that is um an object um um only exposes or or the only um thing that that theoretically at least that is that is exposed to the outside world is the interface of an object and all um external world entities should interact with the object through the interface by calling particular methods and um and changing attributes and so on so um again just to give an example or just to give an analogy a car for example gives an interface in the form of a steering wheel and um and a gear ok so so you can interact with the car only through the steering wheel and the gear and of course the pedals that is the the break pedal and the clutch pedal and so on you can not directly go um when you are driving a car you can not directly go and manipulate um how the engine behaves for example or how the wheels behave and so on you have to deal with the car through its interface so interface is the as for as the user is concerned the interface is the signature of the object the the if you want to learn to drive a car you should um um you should know how to handle the steering how to handle the break and clutch and gear and so on and so forth so so you should be able to know how to handle the interface rather than what lies within the interface and of course um interface um um in in software interface is made up of methods which are function calls which um which change the state of the object and methods themselves have have particular signatures that is each method has requires zero or more input parameters and has zero or um um or one output um um parameter as well so so that forms the signature of a method and attributes of of a class is is the set of variables that that define the state of the class for example in a again in a car the the attribute would be something like which gear the car is in or what is the speed of the car or what is the acceleration of the car and so on and so forth which which basically describe what is the current state of the um car so any method call would change the or would influence the attributes of this object and object state of course is um is um is a is a function of what are the values of different each of these attribute so the state of an object is something like um let us say the state of the car is um can be defined as say cruising when um speed is so and so and the and the gear is in over drive and so on and so forth so so basically you define um set of values and say now if if these are the set of values then this is said to be the state of an object and there are again um some more param some more concepts pertaining to object orientation um which which are also which may also be important that is some notions of say message passing so when um  noise  when an when an external world entity invokes the the method of an object it is said to have pass the message to the object and the message in turn invoke the um object that is in invoke method of of this object and um some um some more concepts of object orientation which which are particularly useful are the notion of inheritance polymorphism and over loading that is when we define a class um we can define a generalization specialization relationship um which we also saw in let us say the enhanced um um er model where a generalized class um represents a general um more general entity than um its specialized classes  refer slide time  15  21  that is wherever um um an object of the of a generalized class is required it should be safe to substitute it within an object of a specialized class therefore um  noise  suppose i have a generalized class called um um class called class called um say suvs or whatever sports utility vehicles and so on so and there could be different classes of suvs whatever um qualis and um scorpio and so on and so forth so so several different um kinds of suvs and for um and what constitutes a correct generalization specialization relationship um the um wherever i need an object of the generalized class it should be safe to substitute it with an object of the specialized class so the specialized class is said to have inherited um properties from the generalized class and of course extended on the properties or over ridden on the properties um um and so on so um um um a specific property or a specific method for example um um again coming back to um um cad databases or um um which which we said we are we are going to have a um um running example so suppose i have a cad database and i have a generalized class called say a transistor and i have a specialized class called a specific kind of transistor of of of of some particular number so um um um  noise  whatever behavior is um um is specified by the generalized class um can be actually overridden by the specialized class and and in a sense the same method signature seems to be giving different kinds of behaviors depending on which object of the specialized class is um um is is substituted so so that s bring in the notion of polymorphism that is the the same signature um method signature giving rise to different kinds of behaviors that that emulate from the system and there is also the notion of over loading where um um which which is a feature that present in many object oriented languages where um the the where slight changes in in method signatures can be used to um um to to perform different classes of the same activity for example we can say something like um um um something like add ok now um when i say add to um i can i can say add int comma int where it add um it takes in two integers and gives out an integer now the same add could be defined as int comma float or float comma int or float comma float where where you can add different combinations of integer and floating point numbers and return back so um at run time the um the the message passing framework is going to determine which kind of add is being called depending on what um um what is the type of the parameters that is um um that is passed and then there are pure object oriented languages were everything is an object there are no what are called as native types so every single entity like um an integer every integer or every character is an object and there there no native um um or or fundamental data types other than objects and then there are hybrid object oriented programming languages where which do allow native types and of course um um what what may be termed as semi object oriented programming languages where you can do you can perform both object orientation and procedural programming in the same language  refer slide time  20  05  now let us come to object orientation as pertaining to databases now what extra features to be required in the concept of object orientation when we talk about databases the main um um concept that is required for databases is the notion of persistence persistence of an object or persistent object what is the persistent object a persistent object is something that that can exist persistently or permanently that is um the the objects can exist even after the the program using the object has finished that means the object exist on some um persistence storage like disk and can be recreated or can be um um reread back from disk whenever required now  noise  um for for storing persistent objects um in in object oriented databases um another um um important requirement is the notion of an object identifier that is it is important to  noise  um it is important to uniquely identify each persistent object that is stored in the database you might think of this as a as a primary key as in um um that that that we discussed in relational database systems but there are some slight differences between an object identifier or an oid versus a primary key object identifier or an oid is automatically created by the system whenever a new object is added to the system whether the the user specifies it or not on the other hand the it is the user who specifies what forms the primary key in any database relation and of course in a in a um in pure relational algebra each tuple is unique that is um um its um table is a set of tuples not a bag of tuples therefore in the worst case the entire tuple form a primary key for for the um um for the table however object identifiers are um um are um separate um separate attributes that is the entire object can not form um um can not form the the or can not uniquely identify um given object this is because two or more objects belonging to the same type can have the same state um um and and hence be indistinguishable as far as their other attributes are concerned but they still would represent two different objects for example i can always have um two different transistors um um in in any given circuit board which which have the same input voltage at the same time um fo at at or same input or output voltage at each of its pins at the same time however they are still different transistors so um so by default an object database is a bag of objects that is um um all attributes of an object do need not necessarily uniquely identify an object so we we um necessarily require an oid or a or an object identifier and the way objects are stored in databases are um um as far as possible should be with direct correspond in direct correspondence to real world objects so i can store an object like transistor or capacitor or pcb or whatever so where where you have direct correspondence to what um one can see tangibly in the real world and  noise  ya um and of course there are there are several different um um attributes that define an object which um um which which alter the state of an object and of course these variables for for um attributes are defined at a class level whereas um um when an object is instantiated these become instance variables and the instance variables of different objects could be different even though they belong they they um um they represent the same attributes and  noise  just like an object oriented programming languages objects are defined by um um signatures which um which are the interfaces of a of of objects and even methods have signatures that um that that are defined by um um that are defined by um for the objects and um um um every other notion in an in an oopl are also reused here something like the the inheritance and um reuse of objects that is when when you inherit um the base the the derived class or the or the specialized class reuses certain properties of the base class that is because it inherits certain properties of the base class we can think of it as some kind of reuse  refer slide time  24  05  and um there is also a notion of referential integrity in object oriented databases by the use of oids that is every oid um um suppose an object a refers to another object b um this references is is captured by ma by putting the oid of object b as an attribute of object a and referential integrity is um is enforced by ensuring that at every at any point in time the oid that is represented as an attribute in in any given object is always a valid oid and of course there is operator polymorphism and and overloading and um other concepts that that are um common in um oopls ok so  noise  so let us have a lo um um let us come back to the object identity aspect like i said before the the oid is is mandatory in any object oriented database system and this is usually a system generated unique identifier that is um the the user need not even be aware that there is an oid that is created for each object however the the system by itself creates um unique object identifiers and of course the oids have no relationship to the values of attributes that is the set of all values of attributes of an object need not necessarily um uniquely identify a given object  refer slide time  25  53  and um um usually oid is a is a logical number and it is it is not advisable to base oid on the physical address of an object suppose i have stored object in a particular directory tree um we should not keep the directory tree as the oid of an object because um the um because of of several reasons like um if the um if the database is migrated to a to a to a different system or if the directory tree is changed then then the the oid changes in the object becomes in accessible now um like i said before um um every instance of an object is characterized by a state of an object now how do we define the state of an object in terms of the database um um or in the database parlance this slide shows  refer slide time  27  00  a formal model of how the state of an object is is represented or in a sense the structure of an object the the structure defines um um defines the state space in a sense so so the the different kinds of state that that an object can be so um an object structure is is de defined by a triple um comprising of three um values i c and v where i is the object identifier and c is what is called as the type constructor and v is the object state so um um i is the is the well known oid that that we have been talking about and c can be um um c is what is called as the type constructor that says what type of type in a in a sense  refer slide tiem  27  52  or what type of value um this is go is this going to be and usually um um um um object databases define different kinds of type constructors like atom and tuple set lists bags arrays and so on an atom type for example defines a specific atomic value so i can say atom and then give a value of five for for the um value so this object represents an atomic entity with whose value is five ok on the other hand i can represent a tuple also as an object so instead of um um one single value a tuple represents a list of values an ordered list of values and um  noise  list of atomic values essentially and a set is an unordered um um um set of um set of values unordered collection of distinct values um that that can that we can take up and list is similar to a tuple except that um um in a tuple the size is fixed the size of a tuple is fixed but in a list um different instances may have different sizes of um for for the for the sequence of values that we can take and bag of course is a is a multi set the that is a set with  noise  a set with repetitions and so on ok so um um this slide shows  noise  shows some examples here um where we defined all these things already that is when um type c is atom object state v would be one particular value in a from a domain of basic values and when it is a set it is the set of values and and so on  refer slide time  29  27   noise  now this slide shows some examples here  refer slide time  29  36  let us say i define an object o one um um as a triple where where i one is the oid of the object and the object if is of type atom and the value of the atom um of this object is chennai that means this object essentially stores an atomic value or an atomic entity called chennai as as part of this object ok similarly o two um has has as oid of i two and it stores an atomic value called thirty five and o three is a um is a tuple wherein each element of this tuple is an oid that is um um i one is a is a oid of of belonging to the class called place so o one um um that that is i one um see look look here that i one refers to this i one here right so this i one is an object um will um is i one basically represents an object called o one and o one belongs to um um a class called place and similarly i two um represents an object called o two which belong to a class called num right so um um this o three is a tuple of different oids where different its in in a sense it s a composition of of different objects of of different types um um in the form of a tuple similarly o four is a set comprising of three different oids o one i one i two and i three so as you can see here um um it is possible not only to represent specific atomic entities its also possible or rather um um or even collection of atomic entities it is also possible to start composing objects um one um um one object inside another for example o three in a sense is a composition um um that is made up of o one and o two right so and similarly o tw o four is a set or a is a collection that contains all three elements that is um o o one o two and o three right so um depending on these kinds of associations between objects whether it is a composition association or um or some kind of a um um whatever other kind of association that we can um that we can um um define an object database can actually be represented as a graph structure so so where each object um in turn has some kind of an association whether it s a containment or inheritance or some other kind of an association with other objects in the database  noise  and when we are when we are talking about the the states of um um objects remember i had mention that two or more objects may have the same state but that doesn t necess necessarily mean that they are the same object um um because as long as their oids are different um they they essentially  noise  refer to different objects so this slide shows such an example  refer slide time  32  41  so at any at any instance of time i may have two different objects o one and o two whose states are the same that is they represent one atomic value whose value is thirty five and there is one more um um  noise  object of the same type called num that that we define in the previous slide which um represents a um value called twenty however um um even though both of these um have the same state and this has the different state all three are different objects namely because the oids are different i one i two and i three ok  refer slide time  33  17  similarly here um um these objects i one i two and i three um um or i two i three and i two or whatever so so these two o four and o four here ok have the same state that is a one i one a two i two a three i three and in this case it does um um represent the same object why because i four is the same as i four here so at the end of it is just this oid which determines whether two objects are the same even when this um um regardless of um what is the state of this objects and different object oriented database systems um um have um provide different mechanisms for defining um custom types or custom classes  refer slide time  34  09  so um um here um the this is some kind of pseudo code for for for particular kinds of um object database systems and later on um um we will be looking at one particular um standard for for representing types the namely the um um omdg standard but um um the the idea here is that the the the user can define um his own types for example um the user defines an object of type employee comprising of different attributes that is there is um first name last name salary supervisor and so on and so so so um this forms the tuple of different attributes that um  noise  that represents the um um um an object of type employee similarly there is um um um there is nested um declaration here that is um um department is a tuple comprising of department name and department number and manager which itself is a tuple um um um comprising of an oid um of um of an object of type employee and start date and so on ok um  noise  so that was um um um you might have got a question now that um what is the difference what really is the difference between um declaring objects or or types using what we saw here and with the relational data model itself  refer slide time  35  37  that is both seems to be different ways of doing the same thing that is def defining a set of attributes however object database is differ um from um in one important factor from relational databases namely that of object behavior so so the um um you can um you need not um when you when you defining a type of an object lik like say employee and department it is not just the attributes that you define but also the set of behaviors so so let us look at what um um what is the importance of behavior when it comes to um object oriented database systems now object behavior is um um is abstracted by a set of methods and which is visible as the object interface to the external world now the interface as i said before is also called the signature of an object that is each object should have a unique interface each class um um um which uniquely identifies what are the kinds of behavioral abstractions that it provides ok so um for example um um if i have a object of of type ic ok of of a particular ic type let us say um um some kind of um let us say logic gate ic seven four zero four ok so this object has particular kinds of behaviors that is you can you can provide um um input voltage to to a particular pin and you can provide ground to a particular pin you can provide inputs logic inputs to to particular sets of pins um seven four zero four basically has um um um implements and gates and so so you can basically provide logical inputs to to certain pins and get logical outputs from certain pins and so on so um um probe an input pin or um input voltage to a particular pin all of these are methods that um that are abstracted by the object and  noise  and of course um um when we are talking about attributes itself by default or in pure object orientation um every attribute of an object is actually hidden from the external world that is the external world can access an object only thorough its interface or only through its method declarations but in reality though some um attributes are visible to the external world while some attributes are um hidden from the external world that is the um which can be accessed only through method interfaces um in most object oriented databases um the the database management system allows the user to specify the interface of an object um along with the um um along with the attributes like like the attributes here  refer slide time  38  17  first name last name salary and so on um the the user can also specify a set of interfaces and the implementation of these methods that is method declarations are provided here and the definition of these methods or the implementation of these methods can um actually be return um um or can be um um return else where using a programming language or method definitions can be use um can be done um using any standard programming language like like c plus plus or java or so on so so the the object database itself does not provide primitives or need not provide primitives um to um to to define methods but rather you can actually use cert um an existing object oriented programming language in order to um define a method interface so um defining a method interface now i mean um um um um embedding methods in addition to attributes will now enable us to define a define a class rather than a particular type  refer slide time  39  21  so um one can define a class here for example this this slide shows the definition of a um of a class called department where um um um where where the classes certain attributes like um um tuple um um which which is a tuple of attributes which which basically contains um department name department number manager which is another tuple and projects and so on in addition there are certain attributes um um certain operations that are also define like number of employees um um is um number employees is is the name of the method which returns a integer so so when when the um um when the external world calls this method an integer is returned which which essentially says what is the number of employees in this department um similarly create department which is which is what is also called a constructor method that that creates and instantiates um an object of um type department and um um put some default values in um um in several one or more of these attributes and assign employee that is add an employee to the department and so on so um when operations are defined in addition to um um in addition to attributes we get the definition of a class in in contrast to a type  noise  and object persistence so so how are objects themselves persistently stored um um and and referenced uniquely of course at the um um at the implementation level the object database system uses the notion of oids that is when we refer to an object um um and if it is a valid object in the in the database it is given a unique oid object id identifier  refer slide time  41  13  but what is the abstraction that is the the oids completely hidden from the user that is the the user or application program that is that is using this um um let us say the the cad tool um does not or need not have to know the oids of each object that have been um instantiated and stored in the database um instead um the the application program refers to each objects by different kinds of mechanisms um um one um well known kind of mechanism is um is by the use of a naming mechanism that is um i can refer to a particular object like say um um ic se ic seven four zero one um or seven four zero four number two whatever ok so um each specific object of seven four zero four that that have created can be given a specific number or um this is the um this this is the first ic or second ic and so on and like that um using that as as a mechanism the application program can uniquely refer to each object and the each unique name um in turn translates um um internally to each unique oid on the other hand there is another kind of um um there is another kind of um mechanism by by which objects are referenced in in an object database system by the notion of reachability that is it may be difficult to give a unique name for every object that are that is stored in the database system for example if my um um let us say i am storing the um circuit of um um of a of a big computer like this ok now it has several hundreds of components and um um this particular circuit is part of larger database of circuits and each different um each of this different um units or each of the object that that are stored in this database has to be given a unique name um and um which might be impossible i mean it um um it may it may not be a practical thing to do so another way of um um representing or referencing objects is to re is is through the notion of reachability that is let us say that um one particular element in a circuit can be reached only through another particular element let us say transistor um um x can be reached only through the the pin number five of um um of this ic ok or or whatever ok so we don t in in such cases we don t give a unique name to this transistor and instead we contain with just the name of the ic and um any other object that um that can that can be uniquely reachable through the ic can be uniquely identified by naming the ic and then following the links so it s a um um so reachability essentially defines a a sequences of references in in the object graph that that would leave from a well known or named object a to um an unnamed or reachable object b so this slide shows um um shows an example  refer slide time  44  16  where um  noise  um where in some object oriented database systems where some um um objects can be declared to be persistent that is when we are working with an object oriented database system lets say um cad application a cad application is built around an odbms and um um the the the way of working with an odbms is um um is seamless that is um  noise  the the user would be writing the application program and um as um as part of the application program itself the user would be interacting with the object database system so um for example there are several let us say there are several class definitions that that make up this application program  refer slide time  44  59  and let us say one of these um um objects um um the the define by this class is should be persistent that is should be persistently stored in the database so um um you you basically define this this this objects say all departments which is a persistent named object of type department set which is defined in the class here so as part of your application definition itself you define which object should be persistent and which objects can be transient that is um um um  noise  that is they they lose their um identity or they lose their state when the program finishes execution and of course um um this is use um this is using the pascal exsyntax where you can say d equal to create department where um create a new department object in in this variable called d um and then um um make d persistent by by adding it to a persistent set called all department so so um um and and then save it to the database system so in addition to um these these different um um um features that that are provided by an odbms like like say type definition class definition um method definitions and um naming conventions and reachabilty and persistence and so on there are other kinds of um um features that that are available in um in an odbms where um um of of um what are called as type hierarchies and and inheritances and so on so um the the concepts here are more or less analogous to the the concepts in oopls itself that is um whenever i use a type hierarchy i am um i am um essentially referring to a generalization and specialization relationship  refer slide time  46  40  so  noise  um and a type hierarchy is defined by a um subtype and a supertype that is um um i can define something like a student is a subtype of um um of person and so so where where i i could have define person as as a tuple comprising of name address age social security number and so on and i can define student as a subtype of person where um um it contains all attributes that that make up a person in addition there are attributes called branch and gpa which um um which are important for defining a student as well ok and similarly  noise  um um i can um one can define  noise  inheritances that is um um an object of type rectangle has a subtype of a geometric_object which is which is not just um um tuple here but um um tuple comprising of tuple in addition to certain behaviors so um and then you say a rectangle is define by width and height in addition to whatever makes up geometric_objects um then there is the notion of an extent in um in object oriented database systems extents is in some way um to to give the to to give an analogy to er modeling um in entity relationship modeling we had the ide we had the idea of entity types and entity sets um um ent an en an entity type defined a type or a class of entities while an entity set is an entity type coupled with a collection of different instances of of this entity type so um the concept that is used here for an entity set is an extent that is um extent is a collection of objects of the same type that is the a type definition plus a collection of instances forms an extent  refer slide time  48  47  so um um the the object database system is organized in the form of extents um different extents that is different typed objects are stored um um in their in their own extents and then usually because the the in um in most object oriented languages there is always a type hierarchy and there there s usually a root class or um or like in java there is what is called as the objects class um there is a default extent that um that every object belongs to which is the um object extent or the or the root extent and um um depending on the class hierarchy or the type hierarchy there can be different sub extents that that can be um defined on each of these um um dep depending on the class definitions of each of the object that are stored in the database  noise  and um of course the the the way in which objects are stored can either be structured complex objects i mean now now we are explicitly calling it complex objects that is um objects which which are not necessarily amenable or data that is not necessarily amenable to the to to storage in a um um in a relational database form so um one can think of structured storage of of a complex object or an or an unstructured storage structure storage essentially is a is um is is some kind of a nested structure a tuple comprising of other tuples or sets and so on  refer slide time  49  59  so so um some kind of um structuring that is made out of the the um the the types that that that we define or the um um or the constructors type constructors that we define something like um um atoms and sets and tuples and lists and so on on the other hand there could be unstructured complex objects where um es especially multimedia objects um where i could have a video sequence or or an audio sequence and so on where um um there is no specific structure as such but its just one um heap of data or binary data that that makes up this object and there also called as blobs or um what expanse to binary large objects so so they are just binary data um the the which are just stored and then um um and um and stored in the database and defined as part of this object now um  noise  there are several different object database standards that um um um that existed that exist and um um there was several different commercial implementations of object oriented database systems but um um many of them have um in a sense of gone out of business but quite of you of them have still survived and um like i mentioned earlier the main um um at least as of today the the main um application area in object oriented database systems is in cad applications where um we need to store objects of a particular um um having not only particular properties a particular structural properties but also behavioral properties and this behavioral properties um um are um the abstraction of this behavioral properties are extremely important um when um when trying to built let us say an electronic circuit or a or a mechanical design and so on as part of a cad application now um um until now we have been mainly talking about um object database systems from a pseudo code perspective that is um um these are the features that that several of these object oriented database systems um um have or or had in a sense um but um um but but more concretely there are there have been few standards that that define what an object oriented database system should look like  refer slide time  51  29  and among them the the um um a well known standard is the o odmg standard that is the the object data management group standard and the the idea of the standard is um is to um um is to enable certain kinds of features or certain kinds of properties that that make up an object oriented database system so that these um um they can be um um seamlessly ported across different um object database management systems or odbms and the the main idea behind the standard is the um interoperability between um um between different odbms and odmg two dot zero defines several different concepts it it defines a basic object model and an object definition and a query language and it also defines um um different kinds of bindings to programming languages so let us briefly look at what are the main or salient features of the of the odmg two dot zero standard um um  noise  and in the in the interest of time and brevity we shall not be looking into um um great we shall not be looking in great details in in into the standard  refer slide time  54  18  but rather look at what are the main features that are provided by the standard and um odmg standard provides this um um basically it s the idea here is is the standardization of terminology and as shown in this slide  refer slide time  54  26  objects in the odmg standard are defined by these these different entities that is name um um identifier life time and so on and it also defines several types of um attributes atomic attribute collections structures and so on and interface definition is more or less the the the same  refer slide time  54  44  that that we saw in the as in the pseudo code and um um there there are um several default objects or default classes that are defined by um odmgs standard  refer slide time  55  06  and of these um um very interesting um default object is the um or default class is the collection class which defines a collection of different objects and this collection class has several um methods that are that are defined like you can query the cardinality of a collection um you can query whether the collection is empty or you can insert an element into a collection or remove an element from a collection and um and so on and so forth  refer slide time  55  26  and um there is specialized built in um um collection objects can further be specialized into different um kinds of these um you can specialize a collection as a set or a or as a list or a bag and and so on so all of these inherit the collection interface  refer slide time  55  41  so um let us not go into each of these in detail um  refer slide time  55  43  let us um um on the other hand look at some of the type hierarchies  refer slide time  55  46   refer slide time  55  47   refer slide time  55  48  um um look at the main type hierarchy that is defined by the odmg standard itself  refer slide time  55  55  um um like in java there there is a root object in the in the odmg standard which is object that is every object belongs to this class called object um root class which is this which is called object and then collection um um is a special class of objects which represents a collection of objects and then there are several other kinds of objects like date timestamp interval set bag dictionary and so on and so forth so so this is a um um this is a partial type hierarchy that is defined by the odmg two dot zero standard so so um let us summarize what we have learnt in this session today um we talked about complex data objects and how they they need not be amenable to a relational um um reduction to a relational storage so so we looked at object orientation concepts and wherein the notion of a state of an object and the oid of an object become important in order in order be able to store objects then um storage of objects are called persistent objects and how they can be accessed through naming and reachabilty and so on  refer slide time  56  36  and then we also looked at the odmg standard odmg two dot zero standard which defines its own um class hierarchy of of different classes so that brings us to the end of this session thank you transcription by  vidhya proof read by  shobana database management system lecture # 37 object oriented databases ii dr.s.srinath hello and welcome um in the previous lecture we were looking into um new kinds of data management problems were the kind of data that we have may not be easily amenable to reduction to a relational model we were specifically looking at cad databases that is databases in computer aided design were um the were the uod or the or the universe of discourse is um um better described by a collection of objects  refer slide time  01  39  and and some kind of relationships between objects rather than um a pure relational schema and how is an object different from um um a tuple in a relational schema well a tuple is um um part of an object but an object um um basically a tuple is an abstraction for um um particular structure um of attributes the the way in which attributes are um  noise  are put together to form a tuple but then an object is an abstraction for not only structure but also behavior that is an object um um um represents some kind of a complex data entity like like say an electronic component like like an ic or a um or a transistor or some thing like that were its not just the attribute or its not just the um um structure that s important but also what kinds of behavior that um that the that the object performs i mean you cant expect a transistor to work as um um work as um let us say um something else i mean um um i don t know may may be some kind of a current source or um in in some other form that is there there is a there is a specific set of behaviors that are associated with with an object and um um um in in conjunction with the attributes its also the behaviors that the objects um um um provide an abstraction forum ok and we also saw um um um different notions that that define um object oriented database systems for example in an odbms um it is mandatory or it is essential to have um a unique identifier for each object for each persistent object in the database and this is exemplified by the oid relationship and oid is contrasted from a primary key in a rdbms by the fact that um an oid need not be  noise  explicitly specified that is the the user need not even be aware that the um um that the database system is tracking each object using an oid and um just because the the state of two objects are the same that is they have the same set of attributes and the and the same set of values for each attributes doesn t necessarily mean that they are the same object um which is very much unlike the case in relational algebra were two tuples that are the same represent the same data um object essentially or or or the same data element but here the um even if two or more objects have the same state as long as their oids are different they represent different objects and then um um there are other issues like say inheritance and polymorphism and and so on where um um um um or type hierarchies and were an object can actually derive or um specialize from from a more general object and um and um and essentially as correct specialization is one were wherever um in the system you require an object of the general class um it should be correct to speci to to substitute an object of the specialized class as well ok and um there are other issues were were the were the database system can be represented as a graph um by um associating different objects based on different relationships like containment and um inheritance and other kinds of association so um and associations are made using oid references that is object a associates with object b if the oid of b is an attribute of object a right and um um there an odbms essentially is um is tightly integrated with um with opl or or an object oriented programming language um were um there are some kind of programmatic constructs that are that are provided to the language um um were the programmer who is writing the application program can identify certain objects to belong to the database um in a in the sense that these objects can be declared persistent as soon as um there um define that is as soon as their instantiated so as and when they are instantiated um there also associated with a um um with with an object in the database rather than just in the programming system other objects which are not persistent are called transient objects now um having looked into all those um we started looking into some of um we started getting little bit more concrete and started looking into the object database management group standard the there is the odmg two dot zero standard for um  noise  for defining object databases let us look at this standard in a little bit more detail today um um or in this lecture and um look at what other options does does the odmg standard provide first of all let us look back at what is the odmg standard which is slow shown in the slide here  refer slide time  07  01  the odmg standard or the object data management group standard two dot zero model um is a model essentially meant to standardize the notion of object databases and the main idea or or the main reasons for this um include portability and interoperability between different object databases and odmg two dot zero standard defines an object model and um a language for defining objects and querying objects so so object definition language and object query um language plus it also defines a number of bindings to existing programming languages like c plus plus and smalltalk and so on um were application programmers programming in any of these languages um can directly interface with um odmg objects or or the or in odmg databases system  noise  so essentially like like we said before um the the object model is meant for um um um meant to be a standard so that the so so that the terminology is standardized and then which which in turn um um help aids in the aids in interoperability and portability of um database models  refer slide time  07  56  so in the odmg two dot zero standard objects are um  noise  described by a set of um um different um um values of course an object has the the well known oid or the or the identifier and an object has a specific name that is associated with it plus also a life time um life time in the sense um you you can either called call an object as transient or persistent so so um a persistent object is sai um is said to exist even after the software or the application program has finished sh finished its execution in a sense permanently and of course the structure of the object and um like we saw in the previous session there are different kinds of um um data types that are usually um um supported by an o odbms  refer slide time  08  34  and the odmg two dot zero standard um supports different kinds of object types like literal types like atomic um um atomic attributes or atomic objects collections and structures this slide shows  refer slide time  09  22  um a typical interface definition of of um an object in an odmg um remember that the that the interface is the signature for a persistent object that is the interface tells the external world how to interact with the object and um um the the interface for example shows here um um the the interface does not show um type declarations however it it is showing the operation declarations like um there there is an operation called equals which takes in an object of type object and returns true or false and um and there is an um um there is a method called copy and a method called delete and and so on and the the odmg two dot zero standard defines a a set of um um or um also proposes a predefined set of classes along with the class hierarchy and um one of the classes in the class hierarchy is the um is the class called collection  refer slide time  10  19  and um as we saw again in the previous session um at the top of the class hierarchy is a class called object so every object in this in an odmg database by default um belongs to the object class that is even if it even if you cant resolve in any other way which a class that a that a particular object belongs to you can always say that the the um um an object of the odmg database belongs to the the root class which is the object class um a collection is another um derived class or a sub class of the object class which actually defines a collection of different objects that is a collection class can have um a set of different objects um or um objects of the object class and then the collection class also defines a number of um um number of methods like say cardinality of of a collection is_empty um um which checks for whether the collection is null or um or is empty um then um it supports insert_element where you can add an object into a collection remove _element were you can remove an element from a collection contains_element the searching element for for a particular object and creating_iteratives we will come to iterative soon what what exactly is meant by an iterator and what are its use if you have um program with this this standard template library in say in c plus plus for example um you probably um have guessed what an iterator is where um it s a kind of a template um wer um which um um using which you can iterate over um several different objects of any kind of a collection there are other kinds of built in interfaces that that are also specify um specified by the um um odmg um standard and um collection objects can also be further specialized into different kinds of these um um classes like say a set of collectors i mean  noise  set of collection or list or bag or um array or dictionary and and so on  refer slide time  12  25  so um all these are derived from the collection interface collector um collection interface that is the collection class interface we now come to um some iterator mechanism that is um um or or rather the requirement for iterators um um if you are familiar with c plus plus programming you would have probably come across the notion of a template um were wherein a template is um um is in some sense um a definition with a hole or um a method or a operator definition containing a hole were um different other objects can come and fit into the hole or different other objects of other kinds of classes can come and fit into the hole  refer slide time  12  59  so for example if i um um if i define a template called list i can have a template i can have a list of different um um objects of class a or objects of class b or um any of any kinds of objects so um similarly the the odmg standard def defines different template  noise  the first of which that we are going to see is the set template so set as shown in the slide here  refer slide time  13  55  set followed by a template mechanism here will create a set of classes of this particular type note the certain difference between a template like this and a collection a collection by default is a collection of objects that is um um it it just reads every entity in the  noise  every entity in the um in the collection to be an instance of type object however every class or every object in an odbms is definitely an instance of type object because object is the um um is the um um root or or is the top most class definition in um the in an odmg database um now therefore a collection can be a collection of potentially objects of different classes at at lower level so so you can have a collection comprising of one um car plus one transistor plus one ic plus one truck which which is very well valid on the other hand a set here is a set of only a specific kind of objects so um so so you can say that it s a set of cars so you cant really um um you cant really include um um an object of class transistor in a car unless of course for some strange reason um um transistor is is a derived class of car ok and the the set template also defines um um defines different kinds of operations um for for over over which are basically set operations like create_union or create_intersection um create_difference or subset_of or proper_subset_of superset and proper_superset and so on similarly there is um um just like the set template we have the bag template were um um bag is like a set um as as we have seen earlier that bag or a multi set is a set which can allow duplicates in the collection  refer slide time  15  56  however duplicate should also be objects of the same type or um of derived types which is specified in the template it cant be arbitrary objects and of course bag also specifies different kinds of um um methods like create_union and create_intersection difference and so on again in um like set and bag we have the list object type were um unlike set and bag in a list list is not just a collection of different objects it is its an ordered collection of different objects that means um the ordering between objects is also important so um you can insert an object at at the um at the head of a list or you can insert at the tail of a list you can insert after um an element um um after a particular element in a list or you can remove the first element  refer slide time  16  31  and you can remove a particular element at at some particular um um level in the list or some particular location in the list position in the list and so on and so forth and of course the the array object type which is again um um um array is like a list were were the order is important it s a collection of or it s a set of different objects or um of the of the same type  refer slide time  17  24  and were the order is important and um um and usually the number of elements are fixed unlike in an list were were the number of elements can vary there is also a dictionary data type which is derived from the collection data type which is um um um which is analogous to a hash table implementation so it it basically um stores a collection of key value pairs and um given a key um um the the dictionary returns a value and the the dictionary object also um defines a number of different methods like bind unbind  refer slide time  17  42  bind essentially means that um you are um you are associating you are inserting a value um um you are inserting a new key value or rather you are binding a a value v with lo with key value k and you can unbind in a sense delete the value associated with key value k and you can contain um um i mean you can look up the value of a key k or you can check to see whether the the key um um the the the given key um um is actually present in the dictionary or um or not and this slide  refer slide time  18  44  which we also saw in the previous session um shows um a typical i mean shows the class hierarchy that is um um or rather its more like the in inter um interface hierarchy that is the odmg does not define a complete classes rather it just defines interfaces and um the the definition of each methods is the responsibility of the programmer that is um um it can be the the definition can be written in any high level object oriented language like c plus plus or java or whatever um depending on what what support is available of course um but this defines an interface hierarchy that is um um any sub interface in a sense that is any interface um um some where in the hierarchy will inherit all the interface um um interface elements like like the method interfaces of all interface above in its hierarchy right so um as seen in this figure here  refer slide time  19  44  the the object interface is the um top most interface so so every object um object here is is a kind of a um um can cause a little bit of confusion in the sense that there is a class called object so um every object in a in a database belongs to a class called object by default and a collection is a collection of different objects and sets lists bags arrays and dictionaries are different specific kinds of or or kind of templates were um they define specific um um sets or lists over specific types of objects and then there are several other objects like classes like timestamp and timp time and interval and iterator and um date and and so on the um odmg two dot zero standard also um um defines um or also proposes an object definition language plus an object query language as we had mentioned earlier in this um um in this session so the the odl or the object definition language that is  noise  that is define by the odmg two dot zero standard um is a programming language independent mechanism of um um um defining the structure and behavior of objects what is meant by programming language independent mechanism that is um um you can have the same odl definition um interface with lets say smalltalk or java or c plus plus or or any other programming language and it supports different kinds of schematic constructs um that define structural elements of objects and um it can specify the database schema note here that the database schema is actually a graph of different objects belonging to different classes and their relationships  refer slide time  20  57  so it can specify um a database schema that is also independent of the programming language and um it has interfaces with specific languages like c plus plus and smalltalk were you can use language bindings for mapping the odl constructs the odmg standard in addition to the odl also defines the oql or the object query language  refer slide time  22  25  object query language is a mechanism by which you can query on um either attributes or behaviors of particular objects usually for attributes of course and its a query language that is um that s specified by the odmg data model and it can be integrated with um um existing programming languages like c plus plus and smalltalk or java and so on this slide here shows an example  refer slide time  22  33  oql query were um um note that um um for the programmer the the programmer is not necessarily aware of the oid um mapping or or or the oid management that is perform by the object database system instead a programmer refers to an object or or a or a unique object in the database um using things like an entry point or a reachability condition and so on so um an entry point in the um um in in an oql is a named persistent object that is that defines the entry to which um the the specific object with the particular oid can be accessed so here in this in this case entry point is this object called d so d is the name that uniquely defines an object that is stored the uniquely defines a persistent object that is stored in the database and um there are iterator variables that can um um that can iterate over or that can range over each object in the collection as you can see here  refer slide time  23  43  the the um um the the structure of oql is very similar to sql itself where you just say select d dot dname from d in departments where d dot college equal to engineering except that um um here um um the way it it interprets this query is different from the way um queries are interpreted interpreted in sql or or in the relational model here d in departments essentially means that d is um um d is the name ok for all objects that are in the extent called departments remember what is an extent extent is analogous to um an entity entity set that is um a collection of different objects of the same type or the same class um um as is so so um when when i say d in departments d is in um um is a iterator variable that is it it iterates over every um every oid of objects that belong to class departments and then it selects a particular attribute from that um um note here again that dname should be a visible attribute if it is an invisible attribute you can access attribute names only through method invocation so so you should be um saying something like d dot get name um um select d dot get name from d in departments were d dot get college equal to engineering and so on so however we are we are assuming that um d name and college are visible attributes rather than um um hidden attributes in a in the object definition  noise  just as in sql um one can define views over the um um over the odbms using the oql and um this is um quite similar to sql query except that um rather than say in create view you are saying define  refer slide time  25  58  so um and and look at how views are essentially created view um um a view is created in the form of a method that is you are defining a method called has_minors department name as this oql query that is has_minors department name um is um um select s for um um where where s is an iterator variable um for from s in students were s dot minors in d dname equal to department name and so on so um so um you you are basically defining um some kind of a method in the when when you are saying that you defining a view so you just invoke this method um you you just invoke this view as though you are invoking a particular method so so you just call a function called has_minors um like this  noise  and um the the output of this is the set of all um um students um um were um were those students are minors under working in a department um that that that is what is specified by the query here similarly um um um when when a when a query returns a set of different objects or or a collection of different objects um you can specify one single element of this collection and then use it as um um use it as a separate declaration ya this slide shows a  refer slide time  27  23  an example of this um of such a situation where there is a nested query nested oql query here which says select d from d in department where d dot name equal to computer science d dot dname equal to computer science again so um um as as usual d is a um um d is a iterator variable that iterates over all um objects or all oids of objects in the extent called departments now suppose this um this um there is only one department called computer science um this query returns exactly one object and you can define this object as a specific element and use it as an attribute of um some other um object for um um if required and and and that is the reason or that is the use for such a declaration and of course such a declaration will raise an exception if um if this query actually returns more than one elements or on the contrary it does not return any elements at all that is if there are if there is no department called computer science or two or more departments um um have a name called computer science again um some more operators that that are specified by oql are the the aggregation operator the or rather what are called as the collection operators were you can have aggregate operators just like in sql like min max count sum average and and so on  refer slide time  28  40  so you you can get the minimum of all values of min over certain attributes and max over certain attributes and and so on and similarly count um for example um um count of something like this ok where the in command we have not actually see here but in his um in is this um um is um um is this operator that test for set membership whether um s is a member of has_minors um that is the set called has_minors and note that we are defined has_minors as as another query so um um and and you can as a view defined by a oql query and  noise  you can just have one more um element like this which just checks whether any given element s is in has_minors and um average over this thing and um um um so so you are selecting s dot gpa were gpa is a is a numeric attribute and of course again just like we mentioned earlier gpa is a visible numeric attribute if its an invisible numeric attribute then there has to be a um um associated um method that that has to exist in order to return the value of gpa so so you should um um in in effect say s dot get gpa are something like that and once you have got the the set of all gpa values you can take the average of this so um um and of course all this aggregation operators have to be um um have to be apply to operators of the or collections of the appropriate type so um even though collection can be collection of arbitrary objects um i really cant um um um apply average over a collection of objects comprising of three numbers and four um animals and whatever so so its it has to be of the particular type and um so averages are generally um are average or min max and sum and so on are best defined over um the set t were were t would point to a numerical attribute or numerical object like um number um and so on similarly you can define memberships and quantifications operators in oql um and  noise  um for example you can define something like e in c were were c is a collection object and e is an element  refer slide time  31  34  and so so this um we saw it in the previous slide so were you can actually um this expression actually returns true um um if um um if e is an element of c and similarly you can say for all v in c um colon b ok that means um um in a sense it its checking whether for all v in c does the predicate b or does the condition b hold true ok so um it is true if all elements in collection c satisfy this condition called b now what is this condition um this can this can be again another expression like this that is um um um another conditional expression were e in c or e in v and so on were um um were v is used as part of this expression in b and similarly there exists or exists v in c such that b is true that is there exists at least one element in c satisfying um b therefore you can um as you can see these are quantification operators um that are for example typically used in predicate logic were by you can specify different kinds of queries so so there exist and for all and um that s existential and um universal operators that that are um um defined in predicate logic  refer slide time  33  02  then um you can also define um ordered collection expressions for example here um um um you are selecting a particular struct that is a particular attribute from a um from an object which is a struct were um um were f is the set of all um or f is an iterator over all oids in the extent called faculty and you are ordering by um faculty dot salary and designation or description or whatever ok so desc can can stand for um um so and then fro from out of these you are taking the first element so um so so essentially out of a sorted list you can take um you can look at any specific elements like first and second elements and then um um get or in in effect you are seeing which is the faculty member who has the highest salary and so on and there are um of course um um we saw that an an oql is conceptually different from um um an sql query that is um an oql um even thought it looks very similar to an sql query the way it interprets the the way an oql query is interpreted is quite different from the way an sql query is interpreted and um in oql we we define um extents and attributes and iterators and so on where um whereas in sql you have a tables and attributes and tuples and and so on and there are other differences between odbs and rdbms s especially in um in how relationship properties are handled especially foreign key relationships and um other kinds of relationships and um in a sense  noise  an obd database or odbms is a um is an un normalized database in the sense that it is it is not even in first normal form um  refer slide time  34  31  remember that um a relation is in first normal from um if it is not a nested relation that is the um um a tuple does not contain nested tuples however an object can contain tuples which can contain other tuples and so on to to any levels of nesting so um its not in first normal form and and the way relations are handle in odbms s are mainly through um um oid references and um um and oid references have little if any to do with normalization um in in an odbms and then the handling of inheritance inheritance is an integral part of um um odbms s um whereas inheritance is an alien concept in um in relational database so so you don t really inherit um um um you don t really see um types and classes inheriting or tables and tuples inheriting from one another and of course the specification of database operations and um um the there are also certain techniques which talk about how you can map a conceptual schema written in um written in the er model or the extended entity rel eer model extended entity relationship model to an odb schema rather than a um um rather than a relational schema we shall not be going through this slide in in great detail though except to note that um um just like each entity look at the first step here  refer slide time  36  18  were um each entity in um in an in an er schema corresponded to um one table in a sense in the relational schema here um um it roughly corresponds to one class for each um um eer entity in a sense ok and um relationships are referenced um um relationships are essentially um um shown using associations rather than say foreign keys or having the same values and and so on ok um and um sev several different steps um which which are or several different thumb rules um which specify which which tell a designer how to map from an extended entity relationship model to a odb schema and we shall not be going um um um looking at this in great detail the next topic that we are going to see which is again of quite um a bit of importance is the is the object relational databases or also what are called as ordbms s and ordbms s are in some sense um um um a middle path between rdbms s and odbms s and um both of them have their own um um pros and cons  refer slide time  37  10  odbms s are um um better suited for handling different kinds of complex objects like blobs and um um and say and also in providing say behavioral abstraction um were transistor is defined by a particular kind of methods and so on um um which are not present in the rdbms space however an rdbms by itself has its own um um advantages which which can not be matched by um um a a specific um um by a pure odbms database for example um concepts like normalization and um query handling and indexing storage structure and so on were lot of work has gone into um specifying not just the um um relational data model but also building an implementational um scheme over the relation around the relational data model and all of these um um would have to be reinvented for at least a large part in the object database realm if pure object orientation is being considered now the now the middle path is is to use object relational database that is um um can we use both  noise  objects within relations and vice versa  noise  so an object relational databases um um again are meant for applications dealing with complex data like sa like satellite imaging and um um weather forecasting and and so on and um were the the complex data is in turn associated with um um um number of traditional data like which can be mapped on to tuples  refer slide time  38  57  however um its not just the traditional data that that is of um interest here it s a it s the complex data that is how um um um how these complex data can be manipulated that is um what kinds of methods are there and how do we um check the state of this data and so on  noise  so there are several extension that have been um proposed for rdbms systems um um to to make them compatible to to make them also to to also support objects  refer slide time  39  50  for example there are support for extensible data types remember that inheritance is not an integral part of um um part of the relational model and there have been some um um some efforts to introduce inheritances or data type extensions into the the relational model and then support for user defined routines were were the user um can can define certain methods by which data can be manipulated and implicit notion of inheritance that is you can um um inheritance as an integral part of um part of the dbms system and some other extensions to indexing um the the traditional indexing of um um rdbms systems so um we shall be looking at one specific kind of um ordbms um as a as kind of a mini case study in a in a sense namely the um informix universal server  refer slide time  41  01  that is a um that s from informix which is um um wh which provides extensions to the traditional relational data model by what are called as data blade modules um the the idea behind data blade is as though that um um blade actually means a racer in this um um in this context that is the the actual meaning of blade um the the concept is um as though a new data types or new extensions can be cut through existing um um definitions as though like a blade cutting through a fabric so um you can actually cut through some thing and then introduce new data types and there are several kinds of additional data types that that are um um that that are proposed um for example the opaque type or rather the opaque table in a sense right um um an opaque table is one were the contents of the table are hidden just like in in an object the um if if variables are hidden um or invisible um then it it becomes an opaque table now you can actually in a sense cut through an existing database system um and introduce um um a new type called opaque tables and um so so that s the idea of a data blade module and then there are distinct types and um you you have the notion of a row type that is you can define um um a tuple as a type um um and um and a collection type which which is specified in the odmg standard as well  noise  then um um user defined functions are provided in the form of stored procedures we already saw stored procedures when we are looking into case studies in rdbms s  refer slide time  42  11  so in fact many of today s rdbms actually supports stored procedures which are in some sense um um which in some sense can be argued to be um um some kind of support for object um um databases that is stored procedures in effect define some kind of a method that works on um um or that is applicable to um tuples of specific kinds so um here um there there is an example of creation of a stored procedure something like um create function equal were um um you provide different arguments arg one and arg two which returns boolean and then um um you you define your function and um you also define the language in which the function is written were were you say it is written in language c and then end function so note here that the the stored procedure in um um in um in the in informix univ universal server doesn t really um um require the definition of the procedure to be specified here the the definition can be specified in a third party language like like language c like like c language and then its only the interface or or the stub for the stored procedure which is available as part of the database itself and um the the informix server also supports data inheritance as as an integral part of its ordbms data model  refer slide time  43  58  so for example here um um you are you are saying um you you first create um a row type ok row type is again um a tuple type which which is created like this so you create a row type as as employee type ok which contains employee name and um social security number and salary and so on and um um you are creating another row type engineer type ok which which says um um which has two other variables degree and license and then there is this key word called under employee_type that means um um what this means is that um um the the row type called engineer type is a sub type of employee type or is it a it or it is a specialization over employee type that is in addition to the um um in addition to the fields that that exist in the employee type engineer type has two extra fields like degree and license similarly you can also define function over loading as well then there are several kinds of indexing extensions that that are defined and um um  noise  um um were you can create index not just on data values but also on um user defined routines or user defined methods for example this um this this slide here shows  refer slide time  45  07  um um show shows a declaration um sho show shows an invocation were it s a create index emp_city on this function called employee city address ok so so this is this is actually a um um function over which the emp_city is being um defined so it creates an index on the employee table based on the information given by the city method that is um um it first executes the city method on all um um on all tuples of this employee table and based on this information um you create an index that is um some kind of a b plus tree index or whatever and there there are several kinds of apis that that um that the data blades provide and were were you can um um some thing like two dimensional data types and image data types and and so on there is also construct for um abstract data types were were you can create were you can define an abstract data type along with um specific um um methods to manipulate the um um variables or manipulate the um elements of this abstract data type  refer slide time  46  02  the the next kind of ordbms support um fr fr going from informix universal server let us move on to another kind of um ordbms support namely in sql three or sql three standard sql three standard has a number of extensions from from the earlier sql standard which um um which which provide some kind of constructs which are um in in some sense um um um inline with ordbms requirements  refer slide time  47  12  and for example sql three um um is divided into different parts like sql framework sql foundation um sql bindings and sql object and its its in the sql object part were number of um um ordbms support is provided as part of sql three and of course there are several other extensions that s provides like temporal and transaction aspects of sql and persistent stored modules and call level interfacing and and so on so um um as for as object relational support goes in sql three um sql three supports  noise  um um a new data type called lob or binary large large objects or or a large object and so on where you can define some kind of um um a binary dump or binary data object um to be a large object or a binary large object  refer slide time  47  48  so for example you can have a media file like um um like a video or a music file and um um as as part of as part of an attribute type and um reference it and and call it a binary large object and um there is also support for objects were um um in in the following ways were the the first thing is the the creation of row or tuple types that is just like in informix server um you can create a type out of a tuple so um once you create a type out of a tuple you can create abstract data types um were were like like a struct which contains different elements of type tuples so so you can have different um tuples that that make up an adt and then in addition certain um certain kinds of methods that manipulate this tuples so here are certain um examples that are that are shown in this slide for example you can say this is this is an sql three um um by the way so um so you can say something like create row type um um were um were basically this is the tuple that is name is a varchar um of of thirty characters and age is an integer now this tuple is um is called emp_row_type that is this is a new type which creates emp_row_type and then you can simply say create table employee of type emp_row _type  refer slide time  49  34  so so it will just create a table um of different tuples of this um type and you can extend existing rows again the the the notion of um um type hierarchies comes into play here were you say create row type comp_row_type and then you say employee ref employee row type that means um you you you basically create another um um um um create another element and then just refer to the the previous  noise  previous um um row type that that you have defined and all the attributes that have been defined there automatically come into this definition here and again you can say create table um employment of type comp_row_type or um um so so um with the new type that that you have created and  noise  in addition to row types you can also define the notion of an adt or an abstract data type in sql three so adt essentially is a user defined type for a particular variable for for a particular attribute that is um um you can actually define a struct um um comprising of different tuples and different um um different adts nested adts in in itself and then um um and then use that as a composite type um um composite data object representing a um um type comprising of different um um different attribute and so on and then um um an adt comes with built in functions like um um like the constructor function which um wherein you can initialize and instantiate the adt then there is an observer function which can um um which is basically like the get and put operation  refer slide time  51  20  so so you can get the values of different um hidden variables that that the adt defines and um mutator function which is the put um function were were you can actually um change this thing and adts have can also have may have user functions associated within  refer slide time  51  38  and there are there are several kinds of implementation issues in the in the sql three specifications um like um like suppose i um um suppose a user um specifies a function or user defines a function then um the the implementation system the the dbms implementing sql three should dynamically link to a user defined function whenever required at at run time and there are other kinds of client server issues um as to were the function is defined if the function is defined on the client and it has to be executed on the server and um or whether the function should be defined and stored on the server before hand and and so on and um its not clear whether one can run queries within functions and and of course efficient storage and retrieval of data is also important so um so so let us in a sense um um summarize the the ordbms s issues one is of course that object relational database design is much more complicated than relational database design  refer slide time  52  31  you might have already guessed why that is um um creating indexes over um functions or um or um defining inheritances and so on can hinder or or hamper with the the traditional relational database storage and access functions and query processing and optimization becomes more complicated when it comes to object databases especially because um when user defined functions have to be run stored procedures have to be run there is no way to um further optimizer to know how much time or or how efficient is a what is the behavioral characteristics of the user defined function so so that s the large unknown um which the optimizer has to um um take care of when optimizing a query  refer slide time  52  49  and um and of course there there are the notions of triggers and um transactions and integrity constraints and so on which which can further complicate the matter of um um maintaining um um an object object relational database system similarly um um because the because adts can define user defined types and um um an adt might may contain a tuple which in turn can contain another tuples and so on  refer slide time  53  47  um um it basically um an ordbms can become um un normalized or de normalized because its not even um um in first normal form a first normal form each tuple has to has to contain atomic elements so um it is necessary to remove the restriction of first normal form on um um object relational databases and but um um internally um um even though thats the abstraction that is that is provided to the user internally perhaps it is it is um still more efficient to map the the user abstraction that is that is provided to an internal pure relational database schema so let us summarize what we studied in this um lecture or in this class um um starting from the previous lecture on object oriented database systems um we looked into the odmg two dot zero standard and what kinds of um um object definition language and object query language constructs that that it provides and we saw how um um oql queries although they look very similar to sql queries are interpreted in a very different fashion in a in a odbms and the um the the relative short comings and advantages of odbms we saw rdbms prompted the um um emerges of ordbms or object relational database systems and we essentially saw two kinds of um um specifications the informix universal server and sql three um support for ordbms and and of course ordbms themselves um are by know means as elegant as the pure um um relational database that means they they don t have a nice um specification and there are several issues like query optimization which is a major issue um plus storage and retrieval and so on which which still um um they have to contented with  refer slide time  54  53  so with that we come to the end of this session transcription by  vidhya proof read by  shobana database management system lecture # 39 xml-advanced concepts dr s srinath hello and welcome in the previous session in the dbms couch we have been looking into managing xml data  refer slide time  01  15  as we had mentioned earlier xml um is kind of a mark up language which is um um which is in some way the the de facto standard for information interchange over the internet and the the the power of xml comes from its simplicity in the sense that um it s a um its it s a mark up language which were which can be um read and understood by both humans as well as machines and um its independent of any operating platform or in independent of any um um any standards like how schema is describe and so on and essentially it s a self describing um um it it describes a self describing data set in in a sense ok now um in this session let us um um let us continue with xml with by looking into um some  noise  some advanced aspects of how xml data are stored or how they are queried and and so on and xml as i had mentioned in the previous um um session um has elicited um wide area of interest and there are several different um not just in computer science in the sense that xml is elicited interest um across several different disciplines because information interchange or um um managing um different facets of information different um points of information and integrating them interchanging information is a um um is a common problem in several different domains and there are different kinds of um there are different kinds of standards in specific domains like banking or finance or bio informatics or um um um transportation or whatever and several different domains which have defined um xml dtds um um which pertaining to their um um specific area of concern so so let us look into xml little more deeply in this um um session and see what we can do with xml um first of all let me start by acknowledging that um some of the material in the slides have been um der derived from an invited talk by jayant haritsa in the in the vldb summer school um held in bangalore in in june 2004  refer slide time  03  36  um  noise  so so let us um um have a brief recap of what um what are we studied about xml and um um  noise  and what are its main features like i had mentioned before xml is a platform independent and standardized extensible mark up language now each of these different terms mean a very specific thing platform independent in the um um means that you just store xml in plain vanilla text character data essentially and um um it every platform um no matter what what you use what platform you use would support textual data regardless of whatever underlying encoding that that you are using  refer slide time  05  04  and it s a standardized mark up language in in the sense that there are specific rules um um that that that specify how an xml data set should look like for example it should look like um it should be a rooted tree and um um a begin tag should end with a end tag and there is this notion of well form ness and validness and so on and its an extensible mark up language the mark up language does not define what are the kinds of tags that that you need to have in your xml data you can define your own tags  noise  um and we we took up some example where examples where um um a notice was was a kind of tag notice and slash notice or from and to  noise  and so on and so forth its a self describing dataset in the sense that the structure of the data um um is um is implicit in the data set itself or meta data is embedded um within the dataset itself so you don t need a um um you don t need a separate descriptor for how the data is um um how the data are to be described of course you you may have a separate set of um what might called as um restrictions um or  noise  which which specify whether that description is valid or not and its a very simple standard for data interchange and um um made up of um um simple um um simple building blocks like elements and attributes and nesting of different elements and so on and we also saw the difference between what is a well formed xml fragment and what is a valid xml fragment a well formed xmls fragment essentially conforms to the xml structural requirements i mean structural requirements of an xml um document in the sense that it is it s a rooted tree and it is the the nesting is proper every begin tag is closed by an end tag and tag names do not have spaces in them and they they do not begin with a special character and um and so on and so forth and um but a valid xml data um has to be well formed of course that is um um unless data set is well formed it ca n't be valid however valid is most stringent in the sense that it its not sufficient for um for an xml data set to be well formed but also it should conform to a given document type definition a dtd or a xml schema that is a schema that is specified as part of this document and why um xml or what is the um significance of xml it s a its a very convenient way probably the most convenient way of exchanging data over the web  refer slide time  07  12  and um like i had mentioned before its easier for both machine understanding and human understanding so in the worst case um you can actually open an xml document in um um in a text editor like notepad or emax or whatever and um um understand what what the xml data is um is all about and make changes if required and um um it it has a simple tree structure like a simple hierarchical structure which is easier to understand um um easier to enforce um um because the tree has very specific constraints and well known constraints as to what makes up a tree structure um and um enforcement is also easier that is its its easy to build parsers which um using stacks or whatever there are there are several kinds of tree algorithms that that can validate um a given tree and its also easier to navigate nice um um directory structure is a is a tree structure and hence xml data sets are usually rendered in the form of a directory structure on um on browsers and like i had mentioned before xml is eliciting interest not just in the computing community but perhaps more so in several other communities and information interchange is um is not just a problem with database researchers or um um computer science researchers but it s a it s a problem in almost any domain of concern  refer slide time  08  42  what is the information interchange problem um um simply stated that um um simply stated it means um um it it is a problem of how to integrate the varying or desperate sources of information that exists in any given system um fpml for example as slo as shown in the slide is an xml standard for the finance industry and um um fpml essentially um standardizes the different um um um different kinds of contracts or transaction data sets that um that that are routinely specified generated and specified in the finance domain um it pertaining to banking or stock trading or um loaning and and several different allied activities were um um contracts would would have um more or less some common features in the sense that um or contract would would have to specify what are the parties to a given contract what is the validity period of the contract when does the contract expire um what are the components of a contract and what s the amount involved in this contractual interchange and so on so and there are several um standards that um um that that are used independently from um by different sources and fpml in in effect tries to standardize um this this contractual specifications so that it becomes easier to integrate different um different kinds of contracts that that are generated in different um places similarly bioml as shown in the slide is um um is an xml standard primarily for use in bio informatics applications  refer slide time  10  27  and um that too in um annotating gene sequence data and  noise  gene sequences as you know are very long sequences comprising of um um one of four different characters eighty g and c um sometimes u and  noise  and um sub sequences of this long sequence specify um th this long sequence is what is called as the genome sequence and um and sub sequences of this um genome sequences specify um genes or sometimes what are called as um um some um a set of code on so which which code for a particular kind of behavior in in the organism or or however when when the when the gene folds so um such kinds of code on such kinds of um um code on strings or or genes are annotated and this annotation is done in several different ways by several different researchers and because there several researchers um trained to sequence genomes of different kinds um and including of course the human genome and um and or um and um or annotating different parts of um a genome string now um bioml is a standard which um specifies how this annotations have to be um um had to be performed and of course it it allows for free text for annotations that is it does not um um it does not really place any constraints on what should go into the annotations themselves but um it specifies how and where annotations should um should have and um bioml in addition to its um um for example fpml or bioml actually specify the schema um they they basically specify the xml schema of this so the xml schema also supports full text queries for um different kinds of sequence matching um um problems and then the um um and then a very commonly used standard for um standard that that uses xml is the soap protocol  refer slide time  12  37  you might have heard of the soap protocol which stands for simple object access protocol um which forms in some sense the building blocks of web services um um soap is a um mechanism by which a software can access a remote um object or invoke a send methods to remote object via the internet so um what is the soap protocol do it is sim simply a message passing or a message sent to a remote object that is wrapped in an xml document that is the the message is sent as an xml document and at the other end were the services provided the xml is parsed and the actual message is taken from from the xml document and then sent to the object and question so in this um for example you might have some kind of an object that performs um um a given kind of calculation let us say um um um let us say currency conversion ok so so so you might want to perform some kind of currency conversion um and that is a service that that you provide over the web now um if this currency conversion object is soap compliant then the clients speaking with your object or speaking with your component um sends messages in an xml format and the the object contains an xml parser embedded into it which parses the xml document and divides the xml input into two different parts a header and a body so the header contains meta data about the message saying what what kind of message were is it coming from and um any other restrictions and the body um um contains the actual message itself  noise  so um um this slide shows a schematic diagram of how um um an xml how  noise  xml processing would would probably look like  refer slide time  14  24  given xml document given an xml document um we have um um what is called as an xml parser um when an xml parser um um can um  noise  can check for the well form ness of an xml document so so that it can parse an xml tree and create some kind of a data structure here um also called as an info set which in turn um goes into a document validator document validator um um essentially  noise  validates a given xml document against an input dtd or an xml schema so um um once the document is validated then the xml is ready for the application or the storage system were whatever it is being used for and of course the application itself um um um accepts different kinds of queries using xquery or xpath and so on and can  noise  can answer queries on the um um info set that it requ that it received from the document validator so what are the key concepts in in xml you have the data model which comprises of elements attributes and data  refer slide time  15  57  and rooted tree structure um um plus of course comments and processing instructions and namespace declarations and so on and so forth in addition you have the dtd or the xml schema which specifies the meta data structure and um um um and it it spe there are several different features that that are available in um in a dtd in the sense that it can um um it can define regular expressions over xml tags so you can say that notice can have zero or more um um headers or one or more um um two elements and so on and so forth so here is an x example xml fragment were um this is a rooted tree again um its its an xml element but every element um every well formed element is a rooted tree in itself  refer slide time  16  40  so this is a rooted tree which begins at imdb and slash imdb and of course um you might know that imdb stands for the internet movie database which is um um um which is a huge source of xml information that is it sto it stores a lot of movie related information in um in xml and then um imdb itself contains an element called show and show year equal to nineteen ninety three and comments are given like this that is um um angular braces with with a exclamation mark followed by two hyphens and then again two hyphens with the um angular braces here so show is the element from here to here so the show element contains one or more reviews so um review one starts from here to here and review two starts from here to here and then there is a set of box office numbers and um each review itself um in turn um says were the review is from ok so so this one says the review is from sun times and the review itself is um um a mixed xml plus free text that is um um there is there is free text going around here um um within which there is um there are some xml tags that is roger ebert is um um is surrounded by the reviewer tag and two thumbs up is is given is surrounded by the rating tag and so on  noise  so um so so that s how a simple xml fragment would look like and a typical dtd for this would would look like this  refer slide time  18  22  were um you start your dtd by a doctype declaration and then specify what is the root um um element of your xml fragment then within this specify each element and the set of attributes so imdb um is the root element can contain show start show star essentially means that zero or more occurrences of the show element similarly the show element contains title comma review star that is it has to contain exactly one title followed by zero or more reviews um review elements  noise  and show can also contain an attribute called year which is um um character data and title is character data and and so on so so i have not completed the um um dtd as here but um this this would what typical dtd would look like for this xml fragment in contrast we have um what is called as xml schema um which  noise  um um which is um um an emerging standard for um which is fast replacing dtds  refer slide time  20  00  xml schema is far more expressive than a dtd in the sense and it supports um um it supports its it s a strongly typed um language in the sense that it supports a types of a of a particular kind and most importantly an xml schema document is an xml document itself that is you can use the same xml parser to parse an xml schema as you would use for parsing an xml document so how one how would an xml schema for this um for for this xml fragment look like um um you see that it starts with um um an element declaration and um the element declaration has a um  noise  has an attribute called name equal to show so um um i am not starting from the imdb declaration itself i am just declaring um i am just declaring the schema here for the show element so so this one says the the show element is declared like this that is the show element is a complex type  noise  having a sequence um sequence of a two or more different um um elements where the first element in the sequence is is the title and the type of this element is um string right  refer slide time  22  58  and this is a single element so so the element is closed right here and then the second element is um um and then um and then there is a sub sequence so so the first element of of this sequence is um is an element of um name title and the second element of the sequence is another sequence ok um um which which can have which can have repetitions by itself ok so minoccurs zero and maxoccurs unbounded is another name for saying the star declaration in dtd right that is um um something can occur zero or more times so what is this sequence contain  refer slide time  21  27  this sequence contains um um an element called review and which is of mixed um data that is um um a review can contain both um um um free text and xml tags in itself and of course this this sequence finishes now um so um and then um um and then after this there is a choice um element that is the third um um element of this sequence were i can have a choice between box office or seasons or so on so so one of these two i can have either box office or seasons as the third element in the sequence so note that um um show itself comprises of three different elements title um review star in a sense and box office or seasons which was not specified completely in the dtd in the in the previous slide and of course this sequence finishes and um the the next is um um the next element the next attribute of of this element is year which is optional ok and slash element so so that that is um that is typically what um an xml um  noise  schema would look like as you can see here this is a simple um xml document in itself and then you can parse this xml document using any xml parser and then based on the outputs of this parser you can enforce um um the the enforce the constraints here on the given xml fragment let us now turn to a specific problem that um that that occurs in um when when parsing xml data um namely the notion of naming conflicts  refer slide time  23  20  now consider these two xml fragments here that are shown um um one xml fragment is shown here and the second xml fragment are um is shown here ok the first xml fragment um if you have worked with html you might recognize that this is a html fragment and html fragment can also be treated as an xml fragment there is there is there is no harm in that right so um of course well formed html fragments that is in the sense that um um every um um start tag is paired with an end tag and there is a um nice hierarchical structure for this fragment so so this is a table fragment which begins at table and ends at slash table and um there is a table row which begins at tr and slash t um tr and table descriptor that is some some table cell which begins at td and slash td ok and then somewhere down the line there is um um another fragment here which says table slash table and name and width and length and so on and so forth so so african coffee table eighty and so on now these two could be um um could occur within a single xml document that is um um this could actually be some kind of pcdata that is parsed character data for um um given xml element and this could be another xml element by itself but these two tables are different now this is required because um the the data contains html and and the html has to be rendered and this is required because the semantics require that um um we need to declare some thing called table and these two tables are different now such a conflict is called a naming conflict that is when two or more tags um mean different things but um have the same name in in them so in order to resolve conflicts  noise  how would we resolve conflicts  refer slide time  25  13  one simple way to resolve them is to prefix each of these um um each occurrence of this meta data with a particular um um string so for example um um um this one shows  refer slide time  25  28  a prefix called um h where it says um um were the the html table is prefix by um um a character called h so so h table and slash h table and and use the note the use of the this colon character here so um um so so the html table is um um is discriminated or um or distinguished from the um xml table by using a different prefix called h  refer slide time  25  55  and similarly the um xml table itself is prefixed by f ok now that is one simple way of doing that but again um the the question still remains as to um um what if the prefix also is the same i mean if um um if if the name of the tag can be um um can be identical the the prefixes can also be identical when um two or more um xml documents are um brought together the and and um  noise  and and meant to be integrated into into one schema so for this the notion of names spaces becomes valid ok or namespaces become become important a namespace essentially defines a unique space um globally world wide um across across the web ok so um um how does it define a unique name space across the web remember the notion of a url or a or a uri uniform um um resource identifier a url are a um um are the are the more general form which is the uri is a unique name for a resource across the web therefore whenever you see a um um address like this  refer slide time  27  09  http colon slash slash osl dot iitb dot ac dot in whatever it is unique that is this stands for um it has to map on to a single ip address or a single um um address across the world wherever um um wherever in the world that that it is referred to now using the concept of uris um we can um we can ensure that um naming conflicts in xml documents can be resolved for example we can um when when we say that um when we put a prefix called h for example for for this for this um table  noise  um meta data ok for this table tag then we decr um we describe that h means a specific uri ok now this could be um um this need not actually um contain anything the the uri need not contain anything with re um um anything with respect to this table but let us say this is the uri in which um we work in let us say were we created the xml um fragment  refer slide time  28  15  now once we um alias a given prefix to a uri it becomes unique so in a sense were um whenever the xml parser looks at um looks at a prefix called h followed by colon ok it replaces that with this uri ok so if a different h comes from from a different source and it maps to a different uri it means that um it is a different prefix therefore um its um it it it basically means some other tag and and not the same h as this prefix that that specified here so similarly we can use a different um um that is the same uri having a different prefixes so that different prefixes from the same uri can can again be um um distinguished therefore um um what the xml parser actually does when it encounters name called f colon is that it replaces f colon by this whole thing slash f  refer slide time  29  20  internally that is um http colon slash slash osl triple itb dot ac dot in oslwiki slash f um and so so that several different um prefixes that have been defined at this uri remains distinguishable  refer slide time  29  30  so so the namespace attribute um as we saw um in the previous slides is is the is placed in the start tag of an element  refer slide time  30  00  so for a given element um you can for every um every element you can define a separate name spaces were names for that elements are uniquely describe globally across the world and the syntax for name space um um begins with an xmlnx xml namespace um um declaration followed by a namespace prefix equal to um um a coated string following the namespace or coated string following the uri and um  noise  it basically gives a qualified name associated with a namespace  noise   refer slide time  30  25  so all child elements if if i define a namespace for a given element like like in the example table there all children elements of um of this um um of this element are associated with the same namespace so so there there names are um um term to mean unique um meta data that have been defined in this uri the address is just simply the the uri address that is specified is use to simply identify a namespace but the parser itself does not try to connect to that uri or um um look up that information or or whatever it um it it is simply used for resolving naming conflicts and it doesn t necessarily have to um check or validate whether the whether the uri exists and whether the uri defines this names or anything of of that sort  refer slide time  31  19  similarly let us look at some more issues related to cdata cdata if you remember is character data um character data um um when when i define an element as um as as character data then it means that um everything um um until the slash of that element is taken in without parsing that is the the the data itself might contain other tags which are not parsed and everything is taken until the end of that element so everything is ignored until the end of this element and  noise  but but then if if the text or the character text contains a number of these characters let us say less than or ampersand and so on um it is quite easy for um xml parsers that that especially if if there little buggy it can it can be quite easy for xml parsers to get confused and um  noise  and look and try to parse them and um and especially if it is html data and not well formed and the the parser might flag errors even though the xml itself is well formed and um and valid and so so there is another way of declaring cdata were um um a cdata section can be declared using um um a specific tag like this so um this slide shows an example were um script and slash script is is an element that defines cdata  refer slide time  32  44  and um this element um this element contains a cdata section which says cdata and then um um um defines a um um define a function or defines some script here with less than and greater than symbols and ampersand symbols and then um ends the cdata section like this here  noise  now um what what are some of the rules for cdata sections simply that the body of a cdata section can not contain this string which which defines the end of the cdata sec sections  refer slide time  33  28  hence um which also implies that nesting of cdata sections is not allowed so you ca n't um um you ca n't have nested um  noise  like like you cant have nested comments in c you ca n't you can not have a nested cdata sections and um there should be no spaces or line brakes inside this um this string  refer slide time  33  51  now let us look into um querying xml data and what kinds of query languages are present and  noise  and um um what paradigms of query queries exist ok the the common querying tasks that um that are usually done over an xml data are something like filtering and selecting navigation selection extraction and and so on in addition you you could you could define some kind of joins or aggregation like like we do in sql and um um transformation that is convert one form of an sql data to another so um before we start up with querying itself we have to address a  noise  more fundamental problem or fundamental issue of xml parsing itself and there is one there are two specific kinds of xml parsing which which have implications on how um um querying is performed and let us briefly look at these two paradigms of xml parsing and what kind of impacts they have on um um on queries  refer slide time  35  27   noise  the first kind of xml parsing is um um parser is what is called as a full navigational parser that means um um of of which you might have heard of this name called dom or document object model um this is an example um um of of such a parser which requires an entire xml document to be available um for it before it can start parsing that is the the xml document should be full complete and um and well formed and so on so and which can then be parsed and and dom basically creates um some dom object which can be um called by um um application programs in order to query the xml data so the the application requirements state that it should provide full navigational access to the document so you can not have partial um xml string that is available and ask the dom parser to start parsing and kind of queries that that that dom allows would is something like like this that is dom essentially creates a docu creates an object which contains a xml document and then you can say um um some thing like if document element dot getelementsbytagname show then um so so wh which basically gets a given element of this thing then um for s in this thing that is s is a um element which which is which is the set of all show elements and in the show element if the title contains um the fugitive then um um get the review and and so on so so you can address um um or dereference an element by by the tag name and then it returns an element object to you and then you can get the data associated with that object and so on so um this slide here shows how the dom um  noise  parser works that is you you give an xml document plus a dtd or xml schema to to the dom parser  refer slide time  37  02  the dom parser performs both parsing and validation um um and creates an object um or a dom instance which um um which is then given to the application the application then um start calling the dom instance by um um by using it or integrating it with its um with the other sets of its objects the other kind of xml parsing is what is called as stream based parsing and um  noise  and um example for such a stream based parser is sax which is um um which is also widely available on the um on the microsoft windows platform  refer slide time  37  46  again this is a language independent and programmable um programmatic api um and sax in contrast to dom does not require full navigational access to the the given xml document instead um you can stream the xml data um and it parses as the xml data parses um um parses through it that means you can put an x um um you can put a sax parser on a network stream which is um um which is sending you xml data and as and when the xml data streams through sax creates a um xml um um object i mean it it creates an object on the fly for the xml data that is parsing in other words it also means that sax performs just one parse over the entire xml um stream and there are several applications were were sax um is more important something like stock quotes which are um which are sent in a streaming data um um over the web and as and when codes change data streamed and an xml parser a sax like parser can can parse them and what um um what sax does is it also has um um a feature of call backs that is um sax not only parses the xml data but it also um creates events that can call back into the application and and interrupt the application and tell that something is happened so so um so take appropriate action and so on on the other hand in um in dom it is the application who is in control and the and the applica the the dom just creates a dom parser just creates a um creates an object and the application decides when to call the object and what to do with the object and so on  noise  and of course there is um in in sax its its um read only access for for un typed notes and there is no um um in place updates in that s um thats possible so this slide shows how um um a schematic diagram of how the sax parser works that is um um you have a xml document plus um dtd which is streaming that is which need not be um the the entire um document as such  refer slide time  39  56  now um this this streaming xml um document um parses through a sax parser and validator which in turn sends sax events to the application that is the application keeps interrupting um um sax parser keeps interrupting the application by sending appropriate signals or events to the application and the application has to perform um um specific um tasks associated with each of this events so so it will say that um found an element called show and um um which is valid and this is the element and so on so as and when a show element comes the application um um um says that oh a new show has um has arrived and this is the data that has to be rendered and so on and um so the the application begins the sax um sax parser but once the application begins is the sax parser that s in control which um calls back the application so um um let us look at xml query languages now look at these two kinds of parsing techniques lets see how um um how they can affect queries as well  refer slide time  41  00  but um but of course before that we should look at um um different kinds of xml query languages itself and um some some examples are shown in this slide here you have query languages like xpath um xpath two dot zero which is um which is very commonly used language for specifying um um navigations or selections or or extractions from from an xml tree and its also used in um um i i mentioned the um um i mentioned the name called xslt which is um um which is another kind of um um query language its more like a transformation language that is it can convert um xml to um html or it can convert xml to text or or one form of xml to another form of xml and so on so if you want to render an render an xml document you um um um generally use an xslt query language which will take an xml document and um give out a corresponding html document in return and then there are um there is a xquery kind of language which um um which is a um what what may be termed as a um um composible language in the sense that one xquery takes an um takes an xml document as input and um gives out another xml document as output its very similar to the um um relational algebra queries which takes a relation as input and gives a relation as output and it s a strongly typed xml um it s a strongly typed query language and useful for large scale database accesses itself  refer slide time  42  56  so um let us briefly look at xpath um um xpath is a syntax for defining parts of an xml document and um um basically the way it defines parts um um is to use a directory like parts structures were were um in order to define xml stands or xml elements  refer slide time  43  02  let us quickly go to an example to to illustrate this um um this slide essentially says that path expressions in in xpath look very similar to the directory structure in a computer file system so so i i might have a directory called this slash this and slash this and so on so um um if this what to be an xpath expression all this would be elements so so some um osl dot triple itb dot ac dot in would be an element and grace would be an element and and so on  noise  so this slide shows an example were an xpath selects um um a book element ok which lies under a catalog element um were the price attribute of the book element is equal to ten point eight zero  refer slide time  43  48  and the catalog element is the root element of this xml document so so this just like the root directory in your um in your simple file system language and um um so um the the root directory contains defines the element called catalog under the catalog look for an element called book and look for an attribute um um matching this particular criteria similarly there are other examples here its um um if you have a double slash like this as shown in this example it says that return all titles at any level um in the imdb um xml document  refer slide time  45  14  that is the root element is imdb and title can occur at any level so so double slash essentially means um um any level in in your document and similarly this one double slash the beginning again says that um um return a show element at any level ok um and doesn t matter what what the root element also is but were the show um um year is greater than two thousand that is all shows released after two thousand and you can also have full text operators like text contains russell crowe and and so on so um um were anywhere in um see here there is there is no element name as well that is given right so it just says star so um any element in this xml document at any level um return that element were the text contains russell crowe so that s about um xpath were um it s a um it s um very brief introduction to xpath were um which is which is a um um file system like language for representing different um navigational aspects of of an xml document and this is were the hierarchy or tree structure of an xml document becomes um um  noise  beco become significant in the sense that it is easy to express a tree structure in the form of a directory structure as as done in xpath plus it is easy to enforce constraints  refer slide time  46  04  that is in a tree structure you um um a node can have um at most one parent um that is um any non root level node will will have exactly one parent and um um there is exactly one path from the root to to any given element in a xml tree so so you can actually specify one um long file system like path which um which uniquely identifies each element in the xml document the next kind of query that that we are going to look at or query paradigm that we are going to look at is the xquery paradigm um xquery is a functional um um language in the it s a strongly typed query language for um for querying xml data and xquery as i said um earlier is an xml to xml um converter that is um um um it can it can query an xml document and return an xml document just like the relational algebra um query is that that can query a relation and um um return a relation an xquery in turn can use xpath expressions for its queries so um so xpath can be can become a part of xquery for in in specifying its um um query expressions and um its um um many people would term xquery as um as an sql for xml databases so its its analogous to sql in the sense that it takes an xml document returns an xml document it it contains several different um operators um um it can contain several different um um sophisticated query operators some thing like select from where or um and and so on so so you can actually specify a query like select all papers which have been sighted more than fifty times from the collection of papers um um stored in the xml document called citeseer papers and so on so um you you can give complex expressions based around a select from where kind of kind of class so um what are some of the xml xquery features um um there are what are called as um um flower queries or flwr queries um which um which say that um which stands for for let where return um repeat queries  refer slide time  48  20  that is um its a it s a looping kind of queries where um where you can specify a for condition initial condition um let where where is um similar to the um sql where and then repeat so so you can repeat these queries we we will see an example of this shortly um then there are sql like select from where clauses were um um select so and so from this xml element where um um this um um this condition matches then there are sort by operators so you can sort elements based on certain attributes um then xml construction that is transforming one xml document to to another xml document you can also have user defined functions were um um um on on this and and xml xquery basically supports strong typing so so you can say something like an integer or um um character or you can perform um um operations that are specific to integers versus character strings and so on and it also has supports for processing recursive um um data sets here is an example um of an xquery um um query that is a query return in in the xquery language so  noise  the the query essentially says that for each actor return box office receipts of films in which they starred in the past two years ok so um and essentially um let let us go back to let us go first to this last part of the xquery document were um here you say lot of let for um um and and so on and so forth um for let where um and and so on and repeat and so so um  noise   refer slide time  50  04  but um um this last one here um um is what is is going to tell the xquery engine what to return ok now here that imdb engine or or the imdb xml document did not contain um an element called receipt ok however what it does contain um um is um or elements like box office or actor and so on and what the x xquery um um languages returning is um an xml element called receipts and slash receipts um which in turn is made up of um um actors and totals and so on so so receipt is um um receipt essentially is an xml document comprising of character data which which tells what is the actor and then one more xml element called total which contains the the sum of all um  noise  um um sum of all box office um um i mean films per box office um receipts that that they have obtained over the past two years and here um um an xquery for example can um can first define variables like using a pascal like syntax so so it says let dollar imdb equal to this this document so so imdb basically specifies this particular document and then this is for any actor in imdb actor that is actor is another variable note that all variables are prefix by a um by a dollar sign um and were actor stands for any actor element at any level in the imdb document so for any actor in this in this thing um let films equal to um this this um this one that is um um the show element were um it contains a box office element and year greater than two thousand and um actor name is um um um the the particular actor is um um is reg um is um entered as a star in this in this show so so star name and so on so um so essentially you you let this one and um for each actor this this for let and the the where is implicit here and um um return um um  noise  um is um um is also specified so so so this one um um iteratively performs for for each actor at at any level in the um specified in the um imdb document and then um it returns a set of this receipts elements which um which says actor and total that the actor has grossed and um here are some www links  refer slide time  52  51  or or world wide webs were you can um um were you can get more information about um um dom parsers or sax parsers or xpath queries or um and xquery um standards and so on but um before this um let us go back to um um the  noise  let us go back to the um um the the parser problem that that we are talked about that is how does a parser affect the query queries that that you give on an xml document um both dom and sax parsers for example um create  noise  um create documents or create objects um which the application can access that is which the application can send messages to an access however the dom parser requires a entire xml document to be present um whereas the sax parser um um calls back into the application as and when um um xml um um elements parse through um the the parser ok now um um what kind of implication does it have on queries um let us take the query here um let us say for each actor return box office receipts of films in which they starred in the past two years um this is when you give this such a query to a um xml document that is parsed using dom um um you essentially know that the entire xml document has been navigated and parsed and and that is present in the object that is available here so you can essentially um um go and look through the object and return um um return the query results on the other hand if it is a sax parser and it s a streaming xml um um data then you wo n't be sure whether you have encountered um all possible actors in this loop that is take a look at this  refer slide time  54  45  for actor in imdb slash actor so you will you will you will have no idea whether all actors have been um um have been processed as part of this um um as part of this xml document so in in such a sense in such cases um this query has to be in the form of what is called as a standing query that is in the traditional database setting um the da the database is static and the query parses through the database and then returns query results but here it s the other way around the the query is standing the the query is static and the database or data set parses through the query and the query returns backs um um are call returns events or performs call back um into the applications whenever um um a data set matching the query is available so so as and when the query finds um um an actor and a show um um um satisfying this criterion it returns a receipt kind of xml fragment by calling back into the application  noise   refer slide time  56  14  so let us summarize what we learnt today in this session we started with xml namespaces and and how to resolve naming conflicts by um um by assigning a unique globally unique uri for each um name prefix in an xml document and we also looked at some cdata handling issues were um especially if your document if your cdata contains um um angular braces and ampersand symbols and so on you can embed it within a cdata section next we looked at two different kinds of xml um um parsing namely the um um um navigational um parsing for of the entire document versus stream parsing and then we looked at xpath and xqueries as as query languages um um or query paradigms over um um um um over xml data sets and how these kinds of parsing techniques can impact the the kinds of queries that or how the query behaves in each of these um um parsing techniques so that bring us to the end of this session transcription by  vidhya proof read by  shobana database management system lecture # 40 xml databases dr s srinath hello and welcome  noise  in this final session on introduction to xml we shall be delving um this primarily into xml databases that is we we um until now we have been talk around what is xml and um um what are the benefits of xml or data interchange using xml and um um xml queries or schemas and so on  refer slide time  01  49  but today um um in this session we shall be looking into xml databases and what are the issues that um that come up when xml data has to be stored or transmitted and um exchanged over um um pre existing or some set of existing data stores we will we shall also be looking into the um larger problem of what is called as semi structured data management um semi structured data as you might have imagine from the name is um data which do do not have specific rigid structures and um we shall show that more and more the data requirements of of today s world what we called the post internet world is um is being defined by semi structure data rather than structured organized data um and semi structured data possess some unique problems in fact some um fundamental problems in um um in in database management and um what we shall do here is that we should in this session is we shall only motivate those problems rather than providing any solutions as such many of which are um in some sense um um areas of active research on so we shall um um we shall be motivating the problems with some examples or some um um analogies and instances um and we shall conclude this session with a list of what one could expect in um in terms of semi structured data management so let us proceed further with with this session and um as with earlier session some of these material have been derived from um um from an invited talk by jayant haritsa in um in the vldb summer school held in bangalore in june two thousand and four  refer slide time  03  20  and of course so including many of the analogies and jokes that have been there in the in the slide and so on so um um here is an acknowledgement for for those set of um material that have been derived from those slides so let us um um look back at xml again and recap what we have learnt about xml and um um what are its characteristics um first of all um before we look into the points in the slide itself um let us remember that xml is a um platform independent standardized and extensible mark up language so in a sense that um its platform independent because it is written in character data and every platform is um should at the very least support character data and its an extensible um um mark up language that is it does not have a specific set of keywords as such i mean of course it has key words were were you say um um were were  noise  you give declarations but um for describing the data itself it doesn t have any keywords the the user um who is describing the xml document can come out with his or own set of um um tags or meta data that describe how the data is organized and how um um what semantics to attach to each data set in this in in the in an xml store and um it xml um um syntax has a plain hierarchical structure which is easy to navigate and easy to enforce and easy to parse as well the so we have um um as a result we have query languages like xpath which translate an xml query into a um directory like path expression which is quite intuitive for um for users who have been using computers and so um to to traverse directories and look for something in a in a specific directory and and so on and um last but not the least xml um is understandable or parsable by both the machine and the um and the computer so so if every thing fails what you can do is um just open  noise  an xml document in notepad or vi or emax or some some such text editor and look through the document and see where um um there could be a problem especially if um may be starting tag doesn t have an ending tag or some um some problem of that um sort so um um and and of course there are issues like what about the amount of space extra space that xml takes up and this can be answered by the fact that um one need not store xml in character form on disk you might want to actually compress xml data and then store it on disk so that in you can um um and and decompress it when when reading it so so that the actual disk space that is taken up by xml is is reduced so um um let us um um address the question of data interchange now using xml xml is said to be a platform independent source of um um data um representation and um um representing data that is self describing in the sense that the meta data is embedded within the data now but what is the problem what is real and and of course the the um  noise  the advantage of this being um um that you don t have to worry about whether the end user is using the same platform as as yours  refer slide time  03  38  if you are using linux the end user could be using windows or um or solaris or mac or whatever and um and  noise  the the end user could still um um use your services using soap or um um some other form of xml based message parsing protocol and you can still answer um or you can still provide those services via an xml wrapper but what is the um um interchange problem actually or um or in its entirety um for for the interchange problem we have to first um um note that most data is already stored in some existing databases um its its quite um its quite unlikely that databases that have been existing for um large periods of time now will be replaced by an xml data store or will will be replaced by something else the that is unifying so um databases are going to continue to exist and the the question of um data interchange will balled on to the fact that how to interface these databases using xml or using um um a common interchange language and um databases will not only exist will will be updated through existing interfaces its again quite unlikely to expect that um all um updating and all interface to the databases will will come through um a a common interchange format so what is really required is um um is to provide xml wrapping to existing data bases and soap for example is an ex um is um soap is an example in this in this regard that is message parsing between objects existed um um long before xml came into came into the picture there where several different object middle wares like corba or um or dcom and so on which um um um which supported message parsing between objects over a distributed system and but however um one needed to be corba complaint in order to be able to a platform needed to be corba complaint in order to be able to support corba that is corba had to be return for that platform on the other hand um um in in when using xml it is it does not um bother about what kind of platform that that s that that s being used because every message is parsed using an xml wrapper so soap is some kind of a generalization over message parsing frame works in distributed um object base systems which um um which has been extended from um um simple small um distributed systems like like corba to a to a larger web base services like um using soap so um a similar analogy also exists in um in data integration were data exists in different databases in pre existing forms and pre existing um um applications and they have to be integrated using by wrapping the data around using xml wrappers so one simple way to convert databases and now here we are assuming relational databases and um for the most part we would be correct  refer slide time  10  18  because um um majority of the database implementations around the world are relational databases so so assuming that existing databases are are relational databases and um we need um we need to um we need to be able to um interface between these different relational databases um which are across different platforms a simple um way to perform this is to convert relational tables in their canonical form to xml documents and this slide shows  refer slide time  10  49  such an example were um there is a slide called actors containing of two different columns um last name and first name and this table became one xml document actors and slash actors its it s a rooted xml document and it is some kind of a flat xml document in the sense that the number of levels in this document is fixed that is the actors file or the actors document comprises of several um children called row row and slash row were um each row comprises of again several children each child corresponding to a specific attribute or or attribute name and this is a attribute value within the attribute name so um um its quite simple to map um um a given relational table on to um um a canonical flat xml file and given a flat xml file its again straight forward to map it back into a um um relational table  noise  now let us come to the question of storing xml documents itself that is um um having xml databases itself um um in many cases um um um for example in in several commercial implementations oracle nine i and um db two and so on  refer slide time  12  07  um the the database gives an impression that it supports xml storage on um um and xml based updates however um um what it actually under underneath its still a relational database and then um the there is an xml wrapper around the um database however that is now there are several other approaches to storing xml documents and in many cases it becomes necessary um to to treat xml not just as an interchange language but also as as the language in which the data is stored and um  noise  and the some of the issues that occur in this regard um would be issues like data layout how would you organize the xml data there are again many alternatives to this um to this question one can map xml data on to um um on to a relational database or one can map an xml dataset as a special kind of object in a um in a object relational database or one can um store xml data in in native xml form or in text form and so on and  noise  and um what about updates i mean xml databases are are prone to updates and both um um and here in in xml unlike in say relational database um its not just a values of attributes that can change the the actual structure of the um database can um can also change very frequently in a relational database we basically assume that once we fix the schema um it is in tact and um its only the datas its only the data sets that are updated that is new um um attributes are added or existing attributes might be deleted or modified and so on but the evolution of the schema itself um that is change in a structure of the database itself is um um considered to be far less frequent if at all um um it happens and so um the the database is modified database is oriented the relational database is oriented towards fast updates and retrieval of data and not the structure as such but in an xml database that may not be the case that is there could be updates of not just the values but also of the structure um um of of in which data is stored in the database and in many case there may be no explicit schema that s available so so well form ness um itself is the schema so so we don t we don t really know which is a valid structural update and which is an invalid structural update and um what kind of constraints exists between different elements and so on unless there is a xml schema or a um dtd that s available for us similarly there are different kinds of query supports that that need to be supported by um an xml database this this standard flower queries that that we saw earlier there for let where return kind of queries and also select project and join um um based queries were um um which is also supported in xquery in addition to recursion or document construction that s transformation from um one xml document to  noise  the the input xml document to the output xml document and um um a far bigger problem is of indexing how do we um um how do we maintain meta data or how do we maintain indexes into an xml store such that we can retrieve elements as quickly and efficiently as possible when you were required and its not just um um the normal attribute and value index that that we need to um um store but we also need to store full text indexing what are called as inverted indexes so so um to be able to search a full text um um elements or data sets quite efficiently in addition to these kinds of indexes we also need what are called as structural indexes i mean um um structural indexes essentially um talk about what are the relationships between structural relationships between um different elements is one a child of the other is one um reachable from the other or um what are the siblings of a given element and so on and in addition there are um general requirements like scalability and um recovery in case of failures concurrency control um updates and and ever during updates and so on so let us look at xml storage um um structures itself and what are the different kinds of um choices we have for for xml storage  refer slide time  17  00  um  noise  basically we can divide the different kinds of xml storage that storage structures that are um um st storage paradigms are um um storage mechanisms that are available into three different classes what might be termed as flat stream um based storage or native xml storage and colonial storage so what do is each of this terms mean a flat stream based xml storage essentially stores xml data as um um some kind of objects in an object relational database um they are stored as clobs or um remember what is a blob a blob is a binary large object and a clob is a character large large object so um um you just store um um an object comprising mostly of characters so um character large object which contains its own mechanisms of access and retrieval as one of um one of the attributes in a relational database so um just like storing any multi media object or um um some such um object you can store um an xml um um dataset in um in a relational object relational database itself but um um of course the advantage of this is that um you don t need to um um reinvent anything in the sense that there are several object relational databases that are um um that are already available and its just a question of using um one of the object relational databases to to store xml documents but however of course the um the um um flip side of this argument is that um the the database itself does not support any xml centric queries so you can query the database based on object relational constructs but not an xml constructs itself and um all those xml specific um query constructs like flower queries or um xpath queries and so on um um can not be directly supportive the next kind of um um database storage strategy for um for xml is the native xml storage um native storage essentially means that you design a new database from scratch for storing xml data optimized for storing xml data so um here you have to um um worry about every thing that um um that you thought about lets say for um um for designing a relational database one is to one is to think about um um the storage structure block accesses that is the physical storage structure indexing structures um um then some kind of query mechanisms and um recovery mechanisms concurrency control um transaction um um i mean through ports and i mean um um how how to maintain efficient transactions and and so on so everything that that um um that that goes into designing any conventional dbms should go into um designing this native xml storage as well so um and and there are quite a few um quite a few examples of databases supporting native xml storage the third strategy is what might be termed as a colonial strategy a colonial strategy essentially means that um  noise  um um colonize and um existing paradigm um um using xml that is use some kind of um use an existing paradigm like say relational paradigm and then map every xml construct to a relational construct rather than um um note the difference between a colonial strategy and and the first strategy which you which just put xml um storage as character large objects or one of an attributes of um um of a relational um table however a colonial storage essentially um um decomposes or deconstructs an xml document into relational constructs and reconstructs them back so whatever xml related query that that you provide like xpath expressions or um um flower queries and so on um they have to be re return in sql and um you have to have a mapping one each to have a mapping between um um xquery constructs and sql constructs and vice versa and of course um um this can be mo this can be both an advantage and a limitation the advantage primarily being that you don t have to worry about number of techniques that that the native xml storage has to worry about storage structures block um storage structures concurrency control um and um recovery and so on however um um there is a significant if not huge performance overhead in by in in terms of mapping an xquery construct into an sql construct especially when when um when a when there is a recursive kind of a query that s that s given mapping it into an sql construct can can take quite avail and running the sql um um query on the database can also be quite inefficient  noise  so let us look into the second um um strategy in a little bit more detail namely the strategy of native xml storage that is redefining or redesigning um an xml database by looking at all the different aspects that that needs to go into an xml um um store so what are the issues what are the typical kinds of issues that that one needs to worry about when um um  noise  when designing a native xml storage one needs to think about data layout how is data organized on the disk and physical data layout is essentially that is if my disk is organized in terms of um pages disk pages or disk blocks what should each block contain and and how should the blocks be organized and so on indexing is again a major um um requirement like we said before its not just attribute and value indexes that are important um we need to also look into full text indexing or um um indexing of phrases within within text or um or structural indexes that talks about how elements are related to one another and so on  refer slide time  22  25  and um it needs to also address query support what kinds of queries are supported in this um um in this store and um um um how do we optimize queries and how do we um um pre process do we need to pre process or and if so or how do we pre process um for managing  noise  efficient um query retrieval and so on then access control concurrency control updates and and so on transactions recovery and and so many other um um issues now um let us look it um the  noise  lets look at the question of um data storage in a little bit more detail and um what are the different approaches that are used for um um managing or storing physical um disk blocks are um are managing the physical data storage that is how is an xml tree mapped on to physical disk blocks um essentially one can think of essentially an xml um um data set is a tree so this slide shows a um particular tree like this were there is an imdb at the root node and there are different show elements in the in the second node and each show element has different um um sub trees like title year box office and and so on and so forth so um and finally the the leaf node contains the um um dataset that is available in this xml tree  refer slide time  24  00  now how do we divide this tree in into into physical data blocks and um um and which kind of division makes sense one way to think of um um one way to look at dividing a tree into disk blocks is to cluster um um trees based on sub trees that is um this show sub tree would go into one disk block and this show sub tree would go into another disk block and um and the imdb would just be in the in an index with which which in turn um just stores pointers to each of this disk blocks now what is the advantage of such such a storage um the simple advantages that the entire sub tree um is in one disk block hence if i am um if i am um looking at navigational queries were the user just navigates from um um like opens a um um  noise  opens an element and um um looks at the sub tree and opens another element underneath and looks at that sub tree and so on like in a um um explorer kind of um file system navigation um for such kinds of navigational queries um this um storage structure is very efficient because um um when when um when the user is navigating the navigation path um is quite um um is quite close to the actual way in which um um in in which elements are related in the tree itself it is quite unlikely that the user would would open this sub tree and and start navigating here so so um so in one disk access one can um um  noise  one can read an entire sub tree however there is a flip side to this um to this kind of um um um to to this kind of data access itself suppose um the user gives a xquery kind of um a user gives a query saying um  noise  show me all elements um um show show me all show elements were the title contains um whatever the fugitive or um or some thing like that ok now um um if um if the query has to be searched on a particular element like say title um and even though we know which element has to um has to be searched for we still have to access every data block um in in the disk because um every data block which contains the show element which stores the show element would contain a title element so um we still have to access every data block containing the show element in order to access such a query so um  noise  so so to answer attribute kind kind of queries such a kind of organization is um a file organization um um is actually counter productive or um is not very efficient however for answering navigational kind of queries such a kind of um um access is um is quite useful the second kind of database storage structure is to cluster similar elements within a database um in in a within a data page that is within a disk block so this slide shows  refer slide time  27  23  such an example were um the same tree is taken imdb and show and title year seasons and so on um um however rather than storing an entire sub tree within a data block here each um um element um um is each element at a particular level are clustered together so so all title elements are clustered together under stored in one in one data blocks so so in the red data block let us say and all year elements are clustered together and stored in one um um in one single data block that is a blue data block and so on and so on and then there are pointers that that that point to each of this data blocks and so any of these elements that that don t contain um cdata or pcdata you can cluster all of these into one data block and then maintain pointers from um from them to each of these data blocks now um again this is in some sense the dual of the earlier um mechanism were um um  noise  were um such a kind of organization is um is very useful or um um very efficient for answering attribute queries so so if i want to say show me all show elements or return all show elements were um um were the title contains the the term the fugitive then um all that you need to do is um to to first access this block um which con which contains the show element and um find a pointer for the data block containing all the title elements and with just one data block access for um um one will um um and of course one or more i mean um um depending on how many title elements are there but um um generally um with um with far lesser data blocks than um than that are necessary in the in the in the previous case we can um access all title elements that are there in this um xml store so answering a attribute query is is far simpler however answering a um um  noise  answering a um um navigational query becomes difficult in this case and and there are other techniques for um for this thing it depends on how um um how some thing is clustered um if um if clustering is um is performed based on what may be termed as the lowest level elements were the elements contain just cdata or pcdata um um it is it becomes difficult to navigate that is um um from show you need to open a title and year and from title to year it it requires a different um um different block access and so on however it might be possible to cluster based on parts rather than based on single elements so um um rather than clustering similar elements some other techniques cluster similar paths therefore one one would say that um um cluster all paths of the form imdb show and title in one block and all parts of the form imdb show and year um um in another block and and so on so um um there are different variants however um um if one were to ask the question which is the best um way of storing um um which is the best storage structure for for xml databases the answer would be depends um um essentially there is no single um um there is there is no single um technique that can um um that is universally most efficient way of accessing or um or storing xml databases and to a large extent it depends on what kind of queries that you expect from the users so if it is navigational queries it might be um better of to cluster um um the the tree based on sub trees so so store sub trees within data blocks on the other hand if it is um if it is more of attribute searches or or even say full text searches it might um make sense to um cluster similar elements in a page rather than um sub trees let us look into the problem of indexing xml documents and what kind of indexing um indexing requirements arise for xml document firstly we sa we said that in addition to attribute value indexing for which we can use traditional um um rdbms indexing like say b plus tree or um or a or a b tree  refer slide time  31  54  here we need to um we need two other kinds of indexes one is um um what is what may be termed as full text indexing were um um it we should be able to efficiently search on um free text um free text data that that um that are written with within an xml document so so one might just write a paragraph um within an xml document and be able to search for um some key word in that paragraph and um very common mechanism of um indexing full text is what is um called as an inverted index an inverted index is very similar to what you would find at the end of a book like like say text book were were you have an index and there are certain keywords  noise  and um if page numbers or section numbers in which the keywords are um um in which those those keywords are either defined or used or um whatever so um an inverted index on an xml document would index different keywords that that appear in the document and then maintain links um in  noise  to the xml document saying were each of these keywords can be found and keyword based indexing um can be of of two kinds um it may either be an xml aware um keyword indexing or an xml unaware keyword indexing so um what is the difference between the two xml unaware keyword indexing just looks for keyword searches so um so you just give a keyword search called the fugitive or um jerry sign field or whatever that um um  noise  that appears in the um um xml document and keywords are searched this table here shows  noise  xml unaware keyword keyword searches that is um for every term that appears it just stores the document reference which document contains this or probably the element  noise  reference or whatever so it doesn t bother about where um um  noise  keyword appears and it it only bothers about um um the keyword and and the value in the keyword on the other hand an xml aware keyword um um or an xml aware index not only contains  noise  the the keywords but also um um but also another element um or another column which says in which element does the keyword appear from so so there is one more index that um um this ampersand t one here is an element index or is a key into an element index so um where where um um where it identifies each element uniquely in the that is present in the xml store so um um what this says is that the the term fugitive um um can be accessed or or the term fugitive appearing in the element whose key is t one can be accessed using this um pointer so um from from wherever so um if you are looking at attribute based searches and we want to say return all um show elements were the title sub element of show contains the term the fugitive then um an xml aware indexing makes much more sense than xml unaware indexing and of course the um the flip side of xml aware indexing is that as the number of elements um um increase and keywords are repeated across different elements there is um there is a huge amount of combinational um um choices that that appear between um a given term and an element pair so so the term nineteen ninety three for example may appear in different kinds of elements it might appear in um um births date it might appear in release date it might appear in um um um show date or whatever and so on so um so different kinds of elements so each of these have to be indexed separately and and a which which leads in um um um leads in an increase in the size of the index structure so this table shows um um different um xml native xml databases that that are available and quite a few of them already available like xyleme or natix or goxml and so on and most of them have been built from scratch and some of them like exelon or tamino have been built over existing databases so so its not in a pure sense um um a native xml um storage but um but they do they they are called native mainly because they do address many of the questions that that are typically addressed in native xml storage and there is a wide um um um diversity or wide ray um um ya wide diversity in the kind of features that are that are supported  refer slide time  36  43  so um um so xyleme for example supports full text searchers and xpath searchers and xquery searches but um um it doesn t have any um um what kind of apis that it provides or or unknown and natix doesn t support any of these but but um just supports some some low level primitives and um and so on and there is there is partial support for xquery and so on  noise  let us now um move into the second part of the stock um where we look into managing semi structured data  refer slide time  37  10  so until now we have been um we have been um  noise  looking primarily into xml and um xml is is a very um comprehensive tool to manage semi structured data and and so so what is semi structure data and and what is the significance and why is it important to study about semi structured data  refer slide time  37  53  um so let us first define what is um semi structured data there are sever several different um um definitions and of course um um semi structured data um what we understand um um what we understand often is data that whose structuring is not rigid and who doesn t um data which doesn t conform to a very rigid structuring mechanism um there are other kinds of definitions as well like um data that is inherently self describing and self describing data so um so so with no rigid schema which ba which basically implies there is no rigid schema the the data itself defines its schema and um that is known as semi structured data and um um and again data which are generated half hand and without planning and so on there also called semi structured data now um in in todays world um semi structured data is is getting more and more prevalent and um data structured ness is um um um is is becoming much harder to impose and define an impose  refer slide time  38  29  for example what is the structure of the world wide web i mean the the world wide web is the huge um data store but without any structure in fact um but um one one can when can one can not even say that it is an unstructured data store because there is some semblance of structure that that is um that is present like um one can think of some meta data tags that that are available for each html text some um um um href hyper link references and and so on and directories and um um so on and so forth so but um on the whole it is not possible to define um specific structure and then impose the structure on the world wide web so um the um the world wide web is is the is the best example for huge semi structured data store and um um and and most semi structured data stores are characterized by rapid or dyna  noise  rapid changes or very frequent changes to the data set so um not only is the data not defined apriori or the or the or the structure of the data not defined apriori it is also changing dynamically or its also changing rapidly and um it is not able to its not possible to formalize um semi structured data using a nice formal model like like the relational model and the the best data structure that that s use to formalize a semi structured data are usually graph structures and there several different examples for semi structured data like web information systems and digital libraries or even data integration from heterogeneous data sources can be considered to be a semi structured data problem were um um there are several different databases um um that are already defined and there so many such databases that it becomes impossible or impractical to be able to impose um um a common structure over all of them and it is um easier to treat them as um um a large semi structured data store so the um um very common example of semi structured database is um um is of course the internet movie database which which we have been seeing continuous examples of when when we are talking about xml as well so the um um so imdb is a is a classic example of a collection of um semi structured data and um and um even though i mean what makes a semi structured the the fact that um even though it just stores information about movies each movie is different from the other um  refer slide time  40  54  each movie may belong to a different  journal  and it may belong to um um different country it may it some some may have um language um um fields some may have some star cast fields some may have some um some other kinds of um um fields which may not be available in the in the other and so on in in other um records and so on so lets consider an example um from from a movie database um um lets say um um imdb is the database and of course imdb not just stores movies it also stores information about um tv serials and um documentaries and and other such um movie related or movie like um um um data that that is um that have been released  refer slide time  41  49  so um even within a movie um let us say um um even mo within the movie category um different movies could be different um movie one may um um have information about the cast um and and the director in the movie and um um who who cou who could be the cast in the director but but movie three may have something called actors and actresses um it may make a distinction between who is the actor and who is the actresses and um um um the direction that is um um rather than just the director it can it can um talk about a direction team or um or were it who directed it and so on so the so the structure of each record that makes up a movie um um um um movie element in in the in imdb may be different from one another and um some data elements may annotate more information than others and um um and some may have missing fields and and the kind of relationships um that exist between each of these um different elements may also change um um may also vary between different records  refer slide time  43  02  and in addition to that um um in addition to changes in structure the the um um the the way in which data is organized itself could um could could change for example um um one might represent an actress name as first name comma last name and one might represent it as last name comma first name or one might um um some other record might have its something like just a name and so on  refer slide time  43  20  and data gets added to this database dynamically and as a as a result it and dynamically and from different sources um um um from different independent sources so um it becomes difficult to enforce a particular kind of schema restriction on this um database so what is the problem here or um what is the main problem in um managing semi structured data um  noise  the main problem is trying to ascertain what structure or what is the common structure to use for different data sets that that are being added to the database  refer slide time  44  15  and um and to be able to formulate um queries and formulate query languages and so on and in addition um um we should also note that the structure of data element is um is implicit um so its not that um the the um user providing the dataset first defines the structure and then um provides data according to it but structure is embedded within the data itself now the structure has to be first discovered and then a common structure has to be evolved over the entire data set  refer slide time  44  33  so um  noise  and um um um this is what is called as the problem of discovery of structure that is um the structure should be discovered um such that the structure is indicative in nature rather than constraining in nature  refer slide time  45  09  that is the um the the common structure that evolves out of a database should not constrain the database to to um um to to adhere to a specific structure but rather um should be indicative of what kind of um data um is is available in the database and and how they how they are um interrelated to one another  refer slide time  45  45  oops um the change in  noise  so um um here is an example the this slide shows an example of um um um what is the main problem in um or what is the main challenge in um um in semi structured data  refer slide time  45  58  in relational or um or what may be termed as traditional data management what really happens is you have a uod or or a universe of discourse like um like company or an academic institute or um um or university or whatever um and then you have a model of the uod that is there is the schema that that defines how the how data in the uod should be organized um its not ease organized but how the data should be organized and data that that is collected from the uod is first taken through this model and populated into the database so um um so when we say that an employee should have um um a pan number as as the primary key and name and dependents and salary and so on it is um um its only those sets of data that are extracted from the uod and then sent into the database and um especially for example if if an employee doesn t have a pan number and the pan number is the primary key then it is not possible to add that employee record into the database because the primary key has to be not null um it has a not null constraint and so on so constraints are enforce when the database is being populated right and um um the the query also is is formulated within the um model of the uod so so so the query just takes the model of the uod and queries the database accordingly however um  noise  in what might be termed as the post internet data management which is um the main problem with semi structure databases  refer slide time  47  35  the universe of discourse whatever is the universe the the world wide web or or the internet movie database were were user scan um independently add um movie data into the database the the universe directly um um populates the database the the um it doesn t go through any common mental model um by which um the the database is populated in fact um um there might may or may not be any mental model here as to how the data is organized but the data is um is directly populated by the um uod now the query um when when when the query is searching the data um it should not only know what data to search it should first try to find out what is the mental model or how is the data organized what is the schema for this um um by which the data can be searched  noise  so um  noise  that that is basically the schema discovery problem or or the implicit um schema discovery problem  noise   refer slide time  48  29  and um in addition to that um um the the schema discovery problem often encounters the problem of um of what is called as the large schematic um schematic structure that is even when we discover um a schema this is again um um called the maximalist world notion um um um in contrast to the minimalist world model of a of a traditional database system in a traditional database system um whatever is not allowed by um um um whatever is not explicitly permitted by the schema is forbidden so um um everything is forbidden unless explicitly allowed by the schema so its a it s a kind of exclusivist um um data model were um were things are thrown away unless they are permitted  refer slide time  49  05  however um um in a schema discovery process um um it um it um its an inclusive model were every thing should be permitted unless um the there is um um unless it is sure that it is forbidden that is um um unless it is sure that um one one some kind of a relationship can not exist um um all kinds of relationships between data elements are permitted so it is not one can not apriori um define what kinds of relationships exist between data elements unless of course we know that some kind of relationships do not exist in the um um database or can not exist in um in this uod right so um  noise  um um so so the the um the associated problem um from this is that the discourse schema quite be can be quite large um um rather than in contrast to a relational schema were the schema is much smaller than the data set itself so um um in the internet movie database for example we might um um discover that the lot of different um um we might discover lot of different things that that go into a movie based on what um um people add into the database and um we should allow for all such relationships unless of course we know explicitly or unless of course we know specifically that some kind of a relationship can not exist for example we might um um we might um um um we might know as a as a rule um i don t if this is a true but we might know as a rule the um in the that in the universe of discourse that that in the world of movies um it is not possible for um um um um for a director to be the boss of the producer are some thing like that ok so um um the producer reporting to the director or whatever so unless of course we know that some kind of relationship does not exist we have to um um we have to accept um all kinds of relationships that that are um um we might have to accept um um a dataset we which um about a movie which does not contain any movie star we might have to accept a data set were a movie contain ten different movie stars and so on so all such relationships should um um should be accepted unless explicitly forbidden and um and as a result the the actual schema that that is generated is far bigger than a typical relational schema there are several different application areas were um were this is useful and of course um um were were these have been tried out um um and these include data integration were um um um were were you design an interface to um to to integrate different desperate data sources coming from different ar different sou um um different locations each having their own schematic structures  refer slide time  52  26  and the second major um um area of  noise  application is in digital libraries where um which consists of again different kinds of semi structured data coming from different sources several more application areas like genome databases or um um scientific databases that talks about scientific documents similar documents citations references um and abstracts and um um and were it was published and ratings and so on and so forth and um um and of course in e-commerce applications were um um the the discovery of structure problem becomes um um very important um in business to business systems were each business um um if it is um um if it is quite big it might be difficult or impractical to um um to to impose a very specific schematic structure over the entire um um um business house  refer slide time  52  44  so um so you mig one one needs to be able to um um resort to semi structure data management when managing b two b business systems so um so so let us um um skip through these slides were um the the need for discovery of structures are are motivated even more were  noise  um or which talks about how discovery of structures can can go about  refer slide time  53  38  and um um addressing the discovery of structures problem um in itself would would take a complete um would would involve a separate session and um um it um it is clearly beyond the scope of this particular lecture however um um we can give some kind of thumb rules which talk about how implicit structure can be um discovered from a set of desperate data sources and most of these um  noise  most of these um um um revolved around looking at some kind of regularity in the in the data set and then generalizing based on this regularities and um so so several different kinds of um um data mining and um machine learning and um um artificial intelligence kind techniques or  noise  or or um um or explored for um um trying to fit a structure on to a set of um um on to a data set and the um um and there is also a notions of what is a best fit um um a best fit um um data structure or or a schematic structure should not be too general um um and should neither be too specific and um so it has to be the the um the structure discovery process should be able to generalize based on whatever examples were um um were encountered while while passing through the data however it shouldn t be too general in the sense that it can accept anything or any structure um um in in the data set that is it should also identify what are the forbidden relationships um um among data elements in addition to what are all the possible um relationships among the data elements  refer slide time  55  41   noise  and um several different kinds of query languages are also um um supported for semi structured data in add in addition to xquery which is primarily meant for xml um databases and of course um keyword based searches which which are um um which are useful for um um which are useful for full text searching there are other kinds of primitives like um um navigation based queries or searching for patterns um or temporal queries based on how a particular um um data element evolves over time and and so on  refer slide time  56  16  so um um xml um um is an embodiment of semi structure data in the sense that  refer slide time  56  27  xml is the natural choice in which semi structure data can be um um can be organized and um um the the problem of discovery of structure over xml um um over semi structured data can reduced to discovery of a xml schema given a um given a desperate set of xml documents  refer slide time  56  27  and queries can be revisited um using xpath and xquery expressions based on um um whatever structure that have been discovered  refer slide time  56  43  so um let us summarize what whatever we learnt in this session and of course the um um um the idea of semi structure data in itself is a vast ocean and it is um beyond the scope of this lecture to explore all of them  refer slide time  56  51  so therefore we um we looked at native xml storage and xml publishing and different kinds of storage um structures that have been proposed for xml and mainly we we um um we touched upon the problem of um um the semi structured data and and the larger problem of discovery of structure which is very um um important for semi structured data management so with that um um we shall end this session transcription by  vidhya proof read by  shobana database management system lecture # 41 case study  part one database design dr.s.srinath hello every one welcome to another session in database management systems um until now we have seen different aspects of dbms design we have seen what a typical life cycle of a database management system looks like um essentially we saw that a dbms is something like or could be treated analogous to the engine of of an information system and what is an information system um anything um um a part of a larger system that deals with information flow management storage retrieval handling and so on so every thing to do with information is usually um um is usually driven by um um by a database management system at the core right so what i assume that you should know by um um by today s class is that you should be familiar with what is what are the roles of um different um what are as the roles of a typical dbms system what are the different kinds of actors that that exist in a typical dbms and um conceptual modeling using the er model we saw little bit about um er or entity relationship based design for conceptual modeling and also um um the the relational model which is the physical model or um or rather its not exactly the physical model as as in terms of the um um disk storage that s that s used but but the um its its still call the physical schema because that is the way in which the the database schema appears to all the um um all the programs that um um that utilize this dbms system right so it um so so the the relational model and um different terms as as to um um what is meant by a table um relation or normalization functional dependencies and so on  refer slide time  02  15  and and also a little bit of um um set of rules as to how to convert a given conceptual model in a um in in er diagram to um um a given to its correspondent relational schema right so today what will do is um let us look at a typical case study ok database design case study right so how do we go about designing an application ok around a dbms system um note that we are not here talking about the design of a dbms itself but we are talking about design of an application on um on top of a dbms ok so um um when i was talking about uods in one of my earlier class this is what we are going to look at that is we are we are going to consider particular universe of discourse and then um take it up right so rather than taking toy examples ok and rather than taking um um an example comprising of just a little bit of um um database or data management requirements um i have taken up fairly comprehensive example at the same time um one should be aware of the fact that um real life databases for example um the the moment when we talk about databases we are um um we are first reminded about banks and railway reservations and so on i have not taken either of them because they are massive database systems indian railways for example huge um in terms of the amount of transactions that happen and amount of data that that is generated and stored everyday so um it would it would be a disservice in fact it would be plain wrong to to take up um such a massive database as a case study um and in fact we would be si simplifying it so much that um um that that you will not appreciate the the actual complexity that lies in in managing such a huge system so what i have done here is to take up an actual system that um you might actually want to um implement as part of a class project or something which um and which are severals of such um da da database management systems existence practice so so the case study that we are taking is shown here ok it is a conference management system ok  refer slide time  5  04  as you know several conferences today are um are managed by a web based um interface were um wer wer were you can mange all the activities and data that that are related to the conference ok so what is the conference management system contain so here is a brief description of the uod right that um the universe of discourse and the the different kinds of requirements that that make up this uod ok so let me um read it out from the requirements itself of course this is a simplified conference management system um it does not make sense to um take up um um a real life database in its complete gory details but um but la but at least the um what i hope is the that the gist of a particular um requirements of of a given uod should be captured by these requirements ok so let us look at the requirements once again the technical program of a large conference is um decided by a program committee ok so so there is a committee of people who decide which paper should be published or which paper should be presented and which paper should not be presented for in in a given conference and the program committee is headed by a pc chair or a program committee chair ok all other members of program committee will act as reviewers ok the so um so so people who would submit papers to the conference and they would be reviewed by different um um um different members of the program committee ok now thats about the program committee so so let us look at the next set of requirements what about a paper ok a paper is authored by one or more authors of course and it should have a unique contact author ok so so there should be one author in the paper who should take responsibility of of the paper so um so it is to this author um to its him or her that the the um that all further correspondence will be addressed to right so correspondence regarding whether the paper is accepted finally or is it rejected or it should be um accepted after another process of review and what kinds of changes to be made in the paper and so on and so forth right so um lo look at the other set of requirements  refer slide time  07  10  ok so um any person who is a member of the program committee can not be an author of any paper that is published in the conference of course um in real life conferences it s a bit more relaxed than this that is you can actually submit papers um to a conference even though you are p  noise  program committee member but for our purposes let us um um let us keep it kind of um um stringent um stringent meaning its its just going to make things simpler ok so as long as you are on the program committee you you can not um publish any papers in this conference ok what is the reason for that because um um a program committee member  noise  should not push his or her own papers into the conference right so so they should act only as reviewers right um what about authors an author may submit one or more paper there is no um um there is there is no  noise  limitation on that um  refer slide time  08  27  and um but each paper has a unique identity right so um so we are selecting papers and not authors ok so so that s an important thing here and um um and a paper can not be submitted to more than one conference or a journey ok so if i submit a paper some where i can not submit the same paper to somewhere else right and i can not obviously also also summit a published paper somewhere else right now um the the last set of requirements is the the last block of requirements is that a paper is reviewed by at least three reviewers ok so when i send a paper to a conference it goes to at least three other reviewers and a reviewer will give suggestion as to whether to accept or reject the paper ok so so that is shown here  refer slide time  08  44  a reviewer may either accept or reject a paper or be neutral towards a paper if the reviewer can not take decision um the the reviewer just says that ok i am neutral to this paper ok so um the actual decision should be taken by the other two reviewers ok and in very rare cases all three reviewers would be neutral and um th well the the program committee chair or the pc chair should have to ta should has um should take a call on such papers right so based on reviewer comments pc chair prepares a set of paper for acceptance and then um um tho those set of papers are accepted into into the conference ok now um let me pause for a little while here and um um go through the requirements once again ok so carefully look at the requirements of um of your end user ok there there is a program committee there is a pc chair ultimately what is that we have to do we have to take care of the conference activities ok now if you look at any set of requirements carefully you ll see that there are two um things that that a requirement says right um um  refer slide time  09  47  a set of requirements will um will indicate an explicit set of required behavior ok that is these things have to be there ok the the paper says that um every paper has to be reviewed by three um reviewers right and every paper has to have a contact author and so on ok there are some things which are explicitly required by our specifications right similarly there are some things which are explicitly forbidden by um this specifications if i have sent a paper to um a conference i can not or i may not send the same paper to some other conference right so so this is a specific forbidden condition you should not do you shall not do this and so on ok but but if you see again carefully there are number of requirements or number of things here um which one might um one might talk about which or neither required nor forbidden by the requirements right um um can you can you think of some requirements for the conference management system itself that um um that is neither required nor forbidden ok let us take something like how many paper should a reviewer review ok um can i say that a reviewer can review five papers ten papers or exactly one paper and so on ok there is nothing that is said in the requirements here if you if you look at this carefully the the requirements says neither yes nor no ok so um th there is nothing said about th about this requirement itself ok so that is an important thing to note in most application design um when we when we capture requirements um the requirements tells us something that that needs to be there and tells us something that should not be there ok but is silent on on a large number of um um things as well ok so that greatly affects how we design our application and um whether our application um suppose you design a dbms system and you say that because of some constraint some where you say that um a reviewer can not review more than two papers is it correct or is it wrong ok so the so there is no specific um answer to this because th th the requirements neither requires this nor forbids such a thing right so um so usually this is how a systems development life cycle some of the top or or the early stages of a system development life cycle would look like right so if you look at the slide here um  refer slide time  12  35  you have the systems requirements specifications were there are set of explicitly required conditions and there are set of explicitly forbidden conditions and this is the entire uod were um the number of things which are not addressed by the the requirements ok now based on these you get a high level design of your um um of your system usually this in the um in the form of a er model or whatever when it comes to dbms design right so you you end up with a er diagram here in turn you reduce the er diagram to a relational schema ok or a or a low level design and then you get a system model ok relational schema plus transactions and so on um small set of application logic and some set of constraints triggers um um stored procedures or so on and then you get a system model ok usually what happens is this process how do you get design from requirements ok or how do you move um from high level design to low level design ok these sets of processes involve human activity or human creativity to um creativity to be more specific right so um and as as is so common with human um um actions the system model may not exactly reflect the systems requirements specs ok um ideally what should the system model do the system model should should exactly reflect the requirement specification here right so um as as shown here the red spot is slightly bigger here ok what is that mean that that the system model um has more forbidden conditions than what is explicitly forbidden by the requirements specification itself right so its brings us to um some um  noise  two important concepts when when we are designing real life system the the concepts are what are called as liveness and safety right so um look at the um um look at the english definitions of liveness and safety ok  refer slide time  14  14  liveness means what that that something is alive or something um um is is existent and so on and safety of course is is obvious ok now um if you look at the systems requirements specifications why would a set of requirements so so let us go back here  refer slide time  14  31  why would a set of requirements say that this is forbidden ok why would a set of requirement say that um a member of the pc committee shall not be an author of a paper why why would a set of requirements say because it would compromise  noise  the integrity of the system if that were to be alone ok um because if i allowed a pc committee member to be an author of a paper there is quite a likelihood um or there is quite a chance that the pc committee member may push his or her own paper um and um and have an unfair advantage over others right so it is the safety of the system is getting compromised ok so that is why i forbid the behavior um fo forbid this activities ok so essentially whatever is forbidden usually constitutes of safety requirement ok in in order to safe guard the system against integrity violations i say that this shall not be there ok however what is the simplest form to build a safe system how do we build a system that is absolutely safe and from from any kind of integrity violations simple don t start the system at all if a system that doesn t work it is absolutely safe if your database system doesn t work at all it is absolutely safe because it does not violate any integrity constraints at all right so that is why um um a trivial way of ensuring safety is to is to make a system that doesn t work but that is not what what we want right in addition to safety we need um we require certain behavior to to to happen ok so those are what are called as liveness requirement that is um the system should perform certain activities and should not perform certain activities right so um um so so so let us use some notations when um just just to talk about mis matches now suppose i say that r of srs here  refer slide time  16  17  ok is the set of required behavior by the srs or the systems requirements spec ok similarly f of srs is the set of forbidden behavior or or safety constraints specified by the srs ok similarly let us say r of m ok or um were m is the model that that we build the final system model that we build right so let r of m denote the set of all liveness criteria in the system model ok that is the system model will do this ok and f of m denote the ses set of safety criteria in the system model that that is the system model will not do this and so on ok now um when we talk about a system model that is when we talk about building a system model from a set of requirement spec um we can think of various kinds of mismatches that can occur ok so various things can go wrong when when we are talking about um um capturing user requirements into into a system model what are the things that that can go wrong um a tentative list of things i mean the these are not the only thing that can go wrong in fact in addition to this a huge number of things can go wrong but anyway ok  refer slide time  17  20  now let us say what if r of m ok remember what is r of m r of m is the set of required behavior um um of the model ok what if r of m is a proper subset of r of srs ok what is this mean the set of required behavior by the model is a proper set of the set of required behavior by the srs ok that means that the model is incomplete the the srs require certain behavior to be done but you don t implement all the behavior ok you don t factor all those behaviors you you factor a subset of those behaviors ok now  noise  what if in addition to this ok r of m being a proper subset of r of srs in addition to this let us say the the um um r of srs minus r of m ok um that is the the set of requirements specified by the the systems requirements spec ok which are not factored into the model is actually a part of f of m ok is actually a part of the set of forbidden behaviors by the model ok what is what is that mean it means that not only dos does the model address all the requirements in the um in the in the requirement spec in fact there are certain requirements of the requirement spec that the model actually forbids that is that that the model will not do ok so it means that the model is not only incomplete it is incorrect it forbids certain required behavior right similarly um what if r of srs is a um is a proper subset of r of m ok that means that the model is is performing more activities than than what is required by the srs itself right that is the model has extraneous behavior ok and having extraneous behavior is not that is having an added feature for example ok um um suppose the the model as for the birth date of the author ok when were you born and so on ok um does not always um be a desirable feature it can actually be potentially unsafe ok when is it potentially unsafe when um when r of srs is is a proper subset of r of m that is the um um what i saw here and the the difference between r of m and r of srs is actually a part of f of srs that is um the um the extra behavior that um um the or the extra so called value addition that your model is doing is actually part of the set of safety conditions that is is actually forbidden by the by your requirements ok so um so the model has extraneous and unsafe behaviors right so when you build a system model um you should be um um careful to or this is one set of guidelines by which you can measure whether your system model is um is good enough against the requirements that is um just just try um separating the requirements into set of required behavior and set of forbidden behavior and your model also into set of required behavior and a set of forbidden behavior right so let us see some more mismatches here ok now um what if f of m is a proper subset of f of srs ok that is the set of all forbidden things of the model ok is a proper subset of the set of all forbidden things of the srs that means that the the model is unsafe that is the the requirements require you to forbid certain things which which you are not forbidding right similarly um what if it is the other way around that is the the model forbids more than what is required by by the srs ok then you say that the model is conservative ok now um um it conservative again doesn t mean that you are safe ok usually in english we say that oh let us be conservative and go about like this um um and take this action and so on but just be saying let us be conservative doesn t necessarily mean that you are safe ok why because you could actually be violating a liveness criteria right so um so this is the case here that is um f of srs is a proper subset of f of m that is the model forbids more than what is required to be forbidden by the srs and the difference  refer slide time  20  58  that is what the model forbids ok um which is not forbidden by the requirements is actually part of the required behavior of the srs ok so if forbids something which actually needs to be there ok so so in being more conservative you are you are actually hampering the liveness of the system right so um so so just being conservative doesn t always mean you are building a safe um model right ok so so let us see um let us take a step by step approach to see to let us try to divide our requirements coming back to the um conference example to see what kinds of required behavior are there by by the model and so by the requirements spec and so on ok so um um of course the the kind of um the the kind of example that we are seeing here is a simplified example and and real life examples are far more um um difficult than these but anyway this this gives the gist of um um how to factor a requirements into set of required behavior or liveness behavior or set of safety conditions and so on ok so um so what could be the step by step approach let us um the the first thing is we have to find the set of required and forbidden conditions ok then um then once you start that then start identifying the various entities there attributes the relationships between entities and so on ok then um build a complete er model for the um for the problems statement and then convert the er model into a um a relational model and perform normalizations um if if they are not already normalized right  refer slide time  22  56  so let us look at some of the required conditions ok a paper is reviewed by at least three reviewers ok that actually means that a paper should be reviewed by at least three reviewers right so um so if i um um um if i try to review or if i try to accept a paper that is being reviewed by only two reviewers then um your conference um um management system should flag an error it should not allow it to do that right so this is a required condition that is um a paper is reviewed by at least three reviewers ok each program committee has a pr has a pc chair ok so so this is another required condition you can not have a program committee without a chair ok  refer slide time  23  46  each paper has a contact author ok if you go back through the requirements that that we saw all these have been picked from the set of requirement itself right so um each paper has a contact author ok that means that each paper should have a contact author and so on ok a paper is authored by one or more authors ok so um obviously this means that you should not accept a paper without any authors in it ok and reviewer must comment ok or must give comments which can be one of the following um accepted rejected or neutral ok you can not um um the reviewer can not give any other comment other than this three ok so and the reviewer should give one um one of these three ok the reviewer can not remain silent ok saying that i am neither accepted nor rejected nor i am neutral about the paper and so on ok um um and the reviewer should um um should give only one of this right so so this is a set of required condition what are some of the forbidden conditions um we saw some examples already right a paper can not be submitted to more than one conference or a journal right so you may not submit a paper to more than one conference and so on ok the an author of a paper may not um um be a member of a program of the program committee right so so that s another forbidden explicitly forbidden conditions ok so um that that have been explicitly stated in the requirements that these are forbidden ok and a paper um may not have more than one contact author ok so so there has to be one and only one contact author so so that it may not have more than one ok so these are some some kinds of required conditions and forbidden conditions and and so on ok so um so when you build a system model what you should be able to do is take up each set of required conditions and see whether your model also um um has that required behavior take each set of forbidden conditions and see that whether your model also forbids those conditions ok and the other way around ok take each set of required behaviors by your model and see that whether they are actually required by the set of requirements ok so on ok  refer slide time  24  53  so so let us now go to the next step and start identifying entities ok so how do we identify entities and what is an entity an en entity is some logical um um logical item one could say or logical um um something of um um which which has an independent existence of its own ok so i am i was about to say logical entity which which ki which kinds of bec becomes a circular definition in this case ok so any way so let us look at the problem statement once again  refer slide time  25  57  the technical program of a large conference is decided by a program committee headed by a pc chair if you just look at that um statement you can already find several entities here right so the conference is an entity ok of a large conference ok conference is something that has a logical existence program committee is a is an entity here ok um um essentially the nouns in this in this sentence right and is headed by a pc chair is an another entity right so um and one could even say the technical program could also be an entity and so on ok so um  noise  so so ju just reading through each sentences you can identify what could be potential entities in your system right similarly the next statement all other members are some statement down here all other members of program of the program committee will act as reviewers ok so reviewers is another entity um as soon as we found ok so um similarly um a paper is authored by one or more authors ok so paper is an entity ok now author is is a um now um here there is a question ok this this not as simple as that ok  refer slide time  27  25  so is author um is it an entity or is it an attribute ok is is an author and attribute of a paper that is a paper has an entity and this paper is authored by so and so authors and so on ok now um some cases we can make author as an attribute of a paper but here ok but here we see that author also has an independent existence why because we have something called a contact author ok we have something called author may not be a member of the program committee and so on ok so um um so so the author may actually participate in other relations as well ok and an author may write more than one paper ok if i make author as an attribute of um of the paper entity there is no way to relate or there is no relate to equate that um paper one and paper two have been published by the same author and so on ok so so there is no way to equate those two papers ok so um um so in our case um um it is its better to take author or its better to design author as an entity itself right similarly again some more a paper can not be submitted to more than one conference or a journal again there there is an entity called journal and so on ok conference we already saw um is is is an entity right  refer slide time  28  31  now  noise  what about relationships now now that we have identified entities ok of course we are we are no way near to um finishing the identification of entities but um let us look at relationships i mean you should have got the ideas has to how to go about identifying ent entities and its attributes and so on ok now um um again take a look at the statement ok now um what are the entities um the the that you can see in a given statement the entities would generally be the nouns of a particular statement right now what could be the relationship here ok now look at the verb ok something is headed by something else and so on ok so or handled by and so on ok  refer slide time  29  05  so um so so so the verb statements that that connect two or more nouns would actually be prime candidates for relationships ok so if you look at this um statement here the technical program of a large conference is decided by a program committee headed by a pc chair ok so as you can see here this part already forms a relationship right that is um or or rather the first part um is is a relationship here that is um conference is handled by program committee right that is handled by or technical program decided by if if i have to make it very explicit ok so conference is handled by a program committee ok and as you can see um um the technical program of a large conference is decided by a program committee so so basically it is a one to one relationship right that is one conference one program committee ok but then look at this here um program committee we have made program committee into into a week entity here ok why is it a week entity first of all what is the week entity um if you notice carefully um a week entity is an entity or if you remember your er um modeling classes a week entity is an entity which does not have an independent existence of its own ok its existence is defined by a relationship ok so um and um and the the relationship that defines a week entity is also called a defining relationship right so so this is a defining relationship ok so um um shown by double arrows like this  refer slide time  30  53  and this is what is called as the total participation if you remember er er classes again so um a program committee totally participates in this conference right that is the same program committee may not participate in more than one conference entities and this is the defining relationship ok so um um if there is no program committee then there is no conference right similarly let us look at another statement the technical program committee of large conference is decided by a program committee headed by the pc chair same statement ok so um um so again program committee here and pc chair heads program committee right that is um a pc chair um um is um um is is an entity which we have found and um a pc chair heads a program committee ok now here if you can see the um again um um we see that this cardinality ok that is um a program committee is headed by a pc chair is clear ok that is one program committee should have exactly one pc chair ok  refer slide time  31  53  but one pc chair can head how many committees it is neither specified um or rather it is neither required nor forbidden ok its not specified in the requirements ok so so here we have made into a n cardinality one to n or whatever right that is a program a pc chair can head any number of program committees and so on ok so because um there there is no explicit specification as such in terms of how many um program committees can can a pc chair head right  refer slide time  32  26  again some more relationships ok so so take a look at this statement the technical program of a large committee whatever headed by a pc chair and so on all other members of um of the program committee will act as reviewers ok so so let us say um we already had had this this one right that is um program committee is here and that is headed by a pc chair and one pc chair can head um one to n program committee um program committees and so on ok now a program committee um consists of reviewers ok which is apparent by the second sentence right that is um um all other members of program committee will act as reviewers however however it is not exactly um um its not exactly correct right its not exactly what what is specified by the requirement right that is um um what is the requirement say all other members of prog of the program committee right so um um so so basically what what does this um this means that is um all members who are not pc chair that is who who are not a um um who are not acting as pc chairs can be reviewers of this ok so if i take two separate relationships like this in isolation they don t form a consistent um set here because it is violating a forbidden condition it is violating a condition that the pc chair may not be a reviewer ok so how would you um rewrite this um this condition here um so basically we will introduce a new entity called um  noise  called members ok and um um and and basically form um what what might be termed as  noise  a generalization specialization relationship right so remember the extended er model allows for a specialization relationship were um a given a member or given a entity you can um inherit one or more entities from it right that is it actually shows the is a relationship ok and in addition to the is a relationship here we we um um we have this circle called d what is the d specified d basically specifies that these are disjoint entities that is um no entity instance that is part of reviewer can also be um um pc chair and vice versa right  refer slide time  34  07  so um so our  noise  committee would now look like this committee would consist of members ok were members would in turn consist of um reviewer and pc chair which are disjoint ok so committee can consist of one to n member n number of members but there has to be exactly one pc chair ok so so basically in addition to this we have to give a cardinality of one here and um n here for for um um for n reviewers and one pc chairs right  noise  again let us look at some more um some more statement when um to to identify relationships a paper is authored by one or more authors ok and has a unique contact author ok so so again we can see that um um we can identify relationships straightaway here paper is authored by authors ok one or more authors and an author can um um can author how many papers there is no specification so so we just introduced one is to n ok so so we are kind of being liberal model ok we are we are not being being very very um um conservative model that is we are we are allowing for more um behaviors than than has been required that is an author can submit any number of papers unless it is explicitly forbidden of course ok  refer slide time  35  31  and a paper should have a contact author ok that is um a paper here um any number of papers should have exactly one contact author ok so um one author who ac who acts as the contact author again in isolation these two relationships are not sufficient because why do you think they are not sufficient le let me pause for a little while here why do you think going back to these two set of um entities why do you think they are not sufficient in in themselves i am sure you would have got the answer the the thing is while a paper can be authored by one or more authors and a paper can have contact authors ok there is no um relationship that s states that the contact author should be one of the authors here right so um one of the authors from here should be taken and be formed as the contact author fo for a given paper that is um so so you have to in the earlier case there was a disjoint relationship pc chair can not be or may not be um um a reviewer ok and here there is a membership requirement that is a contact author are to be one of the authors of the of the paper right so um how would you go about um um um let us come back to this again later so so again um any person who is a member of the program committee can not be the author of any paper ok  refer slide time  37  29  so so we will come back to that um earlier thing after taking this other constraint also into perspective and and then draw the entire um set of relationships at one go ok so um um what is this say here any person ok note that now again person is a noun here ok so so we need another new entity called person ok so any person who is a member of the program committee can not be the author of any paper ok one way to show this is um have a person called um um have an entity called person and make a disjoint um um specialization between member remember we we had a we had an entity called member here ok members or whatever ok so we had an entity called member and um an entity called author ok so so an author may not be a member or a pc committee member and a member may not be an author and both are persons and so on ok so um so so that way you can identify that um um any person can not be both an author and a member of the pc committee or the program committee right and um um a paper may not be submitted ok um even though sometimes when when talking in english we we say a paper can not be submitted to more than one conference or journal ok to to be more precise it actually should be a paper may not be submitted to more than one conference or a journal ok so again um um here um um wh what first of all what can we imply from that a paper can be submitted to a journal and a paper can be submitted to a conference ok so so these two can can be can be implied ok  refer slide time  38  35  but but what is that we actually need ok what we actually need is that while a paper can be submitted to a journal as well as submitted to a conference it may not be submitted to more than one conference or a journal ok the same paper may not be submitted to more than one conference or a journal ok so um so here in order to identify that we have used um um  noise  um we we have used the the union or or the concept of a union were um um were it is one one might call it the opposite of the specialization condition  refer slide time  39  21  were you take two or more entities and form a union out of them and and form a single entity right so so take a journal or a conference ok and form a union out of them and make an entity called event ok and the paper is submitted to an event ok and um um how do you say that um it has to be submitted to only one conference or a journal at any point in time um only thing is make this the cardinality of event as one here right so given paper can be submitted to um um one paper here um um or one to n papers that is a given event may have n papers and a given paper may be submitted to exactly one event ok and what is that event an event could be a conference or a journal ok calling a journal as an event is is not exactly um um is not um is not exactly um correct sounding in terms of the the english definition but anyway we have we have use this term but um you might think of a better term than event to to specify or to take the union of um journal and a conference right similarly a paper is reviewed by at least three reviewers ok and a reviewer may either accept or reject or be neutral towards the paper ok  refer slide time  40  50  so um um so so what is this um what is this statement say a paper is reviewed by reviewers ok so n number of papers is reviewed by three to n ok that is at least three or any anything more than three ok now now take a look at the second half of the statement a reviewer may either accept reject or be neutral towards a paper ok that means the reviewer is going to give a result ok the result is either accept reject or neutral or be neutral ok now um um if you see carefully the the attribute called result does not belong either to the reviewer nor to the paper ok because a paper um the result of a paper is actually a combination of the results of three or more reviewers right and um a reviewer may be reviewing more than one papers so so you ca n't assign result to a reviewer as well ok so the the attribute called result is actually an attribute of the relationship itself ok so so remember that we had talked about um um attributes which belong to relationships ok so as long as there is an instance of this relationship existing in the um in the system um um an instance of this attribute may also exist in the system um whenever um the relationship instance does not exist ok um um when when can a relationship instance not exist for example when um when there is a paper which is not assigned to any reviewers for example ok then there is no instance of the rela relationship at all that is reviewed by and so on ok so um um so so there is no question of a result existing in this right or even when a paper is assigned to just two reviewers ok so um um if you look at the relationship here the relationship requires that a paper be um um submitted to at least three reviewers so so therefore there is no instance of such a relationship existing and therefore there is no instance of the result in in the um databases ok similarly um um coming back to attributes now now now um let us say conference ok now the requirements doesn t say anything as such ok but um as application designers it is our responsibility to to identify some um um some re some some of the major attributes of of a particular entity and also identify key attributes ok so here in this case a conference um um conference name and and the date and the place topic and all of those things would be attributes of the conference and usually something like the conference name would be the primary key or wou wou would be the key key attribute of the conference ok  refer slide time  43  21  similarly um program committee does not have any primary key because it s a week entity type which um um which we actually saw earlier right  refer slide time  43  37  so a program committee does not have a key attribute but it may have other attributes like what is the strength of the program committee how many people are there um in the program committee um as of now and so on right so a program committee has a um um um is a is a week entity type having no key attribute but it ha it has its own um other attributes ok and um um have a look at the entity called person ok for person again you can think of lot of different attributes what is the name of the person ok now when you say name um usually in several cultures you you um you actually divide a actually name into first name last name middle name um and so and so on ok the initials and title and so on and so forth right so so name could actually be a composite attribute here ok which in turn has multiple other attributes may many other attributes say first name middle name last name title initials and so on and so forth  refer slide time  44  11  then there could be age or date of birth address and usually you need to have a unique identity to um unique way of identifying a person um well um this is um this was actually created by some of my students were who whose pan number has a has the um um has the key attribute for a person but usually in a um it is quite unlikely that in a in a conference setting you would ask for a persons pan number um usually it would be the email address of of this person um which um um or the contact email address of of this person which would be the key attribute similarly they could have something like phone numbers and phone number here is is treated as a multi valued attribute which means that um this attribute can have multiple values so what what is that mean that um a person can have multiple phone numbers ok and i hope you know the difference between um a multi valued attribute and a composite attribute right a composite attribute is also made of multiple attributes but um each of these different attributes may belong to different domains right so name can be can have first name middle name last name were a middle name can be um can be constraint to be a single letter um if if the middle name is an initial right whereas first name and last name can be varchars or um or um strings and so on but when i say phone number um when there are multiple um um values for that phone number all of these values belong to the same domain or or of the same type right so so that s the difference between a multi valued attribute and composite attribute right  refer slide time  46  25  so similarly other these thing um when when i say that when i say author you can give an author_id for um um for each author a login id or whatever and um um every other um um attributes of a of a person would be inherited by author ok because author is a person and um so so an author is suppose to have a pan number and date of birth and phone number and and so on and so forth right  refer slide time  46  45  similarly for paper um its already specified there that each paper would have a um unique identification or a unique key right so so for paper paper_id would be the key and um several other things what is the title what is the um category the the classification of those attributes the the paper content itself the um the keywords that that that are given for the paper and so on all of them could be attributes of a paper right  refer slide time  47  16  and when i say pc member you can again give member_id for for each um um for each members again some kind of a login id or something which um um um which which would form the primary key for um each member right  refer slide time  47  32  and um again several other this thing reviewers and pc chair so so so reviewer would have something called um subject of expertise and pc chair would would have something called conference headed and so on which which can be attributes of those um respective entities right and journal again so so journal _id year of publication topic were were i can always have a journal_id as um as as the um as a as a primary key  refer slide time  47  49  so finally we we come to um an overall um schema for for the entire um system were um were we put all of these together to to form one big er schema ok so let us spen spend um um a little bit time in turn by reviewing this schema ok so so what all did we go about um looking at ok we we started out by saying were is the conference ya conference is here  refer slide time  48  06  so conference is handled by a program committee right and um program committee is a week entity type so it has no existence without a conference ok and pr program committee consists of different members ok so um among the members there are reviewers and a pc chair ok and there is one pc chair there there is exactly one here right so and this is a disjoint um um relationship that is a pc chair may not be a reviewer and um um a reviewer may not be a pc chair and a pc chair heads program committee so so pc chair may head one or more program committees like this right so um and similarly um um you have a conference and a journal forming an event right that is um um a given conference or a journal may may be form forming an event to which a paper is submitted right so a paper may be submitted to zero or one event ok so um so you may not submit a paper at all or you may submit it to at most one event ok and an event may um should have at least one paper or it may have any number of papers and so on ok and a reviewer reviews a paper or paper is reviewed by reviewer ok and there is a constraint here that is a paper is reviewed by at least three reviewers ok now the reviewer the review of a paper ok the process of review of a paper will um will result in a result being assigned ok or will create a new attribute called result which the reviewer assigns for this paper ok so so this result is actually a attribute of the relationship itself ok now again a paper is authored by an author ok and there is a contact author ok so so there is exactly one contact author and it is autho authored by one or more authors ok so um um  noise  and and so on and um and both author and members ok or persons ok why do we need this persons because we are we are having attributes of a person separately that is a person should have a pan number and um um address and telephone number email and so on ok so all of those attributes of a person are inherited by both members and authors ok similarly all of the attributes of um um or a combination of the attributes of conference and journals um is is inherited by um um comes to event and all attributes of members are inherited by reviewers and pc chairs right so so um a member should have certain privileges or benefits or whatever all of those are um inherited by both reviewers and the pc chair ok and because pc chair is a separate member a pc chair may also have some attributes which are not um shared by reviewer um um or which um which which may not exist for a particular reviewer and so on right so so um so what we saw today is um we have taken a fairly um complicated example i mean its not a um and this is a realistic example a conference management system in fact you can search the internet for something called confman ok which which is a freely available um um i guess open source conference management system which uses a back end database management system in order to manage activities like this or um you you might going to um um msrcmt which is the microsoft research um conference management tool um which is actually used by major conferences around the world and um which also has something like this that is there are reviewer there are authors there are papers there are pc chairs there are committees and so on and so forth and um and there little bit um or rather significantly more complicated than this but the the level of complication to which are the are the level of detail to which we have seen um in this is fairly representative enough because we have we have seen some of the major kinds of um um conceptual requirements that arrive for example um um a pc chair is a member of the program committee but may not be a reviewer um um but may not be any any other reviewer right and contact author should be part of the author list and and so on and so forth so so all of this form tricky um details which um which manifest themselves during um during your conceptual design right so um what will do in the next class is to take this idea forward and take up individual um chunks or pieces of um this er diagram and try to convert them to um um the lower relational schema and see what kinds of tricky situations arise when when we convert them to a relational schema right so um so let us finish this class here and see you all in the next class so so this brings us to the end of this session transcription by  vidhya proof read by  shobana database management system lecture # 42 case study  part 2 database design dr.s.srinath hello everyone um welcome to this session in dbms so in this session today what we would be doing is we would be continuing from the previous session from um um the previous session were um we talked about a case study of a database design right so um again um um let me put forth a kind of assumptions or the or the prerequisites that i am expecting for for this session um i am assuming that um um you um know or you have gone through the first few sets of lectures on um the life cycle of a dbms and um you also know what is conceptual modeling what are the building blocks of conceptual modeling what are the different kinds of nuances in conceptual modeling like a week entity type like a multi valued attribute like a composite attribute and so on you also know the relational schema um or the or the relational model right and the the characteristics of relational model like the um models like the functional dependencies and normalization and so on and you also have um um an idea of how to convert um what kinds of rules that that you can use to convert a given um er schema to a relational schema so so i am assuming that so so will not be going to going into the rules of how to convert a er schema into a relational schema um except that um we um we are going to take a running example um um of of a conference management system that we started in the previous class and start by explaining which kinds of um we we just take up specific aspects of the er diagram in the of of the conference management system and um and just convert them to the the corresponding relational model right and of course wi will also um um look at um few kinds of transactions on top of this relational model i am i know that um we have not really looked into transactions as part of this series of lectures um as such but um i will also define transactions as we go along and um in an informal sense of course there um there um we shall be looking at transactions and transaction processing in much more detail in um in in later sets of classes but at least i will be de um defining um transactions in an informal sense so that we can see what kind of um transactions at least we can think of on top of these se sets of um um sets of requirements so so the main idea behind this being that um to try to cover the large or the first few aspects of um um a systems development life cycle that the high level design the architecting the the conceptual modeling the the high level transaction design and and just have a look at how how does the entire system look like and then you can um um over a period of time um um when we address several different um um questions through out this course you can try to target each one of these different um um architectural um um aspects of of the system like the like the relational model the the conceptual schema the um the business logic and so on and transactions um transaction manager and so on which and then try to see how you can um make a detail design into into each of these different um um aspects of each of these different building blocks of the um overall system right  refer slide time  04  28  so um let us come back to the case study for a database design that we are looking at so today we look at the the relational schema design and the transaction design for the case study that that we are going to look in  refer slide time  04  52  so what was the case study that we have been looking at let us briefly review the um case study once more and um before going into um um today s work so so the case study was about a conference management system right and we went through the set of requirements in the previous class so so let me briefly go through them once once again um to try to see what what exactly is required by this conference management system like i said in the last class its this is a this is um a fairly um comprehensive example in the sense that its fairly representative of a real life conference management system um although not as detailed as one a real life conference management system would be far more detail than this but at least it kind of captures the main essence of um of a conference management system ok so um what are the requirements the technical program of a large conference is headed by a program committee right so um a conference is um a large conference the the the adjective called large actually we haven t really made use of here because um um there is there is no other further set of requirements as to how to handle small conference so so we are just um um talking about a conference so so as far as we could um understand um as a systems designers or systems analyst um the the technical program of a conference is headed by a program committee right and the um um  noise  is determined by a program committee and the program committee in turn is headed by a pc chair right a program committee chair were um um were were we saw a program committee chair is a one person who heads the program committee right and all other members of the program committee will actually act as reviewers right so um and again we saw in the previous class that the reviewer and the program reviewer and the program committee chair should not be the same person right so um um um a person who is the pc chair will not be a reviewer and vice versa right um um and um so so that s about the program committee the next chunk of requirements for about the paper itself the the papers which are going to be presented with the conference right so um um a paper um is is an entity again ok which is authored by one or more authors and as a unique contact author ok it it basically says that it should have a contact author and it s a unique contact there there is only one contact author for a paper ok and any person who is a member of a program committee can not be the author of any paper like i said um in the previous class also this is not a realistic requirement were um because in most real life conferences this is not exactly true um but just to make things simpler we are um um we are imposing this constraint um in order to see the repercations on our um on our um database design right  refer slide time  07  56  and um um an author may submit one or more papers ok we each of which has a unique identity so so a paper has to have a unique identity which in turn means that a paper are to be treated as a separate entity in its own right right so um an author may submit one or more papers there is no um um there there is no constraint on that however a paper can not be submitted to more than one conference or a journal at the same time ok so um a given paper so a given paper can not not be submitted to more than one conference so so given a particular paper it has to go to a particular conference and so on right and a paper is reviewed by at least three reviewers so is reviewed so um it basically says that a paper shall be reviewed or should be reviewed by at least three reviewers ok so um even if a paper is reviewed by two reviewers its not sufficient it has to be reviewed by three reviewers and a reviewer should give a decision ok and the decision should be either accept or reject or be neutral towards the paper ok so so a reviewer has to give one of the three um um decisions and the um reviewer ca n't say i am i am neither going to say accept nor am i going to say reject nor am i going to say that i am neutral about this paper i have a opinion but i am not going to state it and so on ok so anyway based on the reviewer comments the pc chair um um either accepts or rejects papers and prepares a set of papers for acceptance right  refer slide time  09  09  now again we also saw this um um um in the in the previous class were the the characteristics of a given requirement specifications so it s a its again um good to review them once more um  noise  before we going to relational model design as well so so that will help us understand um um the the the suitability of our model um much better so any given requirement specifications has a set of explicitly required behavior right so so the requirements or the specification say that this this and this has to be there ok it has to this functionality has to be there and then there are a set of explicitly forbidden behavior right which says this this and this functionality should not be there or shall not be there and so on ok so we also noted that the the set of um forbidden behavior constitutes what are called as safety constraints ok which usually you you forbids something in the interest of safety so if you um violate that constraint it means that you are compromising the integrity or or or the safety of the system similarly the set of required behavior constitute the liveness um of the system so um um a completely safe system is one which does not work at all right so um if you don t go out go out into the battle field then there is no um um there is no danger of you being hit by a bullet but um but the thing is that that does not make a soldier the you have to go into the battle field so so there is a set of um required conditions were which form the liveness properties of the system so so that that are specified by the explicitly required behavior ok and in addition to liveness and safety usually um um in in almost all cases the uod will contain um a number of um um behaviors that are neither required nor forbidden ok so um um the the requirement specifications neither says yes um it is required nor does it say no it is its forbidden so um um one might call that call them as permitted behavior or whatever so so um depen um depending on how you treat this um this kind of um um mid mid way area um your conference management tool can offer um um offer an edge over the others in the sense that oh my conference management tools manages birth dates of um and sends greeting cards and so on i mean they they are not they are neither required nor forbidden and so on so and coming to um um the the first few steps of a systems development life cycle or or an sdlc we see that what we want to do or what we are going to do here is given a set of requirements specs ok um um requirement specifications we come out with the high level design ok we kind of address this in the previous class in the sense that given um um srs we created an er diagram for for the given srs ok so today we we are going to be doing this one and of course um with with a little bit of  noise  transaction design will also be doing this one right  refer slide time  11  35  so um from the high level design we go to the low level design and from the low level design um add dynamics to that um that is high level design low level design were were purely in the static so add dynamics to that by by adding the transactions and so on so you get into a system model ok so so the system model um should um ideally um reflect the systems requirement spec or the or the srs document ok and of course there are um um usually there are mismatches between the systems model and and the requirements spec and um rather than um going about wildly um um looking for features um um or looking for characteristic features of the systems model it is good to classify this deviations of between the systems model and the requirement spec into different kinds which helps us understand um in what way is the systems model deviating from the set of requirements right so we also saw this in the last class  refer slide time  12  47  were um we saw how to um  noise  how to quantify in in some sense ok um or the the kind of um deviations that that system model has from the re set of required um requirement specifications so let us briefly again um um revise through all these so so that it helps understand um how to evaluate um a given system model um two or two or more systems model against one another ok so one can say that a given systems model is incomplete if the set of required behaviors that that the model performs is a proper subset of the set of behaviors of the um requirements itself right so um it is incomplete  refer slide time  12  48  and being incomplete itself um doesn t um may not be all that serious but sometimes it maybe quite serious it could be incomplete and incorrect  refer slide time  13  07  so when is the systems model incomplete and incorrect when not only does it not um not only does it not satisfy all requirements but the um it forbids certain requirements that is it it its explicitly does not satisfy certain required behavior so um so it forbids certain required behavior that is that s when you say that the model is actually incorrect and its possibly unusable ok and um if the set of requirements ok um  noise  if if the set of um required behavior by the model is a super set proper super set of the set of um um requirements spec then the model has extraneous behavior ok it it does something extra and again providing an extra feature doesn t um always necessarily mean a good thing um it could potentially be unsafe as well when when will it be potentially unsafe when  noise  when this extraneous behavior actually trespasses on the forbidden conditions of the requirement spec right so um r m minus um r of srs um actually belongs to f of srs that is were um were you trespass on the forbidden um um requirements right  refer slide time  14  57  similarly if i have a set of forbidden conditions by the model being a subset of the forbidden conditions by the set of srs then the model is unsafe that is the model does not forbid everything that the srs is um asking you to forbid right and if it is other way around that is if the model um um if the model forbids more than what the srs um asks you to forbid then you say that the model is conservative ok but again conservative doesn t necessarily mean that it s a safe system why because  noise  in trying to be conservative you might be blocking certain liveness behaviors ok you might be um um you might be actually stopping certain behaviors that are required functionality for the model right so um if um f m minus f srs is um is actually a part of r of srs that is um part of the required behavior then the model is um not only conservative its actually incorrect so so it is it forbids certain required behavior right and what are the required conditions what are some of the required conditions by um um in this case study um we saw certain required conditions in the case study right that is a paper should be reviewed by at least three reviewers ok for example and each program committee should have a pc chair ok  refer slide time  16  10  and each paper should have a contact author ok and a paper is authored by one or more authors i mean we we can not accept papers um with no authors in them and so on ok and um a reviewer must give um a comment or must give a decision either accepted or rejected or neutral so so reviewer ca n't um um can not afford to keep silent on on a paper ok and um um paper and what are some of the forbidden conditions in the in the model a paper can not be submitted or may not be submitted to more than one conference or a journal ok which is shown in the slide here and um an author of a paper can not be a member of the program committee like like we said even though um this is not realistic um we have we have chosen that for the sake of simplicity here that is i can not submit a paper as well be in the program committee to review the paper and so on right and a paper may not have more than one contact author each paper should have one and only one contact author  refer slide time  17  11  so um so there is only may email sent out from each um for each decision and it has to go to the contact author and so on so um having said that um in in the previous class we went into each requirement um sentence by sentence and identified several kinds of um um entities and relationships among them and a and also attributes for these entities right  refer slide time  17  40  so and the finally put all of these entities and relationships together um to form one big er diagram for the problem ok so let us spend um a few minutes to to to revise the the er schema for this problem um um this is important because um now we are going to um cut this er schema bit by bit and then convert it into a relational schema ok now um what are the what are the first thing that we did we we started by a conference a conference is handled by a program committee and so on right so um a conference is an entity and it is handled by a program committee right and we also saw that the program committee is a week entity type ok its not if you can see if you can notice carefully it is not really specified by the requirements and that that the program committee are to be a week entity type but um we have to infer it i mean it s a um its its that is why the high level schema design that the conceptual schema design is a very human centric process the you have to use your common sense in order to um identify or in order to say which kinds of um um what what should be the characteristics of each of this entity type and so on ok so program committee is a week entity type ok and this is a identifying relationship um which says that a program committee um that handles a conference defines the existence of the program committee in the first place right and a program committee consist of different members ok and the set of members are divided into a disjoint set of specialized members um which are called reviewers and pc chair right so um given a member a member could either be a reviewer or a pc chair and there are to be only one pc chairs so so so there is a cardinality constraint here right and these are disjoints so therefore a reviewer may not be a pc chair and vice versa right um so a program committee consist of members and the pc chair heads the program committee so one pc chair heads um um any any number of program committees ok or given a program committee there is there is just one pc chair for that program committee right and um similarly a reviewer reviews a paper or a paper is reviewed by a reviewer ok so um um so a given paper a given reviewer may review zero to n paper that is a reviewer may not be assigned any papers at all there is no um there is neither a requirement nor a forbidden condition as to how many paper should be assign to a reviewer right however there is a condition on how many reviewer should be assigned to a paper right that is um a re a paper should be assigned at least three reviewers so so three to n so there can be any number of reviewers but there should be at least three reviewers ok now once a reviewer reviews a paper the reviewer gives a result ok now the the reviewer has to give a result it is a it s a required condition again right that is a reviewer has to say either accept reject or um neutral ok so now now this result is an attribute of neither the paper nor the reviewer itself why because a paper is um um reviewed by more than one reviewer and a reviewer may review more than one paper so so you you can not assign um result to any one of them in fact the only way place you can assign result is to the relationship itself that is the the that the process of being um the process of reviewing a paper by a reviewer um um gives out a um um attribute called result ok so in addition the paper itself um is related to authors ok and and a paper is authored by one or more authors ok note that here it is one or more it ca n't be zero or more ok so we also had a condition forbidden condition that um um authorless papers are not accepted ok are are not acceptable and a paper has to have exactly one contact author ok so um um so so one of them is is a contact author and it it is authored by um many of these um other papers and so on ok so um and and in addition um  noise  and in addition authors and members um are form a disjoint um um specialization for person that is um um we store certain personal details of um um people in the um in the system there um address name date of birth phone numbers and so on and all of them have to be stored for both members and authors right and similarly a paper can be submitted to exactly one event ok or zero or one event can maybe submitted to exactly one event that is it need not be submitted so um but if i choose to submit it i can submit it to exactly one event and what is an event event is a conference or a journal right so um um if i am submitting a paper to a conference then i should see that its neither submitted to um any other conference or nor to a nor to any journal as such right so so this forms the over all er schema as part of the problem that that we saw in the previous class ok now um let us go to the next step now that is from the er schema we should reduce it down to a relational schema ok and um um which is actually managed by the the the dbms ok now the parlance of a of a relational schema is quite different i mean you have um you don t have entities and relationships and attributes and so on although you do have um something called attributes ok but in a different sense ok you have tables and columns and rows and attributes which which is a column and there are key relationships then foreign keys and um um  noise  and um um decomposition of tables and functional dependencies and so on and so forth right so um so so let us um take um things step by step and take up er to relational mapping ok so so the first um set of er to relational mapping that we are doing is is shown in the slide here  refer slide time  23  34  so we started out with conference ok now conference is handled by a program committee and we also saw that um one conference one program committee ok and um um program committee is a week entity type that is its existence is defined by a conference ok and the um conference is defined by several attributes like conference name place date topic and so on and the conference name is used as the conference key or the or the primary key for the conference ok so how do we reduce this to um  noise  um this to an er model ok first thing we s we notice that the entity conference can become a table called conference ok and all the attributes that that the conference has will form the attributes of table of which conference name is the primary key right now here if you look at program committee program committee may have a key but um but that is not sufficient why because the program committee has no existence without a conference right so um um so essentially um um while we can create a table called program committee which which corresponds to the entity called program committee it is not sufficient to just add the attributes of program committee here and leave it at that why because we need the conference name as well so um um because it is defined by the conference name program committee for which conference right so um take the primary key of conference and make it into the primary key of the program committee itself right so um in fact this would be a foreign key relationship into the conference um table right so so the next set of um um um conversions ok  refer slide time  25  17  now a pc chair heads a program committee  noise  right and a pc chair and every other member ok um note that um again for the um sake of clarity i have not shown the entire er diagram here but if you remember how it was um a pc chair here  refer slide time  25  41  ok is a member ok th um this kind of relationship here defines a is a relationship right so a pc chair here is a member ok and every member is defined by a member id right the the the every member um is uniquely identified by a key attribute called member_id and whenever um um a general entity class or a entity set has a key attribute every subclass of it or every specialization of that entity um type also inherits the key attribute therefore the the pc chair also inherits the member_id as its key attribute right so um now so so the member_id is the key attribute of this pc chair and what is the key attribute of program committee it is the it is the conference name right so so there is a key attribute of the program committee now a pc chair heads program committee and um look at this relationship it s a one to n relationship ok so so one pc chair heads may head several program committees and so on ok so how do we convert um um this first of all converting a pc chair to um um a table is trivial so so make a table called pc chair and put all the attributes of pc chair in addition to all the attributes of member um into this table and make the key of um um of the super um of the general entity type that is a super class if if one might call it that so of the general entity type called member plus any key attributes in pc chair ma make into composite key in the pc chair table right now um for this relationship itself how do we go about doing that take program committee as a as a um um um as as the key ok now um as the table and its key attribute was conference name ok now in addition to the key attribute called conference name just put the member_id of the pc chair here as one more attribute right so um so there are two foreign keys here um so one um forming the conference name attribute and um that that is one is the conference name and the other is the member_id however it is um it is sufficient if we just keep the conference name as the primary key even though this is a foreign key here why is it sufficient to keep just the conference name as the primary key because um let us look back here ok  refer slide time  28  02  program committee is a week entity type ok and its existence is defined by the conference and what is the primary key a primary key usually of course um um is something which um a key attribute which defines the existence of a particular tuple in a table right so um um so it s the conference name which defines the existence of the program committee therefore um while we add member_id to to to the program committee table um it is sufficient if we just keep conference name as the key for this program committee table right now um the the next set of attributes um er to relational mapping so so have a look at this um um this thing so so we already saw pc chair here pc chair is a member ok and a member could be a either a reviewer or a pc chair ok and um  noise  and a program committee can um um consist of several members and each member is uniquely defined by a member _id ok now because this is a super class or a or a generalized entity type this member_id is inherited by both reviewer and pc chair right so um um it forms a part of the primary key for both reviewer and pc chair ok and it um they may have other key attributes as well  refer slide time  29  20  so um one way to reduce this is um we already made a table for pc chair with member_id as the um um  noise  with mem with member_id as the key um you we can just make one more table for reviewer and again say member_id as the key and all other attributes ok now um um um now you might ask a question why not make um um table for members itself members is an entity right and why not make a table for members itself note that here it is not really necessary why because um  noise  a member is either a reviewer or a pc chair but not both because if um it s a mutually exclusive relationship between reviewer and pc chair right so there is no special identity assigned to a member other than a reviewer or a pc chair and we have seen that we need this pc chair table in um in some other context in in the previous context here right so so we need pc chair in um as as a separate table in in some other context and therefore we have also made one more table for reviewer and that s it and that that um um both of them are collectively exhaustive in um um in terms of um um in in terms of this thing so so not only are they mutually exclusive there are also collectively exhaustive and and they um um and they collectively define the complete set of members ok now um the the bigger problem here is is the reduction of this larger um relationship ok what is that relationship a program committee may comprise of several members however a member can also be um um a member of several program committees right so it s a n is to n relationship ok not a one is one relationship so how do you reduce a um n to n relationship to um into a relational schema the best way to reduce that is make a separate table so so we are calling this table as membership here were um just take the um um just take the um um pro primary keys of both of these ok what is the primary key here the conference name right so um just take the conference name and member_id right so of of a given member so given a member i know um of which conference is that member um um um a member of of pc member of and so on ok so so n to n relationships can be um um reduced by creating a separate table um um and combining both of the primary keys into that table right the the next set of reduction here so um um we we have a paper that is authored by an author ok  refer slide time  32  21  however um a paper um can be authored by more than one authors and an author can submit any number of papers right so um and of course um a paper has a um um has a key attribute called paper_id whereas author has a key attribute called author_id right so um again because this is an n to n relationship um you need one entity called paper with paper _id as a primary key one um um rather one table called paper with paper_id as the primary key one table called author with author_id as the primary key and a separate table called authored by ok which um um relates this and this ok so its its an n to n relationship right so so which which contains both um paper_id and author_id as part of its tables so so um two foreign keys into both these earlier tables right um  noise  now on the other hand um for the contact author ok each paper are to have exactly one contact author and it are to have a contact author ok so as you can see here  refer slide time  33  01  um um um this forms a one to n relationship ok so in in such case um in in a one to n relationship there is no need to create a separate table here except that for a given paper you just add the author_id here um um which is the id of the author who is the contact author ok so so so the primary key of um of the single um um entity type here can just go as um as an attribute to the table of the um multiple entity type um in a in a one to n relationship right now again um another part of the schema um as you can see here we are taking parts of the schema breaking it down and um um bringing down into um bringing them down to the actual relational schema right now here we saw that both members and authors are persons ok um obviously of course and why why do we need that person relationship here because there were certain other requirements that we needed to capture that is um for every person in the system we need to have some personal details like um um the pan number or the email address or whatever that we use to uniquely identify a person the phone numbers the address name last name first name and so on and so forth and um whatever else that that is that is required um for maintaining records about a person ok  refer slide time  34  10  now we said that um members and authors are persons that is member is a person and author is a person in addition they are disjoint that is the the members and authors are disjoint why there is again a specific um safety condition that is explicitly specified in the requirements that um um no pc member are to be um um or is allow to be an author of any paper ok so so you you only submit papers you only accept pa um consider papers by authors who are not um members of your program committee right and um um members have member_ids and so on and authors have author_ids as as there words ok and a member can be a reviewer and a pc chair ok so among here we we already have tables for um reviewer and pc chair and we saw that we don t need a table for member and we already a have a table for author ok so all that we need is um a table for person ok that is a um person and pan number and so on and so forth right and um um when we say person we also um have to add all of these details but we see here that phone number is a multi valued attribute that is a person can have more than one phone numbers ok so when you have a multi valued attribute um um um you you can not um you don t know how many number of phone numbers a person is going to have one person may have just one phone number another person may have um five phone numbers another person may have three phone numbers and so on ok so um in order to deal with multi valued attributes you have to create a separate table here called phone details were um you you put the um um primary key for person plus the phone number of the pe um of of the person ok so um um the primary key may repeat while the phone number may change so so you can have all um um all possible phone numbers associated with a person ok and of course you might have um got a doubt that why are we creating a separate table called person here and um um and not um um and not club them into members and authors because members and authors are mutually exclusive as well as they are collectively exhaustive as far as this this system is concern that is in this system we are only talking about either a pc member or um or an author ok and and nobody else um um as far as the requirements is concern the the answer is yes you can do that that is um um you can put if you want to do that then then what you have to do is take this pan number and bring it down to um the the member table or an author table ok so that um so that it forms part of the primary key of of these each of them and then take all other details of person and bring it down to either member or author ok and then for phone number you need to take um um you need to take just this one ok the the pan number um of of this one and then um create the different phone numbers here and um so um so that would form um um that would form um a good um i mean that would form a valid derivation schema but as you can see um the the problem there is that um when you bring pan number down to member ok you suddenly have two key attributes there is a pan number and member_id ok so which attribute do you associate with phone number ok now um um it is not incorrect it is not in correct if i associated member_id with phone number as well in phone details it is not in correct if i say member_id plus phone number or um it is not in correct if i say author_id and phone number but it is bad design why is it bad design because um when when you um when you perform any kind of systems design we should always be um concerned about what kinds of changes will the system um expect in in future in the future right so um in the future we may want to add more categories of people into this system for example ok so right now the only two categories of people we have in the system are either member or author ok and they are mutually exclusive and collectively exhaustive but then they need not be collectively exhaustive in the future we might add one more kind of um um um one more kind of a um um person maybe say delegate ok or a conference registrant or conference volunteer or something like that who is neither a member nor an author ok and in that case the conference delegate or a volunteer is also a person who has a phone number and pan number and so on and so forth right so um um so the the safest bet to to associate phone number is with the pan number right so so um um that is um the the primary key of the of the person attribute is the safest bet for for associating the phone number attribute right so as far as this particular um schema is concerned here um it is not in correct if i say that oh i wont have separate table called person ok but um in the interest of or by um in the interest of anticipating what might be required in the future um this might or not to be a good idea in future and the software design or any kind of information system design is full of such um um such implicit requirements in the sense that um um if you write a software that works here and now it is not usually sufficient your software should be extensible and extensible with minimal changes and um with with minimal impact and that is what makes a good design  refer slide time  40  11  so um  noise  so so basically um the um this is what i said earlier that is um um one way to take this up is um um either bring bring all of them down that is um um from from person bring it down to member but then we see that member there is no table called member as um um at all so bring it in turn down to reviewer or pc chair right so you can either bring down everything to to either reviewer or pc chair and author um who are all persons um and then put pan number and every other um attribute there but in the interest of um um robustness of your system it might probably be a good idea to to not do that and and create a separate table called person itself right now um um  noise  coming to some more parts of um um the the er schema ok so a paper maybe submitted um um a paper may not be submitted um to more than one conference or journal at the same time that is if i am submitting a paper to this conference then um one of the requirements is that it is not submitted to any other conference neither is it submitted to any other journal  refer slide time  41  23  note that even though we have not using um um journal at all um in this system that is we are not the the journal is not playing an active role in the system but it is still required because we need to check whether a paper is submitted to a journal or not right so um um but but and here we have how did we model that using a conceptual model we used a separate entity called event ok were an event could be either a conference or a journal and conference has its conference name has its um um key and journal has journal_id has its um primary key and um a paper can be submitted to zero or one event that that that was what we had made here ok so how do you reduce this to um um how do you reduce this to um  noise  the the a relational model um conference we already have um um have a table called conference so just create a table called journal as well because journal is an entity and it has its own  noise  key and attributes and so on now because a paper is submitted ok to one of these two ok um um um journal or a conference just take these two primary keys ok and add them as foreign keys here ok so so put both journal id and conference id or conference name rather um as part of the paper so given a paper you know exactly to which journal or conference was it submitted ok now note that um these are added as um foreign keys and not primary keys of course um that is you you don t they don t define the existence of the paper and um um a characteristic of foreign keys is that foreign keys can be null that is because a paper can be submitted at most to one of these two that is one of either journal or a conference one of these is um um is guaranteed to be null in any given um um um in any in any given tuple ok in fact if they are um um if both of them are non null then then we we have an integrity violation we have a constraint violation in the in the system already right and the last one is the um um is the um attribute which is assigned to a relation itself that is a paper um um should be should be reviewed by at least three reviewers so so zero to n papers reviewed by three to n reviewers and the reviewer should assign some result ok um accept reject or neutral and um the the result as we see here belongs neither to the paper nor to the reviewer in fact the the result belongs to the process of reviewing itself right so so so the result belongs to the relationship called paper is reviewed by a reviewer ok so um how do we um um manage this in order to um capture that the result belongs to um um a relationship make a separate table in the name of the relationship ok so so reviewed by becomes a table and um how do you um recognize or or how do you um um define tuples in this table take the paper id or take the primary key here make part of the primary key here take the primary key here member_id that becomes a primary key here and the result ok  refer slide time  43  38  so for this paper_id this member_id gave this result and so on ok so um um so whenever we have a um um relationship that that is part of the um um that that is part of the um um rather whene whenever we have an attribute that is part of the relationship um then we have to create a separate table in order to maintain this um in order to reduce this to the relational model right so so that brings us to the the relational schema itself so so the final relational schema so so how does the relational schema look like there are so many tables that that we have already found ok  refer slide time  46  01  there is the conference table which which we created to to begin with were conference name is the um primary key and then there is the date of the conference place topic and so on and so forth right and then there is the pro program committee table um which actually represents the week entity type called program committee um therefore um um it has it um it takes the primary key of the conference table um um namely conference name as its own primary key and then there are um um and then there are other attributes like strength ok which is which is the attribute of the program committee and then there is member_id which um which is the foreign key into the pc chair table which um which says who is the mem who is the pc chair of this um program committee and so on and then um we have the reviewer table after the program committee table we have the reviewer table were um um member_id is the um um is is the primary key and of course pan number um we inherited from um um from the person attribute because we may require it for other purposes and then some other attributes like subject_of_expertise and so on right and then there is the pc chair table were member_id is again the primary key and um um maybe some other um um attributes like number of conferences um that that is headed and so on and whatever and in addition um we have inherited any other attributes from members or persons and so on because pc chair is a member which is in turn a person and so on and then there is the membership table ok remember were the um were the membership table came from the membership table came because um um a member or pc member can be a member of more than one conference committee and a conference committee ca um would have more then one pc members and so on right so in order to maintain this n to n relationships um we created a separate table called membership were um both conference name and member_id um um form the primary key and which are both foreign key foreign keys into their respective tables that is the conference and um um and either um program committee or um um either reviewer or pc chair and so on ok and um  noise  its its actually the reviewer not not the pc chair and um then we have the author table were um um author_id is the um um author_id is the primary key then pan number is inherited from um from the person entity and everything else um i have not specified any other attributes that that an author might have then authored by is another um um um table um um in order to denote the n to n relationships between an author and a paper that is an author may submit any number of papers and a paper may have any number of authors in more than one authors in there in there right similarly person is is a separate entity now um we created the separate entity called person like i said in the um um like i said previously um um in the interest of extensibility of the design rather than even though it is not in correct to to just take this all this person details like first name middle name last name date of birth and put them straight to um reviewers or um authors and so on um but in the interest of extensibility and the interest of um um the robustness of the system we have made person into a separate table and of course as you can see there is no um um there is no mathematical rule that says that you have to do that and its again a common sense decision that that we that we expect that more kind of persons maybe added to the program um maybe added to the system at a later point in time and so on then a phone details is another table were which is required because of the multi valued attribute in the in the person table um um wer um were a person can have any number of phone numbers so so you have um a pan number and phone number combination which which details any number of phone numbers that um a person may have and then there is the journal which is required just to keep track of the fact that a paper can may not be submitted to more than one conferences or journals at the same time and then um the journal_id in fact for our purposes here the the only requirement of journal um is is the journal_id or or the primary key of the journal in this thing so so the journal_id comes into the paper um as well as conference name or or not the conference_id and um um and there is a we can add an integrity constraint that um no tuple shall have both journal_id and conference_id as no non null attributes or not null attributes and so on and um reviewed by is um another table which comes out because of because of the attribute called result which is um a member of the relationship reviewed by rather than a member of either paper or reviewer right so um we get a separate table so um um once i create table tables like this  refer slide time  50  35  then you can um um essentially um for for each table you can um you can write the corresponding sql statements so create table conference conf_name place and so on and say primary key conf conf_name and so on  refer slide time  50  57  and or something like create table program committee and um conference name um and strength and so on and say primary key is conference name and but conference name is the same conference name as as the oth as the conference name of the other table therefore you you have a foreign key relationship say saying that it refers to that um that attribute in that other table and so on right so so basically that um i wo n't going to um um the creation of those sql statements again i am assuming that you know sql and um um given a set of um relational um i mean given a set of table specifications i um i am sure that you know how to um um how to write corresponding sql statements and give it to a um dbms client which in turn can create a tables in a um dbms um at the back end ok now let us briefly come to the last aspect of um today s lecture as well um namely that of designing transactions or designing the dynamics of the system we have not really talked about transactions um as yet in in the course of this series of lectures but um nevertheless um um nevertheless for the sake of completeness let us just look at one or two um um transactions and we leave it at that and we wont going to lot of details into into transactions itself ok so whats a transaction a transaction is a set of activities or as or a set of tasks which represents one semantic activity or set of database tasks like read write update and so on ok which which represents um zero or more database updates um um which which represent one one semantic activity this is this is a very simplistic definition of transactions but um um we will just have it at that  refer slide time  52  29  and transactions have to follow um have to possess what is called as the acid property again we will um i shall not be going into more details into this acid property but um you will be looking into acid properties in a later lecture in much more detail acid basically means um stands for atomicity consistency isolation and durability so essentially what it means is that um um the the entire set of activities or tasks that that form a transaction should be performed as one atomic whole either you do the entire thing or none of them and they should be performed in isolation in um of other transactions so so no two activities of um no activities of two or more transactions should interfere with ano with one another logically and so on ok and um  noise  and in order to manage um um a design transactions we should also um um know what kind of system architecture as we use in this ok so let us make some assumptions about the system architecture ok so we are using a client server architecture were um the the databases serve or is powered by a dbms or a database management systems and the client sessions are stateless mean meaning what the the server does not remember previous client interaction so so every time um you need the the server to do something um you need to give every possible requirement or every possible data to it in order to perform this activities ok  refer slide time  53  42  so let us look at some transactions some simple transactions that that give us an idea um um as to um what kind of activities would go on ok so how let us say registration ok let us say i want to register a new author or user or whatever reviewer or pc chair into the system ok  refer slide time  54  37  so how would you register typically you would have a form in front of you ok were um you would have all details um um what are the details look at the person or author or um reviewer um um tables and just make all those details there the the name first name last name um pan number and so on and so forth and phone numbers and so on ok so so verify those details for any syntactic errors ok then read the corresponding tables verify that there are no duplicate registrations that is one person is not trying to register multiple times once this is not there um um once this is not the case once we verify that this is not the case then you can update the table ok now what this means is that all this four activities have to be performed as one semantic whole ok as you can see how many different activities are happening here there are um n number of reads happening here and one updates happening here ok either all of them should happen or none of them should happen so so all of these form one semantic activity called the transaction right similarly login ok so so let us say that i have created user id for my self i have to login ok so verify user name and password field for syntactic correctness that is i provide my user name and password for login then read the corresponding table ok author pc chair reviewer table match specified user name and password ok we have not really taken care of password then  refer slide time  55  35  so so another idea here is that once you start designing transactions you will see that you need these two um attributes also that is user name and password which we had not factored in at all when we are looking at the um um at the er or the relational model so so so you have to go back and change those make those changes and this thing ok and then you have to maintain sessions and so on and let me not go into sessions um session management and so on and one more transaction which i am not going to go into detail here upload paper transaction or whatever ok  refer slide time  56  19   refer slide time  56  27  and you can think of several more transactions like this assign reviewers submit review arbitrate so so all of these are semantic activities which perform um zero or more updates of the entire system right so how would the entire system look like see why why did i just take up transactions um um for for a brief while because just to show you how does the entire system look like now ok  refer slide time  56  49  so here we had the um er model and from the er model we we develop the relational schema ok and this the dbms here um actually this whole thing would be the dbms so so this the database here and on top of the relational schema there would be a transaction manager um which would manage transactions um on top of this would be what are what is called as a business logic manager which um which handles multiple transactions and so on um which in turn would be served by some kind of a dbms server um through which the client connections are um um are handled right so so this would um um basically um um this would um um conclude the overall system design of of one particular case study of of the entire system of course um firstly we are not comprehensive i mean this not um comprehensive enough um um to to to take up a real life system but but mainly the the whole idea here was to give an idea um give the the whole idea behind this is to give you a perspective into um what it takes into designing a particular dbms ok so with that let us come to the end of the session transcription  shobana proof reading  vidya database management system dr s srinath lec  1 conceptual design hello um in today secession we would be looking at an important concept in the design of databases namely  refer slide time  00  01  24  what is called as the conceptual design of databases using model called the entity relationship or the er model now you might have heard somewhere that databases today are based around the relational model rate where the the fundamental aspect of the building block of a data base or tables so where does this er model coming to the picture and what is its relevants and why do we need um something like an er modeling for databases so in order to answer that question let us begin with looking at a small um  noise  description  refer slide time  00  01  57  of what a typical database design process looks like um this is just a simplified representation of what a database um design process looks like um however um this this captures the gist of most database design process whenever we talk about databases we should always remember that databases are always um embedded within some kind of an application context and this is called the information system within which the database is embedded ok so a databases could be embedded within um um for example for a banking solution the databases could could be embedded within railway reservation solution there could be a embedded within a university setup a company whatever ok now whatever this this setup is it this context or the information system context is what is called as a uod or or the universe of discourse the universe of discourse basically tells us what are the building block that make up this context within which this database is going to be run ok now generally when we talk about designing a database system we first talk about um looking in to the uo uod and collecting requirements from the uod we have to make an analysis of the uod saying for example suppose you want to build databases for for a bank we have to first identify or we have to first understand how does the bank conduct its operations what kinds of um operations do they have and what are their rules what are their norms and um what identifies a correct operations from an incorrect operation and so on ok and that is going to give us two kinds of um requirements the the first is what is called as database requirements or you might also term it as data requirements and the second is what is called as a functional requirements now as you might have imagine that database requirements are handled by dbms or the database management system ok however a dbms by itself is not a complete solution to give you an analogy dbms is something like an engine ok now we can t just have an engine we have to build a car around it or we have to build a bus or whatever it is around it ok so this this layering around the database system is what is called as the application programming or or the information system so the functional requirements go to go into designing the application program or the larger information system with in which the database is going to be ok  refer slide time  00  04  24  now  noise  continuing with this process um we take the database requirements and come out with a first level or or what we called as a high-level conceptual design we have to first understand how the how this database requirements look like what are the different data elements and how are they related we need to first understand in a way or rather represent it in a way that is that is understandable by human beings rather than computers we have to first understand the solution problems first before we um give it to the computer right so um so database requirements um go into what is called as the conceptual design and the the output of the conceptual design is what is called as the conceptual schema ok now this conceptual schema is what we are going to be taking up today in more um in more detail ok but um going on with the process itself let us take let us see what happens with the functional requirements ok so functional requirements also go through a similar um  noise  similar process where you do functional analysis you analyze what are the different functions if it is a bank um  noise  what are the different functions opening an account is a function  noise  or or making a bank transfer is a function now what are the constrains that that holdup these functions what is a correct function what is an incorrect function and so on ok so once this functional analysis are made we get what are called as high-level transaction specifications infact we will be revisiting this topic called transactions as a separate module where um we will see that um um  noise  many times we need to club different operations of the database into one logical unit or what we called as a transaction ok now transactions are what are going are the building blocks of the functional requirements or the or or the functional ana analysis of the information system now you can also see down here that all of these until now all of these are dbms independent process ok so what is meant by dbms independent process it means that doesn t matter whether you are using an oracle database or an ibm db two database or a mysql or postgres or whatever it is you will have to look at the uod you you will have to understand your uod you have to come out with a conceptual design which has a which has no way relevant to the actual implementation which the which the dbms does and you have you have to do a functional analysis ok which is completely independent of the dbms that we are using  noise   refer slide time  00  06  50  next comes um the dbms dependent um processes ok so we ended up with the conceptual schema previously in the in the database stream you can say that there are two different streams of a process is happening one is the database stream and the the other is the functional stream or the application stream ok so in the database stream we had ended up with the conceptual schema now the conceptual schema in turn is um should be transferred in to what is called as a physical design ok now what is physical design the physical design actually um tells you how the the how the data itself resides on the database on the computer ok so obviously this is a dbms dependent process ok so the way even though there are many standards um that that exists now the way that let us say oracle stores its um data would be different from the way db two stores its data ok even though there the due to some standards there there are going to be slight variations ok so this physical design results in what is called as an internal schema or or the the schema which actually should goes into the dbms ok and similarly the the high-level transactions specifications that we are talked about are also going to impact the physical design of the database i mean um um um suppose certain kinds of operation have to be performed together it makes sense to design them near to one another so to say let me just use the word in um slightly lose fashion here to one another so that um it becomes easier to handle transactions on ok so you get inputs both from the conceptual schema and from the high-level transaction specs in order to make your physical design of the database similarly the the um application stream goes of in to into application program design and transaction implementation finally the application software ok so um under from all this now we are going concern hassles in this section with just one aspect of this entire process that we looked at namely the the conceptual schema ok now what is a conceptual schema and why is it important for database design  noise  ok  refer slide time  00  09  06  so let us um have some observations about what is a conceptual schema we just know saw that the conceptual schema is a is an outcome of the high-level conceptual design of a database ok so this is one of the first things we are going to do we are going to understand or uod or the or the universe of discourse it tells us what kinds of data that the the uod is going to handle and based on this we are going to come out with a high-level design ok so it s a very concise description of the data requirements what kinds of um requirements to to  noise  do users need and how are the data related between one another and so on again let me take a brief examples suppose you are building banking solution ok um one of the first things that you are going to note is that in a bank there should be accounts there should be customers there should be um um customers or account holders or whatever you called them ok and um there should be of course monitory transactions and there should be some kinds of logs and ledgers and account books and so on ok so all of these represents some kinds of data elements ok and a a very high-level conceptual  noise  design would identify each of these um data elements and and say this is how they are related ok so it it includes the description of what you called as entity types these are the entities that makeup the the uod ok um in a bank the the customer is an entity an account is an entity a ledger is an entity um  noise  similarly there in any other um uods there would be several different entities like that whether it is a railway reservation system or a university or whatever there there going to be different kinds of entity types ok um and there are relationships across the entity types ok so customer holds an account an account transfers to another account and so on and so far ok so there are relationships between different entity types and there may also be certain constraints that that are imposed either by physics or um or by the uod itself ok in a bank for example um um one might say that in in a sbi account balance may not go below fifty rupees or whatever that s a constraints that is imposed by the uod that that says you can not have less than fifty rupees in your account in order to make a term in in order to make a term working ok on the other hand there could be other constraints that are imposed by physics itself the the physical reality for example if you are um if you are modeling a railway reservation you can t obviously have two trains starting at the same time from the same place from the same platform so it is ok um simply because these these two those those two can not exit ok at at at the same time ok so there are different kinds of constraints that exist among data elements and um this is what we are going to capture during the conceptual design and um another thing to note here is at a conceptual design um has no implementation details here note that it um earlier we had noted that a conceptual design is a dbms independent process ok so that means  noise  we are not really worried here about how um this conceptual schema is going to be implemented within a database systems and the main idea of a conceptual schema is ease of understanding we should be able to say to your end user this is what we have understood from your description this is what we have understood of your needs ok and is and and your end user should be able to um understand your understanding so to say ok so he should be able to understand your understanding even though um your the the end user may may be a non-technical user ok so the main idea for behind the conceptual schema is ease of understanding and it is used mainly for communication with end users who or who or of and non-technical nature  noise  ok  refer slide time  00  13  08  so um here is a small example um an example company database ok so suppose we look at a company um what are some of the first things that we can identify first some of the first things we identify  noise  a company of course has employees we can see them ok um then um company has departments each employee works in different departments and so on then once we starts speak to employees we find that there are also different projects and each department has some one or more projects and a project may span different departments and so on ok and once we start to the hr talk to the hr department we note that um each employee also has some dependence and um and there there are also covered in someway or supported in someway from the company and so on ok now if you look at this slide carefully here we see that there are two different kinds of things that that we are noting here all these things before the colon we see the things like departments projects employee dependence and so on and after the colon you you have a terms like name department id manager or name project id location or name pan pan number address and so on ok so what this says is that all this names before the colon like departments projects or entity types ok they are different entities say department is an entity project is an entity employee is an entity and so on but the once after the colon like name or manager or location and so on are what are called as attributes of this entity ok that means they belong to this entity for example the name occurring after department here belongs to this department the department id belongs to this department similarly the the project id belongs to project and address belongs to employee date of birth belongs to employee and so on ok so um it not also not only belongs to employee it also tells us or it it describes something about the employee ok so if there is an employee an employee has a name an employee has a pan number um if he is paying income taxes regularly that is he has an address um he he has a salary he has um well he or she has a has a has a gender and  noise  date of birth and so on right so all of these attributes define what is an en entity type and what are the characteristics of that entity type ok  refer slide time  00  15  35  so let us formulates things a little bit now um um in the er model or or the entity relationship model we have standards notation of representing entity types and um and their attributes as you can see in the slide here um um entity types are represented by rectangles ok so so department is an entity and attributes of the department like department name department identifier manager of the department and so on are are represented in the form of ovals or something like balloons handy hanging over this entity types ok so um there are some definition here so um so if you review this an entity represents an object of the real world that has an independent ex existence note that term independent existence that is you should be able to identify the the an entity independent of every every other entity in the in the system ok um address is not an entity because address belongs to some entity called employee ok or department of whatever ok however the employee itself is an entity because it is an independent um independently occurring object in the uod ok and an entity has entity has attributes or which are properties that describe the characteristic of this entity ok  refer slide time  00  16  53  now what are the kinds of attributes um or what are the different kinds of attributes that could exist we saw name and age and date of birth and so on but can we classify this attributes into different kinds of attributes and it can tell us some thing about characteristic of the entities ok so there are several different classifications of attributes we are going to look at um um small number of such classification one of the first classifications of attributes is simple verses composite ok so um um what is a simple attribute um a simple attribute is um let us say something like age ok you are thirty years old thirty age equal to thirty ok so so that s a simple attribute that is that is just one value associated with an attribute um so when you say age you you get back one value called thirty on the other hand suppose you say name um in some countries ok we say name is name you you you just talk about one name for for an employee but in several different places we have um when you talk about name you you have to specify a first name and middle name and last name and title and um nick names or whatever so so so basically a a name is composed of several other attributes within it so that is what is called as a composite attribute versus a simple attribute which which has just one value ok and there could be a um something like a single-valued attribute or a multi-valued attribute ok now what is a single-valued attribute again age is a  noise  single-valued attribute ok at any point in time atleast you have just one age right that is at at this point in time let us say you are thirty years old or thirty five years old or whatever ok on the other hand at any given point in time an attribute may actually have multiple values ok now take the color of a peacock for example ok no matter when you are going to measure this no matter at what time you are going to take a snap shot of a peacock you are always going to find many colors ok so um um so so so so this is a multi-valued attribute that is the attribute itself is defined by multiple values how is this different from a composite attribute let me pass here for a for a little while so that you can ask this question to you to yourself again um how is a multi-valued attribute different from a composite attribute now composite attribute if you um if you saw if you notice closely is made up of different sub attributes for example name is made up of first name middle name last name and so on ok and initials for example or title and so on ok all of these need not be of the same type um i can say title can be either mister doctor misses whatever and so on ok only these three um um types on the other hand name could be any thing any string and so on ok on the other hand when we look at multi-valued attributes all the different values that it takes color of a peacock for example it might have different um um different attributes many attributes but all of them denote color ok blue green red white whatever all of them are color all of all of them are the same types ok so multi-valued attribute are um um different values of the same attribute type but a composite is a collection of different smaller attributes in itself ok  noise   refer slide time  00  20  19  and then their what are called as stored attributes versus derived attributes ok now stored attributes is something which you just store in the database your date of birth for example ok so um um when when you ask what is your date of birth you you just give some date and then store it in the database ok however suppose you are to asked what is the age of an employee ok now suppose you know today 's date and you know the date of birth of the employee you can always derive the age of the employee  noise  right so so so age in in a sense is a derived attribute a good database design would um would put age as a derived attribute because a database hopefully is going to be used for a long time right its its not just today today an employee might be thirty years old but five years later he wo he still wont be thirty years old he would be thirty five years old right so um so its its always best to have age as a derived attribute because um you you always know what is the current date you always know the the stored attribute called the date of the birth ok um similarly there are um um what are called as null attributes or or null values for for attributes sometimes an attribute may not have any associated value um um um for a particular case ok and let let me give an example suppose a student um can take um at most two elective courses in in a semester ok and at least one elective course and at most two elective courses ok now let us say there is a student who has taken just one elective course ok now because he can take at most two elective courses there are four differents um um there are two different slots ok elective one and elective two ok now let us say after at the end of the course is give a a grade for for each of the courses that each of the courses has taken up ok now he gets a grade for the first course but he doesn t get a grade for the second course because he didn t take the second course at all elective two has no grade but this grade is not um um we can t say that he has scored zero in in elective two ok so a null value is different from zero or unknown or missing value ok this is not a missing value this is not a this not zero or this is not an unknown value this is a null value  noise  that means that something like when you say n a when you write in  noise  application forms this is a null attribute ok  noise   refer slide time  00  22  47  so so let us come back to entities and define a few more terms here with with respect to entity types ok so we saw that an entity type ok have been using the word entity type but have never never really um formally defined this term right so an entity type is is a definition or or defines a collection of different entities of the same type ok for example um um in this slide we we give this example of department ok so a company could have different departments but when you say an entity type department it means that it represents all departments in in the company ok so it it is it is a definition of a real world um um object type ok so its not this department or that department it is just department it is it it is a type that that the entity is going to define ok now any specific collection of entities of a particular type is what is called as an entity set for example if you take supplies departments plus accounting department ok now suppose we we take both of them in some collection we we call this as an entity set these these are two different departments of the same entity type ok so um so we we also say that an en an entity type as oppose to an entity set ok so an entity type is set to describe the schema or or the intension of an entity set what is that mean it it basically says that how should the structure of this entity or entities of this type look like ok so suppose we say that department should contain a department id it should have an address it should have a manager um it should have several other attributes that means that no matter whether it is the supplies department or the accounting department or or the systems programming department or whichever  noise  department it is they should be defined by these attributes that is they should have a department identifier they should have a clearly defined address they should have one percent designated as manager and so on ok  noise  so um um so an entity type describes wh what is called as an intension or or the schema for an entity set ok or or different entities of the same type  noise   refer slide time  00  25  03  so um here are again a review of the representations entities types are defined by um boxes and attributes are defined by ovals and different kinds of attributes have different um kinds of ovals um may be its not completely visible here the multi-valued attributes are defined by um an oval having a double line ok and similarly derived attributes derived attributes are are those which can be derived from other um um um other attributes are defined by a dotted line instead of a solid line  noise  ok  refer slide time  00  25  38  and um we now come to another um key issue when we are talking about entity types and namely this is um um about key attributes ok now um what is a key attribute now if you notice carefully when you said that each department has to have a department identifier and a and manager and address and so on ok now why do why do we need a department identifier for for each department this is to to be able to identify each department uniquely ok now if i say department number ten it means just one department no matter what what its name is ok department name could be ambiguous it could it could means supplies and accounts and um suppose we we might have a department called administration it might mean both supplies and accounts or whatever so or name could be ambiguous but when we say an id or an identifier it s a something like what you have a pin code in post letters it uniquely identifies each entity of of this entity set  noise  ok so  noise  the department id attribute is unique for each department that belonging to this entity set ok so such attributes which can uniquely identify um entities or what are called as key attributes or keys ok so um the key attribute as you can see here is specified by an underlined um definition ok so so the department id is is is is underlined here to denote that it is a key attribute now in some cases  noise   refer slide time  00  27  14  it need not it need not always be just one and that is the key attribute ok there could be more than one attributes um which are key attributes in which case all attributes which form part of the key are all are all showed underlined ok now what is meant by a key attribute  noise  so an attribute or a set of attributes that uniquely identify entities in an entity set so is what is called as a key attribute now um  noise  like i said it need not be just one attribute that that is a key attribute there could be more than one  noise  um um attributes which which is a key ok so in which case the key attribute is called a composite key ok so it s the key is made by combining um um  noise  two or more attributes together a a composite key should be minimal what is it mean to say a composite key is minimal ok um let let me first give you the definition of what is mean by minimal and then we can see an example ok so no subset of a composite key should be a key itself ok for example um let me take the attributes departments id and department name is it key attribute of course its a key attribute ok because if i take department id department number ten supplies it uniquely identifies one particular department ok however the um the second attribute here the the name called supplies is redundant we don t need we don t need to um have this um um um attribute here in order to uniquely identify a department its um um sufficient enough if you say department number ten ok  noise  so so so that s what is meant by a composite key that is um um department id and name is not a composite key because it is not minimal in nature  noise  no subset of a key attribute should be a key attribute in itself in which case this is not the not so ok so key attributes are shown underlined in the er diagram  noise   refer slide time  00  29  14  there also certain other properties of of keys which we are going to revisit again when we are um going to look into the enhanced er model ok um what is called as um  noise  the the retention of key attributes in all extensions ok so um so let me not take this property right now and we will come back to this again that the key attribute should be retained for all extensions of a particular entity type ok now it may so happen that um um a key attribute should uniquely identify an element ok but an element need not have just one key attribute of course i can have two keys to my house right key from the front door and the key from the back door ok now it it is fine but but the only thing is this key should open only um only my house door not not somebody else s house door ok so so that s the idea that is a key should uniquely identify a house but um but a house could be identified by more than one keys you could either enter through the front door or the back door ok similarly um um i can have a a let let us say on on a on a computer network you can uniquely identify a machine by its ip address or or so to say an ethernet address on on a lan and so on but a machine could have more that one ethernet connects in which case both of them uniquely identify the um identify the machine ok so um so so there is no restriction on how many keys that you are going to have but usually we are going to use one key so what is called as a default key usually we are going to enter through the front door ok not always through the back door right so usually we are going to use just one key um but you may have more than one keys that define a particular entity type and there are there could be some entity types which have no key attributes ok we are going to take a look at one such example little later on and um um it may not be possible to define um any kind of a key attribute for um for such entity types and um such entity types are called weak entity types or entities of such type are called weak entities ok so they have to um what we called as totally participate in in some relationship in order to define um themselves ok  refer slide time  00  31  25  now we come to the next definition of definition in the er model what is called as a domain ok what is meant by domain of attributes ok now a domain to put it in a very informal fashion is is going to show you the space in which an attribute is going to be defined ok for example if i say age of an employee ok and there are some set of rules that you can not have employees lesser than eighteen years and greater than sixty five years the domain is basically the a number between eighteen and sixty five ok obviously it can be um um um there are there are also some physical limitations in the sense that the the domain of an age can never be negative ok so so that s the nature imposed restriction but there could be other restrictions and the which which basically defines the space within which any value of this  noise  attribute can reside ok now if a composite attribute has um um different sub attributes or like we like we saw um name has different sub attributes first name middle name last name and so on ok so um and each of them have the room domains let us say d one d two d n so so we see that the domain of the composite attribute is cartesian product of the domains of individual attributes so the first attribute can take a value between eighteen and sixty five and the second attribute can take a value between zero and ten and so on ok so so the domain of the entire attribute is just a cartesian product of all of these terms ok  noise   refer slide time  00  33  01  so let us briefly come back to this um company example and see likewise what whatever we have um seen until now so we we could we can think of different kinds of um entity types like department project employee and so on and um there are different kinds of attributes for each kind um each of these entity types and there are certain key attributes here which are underline and then there are certain um um composite attributes which are shown like this name is composed of first name middle name last name and so on and like that so um and several other entity types that we can identify during our analysis ok  refer slide time  00  33  39  the next um concept that important concept that we are going to be looking at in er modeling is the notion of relationships like the um um modeling itself says its about entities and relationships ok so we just until now we looked into what are called as entity types ok so we we defined as an entity type an entity set attributes different kinds of attributes and keys and domains and and so on ok now let us see how how can we relate entities of different types ok  noise  so a relationship are to be precise relationship type um defines a relationship between two or more entity types ok so the slide here shows a relationship type ok between um um an entity type department and an entity type employee ok and and it says managed by ok so it basically says um a relationship type specifies that any department or any entity of type department should be managed by some employee or some entity of type employee ok  refer slide time  00  34  54  so so let us go further and make some definitions ok first of all let us let us first defined what is meant by relationship type ok so relationship type r is an association ok is an association among different entity types it need not be just two entities it can be any number of entities ok so um so there could be n number of entities and defines it it basically defines a set of associations ok what is called as a relationship types ok so if you if you look at it a little bit carefully a relationship type is just a subset of the cartesian product of all of the entity types ok so that means um um  noise  an um an entity instance or entity or entity of this type could be related to some entity of this type and some other entity of the next type and some other of the next type and so on and all of this define one instance of this relationship ok  noise   refer slide time  00  35  49  so so let us delve deeper into what are meant by relationships and what are some of um the the characteristics that that defined relationships ok firstly the the notion of the degree of a relationship type ok so what is a degree of a relationship type its simply the number of entities or um that that are involved in this relation it is how many between how many relations is this relationship going to establishes an association we saw earlier um a re a relationship type of degree two that is a department is managed by an employee ok so so there are two entities that that that participate in its in this managed by relationship ok such kinds of relationships are called binary relation relationship binary basically because there are two entity types involved ok similarly there could be unary relationships there could be ternary relationships and so on and there could be n array relationships n different types of relationships ok  noise   refer slide time  00  36  49  now um we come to kind of a tricky problem here now have a look at this slide carefully the first um um picture above shows the relationship type ok it shows employee work for department and the second relationship and the second picture below shows employee as an entity and department in which the employee works in as an attribute of um um of employee ok so which is correct um is is department and attribute of employee that is does the department in which an employee works in describe the characteristics of an employee or is it that departments and employees have separate existences or seperate  noise  entities and there is a relationship type between them ok so um let me confuse this problem you and further  noise   refer slide time  00  37  50  take a look at this slide here ok so um in the earlier slide you could have probably said that no no department can not be an attribute because department has an independent existence we we already saw that each entity type should have an independent existence right so because departments have an independent existence they ca n't be  noise  they ca n't be an attribute ok but have a look at this slide here ok the first picture shows employee works for department as a as a relation and then department has a department id as one of its attributes ok now i am going to take this department id attribute and put it in to employee and says employee works in this department ok so this employee is associated with this department id so now which is correct or or which is wrong ok so is is the first one correct or or the second correct um well the answer to this um obviously is it depends ok  noise  it depends on the the particular situation in which you you are you are looking into ok now um the first one um where where we show that employee works for department as a separate relation depicts the the relationship between employee and department ok and the second one just depicts what are all the different characteristic that describe  noise  or what are the different attributes that describe the characteristics of employee ok that means if i am talking about an employee entity ok and if i say that an employee is or in or in some sense or an employee is um very closely tight to his department ok so he has no identity without his department id in which case you have to say department id is part of the attribute or or is one of the attributes of the employee entity type ok  noise   refer slide time  00  39  46  so relationships versus attributes its it s a slightly a tricky problem always so in order to determine whether a particular thing is is actually a relationship or is it is it an attributes ok and um um in some kinds of data models like what are called as functional data models or or in object databases um um um relationships are um are almost always refer to as um in in the form of attributes ok for example in object databases you talk about relationship by um storing an object reference let us say you have a relationship between um employee and department so so there is a reference to a department object within the entity object and vise versa in um  noise  um a and a reference to an emply employee object in the department object and so on ok similarly in relational databases which we are going to study much deeper we see that relationships are established by what are called as foreign keys ok that is there is one table that describes one kind of an entity and a relationship between this table and the other table is is described by a by a for foreign key attribute which says that um this entity of this type is is related to some other entity of of of the other type ok  noise   refer slide time  00  40  59  now um  noise  we come to an other important issue in the in the um when you are talking about relationship this is the notion of constraints ok so what is um um when we talk about relationships almost always relationships are defined by certain kinds of constraints take the examples of um employee managing a department ok um um one of the most simplest constraints that we can think of is department should be managed by at most just one employee ok we ca n't have two mangers or two heads of um heads of a department we should have a just one head of a department ok and you might also establish a constraint that one person can manage at most one department at a time in some cases there could be um um they could be allow to manage more than one department at the same time ok so that means in that case there is no constraints from employee to department but there is a constrain from department to employee ok so um when you are talking about constraints and relationship types we are um mainly concerned with two kinds of constraints what you call as cardinality ratios and participation constraints ok so what what are these cardinality ratios and participation constraints lets have look at them ok  refer slide time  00  42  15  so um take a look at this um um relationship again here now i have replaced this managed by with another relationship called works for ok now works for is is slightly more general than managed by ok a in in which ways it more general then managed by a department can have many employees working for it ok however we might have to sometimes we might have to mandate a requirement that an employees can work for only one department not for multiple departments ok so so so this is shown by these shown in one of two two different ways either like this that is n is to one which says that n employees to one department or or something like this ok so which says it could be n employees at um working in one department and so on ok so um  noise  so on the other hand what happens if this were to be m is to n ok something like four is to three or or two is to one or two is to five or something like that ok so so that means that let us say four is to three ok so that means that a department can have at most four employees and an employee can work in at most three departments and so on so so basically you can you can represent a cardinality constraint or cardinality ratios in in this case that says um participations in this constraint  noise  is defined or constrained in this by this cardinality so or number of entity types  noise  ok  refer slide time  00  43  49  the second kind of constraints that um  noise  that s important important is um what are called as participation constraints what is a participation constraint  noise  take a look at this um um slide here now um um this slide shows another relationship type which says department and handles project ok now a department um may handle several different project suppose there is a restriction that a project has no existence unless it is associated with a department ok now suppose i have a project for um um for for develop developing some kind of software now this project does not exist if it is not associated with some department if there is no if there is no department which is in charge of this project ok basically what this says is that if every project has to be associated with a department then the very existence of project depart depends on this relation so only if an instance of this relationship um exist only can only then can a project entity type exist ok or such kinds of constraints are what are called as participation constraints ok they are defined by a double line here um may be its not fully visible but it is a double line here ok so that means that the very existence self project is dependent upon the existence of a relationship of this of this kind ok  refer slide time  00  45  16  the going on further in to participation constraints we say that um the the entity type in this case project is said to totally participate in to in this relationship because if it doesn t participate it doesn t exist anymore ok so um um a participation constraint in which an entity types existence is dependent upon the the existence of this entity type of this relationship type is called as total participation that is the the the entity type or um participates completely or totally in this  noise  relationship ok  refer slide time  00  45  50  we can also think about um attributes for relationships and not for entities themselves ok just like we saw um attributes could be associated with entities we could also also allocate attributes for relationships have a look at this um um figure here it shows department as an entity type um and and says that department handles certain projects ok so and and project totally participated in this relationship and then um um an attribute called budget is placed for this relationship ok so what is this mean it basically means that this budget or or the budget that is specified here is allocated for this project associated with this department or is specifically allocated for this relationship type ok so um um in cases where let us the project is handled by more than one department ok that means that this budget is not allocated to the project ok this is allocated only for this project for working on this department and and so on ok it is not allocated to the department as well that means the department can not use this budget for anything else it has to use it for this project only ok so um um so so this budget actually is um is is a constraint on the relationship type ok  noise  so it belongs neither to department  noise  nor to the project in its entirety  refer slide time  00  47  23  however there is certain kinds of relationships where we can actually move the attributes from the relationship type to one of the entities ok what are those kinds of um um um what are those kinds of relationships take a look at this slide here the first figure shows a one is to n relationship that is a project can be associated with at most one department ok and there is a um um there there is an attribute called budget that is allocated to this relationship now um if you see carefully um um we do we do not do correctness if we move this budget attribute from the relationship to the project site ok so if i say that this budget is allocated to this project it does not lose any semantics because a project can be associated with just one department in in the previous slide a project could be associated with m different departments that is  noise  it could be associated with many departments ok so here since project can be associated with only one department it can not it does not lose semantics if um um if the budget is allocated to the project itself ok  refer slide time  00  48  36  so um one of the last things that definitions that we are be looking at today are are is what is called as a identifying relationships ok have a look at this slide here this slide here shows a figure where an employee is identified by pan number ok so assuming that every employees um is is a is a tax payer and has and has received pan number from the government um an employee is is uniquely identified by his pan number ok now let us say we prepare the the the company prepare some kind of an insurance record for each employee ok now um um let us say um this this this entity called insurance record which which contains certain parameters or whatever ok is an entity by itself right because it has an independent existence ok you can see an insurance record and and note that its its an entity by itself however um you see that an insurance record has no existence has no meaning unless it is associated with somebody unless it is associated with with some employee or some department you want or whatever ok so this is an example of um um of an entity type which is a weak entity type that means it has no keys ok the key for the insurance record is of course the pan number itself or pan number of the employee ok so so so the key of the employee um which defines an insurance record is um um um forms the key for the insurance record as well ok and the relationship that defines this such a kind of association is what is called as an identifying relationship ok so this relationship here in this um slide which which which is called as insurance details identifies insurance record with an employee it it basically ties in insurance records with with employee so that an insurance record also gets independent existence by themselves ok so as you can see um insurance record totally participates in the in this relationship obviously it has to totally participate in this relationship but on the other hand um um not every total participation may need an identifying relationship we we actually saw an example earlier where we saw um um um um two different entity types department and project in participation but which is not a weak entity type ok so identifying relationships are again defined by double lines on the relationship types itself ok  refer slide time  00  51  10  so um so let us briefly pursue a summary of the different notations we saw the the first thing that we saw was um an entity type and um defined by um a box and a weak entity type which is defined by a dotted line which which is not clearly visible here ok then relationships types are defined by um um rhombus and the identifying relationship types are defined by double lines ok  refer slide time  00  51  38  similarly um attributes are defined in in different ways that is normal attribute is an oval a key attribute has an underline underline there and a multi-valued attribute has um um double line and and a derived attribute has a dotted line ok so um so with this we come to the end of this this session where we had a brief look at what is meant by conceptual um  noise  design of of a database ok so to briefly summarize that a conceptual design of a database is meant for non-technical users its it s a high-level design and um um mainly composed of diagrammatic notations like entities and attributes and so on and within this um um diagrammatic notations we saw that there are several um different characteristic there there could be entities entity type entity sets attributes multi-valued attributes and key attributes and and relationships an identifying relationships and weak entity types and so on ok so one of the first step that that we do in um in database design is is to be able to identify this entity types and and relationships and to be able to build this er schema ok so um that brings to the end of this this first secession on um um conceptual modeling of database systems thank you 