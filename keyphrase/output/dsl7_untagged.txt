data structures and algorithms dr naveen garg department of computer science and engineering indian institute of technology  delhi lecture 5 hashing  contd  today we are going to continue our discussion on hashing in the last class we saw about the hash table  the concept of hashing and also saw how to resolve collision in hashing using linked list that method of collision is also called chaining today we are going to look at 2 other methods for collision resolution  linear probing and double hashing we are also going to spend some more time discussing how the good hash function should look like what is the good hash function the function which can be computed quickly and as said in the previous class  it should distribute the keys uniformly over the hash table all the keys should not get mapped to the same location because then the performance of hashing would become as worse as that of a linked list good hash functions are very rare and there is a famous paradox called birthday paradox there would be about 35 or more students sitting in the class there is a very high probability and you can actually compute that probability in which 2 of you would have the same birthday although you would think that there are 365 days in the year and if each one of you were to have one of these days as a birthday then there is very small probability that 2 would have the same day but that is not the case  even with just 35 people you would have fairly high probability that 2 people would have same birthdays the same kind of thing is happening here your days of the year corresponding to your slots in the hash table and even if i were to take a key and put it randomly in one of those slots there is very high probability that 2 keys would end up in the same slot that is birthday paradox collisions may take place in any kind of hash function you use then there is also a problem of how to deal with non-integer keys in fact we saw an example in the last class where the keys were telephone numbers and we had returned the telephone numbers with hyphen how did we treat telephone number as an integer we just dropped the hyphen in between and then we thought of it as an integer you are going to see some more techniques of converting non-integers keys in to integer ones the other example that i had taken in the last class was your entry number where again the key was a non-integer because it had c s y or some other thing we have to convert in to integers and what we did in the last class was as i said  those keys are just going to take the last 2 digits as the hash function value we are going to see some more techniques of converting non-integer keys into integer ones hash function can actually be thought of as being in 2 parts there is a hash code map and there is a compression map and these 2 together make up a hash function a hash function is basically a mapping of keys to indices of a hash table your hash code map  maps the key to an integer if your key is already an integer then there is no need for this but when your keys are not integer keys then you will have to 1stconvert them in to integer keys key integer  this integer could be from an arbitrary range but we need to bring it to the size of our hash table if n is my hash table then i need to bring this integer to the range of 0 through n-1  so that it can be mapped to an index of my table that part we will call as compression map we will see what kinds of functions are used for hash code map and compression map another important requirement of hash function is that if 1 key gets mapped to a certain index then the next time when i want to map a key it should get mapped to the same indexed location it is not like  the next time it should get mapped to some other indexed location in the last class we took an example of key which was 2004sa10110 and we mapped to location 10 i can not have a hash function which sometimes maps to location 10 and sometimes maps to location 13 there could not be any kind of randomization happening there why is that because when i insert it  i may be mapped to location 10 when i try to retrieve or search for it then if it gets mapped to location 13  i would not know the location of the key it should map equal keys to the same indices and of course and we should try to minimize the probability of collisions let us look at the popular hash-code maps the hash-code map is the part which converts your key to an integer one thing is that we could just take anything as the bit pattern and interpret it as an integer if you have a numeric type of 32 bits or less  we can reinterpret the bits of the number as an integer your key which has more than 32 bits in it which is a long or a double real number which takes more than 4 bytes  then you can take it in chunks of 32 bits and add them up take the first 4 bytes and add the next 4 bytes to it and so on to get eventually some 32 bit and that could be an integer you are working with such a kind of tree could also be used to compute the hash code map of a string suppose i was using the key as your name given a particular name  let us say ankur i want to convert it to an integer one possibility would be take the ascii code of a  n  k  u  r add them up and that i will interpret as an integer why is this a bad strategy why would the number of collisions be high why would the sum of 2 different names be the same only if the order is different and that happens for many different words it is not the case for the names  but many words in the english dictionary would be obtained from the same letters if you have 2 words such that the letters was same as g o d and d o g  then when you sum up the ascii values they will be going to the same location only we have to avoid such a kind of things even if the words were not the same but a was replaced by b and n was replaced by m even then we will end up with the same these are all the reasons for why it is not a great strategy especially when you are trying to convert character strings in to an integer one technique used in such settings is called as polynomial accumulation you have a certain string and 0 a is the ascii code for the 1st character of the string and 1 a is the ascii code for the 2nd character and so on you are going to think of it as a polynomial whose coefficients are 0 a  1 a up to n 1 a  1 01 1  n n a ax x a 012 2 1         n n a x a x a x a xa the above given expression is your polynomial and you are going to evaluate this polynomial at a certain value of x the evaluated value is going to be the integer corresponding to this  01 1  n aa a  string that integer might be from a large range then we will use the compression map to map it to the table but 1stwe are looking at the hash code map were in we are trying to convert a string or a non-integer data in to an integer we are looking at the setting where the string we have is this  01 1  n aa a  and we are trying to convert it to an integer evaluate the below given polynomial at some integer value 012 2 1         n n a x a x a x a xa the value of x has been the experimental stuff  people have looked at and found that if you work with  x = 33  37  39 or 41  thee values and if you take an english dictionary with about 50  000 words in it and use this technique to convert your words in to integer then you will not get too many collisions at a particular time you will have at most 6 collisions there is no theory behind it  this has been observed experimentally this is an experimental study in favour of this kind of a hash code map let us look at some compression map given an integer you have to map it to the small range of your table one natural thing would be that k is your integer and your table is of size let us say little m just do k mod m and k mod m will give you some integer in the range 0 through m-1 where k is the key and m is the size of the table suppose you were to choose your m and let us say your table is of size 1024  m is basically 10 2  when i am taking some integer mod 10 2 then essentially that means i am taking the last 10 bits of that integer write the integer in its binary representation and then when i am taking mod 2 that means i am taking the last bit of the integer if it is 0 then i get 0 always  if it is 1 i get 1 if i am taking mod 4  i am getting the last 2 bits so if i am taking mod 10 2 then i am getting the last 10 bits all the integers which have the same last 10 bits would get mapped to the same location this is bad because we are forgetting the other bits of the integer we are just taking some small set of bits that is the last 10 bits based on the hash function hence one should not do such a thing in this case if you are using the simple compression map then you should not pick up the size of your hash table to be some power of 2 in fact it helps  if you take the size of the hash table to be a prime number.let us look at an example suppose i had 2000 strings and i am trying to put it in hash table i will try to pick the size of my hash table let us say at 701 which is the prime number this will ensure that on an average i would see only 3 strings per location that is 701 x 3 is roughly 2000 in my chaining  i would have 3 as the length of the linked list one important thing is that one should not pick up the size of the hash table close to a power of 2  because the same kind of effect will start happening when you have the size of the hash table to be exactly the power of 2 if you are going to use that kind of a compression map which is just key mod m  then keep in mind that m should not be a power of 2 or even close to a power of 2 and preferably it should be a prime number things do not work when you see a lot of collisions happening lot of it depends upon the data and the keys you are trying to insert in to your hash table these are generic principles which if you follow will improve the performance there have been instances in which we did some experiment where it is better to take a number which is not necessarily a prime what are the other kinds of compression maps there is other compression map you can use  essentially first i read out the 2nd part of the above slide suppose your keys are in the range of 0 through max k  recall now assuming that our keys are integers because we first used the hash code map to convert anything that was non-integral in to an integer the keys are in the range 0 through max  so first covert them from this range  0 max k  in to a range through max k times a essentially we multiply each key with a where a is some number between 0 and 1 first we converted to this range  0 max k a   now we take the fractional part of the each key that corresponds to k a mod 1 as a consequence we get a number between 0 and 1 because we took the fractional part we have to map it in to the range 0 through m-1 so i can just multiply that number i get between 0 and 1 by m this number  ka mod 1  was between 0 and 1 and when i multiply by m i get fractional number that is why i took the floor function which means round down thus i rounded that number down to the nearest integer h k m ka    mod1  i will repeat it again you first took a key and multiplied by a where a is some number between 0 and 1 then from that you took the fractional part of that number which is again something between 0 and 1 and then you rounded it down this is another popular compression map you could have done something different  for instance i could just take this  0 max k a  and map it to  0 m-1  directly although it is not clear about how would you do it perhaps divide by m or some other thing this is one of the popular ways of doing things in the following case the choice of m is not critical even if m was the power of 2 now  the same kind of thing that happened before would not happen because we have done a lot of jugglery we have taken that number  first we multiplied it by a which was a small fraction then we took the smaller fraction part and then plotted it to the range 0 through m here it is not critical that m not be a power of 2  we could use m as 2p  some evidence if we use a as something like 5 1 2 then it turns out to be good if we use that value of a then it is called fibonacci hashing most of this is experimental without significant theory behind it so if you might want to read more about hash function there is a nice book by ronald knuth on sorting and searching which covers hash functions in detail.there is another technique for a compression map called the multiply  add  and divide which says the following  take your key multiply it by a and add b thus a and b are 2 fixed numbers then compute modulo n where n is the size of your hash table  sometimes i use m and sometimes n the first technique was just k mod n but now we are doing something different we are multiplying by a and adding b here a should not be a multiple of n if a were a multiple of n then a mod n will be 0  so ak mod n is also 0 for any key you will always get mapped to the same location b in fact a and n should be co-prime if possible to avoid any kind of patterns happening such a technique is used in your random number generator also you might have used the function random as a part of your programming if you specify the range it gives your random number in that range how does it come up with a random number many of the random number generators are based on the technique called linear congruential generators they start with a certain seed seed is a starting value which could be user defined  you could provide what the seed is or it could be a random number generator which could just take the system time at that point or some other information and use that as a seed that seed becomes the initial k value and then you compute this quantity  ak b n mod  and the value you get becomes your random number h k ak b n   mod the above function will give random number in the range 0 through n-1 then for the next random number  you are going to use k which is the last value you return we will use the last random number generated as a value of k and once again compute ak b n mod  you will use the value you get for the next time and so on this is how you generate random number such numbers are actually called pseudo random number because they are not truly random once you know the seed you can actually figure out all the numbers that you get there is another technique called universal hashing which i am not going to go in much detail  i will just briefly tell you the idea i pick up a hash function and tell you what the hash function is you can always come up with set of keys such that all those keys using my hash function will get mapped to a very few locations i think of you as an adversary who is trying to make life difficult for me let us say  by picking key which all get mapped to a very few locations in the hash table so that i have to spend a lot of time doing insertion  deletion and searching one solution i can imply is that i do not even tell you the hash function which i am going to use that means i am going to have a bunch of hash function let us say 15 different hash functions and before the process starts i am going to randomly pick 1 hash function out of these then with the keys that are given to me  i am going to use this hash function to put the keys in to the table i have to use this same hash function for inserting all my keys  for doing the search  deletion and so on for one run of the hash table implementation i have to use the same hash function i can not change the hash function in the midway but the next time when i invoke this program  i could perhaps use a different hash function because that i have picked up randomly from my set of hash function so even if you came up with the bad set of keys for one of my hash function  may be that is the hash function i did not pick up at all  when i was doing my implementation there are some results which say that you can pick up a collection of hash function and such a collection of hash functions is called universal  such that for any 2 keys the probability that they get mapped to the same location is no more than 1 m  1 pr       fk fl m as i said  this is just a brief idea about the universal hashing and i am not going to see in detail when you do your next course on algorithms in the 3rd year you will see more of universal hashing so that is as far as the hash function is concerned when you use hashing you will get collision  there is no way around it and one technique we saw in the last class was to resolve collisions what we call chaining if many keys go to the same location you just chain them up and put a linked list there you can still do insert  search and delete by doing that operation in the linked list you are going to see 2 other techniques today which fall under the general class of open addressing one of these is called linear probing and the other is double hashing open addressing differs from chaining in the following key fact recall in chaining none of these elements were actually stored in the table they were all stored outside the table  in the table all we had was a reference to the starting element of the linked list the table was only storing the pointers or the references to the first element of the linked list but now we are going to put all the elements in to the table itself as i said hashing could map 2 elements to the same location in the table  we can not put both of the elements to the same location still we want to put all the elements in the table  we will have to find some other locations for the element clearly if all elements have to reside in that table  then the number of elements that we are trying to put n has to be less than the size of the table which is m i am going to work where m is the size of my table and n is the number of elements that i am trying to put this was not a requirement for my chaining technique i could have the number of elements as larger than the size of the table  because there the elements were not residing in the table they were residing in the nodes which were a part of the linked list each entry of the table is now either going to contain an element or it is going to be null it is going to be null which means that does not have any element in it when we are searching or inserting or deleting  we have to probe the elements of the table in a suitable manner we are going to think as if we are modifying the hash function a little bit the u is the universe from which the keys are picked our hash function is mapping the keys  earlier this part  0  1  m-1  was not there we were mapping the keys  u  to 0 through m-1 and that would tell us where this key sets  for instance in the case of chaining we are going to have a second parameter and when i am trying to insert the key that will be my first probe i will compute the value of the hash function for that key  k  0  let us say for 0th probe and i obtained h  k  0  as the value of my hash function i look at the 0th location in the table  if that location is occupied then i have to look again when i look up the next time i will have a value of 1 as the 2nd parameter the 1st parameter is still the key k i am going to compute the value of the hash function for  k  1  which gives some other location in the hash table and so on i am going to different location in the hash table till i find an empty location  if the operation was one of insertion depending upon the hash function we will have many different techniques the hash function h is really determining sequence of slots which are examined for a certain key the u was the range of the keys  u is the set which specifies the collections of keys that we have the number of elements we are trying to insert in to the hash table should be less than the size of the hash table if i try to insert all the 100 students of this class to a hash table that i create then clearly the size of the hash table has to be more than 100 because each of this student has to go to 1 location of the hash table  the first technique under open addressing is called linear probing i have the key k which i am trying to insert i have a hash function h  i compute h  k   this probe = h  k  is the first place of the hash table that i am going to look at if table  probe  is occupied then i just go to the next location so probe is incremented by one and then once again i check if it is occupied if it is occupied then i increment again till i find an empty location and at that point i will put the element k this is the guiding principles that if the current location is used  just go to the next location the mod m is used to do rap around  if you reach the end of the table then you start at the beginning your question is what happens when we retrieve the keys we will come to that in a short while when you are trying to insert  you compute the value of hash function and you go to a specific location as specified by the hash function for that key if that location is occupied that is there is an element already sitting there  you go to the next location and if that is also occupied go to next location till you find the empty location one advantage it has over chaining is that it uses less memory in chaining you have to keep track of references each of your nodes should have place for the element that it is storing but it should also have the reference to the next node so that space is wasted but this technique might end up slightly slower than chaining let me show you an example my hash function is k mod 13  a very simple hash function my keys k are integers and i am trying to insert these keys in to the table 13 is the size of my table and the location is from 0 to 12 the 18 mod 13 is 5  so 18 goes to location 5 because at that point the table was empty  so it can come there 41 mod 13 is 2 so 41 goes to location 2  22 mod 13 is 9 so 22 goes to location 9 till this there is no problem in inserting  as the table is empty 44 mod 13 is 5  we want to put 44 in the 5th location but this location is already occupied by 18  so 44 will have to search for the next location as the 6th location is empty we put 44 there 59 mod 13 is 7  we place 7 there as that location is empty 32 mod 13 is 6  as 44 is sitting in 6 we go to the next location then 59 is sitting at that location  again we go to the next location and as that location is empty we put 32 there 31 mod 13 is 5  so we should put it in 5th location but this location is occupied with 18 and the continuous locations are occupied by 44  59  32  and 22 so we go to the next location which is empty and we put 31 in that location 73 mod 13 is 8  as 8th location is already occupied we check for the next locations and we put 73 in the 11th location all the elements are sitting in their respective position that is 41 at location 2  18 at location 5  44 at location 6 and so on this also shows you one problem with this technique the elements tend to aggregate  form clusters so you might have to go through many locations while searching for an element how would one search the hash table is given in the slide below which is after inserting those elements suppose we are searching for key k  we are going to compute k mod 13 because that was our hash function then this  k mod 13  is the first location we go to and after that if we do not find the element we do not say that the element was not in the table  rather we go to the next location if at the next location there is some element present then we go to the location following it and so on till we either find the element or we reach a an empty location if we reach an empty location that means the element is not their in the table because if the element had been their in the table it would have been inserted at one of the locations that i have checked let us see suppose i am searching for 31 so we go to 31 mod 13 which is 5 i come to the 5th location in which 31 is not there  so i go to the next location and search the element till i find it i found the element in the 10th location when i did not find it  i can not say that the element is not their in the table it could be their  infact it is their suppose i am searching for 33 mod 13 which is 7  i would start searching it from the 7th location till 11th location the element is not present and the 12th location is empty this means 33 could not be their at all in this table because if 33 had been there in the table  then by this time it would have been definitely inserted in to the table till the 12th position because this is an empty location that is an unsuccessful search  in an unsuccessful search the search terminates when you reach an empty location but a successful search will terminate when it finds the element how do you delete the following slide is my picture which is from the previous slide and i want to delete 32 first i have to search for 32  32 mod 26 is 6 i come to the 6th location  it is not there then i go to the next location  also it is not there then i find the element 32 in the 8th location suppose i removed it by setting this location to null i removed 32 from that location is this a good idea no why this is not a good idea suppose now you search for 31 the 31 mod 13 is 5  so we come to the 5th location but we did not find it there then we go to the next location for that element and we did not find it and at last we reached the empty location hence we will say that 31 is not their but still 31 is their why is a problem coming in because when 31 was inserted that was the full location that is why 31 was inserted later  but if you delete the element in the 8th location then you have a problem some how we have to do something different because we can not just set this location to null or we can not mark this location empty also look up will declare that 31 is not present  which is wrong how do we delete instead of setting this 8th location to null we will place a tombstone  actually an x tombstone is just a marker so you could set up a bit at that location which specifies that this location was occupied by some one it is not always the case that this will be an empty location  at some point this was occupied by some one how it will help us when we are doing a look up and we encounter a tombstone  we do not declare that the search is ended and the element is not present but we continue as before if i was searching for 31  31 mod 13 is 5 so i would come to location 5 and go to the next location and at the 8th location i would see an x and not null which is a tombstone so i continue till i find either a null location or 31 i found 31 and declare 31 is their when a look up encounters a tombstone it ignores and continues when an insert encounters a tombstone what does it do it will put the element at that position we have to reclaim this space what happens if there are too many tombstones you do not have elements in the table  those are actually empty locations but in your search you still have to go beyond them the performance of your search degrades if you have a lot of tombstones you should just rehash just remove all the elements and put them back again the same kind of a technique you have to do when you grow the table now you are not growing the table  you have too many markers in the table so just do a rehash and that will create empty slots without the tombstones and your performance will increase again i will come to the other open addressing techniques we looked at linear probing  we compute the hash function we look at that location and next location and so on in double hashing we have 2 hash functions h1 and h2 the value of h1 gives me the first position were i am going to look for the key k then h2  k  will tell me the offset from the first position were i am going to look again for the key k let us look at the piece of code given in the above slide probe is set to h1  k   so that is the first position i look at and offset is set to h2  k   first i will look at the locations specified by probe and the table  if it is occupied then the next location i will look at is probe + offset probe is set to probe + offset which means this is a next location i look at if this is also occupied then the next location i will look is probe + offset + offset which mean offset is determining key with how much distance i am going to advance every time i do not see the element that i am searching for for linear probing your offset is always 1 you were always going to the next location so that corresponds to an offset of 1 instead of going to next location i jumped one location ahead that is i jumped 2 locations  then offset would have been 2 and so on offset in this case which is in the orange color in the slide below is determined by the hash function h2  k   this offset could be different for different keys we will look at an example of how double hashing works if m is the prime then this technique will ensure that we look at all the locations of the table in linear probing because the offset was one we would look at all the locations in the table if there was an empty location you would always be able to insert the element we would not like the following to happen there are empty locations in the table but you start from a certain location  since the offset is 3 you go 3 units ahead and you keep finding everything is full and then you come back to the starting location because you will not be able to insert the element at all may be all of these elements that you looked at were full but the other locations in the table where empty some how you do not cycle back when you will cycle back when your offset divides the size of the table if the size of your table was a prime number then your offset would never divide it and this kind of a thing would never happen in fact you would look at all the elements of the table this is the small fact you can go back and prove that if m is prime then i have given you the rough arguments for this case  but you can also prove it more formally this has some of the same advantages and disadvantages as linear probing one of it is it distributes keys more uniformly because you do not form clusters any more these clusters were getting formed because you were just going one step at a time if for some key you are going 7 steps ahead and for some other key you are going 13 steps ahead and for some other key you are going 2 steps ahead  then these clusters are not getting formed any more that makes the performance better we will look at an example i have 2 hash functions h1 and h2 the h1 is the same as before  k mod 13 the element is also as same as before  we have a table of size 13 the h2  k  is my 2nd hash function and is 8  k mod 8   it will always be a number between 1 and 8 it can not be zero  because k mod 8 lies between 0 and 7  so it is between 1 and 8 the zero does not make any sense  if it is zero then we are in trouble if h2  k  is zero for some k then that means you are continuously looking at the same place and if that place were occupied then you can not insert the element at all let us insert the first element 18  18 mod 13 is 5 so it will go to location 5 the 41 mod 13 is 2 so it goes to location 2 the 22 mod 13 is 9 so it goes to location 9 the 44 mod 13 is 5 so it tries to go to location 5 but the location 5 is already occupied we have to compute h2  44   what is h2  44  8  44 mod 8   44 mod 8 is 4 so 8-4 is 4  i have to go 4 steps ahead i will go to location 9 but that is also occupied  so i will go to location 0 that is empty so 44 will go to location 0 the 59 mod 13 is 7 so 59 will go to location 7 the 32 mod 13 is 6 so 32 will go to location 6 the 31 mod 13 is 5 so we go to location 5 but that is occupied i compute h2  31   31 mod 8 is 7 and 8-7 is 1 so 31 will check for the location 6 but 6 is also occupied we have to go to 7  it is also occupied so go to 8 and this is not occupied  thus 31 go to location 8 the 73 mod 13 is 8  so it will try to go to 8 that is occupied we compute h2  73   73 mod 8 is 1  h2  73  is 7 so we will go to 8 + 7 = 15 the 15 is 2 mod 13  we go to location 2 that is occupied so 2 + 7 is 9 where that is also occupied the 9 + 7 is 16  16 mod 13 is 3 so it goes to this location which is unoccupied this is how the elements would be distributed in the table we will do some analysis of double hashing recall that i am going to assume that the load factor is less than one what is the load factor the number of elements divided by the size of the hash table n m that is less than one i need it to be less than one otherwise more than 1 does not make any sense we are talking of a scheme where all the elements have to sit inside the hash table we are also going to assume  this is similar to the assumptions that we made in the last class that every time i probe  i actually look at a random element in the hash table which is uniformly random the first time i probe i will take a random location in the hash table and try to put the element their if it is occupied then once again i will pick a random location in the hash table and try to put it their if that is also occupied once again i pick a random location in the hash table and try to put the element their let us see how this performs  because we will only be able to analyze such a scheme because the other schemes are too dependent upon the hash function that we are using and we might not be able to analyze them if is the load factor then that means 1 fraction of the table is empty if is half that means the number of elements divided by the size of the table is half which means only half the table is occupied and half the table is empty  1 fraction of the table is empty suppose my search was an unsuccessful search.what does an unsuccessful search mean that means the element is not in the table when does an unsuccessful search stop when i get an empty location how many probes will be required before i get to an empty location the 1 fraction of the table is empty let say 1 10th of table is empty and 90 % of the table is full that is 10 % is empty the expected number of probes required before i hit 1 10th fraction of the table which is empty would be roughly 10 because the first time with 9 10th probability  i will get to an occupied location and so on so roughly after 10 trails i will hit an empty location because only 1 10th of the table is empty if 1 fraction of table is empty then roughly in an excepted sense 1 1 probes are required before i hit an empty location and declare it to be an unsuccessful search this is the excepted numbers of probes required for an unsuccessful search let us look at a successful search i am going to talk about the average number of probes required for a successful search  not for one particular search but if i were to look at all the successful searches what are successful searches successful search are searches corresponding to the elements in the table i have some number of elements in the table  let us say i search for the first element then how many probes are required suppose i search for the second element how many probes are required and so on then i will take their average let us try and compute this quantity if you recall from the last class the average number of probes required for a successful search is the average number of probes required to insert those elements because when we are inserting those elements we are essentially doing the same thing it is the same as the average number of probes required to insert all these elements and this is the quantity i am going to compute what is the average number of probes required to insert all the elements that i have in the table when i am inserting an element i need to find an empty location again suppose i begin with an empty table and i am looking at the number of probes required to insert the first 2 m elements size of the table is m  let us assume m is 100 i am talking of inserting the first 50 elements suppose i have already inserted 48  49 elements and when i am trying to insert 50th element what is the excepted number of probes that are required the half of the table is empty  when i try once i may hit a full location may be when i try again  in expectation i just need 2 probes to be able to insert this 50th element for the other first 49 elements i might on an average even required less  but all i can say for sure that the average number of probes required for inserting these elements is 2 how many elements am i inserting the 2 m elements  on an average the total number of probes required is m for these 2 m elements when i show you the rest  you will understand why i am doing this way suppose i have already inserted 2 m elements in to my table and i am trying to insert the next 4 m elements in to my table when i am trying to insert the next 4 m elements  just assume that i have already inserted 4 m -1 and i am trying to insert this last element how much of the table is already full when i try to insert this last element the 3 4th of table is already full only a 1 4th of the table is empty so on an average i am going to require about 4 probes before i get to one of the empty location i am searching for an empty location to put this element in i need roughly 4 probes  infact i am just praising this as an upper bound and i need at most 4 probes to insert all of these 4 m elements the total number of probes required to insert these 4 m elements is 4 m times 4 which is no more than m similarly for these next 8 n elements  when i am trying to insert the last of these 8 n elements only 1 8th of the table is empty on an average i require about 8 probes before i can get to one of those empty locations for these 8 n elements or for any one of them i would not have required more than 8 probes i would have required between 4 and 8 probes for these 8 n elements because when i was inserting the first of these 8 n elements only 3 quarters of the table was full one quarter of it was empty  but i am just upper bounding it i am just saying that no more than 8 what is the total number of probes required for 2 m recall from previous slide i said m  for 4 m this also i said m what is the total required for these  248 2i mmm m elements it is m times i  m x i   how many locations are empty in the table what is the total number of elements in the table now after i inserted 2 m elements what fraction of the table was empty it is half after i inserted 2 m + 4 m how much of the table was empty it is 1 4  so it is really this last number after i inserted 8 m how much was empty it is 1 8  so after i inserted all of this that is 2i m how much is empty it is 1 2i which is 2 i fraction that was empty after i have inserted all of these fractions i have only 1 2i fraction of the table empty and the total number of probes required to insert these elements is m times i we have a load factor of  we already inserted enough elements so that the load factor is  when the load factor is  1 fraction of the table is empty if i have 1 fraction of table empty  then how many probes are required if i have 2 i fraction of the table empty then i require m x i probe what is i the i is basically minus log of this  2 i  quantity if i need to have 1 fraction empty  so i just need m log  1   these are the numbers of probes required if i have 2 i fraction empty  2 i is the number smaller than one so to get to this point i require m x i probes so to get to a point where 1 fraction was empty  i need m log  1  this many probes the above what we saw was the total number of probes required and the average was just divided by n that is  1   log  1   we will be able to capture it to a table for an unsuccessful and successful probes  when we had chaining it was 1 +  for probing  for an unsuccessful search it was  1 1  and for a successful search what i just showed you is  1 1 ln 1   the last slide which shows how this performances of changes 