transcriptor  v.srinivasa rajkumar educational technology i.i.t.delhi presents a video course on programming languages lecture 1 introduction so welcome to programming languages today we will just do a few elementary concepts broad classification of programming languages um without going in to too much detail um so this is the first lecture and so lets let us just look at the notion of a program or a programming language so um you are all familiar with the notion of a machine a computer and it has what upon would call a bare machine has just a piece of hardware it is usually in binary um well you can just think of it as a whole lot of switches connected with complicated circuitry um the memory the arithmetic unit all of them consists of switches activated one way or the other and even in a and you can see it is going to be a big problem operating those millions and millions of switches you know so what you have is a language it just consists of binary strings so in a bare machine what you would just have is a language consisting of binary strings so these binary strings um in what is known as the phenomenon architecture um which is called the stored program concept alright both data and instructions have the same format and um everything is a binary string and depending upon how you look at it it is either a data item or an instruction to execute something right so so that is a phenomenon concept so which means that programming such a machine um basically requires you to be able to um interpret certain sequences of bits either as data or as instructions and if they are instructions to i don t know manipulate some registers may be load into memory store um load from memory store into memory or perform some arithmetic operation or some logical operation and so on and so forth right so in general even that language what we might call the machine language can be called the programming language thus let us take a very general view um what is  03  15  a programming language a programming language is just some notation for describing algorithms and data so in general we could consider a programming language to give you a means of representing algorithms and data structures and when you have a representation of algorithms and data structures presumably you are able to perform your manipulations here so the first the first thing about the bare machine is that if you are going to use the machine language itself then there is really no fundamental difference between the algorithm and the data which means a sequence of instructions could just as well as regarded as data items a sequence of data items provided they have some circuitry could also be executed as an algorithm i mean god forbid what might actually happen but i mean in principle you could execute even a sequence of data item as instructions by interpreting it suitably  noise  so the first distinction we would like to draw is between what constitutes the data item and what constitutes a part an instruction ok so there are various um so lets um but let us take a much more high level view i mean right now we are no longer in the fifties when the early machines came in and you have to program in machine language or assembly language so we will just look upon a programming language as just a notation for describing algorithms and data and we  05  05  could look at a program as just a sentence in this language right i mean this so its it is a language like any other language and it has certain rules and you have certain what might be called well formed sentences and a program is just some sentence of a programming language a program is not necessarily an algorithm um well simply because i mean you might have a well formed sentence which is which is not very meaning full ok for example it could the program could be a non terminating program in which case it is no longer a algorithm ok um and so um important thing to realize is that an algorithm is a very abstract object it doesn t have any concrete um form what is concrete is whatever is put down as a program so the only concrete object that you can have is a program so the notion of an algorithm itself is an abstract object um is an abstract entity which requires a concrete representation in the form of the program and if a program is a sentence of a programming language then what you require really is a pro is a programming language and another  06  50  another alternative way of looking at the notion of a program these are the programming language is a as you might think of a program as a specification of a computation you might think of a program as a specification of a computation which means we have some notion of a what constitutes a primitive step of the computation and the program gives you a finite representation of possibly an infinite sequence of steps in a computation process yeah so the emphasis in all these cases is in the nature of a finitary specification ok i mean you should have a program is should be finite object by itself um a programming language itself is not a finite object because there are a infinite number of programs that are possible but each program itself is a finite object because it is just a sentence of the programming language yeah and then we might think of a programming language if you look upon a computation and the steps in the computation as the most basic feature then you might think of a programming language just as some notation for writing programs and in all this case we should emphasis the fact that this notation is important because um our notation is to give you a finitary specification of possibly an infinite object right so we might emphasis that this is actually a finitary specification yeah and these programs themselves as concrete objects are finitary but their effects could be infinitary right so so the moment you are trying to represent any infinitary object in a finite manner you require some you and it has to be machine understandable you require certain rules ok so um well so let us so let us look at this process of a of essentially giving a finitary representation to what you might consider infinitary objects ok so what kinds of infinitary objects are we normally concerned with where in the most general case an algorithm is what you want to represent in a program and an algorithm really in the most general case is a function from some domain to some codomain and a function need not necessarily be finitary because the domain could be infinite the codomain could be infinite and so we might think of an algorithm in general as computing either a function or a relation a method for computing some mathematical function or relation and these functions and relations could be infinitary yeah so and so let us so we are looking at infinitary objects basically mathematical functions relations can also be considered functions ok all relations could be considered functions so in general we will concentrate on trying to get finitary representations of infinitary objects and these infinitary objects are really functions so so you can think of the whole study of programming or computation as trying to compute or trying to give finite specifications of computation steps of abstract mathematical functions yeah so so however when you look at so so um so if you look at mathematics itself it has a fairly rigorous notation so you could think of mathematics itself as a sort of programming language except that it has one important drawback that is that it does not specify what are the primitive computation that are possible within the mathematical language ok so when you normally um when you when you are talking about an algorithm to compute some function what you have implicitly defined is a set of primitive functions or primitive computation steps in terms of which you are going to express this algorithm ok so one obvious case in which a lot of mathematics is not is does not fit into the um fit into the general framework of a programming language is this um for example the representation of infinite sets ok so if you look at i mean what ever you must have studied in school and so on if you look at infinite sets um well the standard thing in school is to say that you can either represent a set in a roaster form or in set builder form is that clear so roaster form just means enumerating a list of elements and a set builder form essentially means um giving an abstract giving a predicate which the elements of the set should satisfy ok so the main difference between the roaster form and the set builder form um the the the um the set builder form is also called a definition by abstraction ok so the main difference between the two really comes up for infinite sets so in the case of in finite sets what you normally do is um supposing you want to specify the set of even numbers so you open braces you write may be zero or if you don t include zero then you write two comma four comma six coma and then dot dot dot i mean that is so that is where the inadequacy of mathematical notation comes ok simply because you are not interested really in any underlying computation process as far as mathematics is concerned a large part of it is just the existence is more important than um than a computational method ok um and whereas the set builder notation or the definition by abstraction gives you a finitary specification so you can represent the set of even numbers through a notation which consists of braces which consists of a bound variable ok a definition a bound variable and a predicate in terms of the bound variable so um so a typical definition of let us say even numbers would would look like something like this  14  38  you would  19  25  say it is two x where x let us say belongs to the natural numbers right so here um if you look at x x is like a locally declared variable ok in fact this is a sort of declaration of x and this two x is a property that the element of this set should satisfy now here is a case of our finitary specification as of oppose to this infinitary specification right yeah in fact this is a finitary specification in more ways than one firstly this represents a logical predicate expressed in first order logic in a finite sentence of the first order logic ok and this you might consider this as a succinct finitary specification of essentially an infinitary object the even numbers whereas this is really open to many this is this is really ambiguous in the sense that it is not at all clear from this enumeration what should be the next one i mean you are using you are implicitly using human intelligence and human understanding to um or the human um human ability to perform induction to claim that the next number would be eight but i am not all sure that the next number should be eight there might be other patterns for example ok it might satisfy other predicates whereas this is what one might call an accurate succinct finitary representation using just the language of first order logic built up on a single predicate a single binary predicate on sets right this the binary predicate is this belongs to um yeah ok  noise  so so a lot of so what are um a lot of what we are going to do is also going to be related to the language of logic in some ways you will see the analogies between programming languages and logic as we go along the main motivations of logic were are are really of a slightly more abstract nature but programming language derive mainly from logic yeah in the sense that a language like first order logic does not allow you the freedom to write these dots and there is no such thing ok you have a method of construction of predicates which is always finitary you have rules of inferenceing logic which are always finitary or they might be infinitary like if you have like axiom schemas if you have rules like modese responds they are finitary representations again of infinitary objects ok further in a logical language with rule with axioms rules of inference there is implicitly understood that those rules and axioms those axioms and rules of inference are such that there exists an algorithm which given any instance of the hypothesis of this rules should be able to tell you whether the conclusion of the rule is a valid inference yeah so if you were to take a simple logical rule like  19  25  let us  39  20  take more respondents what you have is a i will write this so you have a predicate x you have a predicate x arrow y and you have y ok so this x and y are so this rule actually specifies a three tube um a pair of this form right where x and y are belong to i don t know let us say the language of first order logic which i will write l one um as suppose to proposition logic which i will write l not ok so l one is let us say first order logic so what you are saying is you take two sentences of first order logic and if they have this pattern that call one sentence x and the other sentence has the pattern x conditional y then you are able to infer y and you can not have all rules of inferencing logic are finitary there are also finitary specification yeah and something that is absolutely essential is that it is decidable by an algorithm whether a certain a step in the proof of a logical statement was derived by an application of a rule of influence on some preceding steps ok so which means that if you claim that you have some predicates um um you you have some predicates of the form um let us say a arrow b arrow not c where let me put brackets here and you have something like a arrow b and then you drive from these from these premises if you were to claim that by the use of more respondents you are you can infer not c then there has to be an algorithm which given these two as input will be able to tell you whether this is an instance of an application in this two definitions ok in this case the algorithm should actually tell you that it is not an application in this rule of inference ok in other case so it should be able to give you yes and no answers infinite time ok so all most programming languages that we will study will have will have a lot of their motivations actually derived from logic a large part of logic was actually concerned with the notion of how much you have mathematics is actually do able by a machine how many what kinds of proofs in what kinds of theorems in mathematics can be actually proved by algorithms by a machine whose basic primitive um by a machine whose primitive operations are that they are able to do pattern matching and substitution ok so this is an instance of doing pattern matching and substitution an inference rule is really an infinite object it is a relation of this kind is a finite representation a proof is a finite object a theorem is a finite a theorem it self is a finite is a sentence of logical language and it is a finite object representing possibly an infinite number of instances ok so the finitary nature of all these will actually influence the nature of our logic so for example you can not give axioms and rules of inference which are infinitary in a logical language ok um so um so everything should that is infinitary should have a finite representation there are of course infinitary objects which will have no finite representation they are clearly not gon na be part of our computational process ok so for example generating an infinite sequence of random numbers i am by random numbers i don t mean pseudo random numbers i mean pure random numbers is well it is not a computational process period we are so we are interested in those kinds of infinitary objects which some how have finitary representations yeah so like may be infinite sets represented as predicates um urinary binary ternary but some finitary with a finitary representation we are interested in infinitary computational processes which have finitary representations we are interested in programming languages which allow for finitary representation of inherently infinite objects infinitary objects yeah so um so so lets let us go ahead with i mean this much of philosophy is perhaps sufficient for the moment but it is important to realize that right from nineteen hundred when the mathematician david hilbert post this problem to the congress in mathematics the main emphasis of logicians and computer scientists has rather computer scientists came very late in the game but logicians mainly has been to tried to find to define the notion of an algorithm to define the notion of the computational process to be able to exactly define what is possible by a computational process what is not possible by a computational process every thing that is possible by a computational process should have finite representation and anything that is infinitary is not part of the computational process with some restrictions yeah so so if you so if you just come down from logic a bit then what we are looking at then we can look at a logical language itself as a mathematical object for example there exist only a finite number of rules for generating an infinite number of sentences of that language ok so you take take a language like first order logic you have only a finite set of formation rules which allow you to generate an infinite number of logical sentences ok not only and that finitary nature of the rules also gives you an algorithm to check given a string of symbols whether the um whether the string of symbols is a syntactically valid sentence of the logical language yeah so an important element of that logical language is that the generation process should be finite should be finitary there should be only a finite set of rules and there should be an algorithm which will check which can give which can clearly tell you whether a given sentence is a well formed sentence of the language alright and if you look at propositional logic it it it does not allow you to specify for example infinitary objects like for which we require i mean like i mean if you were to actually apply proportional logic to some some area of mathematics like say theory or number theory what what you see is that it allows you um it does not allow you to specify infinite sets easily ok or certain properties of infinite sets so very often so an extension of proportional logic which allows you to do this in a finitary way is the use of quantifiers and the extension of proportional logic to first order logic ok so you can for example specify the whole of set theory in first order logic the axioms of set theory the the predicates that are valid for all possible sets which do not i mean by set theory i mean by axiomatic set theory in the sense that we don t assume number or we don t assume any predefined set of objects the only notion is the notion of a set ok you generate all sets you generate numbers everything from the notion of an empty set and a single predicate binary predicate called belong stood yeah so and they have this formation rules and so on and so forth um so any way so we are interested essentially in capturing infinitary a processes within finitary languages yeah so so we might so lets so and the main way so the main difference between um so you can see a progression of ideas firstly there is pure mathematics which is platonic in nature in the sense that the notion of a computation itself is not important is not an important element of the formal discipline of mathematics then you have logic which actually gives you a loose notion of what is possible by a machine and what is not possible and allows you to specify infinitary objects in some finitary ways and lastly we have programming languages which specify with a great deal of accuracy exactly the primitive computational processes that you are allowed to use ok and so a programming language is also also has to satisfy all the constraints of logical language and in addition it should be consistent with what might be called the primitive computational processes ok for example one primitive computational process that you must all have studied in school is that of ruler and complex constructions yeah ruler and complex constructions ok yeah so one so there are only two computational primitive computational steps you are able to um draw lines with the ruler mark off segments you are able to use a compass to draw certain angles or to draw arbitrary angles ok so one impossible computation in this case is is there a algorithm using only this primitive concepts to trisect an arbitrary angle ok for example you are not allowed to use a protractors and so on you are not allowed to measure the angle you can only prove that an angle is of a certain measure for example if you draw a line perpendicular to another line um by with a construction proof it shows that it is perpendicular and then you bisect that then you can claim that the bisected angle is let us say forty five degrees but given just an arbitrary angle to raise from a point um to be able to trisect it with just these primitive tools for example is an impossible one um task ok so you might think of the algorithms of ruler and complex constructions um i mean the very the algorithms are i mean it is not it is a programming language with in which you know you have only these two computation processes if you like it is not it is not machine readable it is meant to be human readable so well so you write in a loose fashion but essentially you use only those computations which are possible within the domain of euclidean geometry which means you are not allowed to measure out angles yourself you are only allowed to prove there is certain angle has a certain measure you are not allowed to measure out lengths in terms of centimeters or meters or what ever you are only allowed to measure out an arbitrary unit and take multiples of that arbitrary unit you could bisect that arbitrary unit you could trisect that arbitrary unit of length measure ok and therefore calim that it is that that one that is actually one third of the unit you took but for example you cant claim that you have constructed one by pi of a unit of length unless you can prove that just by this process you are going to get something that is one by pi of a unit yeah anyway so um so let us so let us so our programming language has in grained in it a um a normal computational process is that we associate with a digital computer i mean that is not that is not the last word when you could have other computational process such as the ruler and compass constructions um you could have analog computers so on and so forth but we are interested  noise  excuse me we are interested primarily in the computational processes associated with um digital computers yeah so what we have our so our so as i said we could um we could look at um the we could look at even the machine language as a programming language but we are not really interested in machine language because it has it is a very simple sort of a language very difficult to get any program right i agree but it is the language itself is a very simple language ok and probably that is why it makes it so difficult to program and what we are interested in primarily are what are known as high level languages where you where the primitives of the computational process the primitives of computation or what you might say is the the machine that is made available so once you have implemented a language on a machine you could think of the mach you could think of that as a machine of that language supposing you have implemented it most of have done some programming in pascal so at that point when you are doing pascal programming you you are not really worried about the underlying machine language the underlying architecture you are not worried about anything as far as you are concerned what you have is a pascal machine ok so there is a level of abstraction at which you can consider that you have built to you it makes really no difference whether that pascal machine whether pascal is the hard ware machine language of that machine or is it or it is just some software language as far as you are concerned it is a pascal machine ok so it is important to realize that you you can actually take some bear machine and cover it up with layers and layers of software and think of just one abstract machine which gives you certain capabilities so so if you looked at the so if you look at the bear machine it gives you only the capabilities to manipulate switches to write programs in binary and so on if you look at a pascal machine it gives you no no extra computational power but it gives you the ability to look upon the whole thing as a single machine which allows let us say construction of complicated structured programs it allows various kinds of abstraction mechanisms procedures functions and it allows you to um to express um in term i mean it allows you express things differently ok from what the bare machine would have given you right so so let us look at why we should study programming languages because all the time we are looking at the construction of some virtual machine and what facilities we are not really interested in the facilities that the machine gives us we are interested in um  noise  we are interested in various um in what kinds of features are there in that machine in the case of a bare machine you are interested in its architecture now if you have a pascal machine its architecture is really the features of pascal ok if you have a list machine it s architecture is really the features of list right so we  39  20  will our study of programming languages is mainly to for example if you want to understand why certain features have been included in the programming language you want to understand for example how best those features could be used ok you this and if you you also want to understand how that language is implemented ok so that presumably you would be able to with all this understanding presumably you will be able to learn new languages easily may be you could design new languages that is more important and perhaps you would also be able to if you understand the underlying implementations may be you would be able incorporate new features in a programming language yeah so so  40  20  let us just look at languages we sort of classify what kinds of languages there are firstly we have this low level and low level languages some machine and assembly language which are not interest which are not of our interest i mean you will learn about them in some course on architecture or organization but what we are interested primarily are these high level languages ok of which we can think of three broad classifications one is a class of imperative languages in which most of our most of the last forty years is gone in since the first digital computers has gone in the design of the imperative languages then there are what are known as functional or applicative languages and then it is possible to use logic itself as a programming language yeah and then what you can do is you can actually mix up all these things and for example you can have impure functional languages ok um so an imperative language means that it it uses the notion of the command it uses the notion of the state to change a state so the commands change states that is what an imperative language would be ok so a functional language is one which allows you to program in something that is as close to um mathematics as possible um ok we will get into these notions um a bit more in detail later ok so so a broad classification of languages is just in terms of high level languages imperative functional logic um you could  42  27  also classify languages by features and by features in the sense that what is the most glaring feature in the language well so a large part of our languages are really what are what might be called sequential languages yeah so most of the languages that you have programmed in are purely sequential languages then you have what are these parallel languages which are very often languages meant for certain specialized architectures like um you um like you have a single instruction multiple data you execute things in parallel and there is a there are implicit methods to do things in a parallel fashion you have what are known as distributed languages in which they are not so in the case of a parallel language you assume that there is a  noise  you assume that there are so many processors which will all execute the same instructions in a synchronous lock step fashion ok so most of the vectors processors actually have a sequential languages vectorized or made parallel like in the case fortan ninety vector processing fortan and so on and so forth yeah distributed languages are those in which you actually assume that the different units of a program are going to lye geographically distributed across let us say a network and they have to some how co operate to achieve may be some common task right then you have this in both this cases in both we might we might loosely assume in both the in both parallel and distributed languages that the notion of a process of a computational process into which a program is split is inherently or intimately related to really the computational power to the number of units of computation that number of units of let us say cpus that you have so the notion of a process and a processor are really the same you are writing you are riding one process per processor in both these cases yeah in the case of concurrent languages what you are you are basically taking the notion of the process to be an loose entity completely different from does not necessarily have to be mapped on to the existing processes the notion of the process gets delinked from the notion of the processor then you have other kinds of languages those whose primary feature is that of modules separate compilation and more recently you have what what might be called object oriented languages these these add extra features on top of the existing languages usually but there is something fundamental about the new feature that they introduce yeah so um so when  46  48  we look at so lets so lets just quickly go through for example some of these languages so you would if you were to take the history of programming languages you would find that there is a certain chronological dependence right so the first high level languages so to speak were fortan which is mainly meant for scientific computation and then cobol which is meant for business it was more verbose it actually used full english sentences to represent computations it made the first division distinction between data and program and was meant to use a large amount of data and do very low processing and it was io bound programs whereas fortan was meant for minimal io and maximum computation yeah and these languages gave rise to one important class of languages called the algol like languages which came from the report the algol sixty report fortan also had its offshoots in basic then there were these among the algol like languages among the algol like languages you have pascal pl one simula pl one was an attempt at a unified language for both scientific and business commercial processing and from pascal you get extra features some thing like modula and ada from simula you have got these object oriented languages starting from small talk eighty ok and all these and somewhere in a parallel stream which is marked by this orange you have bcpl and c actually bcpl went was a transformation of a language called b which itself was a transformation of a language called a and the c was derived from bcpl by modification yeah and then one small talk eighty came up and c was there an object oriented ness became a bigbuzz word you had c plus plus yeah which is not here ok um so so so that is briefly what we are um you might say our pedigree of languages is mainly oh we also have this functional languages right so let  48  53  us let us look at functional languages so you have in in apart from this imperative languages you had basically the first functional language was lisp from which we derived various versions maclisp scheme commonlisp um maclisp can um maclisp and commonlisp are really impure versions of lisp when we understand functionality i will we will come to what they mean impure versions but many of you have probably studied scheme ok it is a cleaned up version of lisp and those imparrallel with lisp which is meant for lisp processing it was also a language designed in the sixties called the snobol which was meant for string processing which allowed efficient pattern matching constructs to be programmed and these have actually yielded along with the emphasis on tied checking to a language called ml which came up in the eighties and actually all these languages lisp and ml were all inspired by the what is known as lambda calculus which we will study which is the basis of all functional languages ok transcriptor  v.srinivasa rajkumar educational technology i.i.t.delhi presents a video course on programming languages by dr.s.arun kumar deptt  of comp.sc & engg i.i.t delhi lecture 2 syntax welcome to the second lecture if you were to look at let us just go briefly through what we did in the last lecture so as i said we are mainly concerned with what might be called high level programming languages of which i said there are three kinds imperative functional they are the most important and logic itself can be used as a language but our main concern is with this imperative and functional languages um they are also similar so in certain respects so imperative language is a what might be called state based languages where state updation is the main action um whereas functional languages are really value based languages much closer to our mathematical languages in the notion of variables in functional languages is very much like the notion of variables in mathematics whereas the notion of the variables in the imperative languages is more like quantities in physics which can change over in time quantities like acceleration velocity um and um so so state based languages means that there is a state change along along let us say something like a trimaxes though unlike physics we are not talking of continuous time we are talking of discrete time in the case of functional languages the notion of the variable is really a variable in mathematics which means that a variable just is a name for a value and it can not change in time during the execution of a program any way  02  24  so um the if you so let me just briefly look at so if you look at the history of languages most of the work on the high level languages is really concentrated on the imperative languages and there are hundreds and hundreds of programming languages so firstly it is impossible to study all of them ok and if you look at the history you will find that a large portion of the time during the fifties and sixties a large portion of time was concentrated on what might be called the basic features so which means these these represent the basic control structures the exploration of the basic control structures in the basic data structures in order to obtain clean readable programs efficiently implement able programs um efficient running programs and so on but later once these things were fixed in the seventies and eighties most of the exploration of programming languages was in terms of what might be called features like if you were to take modula it is most important feature it is just a extension of pascal most important feature is that of a module if you were to take ada it combines the module features of modula and adds more features like concurrency as an important feature exception handling generics or polymorphism um if you were to take um so similarly clu is a module based language very much like modula but with a different syntax but they are all extensions of pascal in that in the sense the basic contrl structure remain the same and then you extend the language if you look at these the basic control structures in these languages could be different too they are not very similar except that these arrow mark denote the decendency in terms of similarity of even the basic control structures so from simula let us say um even though the small talk eighty control structures or syntax are different from that of simula the basic extensional feature comes from a new feature of simula which was extended and that was the notion of class or objects ok so these  05  00  so if so what we mean by features are these kinds of features this is the language of sequential most of the languages in the early sixties were sequential whether they are concurrent whether they are modular whether they are distributed whether they are parallel whether they are object oriented and so on clear so  noise  so and nowadays biolarge their exploration is mostly in terms of new features what kind of new features and what i have listed as the various kinds of features is let us say the current state of art large amount of work is into trying to make them more efficient trying to make them more readable comprehend able and so on ok so um the basic functional languages also have this feature that the early functional languages where all an exploration of very basic data structures and controls and control structures then languages like ml caml actually signify the addition of new features in fact the syntax of ml expressions and so on is very much is much closer to that of pascal than let us say of lisp but philosiscally it is a more lisp like language because it is a applicative or a functional language it is not a state based language and it goes far beyond lisp in the sense that there is a there are new features like the introduction of modules introduction of exceptional handling the introduction of very powerful data abstraction mechanisms and a type checking ok so lisp is lisp has no type checking at all right so let us look at in we will so we will first study the basic features of languages and so let us look at what constitutes um may be what are the issues in in some thing like language design if you  07  20  were to design a language what kinds of issues would you really have to study so once um one very major issue that the implementation that the language pascal has taught us is that it is a good idea it is a very good idea for a language to have a simple clear and a small set of unified unified primitives for expressing basically all your algorithms and data structures one of the nicest thing about pascal or in general about what are know as algol sixty like languages is that most algorithms are written in in some crude variations of the dialect of pascal or algol system so the nice thing about pascal is that it is a small language easily learnable for example which is why most people are taught initially um and so one issue could be that you should start with a small language with a small set of primitive operations over which you can build yeah and secondly the next issue would be that you should have an absolutely clear syntax there should be no ambiguity and you should be able to get highly readable programs it is very important um to have readability that means by readability it means that i have to be able to read the source code of the program like a book and and a very good reason for that is that no piece of software that you write is ever permanently fixed what hsppens is that well most pieces of software have bugs in them so bugs might be detected years and years after their software is been commissioned so what and years and years after means that there might there must be so there must be somebody else if a bug is detected who should be able to read the source code and be able to modify it and in order able to be read the source code he should be able to understand the algorithms of the source code contained so in fact efficiency and such consideration come much later readability is the most important thing because it includes maintainability of the software yeah so normally the normally the person who write the person or persons who have written this software are not likely to be present um to be to maintain the software so the software in general has to be maintained by somebody else so which means that the source code must be readable so that is the that is the first thing the second thing is that over the years the software actually has more and more users use their piece of software more and more users feel their need for its extension so you have to be able to extend the software may be by adding new features by adding some more conveniences what ever may be the reason so part of the maintainability of a piece of software is not just the detection and correction of bugs but also the extensibility of the software as years go by and more and more needs are felt it is necessary for somebody whose usually not the original programmer or the original team that wrote the software to be able to extend and for all that readability is most important yeah and secondly that s the language should provide what is know as a support for abstraction the basic abstraction mechanisms you are already aware of the control abstractions or things like procedures functions in pascal  noise  loops loop statements there are various kinds of control abstractions there are also data abstractions like for example the most primitive kind of data abstraction that pascal provides are the record structures of course the arrays which almost all languages even the early languages have so arrays are one data abstraction that means you think of a whole set of a whole sequence of elements as a single unit and you try to as a single logical unit um records variant records and of course what what what has been felt in um later years is that you should have abstraction data abstraction which allows you not just the ability to take sets of data together as a single unit but also to operate on them as a single unit so combinations of operations and data abstractions in in a module basis ok or further abstractions like you might require i mean um for example languages like ml ada provide the notion of a generic or polymorphism where you can use types as themselves as variables and change types and instantiate the same kinds of algorithms for example stacks it doesn t matter whether you are talking of a stack of integers a stack of characters a stack of reals a stack of um some complicated data element may be stack of some record of things or stacks of arrays of things the basic operation on stacks are like pop push checking for emptiness and so on they should all be available in one form and it should not be necessary for me to repeat the code depending on the type of the element right so that is an abstraction where the basic element of the stack itself is a variable which can be instantiated to a different type each time and i use the same piece of code carefully written verified may be thoroughly tested and instantiate the types so that i get different kinds of stacks with the same kinds of operations right so the support for abstraction is an important modern language design issue and um other kinds of issues that have come up or which are becoming more and more reasonable is  14  25  one is that you should there should be able to you should be able to verify your programs so the language should also provide support for verification provability of programs not necessarily machine based provability but possibly hand based provability or a mixture or a user interactive provability of programs and most and something that was felt even in the sixties where a lot of time and effort was expended in the case of fortan and cobol compilers was portability ok nowadays so when i say portability what means what it means is that the language design should be such the language should be such that it is oriented towards an end user and it is not architecture or machine independent or machine specific simply because a certain machine as a certain kind of assembly instruction it does not mean that you make that available in the language other machines may not have that so you should um machine independence means that you should be able to provide as far as possible an abstract form which is not related to the machine ok which uses only the basic instruction set  noise  which will be available in all machines if you ensure that you language is really architecture or machine independent in the sense that your main concerns have been with are with the users um convenience the abstractions required for the user and are not specific to particular machine instructions sets of particular architecture like you might have register based architectures or stack based architectures if if you can design the language in such a fashion that it is not architecture specific or machine specific then what it means is that i can move the entire language to another machine with a different architecture and with the minimum amount of effort there are certain archi certain machine specific details which still have to be changed when i move an entire language implementation from one machine to another or from one architecture to another but the whole idea of the language design or the design of its implementation should be that the amount of changes have to be made should be minimum so we will look at some reason for that so in addition and then there are things like you should be able to easily implement i mean you can not compromise as everything at the alter of portability um ease of the implementation the availability of ready algorithms for implementing the language um should also i mean should be a consideration one of them i mean ease of the implementation was perhaps the most important reason um for this and the fact that it used very low level primitives perhaps the most important reason for the success of c the c language for example um and well efficiency is another important reason c c programs run very fast and they are very efficient um and something that some thing that if you are going if you have a language and you want it to be generally acceptable then what it should have is a clear definition of what its constructs do so that if it has to be widely acceptable there might be many different implementers trying to implement it on various machines and only if there is a common clear syntax common clear semantics or the specification  18  45  of the effects of each language construct each construct in the language only then you can you can expect to get wide applicability ok so we will get to the notion of semantics later then of course there has to be i mean it has to be run time efficient by run time efficient i mean that the programs have to run efficiently this is the efficiency of the implementation which means very often it means compile time efficiency how fast can you compile programs written in the language whereas this runtime efficient is that it should have an excellent runtime support and the program should run as fast as the programs written in the language should run fast yeah ease of maintenance here i am talking about the maintenance of programs as well as the maintenance of the language by an implementation maintenance of the implementation of the language there might be bugs in the language implementation and um  noise   19  46  well fast  19  48  compilation translation and support for extensibility which is in fact the most important reason why for example pascal is used as a basic support as a the language of pascal is used as the basic language from which you add onto which you add new features to get newer languages this is one controversial feature that is the support for subsets this used to something that most programming languages books used to specify that every language should support subsets of the language in the sense that it should be necessary for me to use only a smaller set of operations or features of the language than is actually necessary and you should by support for subsets what i am saying is it should be possible to take you should be able to divide up the language so that there is a small kernel and larger large sort of extensions ok so so that all programs written for the small kernel any way run on all machines regardless of the subset supported ok however the language of ada for example the eighties has clearly specified that there should be no support for subsets and their reasons is that then it affects the portability of programs written in the language yeah so so support for subsets is a controversial thing which it is not at all clear whether it is desirable especially in the as in the case i mean the most important language for embedded systems you know real time things with systems that control sensors various kinds of hardware ballistic missiles and so on it affects it could affect their portability because you would have some feature which which is used in some in some implementation and then you move the program to another implementation which does not support that feature like your programs don t run right so the portability of actually programs written in the language gets affected if you allow arbitrary subsets yeah  noise  ok so if you look at all these features um and so finally what we can do is and then there are hundred and hundreds of programming languages it is impossible to study all of them what is more it is not necessary so what we would like to do is to divide up the study of programming languages into a few small parts so what we will do is we will look at  22  50  so what is a theory of programming languages contain yeah so  noise  a general theory of programming languages is based on three things what might be the most important thing is or the most basic thing is what is known as the syntax of the language ok so a programming language is really like a very highly simplified natural language it has a certain syntax right so which means that it has a certain what might be called the grammar certain things can occur only in certain ways you can not you can not arbitrarily take um you can not arbitrarily form sentences of the language as i said a program written in that language is just a what might be called as the sentence of the language and in a natural language you take a sentence of a natural languageit has various parts so for example in almost all languages all natural languages one thing that any full sentence should have is a predicate that is a it has a a syntactic category called predicates ok which in general may not be words a predicate could be a clause or a phrase um and so it might have a subject too in addition it might have a object so if you take a sentence in natural language so if i say run that is a complete sentence because it has a predicate no complete sentence in the natural language is without a predicate ok so run is a complete sentence it has a predicate optionally um it might have a subject may be also a object clause so and the subject clause is have to be of a grammatical form called they must be subject well subject phrases may be noun phrases which means they might have nouns qualified by object um adjectives may be an article and so on and so forth yeah so what you can do is you can divide up any grammatically correct sentence in a natural language into firstly various clauses each clause may be into various phrases and all these phrases have certain grammatical properties yeah in very much a similar manner in a greatly simplified form um what might called the parts of speech of a um of a program yeah in so every programming language has a certain grammar there are which specifies various parts of speech it has a certain what might be called vocabulary and it is possible to take a sentence of this programming language and parse it we will get into the meaning of these things more clearly later where the similarities with natural language after all a lot of the theory of the syntax was actually inspired by natural languages where the construction of artificial languages which did not have the which did not have a lot of problems in the natural language yeah um the next thing is that is what might be called the semantics very often this semantics of a programming language is really specified by its reference manual so if you in your i mean in your study of pascal programming um one of the important references would be the iso standard pascal reference manual by janson edward ok so that reference manual really specifies for each syntactic entity of this of the language what is the effect to be expected by executing that syntactic entity so in other words what we are saying is for each language construct what are the meaning associated with it yeah so the notion of meanings is something we have to look at unlike natural language the notion of meaning is something that can be specified um mathematically in a machine independent fashion so when we are talking about this semantics of this programming language we are talking about a pure meaning associated with the language regardless of any machine on which the language is implemented in general we are talking about meanings in a abstract settings in the sense that you assume if you like for practical purposes that you have no restrictions on the word lengths you have no restrictions on memory you have no restrictions on computational power except that you can only do a finite number of operations at any instance right so you assume a sort of so the semantic the programming language itself can be thought of as a mathematical entity quite independent from all its implementations and in general when you think of it that way then what you are saying is you are thinking of some kind of an ideal machine which has no restrictions except one restriction that at any instant of time only a finite number of operations can be performed i mean you can not do an infinite number of operations at a instant of time so if you consider such an ideal machine and um and then you are really looking at the programming language as a mathematical entity in some ideal environment ok and the meanings of that of the constructs of the language in that ideal environment form what might be called um actual reference manual so if you were to go through the reference manual of pascal let us say since that is you all have studied there are certain most of the reference manual is independent of any machine there are certain specific machine paragraphs or what are known as implementation dependent features but if you look at the language itself it is capable of looking at the language as an entity devoid of any machine yeah so most of the times so in  30  13  general the semantics follow the syntax the syntax has a certain structure which will come to and the semantics so the syntax has got some what might be called some basic elements and some compound forming operations they are connectives compound um connective which allow you to form um compound that is the sentences from simpler once um so the meanings in general should be such that you gave the meanings of the basic elements and then you give the meanings of the connectives in terms of the basic elements so very often what it should be necessary is since a programming language is really a finitary object which allows you the construction of the infinite set of programs ok it is not possible to unless you have this basic discipline that you express the effects of connectives in terms of the effect of the basic elements of the language it is not in general possible to predict the behavior of the language of a program written in the language unless you follow this discipline ok so basic feature of any kind of semantics is that it should allow for the derivation of the meanings of an infinite number of programs which means the only way to derive the meaning of the program should be from the meanings of its finitary elements so you have certain basic elements from which complex elements are formed so the meaning of the complex element should be derivable in terms of the meanings of the basic elements and the meanings associated with the connectives which formed a complex element from the basic elements ok so what it means is that semantics is going to b intimately related to the syntax the syntax is really a way a finitary way of specifying an infinite set of allowable objects ok the infinite set of allowable objects are the programs of the language or the sentences of the language and the syntax gives you finitary mechanism for specifying all possible allowable programs very much like a set theory notation right the set builder notation which allows you to give a finitary specification of an infinitary set of an infinite set and so the only thing that is really analyzable for a arbitrary program is its structure in terms of its syntax the finitary specification so the meaning of the program has to be derived from there from its finitary specification as to be expressed in terms of its finitary specification yeah so lastly so this is a semantics of a language is in a sort of an ideal environment don t worry about machine constraints don t worry about architecture they are all we don t want to worry about them because for one important reason i mean the programs have to be the programming language has to be portable ok don t worry about word lengths don t worry about limits don t worry about memory constraints assume infinite amount of memory available so then what happens is that you get  33  58  into what are known as pragmatic considerations yeah once so most of these implementation dependant features are really pragmatics for example the pascal reference manual does not tell you how to associate a disc file with a file variable in side the program that is one example of an implementation dependant feature which will vary depending upon the operating system interface you have yeah then there are various simple implementation um so the pragmatics firstly um firstly involves really all the algorithms that are going to be used for the implementation of the language all kinds of machine and architectural constraints for example this the maxint the maximum integer allowable in a pascal program is a typically implementation dependant feature because it really depends upon word length or byte length or um byte length or whether it uses two bytes for representing a integers and so on and so forth right so the value of the maxint can very from machine to machine the amount of memory that is available for a program can vary whether it is a stack based machine or a register based machine um those things are implement um those things are implementation dependent features ok so normally what we would do is even in our implementation in order to make the language portable we will separate out the basic algorithms of implementation from the architectural specific nature of the implementation when we do a when we read through a small compiler you will see this happening so there are certain basic algorithms which are really machine independent and then there are some machine dependant features the actual code so then you have things like the os interface what is the nature of the input and output that is file based terminal based sensor based what ever signal based and the os interface also includes the file server how the language has to interact with the file saver um with how the language has to interact in general with the directory service of the machine and so on and so forth and lastly what what is to be done about errors and by errors i mean errors written by errors introduced by users in their programs so errors could be of a syntactic nature errors could be of runtime nature what is to be done about these errors as a blanket um there is a blanket policy could be just is just that you abort but you know that doesn t help anybody in particular what is the nature of the error reporting is there some error correction that can be done is there some way of recovering from errors so that as soon as the first error comes you just throw out the program no it is if you can at least you should be able to point out all the errors in the program or all possible things that are errors so that the user gets to know all the errors at one time it reduces the amount of compilation effort and similarly you cant do that at run time may be but at least there should be some decent error reporting mechanism some error handling mechanism so that is one thing but errors is a very  noise  dicey object it is very dicey policy and different languages have taken different attitudes different implementations take different attitudes um so so it is a very very pragmatic feature yeah so i will just briefly go through the notion of syntax and then may be i will stop so these are the three issues and it is better to study all three issues sort of separately the semantic and pragmatic issues will closely depend upon the syntax and it is preferable that they depend upon the syntax so let us look at what constitutes syntax yeah so a syntax  39  20  has to do with a form or a physical representation of possibly an abstract object yeah so if you look at numbers um numbers really are not very physical at least um the twentieth century attitude towards numbers is that they are a conception of the mind and they are not physical what they do have is a physical representation in the form of numerals ok so what you write out and think of as a number is actually a numeral right so while you have various ways of representing the same number let us say the number one hundred and twenty six um so you can have what what is known as the positional representation which is what we normally use um and well this this is one hundred and twenty six written in hexadecimal and i hope it is correct yeah um so you um so in the basic form all these three representations of the number one hundred and twenty six are really the same ok the roman representation differs from the theonagri representation in the sense that the characters used are different what might be called the basic alphabet for the positional representation of numbers is um is different in the two it is just that the character set is different um this the character set is also different it is a positional representation which uses a base the character said therefore is also different but that is incidental yeah but in a in a basic forms all these are unified by the fact that they are all positional representations yeah positional representations i hope everybody understands that um right you go in units tens hundreds here you go in units sixteen s sixty four s and so on right um then of course you have the roman numerals which is non positional which has well firstly it has a different alphabet you could represent the same um roman same number one hundred and twenty six with different character sets but there is something unifying about um about this there is some thing fundamental difference about this representation from this from any of this yeah so and what is it fundamentally different if you disregard the change in character set what makes these two the same what makes these three the same and what makes these three different from these two ok what make that what that do what makes these two classes different is the grammar of the numeral of the language used for representing them yeah in both these cases the grammar is exactly identical the character sets are different and by enlarge the grammar of all these three is the same except for the character sets but the grammar of this is different from the grammar of this in the sense the form of representation is fundamentally different how you represent compound forms from simpler forms is different between the roman and arabic case ok so what is that grammar is what we have to study yeah so so let us look at the so let us look at this in a more general setting of the programming language so we might think of every programming language as containing of as having a vocabulary ok  43  08  a vocabulary is well what you might call a complete dictionary of words of the language and a word of a language is formed from a character set from a fixed character set and well you identify certain strings of characters as words as allowable words as part of the vocabulary of the language so a complete dictionary of the language is what constitutes that states vocabulary so if you look at i mean what i am saying is so so if you take languages for example natural languages like konkani or sindi what they have is different character sets and but the same collection of words ok sindi for example is written by different people some people write in devanagri some people write it in the urdu script the arabic script ok but the collection of the words is the same so a person who knows devanagri can communicate with the person who doesn t know devanagri but knows the urdu script by speech because the words are the same but you can not communicate by letter ok so there is a certain fixed collection of words whose actual form might depend on the character set right um but given the words there are what are known as formation rules yeah so given a vocabulary there are ways of combining words of the language to form sentences of the language and there is a finite set of formation rules which are called the formation rules are called productions which allow you to generate all possible sentences in the language yeah so let us quickly go through one example um so we will we will we will not worry too much about this character set because i think that s so for a programming language for example the character set really depends upon the kind of codes you use nowadays most of the time we use ascii codes but then you know we have eight bit ascii s and pcs and seven bit ascii s on main frame the character sets are different there are all these kinds of differences ok but let us disregard them um and let us just look at so let us just look at what just constitutes the a grammar so so i would say grammar grammar is  46  00  really a four tuple of objects ok where there is a set n which are called which is called the set of non terminals and this set of non terminals really specifies various kinds of grammatical categories of the language like you might say the parts of speech noun phrase verb phrase adjectival phrase noun clause subject clauses subject phrases object clauses predicates and so on ok so this set n consists of the basic grammatical categories all these all these sets are finite ok then the terminal t is the set of what are known as terminal symbols or terminal words and t is the complete vocabulary of the language ok so and p is the collection of formation rules or what are known as productions and s is what is called as the start symbol but s really is represents a grammatical category in n called as sentence yeah so so so it so the basic element of the language is what constitutes a sentence right so let me just quickly go through um ok so so  47  56  here is a simple grammar specifying boolean expressions so there is a start symbol s every grammar should have a start symbol s and some grammatical categories the grammatical categories that i have chosen are a v and c which actually stand for an add boolean expression or a or boolean expression or a complement expression ok the vocabulary of this language consists of all possible boolean variables that we might take the two left and right parenthesis and the three connectors and not and or ok these are the productions from the start symbol any boolean variable is a boolean expression that is so you take any b belonging to this set of boolean variables that s that one itself a boolean expression so the sentence of this languages are boolean expressions actually they are more than fully parenthesized boolean expressions ok so any a sentence is either a complement of a boolean expression or the or of the two boolean expression or the and of the two expressions an and clause if an and expression is one of the form which consists of two boolean expressions enclosed in parenthesis and separated by the word and ok an or boolean expression similarly is two boolean expression enclosed in parenthesis and separated by an or and not is similarly this yeah so these productions are really replacement rules so whenever you find a s you can replace it either by a a or v or a c or one of the boolean variables whenever you find an a you have to replace it by this there is no other way you can replace a by anything else similarly whenever you see a c you have to replace it by this and so here is a simple sentence generation you start  50  17  from s and one of the possibility is that you can you can replace s by c ok whatever i am replacing i have circled in orange so you can replace s by c c has to be replaced by something of this form by this and now i am replacing this s leaving everything else intact so i have chosen here to replace s by a and once i have replaced once i have got a a there the only possibility is to replace it by something of this form s and s i have chosen to replace this s rather than this s first and i have chosen it to replace it by a boolean variable so let us assume that there are only two variables b one and b two and i proceed in this fashion and lastly i get a complete sentence of the language which consists of only the terminal symbols and this is a sentence of this other language generated by this grammar yeah so we  51  30  talk of a language as being generated from a grammar as a set of all possible sentences that may be generated from the start symbol s important warnings  51  40  are that the set of non terminals and the set of terminal symbols are disjoint and a production is really a replacement it replaces a non terminal by a string consisting of terminals or non terminals yeah and sentence is just a string of terminal symbols generated from the start symbol yeah i think i will stop here now um transcriptor  v.srinivasa rajkumar educational technology i.i.t.delhi presents a video course on programming languages by dr.s.arun kumar deptt  of comp.sc & engg i.i.t delhi lecture 2 syntax welcome to the second lecture if you were to look at let us just go briefly through what we did in the last lecture so as i said we are mainly concerned with what might be called high level programming languages of which i said there are three kinds imperative functional they are the most important and logic itself can be used as a language but our main concern is with this imperative and functional languages um they are also similar so in certain respects so imperative language is a what might be called state based languages where state updation is the main action um whereas functional languages are really value based languages much closer to our mathematical languages in the notion of variables in functional languages is very much like the notion of variables in mathematics whereas the notion of the variables in the imperative languages is more like quantities in physics which can change over in time quantities like acceleration velocity um and um so so state based languages means that there is a state change along along let us say something like a trimaxes though unlike physics we are not talking of continuous time we are talking of discrete time in the case of functional languages the notion of the variable is really a variable in mathematics which means that a variable just is a name for a value and it can not change in time during the execution of a program any way  02  24  so um the if you so let me just briefly look at so if you look at the history of languages most of the work on the high level languages is really concentrated on the imperative languages and there are hundreds and hundreds of programming languages so firstly it is impossible to study all of them ok and if you look at the history you will find that a large portion of the time during the fifties and sixties a large portion of time was concentrated on what might be called the basic features so which means these these represent the basic control structures the exploration of the basic control structures in the basic data structures in order to obtain clean readable programs efficiently implement able programs um efficient running programs and so on but later once these things were fixed in the seventies and eighties most of the exploration of programming languages was in terms of what might be called features like if you were to take modula it is most important feature it is just a extension of pascal most important feature is that of a module if you were to take ada it combines the module features of modula and adds more features like concurrency as an important feature exception handling generics or polymorphism um if you were to take um so similarly clu is a module based language very much like modula but with a different syntax but they are all extensions of pascal in that in the sense the basic contrl structure remain the same and then you extend the language if you look at these the basic control structures in these languages could be different too they are not very similar except that these arrow mark denote the decendency in terms of similarity of even the basic control structures so from simula let us say um even though the small talk eighty control structures or syntax are different from that of simula the basic extensional feature comes from a new feature of simula which was extended and that was the notion of class or objects ok so these  05  00  so if so what we mean by features are these kinds of features this is the language of sequential most of the languages in the early sixties were sequential whether they are concurrent whether they are modular whether they are distributed whether they are parallel whether they are object oriented and so on clear so  noise  so and nowadays biolarge their exploration is mostly in terms of new features what kind of new features and what i have listed as the various kinds of features is let us say the current state of art large amount of work is into trying to make them more efficient trying to make them more readable comprehend able and so on ok so um the basic functional languages also have this feature that the early functional languages where all an exploration of very basic data structures and controls and control structures then languages like ml caml actually signify the addition of new features in fact the syntax of ml expressions and so on is very much is much closer to that of pascal than let us say of lisp but philosiscally it is a more lisp like language because it is a applicative or a functional language it is not a state based language and it goes far beyond lisp in the sense that there is a there are new features like the introduction of modules introduction of exceptional handling the introduction of very powerful data abstraction mechanisms and a type checking ok so lisp is lisp has no type checking at all right so let us look at in we will so we will first study the basic features of languages and so let us look at what constitutes um may be what are the issues in in some thing like language design if you  07  20  were to design a language what kinds of issues would you really have to study so once um one very major issue that the implementation that the language pascal has taught us is that it is a good idea it is a very good idea for a language to have a simple clear and a small set of unified unified primitives for expressing basically all your algorithms and data structures one of the nicest thing about pascal or in general about what are know as algol sixty like languages is that most algorithms are written in in some crude variations of the dialect of pascal or algol system so the nice thing about pascal is that it is a small language easily learnable for example which is why most people are taught initially um and so one issue could be that you should start with a small language with a small set of primitive operations over which you can build yeah and secondly the next issue would be that you should have an absolutely clear syntax there should be no ambiguity and you should be able to get highly readable programs it is very important um to have readability that means by readability it means that i have to be able to read the source code of the program like a book and and a very good reason for that is that no piece of software that you write is ever permanently fixed what hsppens is that well most pieces of software have bugs in them so bugs might be detected years and years after their software is been commissioned so what and years and years after means that there might there must be so there must be somebody else if a bug is detected who should be able to read the source code and be able to modify it and in order able to be read the source code he should be able to understand the algorithms of the source code contained so in fact efficiency and such consideration come much later readability is the most important thing because it includes maintainability of the software yeah so normally the normally the person who write the person or persons who have written this software are not likely to be present um to be to maintain the software so the software in general has to be maintained by somebody else so which means that the source code must be readable so that is the that is the first thing the second thing is that over the years the software actually has more and more users use their piece of software more and more users feel their need for its extension so you have to be able to extend the software may be by adding new features by adding some more conveniences what ever may be the reason so part of the maintainability of a piece of software is not just the detection and correction of bugs but also the extensibility of the software as years go by and more and more needs are felt it is necessary for somebody whose usually not the original programmer or the original team that wrote the software to be able to extend and for all that readability is most important yeah and secondly that s the language should provide what is know as a support for abstraction the basic abstraction mechanisms you are already aware of the control abstractions or things like procedures functions in pascal  noise  loops loop statements there are various kinds of control abstractions there are also data abstractions like for example the most primitive kind of data abstraction that pascal provides are the record structures of course the arrays which almost all languages even the early languages have so arrays are one data abstraction that means you think of a whole set of a whole sequence of elements as a single unit and you try to as a single logical unit um records variant records and of course what what what has been felt in um later years is that you should have abstraction data abstraction which allows you not just the ability to take sets of data together as a single unit but also to operate on them as a single unit so combinations of operations and data abstractions in in a module basis ok or further abstractions like you might require i mean um for example languages like ml ada provide the notion of a generic or polymorphism where you can use types as themselves as variables and change types and instantiate the same kinds of algorithms for example stacks it doesn t matter whether you are talking of a stack of integers a stack of characters a stack of reals a stack of um some complicated data element may be stack of some record of things or stacks of arrays of things the basic operation on stacks are like pop push checking for emptiness and so on they should all be available in one form and it should not be necessary for me to repeat the code depending on the type of the element right so that is an abstraction where the basic element of the stack itself is a variable which can be instantiated to a different type each time and i use the same piece of code carefully written verified may be thoroughly tested and instantiate the types so that i get different kinds of stacks with the same kinds of operations right so the support for abstraction is an important modern language design issue and um other kinds of issues that have come up or which are becoming more and more reasonable is  14  25  one is that you should there should be able to you should be able to verify your programs so the language should also provide support for verification provability of programs not necessarily machine based provability but possibly hand based provability or a mixture or a user interactive provability of programs and most and something that was felt even in the sixties where a lot of time and effort was expended in the case of fortan and cobol compilers was portability ok nowadays so when i say portability what means what it means is that the language design should be such the language should be such that it is oriented towards an end user and it is not architecture or machine independent or machine specific simply because a certain machine as a certain kind of assembly instruction it does not mean that you make that available in the language other machines may not have that so you should um machine independence means that you should be able to provide as far as possible an abstract form which is not related to the machine ok which uses only the basic instruction set  noise  which will be available in all machines if you ensure that you language is really architecture or machine independent in the sense that your main concerns have been with are with the users um convenience the abstractions required for the user and are not specific to particular machine instructions sets of particular architecture like you might have register based architectures or stack based architectures if if you can design the language in such a fashion that it is not architecture specific or machine specific then what it means is that i can move the entire language to another machine with a different architecture and with the minimum amount of effort there are certain archi certain machine specific details which still have to be changed when i move an entire language implementation from one machine to another or from one architecture to another but the whole idea of the language design or the design of its implementation should be that the amount of changes have to be made should be minimum so we will look at some reason for that so in addition and then there are things like you should be able to easily implement i mean you can not compromise as everything at the alter of portability um ease of the implementation the availability of ready algorithms for implementing the language um should also i mean should be a consideration one of them i mean ease of the implementation was perhaps the most important reason um for this and the fact that it used very low level primitives perhaps the most important reason for the success of c the c language for example um and well efficiency is another important reason c c programs run very fast and they are very efficient um and something that some thing that if you are going if you have a language and you want it to be generally acceptable then what it should have is a clear definition of what its constructs do so that if it has to be widely acceptable there might be many different implementers trying to implement it on various machines and only if there is a common clear syntax common clear semantics or the specification  18  45  of the effects of each language construct each construct in the language only then you can you can expect to get wide applicability ok so we will get to the notion of semantics later then of course there has to be i mean it has to be run time efficient by run time efficient i mean that the programs have to run efficiently this is the efficiency of the implementation which means very often it means compile time efficiency how fast can you compile programs written in the language whereas this runtime efficient is that it should have an excellent runtime support and the program should run as fast as the programs written in the language should run fast yeah ease of maintenance here i am talking about the maintenance of programs as well as the maintenance of the language by an implementation maintenance of the implementation of the language there might be bugs in the language implementation and um  noise   19  46  well fast  19  48  compilation translation and support for extensibility which is in fact the most important reason why for example pascal is used as a basic support as a the language of pascal is used as the basic language from which you add onto which you add new features to get newer languages this is one controversial feature that is the support for subsets this used to something that most programming languages books used to specify that every language should support subsets of the language in the sense that it should be necessary for me to use only a smaller set of operations or features of the language than is actually necessary and you should by support for subsets what i am saying is it should be possible to take you should be able to divide up the language so that there is a small kernel and larger large sort of extensions ok so so that all programs written for the small kernel any way run on all machines regardless of the subset supported ok however the language of ada for example the eighties has clearly specified that there should be no support for subsets and their reasons is that then it affects the portability of programs written in the language yeah so so support for subsets is a controversial thing which it is not at all clear whether it is desirable especially in the as in the case i mean the most important language for embedded systems you know real time things with systems that control sensors various kinds of hardware ballistic missiles and so on it affects it could affect their portability because you would have some feature which which is used in some in some implementation and then you move the program to another implementation which does not support that feature like your programs don t run right so the portability of actually programs written in the language gets affected if you allow arbitrary subsets yeah  noise  ok so if you look at all these features um and so finally what we can do is and then there are hundred and hundreds of programming languages it is impossible to study all of them what is more it is not necessary so what we would like to do is to divide up the study of programming languages into a few small parts so what we will do is we will look at  22  50  so what is a theory of programming languages contain yeah so  noise  a general theory of programming languages is based on three things what might be the most important thing is or the most basic thing is what is known as the syntax of the language ok so a programming language is really like a very highly simplified natural language it has a certain syntax right so which means that it has a certain what might be called the grammar certain things can occur only in certain ways you can not you can not arbitrarily take um you can not arbitrarily form sentences of the language as i said a program written in that language is just a what might be called as the sentence of the language and in a natural language you take a sentence of a natural languageit has various parts so for example in almost all languages all natural languages one thing that any full sentence should have is a predicate that is a it has a a syntactic category called predicates ok which in general may not be words a predicate could be a clause or a phrase um and so it might have a subject too in addition it might have a object so if you take a sentence in natural language so if i say run that is a complete sentence because it has a predicate no complete sentence in the natural language is without a predicate ok so run is a complete sentence it has a predicate optionally um it might have a subject may be also a object clause so and the subject clause is have to be of a grammatical form called they must be subject well subject phrases may be noun phrases which means they might have nouns qualified by object um adjectives may be an article and so on and so forth yeah so what you can do is you can divide up any grammatically correct sentence in a natural language into firstly various clauses each clause may be into various phrases and all these phrases have certain grammatical properties yeah in very much a similar manner in a greatly simplified form um what might called the parts of speech of a um of a program yeah in so every programming language has a certain grammar there are which specifies various parts of speech it has a certain what might be called vocabulary and it is possible to take a sentence of this programming language and parse it we will get into the meaning of these things more clearly later where the similarities with natural language after all a lot of the theory of the syntax was actually inspired by natural languages where the construction of artificial languages which did not have the which did not have a lot of problems in the natural language yeah um the next thing is that is what might be called the semantics very often this semantics of a programming language is really specified by its reference manual so if you in your i mean in your study of pascal programming um one of the important references would be the iso standard pascal reference manual by janson edward ok so that reference manual really specifies for each syntactic entity of this of the language what is the effect to be expected by executing that syntactic entity so in other words what we are saying is for each language construct what are the meaning associated with it yeah so the notion of meanings is something we have to look at unlike natural language the notion of meaning is something that can be specified um mathematically in a machine independent fashion so when we are talking about this semantics of this programming language we are talking about a pure meaning associated with the language regardless of any machine on which the language is implemented in general we are talking about meanings in a abstract settings in the sense that you assume if you like for practical purposes that you have no restrictions on the word lengths you have no restrictions on memory you have no restrictions on computational power except that you can only do a finite number of operations at any instance right so you assume a sort of so the semantic the programming language itself can be thought of as a mathematical entity quite independent from all its implementations and in general when you think of it that way then what you are saying is you are thinking of some kind of an ideal machine which has no restrictions except one restriction that at any instant of time only a finite number of operations can be performed i mean you can not do an infinite number of operations at a instant of time so if you consider such an ideal machine and um and then you are really looking at the programming language as a mathematical entity in some ideal environment ok and the meanings of that of the constructs of the language in that ideal environment form what might be called um actual reference manual so if you were to go through the reference manual of pascal let us say since that is you all have studied there are certain most of the reference manual is independent of any machine there are certain specific machine paragraphs or what are known as implementation dependent features but if you look at the language itself it is capable of looking at the language as an entity devoid of any machine yeah so most of the times so in  30  13  general the semantics follow the syntax the syntax has a certain structure which will come to and the semantics so the syntax has got some what might be called some basic elements and some compound forming operations they are connectives compound um connective which allow you to form um compound that is the sentences from simpler once um so the meanings in general should be such that you gave the meanings of the basic elements and then you give the meanings of the connectives in terms of the basic elements so very often what it should be necessary is since a programming language is really a finitary object which allows you the construction of the infinite set of programs ok it is not possible to unless you have this basic discipline that you express the effects of connectives in terms of the effect of the basic elements of the language it is not in general possible to predict the behavior of the language of a program written in the language unless you follow this discipline ok so basic feature of any kind of semantics is that it should allow for the derivation of the meanings of an infinite number of programs which means the only way to derive the meaning of the program should be from the meanings of its finitary elements so you have certain basic elements from which complex elements are formed so the meaning of the complex element should be derivable in terms of the meanings of the basic elements and the meanings associated with the connectives which formed a complex element from the basic elements ok so what it means is that semantics is going to b intimately related to the syntax the syntax is really a way a finitary way of specifying an infinite set of allowable objects ok the infinite set of allowable objects are the programs of the language or the sentences of the language and the syntax gives you finitary mechanism for specifying all possible allowable programs very much like a set theory notation right the set builder notation which allows you to give a finitary specification of an infinitary set of an infinite set and so the only thing that is really analyzable for a arbitrary program is its structure in terms of its syntax the finitary specification so the meaning of the program has to be derived from there from its finitary specification as to be expressed in terms of its finitary specification yeah so lastly so this is a semantics of a language is in a sort of an ideal environment don t worry about machine constraints don t worry about architecture they are all we don t want to worry about them because for one important reason i mean the programs have to be the programming language has to be portable ok don t worry about word lengths don t worry about limits don t worry about memory constraints assume infinite amount of memory available so then what happens is that you get  33  58  into what are known as pragmatic considerations yeah once so most of these implementation dependant features are really pragmatics for example the pascal reference manual does not tell you how to associate a disc file with a file variable in side the program that is one example of an implementation dependant feature which will vary depending upon the operating system interface you have yeah then there are various simple implementation um so the pragmatics firstly um firstly involves really all the algorithms that are going to be used for the implementation of the language all kinds of machine and architectural constraints for example this the maxint the maximum integer allowable in a pascal program is a typically implementation dependant feature because it really depends upon word length or byte length or um byte length or whether it uses two bytes for representing a integers and so on and so forth right so the value of the maxint can very from machine to machine the amount of memory that is available for a program can vary whether it is a stack based machine or a register based machine um those things are implement um those things are implementation dependent features ok so normally what we would do is even in our implementation in order to make the language portable we will separate out the basic algorithms of implementation from the architectural specific nature of the implementation when we do a when we read through a small compiler you will see this happening so there are certain basic algorithms which are really machine independent and then there are some machine dependant features the actual code so then you have things like the os interface what is the nature of the input and output that is file based terminal based sensor based what ever signal based and the os interface also includes the file server how the language has to interact with the file saver um with how the language has to interact in general with the directory service of the machine and so on and so forth and lastly what what is to be done about errors and by errors i mean errors written by errors introduced by users in their programs so errors could be of a syntactic nature errors could be of runtime nature what is to be done about these errors as a blanket um there is a blanket policy could be just is just that you abort but you know that doesn t help anybody in particular what is the nature of the error reporting is there some error correction that can be done is there some way of recovering from errors so that as soon as the first error comes you just throw out the program no it is if you can at least you should be able to point out all the errors in the program or all possible things that are errors so that the user gets to know all the errors at one time it reduces the amount of compilation effort and similarly you cant do that at run time may be but at least there should be some decent error reporting mechanism some error handling mechanism so that is one thing but errors is a very  noise  dicey object it is very dicey policy and different languages have taken different attitudes different implementations take different attitudes um so so it is a very very pragmatic feature yeah so i will just briefly go through the notion of syntax and then may be i will stop so these are the three issues and it is better to study all three issues sort of separately the semantic and pragmatic issues will closely depend upon the syntax and it is preferable that they depend upon the syntax so let us look at what constitutes syntax yeah so a syntax  39  20  has to do with a form or a physical representation of possibly an abstract object yeah so if you look at numbers um numbers really are not very physical at least um the twentieth century attitude towards numbers is that they are a conception of the mind and they are not physical what they do have is a physical representation in the form of numerals ok so what you write out and think of as a number is actually a numeral right so while you have various ways of representing the same number let us say the number one hundred and twenty six um so you can have what what is known as the positional representation which is what we normally use um and well this this is one hundred and twenty six written in hexadecimal and i hope it is correct yeah um so you um so in the basic form all these three representations of the number one hundred and twenty six are really the same ok the roman representation differs from the theonagri representation in the sense that the characters used are different what might be called the basic alphabet for the positional representation of numbers is um is different in the two it is just that the character set is different um this the character set is also different it is a positional representation which uses a base the character said therefore is also different but that is incidental yeah but in a in a basic forms all these are unified by the fact that they are all positional representations yeah positional representations i hope everybody understands that um right you go in units tens hundreds here you go in units sixteen s sixty four s and so on right um then of course you have the roman numerals which is non positional which has well firstly it has a different alphabet you could represent the same um roman same number one hundred and twenty six with different character sets but there is something unifying about um about this there is some thing fundamental difference about this representation from this from any of this yeah so and what is it fundamentally different if you disregard the change in character set what makes these two the same what makes these three the same and what makes these three different from these two ok what make that what that do what makes these two classes different is the grammar of the numeral of the language used for representing them yeah in both these cases the grammar is exactly identical the character sets are different and by enlarge the grammar of all these three is the same except for the character sets but the grammar of this is different from the grammar of this in the sense the form of representation is fundamentally different how you represent compound forms from simpler forms is different between the roman and arabic case ok so what is that grammar is what we have to study yeah so so let us look at the so let us look at this in a more general setting of the programming language so we might think of every programming language as containing of as having a vocabulary ok  43  08  a vocabulary is well what you might call a complete dictionary of words of the language and a word of a language is formed from a character set from a fixed character set and well you identify certain strings of characters as words as allowable words as part of the vocabulary of the language so a complete dictionary of the language is what constitutes that states vocabulary so if you look at i mean what i am saying is so so if you take languages for example natural languages like konkani or sindi what they have is different character sets and but the same collection of words ok sindi for example is written by different people some people write in devanagri some people write it in the urdu script the arabic script ok but the collection of the words is the same so a person who knows devanagri can communicate with the person who doesn t know devanagri but knows the urdu script by speech because the words are the same but you can not communicate by letter ok so there is a certain fixed collection of words whose actual form might depend on the character set right um but given the words there are what are known as formation rules yeah so given a vocabulary there are ways of combining words of the language to form sentences of the language and there is a finite set of formation rules which are called the formation rules are called productions which allow you to generate all possible sentences in the language yeah so let us quickly go through one example um so we will we will we will not worry too much about this character set because i think that s so for a programming language for example the character set really depends upon the kind of codes you use nowadays most of the time we use ascii codes but then you know we have eight bit ascii s and pcs and seven bit ascii s on main frame the character sets are different there are all these kinds of differences ok but let us disregard them um and let us just look at so let us just look at what just constitutes the a grammar so so i would say grammar grammar is  46  00  really a four tuple of objects ok where there is a set n which are called which is called the set of non terminals and this set of non terminals really specifies various kinds of grammatical categories of the language like you might say the parts of speech noun phrase verb phrase adjectival phrase noun clause subject clauses subject phrases object clauses predicates and so on ok so this set n consists of the basic grammatical categories all these all these sets are finite ok then the terminal t is the set of what are known as terminal symbols or terminal words and t is the complete vocabulary of the language ok so and p is the collection of formation rules or what are known as productions and s is what is called as the start symbol but s really is represents a grammatical category in n called as sentence yeah so so so it so the basic element of the language is what constitutes a sentence right so let me just quickly go through um ok so so  47  56  here is a simple grammar specifying boolean expressions so there is a start symbol s every grammar should have a start symbol s and some grammatical categories the grammatical categories that i have chosen are a v and c which actually stand for an add boolean expression or a or boolean expression or a complement expression ok the vocabulary of this language consists of all possible boolean variables that we might take the two left and right parenthesis and the three connectors and not and or ok these are the productions from the start symbol any boolean variable is a boolean expression that is so you take any b belonging to this set of boolean variables that s that one itself a boolean expression so the sentence of this languages are boolean expressions actually they are more than fully parenthesized boolean expressions ok so any a sentence is either a complement of a boolean expression or the or of the two boolean expression or the and of the two expressions an and clause if an and expression is one of the form which consists of two boolean expressions enclosed in parenthesis and separated by the word and ok an or boolean expression similarly is two boolean expression enclosed in parenthesis and separated by an or and not is similarly this yeah so these productions are really replacement rules so whenever you find a s you can replace it either by a a or v or a c or one of the boolean variables whenever you find an a you have to replace it by this there is no other way you can replace a by anything else similarly whenever you see a c you have to replace it by this and so here is a simple sentence generation you start  50  17  from s and one of the possibility is that you can you can replace s by c ok whatever i am replacing i have circled in orange so you can replace s by c c has to be replaced by something of this form by this and now i am replacing this s leaving everything else intact so i have chosen here to replace s by a and once i have replaced once i have got a a there the only possibility is to replace it by something of this form s and s i have chosen to replace this s rather than this s first and i have chosen it to replace it by a boolean variable so let us assume that there are only two variables b one and b two and i proceed in this fashion and lastly i get a complete sentence of the language which consists of only the terminal symbols and this is a sentence of this other language generated by this grammar yeah so we  51  30  talk of a language as being generated from a grammar as a set of all possible sentences that may be generated from the start symbol s important warnings  51  40  are that the set of non terminals and the set of terminal symbols are disjoint and a production is really a replacement it replaces a non terminal by a string consisting of terminals or non terminals yeah and sentence is just a string of terminal symbols generated from the start symbol yeah i think i will stop here now um transcriptor  v.srinivasa rajkumar educational technology i.i.t.delhi presents a video course on programming languages by dr.s.arun kumar deptt  of comp.sc & engg i.i.t delhi lecture 3 grammars so welcome to the third lecture so today we will continue with we just started on grammar on last lecture so we will continue it and go in some detail so let  00  57  me just briefly summarize what we did in the last lecture so as a set a grammar is a four tuple consisting of um a finite a set of non terminal symbols or grammatical categories if you like um a finite set of terminal symbols which usually constitutes the vocabulary of programming language um a finite collection of formation rules or productions which are really rules of replacement and a start symbol which really signifies the grammatical category called a sentence of the language of a language yeah so um so we went through the um we went through one simple um grammar for example for um for the generation of boolean expression alright we have this various syntactic categories here so we have the set of non terminals or the grammatical categories consists of the essentially the and expressions the v stands for or expressions the c stand for conditional exp um complement expressions and there is a start symbol the terminal set consists of open and close parenthesis and the connectives and not and or i will normally while we are dealing with grammars i will use the color black for terminal symbols and since the grammatical categories are a level of abstraction higher i will use green um light green for some things and dark green for some other things ok so this  02  57  was the grammar we had we also saw how a sentence which is a string of terminal symbols can be generated from this grammar by applying the rules of um the production rules right so in each of these cases i have circled in orange the non terminal symbol which i am replacing and by um and of course i have several choices for replacing s and i could choose one of the them all right so if you choose different choices then you will get a large number of other sentences you will generate a large number of other sentences in fact since there is absolutely no restriction on how long you can keep getting s in this in the case of this grammar you actually can generate an infinite set of sentences right so as you can see in a grammar is a finitary representation of an infinite set a large part of computer science mathematics and logic really has to do with how to represent infintary object in a finite manner and this is one such example yeah so as  04  20  i said the some of the warnings and cautions that you must keep in mind or that this set of non terminals and the set of terminal symbols should be disjoint and we have got the production set is really a binary relation from non terminal symbols to strings of non terminal and terminal terminal symbols right so the replacement rule allows you take choose any non terminal symbol and replace it by a string consisting of terminal and non terminal symbols yeah um there is a star here which is to denote that you can replace that this set is really the set of all possible finite strings that can be generated from this set n union t ok so i  05  28  will explain some of this so in general for any set a a star is the set of all the strings of finite length in particular the finite length could be a length of zero ok so a zero length string is really nothing ok it is called the empty string and we usually use the greek letter epsilon to denote it yeah a plus is the set of all non empty strings generated from this um set of all non empty strings of this there is only one string of zero length that is the empty string ok so it is the set of all non empty strings and a plus is equal to a star with epsilon removed from it since there is only one zero length string you know so the particular kind of grammar that we have been considering um as an example is what is known as a context free grammar right so when  06  37  the production rules are such that you are allowed to replace a single the rules or such that on the left hand side of the arrow mark you have a single non terminal symbol and on the right side you have some string of terminals and non terminals right that is called a context free grammar it is called context free as suppose to for example a context sensitive grammar right a context sensitive grammar has production rules in which given a certain string of terminals and non terminal symbols you are allowed to replace them by some other um you are allowed to replace the non terminal symbols by some more by some other string of non terminals and terminal ok ok the meaning of context free is that these production rules like it says so let us take a context let us take our example and um obtain so for  07  59  example in this let us take a arbitrary string in this example and we have so we are replacing this s let us let us assume we are choosing this arbitrary string so this s appears in a context and the rest of this is the context ok similarly this s here for example appears in this context ok and we are saying and we are calling this grammar context free because we are allowing this replacement of s by a string of non terminals and terminals regardless of what context that s appears with ok so this is the context in which this s appears and we have a uniform rule the production rule says that uniform in the sense that regardless of what contexts the non terminal appears in you are allowed to do a replacement ok as suppose to what might be called context sensitive in a  09  13  context sensitive grammar you could for example specify that a certain non terminal can be replaced by a string only if it appears in a particular kind of context ok in particular you might define that context to be an empty context ok so in general a context a context sensitive grammar is really more general than a context free grammar ok you might look upon every context free grammar production as specifying a context which consists of the empty string ok ok anyway we will go into that a little later but let us first worry about some simpler grammars also yeah so  noise  so if if it follows these rules in its context free let us  10  23  look at languages the language generated by any grammar is a set of all possible sentences that may be generated from the start symbol for example if you take the start symbol located in some context and generate a string that string may not be in the language at all unless that context it self could have been generated from the start symbol yeah so so  10  58  the um so this is what we would call a language and in general we would call it a language on the set of terminal symbols and a language on set of terminal symbols is a possibly infinite set of strings from the terminal set that we are saying that any subset of t star is a language um and for example here are some trivial languages that you can define on any set t you have the empty set for strings which is the empty language you have this language consisting of a single element the empty string ok you have t star itself is a language and t plus itself is a language and in between you have a whole lot of other languages so you might regard this t plus and t star as two extreme as one extreme and these the empty language and the language containing the empty string has the other extreme and you can have lots of subsets in between ok so and they are all languages the problem is that when you have a programming language you have a infinite set of possible programs and the problem is of defining exactly what grammar can generate that language right so um so we will say that so just as we have defined um grammars we will also define languages in a similar fashion but first  12  47  let us go into a particular into a into some particular kinds of grammars called regular grammars ok so in a regular grammar every production is of this form supposing you take a grammar in which every production is of this form where this capital a denotes a non terminal symbol this capital b denotes a non terminal symbol and this small a denotes a terminal symbol in fact i should have made it black but any way um right so if every production is of this form then we call this a right linear regular grammar ok the first thing to realize is that a right linear regular grammar is also a context free grammar there is really no difference ok a context free grammar allows productions which are not for example a right linear regular grammar means that on the right hand side you should have just one non terminal symbol and one terminal symbol appearing in this order the terminal symbol is followed by the non terminal symbol in a context free grammar we do not have that restriction for example we had um s goes to in a context free grammar for example  14  30  if you take this any of this rules they are not they are not at all regular ok they are not right linear or regular and of  17  58  course you might also allow you would also allow just a terminal to be generated right right so after all you have to generate strings of the language form the terminal set if you always had only non terminal if you always had non terminal symbols appearing on the right hand side then you will be never be able to generate a full sentence of the language so a right linear regular grammar is one all of whose productions are of this form similarly you might define a left linear regular grammar as one in which all the productions are of this form yeah and of course it has this terminal generation rule too yeah so every production is either of this form or of this form then you would say it is a left linear regular grammar right so let me right that out for completion all right so so such a grammar is called a left linear grammar and if you have done some hardware if you have defined for example designed some hard ware using finite state machines it turns out that you can actually all right linear grammars actually represents finite state machines you know machines without output i am talking of those kinds of machines right so in fact you can represent all you can take the state transition diagram of the machine and refer to each state as a non terminal symbol and refer to the input symbol the input into that state has a terminal symbol so a finite state machine automatically defines a right linear grammar yeah so most finite state machines have a start symbol the start symbol is the start state right um so the what we are talking about is really quite a powerful language and let us if you  18  40  suma let us summarize what we have looked at the general properties of grammars are that firstly every regular grammar whether right linear or left linear every regular grammar is also context free every context free grammar is also context sensitive in particular all the productions of the context free grammar can be considered in the context of empty strings on both sides of the non terminal symbol  18  40  so so  18  42  in general we can look upon any string for example if you were to take a string like this if you were to take one of these strings i can think of this any of these strings as let us take the top most string right i can take any of these strings as having a not symbol then an empty string symbol then a open bracket then an empty string and then an empty um and then a and an empty string and a close bracket ok so you can look upon every context free production as being padded appearing in a context which contains empty string and the empty string implicitly appears everywhere between terminal symbols between terminal and non terminal symbols yeah so so it is for that reason every context free grammar is also context sensitive and what also is true is that every right linear regular grammar um so if you so our interest in grammars is ultimately in generating languages so you take any language supposing that language can be generated by a right linear grammar then it is also possible to define a left linear grammar which will generate the same one the language similarly if you were to take any language generated by a left linear grammar left linear regular grammar then the same language can be generated by a right linear one yeah  student  um grammar is also context sensitive  sure ok so what we have as a general kind of production let me just rewrite that so let us look at let us look at the set t star yeah so the set t star for any set of terminal symbols ok is just the set of all strings obtained by this terminal symbol ok obtained form t um in particular i can look upon t star as consisting of the empty string ok the set of all strings of length one which is the set t itself the set of strings of length two which is like t cross t the set of strings of length three and so on and so forth right so t star is really the set which is obtained as the union of cartesian products tn where n is greater than or equal to zero right now what i can do is i can define a binary operation called catenation ok the effect of catenation is to take one string is to take two strings and put them together ok for simplicity let us assume that the set t consists of just two symbols ok so let us just call those two symbols a and b right so i can take a string in t star let us let us take a let us take a string in the set so let us say ababb let me take another string let us say bab the operation of catenation ok which i will denote by just a dot for the moment is just to produce the string ababbbab fine so this so this is equal to just so the operation of catenation just juxtapose the two strings the two strings it is a binary operation on strings it just puts the two strings together and gives you a new string so for example this string this string is of length five so this belongs to the set t five um t raised to five this string is of length three this belongs to the set t cube and this string um has a length eight and it belongs to the set t raised to eight and of course all these sets are subsets of t star and so catenation is really an operation from t star cross t star to t star right so it has a functionality which is to take two strings of finite length and juxtapose it yeah  25  30  so supposing  28  48  you consider the empty string ok what happens when you take a string and juxtapose an empty string to it you get back the same string what happens if you take the empty string and to it you juxtapose some other string you get back the other string so the empty string satisfies these conditions that for any um for any for any string in t star for any s belonging to t star s concatenated with the empty string equals s and the empty string concatenated with s also gives you s so the empty string is a very often catenation is juxtaposition operation we just get rid of the dot symbol ok but you could have a dot in between right now what you are saying ok so now one obvious property now is that this epsilon is in fact the identity element for catenation it is like a zero for addition right secondly catenation is associative in the sense that if i take three strings s t and u and i catenate them i can catenate them in any order ok so the set the set t star under catenation and under catenation and with the empty string is really a monoid because this operation is associative so catenation is associative and it has a identity element however it is not communicative so it is not an ebleion monoid it is just a monoid yeah now what do we mean by a context sensitiveness so if you have a production let us take a production let us take a production of an arbitrary context sensitive grammar ok so let us keep this here so that we might we might in fact need it for some reason  28  48  so if you were take a production of a context sensitive grammar what it specifies is that i have a non terminal symbol a and if it appears in some context ok let us say ok then i can replace this non terminal symbol by some some other string so which means that the context still is going to be preserved ok but this a is going to be replaced so it is a conditional rewriting i can replace a by um let us say um let us say some some um some b um c and may be some xyz some x and y padded with some i don t know some x and y ok so what this production drew what this production says is that i can replace a by the string xbcy only if on both sides of the a i have small a and small b appearing otherwise i can not do that ok in that sense this production is context sensitive it says that if supposing that there were some other symbols appearing somewhere on two sides so there is a large in the generation process there is some large string which was there there is a ok there is a arbitrary string and turns out that there are some other symbols which actually are around a um let us say yeah ok supposing so a here appears in a context which does not contain this which does not contain small a and small b on either side ok so then a appears in a context in which this rule can not be applied ok what we are so a context specifies a certain minimal a minimal shell within which that thing should appear in the case of our context free grammar you are implicitly specifying that the context in which it should appear is epsilon on both sides ok so any of these strings ok in this case um in the case of our context free grammar  32  10  in the  37  10  case of our context free grammar we are implicitly specifying that if s appears in an empty context which means that you don t care what appears on either side of s then i can replace s by a ok so a context really specifies the smallest kernel that you around that symbol which we should satisfy and here it is the small what we are saying is regardless what the string in the generation process is if it is of the form if it is of the form a um a b regardless of what else occurs in the string if if you can this a is a candidate for replacing by this rule ok context sensitive grammars are actually more general i mean in the sense that this padding of a and b also perhaps not necessary you might get rid of this a small a and small b too ok but let us not worry about it the inclusive meaning of context sensitiveness is really um what is you are specifying some minimal padding around that non terminal symbol which will enable a rule to applied and in the case of a context free grammar the minimal padding is nothing ok so all those rules can be thought of as rules in the context in which on both sides of the padding the minimal padding that you require is the empty string which means you don t really care what the rest of the string is in the generation process regardless of what the rest of the string is you can apply the production yeah um actually it turns out that the context sensitive grammars or what are programming languages all are but there are practical reasons why we do not take their context sensitivity into account it is much simpler to deal with the programming language as a context free grammar as generated by a context free grammar and deal with a context sensitive aspects later on in the process of compilation ok a typical context sensitive feature even in languages like pascal is that no variable can appear in a statement unless there is a declaration of that variable ok only then if you had a context free if you had a if you have a context free grammar for pascal what it will do is it will fail to check on these context sensitive issues ok so undeclared variables can appear in your program in your um in your program if you just go by a context freeness but there are but the practical reasons are that there are no efficient algorithms to recognize or pause context sensitive languages represented as context sensitive grammars we have efficient algorithms to recognize and pause context free grammars if you take context sensitive grammars then you are not likely to get a linear algorithm ever there are no linear algorithms available linear time algorithms available for phrasing context sensitive grammars so what we do what is usually done by many people is to in specifying the language is to specify it as a context free grammar so that and later as part of the semantics specify it s context sensitive aspects many people in fact consider context sensitive to be synonymous with the semantics of the language but that is not quite true yeah right  37  10  so coming back to what we are saying um so every regular grammar is context free every context free grammar is context sensitive every language generated by a right linear regular grammar can also be generated by a left linear regular one every language generated by a left linear one can also be generated by a right linear one and we can in fact go supposing you have a grammar which is regular but not necessarily right linear or left linear what does that mean it means that some of the rules might be right linear some of the rules may be left linear such grammars can also be always converted into either purely right linear ones or purely left linear ones yeah in fact that s that conversion is what helps us to design machines for recognizing languages of this grammars yeah  student  can you give an example for that  well um it is not absolutely important now it is really a subject of the theory of computation course so we will we will just yeah so when we are looking at languages um we would say that the language is regular if there exists a regular grammar which generates it similarly a language is context free it there exists a context free grammar that generates it and a language is context sensitive if there exists a context sensitive grammar that generates it it is possible that for some language you have generated a context free grammar it is context free but the language could still be regular yeah so um similarly i mean it is possible that you generate you have written context sensitive grammar for a language which actually could be context free in the sense that if you work hard enough you might be able to come out with a grammar which is purely which is context free yeah um let us just look at a few small examples of let us say regular languages yeah so here  39  55  is take our arabic numerals um um let us remember one thing it is one thing to design a grammar and then ask what is the language generated by the grammar ok and another thing is to take in existing language and try to define a grammar for it and our were no long before any grammar was used for them actually large number of numerals actually come from the notion of the grammar in natural language and um sanskrit grammar for example was always a very neat um rigorous um art form if you like so that is one of the one of the reasons perhaps that we have such a neat we have evolved such neat notation for numbers right so take the arabic numerals the terminal set is the set of all possible digits that you have zero i am considering representation in decimal so zero to nine and i require just one non terminal symbol s ok and i have just these following productions s goes on a s can be replaced on a digit followed by s again or s can be replaced by a digit ok so very nice and simple grammar right and this a right linear grammar and the corresponding let linear grammar which is equivalent to it is this if you were to take roman numerals firstly it is not at all fully clear what the terminal set is in the sense that the romans never considered a things beyond a few tens of thousands the romans had a pattern in the sense that a five every five um so they had five ten fifty hundred five hundred thousand they had symbols for all of them and the assume that they had symbols also for ten thousand fifty thousand um hundred thousand five hundred thousand  noise  but then you would require if you continue that pattern then you would require an infinite set of terminal symbols ok supposing you did have the infinite set of terminal symbols which means your condition for being a grammar already is violated but supposing you could have a infinite set of terminal symbols then the way the numerals are written is that they were very very context sensitive for example an x can not precede a c um um for example i um x which is ten can not precede v which is five or rather i am sorry an x can not precede let us say d which is five hundred ok so it is very context sensitive what can appear on the left of what and what can appear on the right of what ok in that sense the roman numerals is not as simple as this a regular language is implicitly a very simple object it is easy to see what language it generates very often it is easy to construct a grammar for it yeah so  noise  so that is and also so what we would say are that is that these two grammars are really equivalent after all remember that our ultimate aim is to represent languages some how in a finitary fashion so what should be the criterion for saying that two grammars are equivalent they should have well let us say the same terminal set they need not have the same non terminal set they need not have the same production set but the language they generate should be the same all right so the language that we have previewed for the context free language that we have previously given here is an equivalent context free grammar yeah so so for  45  24  example what i have done in this case is to just um take is to just factor out um the open parenthesis and the s and introduce a new non terminal symbol called b which for which i have two rules so i have gotten rid of the non terminal symbols a and b and c i have gotten rid of because i directly wrote this but i could have had c if i had wanted right so let us let us quickly go through can we just quickly see that grammar um here is the grammar so here was here was the original grammar and here is the equivalent grammar in which what what we have done is we have taken we have taken this the fact that there is a common occurrence of left parenthesis followed by s we have factored that out into this ok an equivalent way of writing this grammar is to use a new symbol s let us say s arrow d and let d produce this string yeah um this the elimination of the c was just well just to make a grammar smaller to reduce the number of non terminals it is an important constraint it is a important thing to reduce the number of non terminal because you are parsing of the language really depends how many non terminal symbols there are ok so it is a good idea so which means that for the same language you might have variety of grammars it is a matter of decision making to choose the right kind of grammar right correct kind of grammar which generates that language the and the criteria for choosing a grammar are there firstly the grammar should not be so complicated then it is impossible to parse the language preferably it should be a context free grammar keep the um keep the number of non terminals low and another important constraint which which we will not be able to appreciate now which we will come to later is that it should facilitate an easy explanation of the semantics of the language in fact the arabic numerals um the left linear and the right linear once they have that difference they both equivalent in terms of actual generation but the fact that but the  48  27  fact of the matter is that it is easier to specify a semantics for the left linear one rather thatn the right linear one now it is easier to specify a semantics for the right linear one rather than the left linear one and certain parsing algorithms actually will choose the right linear over the left linear because there is a interest inherent constraint in those parsing algorithms you know um the inherent constraints has to with recursive calls ok so as you can see you know the recursive calls in this case will could lead to an infinite recursion whereas in this case they would lead to a finite recursion based on a look ahead we will we will look at that later um so so that s um basically all that we have time for today thank you transcriptor  v.srinivasa rajkumar educational technology i.i.t delhi presents a video course on programming languages by dr.s.arun kumar deptt of comp.sc & engg i.i.t delhi lecture 4 ambiguity today we will talk about ambiguity but before that we will go through some simple definition of a simple programming language then look at how ambiguity comes about naturally and let us see so let us get back to our favorite grammar our favorite context free grammar which you have seen several times  00  55  is this and let us look at let us look at the sentence that we generated using this grammar so now  noise   01  15  if you take a look at this let us take a look at this sentence generation carefully if your ultimate aim is to generate this sentence of this grammar then let us look at how we have um applied the productions or fired the productions so initially there was no alternatives we took we chose one out of three possibilities right there were  01  45  three possibilities for us s goes s replaces replaced by a s replaced by v s replaced by c s replaced by an identifier so there are four possibilities of which we chose one in fact  02  00  if you have to if you have to generate this sentence then there is really no other possibility you should take any other possibility will not give you this sentence ok so out of the four possible choices we have to choose this um then there was only one possibility for c and we have chosen that the  noise  here again unless you chose the possibility a this possibility a you can not generate this ultimate sentence ok and for a there is only one possibility namely this is this now you actually have two possibilities ok we have chosen to fire this with s goes to b one ok which is essentially i mean which is essential in the sense if you had to some produce the b one in the end you would have been forced to apply this s goes to b one ok however there was another possibility that is that we could instead we could have ignored this and instead fired this ok so if we had chosen this s then what would have happened is we would have chosen we could have got this this step v first so these two steps would have been computed ok so what we would have got is that if we would have chosen this um chosen to apply a production on this s always keeping in mind that this is the ultimate sentence that we have to generate what we would have got is not open bracket open bracket v and s and then again we have the possibility of either firing this v or this s ok and we could have chosen any one you could either replace this s by this b one here or we could have gone ahead with this v and replaced it with s or s ok and then here again now you would have um supposing you had chosen this v you would have s or s and you would this s would be there when you have three possibilities of replacement and you could choose any of them ok so what i am what i am so if you look at the derivation of a sentence this particular order is not sachrosite out of the various non terminals in the intermediate sentence generation you could have and applied a appropriate ok so there would be many such derivations after same sentence ok depending upon the order in which you chose to apply productions in a sentence in other words what they are saying is whether you choose this or this it really does not matter so the so the various derivations that you have um just give you various orders either you for example here we have not chosen any particular order we could have chosen for example um to always fire a production of the leftmost non terminal symbol in a in a intermediate string so here for example we have violated that we have not chosen the left most ok so here you might consider this derivation as one of um several possible derivations of this sentence ok so since we are talking about a context free grammar and the replacement of non terminals by their right hand sides in the production rule the we were talking of something that is independent of context so if there are several non terminals it really does not matter which non terminal is chosen first for replacement provided you you can so provided you choose the right one which will ultimately give you this sentence so if you were to justify each of these steps in this derivation by by which production you have applied ok so you  06  48  could number these productions as one two three four five six seven and you  06  56  could write a justification which just tells you which production number you applied then even though um then what you could do is essentially you could permute the order of the productions of these applications so so for example in here you don t have much choice but here as i said you have two possible choices either an application of production four which is this or an application of production two which is this right so you could have just permuted the order in which you apply these productions but your intermediate sentences your intermediate sentences will not be the same ok but you could have permuted the order of application of these productions and you could have derived the same sentence so what it means is that the application of productions for the derivation of sentences in a context free grammar can often be need not be totally ordered so there could because of the context freeness of the grammar there is an independence at a intermediate stage between the various non terminals that you can choose to replace right so what it means is that there is nothing sacrocite  08  32  about this derivation we could have got another derivation by just applying the same productions in a different order so so what it means is that these productions because of their independence what we can however there is a certain amount of order for example you could never have applied any of these productions let us say before applying this production ok you would never have any of these possibilities before applying this production c goes to not s if you are interested in generating this sentence so given this sentence you would have to apply this before any of the others but here you have a choice whether you want to replace this s or this s but the actual production that you have to apply is still place specific if you have to generate this b two or b one you have no alternative but to replace this s eventually but whether you replace this s before you replace this s or after it does not matter ok so the in any derivation this independence between the two or several other possible replacements that might be there gives you a partial ordering on the application of the productions ok  noise  so this partial ordering is really what is sacrocite so what does this partial ordering specify this partial ordering specifies that if there are certain productions which have to be applied in a certain order but other productions need not be applied in the same order they could be applied independent of each other so if you were to look at the various derivations possible for this sentence we could take all those derivations and collapse them to give us the partial order colas them to actually see independence look for independence and dependence in fact what we can do is we can draw draw a tree of exact dependences ok so that tree is what is know as parse tree yeah so if you were to take this this is  11  20  the same um same sentence that we have generated ok and what this tree tells us is what are the dependencies in the applications of various productions so the root of the parse tree is always the start symbol ok now in the generation of this sentence the first production that was applied was s goes to c which i have not written here but you could you could for example write s goes to c as um yeah then the next production was c goes to not s ok so that c goes to not s is what is written so the various terminal symbols i mean i am looking upon the whole of not as a single symbol ok open parenthesis as a single symbol um remember our convention that black denotes terminal symbols the eventual strings that you generate will all be strings in black strings of the language the colors denote um denote certain abstractions yeah right so the first production was s goes to c c goes to not open parenthesis s close parenthesis then then there was a production then there was a application of s which yielded a and that is this this is an application of the production s yields a and then you have this a yields open parenthesis s and s close parenthesis and now we are here at this point we are at the same position that we are here in the original sentence ok and now the fact that you can choose this as first or this as does not really matter eventually in what ever order you apply the productions for them if you are interested in generating the sentence that you have generated then the right hand s should go to a b one and the left hand s be expanded into a v this v should be expanded into open parenthesis s or s close parenthesis and it does not matter in which order you perform these two productions the first one if you are interested in generating this sentence the first s should produce a b two and the second s should produce a b one yeah so that is also the reason why i have used a brown color for the productions ok so we obtain a tree which we can call the parse tree and the branches of so the leaves of the of this tree are all terminal symbols notice that all the leaves of this tree are all black so if you read if you read the tree if you read the leaves from left to right you get the sentence that you generated so you have not open parenthesis open parenthesis open parenthesis b two or b one close parenthesis and b one close parenthesis close parenthesis which is what this sentence is right so so what we are most interested in is in a is in the parse tree rather than in the actual derivation ok so for any sentence we have a corresponding parse tree for generating any sentence we do not necessarily have a unique derivation ok so in this grammar that we have defined for every sentence is actually a unique parsed yeah so let us let us and parse trees are very important from the point of view of compiling language implementation from the point of view of specifying semantics um it is the only thing that is really sacrocite and the fact that there are many orders of derivation for the i mean there are many ways of traversing this parse parse tree gives you several possible derivations yeah so  noise  so let us just summarize that briefly so we  16  41  can look upon a parse tree as presenting the partial order in the firing of productions yeah and we can look upon a derivation of a sentence as just one of many possible traversals of the parse tree of the sentence so for each sentence what we would like to have is a unique parse tree not necessarily a unique derivation what we want is a unique parse tree ok so the traversal of the tree um depending upon which how how you decide to traverse the tree you have various possible derivations so you can look upon a derivation as just a linearization of a partial order yeah i think most of you are familiar topological sorting right so for example a topological sort just takes a partial order and linearizes it you sort it so that you you you provide a linear order a total order of all the elements such that the dependency specified by the partial order are maintained but their dependencies do not exists you might place them in any order you like ok so a linearization of a tree always gives you a total order ok and you can have and in fact for partial orders it is also true that a partial order is completely defined by the set of all possible linearization s of the partial so essentially a parsed tree is also completely specified since it is a partial order by the set of all possible derivations or traverses you can make of the past yeah so this is a fundamental property in the theory of partial orders which is that you can look upon every partial order as actually a set consisting of all total orders of that set yeah so as being completely defined by the set of all total orders ok so now what actually the parse tree that we have presented is a is somewhat syntactical yeah so it is syntactical in the sense that um from the point of view of compiling or language def um or language implementation it is all fine in the sense that well from the point of view of of a language implementation it doesn t really matter what what is the nature of each of these terminal symbols as far as it is concerned it is some stream of symbols that are coming and just looks upon them as a stream of symbols but any kind of language also has a implicit um type of terminal symbols which is clearly distinguished for example we make we make a clear distinction always between a identifier and a operator yeah so for example so um so this we might call a concrete parse tree where we make no distinction at all between identifiers and operators if they are terminal symbols they are leaves of the parse tree that is it ok so what we are more often interested in is what might be called a abstract parse tree where the same sentence you actually make a distinction between what are the operators and operands so if you look at the sentence that we have previously generated you look at the sentence it is clear that our intention was to define a language of boolean expressions where the operations are not and and or and the operands are these boolean variables like b one b two and so on right so this  21  33  clear distinction between the operands and operators is what leads us to what you might call abstract syntax tree yeah so this syntax tree actually elevates replaces non terminals we have been i mean i have been um we have designed the language in such a way that we could easily elevate the operators to the intermediate nodes of this tree we could talk we can talk of a root operator which is not we can talk of a intermediate operators and and or ok so in an abstract syntax tree the operators are always the intermediate nodes ok and the leaves the leaves are all the identifiers of the operands right and this is of course not some thing that this is of course not some thing that s absolutely full proof or like anything but um when we talk about distinction between operands and operators we are saying that we are making a distinction between the various kinds of terminal symbols that are there in the parse tree and we are  22  46  we are elevating some of them see if you look at the ultimate language the ultimate programming language that we are interested in we really not too interested in the non terminal symbols the concrete parts of the language the the real down to earth as far to the road if you like are whatever is are all the terminal symbols but those terminal symbols in any programming language have some meaning there are some distinctions between what is an identifier what is an operator ok if you want to bring about this distinction then what you are interested is not in this concrete syntax tree but we are interested in what order should i apply the operators on the operands in fact  23  52  if you look at this sentence the reason why we included parenthesis why we have parenthesis in all our mathematical language or anything is to specify an order of application of operators on operands ok right so this is especially true for what might be called infix operators it is not absolutely essential to have parenthesis but you can see that otherwise we would have to have a a uniform post fix notion or a uniform prefix notion if you have to avoid operators ok so in fact every language construct can be regarded as a operator yeah  noise  so so so essentially if you are interest so if you are interested in giving meanings to languages you are not interested in the concrete parse tree but you  24  54  are interested in the abstract syntax tree ok if you look at the way you have done arithmetic calculations in school you will see that you have actually i mean um remember that in a abstract syntax tree there are no brackets ok given an arbitrary expression there are various ways of evaluating that expression you can choose to evaluate one operand rather than rather than another unless there is a explicit dependency that you can not evaluate one operand before the other so for example if you have if you have um let us say um let us say an arithmetic expression like um  25  42  three multiplied  27  10  by five plus four multiplied by six yeah in in our normal arithmetic there is an implicit order this is the order so the abstract syntax tree of this is just is just the following i mean it is just this you first apply multiplication you can not do this addition before doing both multiplications but it does not matter in which order you do these two multiplications and then you do the addition right so all are evaluation mechanisms actually refer more to a syntax to an abstract syntax tree than to the concrete parse tree of the expression yeah so we have to keep keep this in mind so we will i will oftenly using these abstract syntax trees once we go once we cross syntax i will be using only these abstract syntax trees yeah so let us keep that in mind so now keeping these things in mind let us define a small programming language yeah it is it is very simple it is really amazingly simple that it is actually completely useless if you like yeah  27  10  so what  27  14  does this programming language contain it contains no declarations contains only boolean variables and expressions it contains the following commands assignment sequencing um conditional and a simple looping command yeah and what i would like to do is to specify the syntax of this programming language so that it clearly gives me all the possible syntactically valid programs in the language i can generate so that i can generate from the grammar all possible syntactically valid programs of the language for anybody who has done some programming it is clear that i mean um it is clear that you know what what kinds of programs we are talking about in the sense that what would be syntactically valid what would not be syntactically valid but now there is a question of formalizing it and giving rules production rules for the syntax in such a way that for example if you had to build a compiler or a translator for this language you should be able to do it without any problems yeah so it is so in fact if it were a course on programming what i will what i could do since all of you know some programming or the other is i could just specify this much and then we could start writing programs yeah but but the for the purposes of translation and compilation you require to specify things in a great in some more detail so let me just summarize the construction um summarize my coding of various constructs so the  29  23  this this brown is part of the context free grammar notation i mean the notation for productions as i said this this brown looks very much like the bark of a tree so branches of parse trees and so on are coded in this color the actual terminal symbols are of course in black yeah um in this since we have two different kinds of entities commands and boolean expressions um i have i have used this dark brown for boolean expressions and what might be called atomic commands the assignment is a atomic command for which i have used light green all other compound commands they use this light blue and the actual program or the sentence of the language if you like is in dark brown yeah so this is whenever i change um change my color coding i will inform you but this is how i have defined the grammar so what we will do is we will define the grammar in a top down fashion i mean the best way to do anything is top down so let us let us look at it top down so so let us look at this grammar  30  50  so i  34  50  have the start symbol so as far as i am concerned i mean i am only interested in sentences of the language of which and the sentences of the language and the sentences of this language are all programs ok unlike say a language like pascal or something there are no um there are no um there is no program heading and there are no let us say declarations so on so what is a what is a program in this language so this rule specifies that a program is any command so given a any command is a full program of this language yeah so and what is the command of this language well a command could be an atomic command or it could be a sequence of two commands ok it could be a conditional command or it could be a looping command ok um so here i have used this these brown bars to indicate that this actually is four productions c arrow a c arrow c semicolon c c arrow if b then c else c fi c arrow while b do c od yeah right so this bar actually specifies the various alternatives that you have for each non terminal symbol right so so at the command level or what in most of these imperative languages are called statement level this the thing um this is these are the various productions we can look upon the definitions of the programming language in terms of several levels and this so this s gives you the production level gives you the program level and how the program level goes into the command level ok this command level essentially tells you how um it is i have so far not specified what this atomic command a is but at this point it tells you how to form compound commands from simpler commands so it just says that any simple command is a command and the sequence of two sequencing of two well simple or compound commands that s why it is in blue this semi colon is the reserved word of the language so it is in black so the sequence of the two commands is a command in itself and if b is a boolean expression then and then this conditional command this conditional compound command might contain might contain some compound commands inside it and this while b command contains a boolean expression and a possibly a compound command um right so so what it means is that as far as the grammar of commands is concerned this b and this a are also terminal symbols of this this level of grammar specification yeah so so but what we should do is in order to get a complete definition we should also look into these atomic commands and the boolean expressions so let us look at the atomic command there is only so since i specified that there is only assignment statement there is only one atomic command yeah  34  50  so the  37  20  grammar atomic commands is just of this form let us assume that v is a boolean variable and b is a boolean expression then v assigned to b is a um ok so then let me look at the boolean so the language of boolean expressions i am defining a different grammar just for variation ok i could have used the grammar that i have previously defined but that grammar is sort of a hatch patch grammar this is so let me look at this so for one thing in that grammar there were the constant true and false were not there ok so the terminal true is a boolean expression the terminal false is a boolean expression any boolean variable is a boolean expression ok so these are the so if you take the language of boolean expressions alone as a separate entity these are the terminal symbols of the language ok and the next level specifies how do you make compound boolean expressions from simpler boolean expressions right so if b is a boolean expression then not b is also a boolean expression and given two boolean expressions this is a boolean expression and this also a boolean expression and this is what i have used here i have changed the grammar in the sense that here i have designed a fully parenthesized notion ok so i am for all boolean expressions which are not atomic these are the atomic these are the atoms of the boolean expression language for all boolean expressions that are not atomic i have enclosed a new parenthesis yeah right so so now supposing we did not to that ok supposing instead we defined this grammar um considering i have  37  25  gotten rid of true and false now just consider this grammar supposing we did get such we define such a grammar without parenthesis ok then consider this sentence this sentence is generated by this grammar it is very easy to give a derivation for that but not just that this sentence can actually be derived in two different ways yeah  37  50  so look  38  48  at this sentence v one or v two and v three and this is my grammar and what i could do is i could apply the b or b rule and derive this i could choose to expand this and get since my first symbol has to v one i don t really have any other alternative may so i get v one or b then i can choose to expand this replace this by b and b v one or v two and b you can choose to expand this and replace it by v two then i can choose to expand this and get this yeah right  38  48  now i  39  36  could also make a different derivation so for example i can decide to apply this b and b here yeah and i can choose to expand this and give me v one or what did i do um i chose to expand this and replace it by b or b then i chose to expand this and replace it by v one then i chose to expand this and replace it by v two then i chose to expand this and replace it by v three yeah and i get the same sentence right  39  36  but yeah  40  40   student speaking  there are not brackets in the language  student  then how we know that it is v one or v two  right so that is a point so that is what ambiguity is all about yeah so so the point is that if you were to look at the way in which this these productions have been done then and the way in which these productions have been done here and is a root operator so if you were to look at the abstract syntax tree then and is root operator and below that is a or and this the abstract syntax tree that you get yeah i have chosen to just to give you the abstract syntax tree but you could for example take the concrete parse tree also of this derivation ok now if you were to look at the abstract syntax tree of the other derivation where or is the root operator and and comes inside and  40  40  if you  41  15  look at these two symbol these two tree you can see that they are both actually different ok there are not just two different derivations of the same sentence there are two different derivations with different syntax trees and these two different syntax trees actually affect how you can specify the meaning of this language so for example v one had the value true  41  15  v one  41  16  and v two had the values true and v three were false then the evaluation of this syntax tree would give you value of true and the evaluation of this syntax tree would give you a value of false ok so if you look upon syntax as ultimately having to specify a meaning then what you would like is a unique meaning to be specified in that sense this grammar for example falls short of being adequate representation of a unique expression language  41  55  an expression  41  58  language with unique meanings yeah so we call such a we call such a grammar ambiguous it is called syntactically ambiguous but the syntax is really a via media in which you are going to specify semantics it is only a initial handle for the specification of semantics so so let  42  30  us look at this a grammar is ambiguous if there exists a sentence in the language with more than one parse tree ok right so so ambiguity is an important constraint in the sense that it is not just that there are just two different derivations it is that there are two different parse tree those parse trees are important both from the point of view of translating which means running programs i mean if you are looking into the problem of compiling then you are really looking at the problem of executing programs in order to get meanings so it is necessary for a program to be interpreted in exactly one way i mean and actually the the compiler for that program and the user of that programming language should both come to an agreement on how that program is to be interpreted right so ambiguity has very serious consequences in specifying the meanings which means the execution behavior of programs right and actually a lot of our programming languages for example i have um a lot of our programming languages actually do have ambiguity so the the programming language that we have defined is totally unambiguous yeah this grammar  44  18  specifies a language in which there is absolutely no ambiguity ok there is exactly one parse tree for every sentence in the language um now if you were to look at languages like  noise  pascal so one one reason why so in the expression language there is always ambiguity in most languages but that is because that is because the expression language uses allows you to use normal mathematical notation ok so for example it allows you specify various things within an implicit order of evaluation so if you look at look at our if you look at our these expressions for example  45  25  it allows you or normal mathematical notation allows you to specify this expression without any parenthesis where it is implicitly assumed that the syntax tree for this expression if you remove the parenthesis it is still going to be this and nothing else for example it does if you if you remove the parenthesis there is absolutely no reason except normal mathematical convention why you cant have a syntax tree of this form there is absolutely no reason why except for normal mathematical convention there is absolutely no reason why you should not construct a syntax tree supposing i removed this parenthesis ok but it is just that our mathematical convention says that well there is a precedence of operations which ensures that multiplication is done first and addition is done later so if there is ambiguity if there is syntactic ambiguity then the order of evaluation should be that multiplication should precede addition other wise the order of evaluation is as specified by parenthesis ok so since this our normal mathematical notation most programming languages actually implement this ok there are this there are also in the at the command level too there are syntactic ambiguities for example in languages like pascal one is what is known as the dangling else problem yeah so um if you look at the difference between our conditional the conditional that we have defined and pascal is that firstly we do not have both an if then and an if then else construct we have a perfect bracketing between um between these for example  48  00  our conditional construct has a perfect bracketing there is an if with a fi and a while do with an od ok so there is no possibility of ambiguity except in the case of one construct what is that no where is the ambiguity where is the ambiguity  student  yesterday um  this grammar  48  55  does not provide ambiguity for that it is totally unambiguous for that but there is still one place where there is ambiguity the sequencing is ambiguous so think about it we will come to it later transcriptor  v.srinivasa rajkumar educational technology i.i.t delhi presents a video course on programming languages by dr.s.arun kumar deptt of comp.sc & engg i.i.t delhi lecture 5 plo  syntax welcome to lecture five so today we will do a slightly more complicated programming language um for which there is also a compiler um but before that let us briefly summarize what we did last time um and also answer the question so we  00  47  will follow more or else the same abstraction levels except that this brown i will change um so programs and commands and atomic commands and a context free grammar notation will remain the same um  noise  so let  01  06  us look at last times grammar um we had this grammar firstly the grammar of programs was that every program is just a command  noise  a command could either be some atomic command um or it could be a sequence of commands um it could be a conditioner with a if b then c else c if where the where all these words in black are reserve words so including this which you might not have seen um it acts as a closing bracket for if and similarly the od here acts as a closing bracket for the do um ok so one of things i said was so um we also define the notion of ambiguity for example we saw that for example so for example we ask the questions whether this given grammar was ambiguous so as  02  05  i said our grammar is ambiguous if there exists a sentence in the language generated with more than one parse tree it is implicit that if a grammar is ambiguous not only will its parse tree be um not only will there be a sentence with more than one parse tree the same sentence will also have more than one abstract sentence tree because the abstract syntax tree is just obtained from the parse tree by elevating the operators to the root nodes and replacing the non terminal symbols by their appropriate operators yeah that is for a restricted class of parse trees ok so the question was how this grammar is ambiguous and actually  02  57  by in the case of these two control structures the conditional and the loop i have eliminated the ambiguity by introducing two reserved words the closing bracket words fi and od however this sequential composition or the sequencing operator which is a binary operator on commands actually gives you ambiguity so let us see how that happens so for example you could have um you could have three commands  03  30  so let  07  50  us assume c one semicolon c two semicolon c three semicolon these three are either atomic or compound commands i don t care but these three commands actually have give you two different parse trees so you could for example you could have a parse tree like this i am drawing triangles here to denote that these seats be the various commands themselves can expand into trees so this is a tree in which this first this first semicolon is the root and the second semicolon is a right circle is a root of the right circle right another possible um parsing of this is is the following you could have a semicolon c one c two c three in which case in which case this the first semi colon is actually here it forms the root of the left sub tree and the second semicolon is the root of the tree so strictly speaking there is ambiguity in this grammar but um what we will see and what is obvious to anybody who has done some programming is that the sequencing operation in any programming language is associated in the sense that this so this really these two trees really correspond to different bracketing right so for um this tree for example corresponds to the bracketing where you have c two and c three bracketed inside and c one and c three bracketed also outside and this tree corresponds to the case where you have c one and c two bracketed inside and then c three bracketed outside right so this corresponds to c one semicolon c two the whole thing semicolon c three and this corresponds to c one semicolon c two semicolon c three right but since in general um sequencing operation as we will see later in the semantics is really a function composition operation and the function composition itself is associative we will see that sequencing is also associative so what it means is that as far as the implementation of the language is concerned any implementation can take any decision with regard to the semicolon operation so the fact that it is ambiguous does not matter as far as the runtime behavior or the meanings of the programs is concerned yeah so that s a that s a small matter which we have disposed of but in other case things can change as we saw we saw an expression of boolean expressions where there was ambiguity if you did not introduce parenthesis and it can actually change the value of boolean expression depending upon how you parse the boolean expression right so so let us look at how if you were to actually look at a language reference manual um what would it what would it look like  07  50  so we  08  03  have so most languages since algol sixty use a notation called the backus naur form ok it is actually a notation um created by john backus and peter naur in the definition algol sixty the algol sixty was the first um language which used rigorous syntactic form based on context free languages and context free grammars to define the language abs accurately before that for example um um backus was involved in the creation of fortan and the net result was that since there was no clear syntactic definition of the of the fortan language every fortan compiler written by various people gave different interpretations to the syntax of fortan and what that resulted in was fortan comp fortan programs were not compatible across machines so for example they way one compiler treated um the fortan syntax was different from from another compiler and moving programs from one from one machine or one compiler to another compiler became a huge problem it became a huge problem in the sense that you required a whole team of programmers to actually either entirely rewrite that program to suit the new compiler or the new architecture of the machine or it required substantial rewriting and um and um patching up of programs so that they would run correctly in the on the new machine or on the new compiler ok um so  noise  by that time of course context free grammars had become quite popular as it a form of theoretical study and backus and naur define the algol sixty language using this notation um they were well the as far as as far as we are concerned what they used was for ensure readability they did not use non terminal symbols um um i mean they did not use single symbol um single character non terminal symbols they used full words so they wrote statements within angle brackets and since there was no arrow mark on the type writer they used double colon equal which is now become standard ok and so they wrote all the productions in this form in full the non terminals being enclosed in angle brackets and the arrow being replaced double colon and equals but actually what what is more convenient and which and which came up very soon is what is called the extended backus naur form ok the extended backus naur form um it just adds the power of specifying regular expressions within context free in a convenient fashion that s that all it is merely a convince we should remember that um firstly regular expressions are also context free but happens is that there are many constructs which for example allow for options or zero or more occurrences um and and in order to specify them by a context free grammar um with the limited notation that we have what it means is the introduction of new non terminal symbols to aloow for those kinds of iterations so the extended backus naur form essentially uses the backus naur form extended to include iterations in choice so for  11  55  example so here you have one thing so let us let us look at this so supposing you had a backus naur form production of this form ok where alpha beta and gamma are strings of terminals or non terminals ok so if you had a production of this form a goes to alpha note that as usual i am using my light brown brackets for the for the notation for the backus naur form notation or for the context free grammar notation yeah so this would be equivalent to the two productions a goes to alpha b gamma where b is a new non terminal symbol which is not already there in your set of non terminals and b either goes to epsilon or to beta right so this just says this is something this notation this extended backus form notation is something that you will see you will see quite often for example on any unix machine if you run the man pages for some command you will find that there are various options given switches given and they are usually separated um they are usually enclosed in brackets of one kind or other normally they use square brackets to represent the various options so a typical example is would be that for examp if you had more than one option as in this case the two  13  30  options the several options are either separated by bars or they are separated by commas and what it means is that you this is equivalent to this set of productions with a new non terminal symbol and since in the definition of a programming language you do not want to clutter it up with new non terminal symbols which do not have any particular significance ok except that they aide in writing out a grammar more systematically if they do not have any logical significance you wouldn t like to introduce them right so so so for example so so um if you had the option of having an if a language like pascal you allowed both the statements both the if then statement and the if then else statement then the else clause is an option ok which could be which ideally could be sep separated out then the else clause actually belongs with the if and the then as a logical as a grammatical entity so you wouldn t like to separate it and so what you would do is you would put that else clause in the definition of your language if your language allowed an else clause and a normal one conditional like if then you would put the else clause within these square brackets right so so that it um so that you reduce the amount of the number of non terminal symbols remember that a programming language a real world programming language which is actually being used is actually quite a large piece of syntax by itself without without actually complicating it further by adding these extra non terminals which do not have any particular significance yeah so which have significance for the compiler which have significance for accurately specifying the language but other wise do not have grammatical significance yeah so  noise  you should have significance for example with respect to ambiguity parsing and so on and so forth but do not have do not as a logical entity have a separate significance from the non terminal in which they are actually specified as an option right so the other thing is that is that you can have zero or more repetitions of some option so if you were to have a production of this form a goes to alpha within braces beta gamma then this is equivalent to saying  16  13  that you two productions of the form a goes to alpha b gamma where b is a new non terminal and b is this denotes um zero or more repetitions of the string b in your production of the string beta so b either goes to epsilon which denotes a zero number of iterations um number of repetitions or it goes to beta b so that you could have more than one occurrence of a beta right  noise  so so this is what this is the extended bnf notation which is normally used the extended bnf notation is also very convenient for other practical reasons for example um  noise  if you look at any pascal manual the syntax diagrams of pascal can be are directly equivalent to the extended bnf notation so you just have to follow the arrow marks in the syntax diagram and they actually give you the productions ok  17  21  and they correspond more to the extended bnf notation than to the ordinary context free notation that they have already seen so for it is it is important to know this for  noise  sorry for reading manuals for learning a new language and also for and um in general to know about the language right so let us let us look at um a language the last time we looked at a toy language which didn t even exists so let us look at now look now at a language which actually does exists um so this is the language pl zero this language is very much pascal like it was designed for the purposes of teaching programming languages compilers and so on as a first course by nicolas worth himself the designer of pascal he has  18  30  also written the compiler um which is available with us um so the main thing about pl zero is that it is even smaller than pascal it has a single data type the main features are that it has a single data type are called off integers and nothing else no other data types um the only control structures are that are assignment sequencing bracketing looping and a one arm conditional that means it has a if then statement there is no if then else it is just a it is just an if then statement um you can program and if then else two one arm conditions in sequence right where you can neglect the boolean um of course there are no boolean data types in this what it means is that you will have to encode your booleans may be as integers ok may be you zero for false one or any thing greater than zero for true ok so um and  noise  but it does contain one important feature and that is the control abstraction mechanism  so it actually contains a parameter list procedures which allows for a step wise refinement of programs which allow for an abstraction of um which allow abstraction complicated programs to be written in a structured fashion and it allows these parameter list procedures also to be true to be nested ok so you can have a nesting which allows for a step wise refinement in the development of programs um so let us let us look at the syntax of pl zero um let us look at so i will i will keep using the arrow mark instead of the double colon equals for a production but i will use extended bnf notation otherwise right  noise  let us  26  30  define it in a top down fashion so a program my start symbol is p in this case i will not explicitly specify  noise  the terminal symbols and the non terminal symbols  noise  i am sorry um i will not explicitly specify the terminals and the non terminals it is obvious that the terminal symbols are those that are colored black and the non terminal symbols are um others yeah and so the start symbol is however is p um so a pl zero program is a is a block which terminates with a dot with a period this is as in the case of pascal programs you terminate the program with a dot right a block consists of a declaration followed by a statement i am saying a declaration and a statement um we will we will see that so a declaration and also for brevity i don t write full names for the non terminals i have tried to use non terminals um i have tried to use single um single letter non terminal symbols which are sort of obvious what they mean so a declaration d um can be this clause is optional since it is enclosed in light brown brackets so you can have a constant declaration a constant declaration which means this word const is a reserved word ok and a constant declaration is a i have put these in dark brown i and n they stand for identifier and number which we will look at later the reason i have put them in dark brown is because they are not really non terminals i mean there could be a infinite set of identifiers possible and of course infinite set of numbers we are considering an ideal machine for which we are specifying the syntax the actual limits on the numbers and the lengths of the identifiers it is just going to be implementation dependant right that is not part of the syntax definition of the language right so so they are not really non terminals they are actually terminals but then there are in infinite number of them possible so i will we will define them also but for the present just take this light um this dark brown i and n at phase value as identifiers um names and numbers since we have only integers those are the only kinds of data type we have and of course you could have a number of constant specified separated by commas so the commas are reserved word of this language and this the moment the word const occurs there has to be at least one constant definition ok so this combination specifies one or more occurrences of the clause i equals n this also specifies how they should be separated they should be separated by a comma if there is a const reserved word it should be it should be terminated by a semi colon so n so you can define a sequence of constants in a single declaration and terminate that sequence of constants the um terminate the entire declaration by a semicolon and of course you don t need to have any constants at all so this this entire clause is optional ok now whether you have any constant declarations are not you could have variable declarations but you don t need to so even that clause is optional since there is only one kind of data type it is not necessary to explicitly specify what the variables are going to be of so you can just specify the variables separated by commas but the moment and the moment you have this reserved word var occurring you have to have at least one identifier if you have more they should be separated by commas and if you have this var the entire variable declaration will have to be terminated by a semi colon ok this is also optional you could have procedure declarations and they are parameter list procedures and a procedure has a procedure has procedure as a reserved word they should be a identifier which should be terminated by a semi colon and there should be a block which is again terminated by a semi colon and the entire the entire procedure is also optional you don t need to have any procedures in your program and whether you have one or more of these clauses they should occur in this order and you have a statement so oh  noise  i should have i should not have written this let us erase this we are just considering declarations this so declarations end here and of course the entire set of since this is optional and this is optional and this is optional you could have a empty declaration too so this production implicitly allows the production d goes to epsilon ok right  26  30  so um  29  45  now let us look at the statement which is s so a block just consists of a declaration followed by a statement so let us look at this definition of statements  noise  so a statement itself could be empty a declaration could be empty which means an empty string itself is a program but that need not worry too much that is sort of a trivial case but a statement could be empty um otherwise you have an assignment an assignment statement is a statement where an assignment is of course a identifier um colon equals an expression note that there is absolutely no relation between what identifiers are declared in this declaration and what identifiers are used in this statement so the syntax is context free but the language feature is such that you can not use a variable without declaring it ok so that is something that is a context sensitive feature which is not specified in the syntax of the language yeah ok so then you have an explicit procedure called statement ok so you can call an identifier and implicit meaning is that this identifier should have been declared as a procedure otherwise you can not use call call is a reserved word you have the one arm condition which is if a condition then a statement this whole thing is a single statement you have the looping construct which is just as in pascal and you have compound statements you can call as a sequence of statements by bracketing them with a begin and end and call that a single statement so this just say that you can have a begin you can have an s note that since s can produce a epsilon this could also be empty ok then you can have zero or more occurrences of this so for example just a pair of brackets begin end itself is a statement it is what it is a trivial statement um which does nothing ok it corresponds to a no operation for example in hardware so you could have zero or more repetition um occurrences of this grammatical entity and they are all separated by a semicolon right and this whole this is a string right so now  29  55  we have this this um our previous our previous um if you look at the only non terminal that i have not really been defied here are i and n which i will define in the end but all other non terminals here have been defined but of course they have also mean they they have also defined at the expense of introducing new non terminals ok so for  30  05  example expressions conditions  noise  these are somethings that we have to define so condition well the language is such that i have modified the language a bit firstly there is a unary condition  noise  which for any expression e um note that since the only data type available is integers the only expressions available are also only integers ok so this unary data type applies over all expressions um this unary condition this unary predicate applies over all condi over all expressions and yields the true or false so this the reason for using this odd is well partly because well it is nice to have a unary predicate some unary predicate secondly this also the reason for choosing odd is because there is a direct jump on not equals in most hardware so it where ever you have jump where or um even in your high level programs a large number of your programs of the are of the form while some variable is not equal to zero do something so in all those cases you could check for oddness of that variable because a variable is also a expression as we will see and you could use this as a condition so it is the other otherwise you have various binary relational operators ok i have simplified the original language to an certain extent by using single letter relational symbols so that is why you have this odd looking symbols ok so this this is standard greater than this is greater than or equals this is equals this is not equals um it is the original pascal compiler for example allowed this as a not equal symbol um this less than and this less than or equals the original pl zero compiler has defined by wirth actually assume that the less than equals and the greater than equals are already available on the key board which they are not so i have to change that um ok conditions really dependant upon expressions either through unary predicates or through binary predicates and  32  40  let us  37  45  look at that expression language so before i get into the expression language i would like to say something um so um here in the case of the expression language you have you have a comprise between two diff two extremes one is of course that all of us think of expressions really in this form supposing you just consider the four operators that you have addition subtraction multiplication division we normally think of a expression as being a sum of two expressions a difference of two expressions a product of two expressions or a quotient of two expressions and of course we use brackets however what what happens is that this is ambiguous this grammar is ambiguous because for example it doesn t use parenthesis sufficiently it is possible to generate sequences which you understand to have a certain priority order of evaluation and which if you define the expression language this way the compiler need not ok so it is ambiguous of course every variable is an expression every constant an integer constant is also an expression those are the atomic statements of this grammar so the other extreme is what we have already seen and that is that you fully bracket every expression whenever you have a binary operator you put bracket around the pair of operands so you can have e plus e enclosed in parenthesis e minus e enclosed in parenthesis e star e enclosed in parenthesis and e divided by e enclosed in parenthesis but however most of us find it tedious to look at to actually write parenthesis over every thing and i mean you will have to key key in parenthesis every time whenever there is a binary operator you would have to key in parenthesis ok so as far as we are concerned in our in our abstract language in our abstract syntax we will just if you are looking at abstract syntax in string form we will just assume them to be fully parenthesized if you are looking at abstract syntax as in tree form we will just draw the trees corresponding to whatever order of evaluations you want we want ok because any fully parenthesized notation can be translated into a appropriate abstract tree which preserves the order of evaluation of the expressions and vice versa given any abstract syntax tree you can transform that into a fully bracketed string of symbols ok but from the point of view of the compiler this because it is ambiguous it is clearly unacceptable this because it is tedious for every programmer to write fully parenthesized versions makes it makes it in convenient so what what one has to do is that um um what one has to do is that one has to strike a reasonable compromise and try to define the expression language in such a way it is ambiguous it is not tedious and it follows all the conventions of um it follows all the normal conventions of mathematical notation yeah right in parsing let me mention that um this this this plus and minus are also overloaded unary operators for example you can take negative numbers or you can write positive numbers as plus something right so then they become unary operators ok so a negative number is just the unary minus of a non negative integer ok so it is a unary operator there whereas when it occurs in such a form it is a bi these these plus and minus are binary operators ok so we have to take that into account too there is overloading and we have we have always in our mathematics two we have used a lot of overloading for example in our programming too um addition subtraction multiplication division multilic and multiplication are used for both real data types and over integer data types in the pascal language they are also used for set operations ok so they are overloaded tremendously um and this overloading is something that one should take care of um so  37  45  so the  37  50  normal convention that we that we follow are the unary operators usually bind the tightest ok that means a unary operator it s influence extends over to the first available symbol and that s it and it takes precedence over everything else over all other operators except when there are parenthesis ok if you have a unary operator and an and a large expression enclosed in parenthesis then it is the negation of that entire expression and not of the first symbol after the expression um after the left parenthesis ok right and plus and minus are of course overloaded and the normal mathematical convention is that um multiplication and division bind tighter than the binary operators plus and minus ok however multiplication and division looses precedence over the unary operator plus and minus  39  00  so if  39  00  you have so if you have for example minus five if you have an unparenthesized expression of this form minus five star minus three then um or minus five star three then what you mean is that this minus refers to this five and not to the entire expression this minus refers to this three and this star binds these two so the appropriate bracketing is this right so um  39  35  we have to we have to take this conventions into account for the purpose of um for giving your friendly user interface as far as the expression language is concerned so that people with a normal knowledge of mathematics mathematic notation mathematical conventions can write programs can write expressions that they they have been normally trained to write right so so let us look at the expression language so what happens is that this the provision of this convenience mean means that you require a more you require a um fairly large number of non terminals before you can expect to define it un ambiguously right  noise  so here  40  27  it is this this language of expression is available in all books this some how or other deal with parsing or compiling or anything so i wont i wont go in to great details about how it is unambiguous and how they have taken into account all the conventions but you can see at a glance that that it will work right so an expression is a term which might be preceded by a unary plus or minus or since this is a optional clause it may not be preceded by any of these ok so it might be an unsigned term if you like followed by optionally an addition operator and a term ok right the addition operators are just the binary plus and minus so um please disregard this ok so any expression can be regarded either as a term a signed or unsigned term or a signed or unsigned term with an addition followed by a addition operator and a another term a term um a term is something which either a factor which we will worry about later or it is a product of two factors or the quotient of two factors so this m is a multiplicative operator so star and division the multiplication and division are multiplicative operators and a term is something that of this form either it is a factor or it is a product or quotient of two factors right a factor is anything regarded as a single unit either it is an identifier which means that we are normally talking about variables or constants or it is actually a number specified as part of the expression or it is a whole expression in itself enclosed in parenthesis right so you can see that these terms that these three non terminals e t and f are mutually recursive because e is defined in terms of t t is defined in terms of f f is again defined in terms of e so they are mutually and circularly non recursive um circularly recursive and they actually take into account the fact that you can look upon so if you can look upon an expression as basically the sum of two things so you do not expand out into a term um so if you have um if you have that um the outermost operation that is to be done is an addition ok you have some large expression whose root operation is an addition operation that means that is the last operation to be done then your context free grammar is such that the left operand supposing it is just an identifier so it will your productions will allow it to go from e to t then from t to f f to i f to e again or f to i and from i again it will keep circularly revolving yeah ok  44  30  so we  44  32  will um so this grammar really takes precedence of operators into account and the um the point is that this syntactical definition is absolutely essential for writing the compiler so for pragmatic reasons it is necessary to have this kind of syntactic definition but semantically we will just look upon a expression as a signed or unsigned expression or something that is a binary operator just in terms of abstract trees so we will either look at it if you want to represent them linearly either as fully parenthesized expressions or abstract syntax trees yeah so so um  45  25  lastly let us come to the last part which is the definition of the number so a number is either a signed or unsigned integer so this is an optional clause and the definition is that it can have a digit and it should have a digit followed by more digits ok and digit of course is defined as a character zero to nine so an identifier we follow normal pascal rules an identifier should start with a letter in the case of the pl zero compiler all the alphabet consists of only upper case letters it is very trivial to modify it to include lower case letters too but what really distinguishes a number from an identifier is the occurrence of a letter or a plus or minus symbol or a digit that is what really distinguishes that ok so the occurrence of the letter this plus or minus is really an operation it is not um it is not really i mean you are actually going to take if you have a negative number you are actually going to take the corresponding integer and neglect it as a operation right so in the case of an identifier that is why it should begin with a letter and um it may be followed by one or more letters or digits right and the reason for removing this n and i productions from the main grammar is that this these rules are really not part of parsing when they are part of what is known as lexical analysis ok for example you can write such rules also for recognizing that the word while w h i l e as been recognized as a single word the word begin has been recognized as a single word so these are actually part of the process or what is known as scanning or lexical analysis so typically if you look at a program written in this language it just consists of it is a file of characters and we would we would like to divide up the program into words or what are known as lexemes or you which which really describe each entity in the program so instead of a file of characters we would like to regard it has a file of words and how are you going to regard it as a file of words you should be able to recognize all those reserved words you should scan all the words and decide whether they are reserved words or if they are not reserved words you should be able to treat them as identifiers ok or if they are constants you should be able to read out the entire constant in this case for example a string of digits representing an integer or a or a string of or a plus or minus followed by a string of digits representing an integer so then you would look at the entire thing as a single unit so a scanner typically takes a file of characters and gives you a file of words  49  15  or actually  50  25  what they are known as so you have the user program is just a file of characters and what you get is a file of lexemes yeah the word file is used in a very general fashion i mean it does not mean a desk file it means any unbounded sequence ordered sequence of object so the process of scanning converts the file of characters in to a file of lexemes and then the process of parsing actually takes over the handling so that is one reason why i have not why i have not worried too much about for example these reserved words  50  25  so i  50  30  am i am what i am saying is the process of scanning would have created a single lexeme out of all these all these things are a single unit yeah and they would they will after scanning they would lose the status of being a string of characters they would be some single entity unit in the form of some structured um an element of some structured data type which gives the identification as to what this unit is ok so it could be an index into a table which says that and with other details like it is a reserved word or it is an identifier or it is a constant it is striped in a more complicated language whether it is a integer type or so on will be filled on later may be after the area after the um after the process of parsing and so on is through so you will actually create a huge table of the amount of information that you have to ab extract from the program through all the process of compiling and keep it for use for checking for example type checking runtime type checks compile time type checks to detect un declared variables which also a good way of detecting spelling mistakes right so all these terminal symbols will actually become single units in some table and they the file of lexemes will just be a single unit which gives an index in to the table yeah ok so that that table is resident always in memory for reference during the process of compiling for example you have to check various context sensitive um issues like hasn t been declared before right if it is been declared what is the type is it been assigned the right type is it being used in a expression in the right type so you require this table of information for each identifier reserved word or each lexeme to know whether it is being used correctly in the program right so this is the syntax of the language and um we will get on in future to defining the semantics of the language so we will start the next lecture with the basic notions of semantics and and we will then add on to this toy language new features and see how they have to be defined the syntactic definitions of these new features is not very critical because you have all the basic material as long as you can parenthesize expressions as long as you parenthesizes the new features that you are um  noise  that you are introducing without ambiguity or as long as you define them in some reasonably good syntax it is not very important how they look it is more important how the abstract syntax trees look and what meaning you give to the abstract syntax tree yeah thank you transcriptor  v.srinivasa rajkumar educational technology i.i.t delhi presents a video course on programming languages by dr.s.arun kumar deptt of comp.sc & engg i.i.t delhi lecture 6 semantics welcome to the lecture six so today i will briefly summarize what we have done before and introduce what we are going to do later and this will complete in some sense the basic frame work the rest of the course um so but before we start on semantics let me let me just briefly go through the definition of the programming language pl zero alright so  01  04  we had this um a program is defined as a block with some syntactic entity unlike a period a block consists of a sequence of a declarations possibly empty and a sequence of statements also possibly empty um declarations of various kinds which specify values or types or functionalities as in the case of procedures um statements a statement  01  50  is a particular syntactic grammatical category which is specific to imperative languages and pl zero is a imperative language because it is a it is based on state changes and we have this usual um suite of statements for example assignment conditionals loops and we have the so the conditionals and loops are usually dependant upon some the truth of some condition the truth or falsity of some condition and so we have various kinds of conditions most of them are relational to deal with arithmetic operators and their relations and conditions in turn arithmetic in turn mean the evaluation of expressions so we  02  40  have various kinds of expressions the usual operators on expressions and what we have done is also is to take into account precedence of operators so that you have a convenient way of writing unambiguous expressions um without having to worry too much about parenthesizing them so um a large part of um context free grammars is really got to do with let us say parenthesis parenthesizing things correctly so that you can get you can generate the right kinds of trees alright so once um and the specificity of a context free grammar also is such that you are interested in various you are interested in pragmatic issues like is there a successful compilation procedure given the kind of grammar i have defined for the language is it compilation procedure efficient if i change the compilation procedure should i change the grammar in some way if the compilation procedure does not work very well it is not able to parse for various reasons how should i change the grammar ok so the context free grammars um for specifying syntax is powerful enough so that you can consider alternate grammars which might simplify the problem of parsing your programs alternate but equivalent grammars um you could also and it is also it can also be coarse enough to specify just the broad outlines of the language so it is so as as syntax normally one would have to specify the grammar in sufficient detail so that firstly you are not restricted to a particular parsing strategy in the case of pl zero in fact that one draw back we have defined the grammar in such way that um that it is actually a a sub class of the set of all context free languages it so the language itself as been defined so that it falls within well within the subclass um and the so can use various strategies for let us say parsing or compiling this um programs in pl zero but on the other hand the moment you include more powerful features the moment your language becomes larger your grammar also becomes very large and it might be necessary to for example go outside the sub class in order to add more facilities you know and by facilities you mean things that are convenient for the user which don t hassle him unnecessarily which means you might require to design more general grammars so that the specificity can be taken care of at the implementation time any but the point is that you can specify a programming language syntax to a great level of detail with context free grammars and the important thing is from the point of view of a user of the programming language that level of detail is not needed most of the time a user is only interested um in knowing the abstract syntax of the language how are the constructs to be is so an implementers view point of the of the language and a user view point differ in this very important aspect you require a very course level grammar without ambiguity may be for the purpose of the user but you require a grammar which is fine enough so that the implementer has no trouble in um designing a parser for example for this language ok so whereas there are various levels so one um so one instead of instead of restricting ourselves very high highly formalized notation like a context free grammar at the level of the user we might think of the user as being interested only in the abstract syntax trees which this um for each program so he would like to know essentially what should be what would be the order of evaluation if he wrote a program in a certain way for example if he omitted some parenthesis if he omitted some semicolons or some sequencing operators what what would be the order of compilation does it make any does it make any semantic difference whether he introduces those parenthesis or not right  noise  so and there is also a another aspect as i said the um context free grammar formalism is not powerful enough for example enforce certain context sensitive features right so the subject of semantics really deals with these problems so first so lets so let us for the moment um not dwell too much on the static on the semantic um i am sorry on the on those context sensitive aspects of the language let us look further deeper down and see what exactly it is that we mean by a meaning ok so there are various ways of looking at the notion of meaning one is  09  25  that you might just think of the meaning of a program as input output behavior very often i mean for interactive programs sequences of inputs and outputs is what we are interested in or for many programs which can be divided up into neat procedures you are more interested at the procedural level to determine what function that procedure represents right so so does does a program um can we say supposing we do not have too much of io in the program one view point we can take is that every program is a function or at least if it is an interactive program we can divide up the program between it is interactions and look at those segments which are free from interaction with the outside world um from the user s interaction let us say and determine what function they represent and if they if you determine if you want to determine what function they represent then you also need to know what is the domain of the function what is the range of the function and by function i mean a mathematical function so various there are various there are various schools of thought about for example how programs should be designed ok so one one one very prominent view point take the view that essentially all programs are functions or mathematical functions which means they should have a clearly defined domain and a clearly defined range and the program is just a notation to express a mathematical function this view point also subsumes the view point that an algorithm is really also nothing more than a function ok so if you take an algorithm as an abstract object um abstract in the sense it really has no concrete it is only concrete representation is a program that implements the algorithm then really what you are looking for as a matter of correctness is to specify whether this program does not implement that algorithm that is often not very important what you are interested in is whether this program actually implements the function that the algorithm is meant to specify ok so there is a mathematical function which for which you do not have a computational um which you can not for which you have an algorithm which means that you have spilt up the computation of that mathematical function into various computation steps in some computational model in your mind in our case most of the time it is what ever is a computational model specified by phenomenon machines may be ok because there is a computational model in mind whose primitives are used in specifying how a certain function is to be calculated it is a method of calculation of some large function by a large function some function which in which has to be expressed in terms of the primitives of your computational model so that you are actually able to evaluate for each argument of that function um the value of that function at that point right so that is one thing so this so there is a huge school of what what is i mean a methodological thought which actually looks upon functionality as the primary um motivation for writing programs algorithms are simply an intermediate steps which are essentially because it is not the the jump from a function to some very primitive and simple function um computational primitives is a very large job and you require some course grained intermediate steps so that you can start from the function refine the function into some course grain computational steps which you know imputatively are possible refine each of those till you reach the primitives of the computational model and that should be program right so but but the point is that theoretically speaking every program can be looked upon just as a mathematical function ok you can and very often that is also a convoluted way of looking at it though it is theoretically sound there is an another property very um which is that very often you are really looking for what invariance or what other properties are either created by a program or what invariant properties are maintained by the program so you can look upon programs as relating um various data objects which the program is manipulate but the way they manipulate them in such a so as to maintain certain in variant properties or to create fresh properties out of the data that is also a way of looking at it ok so you might look upon programs entirely in terms of sets of properties so you can think of a program as completely being defined by a set of properties an algorithm then which is some what more abstract since it is an intermediate step between a function and a program can be thought of as maintaining some subset of the final set of properties that you should achieve so you are gradually adding to the properties that you require by refining you are algorithm in various stages so that also means you getting into a finer and finer level of detail when you write a program so theoretically both these are tenable because firstly every mathematical function is also a relation and therefore any relation can be thought of as as a property or as a collection of properties any relation or a collection of properties can also be thought of as defining some clauses of functions ok so so the various schools of thought actually even from the point of view of the user of a program um user of a programming language the question of what exactly a program represent it is some concrete notation for something in your mind but is that something in your mind a function a collection of properties a collection of invariances that is a matter of view point ok so so the of what constitute the main imp the main important thing is that it really does not matter because all these view points can be considered equivalent you can always represent any kind of any sets of invariant properties as functions any sets of function properties as um as relations or predicates and but largely it is a matter of um so so the attitude that most people would take is what seems more logical i mean if you were to look at um a database program to look upon a huge database program as a function from some domain to some range i mean it might be mathematically correct but you know what are you what are you trying to do in a database system you are trying to maintain certain consistency properties that s in some sense a more um not logical but certainly um certainly that s one of the primary um motivations in the maintenances of database going through transactions and maintaining consistency ok so you maintain may be some invariant properties some consistency properties you can also look upon programs and data in different in a different manner for example the most of the normal programs can be thought of as data filtering through a program ok and getting altered the transformation of data as the program executes but in the case of a database it is some how seems a gut a gut feeling would say that it is more like a program moving through a mass of data and making some transformations going through some transformations right so there are various view points you can take and some some view point seem more tenable as in certain problem specific areas and some other view point seem more tenable or more convenient in some other problem specific areas right so we will take the function we will take the view at the moment that the meaning of our program we will take an implementers view what is an implementer view of a program while a implementer s view of a program is that he should some how specify state changes for an implementer the computational model is already present a user need not even worry about the computational model except to know that these things are there and he wants this what he wants is completely different from he doesn t um if you look at a ultimate customer of a software he is not really interested in um in the state changes the program goes through or in the state changes of the data item what is more interested may be it is just that it should implement this function or it should maintain this relation or it should maintain this property or it should create a fresh property out of some massive data for example a completely random sequence of items has to be sorted should create a property of let us say a sorted list out of the massive data without changing the data right so from the point of view of an implementer we might be interested we might be actually interested in um in actually this step by step change state changes so the next question is what constitutes program equivalence or when can you consider two programs to be equivalent  21  50  one view  21  51  point is that you just the step by step changes that take place should be the same so given two programs this this state changes that take place should be the same but that is not really right because as i said i mean there are for any for any mathematical function there might be any number of different algorithms and those those various algorithms might use various different kinds of data there might be different data representations for example if you take a simple problem like generating the first hundred primes well i can have i can have different algorithms one is that standard cyavophrathenis for example we just you estimate there are there are fairly accurate estimates of prime generation for example um the n plus oneth prime is usually less than or equal to twice the nth prime ok so in order to generate the first hundred prime or first n primes you can take a large boolean array and just knock out all the composites using the cyavophrathenis algorithms or you can follow an algorithm in which you start with a prime two and systematically generate the next prime and the next prime till you reach end primes for each of these algorithms there is almost an infinite number of programs which implement those algorithms ok if you if you look at the problem as generating the first hundred problems and outputting them then there could be a variety of algorithms of may be different complex days of different space requirement and for each of these algorithms has a infinite number of programs which implement those algorithms so step by step state changes or not um or not let us say a very a very interesting way of determining when two programs are equivalent two programs will still be equivalent if the functions they represent or the same note that it is possible to write the same function in several different ways so the equivalence of functions should also is also important here so very so we would take the simplistic view that a program is just two programs are equivalent if they compute the same functions in the case of interactive programs you really can not think of a interactive program as just one large function but you can think of it as a sequence of input output behaviors ok between any two successive interactions there is a function and we might think of program equivalence in the case of interactive programs as really a form of black box equivalence in which given two programs regarded as black boxes for the same kinds of inputs you get you get the same kinds of outputs then you would consider the two black boxes to be the same they need not be identical because they could implement different algorithms or they both could implement the same algorithm in several different ways and so the so as far as program equivalence is concerned we we can take the view that the input output behavior should be the same or the function that is computed if it is not an interactive program if it has a single source of input and a single output um we could think of it as the function that is computed by the program should be the same um going a little a level lower you have this notion of equivalence of algorithms you have algorithms which are not equivalent in the sense that either i mean in the sense that they are either one is either a space inefficient compared to the other or time inefficient compared to the other and so on and so forth but implicit in all this is that they the two algorithms actually implement the same function yeah it is you can compare two algorithms only if they both implement the same functions or the same input output behavior otherwise it is completely meaningless to say that one algorithm is better than other i mean it is meaning less to say that the binary search algorithm is better than a quick sort algorithm um it s yeah because they really are for different functions you can compare two sorting methods you can compare two searching methods which which which have which ultimately have to give the same function but you can not compare um algorithms for different functions right so  noise  so if you look at so when you look at this equality of functions computed then what you are saying so the equivalence of the algorithm is just a comparison of efficiency for for for the equivalent or for the same function let us say of the same input um input output behavior so when we are interested in program equivalence our main view would be two programs are equivalent if they represent the same function yeah  27  50  if they  32  20  are if they implement the same function so this is what might call what you might call the correctness problem ok the correctness problem is that given a function is it possible to show that a particular program actually implements that function the program equivalence problem is given two programs claiming to implement the same function so they in fact implement the same function all right so um it is only after you have gone through the correctness or the equivalence problem it is only after you have gone through the correctness problem that you can really get into the efficiency problem which program is more efficient after all what applies to algorithms also applies to programs at a at a finer level of detail which program is efficient than the other in what way of course they may be incomp incomparable because some program might use a large amount of space but do it in a very short time another program might use a large amount of time but space use very little which could be a constraint in certain situations even then they may not be comparable but at least the idea of event comparing them can only um can only come up if you are already decided if you have already proved that they both represent the same function yeah so so this is so the equality of functions is so with what you can so what we can do is we can take the implementers view of what is the implementers view of imp um so um we can take the implementers view of step by step state changes of being able to specify step by step state changes and abstract out functions from that in order to decide whether the correctness problem is been solved for a particular program the problem is that all of these have to be mutually interrelated whether it is the user s view point or the implementers view point um they all have to be equivalent in some way right so you have to be able to abstract out and prove properties about the program in order to show that the program that the program is correct against a specification yeah and in order to be able to compare two programs you can compare them only after you have proved that they are equivalent you can compare them for efficiency right so then if you are talking about the meaning of a program a program since it is a concrete object generated by syntactic grammatical rules and there are an infinite number of programs generated by finitary process of generation the question of what constitutes the meaning of a program is something that also has to be gen is also something that has to be derived from some finitary object which is the construction of the program from the syntax rules yeah so there are an infinite number of programs in any programming language and they represent various kinds of functions may be and the only way to be able to determine the meaning of a program is to be able to use the syntax as the basic frame work from which the meaning of the program is to be derived whether it is from the user s point of view or from the implementer s point of view  32  20  alright  39  00  so as far as a meaning of a um program is concerned from a implementers point of view or in fact the meaning of a programming language is concerned it is closely linked to how you are going to derive meanings of individual programs how can you derive a meaning a meaning for a individual program well it will have to be derived from the construction of the program ok so the meaning as far as the implementer of the language is concerned is what kinds of state changes is necessary in order to successfully implement each construct after all the only thing that is finitary in that programming language are the syntactic rules ok and and here and a implementer has to cater to an infinite number of possible programs that might be written by users of the language which means that he has to be able to and he has to be able specify the run time behavior of all the programs and the only way to that is to use the generation process of the syntax ok so using the syntax is absolutely essential the syntax forms the basic frame work for an implementer to specify the execution of an arbitrary syntactically valid program in the language right so the syntax forms the basic frame work but of course as i said the syntax from the point of view of an implementer is a very detailed syntax as i said it avoids ambiguity it takes precedence of operators in account it sometimes in order to suit the parsing algorithm the grammar is changed in a certain way to make it convenient to implement and so on so forth right so however there is an abstract syntax for this language which is a common platform of um understanding between the implementer and the user of the programming language and as far as the user is concerned he should be able to be able to derive the function description of the program again from the from the syntax but from the abstract syntax so so firstly the abstract syntax and the concrete syntax should produce should be equivalent in the sense that if you look at abstract syntax as a collection of trees then the concrete syntax should generate exactly those trees that is an implementers problem ok there is a an equivalence there and the the implementer should somehow faithfully um the abstract syntax is given a meaning which should be captured by the implementer and if you can give abstract syntax meaning on a phase by phase basis that means using the generation process of the abstract syntax the only thing that is finitary is the are these various product productions right so you should be able to give the meaning of each construct whether you are looking at it from the point of view of the user or from the point of view of an implementer they should be able to give the meaning of each construct in the language in terms of the syntax that defines the construct and you should be able to define how constructs are composed to form larger constructs you should be able define this the larger constructors the meaning of this larger constructors also in some form of isolation and that should form the common document between um the treaty between an implementer and a user otherwise one of them is going to yield unexpected behavior right so so there is so so what it means is that now we have already seen that syntax is a fairly complex entity and can be quite intimidating a normal use there is not going to come be able to come to grips with the syntax that is actually being used the fine level of detail which the implementer requires but the implementer so the implementer should ensure that whatever syntax he uses faithfully captures the same set of trees that the user s view of the abstract syntax syntax captures and secondly he should provide an execution behavior which matches exactly the functional behavior that the user has in mind for each construct and both the user and the implementer should be should produce equivalent functional behaviors right so only then you can avoid um  noise  misunderstandings let us say and that would also ensure that there is there is a meaning for a programming language completely independent of any other considerations ok so the abstract syntax will form our basic frame work for giving the meaning of a programming language what we will do is we will use the the abstract syntax we know very well we know how to convert abstract syntax into more concrete syntax into implementation convenient um grammars but the point is that there is there has to be an understanding between implementer and a user as to what is expected of each construct in the programming language and both have to be able to derive the final behavior of the program from the behaviors of each individual constructs in the language yeah  noise   39  00  so um  39  02  so so when we look at meanings in a programming language the various view point one is a designer view point the language designer s view point is that it is really providing a set of high level primitive functions ok um the idea of designing a high level language itself is that your original machine is too is too primitive to admit off convenient programming and what is convenient programming mean that i have some mathematical function and the machine operations are too small and too atomic for me to conveniently come down to that level and program every function in that level so what i what as a language designer what i do is i provide a set of high level primitives which may be conveniently used by some user to define those define what ever functions he is interested in yeah so the designers view point is that he is providing a set of high level functions for a user the implementers view is that these high level functions some how have to get translated into the low level functions of the machine and so the step by step state changes actually define the implementation and a user view point might be just be that look i don t care about all these high level functions and so on so forth but just tell me how i can maintain these properties even more high level properties yeah so and all these three view points have to some how gel together so we wont go into great detail about these three view points  41  00  so these  41  05  three view points form the subject of semantics of programming languages one is called the denotational semantics that is the view point of that is a purely functional view point which just says that this construct denotes this function there is an operational view point which actually is the implementer s view point it is the high level um it is the high level it is the collection of high level translations that the implementer provides to implement the um i mean um for the programming language and the last is the axiomatic which is which is what a which is what presumably one should use as a user and all these view points should some how be equivalent and the equivalence of all these viewpoints is called the full abstraction problem the main problem in the sixties and seventies is that people noticed was that um most programming languages and the problem still persists are not really do not really conform to the reference manual of the programming language or the reference manual for the programming language provided by the designer is not sufficiently detailed many implementer would read the reference manual and feel that it is not sufficiently detailed um in its reference to in describing the functionality of certain features of the language and very often the implementers had to take independent decisions ok so many semantic issues actually moved into the area of pragmatics the result was that many program a large number of programs had to be either patched up or rewritten when they were moved from one machine to another and because there was this misunderstanding implementer and designer yeah secondly where in the in the early languages there was no reference manual even what meant to us is that there was complete chaos as far as implementation is concerned and many users misinterpreted what if if there is a if there is some level of detail which is not present in the reference manual for which the implementer has to take the decision the user s view could also be that some other decision was taken i mean unless you try out particular implementations you don t know you don t know what decision has been taken right so so most of these the problem of full abstraction actually still persists most languages are defined in some loose ambiguous terms implementers are forced to take certain decisions because the language manual is silent on the issue most often it is silent on the issue which is how important to implementers um and it is often very often it is ambiguous to a user what exactly the construct represents it is ambiguous to he is not able to derive how a combination of constructs can what behavior would a combination of those constructs yield because he is not able to infer the meaning there could be one of several possible meanings that is actually implemented yeah a classic case of this even now is the ada programming language ok which is um which is fairly recent in the sense that it went through a ten year gestation period starting from about seventy six when the us department of defense felt that that the us department of defense had various installations all over the united states and of course aboard and um programs were not working as simple as that programs could not be moved from installation to another there was a variety of languages many of them the creation of committees within the department of defense itself variety of implementations for each of these languages many of them created by the sub committees of the um of the implementation committees and there was complete chaos right so when they decided to actually um when they decided that the effort of maintaining these programs many of the department of defense programs are well forty years old and they cant easily rewrite them because the price of well competent programmer is very high and so what they decided was that they are gon na embark on a unified language for all matters concerning defense programming except for the business programming so they assume that the cobol which which is also created by a committee um is good enough for business kind of environment and for all scientific embedded systems real time systems control systems they would have one unified programming language so they went through this process of design over ten years by actually deciding that perhaps they were incompetent themselves so that they should sub contract it to somebody else and if you see the ada programming language reference manual which came out sometime in the eighties is extremely ambiguous is extremely voluminous for one thing it is extremely ambiguous on various issues and there were no there were no implementations at that time so you couldn t even try out you couldn t even experiment with that but you had to take certain implementation decisions it was silent on variety of issues far too detailed on other issues and they they specified that there should be no subsets supported so the resulting document is a huge document which is very very difficult to understand the syntax is clear because but the semantics is not right so we will look at the problem of within so we will not go through all these different things at various stages in your programming life you will come across these things for example many of you may be used a functional language which is um which for which denotational semantics is very easy to specify um for example skeen many of you may used some axiomatic method at least even even if you have not used you are probably told to use them um and so we will be primarily concerned with operation semantics and that is the implementer s view point from a given from a given from a given broad understanding let us say an informal understanding of what the of what the function what function the deno the construct represents so for  49  30  each construct we will give a operational meaning however we do not want to give any kind of meaning which is machine specific or architecture specific ok because if you have to give a operational meaning again there are several grains of um this is there are several levels of granularity at which you can give an operational meaning and we would not like to be restricted by either a machine or an architecture we will just try to give meanings in machine and architecture independent forms as far as possible it used to be a common practice sometime ago to define for a for a language reference manual to define an abstract machine into which they translated each construct in an effort to give um a meaning to each construct of the programming language however more and more it is it is being realized that that does not suit a lot of people it does not users it does not suit designers it is sometimes too specific so we so a nice method of operational specification as now emerged in the last ten years which is machine independent architecture independent and is still able to give to a user um it gives a step by step it gives step by step state changes but it is also perform able to perform abstractions so that you get may be purely functional descriptions you can mix and match your level of detail in this operational semantics you can become extremely machine specific or you can become extremely you can you can become almost denotational in whatever you specify right so we will use this as a general method for specifying semantics or the meaning of constructs  noise  which for our purpose is basically means the runtime behavior of programs ok any kind of semantics of a large program has to be derivable from the semantics of the smaller constructs which make it up so which  52  10  means an essential portion of the semantics hinges on being able to use the abstract syntax trees or the grammar rules in order to be able to semantic yeah so we will most since we understand now how we can go from an abstract grammar to more concrete grammars in some fairly some simple fashion and we would not like to be restricted by we would not likely be hindered by parsing strategies or um scanning strategies unless it is absolutely essential we will just consider abstract syntax trees and specify meanings of abstract syntax trees so the compromises we make or that the abstract syntax trees any way form the common level of syntactical understanding between a designer and a user and a implementer and our operational semantics well while it is being close while it is somewhat closer to the user um to the um to the to the implementer s view can be abstracted out to provide a user s view and it is sufficiently abstract not to get into the details of the machine architecture or the machine language ok so it is it is a reasonable compromise in a general kind of course like this um to give a operational meaning which is architecture independent which is machine independent and which depends only upon the abstract syntax tree because the abstract syntax trees provide a method of induction induction on tree is a powerful method which you can use to derive meaning of complex constructs from simpler constructs yeah so we will we will define for pl zero initially without declarations um as semantics and that should familiarize you with the general method of operational specification yeah so the language of programming language is the um i am sorry the subject of programming language is becoming more and more formal um and i think we should keep up with it right  nosie  ok then thank you transcriptor  v.srinivasa rajkumar educational technology i.i.t delhi presents a video course on programming languages by dr.s.arun kumar deptt of comp.sc & engg i.i.t delhi lecture 8 transition systems welcome to lecture eight today i will briefly go through what ever we did in the last two lectures and a give a basis for something known as transition systems for um specifying the semantics of the language so let us just briefly recapitulate what we mean by what is the frame work for us operational semantics the basic  00  52  um the basic structure on which our operation semantics is to be built is the grammatical rules of the parse trees but in a more abstract form than it is really necessary for compiling that way it is simpler it has less number of grammatical categories which are um which are important for meaning and so we will take usually some form of abstract syntax trees or their representation as a context free grammar so the important thing is that while we are talking about semantics our context free grammar could be ambiguous but we don t really care because what we are interested in is only this trees where there is no ambiguity yeah so let  01  47  us also go through what we said about operational meaning so our main goal is to translate or give a meaning of of a language in to some form which is machine and architecture independent and it should not be into particular machine languages or into particular architectures it used to be it used to be the case several years ago that many languages were just defined the meaning of the construct were defined in terms of some abstract machine so a machine architecture was specified and everything was specified in terms of the running of the abstract machine but i mean these days we what we like to do is we would like to look upon the language as a separate object independent of any particular machine or architecture so we  02  45  would like to give it some something that is independent of these particularities we went through the basic syntactic classes of a language which we said were expressions commands and declarations these are syntactic classes which has specific meanings so um  03  10  expressions well just denote values that is really nothing more to expressions whenever we talk about a expression it s meaning is just a value evaluated in some suitable computational state a computational state consists of a environment which gives the significance of the names used in the computation and also may be the notion of of a store which stores values so commands  03  45  as said were denoted requests for irreversible change in state of computation so which could mean either changes in the environment or in the store an environments  04  03  i said was created or changed by new declarations and declarations are reversible and the changes in the environment are reversible very often declarations can also change the store um and let us just go back for a clarification on reversible and irreversible changes so i would say that a change so the so the analogy is really with something like thermodynamics um a change is irreversible not if it is impossible to undo the change but if undoing the change requires a great deal of work or energy to be expended ok whereas the change is reversible if it requires little or no effect to undo it so we  05  00  will look upon these reversible and irreversible changes in essentially the same fashion so very often a reversible change is one which can be undone automatically or with very little computational effort and in the irreversible change is one in which you could you might have to expend atleast as much computational effort to undo the change as you required to make the change in the first place so so um such changes we we call irreversible ok so now let us look at our specification mechanism  noise  so since we are dealing mainly with syntax with no a priory notion of meaning ideally what we would require and at the back ground we should also remember that there are pragmatic aspects like the um we should be able to implement these things um at the back ground but i mean that is not the most important thing so what  06  10  we will do is we will define a specification mechanism for specifying the meaning which essentially syntactic in nature ok in the sense that it is mostly a form of symbol manipulation symbol pushing symbol deletion and so on and so forth well these symbols are symbols belonging to a to the abstract syntax tree of the language ok since your semantics has to be firmly based on the syntax what and um this the best to give a meaning is in terms of the manipulation of symbols after all that is really all that a compiler does so if you look at a compiler or a interpreter it really does nothing more than so there are some languages um whose first versions specified that the meaning the this language is exactly the compiler designed by us ok so but there are oblivious pitfalls to such things i mean for one thing that it is it is terribly source language dependent it is terribly implementation dependent it is and if the language was not implemented in some bootstrapped fashion it is also extremely machine dependent right and the other possibility is to just give a collection of algorithms for the various constructs of the language but even that is too implementation specific it is too pragmatic it might be for example um no collection of algorithms is complete without a collection of data structures which are manipulated so which means that you are really designing an abstract compiler or a abstract interpreter which is again a terribly pragmatic thing and it is really of no use to most people who are going to be users rather than implementers of the language so what is being done currently is to write a fact volume explaining the meaning of the language so most most languages really have a huge reference manual which explains the various constructs in pity less detail um but they try to be abstract in the sense that they are not algorithmic they are not machine specific they expressed in some natural language um which also has it s own pitfalls so if you look at these three methods that are that have been used and are currently being used then what you find is that any thing that is extremely pragmatic like a compiler or an interpreter or um a set of a set of algorithms has it has a danger that firstly it doesn t give the meaning of the language because it specifies how the language is to be implemented rather than what a construct means in the language secondly when we look upon meaning of a language we we regard that as a abstract mathematical object in itself it is not it is not necessary for it to be related to any particular method of um any particular method of implementation or any particular method of reasoning yeah  10  05  so and  10  09  further a compiler even even for a small language for pl zero runs into approximately ten pages if the compiler were more user friendly it would probably run to more than twenty pages of of code of actual source language code we just which makes it not to concise and because the pl zero language for example is is a very simple language and it is not really necessary to use the full detail of a compiler to explain the language and so and the important and the lastly of course what all this means is all this detail means that their all very very abstract they are not abstract enough there are too many details which are irrevelant to user for example it is too architecture specific may be even if it specifies an abstract machine with a interpreter for it it is still architecture specific yeah and the main problem with this three hundred page volumes that you get is that  11  15  natural language  11  20  is by it is very nature something that was not designed but evolved with shades and vance s some meaning very often this shades and vance s of meaning are geographically specific so and natural languages inherently ambiguous it is too verbose but of course the point the advantage it has is that it really that it does convey a lot to a lay person yeah and you can use natural language to specify um the what what does a construct mean rather than how is the construct is to be implemented how is the language to be implemented so um so currently most so so there is no getting away from a riding a manual for the language in some natural language ok that that is something that has to be there for a for a lay user for example but however i mean that still does not specify the meaning of a language in an accurate fashion right so what  12  45  we will use is something known as transition systems yeah so transition systems for one thing they are mathematically precise and that in itself is enough to put off a lot of people um but they have the advantage that they are sufficiently abstract they specify the what rather than the how aspects of a language and there are no ambiguities and it is also a extremely concise method of specification a lay person would probably call it as two tiers and completely in comprehensible may be but it is accurate and what you require is something that is accurate an accurate definition of the language is some thing that is required ok and the other advantages of this particular method of transition systems is that it actually exploits definition by induction or definition by recursion and it uses induction on the structure of the syntax tree to the fullest extent to specify the meaning of any construct so what that means is that if you have a definition by induction then you can also use the principle of mathematical induction to reason about programs in this method right so and more more important than any thing else is the fact that actually it is not as intimidating as the lay person would think of it is actually quite general in many ways as a specification method because almost anything that you can think of could expressed in terms of what are known as transition system yeah so um  15  00  so for example i mean you could um well and it s it s generality goes to such an extent that very common every day problems could also be specified as transition systems you could specify hard ware for example as transition system and actually any of any person who has done a hardware course has actually delt with transition systems the move machine the meeting machines and so on are actually transition systems of various kinds right and um more importantly it allows you to abstract away from details so you can you can it allows an abstraction which also which also is helpful in defining our equivalences in fact you can be so abstract if you like with your transition system definition of a language that you could take a very small detail you could also give very detailed analysis to the extent of the every of every step a runtime system will take or you can go back to a you can abstract away from all these details and give a larger picture which is which is denotation which allows you to define equivalences which allows you to take a purely functional view of whole programs or whole program units yeah so transition systems are really quite general in that sense and it is a pity that it is not being taught in schools may be um  16  40  so so  16  41  let us let us go through some elementary definitions so what is a transition system a transistion system is really nothing more than um it is an ordered pair it is a structure if you like um consisting of a set gamma this is the greek letter capital gamma and a arrow relation where the gamma is called the set of configurations and the arrow relation is called the transition relation yeah so this arrow is a binary relation on the configurations and we will usually use it in a infix form so given two configurations gamma and gamma prime we will say that gamma moves to gamma can move to gamma prime yeah so the notion of the configuration is of course at this point it is not being specified but that is something we will have to specify when we actually get to the programming language the notion of the configuration in a particular programming language will also depend upon the upon the upon the upon whether if for example it is a declarative programming language or it is a functional language or a or it is an imperative language so the notions of configuration will differ depending upon what kind of language it is but largely all imperative languages will have a similar notion of configuration all functional languages would have a similar notion of configuration all declarative languages like logic programming languages would have it would have a similar notion of configuration right and you can think upon any you can think about any transition system as essentially being some form of a multigraph except that where the configurations are the nodes of the graph or the vertices of the graph but the configurations need not be finite ok and neither neither is the arrow relation the transition relation finite see if you can think of allowing arbitrary kinds of directed infinite graphs and in the case of what we will call label transition system you might they might be even multigraphs if you allow for arbitrary kinds of directed infinitary multigraphs then you get a transition system in fact any directed graph can be thought of as another transition system and your representation of more and merely machines is really the form of directed graphs ok except um with some special states a special start state and a special and a few special may be um final states there are all this additions um additions to the structure of the graph but essentially it is a graph you can look up programs also as essentially as graphs only since programs are capable of infinite behavior graphs will have to be infinitary ok you can look upon programs as graphs in two different ways you can look upon programs as just trees as abstract trees which are also graphs we but we um purely syntactic matter a program is a finite object in a in terms of its text so the syntax tree is also a finite directed graph the runtime behavior of the program is such that it could go through a infinite number of states so the runtime meaning of a the mean the meaning of a program in terms of its behavior in execution could be a graph with infinite number of nodes but directed all the same yeah so our notion of the transition system is just essentially the notion of our directed graph where we don t put any finiteness conditions on the nodes in the edges yeah so just as you can have label graphs that is where the edges have carry labels with them you could also define label transition systems yeah um as in the case of some of the transition system you already done under other names we can define also the set of you can  21  55  also define a terminal transition system as a transition system in which there is a subset of configurations t which are called halting configurations or terminal configurations yeah so you  22  40  could go further for example and define um a label transition system as a labeled transition system as something that also carries labels yeah so there is a collection of configuration grammar there is a set of labels which i will call l and there is a transition relation ok so a three tuple in which the set of labels there is no relationship necessarily i mean the set of labels need not necessary be disjoint from the set of configurations but what happens is that corresponding to the notion of the labeled directed graph the transition relation are ternary relation of this form and and we would say that given a given a label let us say and two transitions gamma and gamma prime we normally would write gamma goes can go on a to gamma prime yeah correspondingly  24  17  you could  26  50  also define terminal label transition systems yeah so a terminal label transition system would just contain you just four tuple of this form where this is a ternary relation and t is a specified subset of gamma is a set of halting or terminal states yeah so this is this is a terminal labeled transition system right and most things almost any kind of computational mechanism that you can think of can be represented in the in some kind of a transition system either just a transition system or a terminal transition system or a label transition system or a label terminal transition system and how you represent it in a transition often might depend on your view point so we could look upon for example regular grammars the fact that i said there exists a finite state machine which recognizes the sentences of a regular grammar that finite state machine is really nothing more than a label transition system yeah you can have finite state machines with outputs with every input which yield an output then all that happens is that your set of labels becomes um it becomes slightly more complex it consists of input output pairs of symbols yeah so this is a fairly general notion and actually we we think of our most of our computations in the form of transition systems how do you go from here to there right so let us let us take a simple example um since you already know um that regular grammars are equivalent to finite state machines what i thought was let us give a transition system definition for context free grammars um ok so here is so in general and you can look upon it completely generally  26  50  so we  26  52  will say that so let g be a grammar which consists of um collection of non terminal symbols and they are all finite um except t of terminal symbols a set of productions or rewrite rules or replacement rules and a start symbol s right so the set of configurations that we are talking about could just bes like the set of all words the set of strings from nut n union t yeah and the initial configuration ok is is s id the symbol s and the transition relation unfortunately the same arrow is used for the rewrite rules and the transition relation yeah but let s let us may be make it distinct um should i um ok let s let us make it distinct so the since i have written the grammar in black the production rules i will write in black ok the the transition system um the transition relation in the transition system gamma arrow is just defined as for each production of the form a replaced can be replaced by alpha in the production set you have the rule that for any string for any two strings beta and gamma beta a gamma can become beta alpha gamma the set of productions is finite so the set of rules that you have is also finite where the beta and gamma are considered completely arbitrary so for any particular context free grammar this is you can just think of it as a represent as a representation of um as representation of the context free grammar in the sense that the transition system defines exactly how your derivation should be done defines all possible ways of deriving terminal strings where by terminal i mean the set t of terminal symbols in the um in the grammar the set of configurations or the set of terminal configurations in this transition system is just the set of all possible terminal strings right so this transition system which is written in blue um this is t the blue t here represents the fact that it is a terminal transition system of this form gamma arrow t yeah so this so this t of the transition is just the set of all um the set of all strings in this in this from this t which is the set of terminal of the context free grammar yeah it is unfortunate that similar um symbols are used everywhere but i hope that the color the color makes the difference um ok so um so so essentially in a it is a it is a terminal transition system so we require these three things to be specified and this relation is completely specified by this by this rule which just says that for any beta and any gamma beta a gamma can go to beta alpha gamma if a replace by alpha is the rule in the production set of this context free grammar is a is a production in this set p and if that is so then this relation the blue arrow is a transition relation which gives you the set of all possible derivations in this context free grammar so we we we in the case of a context free grammar of course the symbol s has a special significance so what it means is that you can add initial states also as part of a terminal transition system yeah so so depends on what kinds of details what kinds of distinction you have to add but essentially the basic frame work is that of a transition system yeah so all these and if you you could actually go further um take your ruler compass constructions in geometry you can look upon each ruler compass construction method itself as an algorithm and what you are doing when you when you do this construction let say let us take a simple problem like constructing a regular hexagon so the method starts with um taking a circle of certain radius choosing a arbitrary point in the circle and with that same radius marking out the six vertices of the hexagon and completing the hexagon ok each of these stages is like a transition the first step of the transition is the initial the start state in this case of course is a blank piece of paper and then you move to a circle then you move six through six steps to marking out the potential a vertices of the hexagon and then you move through six steps in completing the hexagon so they are all transitions so you are essentially they are the steps of an algorithm being executed but the point is that if you look at all of this transitions as snap shots in the process well it is just it just specifies what doesn t specify how the level of detail could be so could be so high that it might actually the how the how might be implicit but essentially it just specifies what if i just take these i don t know fifteen snap shots of the construction of the regular hexagon it just specifies what it just specify transitions so let us take a simple programming example so here  34  20  is a here is a towers of hanoi problem ok we just um i presume every body knows the towers of hanoi problem you got three towers um and you have got some n you have got some n pegs all of different sizes and the rules normally stated very solemnly are that a um all the pegs are of different sizes and a smaller peg a larger peg can not sit on a top of a smaller peg yeah can not please word the um note the word can not another rule that is state is that if you got a pile of pegs then you can move only the top peg you can not move you can not move any other peg from inside the pile and and the problem is to transform um given that these are the rules by various moves to get to get his set of um starting from an initial configuration in which the n pegs are stored one above the other such that smaller pegs always sit on the larger pegs to move that entire collection to another peg alright to another tower right i mean let us quickly go through one execution of this  35  58  so here  36  00  you have some initial configuration so this include these are the three towers and the initial configuration is that i have just taken three pegs um is that you have three pegs which are numbered according to their size sitting on the first tower and the final thing is to move them all to the second tower right so you can through a sequence of transitions of this form where the next state in the transition system is that from here you can move to this which is equivalent to saying that take the top peg in the first tower and put it on the second tower and you can move from here we just take the top peg in the first thing and move it to the third and um well take the peg from the middle one middle tower and put it on the third take the peg from the first tower and move it to the second take the top peg from the third tower and move it to the first take the peg from the third tower and move it to the second and take the peg from the first tower and move to the second yeah so this is so this is a step by step specification of an actual execution of a towers of hanoi program of a of um one possible algorithm clear however the question is how do you formulate rules for example this execution does not tell you that you couldn t have moved this three on top of the two or on top of the one the important thing now for us is not to specify complete execution behaviors because for example a programming language has a infinite number of possible programs and you cant specify their execution behaviors completely so what you have to specify is rules and you have to be able to claim that any particular execution is some thing that follows those rules the productions of the context free grammar essentially are rules or at least in our in our definition of the context free grammar as a transition system they are they act as a hypothesis of rules which give you how derivations are performed ok note that they actually tell you how derivations are to be performed but they do not tell you how derivation should not be performed and there is something in our in the way we stated the problem of towers of hanoi and the way it is stated in most books for example but there is this can not to hold business of can not you can not do this is something that is an integral part of the problem as it but in most of mathematics where no body ever tells you what you can not infer they only tell you what you can infer it is under stood that you don t have a theorem as impossibility unless you can show then it is impossible with what you can do to prove that theorem um this is absolutely important there is something in our natural language specification in which we specify what you can not do and what i claim is that in most of mathematics what you can not do is never specified explicitly ok so the rules of a so we have to design rules for the towers of hanoi problem the specification of a problem without using this can not you should only specify what you do right so  noise   40  25  so what  40  28  that means is that you could for example specify a rule like this if i had a sequence of pegs let us say s some there is some sequence s here and there is some x here there is some t here and there is a y on top there is a u here and z on top um this x y and z denote the top most pegs yeah s t and u denote arbitrary strings in the pegs ok so you could specify this is perfectly fine you can specify a rule which says that if x is less than y or rather if x is smaller than y if you like then this peg from here can be moved on top of the wire um but you can not specify the rules like this so here i have slashed the arrow to specify that it can not move right so our natural language definition of the problem said that you can not for example move a larger peg on top of a smaller peg and this is essentially um a symbolization of that but the whole point is that you should define the rules in such a way that you don t have to do such things afterall what you can not do with a program is really infinite what is finite is only what you can do with a program ok so if you going to keep specifying what all you can not do you have a infinite number of rules and it is not clear that you will ever proceed with execution yeah yeah so what you have to do so our rules will always specify except when you are informally talking about things our rules will always specify what can be done and never what can not be done this is perfectly in keeping with for example logical inference you don t have you don t have a rule which says that from a and a and b you can not infer not a ok you don t have such rules the fact that you can not infer not a follows from various other things it follows from the notion of consistency or inconsistency of a logical system of a axiomatic system what logical system just specifies is what you can infer and never what you can not infer what you can not infer acquires as an impossibility of proof yeah so so this is something that we will follow also in our specification of the language note that pragmatically speaking also it is a nice way of specifying things anything that goes against what you can do is an error it is as simple as that right so you generally never there is of course there are of course certain cases in natural language where this kind of reasoning does not work i agree with that so very often in natural language you have to specify what you can not do but um when you are talking about programming languages and formal objects programs algorithms algorithms are not very formal but they have their representation as programs ok they are all formal objects very highly structured mathematical objects so it is possible to get by with what you can do and assume that what ever you can not do is something that follows as some larger consequences of what all you can do ok so if i were to give rules for the towers of hanoi as a transition system what i will do is i will not exactly specify all the possible configurations or i will just specify the arrow relation the transition relation ok and it is always nice to have a concise a precise set of rules which and a small set of rules if possible  44  54  so what  44  55  i will do is i will take i will take an index i um from zero one and two um i could be arbitrary belonging to this set and whenever i deal with plus or minus or whatever it is always modular three right so what i can say is and the three towers um are actual executions specified a order for these towers the first tower second tower the third tower but the whole point is that for me now i doesn t matter in what order i keep these three towers if i name them zero one and two then i can use modular three arithmetic to just call them si um call them i i plus one and i plus two i could take values from zero one or two ok with modular three so then i can look upon the contents of the three towers as a set of three elements and the set of all possible such sets is is my set of configurations there are specifications about what a initial configuration is and what a terminal configuration is ok but that is that is that goes beyond the that is part of the problem but it is beyond the the actual notion of the transition so regardless of what this um regard less of what this i is regardless of what order the towers are placed i can specify this rule the first rule says that supposing there is a supposing the tower i whatever may be i is has is non empty that means it has at least one the top token the top peg is called a and i plus one is empty if if the tower i plus one is empty then the top peg a from i can be moved to i plus one yeah similarly if i plus two is empty the top peg a could be moved to i plus two note here that we are not putting any particular we are saying that both these are allowed which means starting from some initial configuration it is not clear which one you are going to do you could do either of them for example if you had one tower with which contains all the pegs and the other two towers are empty either of these rules can be applied you can move the top peg either to one tower or to the other tower so it is important to say that it is the arrow relation in a transition system does not specify what does happen or what should happen it specifies what can happen it does not preclude it does not preclude other possibilities yeah so let us go through the next one so we require a we require this something like a base case of an induction ok because you can not compare the top element of an empty tower with a top element of an non empty tower or something ok so then then it is very simple if i have if a is less than b and b is a top element of the second tower then a can be moved on top of b ok and similarly if a is less than b and b is a top element of the first tower then this a can be moved on top of b no where how i specified what can not be done these rules specify exactly what can be done alright given a initial configuration you can verify that this execution we have got actually satisfies in each  49  10  each transition  49  15  here is an application of one of these four rules ok so that is the important thing about transition systems it is in fact part of any kind of mathematical subject you never specify what might be loosely called negative um facts yeah so um so i hope this is convincing enough i mean you can even take some purely functional program like a factorial program um the factorial program by definition is an equality is a mathematical equality but the unfolding of let us say a recursion can be regarded as a one way rewrite rule as a transition so you can take the factorial program take your definition of the factorial program and go through the sequence of transitions for any given value of a factorial of n will go through a sequence of transitions which give you different configurations the notion of a configuration there might be a expression itself the unfolding of the recursion yeah ok so we will actually start on specifying the programming language pl zero in the next class thank you transcriptor  v.srinivasa rajkumar educational technology i.i.t delhi presents a video course on programming languages by dr.s.arun kumar deptt of comp.sc & engg i.i.t delhi lecture 9 p l 0  expressions welcome to lecture nine so today we will define semantics of expressions of pl zero we will take a subset and look at it in some detail before we go on to other things so let me also briefly recapitulate um the notion of transition systems and so on which we are going to use with this so a  00  51  transition system is just a collection of configurations um along with a binary relation called the transition relation and the transition relation um really tells you what is possible not necessarily what actually happens yeah um so um this and we could regard a transition system as some kind of a infinite directed multi graph um in general we could  01  28  we could define initial states um final states final configurations initial configurations and so on and enhance the distinguish ability power of a transition system so a terminal transition system would typically have a collection of terminal configurations which are a subset of a the configurations um you could also decorate the transition system further depending upon the upon the use you are putting it to you could for example define a label transition systems which are which in  02  00  to being transition systems have a collection of labels associated with them this would correspond to labeled um infinite directed multi graphs so the arcs of labels on them um and the transition relation therefore is a ternary relation um which and we would typically um if al is a set of labels then you would say that gamma can gamma prime on a um ok so the question of what so all these the notion of configuration the notion of labels um is not is not is def is loosely specified in fact that is i mean they are really undefined terms and that is in fact what makes the whole structure generally applied i mean you could interpret the notion of the configuration to suit your convenience you could interpret what constitutes a label to suit your con convenience and that is what makes transition systems very general method a general tool if you like a general mathematical tool for specification yeah so um you could also define let us so you could enhance these things further you could  03  25  define terminal label transition systems you could define label transition systems with initial configurations terminal label transition systems with initial configurations and so on right but the basic idea is that of movement from one configuration to another and depending on your distinguishing capability you could enhance or decorate them with more constraints yeah so we  04  00  already saw one example of um context free grammars represented as a transition system with a rule which really is says that it is a context free rule of replacement ok for for any beta and gamma if a goes to alpha as a production in the context free grammar then b a gamma goes to b alpha gamma we will use this notation which actually comes from well it comes from logic but the um the way to look at look at it is as a production as a as a rule which states which gives a hypothesis and what you can conclude so if a arrow alpha is a production in the production set then b a gamma arrow b alpha gamma is a step in a transition yeah or in a derivation yeah so um  05  10  we also looked at the towers of hanoi problem so in this case we saw how the how the how a particular execution of the towers of hanoi problem would look like a particular execution ok so that is specifying the run time behavior of an algorithm so this so this really specifies the runtime behavior of some particular algorithm which is actually un specified ok so it is like an execution behavior and you use the transition system to define the execution behavior of some algorithm or a whole class of algorithms which support these possible this possible sequence of transitions yeah not algor not all correct algorithms for the problem might actually support it but there is a whole class of algorithms which support this so what we like to is abstract away the essentials of what allows these transitions from so the problem of what as supposed to the problem of how is what we want to abstract away by the rules so the problem itself the problem itself can be stated as a collection of rules the execution behavior can be stated as a collection of rules given a algorithm you can follow its execution and represent that execution as a transition system given a problem you can look look upon the problem also as a transition system provided you give suitable rules yeah so um so one so the so this is this is not one possible solution of the towers of hanoi problem because the problem itself is to design an algorithm which will allow you to solve the move n pegs from one tower to another ok so the problem is just that given this initial configuration how do you get this final configuration and it is not um this is just one possible solution one possible execution of one possible of a few algorithms right so the actual rules themselves of the problem which again do not constitute an algorithm they constitute a statement of the problem is given by this right so um  07  45  which essentially say that you can move a the peg on the top of a tower to an empty tower to one of here two tower there might be both tower two towers might be empty and so you could move the peg to either of them ok similarly if the top of a tower um so if a is less than b and a and b are on different towers then you could move a on top of b and um this this also includes the fact these two rules together include the fact that if you have si a s i plus one b s i plus two c where a is less than b and a is less than c then you could apply any of these two rules and put a either on top of b or on top c ok so our transition systems are are inherently non deterministic ok so um  09  00  and that is because the the transition relation is a the transition relation is really a relation and not a function ok if it is a relation then if it is a if it were a function then the possibility of a configuration moving to distinct configurations would would not be allowed but the main point is that many problem statements um the if you notice the execution that we have chosen is one possibility out of a variety of different possible executions ok for um for a whole class of algorithms and so as a result and not and more over non determinism at least in a large part of computer science is merely a form of under specification an under specification in the sense that um you are not extremely concerned sometimes about whether you pursue one path or another right in the statement of the problem it does not specify to you that um from the tower i a peg can be moved only to tower i plus one it can not be moved to tower i plus two no where in the problem is that stated and this is never stated so as a result the problem statement itself is such that it is under specified so so that it allows you a variety of possible solutions and um and so um non determinism is an implicit fact of life in any kind of succinct problem definition in in many cases it is also a fact of life of the solution because you are interested sometimes not in what order you perform certain things so if i asked you to find the fact find all the prime factors of n number then for example it doesn t matter if the number has several i mean is um say some two to the power of four multiplied by three to the power of six it really doesn t matter in what order you find the four two and the six threes so you could have a you could define a solution in which you find the first two and then you find a first three go back find two more twos and find three more threes the fact that you write an algorithm in a deterministic programming language such that it might first find all the tool factors it might just factor out the twos and then start with all the threes but that is just one possible solution there is nothing to prevent you from having find the two and then finding threes and then go back to finding some twos more right so we should even your solutions pairs could be non deterministic depending on the level of detail you want to put in so non determinism is an intrinsic fact of life both of problems and of classes of solutions for the problem so um the problem statement in the case of towers of hanoi is clearly non deterministic right because it  12  35  allows the application of one of two possible rules at each go in fact if it so happens that one tower is empty and another tower has an element b smaller um at the top which is um which is larger than a then any of the four rules could be applied may be um three out of the four rules could applied or rather i am sorry i am sorry that is wrong at any time two of the rules two out of the four rules could be applied so in that sense it is dist it is distinctly non deterministic yeah so if you were to generalize the problem to include many more towers for example then you know you could you might at some um some point find that all four rules could be applied but you always choose one possible but that is not a unique solution right so how ever the important thing is that tran since transition systems are a general formalism they allow non determinism however our programming languages are deterministic all the programming language that use are intrinsically deterministic which means in your semantics you have to be able to show that there are only deterministic behaviors for a  14  20  given program there is only one possible um behavior and and showing that means that proving that your transition relation is a function ok so and very often this is something that will be that will be that you will have to show by induction on the structure because we are going to use the syntactic structure to a to a great extent yeah the next  14  40  important thing that i said was that whenever you specify rules you specify well what might be loosely termed positive rules and no negative rules i mean you never specify what can not be done what transitions are not possible what ever transitions are not possible is inferred by default if you can prove that with the existing rules it is not possible to get a transition of this form it is always inferred it is not some thing that is specific ok so you always specify i mean what what are i mean they are usually called positive rules you don t specify what can not be happen right so let us look at the expression language of p l zero yeah so um so i  15  40  had previously given the syntax um which i called was ambiguous but now i don t care ok let it be ambiguous what i am interested in are a collection of trees rather than actual expressions it is just that in order to be able to write them out i might have to write them out as expressions may be use brackets what ever but really what i am interested is a collection of trees ok so what we will do at this at this moment is that so we will we will deal with this i mean we with these operators however ambiguous they are we know how to get rid of ambiguity for the purposes of parsing and compiling and so on and so forth but we are interested in specifying a semantics which is not cluttered up with all those non terminals which do not have great great semantic significance so we will deal with this ambiguous grammar but whenever we write an expression it is understood that we are writing not an expression but a tree yeah so we are really interested in the syntax trees um in an abstract form and we are not really interested in the what might be called the lexical properties of um what ever grammar we write this way the grammar is more succinct more precise more precise and this way the grammar is actually um part of what is know as a term algebra ok any way we will not get into that so  student  question  of course of course  student  question  no but the point is that i am going to do induction on the structure so if you want you can assume that i have a i have a fully parenthesized notation to get rid of ambiguity ok his question was that you know you can if your grammar is inherently ambiguous then you have more than one syntax tree for an expression and that can change the meaning that is true but what since i am not interested in the lexical aspects when i say i am interested in syntax trees what it means is that you can transform that syntax tree into a fully parenthesized notation using brackets ok what i am the my um my rules the rules of the game of semantic specification are going to based on syntax trees in the sense that there going to be induction inductive rules based on what is the root operator of your of the expression you are interested or of the syntactic category you are interested in yeah so this you can look upon this as you can look upon this expression either as a fully parenthesized expression of the form e plus e with brackets or you can look upon this as a tree whose root is a plus operator and there are two it has two um two sub trees which let us say which are called e one and e two whose internal structure i am not really worried about um  19  40  so each of these each of these expressions you can think of as being specified in this form where e one and e two themselves are trees whose actual structure i am not interested in that is how you do inductions on trees right you right you do an induction how how is that tree defined a single node is a tree given two trees t one and t two and a node with some label on it the tree formed by t one as left sub tree and t two as right sub tree with that node as a root is a tree that is that is precisely what this is given e one and e two as trees with a plus operator as a root you get a tree so trees are defined inductively and we will look at the inductive structure of the trees and not at the actual lexical syntax if you want to clarify the lexical syntax for yourself you can look upon this as representing this expression right fully parenthesized if you like right so we are not going to be interested in ambiguity you are not going to be interested in lexical matters we are not going to be net picky about those various non terminals which were important for parsing like terms factors and so on so forth you are just interested in expressions as a category right so so let us so let  21  30  us assume that we have got a collection script e which is green in color because it is a forest of trees so it is just set of all abstract syntax values of integer valued expressions for the present i am assuming for the present i am assuming that there are no identifiers because we can deal with identifiers only after we have come up with a notion of an environment so we will do that later for the present just assume an expression language which does not contain any identifiers and which  22  14  does not and since we are not interested in bracketing it does not contain this production either so it does not contain this it does not contain this going back to pl zero identifiers um the identifiers in an expression could either be variable names or there could be constant names ok but now when i get rid of when i get rid of these two productions what i am saying is just i just have a pure expression language expressed in terms of the numerals ok so the roots or the leaves of all the trees are just numerals ok so that is that is my initial assumption because identifiers really require the notion of an environment which i don t want to get into now let me first tell you how to give a simple transition system for a for a language of just expressions on numerals yeah so um  23  30  this brown n denotes the set of all numerals in the underlying machine by the um i mean the underlying machine is quite down to earth so it is brown in color and by the underlying machine again i am not saying that it is the bare hardware it could be a virtual machine which means that as in the case of pl zero compiler the pl zero compiler is written in pascal so the virtual machine that you have got is a pascal machine so it is it is a bare machine with a layer with layers of software the outermost layer of which which forms the interface for the pl zero compiler is a pascal machine i mean so the um so it gives a view that what you have got under underneath is just a pascal machine with capabilities of with all the capabilities of pascal and nothing else yeah so there is this underlying machine which might be either a real or virtual ok but the important thing is that i am not interested in its architecture yeah i am not interested in the in the details of the machine except in so far as it gives me certain computational primitives that i can use ok so in the case of an expression language the basic computational primitives that you might require are well addition of integers subtraction of integers multiplication of integers and so on ok so assume that those basic operations are some how implemented either in hardware or in software in the underlying machine which itself might be either virtual or real ok so so whatever i whatever is written in brown is going to be the underlying machine virtual or real is going to denote an entity which really belongs to the underlying machine and whatever is written in green denotes this language a syntactic category of the language that we are interested in yeah whatever is written in blue is part of the transition system ok so so the  26  00  transition system for this language with no identifiers which just works on numerals is a consists of a collection of configurations gamma e which is just the collection of all integer expressions so which is just the set e script e the terminal configurations are all the numerals yeah and fors for brevity i will use this little o to denote a binary operation so it could be one of these which are the which are the binary op integer operations any way in the pl zero machine ok and our underlying computational model assumes that these operations are already available is not not a very unreasonable assumption ok they are anyway already usually available in hardware so i don t make any assumptions about the architecture except that i am assuming that these such op the corresponding operations are already available in the underlying machine and that greatly simplifies my transition system if you want what you could do is not assume that these operations are avail available in the underlying machine but then you would go into greater detail about numeral representation how addition is performed in terms of i don t know if you assume your numeral representation as purely binary how how is addition performed in terms of um well the boolean operations and or and not so could you could assume a underlying computational model which just consists of the boolean operations and essentially give a um minimal specification of what an an addition algorithm of how an addition algorithm is implemented using those boolean operations well but then that goes there is something you could do in your architecture or organization course yeah but at this moment i am not interested in that i mean i would like to keep things simple for one thing and it is not unreasonable to assume that these basic operations are already available in the machine and so the um the basic operations are already available in the machine um will represented in brown so in particular for example i would have i might i might make a statement like this so this plus belongs to the language of expressions that we are interested in that is why it is green this plus is the operation in the underlying hardware so it might either be implemented directly in hardware or firmware or software i don t really care and i don t really care how it is implemented as long as it is available and the effect of performing this um let us say hardware operation on two numerals this some other numeral that is a p right so then this is a so this transition is something that acts as the basis for the specification of your transition system you could go further and not not assume this and further specify how each of these operations is to the permitted ok as i said a transition system method can you can go into great detail about everything you can also perform abstractions here we  30  20  have we are performing an abstraction with this assumption yeah so so the rules so there are just three rules um well not three three rules so the rul the basis rule is that assuming that um assuming that this operation is one of the three opera one of the four operations which is already available in the machine is is available then this transition note that note one of things that we said was that we are looking upon this specification of a language as a form of symbol manipulation that s that is how it can be operational ok that s so so one thing is that um so i will assume that given two numerals um if a given two numerals m and n if m binary operation n gives me a numeral p then in the in the expression language it also gives me a p ok and what we are more interested in is in the high level language syntax ok but this forms the basis right so if e can go to an expression e prime then m binary operation e can can move to m binary operation e prime this is another very simple rule ok and if e one can go to an expression e one prime then e one binary operation e two can go to e one prime binary operation eight ok so and in fact this these are all the rules that you require for this simple language it is a succinct and clear specification but it contains a great deal of information yeah for example ok  32  50  let us  34  10  let us do a little example so so with these rules so consider this expression which has to be evaluated um going back to what we said about expressions as a syntactic category um expressions just denote values or an evaluation mechanism right i mean there is no concept of store there is no concept of environment there are no complications associated with the expressions but i have taken this so that it it illustrates the concept of what are the transitions what transition system rules look like and what kinds of information you can drive from them right so if you were to take this tree um so what happens is that i can derive um so from here this moves by a purely tree manipulation process to this tree which is essentially that this two plus three has been evaluated and this subtree has been replaced by this node ok yeah and if you move further then what you have is this tree  34  13  actually becomes this tree ok so the only the only change is that that this one plus four has been replaced by five and this so you go through this sequence of transitions ok but the thing is that this example is not actually is extremely illustrated so these four steps actually are obtained in a form i can claim that this step this transition occurred because i can claim by rule e one this and therefore i can claim for the entire expression by rule e three that this that this occurs ok however for example i can not for example consider an execut an execution of this form this is  35  08  absolutely faulty yeah and why is this faulty um pardon no he says it allows only one expression no it is not true why is this faulty  student speaking  no one plus four is an expression by itself  students speaking  no  student speaking  but why should this o that you are referring to be minus why cant it be star  student speaking  um ok  student speaking  sure somebody convincing to me oh what you are saying is that i should have gone through several steps ok what you are saying is that my first step should have been one plus four foes to five five minus one plus four goes to five minus five and two therefore two plus three star five minus one plus four goes to two plus three star five minus five this is is it is it just that is it just that a step is missing in between or is this something else yeah pardon no forget about the parenthesis the parenthesis is just um well it is just a pure lexical matter it doesn t matter the parenthesis is just a way of specifying linearly what this syntax tree is supposed to like  student speaking  no no this um rules donot give complete evaluation they give you a rule for a one step evaluation assuming that the underlying operation is available  student speaking  absolutely correct so what he said was if you look at the rules it you look at the rules the rule  38  30  e two specifies that you can take a right hand side operand you can move a right hand side operand only provided the left hand side operand is a is already a numeral  noise  this the rule e three does not specify for example that you can take e two to e two prime and therefore you can infer e one binary operation e two goes to e one binary operation e two prime nowhere is it allowed and as specified only positive rules so what ever is if you can not derive what ever is not allowed well then it is not allowed that is so so for example so this so this kind of a rule ok so a  39  35  rule like e six is essential if you want to be able to perform that kind of reason so the so the faulty reasoning that i showed can be rectified if you add this rule e six which allows a right subtree to perform a transition in a whole expression right but supposing you allow rule e six ok then what is the problem supposing you ok so you want that you want that kind of reasoning that we  noise  like that we decide to do as faulty because it is because it is the rules did not allow it should also be allowed so you add this extra rule which allows a right operand also to move then what is the problem  student speaking  no it is not true no no ambiguity is a syntax oh you have non determinacy not ambiguity ok then what happens is given that given a complicated syntax tree there are there are a variety of possible transitions it can take ok and you no longer have deterministic behavior you have non deterministic behavior in an expression language if you allow this kind of non deterministic behavior then what can happen supposing you extend the language to include functions this that high level functions what can happen if you allow non deterministic behavior and it is a imperative language and you will allow functions and you allow side effects then it is not even clear that you are going to preserve any semantics the results that you get from a program execution will be completely dependent upon at at that point in what order the runtime system chose to evaluate the expressions ok atleast from a pragmatic point of view addition is of course commutative or what ever but at least from a pragmatic point of view um it is necessary to be able to clearly specify an order of evaluation at least in a expression language so that there is not there is no confusion later when you expand the language out i um i might take the pl zero language and add more facilities to it i might add functions to it ok since you donot loose any thing by specifying an explicit order of evaluation it is it makes it convenient pragmatic and decent it does not give an implementer arbitrary choices or it does not produce conflict in interpretation right i mean you could add for the brackets and so on since people mentioned it i mean you could add more rules but the whole point is that it is it is not necessary ok it very often it very often just clutters up the matter at hand so the way one possible way of defining this semantics in a deterministic fashion and determinism is very important from a pragmatic view point from the point of view of an implementer of the language it is also very important for a user in case the language allows side effects right so if in fact that is the problem um with many of the pascal many pascal implementations you will see in in a tumble pascal environment for example you actually can get a choice of menus on how you want an expression to be evaluated in what order you want expression to be evaluated but the manual itself is silent on that matter it is and we should realize that especially in floating point arithmetic for example addition is no longer commutative um not even associative multiplication does not distribute over addition or subtraction we will loose a whole amount of mathematical dogmas you have brought up with when you do actual floating point computation so sometimes just for pragmatic reasons it becomes necessary to lift these constraints to the level of the language definition and at the level of language definition you really  45  05  do not loose anything by making it deterministic it only helps the user and the language implementer to decide on a specific order yeah so adding extra rules which allow non determinism may also be very very faulty sometimes it is open to a variety of interpretations yeah so in  45  40  this case we have played y having just three rules we have we have played absolutely safe it allows only a left to right evaluation of the subtrees of the trees so till you have reduced the leftmost subtree to a single node you are not allowed to proceed with any of the right subtrees that is what these three rules specify and they specify that by default what you can not do is being specified by default right so when  46  18  you add a rule like e six then you allow also simultaneous um for example evaluations or independent evaluations of right sub trees but but these things but adding such rules in order to presumably give more power to your transition system may not be may not be may though value intension might actually lead to flaws in the semantics of the language flaws in implementation if you are underlying computational assumption is that there are unbounded number of processes available to perform all these sure allow this too if you are underlying imple if you are underlying implementation is such that you can not have side effects fine this is perfectly fine but if your language is something that is going to have so that s how a later feature added to a language can affect the complete semantics of the language i mean it is a this is an expression language is actually trivial and simple and all that but the point is that you can the amount of information that you can derive from this has long term consequences on the on adding new features to the language which may not be obvious um initially yeah so a  47  46  clear and cut so this for one thing it is clear and concise actually this is a i have taken a shortcut in the sense that i have used this o to represent one of four operators but as i said since they are all part of the computational model and they are all readily available it doesn t really matter so you can look upon these three rules as really multiplied by four i mean there are twelve rules if you like one for each operator because this o is really not part of the language right i mean this o as a operator is not really part of the language it is it s a meta variable for the four operators we actually have twelve rules but it is concise it is precise it gives a deterministic order so there is only one way a syntax tree can be reduced to a single node yeah so what it means is that you have to craft the semantics very very carefully since a lot of since everything that you can not do is left unspecified it has to be derivable from from what you can do yeah so so so now you can see that you can have a lot of trouble just crafting a few rules it is not necessary to have um a minimal set of rules just to play safe you might add more you might add more rules but then the consequences of that should be that um the consequence of that should be first realized i mean what does it allow a kind of non determinism is that non determinism something you want you might want it i mean um if i am if i am designing this expression um the semantics of this expression language as part of an implementation for um a machine like the parham with sixty four processors there are some hyper cube i might actually want that non determinism i might find it better to have that non determinism in their semantics and get rid off side effects in function evaluation when i add functions to the language i mean i might explicitly ban side effects ok in the larger interest of program clarity whatever but what ever decision i take i have to be absolutely clear if there is some non determinism do i want it in a multiprocessor environment it makes a lot of sense to introduce this extra rule which allow left to right mixed with right to left and so on but if you are if you are pragmatics is such that what ever you do for the um multiprocessor machine should also be applicable for a uni uniprocessor machine in case you wanted to port the language for long term views of that kind then you you better think in terms of what are the constraints therefore that then you better really worry about whether you want that non determinism will having that non determinism actually create problems create more problems or will it solve more problems and that s it could work either way yeah so so we will very often we will just take an implicit assumption that you we want everything to be also implemented for a simple uniprocessor but no where in the rules do they actually specify these things that sense you are totally abstracted away from the machine except for the basic underlying computational model ok so um so i need and it requires a really simple transition system um which seem so trivial to actually drive home this this important point the difference  52  00  between determinism and non determinism the difference between the desirabilities and the undesirabilities of the two things the difference of how you should craft rules so that you what you want what you desire most actually is made specific in the rules what you do not desire is also some how derivable from the rules right so writing semantics is no joke even for trivial things it is it is quite hard which is i mean which is um which is one of the reasons why it takes so much time even to write a few rules yeah ok we will continue with declarations and commands of pl zero next time thank you transcriptor  v.srinivasa rajkumar educational technology i.i.t delhi presents a video course on programming languages by dr.s.arun kumar deptt of comp.sc & engg i.i.t delhi lecture 10 binding welcome to lecture ten um before we start today s lecture on binding we will briefly recapitulate what we did last time  noise  so last  00  46  time i um i discussed transition system for simple expression language in which there were no identifiers  noise  and as i said we are not really too bothered about various compiling or parsing issues you would like to look upon expressions as trees and with that in mind we give a following the following transition semantics so so we assumed transition system um a set of abstract syntax trees of integer valued expressions of which there was a underlying set of numerals um with notations for them and we defined  01  30  a transition system in which the set of configurations was the set of all possible expressions in the language the set of terminal symbols were the numerals and i use this general symbol to denote any of this binary operators and we defined the transition relation by various rules so the rules so we had only three rules actually it is three rules multiplied by the number of binary operators that we have so given  02  05  two numerals along with a binary operator in that under the understanding that this binary operator is directly implementable by the virtual machine on which this expression language is implemented it gives you another numeral um and these two rules applied inductively essentially specify that we are allowing only left to right evaluations so given a binary operator you can not evaluate the right operand of the binary operator till you have completely evaluated the left operator so these two rules so this rule applied several times um will finally yield a left operand for the binary operator so that it reach it becomes a numeral and then the right operand is evaluated yeah um so when i said that it is desirable in many cases to have a deterministic set of rules in certain other cases it might be desirable to allow for a non deterministic set of rules so if  03  19  you for example were to allow this extra rule then you get a non deterministic sequence of transitions evaluation possibilities um and otherwise  03  33  if you allow only the first three rules then you only have a left to right evaluation of expressions and depending upon test you can pepper your semantics with whatever you require for parenthesis or you can eliminate parenthesis all together since you are only dealing with syntax trees so very  03  57  often for pragmatic we would like to have a deterministic set of rules and we will assume very often that we would we will give only a deterministic set of rules because that ensures that  noise  it it defines a compiler for a uniprocessor quite unambiguously um right so today before we start on declarations um it is necessary to give a general impression of what binding means in programming languages the field of programming languages is cluttered up with lot of terminologies so um which many of them come from implementation issues many of them come from theoretical issues to study of many of them come because the terminology in different programming language is different so you require a unified terminology to discuss all programming languages and so on so the notion of binding is one such and it is often regarded as a purely pragmatic issue so it  05  00  is a good idea for us to discuss it now starting with its with the notions of binding that are prevalent early binding compile time binding run time binding etcetera the earliest  12  15  notion of binding occurs really in mathematics without mathematicians consciously aware of it um ok so you have an expression like this let us say a double summation um i have this names binding very often has to do with names and declarations so it is appropriate that before we discuss the semantics of declarations we discuss the notion of bindings but then we will discuss the pragmatic notion of binding too but let us first concentrate on names so there are these names a and b which don t matter at the moment what matters to us at this moment are these names i j k m n ok so here is this we will assume that this the summation in mathematics where j goes from one to m and i goes to one to n and you have these so um this this binding of i this this i is a name and we normally use a name to denote some complex object ok it is nice to instead of having a long expression or some or a long phrase to denote some complex object especially we are going to use it again and again to give it a name but once having given it a name the issues that arises is what is the name mean at least in the context in which the name appears so here the meaning of the name is that i goes from one to n i can range from one to n and that is a binding occurrence so this this is the introduction of the name and the introduction of the name along with its meaning so this entire sigma i equals one to n has i as a binding occurrence here similarly with j in this sigma ok now having introduced the name and what it denotes i am now free to use that name for some purpose so here this i here denotes an applied occurrence so having defined a binding occurrence for the name one has a applied occurrence so similarly with j however the k here for example has a purely applied occurrence without any binding occurrence ok presumably this whole expression was lifted out of context and placed here so hopefully if it is a well defined mathematical context them k would have a binding occurrence somewhere else in the context so binding occurrences could occurred um somewhere fairly deep into inside um inside some mathematical object similarly n and m would presumably have some binding occurrences some where and they are called free occurrences so the whole issues has to do with names and naming and what do names mean and when does a name have a meaning so here is a case where the binding occurrences proceed the applied occurrences similarly if you were to take this double integral here the binding occurrences are dx and dy of course with this with appropriate integral symbols ok the binding occurrences are dx and dy and the applied occurrences actually occur before the binding occur so the binding occurrences appears after the appeared occurrence in each case for each of these bindings there is a scope so the scope of this j will um starts from here from its from its first binding occurrence to the end of the phrase which has a which can have a binding occurrence which can have a applied occurrence  noise  the scope of i similarly starts form here and goes to the end of goes to the end of um end of a possible applied occurrence  noise  so similarly in the case of this double integral um the scope of y is in the dark blue region wherever they so um it starts essentially from the integral from this integral symbol and goes right up to where the binding occurrence is and the scope of x starts from this integral and goes right up to the binding occurrence so this is an example of when the binding occurrences occurs after the applied occurrence if you look upon this as a string or as a tree even as a tree the it s the binding occurrence occurs as a right as a node of some right subtree whereas here the binding occurrence precedes the applied occurrence because it occurs as a node of the left subtree  student speaking  this is the definition of a binding occurrence is just that it is the first introduction which some how give it a meaning ok but meaning is not part of the syntax ok so for example what is the meaning of this sigma what is the definition of this sigma it it might differ i mean um i will just assume that it differ it refer refers to summation but that is not necessarily always valid a binding occurrence usually means a declaration in programming it means a declaration so there are and binding occurrences and applied occurrences occur every where in fact  12  15  so so  16  00  in every for example in logic you have this you have this quantifiers right so you could have a quantifier like for all x there exists y and some statement in terms of let us say some predicate p which has may be several free variables right so you could have may be a z you could have x you could have y all right this is a common thing and it doesn t necessarily so here you have this binding occurrence of x here is a binding occurrence of x here is a applied occurrence of x what ever this predicate is some statement so the predicate is just some statement here you have a binding occurrence of y and you have a applied occurrence one or more applied occurrences of y in this predicate p and the scope of y is is delimited by this quantifier the scope of x is delimited by this quantifier ok and z is free if you look at if you look at any of your school problems in mathematics the one of the first statements you would say is let x be the something something and then the rest of the context is may be a solution towards x or a solution involving x that let x be something is a binding occurrence and all other occurrences which use that x are applied occurrences in every problem you might have a um let x be something and let y be of something in each case the moment you state that you got a binding occurrence ok in different problems the different x s have binding occurrences and they have therefore different applied occurrences so within a problem when you say let x denote something then that x has a scope restricted to your solution to that problem right so this is very much part of also natural language you know it is define it is very hard but to understand it is reasonably easy the fact that you first introduce a name and you explain it through some meaning you give it a meaning and that is a binding occurrence later assuming that meaning you can always use that and every use of that name is a applied occurrence  noise  yeah so in programming um you have let us let us look at a typical pascal kind of environment so i  18  16  have this constant declaration so in in most cases in programming binding occurrences occur as parts of declarations that is when you introduce new variables as in the case of mathematics the binding occurrence in several languages could actually occur after the use of that name so for example in um in mathematics it is also quite common to say let some expression be something where you have already introduced new names which are all applied occurrences and then you write where those new names mean these these these things so those are the binding occurrences um for compiling reasons many programming languages like pascal actually for efficiency of compilation not necessarily because that is the only way to do it but for efficiency reasons many languages actually insists that binding occurrences should precede use or should precede applied occurrences so you can take something like this so here is a constant declaration in a language like pascal though i have used commas so it is you could equally well take it as a language pl zero so m equals ten m is a constant um ten is a literal pi equals three point one four one five nine a equals pi star m star m in each of these cases this is a the first introduction is a binding occurrence and all later uses of those names are applied across yeah  noise  so in the case of a variable declaration you have a binding occurrence then the variable is first introduced and every where that that variable is used you have applied occurrences including on the left side of the assignment all these are applied occurrences yeah ok  noise   18  16  so in  20  30  in a language like ml the typical binding occurrences look like this let a let statement is a typical occurrence of a binding of an introduction of a binding occurrence so let some name x equal some expression e not in some some other expression e end ok where i will assume that this expression e actually uses this x so this is a binding occurrence of x and any occurrences of this x inside e would be would all be applied occurrences yeah so um so in a typical ml session you also have um you also have things like um you introduce names in the beginning as val y equals something val um pi equals three point one four nine so all these are binding occurrences and what you are doing is you are creating an environment of names along with meanings and you are using those names in your subsequent expressions alright so in addition you to so the most so the most important reason for having binding occurrences is usually the introductional names but in the case of um so what  20  35  what can so what are all the possibilities so a binding occurrence usually either defines by introduction or redefines an existing name yeah um so names are identifiers identifiers is the more common term and a applied occurrence just refers or denotes the meaning define by the appropriate binding occurrence right so in general what you could um in general you could the semantics of the language should actually specify what is a binding occurrence and what is an applied occurrence right so you  21  20  could think of a declaration like this in pascal as defining some binding occurrences and also using them applying them somewhere and in the process so essentially what we are saying is that we require names because we can not always be using these arrows ok so you could in in theory if you just have these arrows we will see that may be when we do the lambda calculus but in theory it is possible to completely get rid of names if you had some complex way of referring to some some object or some meaning yeah so um so in a in a language like pascal which insists that all declarations precedues the arrows will all be upwards further in a in a complete program of pascal all variables will eventually be bound all identifiers will be bound there will be no free identifiers and by a complete program of a language i also include um the libraries which might which you might be using so for example a typical in a typical pascal program or a fortan program if you if you use the function sin x then that sin as a applied occurrence in your program however you might never actually define it within the program because it is already available as a library so a typical environment of a fortan program includes not just your program but also all the binding occurrences that are defined in the global environment through libraries or through whatever is the initial um global environment provided by the system yeah um so  noise  let us let us look at the various kinds of bindings um so if you were to look at um if you were to look at the meanings of bindings more closely what we will see is that in a in the case of an imperative language  23  55  so let  32  05  us just consider two simple instances so we will take a pascal like and a ml like language a pascal like language has these constant declarations al right and and has variable declarations of this form let us say an ml like language a functional language has declarations of this form ok um and within let expression and so on and so forth which are unnamed functions so you also have function names for example ok here you have things like this which is some expression right so in this case in the in the case of the imperative language this constant declaration m gives you this the binding occurrence of m which binds the name m to the value ten it is a name value binding ok and in fact the functional language declaration of this form is is exactly the same it gives you a name value by name yeah so that is what binding means so the meaning of m is just the value ten in the scope within the scope which is declared right in in imperative languages further there is there is a var there are these variable declarations which actually denote which actually denote locations ok so the variables in a imperative language are um actually name location bindings there are further so when you have when you have a begin end with this variable being updated so in in this place what happens is that so the variable location binding remains unchanged however there is also a location value binding in the case of a variable which can be changed by commands in a functional language in general there are no locations i mean the whole idea of a functional language is that it is abstract enough not to include the notion of the memory as part of the language and therefore updates and assignments are prohibited so a variable in a functional language is really something like a constant in a imperative language a variable is just a variable in a functional language is just a name value binding and in languages in most cases the function f is also is also you can also look upon the function um function itself a value and a function declaration itself is another name value binding whereas variables in imperative languages implicitly assume the existence of a notion of memory or store and they have an indirect level of they actually consists of two bindings a name location binding which is location is just it is just um pragmatically speaking it is just a memory address ok so there is a name location binding and then there is a location value binding which might be changed depending on what are the contents of the location what content how you are updating that location right in a functional language this complication of locations um memories is completely abstracted away and you just name value bindings through out and that is the essential difference between um a functional language and a imperative language right so you could have so so what people consider a variable in mathematics is really a variable constant variable in the sense that it is unknown but that it is binding is the same over the scope in which there is a you have a variable it represents the same value in a mathematical problem similarly in a functional language in a functional program over the scope in which the variable is declared it represents only one value unlike a imperative program where the variable actually gets updated in this form so what remains constant in an imperative program is only the name location binding the location value binding changes at keeps changing constantly right so the notion of the variable in a functional language is really the notion of the variable in mathematics which is really something that does not vary over time it is a variable in the sense that it is in mathematics usually it is used as the name variable is used as something that is unknown but something that has a constant value and in a functional language it is just since it is just a name value binding it is always constant over the scope in which the binding is effective as apposed to a variable in an imperative language which actually what is constant is only it s the the binding between the name and the location um or the address in which that variable is stored so the so the name x just denotes a denotes a particular address in memory pragmatically speaking and that is the only thing that remains constant over the scope in which this binding is effective and it does not does not say anything about the location value bindings which means which loosely speaking in the case of memory locations it means the contents of that memory location which can keep changing yeah so so you can have various kinds of so but we are we are more interested at this point since the issue of bindings came up it was necessary to point this out but we are really more interested in bindings from the point of view of declarations because we eventually have to give the semantics of declarations of let us say some language like pl zero right so let us look at the various kinds of bindings that are possible so you could have nested bindings right so for  32  10  example here is a typical pascal like program fragment in which you have you have a declaration of the identifier i here and you have another declaration of i as a variable here all applied occurrences of i within this within this um within this begin end block refer or mean whatever name location binding has been specified here ok all occurrences of i here denote whatever name value binding has been specified here right so and of course we have this normal lexical scope rules right so the scope of the variable i declared here extends across extends through his procedure the scope of this constant i extends to the entire um through this entire fragment provided there is no other declaration of i in a inner scope and any other declaration of i is a hole in the scope of the constant i yeah so a scope just gives you an extent in the program text in the case of what are known as statically scope languages gives you an extent of program text over which a binding occurrence applies right ok so you can have of course this this also a language designer the one question is is our nested bindings necessary i mean why does the language like pascal which for example boasts of a simplicity it is a good language for learning programming and so on and so forth and why do why do almost all block structured languages allow nested bindings which can only be confusing for example they affect readability because every time there is a reference in the main core to an identifier you have to find this find the declaration where the actual binding appears ok and if you allowed if you allowed nested scoping like this if you allowed the creation of holes in the scope of a given identifier then all you are doing is you are spoiling the readability of a program secondly it also makes it necessary for a compiler every time an occurrence occurs um every time there is an applied occurrence it makes it necessary for a compiler or a parser to look for the appropriate binding occurrence and verify various things like i mean are you using for example within this scope are you using i only as a real variable are you or perhaps you um it should be checked that it is not being used as a character or a string yeah or as a pointer so it um it puts an extra overhead on the compiler and in the and the runtime system especially in a language like pascal which does type checking as to as to where exactly is the binding occurrence and whether the meaning specified by the binding occurrence is being explicitly followed here so a simple solution could just be that just ban all nested bindings so that everything is so that every time you declare a new scope you have only new names and there should be no problem at all but so  36  20  but the  38  00  um but the problem is that very often when you have a program developed by a team what you would like to give a programmer of an individual procedure which might go deep into some program is the flexibility to use his or her own names for the variables that he is using and um so which means that that particular program if you disallow nested bindings then that programmer would have to know all the global names before deciding what should be the new names to be used locally within that procedure which can be quite quite a task which means that you the first thing the entire programming team should declare should decide what are the names for various global variables it and in a really large software project that can be a constraint rather than um rather than um facility you know um but however as i said its it spoils a lot of things however it spoils readability it is quite disastrous in languages with default bindings we just um i will just show you what default binding are and then of course it complicates debugging for example suppose you you intended to put a um you intended to introduce the name i in your scope but you actually forgot to declare it but there is a i in some other scope then the debugging process becomes quite complicated because every time that glo that i is considered to be global so for  38  03  example so in this case if you had just if you were the writer of the procedure p and you forgot to introduce this declaration then it will be taken that this i always refers to some global occurrence and you will be wondering why  noise  your program doesn t seem to work as you expect so it really complicates and this i may not be in just in one outer scope it might be in some it might be a global and your procedure might be deeply nested inside somewhere so nested bindings complicate matters quite a bit both in terms of debugging and readability but what but what really clinches the issue clinch the issue for example  38  44  in the case of ada is that every body uses i j and k as a counting variable in a for loop i am not joking this is this is something that was actually stated in a discussion on the ada language that every body in the world uses i j and k as a counting variable and so you should give the programmer the flexibility to decide especially when the counting variable has no other significance except as a counter for a for loop for example you all use just i j and k that s if it has no particular significance right so um so nested binding are here to stay even though they complicate some matters so in languages like fortan for example they had this very very funny rule they said that supposing they said that you don t need to have a declaration so which means the you need not have binding occurrences you need to have only applied occurrences however the  39  58  applied occurrences will have default bindings in this fashion so any name that starts with i j k and l which does not have a binding occurrence is taken to be a integer variable further any identifier that starts with any other character any other letter and which does not have a binding occurrence is taken to be a real variable right so they had these things which actually makes makes matter really bad because um what can happen without these binding occurrences is that is that you might you might actually have a variable starting with i which it um you choose your identifier so that they reflect the pro the problem context right you might actually have something starting with i you intended it to be a real variable and you but you forgot to declare it the fortan compiler also compiled it the runtime system is executing it but you are getting disastrous results and you will you may never know about it especially if you are solving a problem for which there are no known test results alright um it is there is a legend that some of this um some of this um on of the disasters um space disasters or one of the satellites was due to the fact that it was written in fortan where there was this supposedly do statement um which in which a comma was mistakenly replaced by a period and therefore that entire do statement was taken as a real variable so do do ten i equals one comma fifteen means it is you execute that loop fifteen times but if the comma is replaced by a dot then this do ten i taken as a real variable since it starts with d and there is no binding occurrence with an initial value of one point one five ok so these are um i mean there is a legend that one of the venus disaster was due to a fortan program bug of this kind which definitely is never detected by a compiler it is definitely not detected by a runtime system because there are no binding occurrences because of the default bindings so so what it means is that the small typographical error spelling errors which we are making all the time in our programs would get um would get bound by default to something other than what we are intended them to be yeah so that is one of the reasons why later languages like algol sixty and um and so on insisted that they should be binding occurrences for every new identifier there are of course some certain special kinds of implicit um there are  43  05  certain bindings which are implicit for example you might in some this this constant declaration is not really intended to be in this scope it could be in some outer scope um you could have two records r and s in a pascal like language defined this way so this c um c is a component of this record declaration which has some type so this is a binding occurrence of this c within the record but however the binding is that that c is only bound to all variables um that have this record that are of this type so this c is not explicitly bound either to this r or this s both of which are of this record type the actual so the binding occurrence can actually be decentralized can actually be stretched out part of the binding occurrence can be here the other part of the binding occurrence can be in a with statement right so within this with statement this c actually gets perfectly bound to r with in this with statement what all references to c actually get bound to the s and the normal scope scope rules apply so within this so this begin end is a scope this with s do begin end is another scope and this with r do begin end has is a hole in the scope of the scene which is bound to s right so pragmatically speaking even binding occurrences can actually be stretched out i mean not all the full meaning need not appear at the at a single binding occurrence it might be distributed across two or more constructs which is actually not very surprising in the case of um in the case of variables for example there the binding occurrence the the name value location um the name value binding occurs at one go and the value and the location value binding occurs several times in different places ok even the name location binding in the case of variables does not fully occur at that point in many cases if you look at the implementation languages like pascal what happens is that a name relative location binding occurs at compile time and relative location to absolute location binding occurs at runtime because of the structure of a structure of a block structured language is such that you you never know at compile time when you are using a dynamic memory allocation you never really know when you going to call that procedure and therefore what is going to be the base address relative to which that variable as to be located so even that name location binding from a when you look at it purely pragmatically can be stretched out across the entire spectrum from compile time to runtime right so um there are other kinds of bindings for example in in all these kinds of in all kinds of binding that we have discussed so far you have an identifier denoting a single object what ever that object might be but you have also what is known as overloading  47  00  and the  47  01  most common overloading available in all programming languages is the overloading of addition and multiplication and subtraction here you have addition um these operators are also identifiers they have their and their binding occurrences are in the global environment in most high level programming languages however even in the global environment they have two different meanings depending upon the operands right so there are there are actually two different identifiers integer addition and real addition or floating point addition integer multiplication and floating point multiplication so what what exactly is the binding so at um within the same scope these identifiers have two different bindings simultaneously available so there is no creation there is no overriding of one binding by another the overriding is completely local and it depends entirely on the types of the operands right so this is known as overloading and this overloading has been in the case of pascal there is of course another obvious overloading which is that you have a function name and you have um you have a variable name with the same um you have a variable with the same name which stores the result of executing the function so that is a case where you have a function name which denotes well a function object but the same name in within the same scope also denotes a name value binding in which the value of the function is returned so besides this overloading this overloading has been carried across to more modern languages also much further for example in in languages like ada you can have this you can have the same name um bindings representing different kind different objects which are distinguished just by some simple thing like the types of parameters so in the case of procedures um you can use you can use for example with in the same scope you can define two or more infix plus operations for example you might do plus for matrix matrixes of some kind you might do some plus to denote concatenation of strings or what ever but just based on so in addition to the global bindings of plus which denote integer addition or real addition you have two more new bindings and all four of them occur simultaneously in the same scope and the only distinction is either based on the types of the operands or the types of parameters which is mathematically the same thing though in terms of language implementation it is different it depends on the types of operands or order of operands just just on those syntactic basis you have you have a distinguishing capability between different bindings for the same name available with in the same scope ok lastly we  50  40  have so let us look at this various kinds of bindings from an implementation view point so you can have this constant variable bindings some of them are compile time some of them are runtime um mostly in in languages which are in languages which believe in a static type checking or atleast which allow the facility of static type checking you try to do the bindings as early as possible at compile time in many other languages you actually do at run time languages even these identifier value and identifier location bindings are often done at run time this is typically with dynamic data structures even in pascal you have various name location bindings occurring at runtime right um the language  51  50  um lisp for example carries it much farther so languages like snowball and lisp believe in um run time bindings as apposed to compile time bindings yeah so they believe in what are known as late bindings so as a result so you can you can take the entire spectrum of doing bindings and you can actually well loosely speaking order languages in this fashion so if you look at fortan and cobol um fortan and cobol do very early early bindings in fact um the difference between so everything is done at compile time the complete memory allocation is done at compile time so even if the even if the fortan program has re locatable code the bindings are completed for example um um the bindings of variables that is name memory address absolute address binding are done at loading time before runtime begins ok  noise  a compile fortan program might still give you a relocatable code that means it gives it might only give you um relative addresses but at loading time before during linking and loading the entire address calculation is completed for every variable for every name the address calculation is completed before so as a result and fortan also has a static memory allocation since you do very early binding you have to decide on exactly the kind of memory allocation well before you start running the program so you a static memory allocation is done very much like in the case of assembly language every code segment is followed by a data segment for that code and so you have very early bindings well before runtimes there are no allocations done at runtime you have a completely statically partition memory and all absolute addresses are calculated well before any program before the execution of the program atmost they are delayed till loading time so from ok so fortan has this property and as an result fortan programs are very very fast they are because the overhead of memory allocation and changing biding and so on is not there but hey are also quite inflexible in various ways i mean you cant have dynamic arrays and so on and so forth you cant have dynamically created data structures so at the cost of flexibility you have very fast executions if you look at snow bal and lisp since the bindings are all late and they are all most of them are done at runtime what it means is that you have to do fresh memory allocation you have to do fresh um yeah you have to do you have to do all the bindings like name address bindings at runtime which means that you also you also require a garbage collector which tells you exactly what part of the memory is being used what part of the memory is not being used so what is available the request for more memory all that is done at the run time so as a result these languages are in general very slow but they are very highly flexible you can do the data structuring completely dynamically with absolutely no static allocation right so most of the most of the allocation is on the heap in such languages languages like pascal modula ada and ml actually follow somewhat in between policy they do a um name relative address binding at compile time but they postpone the relative address absolute address binding to runtime in the case of ml in the case of c actually most of the allocation is done as in the case of fortan mostly static allocation except where the c compiler detects recursion it detects recursion by a graph construction process and the moment it detects recursion it realizes that you really can not do the static allocation that you do in the fortan because you do not know how many recursive activation of that function are required for each activation you require to do fresh allocation so what ever is not recursive it follows a policy of fortan and only for recursive invocations it takes a more it takes a late binding view and actually there is a graph construction process by which it detects cycles in the graph and therefore detects recursion yeah so c is c programs are very fast because they do most of the bindings at um compile time yeah so in the case of pascal modula ada and ml there is of course extra overhead in the form of type checking um which is which takes its own which has its own overhead at least in the case of pascal it is just type checking in the case of ml it is type inferencing so which means there is a huge computational process involved in just compiling a function this to determine all the time since you don t require to determine you don t require to specify types in the function there is a type inferencing system which actually solves the system of equations on types and so there is an extra overhead at compilation yeah so the next time we will start off with actual declarations in pl zero and then go on to transcription  gopalakrishnan c principles of programming languages dr.s.arun kumar department of computer science iit delhi lecture # 11 environments welcome to lecture 11 so we talk about environments today after briefly recapsulating what we hidden the last two lectures so in the last lecture we discussed the notion of bindings and i said that in general we are talking about um we are talking about identify value bindings in the case of constants identify location bindings in the case of variables and in the case of  noise  updations normally assignments you are talking about location value bindings so i also said that you could  noise  pragmatically speaking bindings a such that you could decide to stretch out a binding for example the actual you could actually split up an identify location binding into an identifier relative location binding and then  noise  relative location to absolute location binding and do it in a set of temporarily decentralized manner and so depending on the time at which you do various kinds of binding you might either have early bindings late bindings or some where in between now bindings by themselves or a programmatic matter in the sense the really depend upon um the implementation but there is a certain core of bindings which is not programmatic which is a semantic matter and that is just the fact that there is some something known as a constant  noise  and there are constant declaration is some of different from a variable declaration that is a semantic matter  refer slide time on 02  29 min  okay so  noise  how you actually do the bindings is in um is programmatic for example if you take language like pascal lets say type checking in pascal there is absolutely no guarantee i mean um that is also um variable um typing of an identifier resolves so binding there is absolutely nothing in pascal in the definition of the pascal language we says that you should type checking only at compile time in fact this nothing about the language which really specifies that you should actually only design a compiler i mean you could you might as well design interpret shoots a convenient however the language is being designed in such a way as to make a compile time checking um set of easy for example the name equivalence and so on and the fact that you have to declare before use they make compile time checking easy so normally so most pascal compiler is would try to do as much of the type checking at compile time so that the overheads do not run into execution time when that s um that s an execution and so it s a programmatic issue in the sense that you want to you want to make execution s faster than they are faster than um as fast as possible where as its not the case with languages like snobol lisp so also for which want to produce an extreme of flexibility  refer slide time on 04  02 min  so the delay all windings write up to the end and think of the and resolve done dynamically but it still a programmatic matter there is absolutely no reason why you cant do you cant do some bindings earlier okay so what we will now like to look at it s a semantic specification of certain kinds of bindings especially the creation of environment um the processing of declarations and how they affect the rest of the language so lets briefly go through the previous lecture so we had defined an expression language of this kind in which i had explicitly for bidden i had explicitly for bidden  noise  identifies so even the undo so it was a it was a pure expression language consisting of just numbers right so this prediction i was absent and the reason is that an identify has a meaning only if it is been only if it is got some binding so what it means is that we can talk about the meaning of an expression containing identifiers only after we have obtain a meaning of the identifier in terms of some bindings in this case its an identifier value binding that we are looking at so the expression does not have a value the meaning in our case mostly represents the ultimate value that in expressions don t have and the expression does not have a value till the identifier has a value um so we look at this but we look at it in a slightly more holistic fashion so lets also look at the language of expressions so the transition system so i assume that there is a set of expressions capital script e and a set of numerals script n which is this or this script e is the set of all integer valued expression e i use the small letters normally to denote that and not dealing with the syntax of the language as consider by the non terminals in the language so the capital letters here the capital e is here denote non terminal symbols of the grammar  refer slide time on 06  40 min  the small e is here denote the syntax trees that means um there is some processing which is already occurred okay if you want look at it programmatically  refer slide time on 06  46 min  so i am really looking at syntax trees rather than the actual syntax of the expression so which means there is no ambiguity so i will try to maintain this distinction as well as possible  refer slide time on 07  23 min  a similarly n is the set of all numerals in the underlying machine and we define a transition system with the set of all configurations being the set of all expressions the terminal configurations being the set of numerals which by the way or a subset of the set of expressions and we defined the three rules which actually give as a left right evaluation or expressions  refer slide time on 07  34 min  so now um in an expression language which contains identifiers we will take a simple language with declarations very similar to what is obtain in a ml okay so right here so right now i am not to worried about types so i have simplified the syntax bit in an in actual ml if this identifier is has got a value then you would write let val i = e in e end okay but since we are not dealing with anything else except values i have removed the keyword val okay so the language of expressions is just consists of this and an expression of this form where of course there is a declaration hidden inside that expression okay and this a declaration means there is a binding occurrence of that identifier and that declaration itself expression in terms of an expression e and then if you have a binding occurrence then you have a scope for that binding occurrence which is given by a complete expression right so the identifier aim might occur in the final expression e as an applied occurrence so informally what  check 09  19  saying if you look at this expression what um what we have saying is that this expression is really or ultimate goal the value of this expression is what we want that is the meaning that we want however this expression is to complicated for example and so we have used an identifier inside that the expression and the meaning of that identifier is some of syntactically represented by this by this declaration right so the meaning of this entire expression is really the meaning of e which contains which might contain possible occurrences of this identifier i that is being declared in the identifier i ofcourse has a value which is given by another expression okay so you could simplify these things but by um by for example replacing all occurrences of this identifier i by this expression e okay and you get a simple expression in the language however as you as you must seen through some of our ml programs so always a good idea to allow for such declarations such namings because very often they have um they have meaning that is associated with the problem domain and you would like to separate all those things instead of writing one long one or lithic expression you would like to split of the expression to various parts that are some are logically connected with the solution of the problem okay so the identifiers here in ml could actually we variables in function names but for the present we will just consider variables and ml being a functional programming language the notion of a variable is really that of an identifier value binding and the reason i am not going directly into pl 0 declarations is because the constant declarations in pl 0 or very much like the variables in ml so they both share the same kind of binding characteristic the same semantic characteristic that a variable in zml is just in an ml program is just a value and so is a constant in a pl 0 program or in a pascal program so just a value okay so we look it this we look at these declaration we look at the whole expression language and what is an ml program  refer slide time on 12  00 min  an ml program or an ml like program is really just one expression so if you look at if you look at your if you look at the ml programs that we have looked that we have already seen you can think of them as being you have you have a collection of functions of values declaration and then you have a function and that s what is compiled then you have an expression whose value is validated in that with those functions so you can think of any ml program as consisting of a let with huge number of declarations and finally an expression which has to be evaluated what happens in the case of an ml session is that the out of most let in an end a consider implicit and it s hidden and you don t have to explicitly write it but you should think of any ml program or any functional language in the same holes and scheme to in any functional language you can think of the entire program as being enclosed in a huge let in end and since its an interactive um since its an interactive program what you are what you do is you the out of most let in an end a consider implicit and you just after having compiled all the declarations you just given an expression to give a find a value but each call to an expression there is really like having an entire program which is which all those declarations enclosed in let in end  refer slide time on 14  03 min  so  noise  so what we are doing so right now is actually fairly general so let s look at the notion of identifiers and so as an um any area of mathematics or logic we have to define when identifiers are free the notion of the binding means that an identifier gets bound to something okay otherwise its free and what is the meaning of free um given a certain given a certain expression or a sentence of a language the free identifiers are those which do not have any declarations in that sentence that sentence is suppose to be part of that s largest sentence in which those free identifiers will get a binding okay so free identifier is a one given a sentence a free identifier is one which has only applied occurrences and no binding occurrences so we can define the notion of freeness of identifiers by looking at the structure of the syntax tree and we can define it by induction on the syntax tree so if you had lets look at a grammar for expressions undo we will follow the will follow the syntax of this grammar as closely as possible so you have these set of expressions  noise  okay and notion of free identifier is that given any given any numeral corresponding to corresponding to this reduction give any  noise  given any i am sorry  noise  sorry given any numeral the set of free identifiers in needs to empty so the set of free identifiers is an is a set of um is a set of names that occur in it with without a declaration so this in an any integer value for example the set of free identifier um i am sorry in a given any identifier which is also an expression it is the single turn set of identifiers given any expression of this form with this little o being a the root operator of the syntax tree the free identifiers in this expression is just the union of the free identifiers in the individual components and if you have a let construct then  noise  the actual expression value that you are interested in this e2 and the free identifiers of this entire expression a let construct to as a complete expression the free identifiers of this  noise  a just the set of all free identifiers in e1 since you are using since you are using probably some identifies in e1 to define this identifier i so it s a set of all free identifiers in e1 and this i will presumably be instant e2 which has the whole purpose of having the declaration of i however this any occurrence of i here is an applied occurrence which has a binding occurrence and so i is not in the free occurrences is of is not a free identifier of this entire expression so this this is by the way set um set difference or set subtraction if you like  noise  so the set of free identifiers in this entire expression is really the set of all free identifiers in e1 e2 excluding i okay  noise   refer slide time on 18  26 min  similarly we could define the set of bound identifiers undo it follows a similar pattern the set of bound identifiers is also is also defined by induction on the structure of the syntax tree so numerals an simple identifiers a simple identifier is further a free so the numerals and simple identifiers do not have any bound identifiers and them and the set of bound identifiers in an expression of this form is just the union of the set of bound identifiers in the individual components note however that these two expressions would have the same identifier bound in different ways you could have an x inside e1 with the declaration for x you could have an x inside e2 with the declaration for x and the two x is need not be the same ya however lets lets look at that later so an in this case the set of bound identifiers is just set of all bound identifiers an e2 union the set of all bound identifiers in e1 an ofcourse in e2 any occurrence of i would not be consider bound and so that has to rebound separately  noise  right and the set of all identifiers of an expression is just the set of all free and the set of all bound expression identifiers in it ya  refer slide time on 20  20 min  so the notion of um bound identifier is has to do with already processing a meaning which which means that the expression can be readily evaluated so the notion of um so its important to keep this for example if you worked at the notion of the freeness and bound identifiers exist in all programming languages okay either through implicit bindings or explicit declarations so if you look at a completely closed program if you look at a complete pascal program there are no free identifiers except those which comes from the global environment of the pascal run time system okay so um what are the identifiers that come from the global environment of a pascal runtime system all those library routines which are readily available so if you look at just a complete pascal program supposing you don t use any of the libraries library routines then the complete pascal program has absolutely no free identifier the program heading has a name and that is itself a defining or a binding occurrence of that name of that identifier otherwise every other identify if you not using any of the library routines every identifier is declared before use so if you look at the entire pascal program there are absolutely no free identifiers all identifiers are bound so every that means every identifier in that pascal program has a binding occurrence which defines it completely right so a complete program of this kind which has absolutely no free identifiers is called closed so um for any in any languages which has identifiers which allows identifiers you can talk of a closed sentence of the language as one which has absolutely no free identifiers and a ground sentence is one which has absolutely no identifiers at all so in fact the transition system we consider for the expression language without any identifiers consists of only ground expressions right so closed in ground these are two important terms which occurred in any formal language whatever may be the formal language so the important thing is that a closed sentence so if you look at a complete pascal program since there are no free identifiers the program itself can be executed in an empty environment because there are no identifiers which required to be given a meaning from outside free identifier is one which gets its meaning from outside the sentence that your currently looking at so since so in a complete program since all identifiers a bound they get their meaning from with in the sentence and therefore a complete program executes in an empty environment right  refer slide time on 24  40 min  a ground sentence does not require an environment at all since there are no identifiers which require bindings so what we have so our expression transition system for our expressions actually gives us a transition system in an empty environment so i have not actually mention that  noise  so since we consider only ground expressions without any identifiers they obtain their meaning in an empty environment  refer slide time on 25  06 min  okay so these things are very um actually quite simple i mean after all if you ask me well what is the value of ax square bx + c well clearly i can not give you that value unless you can give me a meaning you can give me a binding for a b c and x okay so under the assumption so given that a has a value something b has a value something else c has a value something else and x has a value something else then this expression has a meaning in that environment so that environment is just the name value bindings for a b c and x so a just a set of such equalities name to value equalities so will define the notion of an environment for um this simple ml like expression language given a set v of identifiers and this moment the only kinds of identifiers we are interested in identifiers which taken integer values so they are like the variables in an a an environment row over these set v is just a mapping from capital v to the set of numerics so you can think of an environment as actually a list or a more accurately a set of values this should be a set of variable value bindings and we can look upon for any given collection of variables v we can look at all the environments set of possible so i will use envv to denote the set of all environments over capital v um very often the subscript v the subscript v will be omitted when its either understood or its not important tense of once a forth  refer slide time on 27  38 min  otherwise we always considering in an environment ins with the given set of identifiers of course that set of identifiers could be extended right so lets look at lets look at the declaration language we could actually extend that s language further so since that is since declarations have a semantic meaning completely different they are logically a different entity from expressions it s the good idea to abstract that out as a separate non terminal which is something evolved done an in fact by abstracting it out a separate non terminal we have also i mean the language of for example pl0 also gives some separate syntax and separate reserved words to make it clear that it s a it semantically a different logical entity from that of expressions or commands so once you have this there is absolutely no reason why you should have only one identifier declare you could for example have several identifiers take the so you have a separate declaration language which whose base is is a single declaration and whose and you can recur on that basis and give sequences of declarations which is what happens in most programming language right so now what we have is that we have a we have a programming language or functional programming language whose main non terminal is the language of expressions and there is an auxiliary language with in it which is the language of declarations and if you look at the dependencies so for examples the expression language allows for declarations inside expressions and declarations in turn allow for expressions inside declarations and they both allow for identifiers so there is as you can see there is a certain circularity in the entire definition which is which is what happens when you apply these things inductively or recursively so further movement will not take about this but it has some meaning that i will explain later so or language whatever we write in green is really or language so lets look at some quick examples so um typical sequential binding could of this form let pi = 3.141 ; twopi = 2.0 * pi the semicolon is very important in the sense that your in a sequential declaration what you are the sequencing might be important what you are explicitly saying is that the second declaration might actually depend on definitions given by the first declaration as in this case very often of course this dependent is not there in which case the could be actually be return in neither add this this is also typically in pascal for example you could have a constant declaration of pi semicolon something another constant which uses the previous declaration of pi in an in the evaluation of its value so you could have a complete expression in the constant declaration provided all the identifiers in that expression already have been declared before okay in the case of pascal of course they should all be constants to they can t be variables but for the moment let s not worry about that and the actual the actual expression whose value you are interested in is this where of course is this identifier radius is free in the entire expression and presumably this expression has been pulled out of some context in which may be radius has a binding but this is this is an expression in itself and we are not to interested in contact sensitive syntax so what we will assume semantically is that this radius in order for this expression to have a value which is what we mean by the meaning of an expression it is necessary that there be an environment in which radius as a value and once that is done i can have i can evaluate this expression right so you can have sequential binding like this you could also have nested bindings like this so the actual expression you are interested in is thus a a * h where h is free so the value of this a * h is what you are interested in and however um you required um expression that presumably was getting two complicated you had to define this a and in order to define this a and in order to defined this a i had to define this part okay again here  noise  there are these two identifiers um two occurrences of the identifier r which presumably was which a free and so they must so is not possible to evaluate this internal expression unless r and h are given some value from some external environment unless they are means declared some where in the context ya so you could have nested windings to okay and this is in fact allowed in ml by by nested bindings what you are doing is your really localizing this identifier a its local only to this scope between this let in and end the identifier pi is local to entirely this scope ya so um since and um such a construction is possible because of the fact that an expression can have a declaration in it a declaration can have an expression in it but if this expression if this is a full fledged expression  refer slide time on 35  10 min  there is absolutely no reason why it can not have a declaration inside it so we have to keep in mind the fact that such things are possible when we evaluate when we define the semantics of this language ya  noise  right so  noise  the localization of new environments is something that we defined here so let s define an operation an environments  noise  so assume you got some variable sets  noise  v1 and v2 whose union is v  noise  not that i am not assuming that v1 and v2 are um necessarily disjoint okay v1 and v2 could have some common identifiers and then after all as we saw in bound expressions um this absolutely no reason why the same identifier could not have been bound in different bound differently in different localities in different sub expressions ya so lets assume that row 1 is an um is an environment over v1 this is a this is a short form notion to indicate that row 1 is an environment over v1 row 2 is an environment over v2 then will define this operation call row 1 with row 2 within brackets ya will define this as an operation i am using light blues always for row is because for environments because i have so far not actually define what an environment means in concrete terms i taken out the minimum minimum information that i require so as what i am concerned an environment is not a data structure is an abstract entity which later after your semantically specified all that your require of an environment you can come out with the suitable data structure it would represent environments so the normal coding conventions i will follow through out i will that anything that s blue is likely to be some what abstract up in the sky some where any thing green is very programming language oriented brown is something the square down to earth may be um may be related to the underline virtual machine black actually i have block is something that i often use by explain things which is actually of course behind the sky into out to space which is very dark but it could also be something very deep down inside the earth you know so it something s could be has block a skol so i have a um sort of skits of unique view to yours block but otherwise i will use this sort of color coding through out ya so i will define row 1 row 2 so that is a new environment it s this is an operation an environments which creates a new environment for each identifier i if i is an v2 then the binding given by row is the same as the binding given by row 2 okay for all identifiers that occur only in v1 and not in v2 the binding given by row is the same as the binding given by row 1 yes we are only considering we are only considering the set of identifiers in v1 union v2 so there is no there is no other possibility right so note that this is an asymmetric operation and that row um we call it as row 1 updated row 2 is not the same as row 2 updated row 1 ya  refer slide time on 39  43 min  so we will be requiring this operation as you will see right so um will define the operation semantics of this since as a said um if you look at a grammar um expressions a required declarations declarations also required expressions so we can not take we can not isolate the true completely so any semantics of the expression language will depend upon a semantics of the declaration language and a declaration and a any semantics of the declaration language will depend upon the semantics of the expression language so we so if you have to give a semantics now we have to give a holistic semantics including both expressions and declarations and we have to make it complete right so let me quickly go through this an all explain the fine up points next time ya so i start with in exactly the form we have a grammar the main language is a language of expressions so i start with the transition system for expressions now i have a difference i have to consider only all those expressions if the language over some set of variables v of course very often i just get rid of this v so the set of all configurations of expressions is just the set of all expressions over v and the set of all terminal configuration is still the set of values because the meaning of an expression ultimately its just a value right okay and will define this transition relation under the assumption of an environment an expression which contains identifier does not have a meaning unless each of those identifier has a meaning in other words the expression does not have a value unless each of the identifiers is been given a value so that value assignment to the identifiers is going into be in this environment row ya so i will just quickly go through the modified forms of these of the transition system so the first thing of course is that we have to introduce an axiom which says that in any environment row give a that means an environment is just a value binding to each of the identifiers right which is the set of the value bindings and expression which is just an identifier which is allowed by a grammar has a value just given by the binding right so given any identifier and note i said once before that transition systems are syntactic or symbol manipulations and this symbol manipulation is that you whenever you encounter the identifier i this transition indicates that is really being replaced if you like by the value given by the environment it still still symbol manipulation but symbol manipulation in the abstract like so the other two other three rules are exactly as we had before exact that now we have to bringing the environment because we really can not um this the rule e1 does not really require an environment however the e1 an application of e1 could would often in a in the general case would never be complete without an environment because e1 would be applied only after you have successively applied e3 and e2 several times just go back to your basic example of a transition system so what would happen is we are specifying a left to right method of evaluation so you would be applying so given an expression of the form e1 some binary operation e2 you would keep applying e3 till the left hand operant became a constant became a numeral then you would start evaluating the right hand operant till that became a constant and then you would just evaluate this constant right so they would be several applications of e3 till the left hand operation became a operand became a constant then several applications of e2 till the right hand operand also became a constant and then an application of e1 right so if you look at the um the syntactic application of this rules in a typical large expression that s what would happen so what um in the case of this in the case of the introduction of identifiers what would happen is before applying before um an expression like e1 became a constant they would have to be an application of a call to an environment some where to replace um at the identifiers in it via appropriate constants or literals if you like right  refer slide time on 45  45 min  so there would be interest  check 45  30  this process of execution would be applications of e0 where identifiers are replaced by constants okay and those and all of them required this environment row right so however so this is this is only for the basic language of expressions which does not contain any declarations so we have to give we have to still give a meaning for the let construct and let construct is something like this so lets so lets look at this so the meaning of a let construct of this form is really the value of e however the value of e depends upon this declaration d inside the let construct and so  noise  there is no way i can evaluate this expression e till i have evaluated this declaration d and evaluating the declaration d means creating fresh bindings so which means that this expression can make some progress only if this the processing of this declaration can make some progress so if this declaration moves from one from as from d to some d prime i am not yet specified what it means for declaration to move from d to d prime that is something since we are looking at things in a top down fashion it will come later  noise  and supposing so several applications of e4 will finally reduce this declaration to some environment row prime okay and now i am in a position to evaluate this e and i can evaluate this e and this e can move to some e prime and how do i evaluate this e i evaluated really in the environment row updated with row prime however this entire expression the let construct really is executed in the original environment row with which it came which in the case of a completely close program this row would be the empty set but in the case of a program which is not completely close any um an expression need not be completely closed it would execute in some environment row so once the declaration has been completely evaluated to produce an environment row prime this expression can be evaluate and the evaluation of this expression proceeds in this modified environment in this updated environment row row prime okay and now in this modified environment essentially what would happen is if there are no let constructs with in e there are no nested bindings then you would be applying rules e0 e1 e2 and e3 to finally evaluate this expression by several applications and having done that what all happen is that you will be left with a constant finally and the result of this expression is just this constant right but not that the entire let construct really its environment is only row and the updation row prime is really a temporary change um change in the environment and that sensitive is reversible right so there is a reversible process so this temporary change in the environment is necessary in order to evaluate this expression e but really the environment in which the entire let construct its being a is being a executed is in the original environment that is started out that ya um so the environments the updations of environments is a um is a typical case of a reversible change in the sense that if you apply this rules systematically there is absolutely no reason why you should be left with a strange updated environment which is different from what you started of originally with you always end with the original environment you started out  refer slide time on 50  25 min  in a completely close pascal program you start with an empty environment an after you have executed you still have nothing more than the empty environment the completely close pascal program executes in that in the original global environment and as declarations come new updations which are temporary created and as those scopes a execute um excited those updations go away automatically right okay i will stop here i will continue again on the declaration transition system next time and we look at some examples transcription  gopalakrishnan c principles of programming languages dr.s.arun kumar department of computer science iit delhi lecture # 12 declarations welcome to lecture 12 so we will continue what we did last time with a emphasis on declarations so i will quickly go through the transition system as we defined last time and then will continue with declarations right so we were looking at simple ml type declarations i am sorry ml like declarations plus type means a different word altogether so we have variables now and for the present we will just assume variables have only values and there not functions and we have the usual binary operators and the let construct this is the binding occurrence of the let construct a binding occurrence some books also called a defining occurrence this is an applied occurrence declarations are also called definitions but we will call um we call them declarations um this is a grammar with non terminals  refer slide time on 01  30 min  i will use corresponding low case letters to distinguish the fact that an actually interested in syntax trees and not in the grammar per say which has other implications besides that for semantics  refer slide time on 02  05 min  so we define the notion of free identifier in an expression and in particular in the let construct the identifier that is declared is not free and we defined bound identifiers  refer slide time on 02  20 min   noise  in this fashion and the set of all identifiers is just set of all free and the set of all bound identifiers right  refer slide time on 02  35 min  we defined the notion of closed and close terms of a language and ground terms of a language and a complete program is closed an expression without variables a term without variables is ground then finally we define the notion of an environment as a function in environment over a set v of variables is just a function which maps variables onto a particular domain of values and an environment is a set of variable value bindings and this function actually defines the binding right so we could go little further and consider the set of all possible environments  refer slide time on 03  05 min  i will define list environment the set of all possible environments is the set of all possible environments over the set of all possible variables ya so i am using i am using the sigma notation rather than a big union um what i mean if i consider unions rather than environments rather than what is known as a disjoint summation then there is a possibility that a sum point you might have two environments which are suppose to be distinct in some logical way but it so happens that data present the same function okay disjoint union of sets is disjoint union of two sets supposing you have two sets a and b then it disjoint union of the two sets a and b is equivalent to having an identity associated with each element in the two sets so for example if the two sets one one not disjoint if they had some common elements then um a union b will just consists of all the elements of a and b without actually specifying an identity as to wave that element came from okay so supposing you have to take some standard representations if i take the union of set of all natural numbers and the set of all role numbers of b.tech student s okay that is just going to be the set of all natural numbers because all your role numbers are natural numbers but if i take a disjoint union of the set of all natural numbers and the set of all role numbers then there is a distinct identity associated with each element so role number like 94141 will occur twice in the set but once as a natural numbers and once as a role number so disjoint union if you want to look at disjoint unions the best way to think of them is as having tag the identities of the elements individually so if you have this two sets then i can lookup on the disjoint union as a cross let say a tag so i will give a tag like may be i will just use to i will use a tag of 0 for the elements of a and at tag of let say one for the elements of b okay if i take this so similarly if i take the set of all naturals disjoint union the set of all role numbers of b.tech students then by this i am actually tagging them appropriately i mean so i would actually have 94141,0 as one elements and 94141,1 as another element and the lb is distinct and the tag of 0 or 1 specifies which parent set it actually came from and disjoint unions are not unknown to you even though you may not studied them in mathematics you actually you actually encounter them in programming  refer slide time on 07  55 min  the variant record construct in pascal for examples specify a disjoint unions there is a case which is a tag an according to the tag you get some values so you could have you could have so it s a disjoint union in variant record in pascal like language actually specifies a disjoint union of two sets where the tag the variant part of the record actually disambiguates whether in a element is from one or another okay i mean i hope everybody is familiar with the variant construct so the variant construct for example is works is follows you have you have something like case this is with in a record declaration okay something you have some record r let say type r equals record something case tag you have to specify something which is let s say 0 or 1 or some so thing yet to specify tag of and do a case analysis of the tags so in this case you have i am using a um if you have 0 and 1 then you will have a case 0 of some declarations so for example that it s a it might be an other record declaration here case 1 of it might be some other okay but the point is had i could have case 0 of i could have single variable a which is integer and this case i could have a single variable b which is also of type integer end case right so right this if you look at this if you look at this kind of declaration in pascal it really specifies that i am using integers but i am using integers to meet to different things and what i mean by those two different things are specified by the tag  refer slide time on 10  23 min  okay so um disjoint unions of data types as is a standard construction though not use very much in mathematics it s definitely used in programming right so this environment just in order not to confused matters this environment is a disjoint union and not um union right so it s a disjoint union of all possible environments that you can have this is the set of all possible environments  refer slide time on 10  55 min  as a separate grammatical entity because it has a separate logical meaning we looked at some simple examples of sequential declarations nested declarations and we defined um the notion of updation of in an environment here is where the disjoint union becomes a vector so you have two distinct environments belonging to on way on variable sets which are not necessarily disjoint and then you want to give a preference as for any identifier what value it should take in the updated environment right so the updated the binding in this updated environment of any identifier i is row 2 of i if i belongs to v2 i could belong to v1 to that is not matter and if i does not belong to v2 then it it is whatever is define by row 1  refer slide time on 12  15 min  okay so we looked at the operational semantics of expressions um with a usual notion of configuration except that evey now one then you have to now decorated with the set of variables that you are actually interested in and we had this three rules where of course now the meaning of an expression is not clear unless you can assign a value to an identifier and that assignment of a value to an identifiers perform by the environment and so you have these three rules now with an extra assumption you can assume that this access an extra assumption under the assumption of an environment row the meaning of i is whatever is the binding define by the row and similarly the meanings of all these expressions depends on the assumption of this environment  refer slide time on 13  26 min  so then we looked at the semantics of the let construct so given any let expression let d in e goes to let d prime in e provided d goes to d prime and d is a declaration so there is a separate transition system for declarations that s why there is a subscript of d here where as this entire let construct is an expression defines an expression so this transition is on the expression transition system so that s why the subscript e is given there and so what so which means that you are essentially in order to evaluate a let construct you are depending upon first evaluating the declarations that the let construct contains and those declarations could be complex so your required this rule and remember that all or transition systems are simple manipulations so this declaration might during the process of evaluation get transform into some other declaration d prime and eventually after several applications of this presumably you will get you would identified in an environment row prime so declarations give you environments and you would probably get a an environment row prime and now this expression is strictly speaking not in the syntax of the language it is an expression in thus which we are using in order to specify meanings in a systematic fashion we have to specify one step transitions so there is a intermediate state in which the environment has been created but it still within the um this is some sort of an intermediate specification rather than either part of the um rather than being part of the syntax so in this environment the meaning of this expression e the this expression e can transit to um this expression with the e prime provided in this updated environment you can show that e goes to e prime um and once so several applications of this along with the rules for ordinary expressions in so on so far will finally should finally yield a constant and then the meaning of this expression is just that constant so however its necessary to know that  noise  all these are intermediate steps and eventually you are still interested in the original environment row you are looking at the evaluation of an expression which contains a declaration in the original environment row and in the process of evaluating you create a temporary change in environment in order to enable the evaluation of this expression and once the expression has been evaluated you revert back to the original environment the original environment is all that you have so these temporary changes what i wants mention does being reversible changes and you can see that they will once you look at the data structures in the programmatic you will see that they really can not they really involve no work at all ya so an interpretation of a of the let construct according to rules can just be defined as can be loose this specified in the following way right so what it means is that you first given an environment row you first elaborate the definition d or declaration d in the row and the elaboration of a declaration yields an environment row prime so you temporarily change row to row updated with row prime and evaluate e in this new environment till it yields an end and then you revert back to the original environment so several applications of the expression transition system should essentially boil down to these steps so that s a but notice that this is really not mean i have i have specified it as a sort of an algorithm but no where in a rules have you actually award decided on a data structure or an algorithm we just saying your apply the rules very much like we did in the towers ni problem whether in the specifications of the towers of ni problem or in the execution of the towers of ni problem just say you just apply rules which are not really algorithms but actually hide algorithms inside them and some of the assumptions actually hide data structures which of very natural inside them and the whole set of rules is sort of is precise is unambiguous and it is by induction on the structure of the syntax  refer slide time on 19  21 min  so lets now actually look at that semantics of declarations as a said this obviously this rule e4 has obviously involve some rules that we still don t know right you can not conclude this unless you can do something about this declaration transitions so that means we have to define a new set of configurations for declaration and define a new set of rules to process declarations also in a syntax directed fashion so now before i get down to the semantics of declaration lets so there is something here which i glass to over last time and again today and that is that in this expression language i have added a row here as part of you can think of this grammar if you like has being expended by an other production which from a declaration gives you this row okay right so this this extended grammar as a said this row is strictly not part of the language its part of our semantical specification language its not part of the original language and we use this this production actually in so if you add this production to the language then what then you have various possibilities like for example the moment you add this row to this to this language you have possibilities like let row in e you have other possibilities for declarations you can have declarations of the form row semicolon d you can have also things like d semicolon row semicolon d you have all these extra constructions possible when you add this when you add this row as a new production ya  refer slide time on 22  10 min  so now as a we required this extra row here as part of our semantical specification and not really as part of the language ya so we will so that s what we have used in some of in also in also some of the rules we share to deal with expressions right so for example that s what we have used here right we are use this construction that you can have an environment itself has been part of the declaration but it is not part of the language  refer slide time on 22  55 min  it is not part of the original language is just part of our specification language if you like part of this notation that we are using in order to specify the meanings of constructs ya so the set of configurations that we are interested in for declarations is given by is given by this set d okay where the d of course is not just of the language constructs but also includes that extra production d goes to row so you can look upon the row as we had we had the base case of this declaration as we had the base case of this declaration as i equals e and then sequencing of declarations now you can assume that for are specification to a precise a base case is include not just this for also this and you can therefore sequence mix actual syntactic declarations with environments in according to this grammar in any wave any which way you like right so you have a collection of configurations which goes beyond just the just the syntactic declaration so this d here should be understood as specifying whatever is specified all the possible declarations there are specified by the extended grammar with this production row ya and the set of terminal configurations is just the set of environments note there are terminal configurations the set td has to be a subset of gamma d right that s um so that s also part of that s also a good reason why you should include this row as part of this to semantic specification language ya and if you did not include this row and then this important condition will not be satisfied ya right so now as for as just a semantical specification constraints we allow all this and this this is a way of well a patch work way of making a language of meanings more expression in general the language of meanings has to be more expressive than the syntactical language that s all you get specification languages okay whose particular cases might be implementations in an in an existing language i think i already mention once that in general specification languages might contain construct which are probably not even implement able okay so in that sense in order to specify meanings you required a language that is more expressive but its not really a formal language it just a language loosely specified in mathematics and it was very convenient instead of i could have specified this gamma d such that it satisfies this condition by including a whole lot of conditions which exactly specified the set of syntactic declarations and the mixtures of syntactic declarations and environments that of allowable in gamma d but this um this scenes are very simple and easy solution to the problem of how you can necks semantical things which syntactical things in order to specify your semantical ya so this configuration set of configurations includes all those next cheers of actual the actually the abstract environment row because they they signify intermediate steps in your computation which since um the um one way are specifying the intermediate steps of your computation is to actually write out the data structures completely write out the algorithms completely and so what for them that becomes to dependent on a particular implementation you would like to abstract a way from if you giving a language definition you like to abstract a way from particular implementations and make it as general as possible and this is a very simple and neat way of making a specification more expressive so that you can specify lower level details without compromising on the abstraction from implementations from particular implementations ya so right so now lets look a declaration so the base case of the declarations is when again at the small the small i indicates that the small i and the small m indicate we are looking at trees i agree that i is a just a single note tree but it still a tree or not and not a non terminal in the sense of the syntax of the language so this is a syntactical construct which represents a tree and the effect of this is to just give you a value binding so you can evaluate this you don t really require this environment row of course in order to create this new environment but that is a but that is beside the point because you will require this row any way as you apply this rules ya so the next thing is what if this construct is an expression is a full bodied expression in itself so if it is a full bodied expression then what it means is that you have to evaluate this expression in this environment row okay now we already know the evaluation of expressions in this environment and which means a this rule d1 will be applied it several times till this expression e reduces to some constant in which after at which time you can apply the rule d0 create this new environment ya so this is something like the induction step which again depends upon the main the transition system for expressions right  refer slide time on 30  15 min  so um i shown you this dependencies um dependencies here right so the fact there is a e means that you get this dependencies circularly but of course its not a circular definition because if you do the induction on the syntax the syntax is finite so the induction on the syntax shows that you have a decreasing sequence of um decreasing a descending chain of complex the expressions so it s the induction is perfect it just it looks circular right okay then supposing you have complex declarations of this form d1 semicolon d2 then this will move to some d1 prime semicolon d2 provided in this environment row d1 can move to d1 prime okay so given a huge given a so which means that this movement of this declaration from d1 to d1 prime is closely linked to the application of the rule d1 where the expression only the expression moves so if you assume the d1 is just a single declaration of the form x equals some expression then the movement of this d1 to d1 prime is link to the movement of this e to e prime now and again as we did in the case of expressions  refer slide time on 31  45 min  we are looking at left to right sequentially evaluation okay this means and particular that even though in the domain of configurations even though in the domain of configurations we have allowed configurations of this form we have allowed configurations of this of such forms even though such configurations have been allowed in the set gamma d if you strictly follow a left to right evaluation of declarations you will not obtain this kinds of configuration ever like but that s okay because in any mathematical specification give um always have redundant things lots of redundant things that s okay  refer slide time on 33  20 min  so what it means is that according to our rules such configurations will never move anywhere ya so let s look at where of course i mean the this d is purely syntactically entity and this d is also purely syntactically entity so such configurations will never will never have any transitions so they will be what on one is stuck configurations okay so and then lastly we have to know what is the effect of sequencing on declarations is just provided by this right so supposing you have already evaluated the left declaration in a declaration of the form d1 semicolon d2 you have already evaluated the left declaration d1 and introduce an environment row 1 and then the evaluation of d2 is sequence in such a fashion that d2 can not move to a d2 prime unless in this modified environment that the new set of variables d2 can move to d2 prime right so in order to evaluate sequences of declarations you have to first have provided and new environment okay  refer slide time on 34  35 min  in a more complex setting where you are not strictly constraint by left to right evaluations where you might allow non deterministic transitions you could add extra rules such that such configurations also become meaningful okay right but note that in general the sequential declaration is such that and this is something that is there in all programming languages  supposing you have a sequential you have a sequence of constant declarations the second constant declaration could depend on the declaration on the first constant declaration right but if so you could for example give a complex set of rules which specify that given three declarations of the form d1 ; d2 ; d3 if you are perfectly clear that there are no free variables in d2 which occur in d1 there is absolutely no reason why you should not evaluate d1 and d2 independent there is absolutely no reason then why you shouldn t evaluate d2 first introduce this environment row and then start evaluating d1 and d3 okay but as a said we will be sort of we will make it we will make it clear that or rules are such that you can have a um let say something like a uniprocessor implementation right  refer slide time on 36  02 min  so what is the interpretation of d1 ; d2 is just that you have a you have a given environment row and then you elaborate d1 in row d yield row um to yield a new environment row 1  refer slide time on 36  55 min  then you elaborate d2 in row updated with row 1 yielding row 2 and then the result of elaborating d1 ; d2 in row is just row 1 updated with row 2 so this um this interpretation for example does not preclude certain possibilities which are not avail um which are not allowed in most languages okay but for example there allowed in ml take an ml like program of this form you could for example have let x = 3 ; x = x + 1 in x * x this is something there is not allowed for example in a pascal constant declaration you can not have the same identifier occurring in the same sequence of constant declarations twice on the left hand side but here what we are here really the way we are defined this declaration is that it really allows you to first declare something like this and use that so this x here is going to be so the updated environment um will have x = 3 after you have process this declaration and that x = 3 will be used in this to give you a new updation of x which is a x = 4 and in that updated environment x * x will be evaluating and so you going to be a x16 but you can try this out for example in in your ml environment and it should perfectly all of such things right so  noise  lets just go through a small example so here is so here is an example of a nested lets construct right  refer slide time on 38  45 min  so um you have a let x = 3 in let y = x + 2 ; x = y + 1 in x + y and you have this various syntactical elements in it which i have named e0 e1 e2 e3 e4 right so now this is a complete ml program in a itself if i want and what i would like do is evaluate it an empty environment ya  refer slide time on 40  20 min  so the program is not hard to analyze really but what you require to do or a few examples to make sure that you are rules are right and that you do not get any configuration that is strut right so let s say go through this so and this rules also give you a reasoning mechanism which is which is very simple which you can do in a top down fashion or a bottom of fashion if you like um this nothing in the rules which prevents you from doing it either top down or bottom up but since we are followed a constant policy or doing things as for as possible drop down will continue do it drop down so essentially the question you are asking is what does e4 go to in the empty environment pi with no predefined set of variables so this is the empty set of variables pi so what does e4 go to e4 is an expression so it it has a transition in the transition in the expression transition system and this e4 really is so this question is the same as asking what is this um what is this do i skipped a few steps in between but those are easy to filling right so which is which is same as things supposing you are process this declaration then what is this transition go to so if you have the answer to any of these transitions you have the answer to the final so you can think of it either as a top down questioning which top down recursively questioning or if you know the final answer you can think of it s a theorem that you have to prove with this is lm you all with this is an intermediate step so supposing you have the value here you know the value here then it s a it likes a theorem that you can prove i can prove that this is this goes to this value eleven provided i can prove the this goes to the value eleven by an application of some of this rules similarly i can prove the this goes to the value eleven so i can prove the this goes to the value eleven provided i can prove this that this goes to the value eleven and so on so the transition of this depends upon evaluating things in the updated environment and finding out what e3 goes to and so this pi updated with x = 3 is just the environment x = 3 so you are asking in the environment x = 3 with x as a single variable in the environment what does this transition what does this expression yield okay and this process is tedious because is um is just pure simple manipulation  refer slide time on 43  26 min  if you are actually strictly applying the rules that we are given it s a very tedious process but the whole point is that it hides an algorithm it hides an execution of an algorithm in it some where and its really meant to be done by a machine okay so i got a long story short by doing some by making some outright claims which are obvious right so here is a claim right so here are two claims i claim that which are which are really obvious and can be proved using the rules that in the environment x = 3 y = x + 2 yields environment y = 5 and this i have put a star here to indicate that this claim might be prove in 0 or more steps of proof of the original transition systems constitutive for expressions and declarations ya it may or may not you may or may not be able to prove this in a single step of a proof but in some finite number of steps you can prove this and um and what i mean is but purely mechanical process of application of the rules and there is a further fact which i highlight later and the next claim is that if you have x = 3 updated with y = 5 which is of course equal to the environment consisting of x = 3 and y = 3 y = 5 with the variables x and y then this of course yields the environment x = 6 and this again can be proved an a purely mechanical fashion and finally we might conclude that the main body of the expression yields this in a so it has to be the main body of the expression really has to be evaluated in this environment x = 6 y = 5 which is of course obtain by the fact that x has been redefined so your considering in an environment x = 3 y = 5 updated with x = 6 and the result of that updation is just x = 6 y = 5 and this yields whatever so um so the whole problem boils down to just evaluating x + y in this environment and that finally yields eleven  refer slide time on 46  09 min  so i will just listed out the last three steps from the expressions in from the expression transition system in all these cases assume that this environment is duplicated in all these step here before the turns type so the final conclusion is so if you just work back this eleven and fill up all those question marks that we given earlier with this eleven then what you have this proof a top down proof what s a top down proof i have a theorems statement and i prove that statement by saying that this theorem is proved if i can prove this this statement is proved if i can prove this this statement is proved if i can prove this and so on so far down to the last line  refer slide time on 47  19 min  okay so you can prove a theorem also has a set of goals to be achieved as a sequence of goals to be achieved till you finally achieve the goal or you can start you can first make a guess about what what is the achieve what um you claim is to be achieve and actually prove it in a normal manner but these alternate forms of prove um something that we have used in mathematics always i mean many trigonometric proves you start with we have to prove this and so therefore that we can prove this provided you can prove this and so on and so forth you you start with a left hand side you start with a right hand side and make the meet but it is in its not a good presentation of prove but we know that that proof can always be converted into a regress proof once you know the final answer right so its its really in that sprit that you can look upon these rules as giving you rules for proof and more important than just being rules for proof after all given a certain set of assumptions there are probably lots of theorems that you can prove right so the important point here is that these rules are purely deterministic in fact if you look at any individual step you know exactly which rule you have to apply in order to well depending on whether you going about it in a top down fashion of proof or a bottom up fashion of proof you know what rule you have to apply its purely deterministic and in that sense is very different from a mathematical proof if it is deterministic what it means is that if you can some over encode this rules a algorithmic protections then you have a deterministic algorithm which will give you deterministic answers which will give you unique answers so the form of these rules is like a mathematical theory the forms of proofs that you want to do as human being also like that but at any point in that proof there is only one rule available and it is deterministic and it s predetermined what rule you should apply in order to obtain the next goal from this previous goal in that sense is not just a proof to be done by human beings it has the at least the potential to be for being implemented in a very deterministic manner and what actually enables this determinism here is that fact that are transition systems are deterministic the rules allow only left to right evaluation of expressions only left to right evaluation of sequential declarations and that itself makes in deterministic that makes the proof process also deterministic so for any um so given a proof there is a so given an conclusion there is actually a unique proof which is not necessarily true of a mathematical theory right so since and this this uniqueness is guaranteed by this two things  refer slide time on 51  02 min  one is the deterministic nature of our rules and the second is that this your always doing only induction on the syntax  noise  or in the case of the rules we are induction on the extended syntax where we included row also in the syntax so you are doing only induction on the syntax and there is a there is an absolutely deterministic there is only one way of obtaining a proof if you rules a deterministic and that means that you can actually evaluated by an algorithm what makes theorem to being a typical task is that given a set of assumptions you do not no what theorem it might proof if a theorem prove a mechanical theorem prove a you do not know what it can prove so the theorem prove a has to be guided at all points of non determinism in order to give your deterministic answer in order to work towards the goal that you are interested so most theorem prove or interacted because there are two kinds of non determinism if you did not give it a goal it might proof something but that s that may not be what you want if you gave it a goal it might get um it might have there might be two many different possibilities two many different ways are proving it and so it requires a user to guide it along some path right there is an this case or rules are such that it is the purely deterministic there is only a single way of reasoning on the induction by induction on the syntax trees and therefore it has a potential of having a deterministic algorithm right and so what it means is that the rules themselves give you essentially an interpreted design the design of an interpreter where you want reversible changes and environments so i nice data structure which allows for reversible changes and environments is a stack but that s an implementation matter that s not bridged by the semantics ya and the fact there it solve done by induction on the syntax tree means that you can implement the control algorithm either the interpretation art core generation followed by interpretation both in a recursive descent manner as part of your purser for the language except that the purser of the language has a more refined grammar which takes into account various things those things you will have to patch but except for that you have the glimmerings of an algorithm to implement right so there is a the recursive descent here is to be note that a semantics only defined an syntax trees and not an actual and the actual syntax that the purser uses which is a much more refined in sophisticated form of sophisticated grammar than what we are considering in our semantic  refer slide time on 54  15 min  so what it means is that the main glitch in going from here to the algorithm is to determine exactly which point of that pursing routine you will have to include the code generation you might have to distribute the code generation in various across various routines in your recursive descent parts okay thank you transcription  gopalakrishnan c principles of programming languages dr.s.arun kumar department of computer science iit delhi lecture # 13 commands welcome to lecture 13 so today will quickly look at commands but what i will do is i look at a commands in a sort of simplified setting so we are previously looked at expressions and declarations which are two important classes of syntactical categories with different meanings um however i don t want to complicate matters by introducing commands on them instead i introduce commands separately with expressions and then latter will integrate them with declarations so further um further purpose of this this lecture we will just assume a wild language which is like the most skeletally complete language for programming structured language for programming if you like so what will assume is that we have a language with no declarations but you can have lots of identifiers and al those identifiers are variables variables in the imperative programming sense in the sense just like in pascal or  check 01  37  so an of course for the present while we are still discussing the various major syntactic categories we will assume that variables are only integer variables um so there are no declarations but we will assume that we can freely um update variables so we have and since there are no declarations we really can not without a declaration we really can not distinguish between a constant and and a variable so we will assume that there are no constants later when we integrate declarations into this imperative language then will also look at environments and so an so forth so for the present will just assume that we have got the simple imperative language which has which consists of a language of expressions which have literals integer literals integer variables and the standard operations um let say the binary operations and integers right and of course since um program in this language is just a command  check 02  58  bottom of fashion really the commands form the um complete program is just a command okay and since we have conditional conditions here we require a sub language of boolean expressions so as so so i will assume a simplified sub language of boolean expressions right and  noise  since there are no declarations i don t want to complicate matters by allowing boolean variables um i will just assume that there is a boolean constant there is a boolean constant in the language call true and the only kind of boolean expression the which can which consists of integer expressions is an equality checking expression so and otherwise you can have negotiation or disjunction i just choose or because valid we are to choose one binary operator the rest is simple the one one unary operator one binary operator and one one constant besides this equalities of expressions and main grammatical category is that of a comma is that of a command whose basis of course in an assignment statement there is a sequential composition of commands as in the case of as in language like pascal then there are conditional commands and looping command right so in the um in the last week um in the last few lectures we looked at an expression language with declarations so it was very much like a functional language here we looking mainly at an imperative language without declarations because in an imperative language the main syntactic category or commands so we lets look at this so if you have to give a semantics of the simplifier language we should in some sense we should be faithful to the previous operational semantics that we already given for expressions  refer slide time on 05  22 min  except that now since we are considering in imperative language with an assignment statement there is an updation of variables and so we have to um we have to come up with the suitable concept which allows for updation of variables so and of course there are the usual criteria like we should not unnecessarily load this is load or semantics with programmatic features we should not unnecessarily bring in data structures or algorithms we should just give minimal concise rules which is specify the language in machine independent um architecture independent fashion right so what will do and something that you must of all seen in your programming courses is the concept of a state in fact that s um that s in fact the most the most common way of describing imperative languages if they are straight day s languages right so there is a concept of state which is some abstract entity and for our purposes and for the purposes of most discussions on um on the provision concept of imperative words its functional languages um a state is really nothing more than the variable value binding in that sense is it looks no different then what we said about declarations in the functional language so there also just identifier value bindings okay so however here the difference the reason as state is different from an environment for our purpose is is that this variable value binding is really really represents a bindings that can change with time right as will see the as will see in the semantics okay so this the semantics of those wild languages important because practically everything that you want if you want to talk about a simplified imperative programming language everybody would pick up a wild language and and give it essentially just the state based semantics or give it its define its meaning in a state base manner in all programming courses also you um the wild language is considered the basic building block from which through which programming a start and you talk about stay its you talk about changes in state and so on so forth so its its instructive to look at it at the same time but looking at it in the simplified fashion we might also understand how to express the semantics of boolean expressions and in general what happens in the case of commands right so we will combine all these things in a single framework and then will go on to actually looking at programming languages in which they have to be in which the notion of the state this notion of the state is not sufficient and you require to argument it with different concept right which is closely related to it and closely related to environments but it actually separate right so we will just assume that expressions are evaluated in a state so they are very simplistic assumption and commands are state transformers that s that s really what that s really what happens in the case of an imperative language  refer slide time on 09  20 min  each command can modify the state in some fashion in and those state changes the state transformations are irreversible changes okay in the sense that um to undo the change requires at least as much effort as was required to make the change right so we look at just state as an abstract concept which is got nothing to do with value memory or whatever and we look at state changes so given to given to state sigma and sigma prime belonging to the set of states we assume that there is a predefined collection variables v always  refer slide time on 10  10 min  since there are no declarations we have to assume an unbounded collection of variables available to us and we will just assume that they all have some value associated with them very often that value could be undefined so you could you could argument your value domain with some element call the undefined and you could initially an uninitialize variable is could be regarded something that is got the value undefined right these things of course have representations in actual hardware and there is a null value which can be used which is not which is not a data value belonging to any type right so i will consider the change from sigma to sigma prime as and represented in this fashion as a one point change in the sense that sigma is sigma and sigma prime as and represented in this fashion as a one point change in the sense that sigma is sigma and sigma prime are everywhere identical except at the variable i okay and at the variable i whatever sigma might be whatever may be the image of i in the state sigma in the state sigma prime its value is n and that s what this indicate right so what we are saying is that sigma prime is everywhere identical to sigma except at i where sigma prime has the value n and what sigma has is no concern or it has some value but it may not be the same as end ya and for all for all that means so what it means for all other variables for all variables other than i sigma and sigma prime yield the same value and for i sigma prime yields the value and right so this is this is the very important thing because the assignment statement is involves the one point state change right so it s absolutely basic so we go through this semantics of this language in set of rapid fire fashion so that it acts a revision of whatever by previously done an introduces if you new the concepts at the same time so the language of expressions well it s not very different from whatever we are previously defined it s just that now we are looking at expressions evaluated in a state so you can think of it us um us replacing row by sigma if you like so but it um to be more rigorous what will um what we have what i am what will do is we look upon the evaluation we look upon set of configurations for expressions so everywhere you carry a state with you in the execution of a program and the set of configurations of expressions is this the collection of all added pairs of expressions and states and the set of terminal configurations is just the collection of numerals along with the state right and so we have to define the transition function  refer slide time on 14  00 min  so i have i have put a prime everywhere to distinguish if it from the last semantics but you will see there it doesn t look to different from the last semantics right so let s quickly go through the expression language and you will see that it is more or less like the last one except that since i am considering expressions to be added pairs of this form the meaning of an expression i in the state sigma is the value assign by sigma to i in the state sigma right so it s just the value of the expression i in the sigma and of course in you carry you carry the state along with you every time the meaning of an operation a binary operation on two constants on two literals is really whatever assuming that the binary operation is already available in the  check 15  09  is just whatever this result true aided by that operation in the  check 15  15  right and this is the base case of left to right evaluation for expressions so this rule just say is that given a constant m binary operation some expression e this goes to m binary operation e prime provided in the state sigma the expression e can moved to e prime in the same state sigma note that we are considering a very simplified language in which states state there are no state changes that occur in the evaluation of expressions then going further given a complicated expression of the form e1 binary operation e2 in a state sigma or left to right evaluation strategy says that in this state sigma e1 moves to e1 prime then this binary of this complicated expression most to this complicated expression in the same state right so the um rules e not to e prime not to e prime three are really set of syntactic translation of the rules we have e0 to e3 of left to right evaluation expressions and some environment right  refer slide time on 16  35 min  so now of course since it s an imperative language with um with conditional statements and looping statements what it also means is that we have separate categories of boolean expressions so in addition to um before we can actually give the semantics of the commands we have to define the semantics of boolean expressions so let s quickly look at the semantics of boolean expressions so i will assume that we the set of boolean expressions there are possible includes all boolean expressions are we already have and to predefined constants t and f denoting true and false there is no reason why we should assume that t and f have a representation similar to that of the constants there um we could be type respecting in whatever we do and we might keep boolean constants separate from the integer constants and this is not two this is not to outland is this is not to outland is because there are architectures for example which are tagged so in many tagged architecture there is a tag associated with um every every memory location which a type could be specified and storage is um storage is allocated only based on the type right so this is not two obscure or a outland is and um as we did in the case of the declaration language though i am not explicitly specified it here we will also include this constants t and f as part of the boolean expression language right so in addition to so you can think of the boolean expressions as having in addition an extra protection of this form an extra two productions of this form and the language that we will be considering first semantic purpose is includes this two so you can form boolean expressions with these constants and so on so forth in the process are evaluating a boolean expression you will you will also include this constants ya as part of this one  check 19  30  no no no that this true is language specific syntactic entity this t is a constant available in the underlined virtual machine  noise  okay the two or not the same i mean they not exactly the same if you want make them the same does the different matter but this true is well it s um in the syntax of the language and you always be using the never in a program actually use this this t or this f right this t or this f is some constant provided in the underlined virtual machine and we um any way lets not need pick to much because um what will happen is that the semantics of this true will just turnover to be the t and that s it in our single step i mean this nothing more to it okay so its not its not a serious its not a its not a very serious matter if you want you could for example have gotten read of this true and put this t and f here just as match as having it separate right but very often you have to distinguish between um an element of as language from an element of its meaning this is something that this is something that should be clear from whatever you done for example all of you are studied truth tables right so if you look upon boolean expressions as a language then there is a separate construct call true and a separate construct call false in this truth table semantics what you actually give or not thus the constructs of the language but of a semantic domain which consists of two different values in which the true and the false have corresponding representation right if you if you been mixing of these two i mean that s it s not it s not a series matter but the distinction between syntax and semantics has to be made as to be kept clear i mean one of the things yet you do is um you actually you can define the language of lets say i don t know propositional logic or boolean expressions without the syntactic constant true and false and may not have it at all after why should i i could just have propositional variables no true and no false if you want a representation of true or false i will take it to be of the form a or not a there is absolutely no need to introduce to into this syntactic language right i mean could keep true and false purely in the semantic language and have a language of boolean expressions which is totally expressive is not as the constant true and false can not be expressive i could just have the various operators i mean i could have let s say various boolean constants other than true and false for example i could just define the language of boolean expressions or let assume that you have truth values right so i could just define it  noise  to be a language which consists of a set of boolean variables right and  noise  just these operations not a a or a a and a there is nothing which tells me that i should have true and false in the language at all right so i can define the complete propositional set i mean this is the complete set of adequate connective so propositional logic alpha boolean algebra i don t need to have the constants true or false what is x x is just an identifier or a propositional variable or a propositional constant boolean variable or a boolean constant other than true or false just okay now what i um now with this this is a completely adequate set of connectors for boolean algebra what i am i do is in specifying the semantics of this through truth tables what i am it specify is that um my semantic function is a truth value function which maps the set of all i don t know um the set of all um the set of all propositions so lets me let me call this the language of propositions it just map this to the collection of truth values t and f right and i will give and the important thing to realize is that whatever is in green is the language of propositional array whatever a specifying the truth table is really function of function call t which maps given a truth value assignment to each of this identifiers it gives me a truth value um truth value of component expressions and this no true any where in the language right this should be um this may be this is not mean explicit made explicit in any of your previous courses  check 26  05  but essentially it s that there is a semantic domain called t and f which is completely different from your from the language of boolean expressions or from the language of proposition logic right so the fact that i um introduce true in the language is um is just something i just did i want to introduce a boolean constant so introduced it that this it s enough for me to have this these three connectors and i can um i can get all possible propositions at i want so this t and f belong to the underlined semantic domain they do not belong um they need not belong to the language to the syntact um syntax of the language at all  refer slide time on 27  30 min  it s a different matter that very often you introduce um in because you want to you want a specify that um for example um tetralogy is an identity for val is an identity for the and operation for a contradiction is an identity for the or operation so what you do you explicitly introduce two separate constants that s a true and false to make a distinguish from t and f in to your language so it you can specify those equations in a convenient fashion but this absolutely no reason why you should do that i could specify at all um of the form a or not a i could specify contradiction of the form a and not a so that the fact that i have introduced a constant into the languages is is really just my business the constant may not be in the language at all its part of this thing is part of a language what is that in the underline semantic domain or these two two primes ya  noise   refer slide time on 28  20 min  so lets look at so what i am saying is that this is not necessarily the same is this and your elementary knowledge of architecture and whatever or hardware will tell you that there is an explicit true in pascal but what um what it what is it actual representation in the hardware its not true at all i mean something else um depending on the underlying representation right so let s look at the semantics of boolean expressions so we have this set b along with these two constants t and f and we have a set of configurations which are all gamma b which is just this entire set actually it should be this um this b includes the t and f two okay um unless it includes the t and f you can not have a set of terminal configurations of this form right so you might as well introduce you might as well introduce this as as equal to sum b prime if you like and call this b prime in straight right so um right um so you have the set of final configuration and my first rule of the semantics of boolean expressions is just that the constant true evaluated in any state sigma gives me this the t of the underline the truth value of the underline um your machine or interpretation right and not of this constant t is f this not is again an entity an operator of the language an it s got nothing to do with anything that  other language  hardware right okay and if you got a unary operator so then this is the basis of the not operation and this is the induction step if you like so if you have a complicated boolean expression b which is being negated with not then you can evaluate this not only after you have evaluated b and drop it down to a truth value right so in if b is some complicated boolean expression it might have to be evaluated in several steps in which case it might remember that there are syntactic transformations so simple manipulations this b might go to some b prime and therefore this not b will go to not b prime and this rule will be applied several times till finally this b has been reduced to a truth value okay  refer slide time on 33  26 min  they should be one more rule which says not f sigma is t sigma two right so um introduce that so that is that is a b1 prime which have introduce here right this is not f equal to t right so and that gives me a complete set of um evaluations of boolean expressions  refer slide time on 33  07 min  except that a boolean expression language really depends upon um that s for the not operation so we have look at the or operation so here is the here is the semantics of the or operation again as in the case of the expression language we will look at left to right evaluation right so if you got an complicated expression of the form b1 or b2 then you first evaluate b1 till you reduce it a truth value then you evaluate b2 till you again reduce it a truth value and once you got two pair truth values um you can actually get another truth value and of course these um this b5 should actually be b4 should actually be replicated separately for t and separately for f similarly b5 will actually have four copies depending upon this truth table right so um i use this question mark inverted question mark and exclamation is being elements of this semantic domain arbitrary elements and the connections between them are given by the truth table right so actually there are like four um four different versions of b5 depending upon the truth earlier truth values set your interested in and similarly several versions of b4 one for each truth value but i don t want to stress it to much because these are elementary materials ya so rather than use new variables and so on and so forth i use these symbols as variables over the semantic domain okay  refer slide time on 35  15 min  so this is as well as boolean expressions are concerned and combinations of boolean expressions are concerned except that are boolean expressions the only way we can form boolean expressions or through equality of expressions right so we have to look at expression equality so which means the evaluation or equalities as again specified in some such fashion it has to be related to the transition system for expressions and b6 and b7 just as the aided for the expression language for binary operators and expression language they do the same for the equality where equality is really regarded as a binary boolean operation between expressions right so this b6 and b7 against specify um b6 b7 and b8 actually specify a left right evaluation of expressions to yield an appropriate truth value where it must be understood that this the resulting truth value says that these two these two constants i ve used three lines here to indicate that we are not talking about just equality of constants but they should be identical as patterns they should be identical as patterns and that s that s the only way a machine with no intelligence can really recognize equality given that it has no other information what it can recognizes equality adjust to identical patterns right so if the two patterns that you get from evaluating e1 and e2 are identical though i given them different names here then you would say that these yields true otherwise it yields a false right now this um this one thing about this in our grammar we actually specified the boolean expressions as a separate grammatical category which used the expression language whereas here we are treating boolean expressions and the equality of boolean expressions at essentially the same level as the expression language we are specifying left to right evaluation of an equality expression and so on and so forth which wireless makes programmatic sense it actually doesn t event make programmatic sense but assuming under a uniprocess implementation that it make some programmatic sense it s um it s really  check 38  09  point it would be simpler actually look upon this replace all those these rules by a single rules of this form so this says that if e1 sigma goes in zero or more steps zero or more moves to some constant m and e2 is sigma goes and zero or more steps to some constant n then e1 equals e2 goes to some truth value depending upon this depending upon essentially this condition depending upon whether the two constants m and n other same or different right so this way you provide an abstraction from the expression transition system and also another important thing is that by giving your rule of this form your clearly abstracted away from the order of evaluation of expressions this rule just say is that this rule say is that in order to able to conclude something you required two premises and it really doesn t matter in what order this two premises are true if you can some of prove that even sigma goes to this constant m sigma and e2 sigma goes to this constant n sigma then and doesn t matter in what are the whether you have evaluate in parallel whether you have evaluate non deterministically whether you partially evaluate one and then partially evaluate the other it doesn t matter in some order you have evaluate them and if you get two constants them i can then this equality of this two expressions most some truth value in a single step whereas an application of b6 to b8 says that the equality of expressions really has to move in several steps based on the depths of this expression based on the depth of the boolean expression e1 = e2 at and what it does is that it mix no difference in the abstraction level between the boolean expressions and the expressions and even for a simple language its actually tedious to go around doing this if you going to do things in the recursive decent fashion  refer slide time on 40  20 min  it make sense do an abstraction at an appropriate level and leave that abstraction as base case of a lower transition system so so b9 is actually what you might say an abstraction from the underlined transition system it also does not specify an any order of evaluation and um and it well and it greatly simplifies the number of rules that you require which is important if such a simple language requires so many rules then you can imagine what a real programming language would actually look like right so we will use this abstraction to defined the semantics of commands so we will assume the existence of the underline boolean expression transition system and the integer expression transition system and look upon commands as some high level entity in which the whole point is that in this kind of an abstraction what we are saying is as for as the boolean expression evaluation is concerned  refer slide time on 41  55 min  how many hours steps the sequential expression evaluation might take all 0f that is regarded as a single step one single step in the boolean expression evaluation right and similarly in the case of the command language we look upon as many transitions of the boolean expression evaluation and the integer expression evaluation steps as constituting a single atomic step of this level right okay so what about the um the set of configurations the set of configurations is just the set of commands as usual i will use this script c to denote the set of all possible commands small c to denote the syntax tree of commands of a command of an arbitrary command and here we have a slight difference in the sense that once we have executed a program there is really nothing more to execute so there is the command at the end of the execution once the program execution as terminated there is no command left there is nothing to what is left it s only a state right so your terminal configurations here or just the collection of possible states so the end of a program the end of a executing a program all you have is a single state which is you can look at all the variable value bindings that s it this nothing else left in the intermediate stages of course you have some commands left so let s look at so you concede that are command language has two distinct types of entities further intermediate configurations and final configuration okay so the assignment statement all that it say is that the assignment statement can go in a single step to this to an updated state provided this expression on the right hand side of the assignment can go in zero or more steps of the expression language to a constant m and that s in this in this in this setting there sol that is the meaning of the assignment statement  refer slide time on 44  37 min  it s you will see that it s it actually have some power reaching consequences right so let s look at the rest of the um so the assignment statement constitutes the basis of the command language right end that s c this rule c1 actually this the base system right now we have consider component statements so where the sequential composition that we have got just like we have a sequential composition of declarations we could have a sequential composition of commands and um and so what does it mean the sequential composition just says that let s look at this let s look at c3 first if c1 can directly go in a single step c1 in a state sigma directly yields you a new state sigma prime with nothing in it then c1 ; c2 in the state sigma yields at an intermediate configuration of form c2 sigma prime right here is where the first state changes actually take place so this you can look upon as a basis for the sequential composition so if you have the whole lot of sequential compositions this c3 tells you that if essentially that if c1 is an atomic command that means it say it s a assignment command in this language the only atomic command the command language is the assignment statement so if c1 is an atomic command which just yield the state change then this sequential composition gives you the sinter mediate change so and c2 tells you the induction step for the induction on the number of semicolon set of their in a command right so this is the basis of the induction over the number of semicolons right so if c1 is not an atomic command but is instead itself some complex command then there is no reason to suspect that in a single step it will give you a just a state it will actually give you some other command again remember that it also in simple manipulation so it should give you some other command with possibly a modified state sigma prime then the um then this entire program c1 ; c2 then moves in a single step to this configuration with the modified state right okay so let s look at so this is as for a sequential composition concern c2 is also applicable when c1 is something  check 47  53  sequential composition for example c1 could be a conditional statement or a looping construct so in the case of a conditional statement we will assume that the boolean can be evaluated in several steps possibly of the boolean expression transition system to a single truth value and if that truth value is t then the effect of this conditional is just to get is just get rid of c2 and everything else in only retain c1 as part of the configuration since a boolean expression transition system did not allow for changes in state the state remains the same so this c4 just specifies a transfer of control an evaluating a boolean expression that s it nothing else happen i mean nothing else happens to the programming state  refer slide time on 48  58 min  it s just transfer of control that occurs and the control is such that c2 is discarded after consuming b1 b and getting a truth value t and this is the program that you have take circuit  noise  right so similarly of course if the boolean condition evaluates in zero or more steps this zero or more step is important because is boolean expression if for example in our language this would not be zero or more steps it would be one or more steps but if you actually had um if you are t and f you are actually part of your language then there is no evaluation involved in this so it could be zero or more steps right but since um my t and f are not part of the language this this is um this will actually turnover to be one or more steps right so if um if the condition be evaluated to false  noise  excuse me then you just discard this branch c1 and you have c2 left and of course there are no state changes because the evaluation of the boolean condition does not change the state and this also just signifies the transfer of control to c2 right and the while statement is very simple if the condition b evaluates to true then you manipulate this while be do see there is no state change what is the how is the control change you execute the body c and then you execute the while statement again okay so what happens is that having evaluated b and having got a truth value t the program that you have now left to execute is c is more complex then the program is started out with because you have to execute the body c and execute while b do c again right um so in the case of a looping construct this is how  noise  you specify in advance what the transfer of control is going to be you first transfer control to c and then you append this entire construct again to the um you sequentially compose this entire construct with the body c right okay and of course if b is false then this entire construct works a to know operation and you get just the state sigma right so this denotes the termination condition for the while loop right  refer slide time on 51  53 min  so this is a this is a since while loops um and most imperative constructs familiar to everybody it s a desired to go through it very fast and anyway gets to boring otherwise what we have done is that we are for boolean expressions we have specified complete evaluations you could actually modify this transition system for partial evaluations by partial evaluations i mean short circuit evaluations and you could even to parallel evaluation of boolean expressions and you could modify all these transition systems also of parallel evaluation  refer slide time on 52  33 min  okay i will stop here this provides a brief overview of um how to specify commands and boolean expressions will get into the more complex issue of stores in a next lecture and why we require some modeling of memory transcription  gopalakrishnan c principles of programming languages dr.s.arun kumar department of computer science iit delhi lecture # 14 stores welcome to lecture fourteen we will quickly run through the semantics of the while programming language and then go on to what complicate matters so this is the syntax of the while programming language  refer slide time on 00  50 min  the most important things or that the consist of imperative commands which change some notion of state and at a find state as the simplified form as a variable value mapping sigma is the set of all states we will assume an infinitive set of variables available to us expressions or evaluated in a state commands state transformers okay  refer slide time on 01  20 min  we will defined a value also has variable value binding and your defining state also um that s right that s a question will address and other discussion so we look at state changes so given to states i defined a state updation in which which really corresponds to having a single assignment statement  refer slide time on 02  04 min  this updation could be changed could be generalized to also multiple assignment but may be worry about that later  refer slide time on 02  20 min  so in the expression language what i said was that at this state at least they have seen to be no real difference between environment and state except so in the expression language at least there is no significant difference so you have the same kind of rules  refer slide time on 02  46 min  they evaluation of starting with the evaluation of identifiers in the state and the normal rules for expression evaluation which are strictly left to right sequential evaluations and then we since a while language also contains boolean expressions we required semantics of boolean expressions in a general programming language with boolean expressions would be treated on power with the normal expressions there is only a type difference in the sense that you would allow boolean variables and assignments to them and whatever you kind do with integer expressions similarly you can do with boolean expressions to however in the case of this particular language since it s a sort of greatly simplified language and we did not allow boolean variables and we did not allow any other kind of with the boolean expressions via use only as a via media between in order to construct commands conditional commands right so this has this following semantics but even when the boolean expressions are an integral part of the expression language you wouldn t the semantics would not change to much okay the except that that one would have to look at the type distinctions between the various kinds of variables and one would have to do something about types and that s the matter will get into after some time so here again we defined the evaluation of boolean expression in a left to right manner with um in um of particular interest is this unary operator which we did not encounter in the expression language the boolean expression language actually assumes the existence of a boolean algebra and a semantics of semantics as defined by a boolean algebra for boolean expressions so for example you have the true and um the ground values t and f are not identical and unless you specify clearly that there are not identical you could have a one element boolean algebra and all the properties of boolean algebra should be satisfied so and the fact there are not identical  refer slide time on 05  17 min  there actually compliments of each other is given by these two rules these two  check 05  17  b1 and b1 prime right so and of course we had to relate then we have this usual boolean operation which essentially show a complete left to right evaluation b4 indicates for example so if you have a general boolean expression of the form b1 or b2 then you would repeatedly apply b3 which means you would be evaluating the left operand till it reaches a truth value and then you would start evaluating the right operand so it shows a left to right evaluation and of course it always satisfies this the standard truth table for a boolean operation  refer slide time on 06  04 min  so everything is subject to the existence of such a model of meaning in the boolean expressions and a since a boolean expressions acted as some sort of intermediate stage between the expression language and the command language we have to relate the boolean expressions also to the derivations of the expression language and of which we had only a simple operation the simple relation um the equality relation an expressions so this these the rules b6 and b7 also show that also give you a left to right evaluation of the equality you would apply b6 several times which means you would evaluate the left  check 06  59  of the equality till it converges some integer value and then you would evaluate the right operand and use till that also converges to some value and you would check the equality of these two based on the fact that m and n since they are there in the underlying machine as patterns if they are identical is patterns then it would be true otherwise it would be false right  noise  right  refer slide time on 07  30 min  so as a said i mean we don t need to go through this whole thing we could for example abstract away from the intermediate stages and simply give fairly non deterministic rule  refer slide time on 08  25 min  we says that if e1 can evaluate in a finite number of steps this star indicates a finite number of steps zero or more to some integer value and e2 can be evaluated in a finite number of steps to an than this can be concluded depending upon this patterns amend n as given by this right so and then this this rule this rule has a nice property that it does a further abstraction from the order of evaluation in the transition system so on so far and you could modified this boolean expression evaluations for example you could have a strict right to left although evaluation if you like or you could have a partial evaluation you could give rules for partial evaluation um especially um especially the or operation for example or any other binary operation you could give rules for partial evaluation  refer slide time on 09  00 min  in particular if if the left operand for example evaluated to the truth value t then you can have a single transition the entire boolean operation could core to t right so that so that you don t evaluate the right operand of the r right so and you could also to parallel evaluation you could to parallel partial evaluation so there are many variations depending on how you want to specify the language right so then we came to the most important thing and that is commands and a define this set of configurations as a said commands change states and the first rule of the command itself tells you one thing um this one important difference between states and environments and that is that in an environment you have a you can have redeclaration i mean the whatever modifications you make or redeclarations okay redefinitions of given identifiers and however if you remember what we spoke what we said about declarations is that the little environments the declarations create or only intermediate stages in a larger computation a declaration does not stand by itself an expressions stand by itself it s specially in a functional language the expression is evaluated in an environment and in during um in the process of its evaluation little environments are created and destroy so the evaluation of the expression from start to finish goes through some intermediate stages of the creation of new environments in which they might be redefinition of identifiers but the whole point is those changes in the environment are reversible in the sense that whatever gets declared also goes out at the end in the case of states we are talking about capturing a general notion of um an irreversible change inside the value of the identifier right so in the so conceptually even if symbolically didn t look to different conceptually at least there is an important difference which we have to capture some of okay and one of the reasons via will introduce stores is to capture this conceptual difference between environments and states right so this the assignment statement is is an examples a change that is irreversible  refer slide time on 12  05 min  for example if you have a large command evaluated in a state so if you have some command which is going to be evaluated in a in some starting from some initial state sigma so let s have some large command may be with some assignment here and um and something else okay when this is evaluated in a state sigma starting from a state sigma not at the end of this you get a new state sigma one which is different from sigma not in the sense that in the sense that x now has the value which is one more than the x given be sigma not right and in and this state change is irreversible in the sense that it s starting from this state that further state changes take place as you execute the command and you finally end of with some final state let s say sigma f at the end of this command right in the case of an environment what you had what you could a had inside um declarations which created little environments but at the end of it you still had the original environment with the same name value bindings that you started of it so the environment row not and the final environment row f for the same there is no difference in the case of state change um so in that sense all the changes that occurred inside in the evaluation of an expression for all reversible where is in the case of state changes and in general in an imperative language we are looking at some concept which allows for irreversible changes to be made if the sigma f need not be the same as sigma not sigma f may not be same as sigma not even for the value x unless an equal amount of work was done to get x back to its original value and sigma not so even if you a command had been such that it restored all values back to the original and your sigma affairs the same a sigma not those changes um irreversible in the sense the amount of effort require to make those um undo those undo the initial changes that you made as much as the amount of effort you require to make the changes in the first place there is in the case of environment there is something automatic about the way and um the new environments are a created get destroyed right  refer slide time on 15  40 min  so there is something conceptually different between states and environments right and this is something we should capture and notion as abstract a state does not really capture that notion as we capture this difference totally but it as capture to this extent in the sense you can see from the semantics that state changes occur and a final state need not be the same as a initial state so um then what is a store we will come to the notion of a store we will come to the notion of a store but let me first go through this um via language um so i showed a sequential evaluation of the sequential composition operation semicolon and so this means that you evaluate the left operand of the semicolon till it reduces to having made just a state change there is nothing more left of the command so commands are something that get consumed and get reflected a state changes and then you start evaluating the right operand okay you could evaluate the right operand and in the case of the conditional we evaluate the boolean expression and you transfer control the transfer of control is like as getting rid of one arm of the conditional depending upon the truth value of the boolean and lastly we had this um a while loop and of course there is something about this while loop which is not quit correct i mean does not quit meat to the philosophy this started out there what is that after the execution of the while loop the loop the state doesn t change it does change if you work to sir c6 ya c6 says that this is equivalent to and this moves in one step to a program which has this form and the sequential composition rules tell you the that could be state changes for example you have some c1 semicolon c2 which is which is really what the c semicolon while we do c is like so this is c1 and this is your c2 and c1 could create state changes and could finally terminate and then you would evaluate c2 and this new state sigma prime right  refer slide time on 18  35 min  so it s not as of there are no state changes but the something like that is seriously wrong with this while loop it confirms or in treason about how while loops are executed but it does not confirm to some other aspect of our semantics and notion of the semantics what is the most important feature about semantics syntax directed translation recursive to  check 19  30  what is common to all this what is common to all this definitions um this is something that is this is what you get by cut feeling after having programmed enough um while set of language but there is something wrong with it and no result the finite machine is even complicated then ever could your trying to absolutely he said the final expression as even more complicated then the initial expression we started of um which means that this definitions is not inducted several applications of this um might only give you more and more complicated means it s like well its like having its like what s wrong with you recursive definition lets take a function on numbers the f of n is f of n is zero if n equals zero otherwise if f  n + 1   1 what s wrong with definition so what s wrong with it mathematically and what s wrong with it computationally  check 21  53  mathematically it s just a equation which has to be solve for and there is a solution to i mean the solution is f  n  = n is a solution to this equation as a mathematical equations what is wrong with it is not mathematically what is wrong with it is computationally in the sense that it is not inducted right and this this semantics of the while suffers from the same problem this semantics of the while suffers from the same problem the example this semantics if you take c6 and c7 then what it means is that in any state in any state while b do c is equivalent to you take a combination of c6 and c7 its equivalent to b then i will use begin and end okay because i require to bracket begin c ; while b do c so c6 and c7 are mathematically quite meaningful in the sense that all that the say is that this is equivalent to this if you had a no operation in your language and you could also right there is else skip a skip is a no option anyway so its they just just gives a equation and does not and which is perfectly meaningful and which is why implementation actually work this way  refer slide time on 24  30 min  it s perfectly meaningful but however its not inducted precisely because the resulting expression is much more complex structurally than the original expression right but the fact that while loops are executed and give you meaningful results means they must be a way of giving a semantics which is induct in valid let you think about it there is a simple very simple solution to make it inductive just like um just like there is a very simple solution to make this to make this computationally morning input right you could other just gives the non recursive definition or you could even recursive definition which is computational warning  refer slide time on 25  27 min  so that is the way simple solution which i will not going to now any way this is what this is what um this is what your gut feeling says should be the semantics of the while loop except that it is not inducted right  refer slide time on 29  42 min  so anyway so um what this is let to is something that i mention previous um that an important notion of an important semantical notion which should come out as part of your operational semantics or any kind of semantics is um programmic equivalence right so let s let s look at programmic equivalence in the context of this language and i would say that we start with the expressions two expressions are equivalent if and only if for every state sigma in which they are both evaluated they yield the same values there are to be something else um so what is so i can define a function which denotes evaluation so i can i can define a function i can define well a function if you like eval of e sigma there is no reason unless you have proved unless you are explicitly proved um there is no reason to believe that evaluate an evaluating an expression gives you a single answer right till you have till you are prove that it until you have proved it you can not be sure so will define eval of e sigma as being equal to the set of all values which so which those look upon this as a set of all values that eventually lead to a terminal configuration right so in the expression language terminal configurations are of this kind so we will just think of um so if for example an express evaluation unless until you proved it you don t know whether expression evaluation actually terminates do all expression evaluation terminate this is something that you have to prove supposing you haven t proved it then you can not assume that expression evaluations do terminate in which case this sat would be empty right so or the might be some some evaluations of this expression which might terminate some evaluations which may not terminate in which case lets just for simplicity assume that the evaluation of this expression consists of just the set of evaluation set of values which which they are which are reached and termination which is not quite correct okay but for the present lets live with that or we could also introduce a new  check 29  32  element call some undefined or something which represents non termination and we could define the set as the set of all possible  noise  evaluations which yields proper values and yield an undefined value when whatever may be a definition of this evolve we of course have to consider only a finite number of steps and evaluation right  refer slide time on 29  57 min  so we would say the two expressions or semantically equivalent provided in all states in all possible states they evaluations of these two expressions yield identical such evaluation right similarly um in the um in the pervious model of functional programming language with environments so on so forth we could think of it us evaluating these expressions in all possible environments so we would say the two expressions are equivalent only if in all possible environments in which there evaluated the yield the same set of values right and the same wholes with boolean expressions and when are two programs equivalent  refer slide time on 32  15 min  i will define the notion of an execution in which i will say that given a um in our case in our simplified programming language a command is also a program so its know distinction really um i can defined this function call executions and i can define it as a set of all final states right so i can define this concept of an execution of a command or a program in our case and i would say that so um i would also call this a behavior so a behavior of a um behavior is a general word so the behavior of an expression is just the set of all possible values it can yield which is what our definition of evolved as and the behavior of a command is a just the set of all possible finite states that it can yield given an initial state why and when not to programs equivalent only when for every initial state in which you start executing these two programs c1 and c2 the yield the same final states at the same set of final states again in this case you do not know unless you have proved it that um it s a the program well actually for a given input state will actually yield only one input one output state there are non deterministic languages which do not necessarily satisfy the functionality state to state functionality okay right so a semantics actually gives us  noise  way of defining program equivalence and what is program equivalence finally work out to what is boolean expression equivalence finally work out to it works out to just all the loss of boolean algebra what is program equivalence work out to program equivalence just works out to equality as functions so if you consider a program as a transformation from the set of all states so if you can think of functions of the form right this is the power set of states note that we are not yet prove that a program would give only a um unique final state it could give several possible final states so typically if you want to regard a program as a function then you have to regard it as a function from states um two the power set of states okay  refer slide time on 34  05 min  so if so when are two programs equivalent they are equivalent if they represent a same function and if so it should be possible to use the semantics to reduce it two two functions so given two program c1 and c2 they represent corresponding functions f1 and f2 and then it should be possible if they are both if c1 and c2 are equivalent it should be possible some other to prove that f1 is the same function as f2 right so one initial promise that i made of being able to define program equivalence um and behaviors and so on is essentially full fill by these definitions and um what will do is you can generalize this as we go along in each case you can defined behavior in such a fashion that it represents some functions um it does not make much sense to really talk about the behavior of a declaration but we can we could define similar notions for declarations we can talk about two declarations being equivalent except that it is one more um one problem associative that what is that what happens in the case of a declaration a new environment fed what is that new environment it some value bindings so supposing a declare um supposing a decide that the behavior of in analogy to whatever we have done the behavior of a declaration is just the new environment it creates of the set of new environment it creates um when it reaches a terminal configuration then i go further and say that two declarations are equivalent semantically equivalent if and only if the create the same new environment now declarations might occur within the expressions one reason why we had to declare behave why we had to talk about the behavior of expressions because programs can not be semantically equivalent till we have actually defined in adequate notion of equivalence of expression since a commands depend upon expressions you have to define in notion of equivalence semantic equivalence in the case of expressions so what what is it that can happen in the case of program equivalence or in the case of expression equivalence what is inadequate about this notion of declaration equivalence may be many more different changes in store no may not on the host 2 just assume an expression language with declarations likely we already done so i i define the same thing two expressions are equivalent to define only under all environments they yield the same values right so lets look at this um now i define two declarations to be equivalent if and only if they yield the same environment  check 38  50  so okay fine so while what i say is that um so supposing i define declaration equipments as two declarations d1 and d2 or equivalent if and only if for all initial environments row  noise  let me call this a new function called elaboration usually talk about elaborating at declaration in an environment right so you could say the two declarations are equivalent if it where of course this function elaboration elab of duro is defined as the set of all little environments row prime such that in the environment row if d is elaborated then in a finite number of steps gives you a new environment row prime an of course um i am force to use this set because give not actually a yet proved but it can be prove that an elaboration at least with the syntax and the semantics that you have given will give you a unique new environment row prime so with this definition of elaboration i can define this but while this might be correct it is some or not adequate it may be because environments theory are temporary um ever change that is true that doesn t really effect expression equivalence so the declarations and mentions make and turn the expressions and the expressions where not be no expression equivalence um expression equivalence means expression equivalence i mean what does expression equivalence finally boil down to whatever you know about the mathematical theory of those expression so if you are talking about expression equality in something like number theory then which means that you know two expressions will be equivalent only f the corresponding um  refer slide time on 42  20 min  i mean there equivalents can some of the justified by number theory eventually because when you evaluate you get this number or at least you get expression on numbers on pure ground terms and you should be able to prove that those two numbers of the same and plight you will just using pure mathematics and no programming okay but there is something what i am um this is while this is perfectly correct  check 42  35  no but but we have we are considering there equality in the same and one this universal qualifier says that you elaborate them both in the same environment and if you get  noise  for all possible such same environments if um those elaborations give you identical new environment but then they are equal but what i am saying is this definition is some what view  refer slide time on 43  03 min  its not incorrect in the senses you can t give a counter example to it but it s weak in the sense that it doesn t really get us very far why is that i mean what i mean is two expressions can be equivalent even if the declarations inside them or not equivalent that sense is weak two expressions could work out to be equal but the declarations inside them could be completely different then might create completely different environments and that s what happens in most cases i mean for example even if the declarations look identical they could still differ up to name changes i could uniformly replace names and one declaration to give me another declaration and by this definition the two declarations will not be equivalent just i just to name changes and this is equivalent to saying that when i have to submit my assignment i take his program uniformly replace all the names i drop a table of names and i give corresponding different names and all matriculations are different okay but the two programs are still equal and the reason the two programs are still equivalent if i have made uniform name changes is really because those names not matter there part of the reversible changes which are very very intermediate okay so talking about equivalence of declarations and more ever declaration is man to perform various kinds of abstractions which are val i mean somebody met use less number of variables and what complicated expressions somebody met be able to deal with only very simple expression so uses more declarations so declarations could be different in various ways and the expressions could still be equivalent so the equality of expressions does not necessarily depend upon the equivalent of equivalence of declarations and this name changes are always done i mean its it s a thriving industry especially in iit right so um so this kinds of um so its that s one of the reason why i am not to bother about the equivalence of declarations or at least i should have more sophisticated definition of equivalence to really worry about it without can be done ya right so there are other important questions with regard to whatever we are said about that about our semantics we have to worry about when configurations get stuck and what is the stuck configuration a stuck configuration is one in which it s not a terminal configuration it s a non terminal configuration from which there is um no movement then you would say a configuration stuck then of course there was this problem of the inductiveness of whether all or definitions are inducted and the while loop definitions were settling or inducted so one way of avoiding stuck configuration was that if you make all the definitions inducted so if you are definitions are not all inductive then you do not know whether you have taken care of all the possible syntax syntactical constructions if you are definitions follow the syntax at least you are sure that you taken care of all possible syntax constructions so at the level of a context grammar at least you are sure that you have taken care of all possible cases if you are definitions are not inducted then there is this problem they they might be certain syntactical constructions which you are not taken care of  noise  and you have to explicitly prove that you are taken care of all possible um syntactical constructions and if you are definitions are not inductive you can not even use induction in order to prove it will have it prove it by some other complicated means right now can there be stuck configurations even if all definitions are inductive  refer slide time on 48  30 min  let you think about that what in the mean while let s talk about stores ya so now it s clear that the environment and the state are really two different concepts and they are try to capture two different things and what and state is a very abstract concept which and is um and we have try to model the assignment commands where will consider the right handed side of the assignment to be a source and the left hand side to be a target but the thing is that or assignment commands are very simple example they did not allow side effects and when you talk about a side effect its equivalent to some kind of an invisible assignment taking place some where without your knowledge right  refer slide time on 49  16 min  it is at least invisible within some um outside some scope right secondly so even if you take or working definition as a store being on association of identifiers and values what happens when you have these complicated kinds of targets of assignments right i can have for example an array which indexes into on other array and i can update one array element in some complicated fashion through an indexing mechanism i can have pointers so my targets are not necessarily just simple variables there not just identifiers they are actually complicated expressions because the identifier p has some meaning and this is vacant operation on p the identifier a has some meaning i has some meaning j has some meaning and this is some complicated ternary operation on a i and j so targets are not necessarily just identifiers they are actually expressions right so then there are further complications what about when identifiers get re declared can you just talk about a store as such an association  refer slide time on 50  50 min  it s a re declared identifier different from the identifier that was previously there what about scope issues right and so and for and also not um all target mean expressions might be meaningful so there is a separate subset of the expression language which targets then d not all source expressions can be meaningful  refer slide time on 51  09 min  there is a separate language of source expressions sublanguage so in the whole language of expressions you can actually divided into well not really divided because they um they also have they not two disjoint sets there are two subsets of target expression valid target expression valid source expression and they are not disjoint then you also have anonymous targets so for example if you had such a loop every time you are create a new p your creating a new anonymous target  refer slide time on 51  52 min  actually you are creating a target which has some meaning if you look upon this as a name it has the meaning there but then the next time you come around the loop the previous invocation is anonymous it has no name how you going to look at the previous um invocation right so your environments themselves are not sufficient to worry about it and or abstract notion of state it s also not sufficient to look at complicated programming language behaviors right and further and when you have recursion you have the same identifier the same name for several different logically distinct identifiers which have got nothing do its scope so there are all um and then you have this you have several different names for the same object when you have something like a call by value or um when you have a call by reference when you have the war parameter declaration in languages when you um i can do a point of assignments so that several different identifiers actually refer to the same object and with anonymous things several um with recursion on an um with recursion  refer slide time on 53  25 min  i can have several different objects having the same name right so um so these kinds of things mean that we have to refine a notion of um state to something that s more tenable right and so we we come out with will come out with the notion of a store in response to the fact that a simple state based  refer slide time on 53  49 min  looking upon states as just identifier value binding is simply not sufficient to account for most um most behaviors in real programming languages so will define the notion of stores and then integrate that with the notion of environment transcription  gopalakrishnan c principles of programming languages dr.s.arun kumar department of computer science iit delhi lecture # 15 summary welcome to lecture 15 today i ll just summarize some of the concepts per done so far and for play ground for the future right  refer slide time on 00  40 min  so we started with the notion of environments we decided that environments are some variable value bindings a for an expression language for an ml like expression language with declarations then we also looked at some possibilities for reversible changes in environments  refer slide time on 00  55 min  we discussed environment updation and we also looked at how these environment updations are always of reversible nature for example in the expression language with the let construct as in ml  noise  we are evaluating an expression in some original environment row and if there is a declaration inside it what it means is that you temporarily update the environment to a new little environment  refer slide time on 01  41 min  the processing of the declaration gives you a new little environment which in which you evaluate this expression but eventually you revert back to the original environment and these changes are reversible  refer slide time on 02  07 min  on the other hand while looking at a simple imperative language we first took a simple view of what it means for um we does we defined a abstract concept of a state and we define that also as this variable value bindings and we um define this semantics using this um state transformation as an updation mechanism with of course and we um and we were of course interested in modeling state changes  refer slide time on 02  25 min  so we have a state updation um notion which is which looks a lot like the environment updation notion but is actually quite different because it leads to um irreversible changes so we also define the semantics of commands within underline expression language um in which for example this is a permanent this is the permanent state change um wherever you have an assignment statement there is a possibility of an irreversible state change um  refer slide time on 02  58 min  we discuss the various control structures um ya this most elementary control structures namely sequential composition um we gave an inductive definition for that and always when we are talking about commands we are talking about the possibility of state change and other control structure like the conditional control structure and the looping control structure right so and and of course in our general discussion on transition systems  refer slide time on 03  21 min  we also had to answer we also have to keep certain things in mind in general which um which is independent of programming languages these are some general questions like um when can configurations get stuck and that is something that you made you made require to no even well modeling other systems which are not necessarily programming language based okay in the particular case of formal languages but formal languages i mean programming languages and specification languages may be any language which has a formal grammar which can be completely defined by a grammar um these definitions inductive so if if um if you are definitions are all inductive then they you have a certain level of confidence that you have that most of your configurations will not be stuck because then you can use the induction process to prove that your some of your non terminal configurations are not stuck  noise  so but they could be stuck configurations even if all your definitions are inductive and this is something we have seen this is in programming languages  refer slide time on 04  55 min  what it usually means is that this would lead to some form of runtime error not compile time runtime error so a typical example is in our definitions for example um we have not explicitly consider various binary operations we just consider a general binary operation but if you what to take something like division as a binary operation then the axiom the corresponding axiom for division will would have to include a site condition that your never dividing by zero and so what that means is that there are configurations in which you might be dividing um in which an expression made contain a division by zero which would be stuck for which would not able to apply this axioms are rules of influence right okay so so the way we defined thing so far um is that provided all our expressions are total functions by total functions it means that they are not undefined for any any particular value of the arguments then you can show that no non terminal configuration ever get stuck and another thing you can show is that the execution of while programs is deterministic so so far we are looking only a deterministic programming languages in fact we will never actually look at nondeterministic programming languages but it is important for us to know that deterministic in this case means that our transition relation is really a function  refer slide time on 06  34 min  which it need not have been to start with right end a powerful proof technique for proving properties about programs is an induction method also one obligation is is that you have to you have to also define what is made by a behavior and we defined behavior of expressions in terms of an evaluation so if an expression in the imperative language if an a given state the expression um can evaluate to some um terminal configuration then the value in the terminal configuration is taken to be the result of evaluating this expression in that state in general as a said since we are not assured that the transition relation is a um is a function in general this means that you could have more than one possible result for the same expression in the same state however once you proved at your transition system for the language is deterministic the this the result of this would be a single ten set right sometimes not empty of course sometimes empty of course but that is  noise  when you consider more complicated cases where evaluation itself could be um whether expression could be a recursive and therefore the could be an infinite recursion or the expression could be could have commands embedded um in which there are infinite while loops etcetera  noise  then similarly for boolean expressions we defined the notion of evaluation and for commands we define the notion of an execution and commands yield states and since are transition relation is deterministic what it means is that every command defines a function from state to state  refer slide time on 08  45 min  so from an input state to an output state and what it also means is that um it also confirms the view that commands or state transformers okay having define the notion of a behavior we defined program equivalence or well two expressions are equivalent if and only and or all states and or all environments the yield same results ya  refer slide time on 09  25 min  this of course has to be taken with the pinch of salt um two commands are equivalent or two programs are equivalent if and only if under all input states there executions yield identical final states  refer slide time on 09  40 min  we also looked at the utility of actually trying to um give a definition for equivalences of declarations and well set s um it s a weak definition but will will leave with it for sometime okay then or main problem was that we had defined the state as have we have defined the state as an association between identifiers and values in our while programming language and then that brings up various other questions when you actually want to extent the language to other constructs um then it turns over that you have you have for example the assignment is the most important form of an imperative command a state changing command and all other commands which change state in visible or invisible fashions can be made can be modeled in terms of sequences of assignment statements so this is the this is the common practice so the assignment statement is very important and what it turns out um notion of state is rather weak because firstly the um the left hand side of an assignment is not necessarily always just an identifier it could be an expression secondly identifiers are redeclared and there is some there is an irreversibility there is a reversible process also when um when you consider scope issues lifetimes of identifiers and so on and so redeclared identifiers means that they actually two different identifiers carrying the same name and then there are issues like scope lifetime extent which will um which so with the simplistic notion like a state  refer slide time on 11  23 min  its not really possible to capture that also the notion of side effects and what happens in side effects is also quit default to capture in the case of states because because of the fact that many different objects might actually have the same name but the same object could also have many different names due to aliasing and therefore when you have aliasing when a when the same object has many different names then what you have a side effects are essentially invisible assignments  refer slide time on 12  13 min  i mean you assign you make an assignment to one name of that object but that means that you alter also simultaneously made assignments to all other names of that objects to of the of that object to so there are all kinds of invisible effects which complicate matters so in general when you look at the assignment statement it might have complicated expressions and also there is a restricted language i mean not all source expressions are meaningful not all target expressions are meaningful  refer slide time on 12  42 min  this se t of meaningful source expressions is not disjoint from the set of target expression there are anonymous target expressions there are anonymous objects which are created which do not um i mean um which and names keep getting transferred as new objects are created and old objects loose their names and so what we um so what it all boils down to is when you have anything like an assignment statement in a programming language then there is um an implicit part of that assignment is that there is some need for new entity call locations which some which some our look like which which some our is a model of memory in um in a in most digital computers in fact so if you look at if you look at all these problems what happens is that it s necessary to identify this notion of this object and we have object creation in several different ways temporary or permanent and we have updation of its value so we require something in between an identifier and a value and that is and for that we require to we require the existence of a new domain of of objects called locations which always a new  refer slide time on 14  20 min  whenever a new object is created it has a new location and the new location has contents right so and this is particularly when you have when you have anonymous objects and especially names changing from one object to another during the process of the creation of new objects we require to be able to locate the old object two even though it has lost its name and so in this process what it means is that are notion of a state which was very close to the notion of an environment really has to undergo some drastic revisions right so and first firstly we require to be able to model irreversible changes and we have to be able to distinguish between environments and and a set of objects which a have irreverse um which are distinguish by this process  noise  and so we will look at locations as some where intermediate between identifiers and values so and then are declarations in our imperative language automatically take a certain meaning so constant declarations would just be treated like the variables of ml  refer slide time on 15  30 min  so they would just be identifier value bindings where as variable declarations would actually be identifier location bindings so during program execution you could you could have new names you could have the same name transfer to several objects but all those objects would have different locations ya so at least you would be able to look at locations some how um as identifying as identifying the basic structure of an object during execution  refer slide time on 16  40 min  so will um will define variables as identifier location bindings and of course um and will define locations as associations between so will define locations as containing values so there is also location value association which is not really a binding the word binding is normally used for declarations and so on so far so and we will another thing is that it we will look upon us the targets of assignments only as locations and the sources of assignments as the values contain in locations right  refer slide time on 17  30 min  so you can think of a location is containing a value which can be changed during command execution but during the lifetime of this location there is an identifier to location binding which is which comes from a declaration right so you can have  check 18  00  so we will assume that new locations are always created by declarations every location has a lifetime or an extent okay which usually lasts still that location is explicitly or implicitly disposed off so the fact that a location is created by a declaration means that it um it does not lost does not necessarily lost forever because a declaration creates a little environment which has a lifetime which is usually govern by the scope of that declaration right so you can so we can have um so what constitutes scope in a program text works out at runtime to a lifetime for each object that is created in that scope so each object that a declaration has um is created when the declaration is elaborated and it has a lifetime which lost still the end of that scope and at the end of the scope it is automatically disposed off of course its possible in the case of for example pointer based pointer based creations to dynamically create new objects new locations which are created with typical um statements like new p for example and which get disposed off also explicitly so the creation of so locations might be created either through declarations or through commands and they might be disposed off either because of and they have a finite lifetime which might depend upon whether they are explicitly or implicitly disposed off right so  noise  and when they are implicitly disposed off we are um usually talking about a reversible change and when they are explicitly disposed off it talking about in irreversible change so we will assume that instead assuming that we have an unlimited number of identifiers which can always be used because of the problems associated with having the same identifier for several different objects and the same object having several different identifiers we will assume that what we have us unbounded is really a number of locations so every time a every time a location is created there is no conflict between that location and anything else that is being created before and which still is living which is not dead  noise  in fact we will go further and say that every time we um create a location we create a brand new location which has nothing do with any location that was created in the past or even died in the past which is something we really can not say for identifiers so locations actually take that  noise  take the roll of an unbounded number of identifiers that we had in the state so a refinement of the concept of state is that you assume at any stage that you have only a bounded number of identifiers but because you um you have declarations when you elaborate declarations you get only a bounded number of identifiers you require only a bounded number of identifiers however because of the fact that you might be dynamically creating new objects to which you might bind the existing bounded number of identifiers what you require really or an unbounded number of locations in um in practical terms what it means is that um you really assuming that you got an unbounded amount of memory in modeling the language in modeling the semantics of the language  noise  which is not which is not completely realistic but it s an idle which can be refine later when you define and implementation document with limits in post on those things right um so it s um the rest of it when you actually have a finite memory and you have to model the language then it s a matter of doing a certain amount of exception handling  refer slide time on 22  34 min  otherwise in the an idle environment you assume that you have um computational environment in which you have an unbounded number of resources but you have only a at any stage in the computation you have only a finite number of identifiers to deal with so um these locations can contain of course varying values in during their lifetime so we will define a store as just a finite set of location to value associations we won t use we wont bindings there because when we talking about this associations we are talking about associations which can change and this concept is not very surprising or it s almost an inevitable consequence of having an imperative model of computation so it really depends upon the model of computation that you process that you trying to that you trying to deal um that you are really trying to deal with and a any kind of imperative model of computation in which states can be updated irreversibly in which there are objects which can be updated irreversibly brings in almost the concept of locations right  refer slide time on 23  35 min  so we will look upon environments has been created by declarations and in particular we look upon constant declarations is identifier value associations with no locations in between and we look upon variables as identifier location bindings and stores which are a refinement of the state concept would just be location value assignments right so we might think of so um the important problem with the assignment is that the left hand side of an assignment is really a target the target of an assignment is um is as a said is um possible collection of expressions um restricted class of expressions and the sources of assignments are also restricted classes of assignments but the important distinction will make is that the sources are all values um targets are all locations and these are called l values and r values right and once you bring in the notion of stores and since you are almost um there is a concept of what can constitute a storable value that means what what kinds of values can you put in locations right  refer slide time on 25  31 min  so we look upon storable values for the present ya we look upon storable values firstly you could have you could have constants and them you could have you could also have locations themselves the storable values right  noise  and lastly because of the um set of an architecture that is that is an implicit underlying architecture of all imperative languages what happens is that your storable values could also be programs in fact this is the only way you can account for self modifying programs lot of self modifying programs so a typical examples either s either s expressions of lisp um with so what it usually means is that what it means is that in practical terms it actually very simple you have a piece of core which actually accesses itself and modifies it and then executes it but logically you are looking at looking at it is a problem of function or a program being apply to itself and function application to itself is logically unsound right however with the notion of locations and storable values these things can be given a meaningful interpretation without actually getting into any contradictions so i mean it s it s totally is mathematically unsound to think of a function f being to apply to itself but um but if you look at some of these self modifying programs written in list or even assembly language oftral you can load the entire assembly program in starting from some location and you can actually access that location and modified things if you like and then transfer control to execute the modified code while it seems practically quite feasible if your transition system has a transition relation which is actually a function then all those a  check 28  44  function being apply to itself a function has a certain type but how the how can you actually applied to either itself or to a modified form which in which um in which the types are that means if you um if you are function was originally defined as going from some domain d to another domain d another domain d prime this kind of application means that for such an application you are actually this function also has a type that it can go from d arrow d prime to d arrow d prime which seems quite ridiculous so there are all these contradictions which which um which is something that mathematician completely avoid but some our which seem to make programming sense because we know it s be it can be done and we know its possible so it turns out that once you introduce locations you can actually look upon locations has been distinct from identifiers and you can give types you can assign types to locations and to identifiers in such a way that such functions become meaningful if you provide a suitable mathematical structure any way so we won t go into that it s enough for us to know that for any consistent theory which requires an explanation of all the kinds of different forms of object creation that we have seen is necessary to have something like locations um and stores right so part of a storable values would also differ for example when we do procedures we will also define procedures has been storable values just like the um just like programs could be storable values right and that s in fact the only way to explain transfers of controls to procedures and to define the effect of procedures an certain values and the returning of certain values ya  refer slide time on 30  44 min  okay so i will stop here today and we will continue next week transcription  gopalakrishnan c principles of programming languages dr.s.arun kumar department of computer science iit delhi lecture # 16 declaration and commands welcome to lecture 16 so today will try to integrate declarations within a command language and see how to manage both aspects thus go back briefly to what we said about ml like programs there environments um variable value bindings and when you have a command language  refer slide time on 00  40 min  you require to have something which allows changes to be reversible and something else that allows changes to be irreversible and as a result you require we require stores in addition to environments  refer slide time on 00  55 min  the relationship between the two will be that the stores will depend upon the actual bindings created by environments and you can only have stores for given environments ya and so as a said the something implicit about an imperative programming language in um there is an implicit assumption about the existence of memory and the fact that therefore memory can be updated requires the use of locations so and these locations are intermediate between um between variable and its value  refer slide time on 01  35 min  so as suppose to constant which would um which would be just identifier value bindings variables would be identifier location bindings and further we would have values associated with locations with loosely speaking refer to values contained in locations and the targets of assignments would be locations in the sources of assignments would be values right  refer slide time on 02  15 min  so will for any given environment a store would just be a finite set of location value associations and these are set of an inevitable consequence of imperative language programming right  refer slide time on 02  30 min  so um going quickly through it will assume that environments will have environments created by declarations and stores will be updated by commands and in the case of the most important command which actually allows updation of stores is assignment statement or either explicit or in some implicit form which has a target and a source um and we have and the target actually refers to some location and the source actually refers to a value in contained in a location  refer slide time on 03  15 min  even though we use the same um same name for both in the context depending on the context you will either take there um the l value or the r value so let s look at the language today so i ll just take this very simple language it s so we have an expression language consisting of val numerals identifiers and binary operations for the present i will assume that these binary operations are fully defined none of them is none of the its not a partial operation we have the usual booleans may be and we have declarations so like in the case of the pl zero language we have constant declarations we also have variable declarations in this in this particular case you can see that since our only data type at the moment um are integers it s not necessary to actually define a type here so its enough to just have the declaration like this and of course we could sequentially composed declarations and then our commands have the usual commands assignment sequential composition may be conditional while whatever but more importantly we also have blocks so this is have so um informally you could you could look upon this as just declarations followed by commands but if you okay so um this language is sort of slightly is not exactly like pl zero because for example in this language well every identifier in a declaration is preceded by reserved word const or var and then they way we have defined we enough there is nothing with says that all constant declarations should proceed all variable declarations so this seems to suggest that you mix variable and constant declarations okay so which does not seriously affect the meaning of the language for example um you could if we if we follow so constant in variable declarations may be may occur in any order in may be interspersed okay but um if you follow the principle the declaration always precedes use then it doesn t really matter because since all the variables are uninitialized in a constant declaration you can not have any occurrence of a variable declared just before it right so the restriction that all constant declaration should precede variable declaration it s just a programmatic restriction and would not really affect much and since variables are uninitialized the constant declaration also do not interfere with the variable declarations okay so it um it really is useful only in grouping together  refer slide time on 07  20 min  the same kind of declarations into one entity and separating out different kinds of declarations for um for readability for parsing may be but otherwise it has no semantic  check 07  41  right so but in a will use this this basic feature that declaration always precede use so if you here to look at if you here to look at pl zero if you here to actually give a bnf for the pl zero kind of declarations then what would have to do is you would have to have a classification like this there according to those rules you could have you need not have a declaration at all for one thing you could have just a constant declaration just a sequence of constant declarations just or just a sequence of variable declarations or a sequence of constant declarations followed by variable declaration um and so i have introduce to um to new non terminal symbols dc and dv for constant declarations and variable declarations and you could for example the pl zero kind of constant declarations the variable declarations could be return out in extended bnf language so if the reserve word constant occurs then they has to be at least one constant declaration followed by zero or more occurrences of other constant declarations separated by commerce and similarly with variable declarations um  refer slide time on 08  54 min  of course you don t to need to have any declaration talk right so as well as blocks are concerned the important thing is that it s that blocks are really like unnamed or anonymous procedures right and as a result what it means is that the question of naming is actually very important in all um in any kind of programming if you have an unnamed block then what it means is that it can not be call from any where right so which means that it s a purely local block and therefore if there were any declarations there purely local to that part of the block so um so in the um in the bnf i have put in this this pair of braces here to emphasis the fact that supposing you  check 10  28  declarations and commands then you can not arbitrarily interest person because declarations create environments little environments and commands update stores given the current environments and this braces indicate actually the scope of a declaration right so this is so the whole point is at issuing the arbitrarily mixing of declarations and commands  refer slide time on 11  15 min  if you did arbitrarily mix of declarations and commands such that a declaration is available a throughout that command and you did not have this braces that s equivalent to moving all the declarations up to one place and separating out the declarations from the commands which is a cleaner which would be a cleaner design so the idea is that if you hurt to actually interest purse declarations with commands then you are you really you really want to have some extra local declarations and therefore you want to create a new block not otherwise right so which means that you have to clearly delimit the scope of those declarations that you interest personal so this is so this is really like um un name procedure and therefore what it allows for example is that it um allows you to so the declarations are purely local to the scope within which they are declare within which they are occur and they can not be called from elsewhere you can next blocks in this fashion because a block is just a command by whatever you the braces act like a begin and an end and so these blocks are really like algo sixty blocks and you could have local declarations within begin and an end right so you could next blocks if you like and so this these braces really have no um have no role to play except that the clearly delimit  refer slide time on 12  46 min  the scope of the declarations contain within them ya and so what happens is that so now lets look at lets look at the semantic definitions with in such a in such a setting so as usual we will have so instead of a state we are going to have a collection of so let s look at the environments okay so let s look at the notion of locations so we will so in any kind of semantic definition we of course assume that as in um the meaning of the language is really independent on the bounds of resources that are available for the implementation of the language so one thing is that we um we assume then there is a infinite collection of locations right and these locations are all um may all contain some values right however at any time in the execution of a program at any incident in its runtime behavior you require only a finite number of locations there are only going to be a finite number of locations used by a program so what um what should be available is an in exhaustible supply of memory but at any instant of time in the um execution of a program only a finite amount of memory is actually being used it might dynamically vary the amount but always only a finite amount so for any finite set of locations will define stores on that set to be the um to be the correction of all location value associations right so given a um given a finite set l the set of all possible values that may be associated with with each location is a store at any incident and the set of all stores is a set of all possible such mappings is set you can drive right and this stores um of course parameterized the set of um the finite set of locations that you have considering at the moment right so and of course as usual stores are just so this this set of all possible stores is just disjoint union of the set of stores for all um for each finite subset of locations right now right ya so this is our notion of locations and this is our notion of stores right ya so then lets then let s look at the notion of environments  refer slide time on 16  00 min  so now we have our um our languages such that it has more constant declarations and variable declarations and the constant declarations in the language are exactly like the variable declarations in ml like functional language i mean variable in a functional language really denotes the name of a value that remains unchanged through out a scope and that property is fulfilled exactly by a constant declaration what does change of the variable declarations so what so um an environment again part of the reason why part of the reason but all of it the reason why there are only a finite number of locations used via program is that its start initially with only a finite number of identifiers and therefore requires only a finite number of locations initially and on demand more locations might be required but always unless its an infinitely executing program always there will be only a finite number of locations used right so and in an environment so given a finite collection of identifiers and an environment is just a binding from identifiers to values or locations and of course this plus here is a disjoint union so which means that at any point you know whether a value comes from you know for any identifier in an environment it means it s possible to determine whether it s a location or a value right remember what i said about disjoint union it s um it s that you if you take the disjoint union of the two sets firstly if the two finite sets then the disjoint union contains as many elements as the some of the two sets and each element also maintains a identity from which of the component sets it actually it is actually taken right so that s a disjoint union and the set of all environments given a collection of identifiers ya you ensure the l value and r value can be or interchangeable this sense that you are meaning for feel that can be either value either t value or r value no all that i am i said last time was that the seems to be some confusion between the l value and r value what i said a few minutes ago was that the same name is being used for both to denote both l values and r values and the context determines what we should take its not that your allow to take anything okay what just further says is that your r value is could also be locations right um so the matter is quite complex its not is not so simple so the fact that locations could also be used as values as storable values means that locations could be r values themselves of certain expressions okay and you could you require a dereferencing facility to find the r value of an other r value which is actually an l value of some other expression so if you so the dereferencing operations in most languages like c you have the star which does dereferencing or you have the circumflexion pascal which does dereferencing that s only says that you can have an expression whose r value its actually a location and if it is the location then it means that you can actually locate its contents so you can take the r value of the r value of the expression but provided you ensure that your taking things in a disjoint union fashion so that the identities are maintain for example you can not look at the contents of an value of this form but certainly you can look at the contents of a value of this form and if these account to be the storable values when um this process can go on infinite term right all that i am saying is that the same notion is being used in most programming languages and the context determines whether it s an l value or an r value secondly r values could themselves be l values of some other objects so for the present let s let s not get into the complex use of l values and r values we just look at the simplest form which is when you have which occurs when you have a statement like this when you have the statement like this for an identifier x then this denotes the l value which means the location the identifier location binding this denotes the r value which means the value contain in the location to which the identifier x is bound and the so this so the context here determines when you should taken l value and when you should taken r value  refer slide time on 22  22 min  so um the context else you whether if with occurs on the left hand side of the assignment then your actually taking about a location if it is occurs in the right hand side then um actually looking at its at the value inside the location ya right so the set all environments is just again the disjoint summations of all possible kinds of environments so there are some other um so for example um what many what many other languages allow is that um declared variables are to be initialized in the declaration itself  refer slide time on 23  36 min  this is to prevent uninitialized variables from being used or accessed for some such that ya however this is not always possible for example if you are declaring a large array in um in a language like a dot then clearly its not possible to initialize it at compile time its simpler to initialize the array but reading the values from a file through an explicit initialization rather than to try to um initialize it at declaration time itself okay so but so um what does means is that variable declarations couldn t general modify the store now as you elaborate a declaration your environment changes so the associated store also changes in fact you are moving from some stores l to another stores l prime as you elaborate a declaration further even though even though you have variables are uninitialized we have to really talk about um uninitialized variables as actually containing some value after all if you have store is a function from locations to values the moment a location has been created that means there is a location um there is a value contained in it otherwise you are stores function is not really a function ya so one so one new condition will impose is that will added new value which um act to the second um and this value really denotes some undefined as its an undefined value and what happens with this value is since its just a value we really have to look at its effect on the expression language supposing the undefined occurs supposing this um uninitialized variable in some expression what is the value of that expression  noise  so most most languages would just tell you that it s since it s an a it s not this is not absolutely essential so for example languages like fortran which do not require declarations of variables actually take an initial value to be if it s an integer variable to be zero if it s a real variable to be zero point zero so if you do not um if you have not initialize the variable what happens is that very often it s it s assume that it has an initial value of zero so it s like an implicit initialization of the variable the moment it is encountered right however that often complicates matters both programmatically and both programmatically and semantically because firstly if i am not initialize the variable why should you assume that its value is zero or its value is some identity element that s firstly firstly a point the secondly a point um a point is that even though a your semantics assumes that there are an unbounded number of locations programmatically what you going to do is you are going to reuse memory as an when as an when it gets deallocated you are going to reuse memory and what can happen in that is that no bodies going to bother to do the house keeping in that in memory that is deallocated which means that that area of memory could could have previously contain some values when it was previously used and if you did not go through a fresh initialization what would happens is that the old value might get used so an uninitialized variable is something that is potentially hazard is if you did not so um its say for to actually assign it a value that is clearly different from any of the workable values set you have because it already contain some value without your knowledge and it is actually being used then it does not differentiate between an error of judgment on your part and a deliberate attempt not to initialize because you think of the initialization would be done automatically right so we will take the safe view that um there is a new value which val is not is there in the underlying virtual machine but it is not part of the language its not something that is realizable in the language its not something that a program can explicitly use its only in order to provide lets say runtime or compile time error checking for uninitialize peripherals ya so and this so the moment you take this attitude then what happens is that all expressions which use uninitialize variables become meaningless so the values associated with all expressions which use uninitialized variables or such that either they become stuck configurations are they lead to runtime errors um right so you will so this is this is what happens typically in pascal lets say an uninitialized variable very often its its even though is easy to do it  refer slide time on 29  40 min  most pascal compilers do not actually check whether you have um whether you variables are all initialized at compile time however at runtime they do point out they there was a variable without a value and what that value um some kind of a null value which is assumed initially to be store right so during allocation the pascal runtime system actually cleans up that store so they it does not contain some previous values but this is not fully um not implemented in properly in many compilers and very often you would be you will find that previous values that that location had would be used sometime without um without any warning or an error message um so the pascal language of course is clear that you have to um i mean um uninitialize variable should be explicitly pointed out and should give a runtime error so now so the first thing is that um before before we get on to the actual semantics the assumption of an unbounded amount of unbounded number of locations means that every time a new variable is um declared you should be um you should be able to get a new location okay if at any stage you have only a finite number of locations used then its always possible to get a new location so will define an implicit function for this purpose which i ll call new which given any finite set of locations gives me a new location that is unused ya so the programmatically if you can assume that the locations are all ordered by some ordering then you can assume the new is the new of l gives you a and a location with a least the least location greater than whatever exigent l for example right so um will we will assume the existence of this function which will use as a site condition this is um this function is absolutely essential because it is also useful when your to do dynamic allocation of memory for example the um when you have pointed declarations then you actually explicitly allocate and deallocate it s also useful for implicit allocations and deallocations so you could correspondingly define a delete but in that s not essentially then you actually explicitly allocate and deallocate it s also useful for implicit allocations and deallocations so you could correspondingly define a delete but in that s not essential that s not essential in a semantic model because um unbounded number of locations ya but delete is essential when you have a finite number of locations so that reuse is possible ya  refer slide time on 32  58 min  so we will look at the transition system now from now an we won t look at the complete transition system we will look at only what modifications have to be made to the transition system so that so as to be brief and clear about exactly what we are doing so the transition systems are that are such that every expression declaration in command so every expression is evaluated in actually in a given environment with the given store so both the environment and store or parameters of our transition tools for um for all possible categories ya  refer slide time on 34  03 min  so will use so a typical expression transition system for example will look like this will combine the two things that we the two concepts previously so previously used so for example we will we will look upon any expression has been evaluated in this environment with this store ya now the expression semantics most of with is can be rewritten in this form without much problem the only thing that requires elaboration is possibly the difference between um so what is the major change the major change is that now you have two kinds of identifiers the either constant identifiers or variable identifiers otherwise all expressions in our in our case the expressions really depend upon um or really the expressions that can occur on the right hand side of assignment statements that s um that s really the only kind of expressions that you have to you need to worry about right so the expressions that occur on the right hand side of assignment statements will all require their r values to be used okay in the expressions undo only thing that can occur on the left hand side of an assignment statement in the case of this language or identifiers variable identifiers that s it so lets just look at the expression semantics of in terms of just these two distinctions so our basic so i can the expression axiom e not is actually going to be split into two right so i um make it e not one and e not two so one thing is that if the identifier so the value of this identifier is just an natural number and provided the environment declares it to be a constant identifier if the environment binding for that identifier is the constant m note that this m actually could be um could be this undefined value two ya it um it one be this undefined value in a constant declaration because that undefined value is not explicitly available to the programmer except by um certain deviates mechanisms like dividing some two previously declared constants by zero assumption so it s um realizable in some in some strange ways but it s not explicitly available to the user right so this m could be um could be an undefined value provided so um so this this tells you that if the environment associates with this identifier note that we defined our environment as a mapping from identifiers to the disjoint union of values and locations so which means that with every identifier is also associated its identity as to whether it denotes a value or a location so if the environment associates a value with it then the meaning of this expression is just the um corresponding value in that environment on the other hand if the environment associates allocation with it then the meaning of this expression is is that firstly there has to be this so firstly the environment has to um give the identifier allocation binding and if it gives it a location binding then it s possible to look at the value inside that location binding inside that location and take that value in either case for an identifier that occurs just as an expression which means that here an assuming that within this language it only occurs an expression on the right hand side of assignment statements then for any identifier you also you have a corresponding value ya but there is a level of indirection if it s a variable which is not there if it s a constant right  refer slide time on 39  05 min  so the next thing so the other once so um the expression language as for as we a concern does not contain l values in this um in this simple language it does not contain any l values contains only r values so therefore we have only these two these two cases to really consider all other expression semantics will be more or less whatever we have done before ya so there is there is nothing much to the expression language however what we have to look at or variable declarations so first firstly that constant declarations is nothing um we are know first to be made because the constant declarations are exactly like the ml variable declaration okay so in an in an environment a constant declaration of the form i equals e means that you evaluate the expression e and then um and then you create a little environment in which i is associated with that value i is bound to that value right in that state and there are no side effects in this language so so there is no there are no other changes to be worried about ya however in the case of a variable declaration firstly what it means is that given given a state given a store sigma based on the set of locations l given an um an environment already which has a collection id of identifiers the new encounter a new identifier or could even be an old identifier which is already present in this then firstly what you do is you acquire a new location and this new is very important in the sense that you are acquiring a location that is so far never been use i mean there is no confusion between this location and any other location that is present in a okay this l has to be different from anything else that is present in this capital l so you first acquire a new location and bind that identifier to that location but that location has to have some well defined value has to have a value which so what happens is that your stores get all to l your store um instead of being a function from capital l to values becomes a function from capital l union this news new small l to values and you have to associate a value with this identifier if and since its an uninitialized variable you associate the undefined value um so as part of a variable elaboration your store actually changes the store does not really get updated in in the sense in with we normally talk about store updation but what does happen is that this store becomes gets extended to a new store where all the values of the old store are preserve plus a new location has been added and therefore a new value to that location ya so the other declaration rules a similar so you can take the sequential composition of declarations and a create new environment stores pairs so this so this declaration is since the environment is got changed and the environment introduces new location  noise  therefore the store also gets changed in a certain fashion ya  refer slide time on 43  19 min  so the so we just so i will assume that you can take the assignment statement semantics as before except that now you have to take the assignment statement semantics as being of the form if you have in a given environment row if in a given store sigma if you have an expression e which let s say eventually goes to a value lets called that m and okay since there are no side effects created by expressions its perfectly okay to consider the same l and id here ya then the effect of an assignment of the form ya i is assigned e in this state sigma l is to create is to create a new store sigma l prime where sigma l prime equals sigma l with the updation that the location of the identifier i has associated with it the value m right so the difference between so here you are actually this is actually a location and not a and nothing and so in the case of an assignment this is clearly an l value so you take the l value of this identifier but through out this expression you only taking r values at and evaluate this expression in this store and this environment to eventually yield the value m an associated that value with this right  refer slide time on 46  40 min  so that s the only change in the assignment statement all other commands remain more or less same except that we should now look at the block commands which have created new declarations ya so that s the only thing that s really new so let s look at the block commands okay order will look at the block commands like that thank you transcription  gopalakrishnan c principles of programming languages dr.s.arun kumar department of computer science iit delhi lecture # 17 blocks welcome to lecture 17 so today will discuss blocks before that will briefly recapitulate further um  refer slide time on 00  35 min  in the last lecture so here is the grammar of the language and the most important new construct is mixing declarations within commands and that s create a block with a local declaration and blocks are like unnamed procedures as for this language is concerned  refer slide time on 00  55 min  the purely local they can not be call from elsewhere however you can nest blocks so we defined locations as an infinite collection um  refer slide time on 01  10 min  we define lock as an infinite collection of locations and store is a function from some finite subset of locations to values stores over  check 01  31  such a thing and the set of all stores as overall finite subset of locations environments are um now locations become values that can be stored and so an environment is a mapping from identifiers to either values or locations  refer slide time on 01  50 min  where the identity as to whether it s a value or location is preserved in the mapping ya okay so i spoke briefly about uninitialize variables and the need to introduce i have new undefined value and which means that we also have to impose strictness conditions  refer slide time on 02  10 min  so expressions evaluated with some sub expression which leads to an undefined value would all be undefined and would that programmatically speaking would just lead to runtime error um and we say that we required new function which given a finite collection of locations produces a new location from this infinite collection loc and there are very simple ways of implementing the availability of a new location  refer slide time on 02  40 min  okay so a basic semantics in transition systems is that any expression declaration or command is evaluated in a given environment with a given store  refer slide time on 03  00 min  so um so the state of a computation in such a language consists of both an environment and a store right so the concept or state which is so far mean sort of abstract is more concrete now ya it consists of both an environment and a store so it still we are still talking about languages which are state based right only the notion of state has to be more refined here the expression semantics as a said the only difference is between um in this language of course the all expressions we look upon all expressions as basically those occurring on the right hand side of assignment statements so here are all the l val there are r value evaluations of expressions so given any identifier if the environment point sit out to be constant value then that s what this evaluation gives as and if it s a variable then the environment should give you the location and the store for that location should give you the value right  refer slide time on 04  15 min  so this is so there is a level of indirection here ya for variables okay so let s quickly look at constant declarations so if you assume that there are no side effects in expression evaluation in our language then you evaluate the expression through the expression language several times till it reaches a value and this the effect of this constant declaration in a given store is to nearly create this new environment new little environment with the same store if um if expressions could have some side-effects ya then what we would then there is a very simple change that you have to make you have it only means that you can not assume that in the process of evaluation the store remains the same as when new started the evaluation so what it means is that with each application of this of this of a single step of the expression evaluation mechanism your initial store sigma l goes to some sigma l prime and it could keep on changing till some final and um so correspondingly what it means is that this declaration also moves to sigma l prime and eventually when e sigma l moves to m sigma l prime  refer slide time on 06  07 min  then this declaration moves to this the creation of this new little environment and a modified store sigma l prime so under the assumption that there are no side effects in expression evaluation the stores are maintain otherwise in general they don t have to maintain right a variable declaration on the other hand is any way guarantee to change the store because it really changes the very function by adding an extra location it really changes the very function that from sigma l to a sigma l union the new location and the effect of a variable declaration is to obtain a new location which um as not is not present in l and bind the identifier to that location with a new initial value for this location um which in in the case of this particular language it is the undefined value ya  refer slide time on 06  50 min  so the other declaration rules a similar ya the effect of this expression language is most importantly failed on in the assignment and the effect of this um so given an environment row and an um assignment statement of the form i is assign e again we are assuming that there are no side effects so which means the evaluation of this expression in zero or more steps zero is important because this expression e could just be a constant value right in zero or more steps could lead to some value m in the store sigma l and the effect of this assignment is to just change the store to accommodate this new value m in l  refer slide time on 08  06 min  so whatever may have been in the previous value contained in l is erased and a new value is store right so where of course the environment gives you the location which has to be updated right and as in um and if there are side effects in the expression evaluation in general all that it means is that the store could also change in the process of expression evaluation  refer slide time on 08  27 min  so if um so if you have a change store from sigma l to sigma l prime in the process of expression evaluation then the new store is whatever is there in sigma l prime updated with this new value right so lets the other the other expression rules and declaration rules and command rules are simple modifications to take care of environment and store from whatever we are previously done so now lets look at blocks right so a block um so if you so we can look upon this block semantics in this fashion a block is a command in our language and the first step is to elaborate the declaration in the block so if this declaration d and for the moment i am assuming the this declaration d is non empty okay so if this declaration d goes to some d prime and i am taking a more um i am considering a more general setting your initial state your initial store sigma l could be changed in several ways in the process of elaborating a declaration the store anyway gets change because new locations might be added further if the declaration contain some expressions which could create some side effects you could have changes and store because of the active okay so both so the store changes for two um the store could change for two reasons one is that new locations are added so the l changes to l prime and therefore the store changes but in addition if there are side effects and expressions in the expression evaluation mechanism then that those changes could also be incorporated in this store right and um the result of this block declaration is to just move from here to here and this role might um might be applied several times and the effect of this entire block is again as usual um in our in our language first specifying the semantics will introduce pseudo commands of this form which are not there in the original language just like we did in the case of declarations we introduced environments also as part of that declaration syntax in order to facilitate easy expression and easy rule for evaluating declarations similarly for commands which have local declarations we assume that an environment is being created and a new meta syntactic category of this kind also exist in the command in the command language for for the purpose is only a specification so by the command rules the effect of this is that this command c is executed in this updated environment in this temporarily updated environment so if the effect of a declaration after several moves in the declaration in semantics finally gives you an environment a little environment row prime then c is executed in this modified environment to reach some c prime with some changes possibly in the store except that your original the in c itself unless c um so in c itself the locations that are preserved it s the same location set but the store might be change because of updations or values so the effect of this so the effect of this is to look at this execution in this updated environment and model it as if as the eventual effect of the block right now this c could be this rule could be applied several times so and eventually um so eventually you will find that um what must happen is that when applying this rule the important question is that you have starting of execution in a um in a um in a store sigma l but when you are executing the c in this updated environment your location set could be modified and therefore your store could be different so sigma l prime is not the same as sigma l except that for all locations that are common to both l and l prime they yield the same values right so since we are interested only in the execution of this block with the starting store of this form and we are temporarily updating the environment and the updation of the um the temporary updation of the environment also means a temporary change in store temporary or a permanent change in store but this change in store is just that whatever locations are not present in the original store might have some values but all the locations that are common to l and l prime should have the same values okay and so the effect of and the similarly in the final store since the um the creation of the locations l prime minus l is temporary because of this the creation of this little environment which anyway is reversible therefore this store also reverse to this where then original set of locations of the store so the irreversible changes and store and anyway subject to a certain scope rule so they have a lifetime so the all the locations in l prime minus l l prime is a largest set than l or really temporary because this environment row prime is temporary right so the main constraint in all these four stores is this or these two for all locations that are common to l and l prime  refer slide time on 15  15 min  sigma l and sigma prime l should have the same value um sigma l and sigma l prime should have the same value and similarly sigma prime l and sigma prime l prime should have the same value right okay and lastly once you are again with the same constraints on sigma l once your command in this updated environment finally produces a store of the form sigma prime l prime then the effect of this entire command on starting from a given store sigma l is to produce a new prime sigma l is to produce a new store sigma prime l with the same set of locations as it started out with and the sigma prime l is really the same as sigma prime l prime except that all the locations in l prime minus l have been removed have been delete right  refer slide time on 16  26 min  the fact that it s a i mean so it s a um its fairly precise and its very concise so its also dense with information so there is a lot of information in this rule right in the block rules okay so what these rules are shown is a there is some relationship for examples between environments and stores which has to be maintained at runtime the stores updation of stores can occur because of two reasons the creation of new environments or just updation of the current store right so its um its good to specify varies kinds of constraints on what happens if some of these constraints or not met right so a standard problem so one thing is that if i word to just take an arbitrary environment row on a set of identifiers id and in an arbitrary stores sigma on a set of locations l then the question is what kinds of relations do they satisfy right so one thing is that every variable that is in id should have a location corresponding to um and every location um so only if every variable has a location corresponding to it so that s one thing that you should have and secondly every location in sigma l every location in l is meaningful only provided there is some variable associated with it there is some variable bounded if there is no some variable bound to it then there is absolutely no way of a program the program can access that location right  refer slide time on 18  41 min  so the main questions are does every variable have a location bound to it and secondly is every location bound to a variable and what happens with lot of normal programming especially where storage is dynamically allocated that means at runtime by a user is that one frequent problem is that you have you get dangling references right so this typically so what what is a dangling reference so given a state which consists of an environment row id and as and as store sigma l there is if there is some location l which is bound to an identifier in the environment but that location is not present in the store then what it means is that there is an identifier which really is um is not part of the program state okay so this is known as a so such a location l is called a dangling reference okay this as well as i can see is the most precise and concise way of explaining a tangling reference and this usually happens when you allocate storage to some variable and then you disposed off while it still in use and it can be complicated  refer slide time on 20  27 min  this macros can be significantly complicated when you have aliases for the same location that means when you an aliases when um a single location is bound to two different identifiers then you would say that that location is aliased so it s quite for so the effect of aliasing is that you might you might um if there are two identifiers p and q which refer to the same location then you might um dispose of p even though q is still in use in the program and this is something that frequently happens with pointer references most languages ya so this is the storage in security so um a simple example of this form you actually there us there are two point of variables lets say p and q you have allocated a new location to p you assign that p to q and then you disposed of p okay  refer slide time on 21  27 min  this kind of this kind of elementary this kind of elementary programming is often got by the runtime system but there is no reason to suppose that this is the only possibility you could have very complicated possibilities very complicated assignments and you might be doing disposes and creating tangling references there by right so the second thing is when you have supposing you have a state a location which carries a value but that value is inaccessible right so typically what you are saying is that  noise  this location l this um this set of locations l has some location which is not bound to a name right so if it s not bound to a name then that location is inaccessible this again happens with dynamic allocation of storage right so you have an identifier ya you don t have any identifier by which you can access that location then you would say that its inaccessible reference right the notion of dangling references and inaccessible references and such storage and securities actually also translates to when you have a more complicated language and your l values or themselves expressions could be compound expressions that s when it really make sense one um one without programming language with um with the way we have defined it one important property you should be able to prove is there there are no dangling references and there are no inaccessible locations right  refer slide time on 23  45 min  okay so one typical instance again with dynamic allocation is that you might actually you might actually being using the same name to create new locations and this this creates a new location which is not assign to anything and this new allocation overrides this previous allocation and therefore the previous allocation becomes inaccessible right and um its often quite difficult to debug a program with pointers mainly because the allocation is being done by user and the user has to be very very careful about making sure there are no inaccessible references or no dangling references the owners is on the user to ensure that it doesn t create the fact okay so we have actually seen two kinds of so we have actually seen blocks in a command language and the pl zero compiler calls such things blocks except that in the pl zero compiler every um every block also has a name the way we have treated blocks here as unnamed procedures which can not be call from elsewhere is a simplifying assumption but is very similar to the let construct in ml right you have a let some x equals something um you have some let some declaration in some expression right so it s very similar to this let construct so we can think of this let construct also as a block right so its um it s a functional language so there are no commands there are only expressions but these expressions are qualified by declarations or by definitions in a functional language you call them definitions rather than declarations but we will use the two terms interchangeably right so our notion has a analog in a language like ml so what it means is that you can qualify expressions with declarations you can qualify commands also with declarations okay then the next question is can you qualify declarations with declarations right so lets so the only the only um operator on declarations that we have used if you look at declarations and their pure form the only the operator we have used is what might be call this sequential composition of declarations right but there is absolutely no reason why what can be done for expressions and for commands cant also be done for declarations and in fact actual in fact ml actual has this specification so let s look at some more aspects of declarations and the creation of blocks right so i am using a loose syntax for brevity i don t want it to be as complicated as an ml declaration so we might consider the following kinds of declarations so most of these imperative languages actually allow only one kind of operator an declarations and that is a sequential composition of declaration right but ml has also this what is known as simultaneous declarations and it has a construct which i i mean in order to be consistent with this syntax i use the word within but typical ml syntax would be of this form so what i write as d1 within d2 would actually be written in ml as local d1 in d2 end is different from the let construct remember that in the let construct has has an expression here  refer slide time on 28  05 min  so it s um it s a grammatically different category it s also semantically a different category right because let construct like this actually denotes a value right whereas a local construct like this in ml actually denotes an environment so the two things are different and so they you um you reflect the difference by having different syntax two right so um the reserved word local in ml is use to distinguish what constitutes an expression block to distinguish an expression block from what might be called a definition block right  refer slide time on 29  09 min  so this within construct that so we will just call it within so um it carries this analog of blocks from declarations with so you can qualify commands of declarations you can qualify expressions with declarations and create expression blocks or command blocks and you can qualify declarations with declarations and create a declaration block and this absolutely no reason why it why this is not for example used in um for example why we can not introduce these new declarations into our original language and give a semantics because these things these constructs are completely orthogonal to whatever is there in the command language so you could you could take that command language and add these declarations also this kinds of declarations ya so a typical ml program would consists of such all possible such declarations so we will just call them this is of course sequential this is simultaneous and this is nesting of declarations so just like so this is this is more like nesting of declaration within expression and declaration within a command to create a block  refer slide time on 30  17 min  similarly you can um nest declaration within a declaration to create a new declaration so in the important thing is so we look upon this in the context of a functional language because then we don t have to worry about store okay its just an extra complication to understand what these things mean doesn t really require to know whether your dealing with a functional language or a um or an imperative language you can deal with them independently right  check 30  43  ya that s a good question so we just we just come to that so lets look at these three operators in some detail right so if you look at these three operators and detail these all these three or what might be call composition operations you have the only composition operation that you learnt are functional composition in mathematics and um they by also in programming so just so these are so sequential composition of declarations is also composition operation and these the other two can also have the same status as composition operations so let s look at it in the analog of what you do with respect to functions right so if you have a simple mathematical function and you actually compose two functions f g of x right so this is a simple composition of functions in mathematics where will assume that the types of f and g are such that you can actually perform this compositions right so for example g if g is from sum set a to b and f is from b to c then this f of g of x this composition of f and g would give you an element of c right so this is all simple mathematical composition works right and you can diagrammatically look upon this as something in which the value x comes into some block box g gives you may be a value y which is fed into the box f till you get an output um right  refer slide time on 32  56 min  this is all simple composition else you can also look upon this as um as a series connection out these boxes right you learnt about the series connection of resistors so you can also connect these boxes and parallel um just like you do it with resistors so what we could do is so what we could do is we could actually consider these series and parallel compositions of declarations viewed as boxes so the effect of so given an input environment row the effect of d1 semicolon d2 is um is exactly like the effect of just like the semicolon operation on commands gives you a sequential evaluation of the two commands right so c1 semicolon c2 is like putting into the input state into c1 getting a state out sigma um lets say an intermediate state out of c1 and putting that into c2 and getting a final state r right so just a sequential composition so its really a composition operation that s why i don t that s why i clearly mention once that will use semicolon as a composition operation rather than as just a terminated its not it s not a mere syntactic sentinel to mark the end of a command okay though um though more and more programming languages are actually taking the view that sentinels are more important than having having it as an operation for since most people understand just a position itself as an operation you can you can eliminate this use it um state as a termination as a terminator okay but since we are dealing with composition operations so i would like to viewer at explicitly as a composition operation so the effect of an environment row on the box d1 is to produce temporarily at least an updated environment so this box d1 actually produces the row produces a little environment row one and all these suppose to be in the context of some larger command or an expression so on which temporarily is going to be evaluated in the row in the environment row updated in row one and of course the effect of this is to create a new updation where of course um this upda ion operation is left associate that means that means you can assume the this bracket is here ya so take the updated row row one and updated further with row two right so if you word to connect things and parallel then what it means is that the same row is supply to actually both d1 and d2 in a fan-out fashion and these d1 and d2 independently produce new environments row um row one and row two and whatever is the rest of the command or the expression that is to be evaluated is actually evaluated in this composite environment however how you composes how do you composes environment i will just take the union of the two environments resume them give a some common variable ya so um so the main restriction that we require is that we should not have the same variable declared the same name declared in both d1 and d2 if you have the same name declared in both d1 and d2 with different expressions giving them values then of course there is a question of what does that identifier denote in this composed environment right so that is a syntactic restriction that you have to impose right so um so lets just yes right  check 37  30  why not no we could taken a disjoint union but if you word to take a disjoint union then what it means is that you have to introduce extra syntax to distinguish between the two identifiers with the same name is not a big problem is not a very big problem because this is something that is used the record select a kind of syntax is often use to distinguish um different identifiers bound in different declarations ya so very often in languages like aida you can overwrite the scope rules by using a record selection method to make available an identifier that is been redeclared in the current scope but you do not want it hidden from a previous scope okay so a simple record selection method can be use to get rid of that but lets lets not complicate matters so lets just lets just impose this restriction which actually ml imposes so unlike aida for example ml has this restriction that this d1 and d2 should actually be disjoint should be disjoint in the sense that the identifiers they declare should not have the same name  refer slide time on 39  10 min  they should be no common names in the two declarations given right so i had actually n sample for the actually right so um so what so the objection rise by him is is something like this right so either you take disjoint unions and allow for new operation called field selection which explicitly tells you whether this y we to disambiguate this y but then what in order to be able to do a field selection and explicitly disambiguate this y what it means is that then this declaration has to be named okay and this declaration also has to be named unless they are named there is no way user can actually disambiguate this but once you have name them then what it means is that you have to have this declaration separately namely um lets a functions okay so and these are complications for example the aida manual as gone into great detail to clarify right so you can actually scope rules can be overridden the hiding the whole in the scope concept which we spoke about can be overridden by using a record selection kind of operation which tells you that which um which basically says that if an identifier if you have two blocks if you have an identifier x declare in both blocks and you want to use this x then this block should have a name let me call that p1 let me call this p2 and you want to use the x that is in p1 then the aida syntax actually allows you to do a require selection of this form so the whole in the scope created by this new declaration can be overridden by record selection methods okay but what it assumes is that you actually have a name for this block you have to have a name for this block and which means that you are creating a new environment with this name this name is associated with this entire block and only then so it s um it s an extra complication very many people believe that scope hiding can often be use deliberately to ensure that you don t make any confusing assignments  refer slide time on 42  09 min  whereas using this record um its quite possible that one might just use x intending x without actually performing the record selection and one might create an error undo it will not be detecting so lots of people believe that as a part of security of programming the whole in the scope concept is an important concept and should be adhere to and should not be overridden by such mechanisms right  refer slide time on 42  30 min  okay so this example clearly illustrates in what what can happen if you have this um you can try it out on the ml system you should get an unresolved error you should get some you should get some error syntactic error it can it s a matter that can be syntactically resolved okay the last composition is some out um tricky end this and this does not fully explain what exactly happens right so you have a series connection of declarations you have parallel connections of declarations with the added restriction that they the identifiers they declare or disjoint and then you have this nested declarations so d1 is nested within a larger block d2 okay so the effect of this is that this input environment row is really required for d2 however d2 is so complex that i can not adequately express the declarations in d2 without introducing fresh new names so what i do is i introduce a declaration d1 but which is purely local to d2 and its not visible outside so the effect of um so this purely local d1 actually creates a little new temporary environment row one and in this updated environment row row one you elaborate the declarations in d2 and the effect of this is that that since this is totally temporary and is bound by a scope which lost only up to the end of d2 what you get is not something of the form row row one updated with row two but what you get is distance row updated with row two that row one was purely intermediate in order to facilitate as structuring of the declarations in d2 right  refer slide time on 44  37 min  so this is the difference between a pure sequential composition and a nesting of declarations so lets lets just look at a simple example to to see what um what this means right so these so here i have got three examples one for each kind so here lets quickly look at this let x equals three in let x equals five of course um of course my ml notion is faulty but that s deliberately so i don t want to be worried about syntax to much and usually there would be val or fun reserve words but i don t want to use all that so let x = 3 in let x = 5 ; y = 6 * x in x + y so this is an expression this is an expression block in which you want the value of x + y where this x according to the normal scope rules refers to a binding created by the  check 45  40  enclosing scope which has a declaration for that x and the same is true for this one these two declarations are sequentially composed so what it means is that you start with the x = 3 initially you have redeclared x to 5 so in the updated environment you have x = 5 and in that updated in environment y is evaluated so the x that y uses in it s so the for the binding of y the x that is used is really the x in the updated environment where x = 5 and in the same after having created this environment row row one updated with row two you evaluate this expression so this x will just refer to this x and this y would refer to this y and you would get an answer of 35 on the other hand if you had a parallel or a simultaneous declaration here then both these declarations are elaborated in the original environment that s great so both these declarations are elaborated in an environment in which x = 3 the result of this declaration is to create an updation x = 5 and the result of this is to create a binding y = 6 multiplied by the value of this x which is eighteen and this x + y is evaluated in this in this scope also therefore it takes the value of this x and the value of this y to give you an answer right supposing you had a within construct which is really like a local construct then what happens is that you have x = 5 within y = 6 directs so this declaration of x = 5 is purely local to this to the elaboration of this declaration after that this declaration loses its mean so the x that is refer to here is a original x that was available here and the y is whatever this wants  refer slide time on 48  00 min  so you get on answer um right so we look at the semantics of these declarations next time programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 31 typed lambda calculus  55  00  welcome to lecture thirty one so today we will begin the typed lambda calculus after breifly recaptulating what we did last time so last time we looked at the question of normal forms normal forms are sort of fundamental to assign meanings and right from the craddle upwards they always thought of the meaning of an expression as a value it denotes which is the normal form so so for any basis of reduction r if there are no redexes then if there are no r redexes then you would say that its an normal form  refer slide time 1  03  so we have so we have correspondingly various notions of for each notion of for each basis of reduction we have a corresponding normal form and in the lambda calculus we asked various questions the main the main reduction being the beta beta reduction what we should ask is are beta normal forms some how guaranteed right  refer slide time 1  34  so does every term have a beta normal form no because there are terms like omega which just go on for ever if a term has a beta normal form does every computation yield it in case if you have non determinism in the semantics of the language then its its clear that which such examples its not i mean not every computation will yield that however the next question is of course then if a term does have a normal form does every non does every terminating computation yield that yield the same beta normal form and the condition for that is that if the if the if the basis of reduction is church rosser then it would its guaranteed to yield the same beta normal form same normal form for all possible terminating computations and if it is not church rosser then there are no no guarantees right  refer slide time 2  40  so lets just briefly look at the church rosser property so we first looked at this diamond property abandoned relation r satisfies the diamond property if for all l m n this diamond can be completed that is if l is related to m and l is related to n then there exists a p such that m is related to p and n is related to p so given this this given these three are forming a triangle if you can complete the diamond for any arbitary relation which does not necessarily have to be a basis of reduction just any arbitary relation so any arbitary binary relation then you would say that it this satisfies the diamond property right  refer slide time 3  35  now in particular if this binary relation is a basis of reduction then we would say that you take this basis of reduction you close it compatiblely over all possible term formations take the reflexive transitive closure of that which finally yields this many step reduction relation and then if this many step reduction relation satisfies the diamond property this many step reduction relation is also a binary relation and if this satisfies the diamond property then you would say that the original basis that you started out with is church rosser right  refer slide time 4  30  so one thing is of course that if in fact a one step reduction given so this fact just says that if you take any bin  ary  any binary relation which satisfies the diamond property then its reflexive transitive closure will also satisfy the diamond property and the proof is very simple in particular what we are saying is that if if this fact is true for all possible binary relations and in particular if this was a one step reduction that means at compatible closure of some basis of reduction if it s a if a one step reduction satisfies the diamond property then the many step reduction also satisfies the diamond property and therefore the one step reduction comes is a is is derived as compatible closure of some basis and that basis is church rosser right so in general so this the proof of this is of course very simple that if you assume that assume that you have a relation r which satisfies the diamond property and now you have to show that so what it means is that if l is related to m and l is related to n then there exists a piece such that m is related to p and n is related to p and in order to show that the reflexive transitive closure satisfies the diamond property what we do is ass  ume  prove it by induction on n and m assuming that l is related to m with through r steps of composition through m steps of composition and assume l is related to n through n steps of computation composition when we perform an induction on n and within it we perform an induction on m an we complete this slice and if you can complete this slice then by the inductive property you can complete this diamond and to complete this slice you use the induction on m to complete each of this very little diamonds right and that would finally yield if if you can complete this slice then you can use lets say n two to complete this slice and then you can use n three to complete the next slice and so on and so forth till you reach n n and you will when you complete the last slice you will get this common term p n m which actually is again r star related to m and r star related to n and therefore it completes the diamond right so our problems would all be greatly simplified with respect to beta reduction if the one step beta reduction satisfies the diamond property so if the one step beta reduction satisfied the diamond property then we would have proved that the basis beta is church rosser however this example  refer slide time 8  01  shows that the one step beta reduction does not satisfy the diamond property right so so and so what it means is that it becomes some sort of technical matter now because of such pathological examples that exist such pathological examples which prevent the one step beta reduction from from giving you a perfect diamond okay so the normal proof of the church rosser property for beta reduction would be that you allow for a parallel evaluation of many steps of the original beta reduction to be regarded as a single step and then complete the diamond so we will we wont worry about that at the moment but its enough to know that beta reduction is church rosser also keeping in mind the fact that beta reduction if so therefore for any term all possible reductions which do terminate should yield the same normal form right so by if since beta is church rosser what it means is that you can not have two distinct normal forms so for any term if through a many step beta reduction you have obtained a normal form m and through some other fashion you have obtained another normal form n right then by and if beta reduction is church rosser then what it means is that its possible to complete this diamond which which contradicts the whole issue of normal forms so if there are indeed two distinct normal forms then by the church rosser property there would be a there would be a common p such that these two are related by many step beta reductions but if these two are normal forms then there cant be any more beta redexes which means m should be the same as p m should be alpha equivalent to p and n should be alpha equivalent to p which means m and n should be alpha equivalent to each other right so if our basis of reduction has satisfies the church rosser property then it have at most one normal form there can not be more than one normal form right  refer slide time 10  24  so so that s the important thing how ever this does not guarantee that you can always find normal forms the previous examples  refer slide time 11  01  i mean the previous example we said was that if that all all computations need not yield the normal form because some computations can be infinite right so some terms may have no normal form there are other terms which may have a normal form only for certain computations but for all other computations which so far but for all computations which terminate they will terminate in the same normal form and for the computations that do not terminate of course there is no question of a normal form right so so what happens now is that this omega and the delta are the prickly issues that s so what should we look at is and the nice thing about for example pioneer arithmetic and all are more fundamental notions that come from school and that come historically also came before all this is that they all had normal forms and they all had terminating computations there were no non terminating computations in basic pioneer arithmetic and so on and so forth so the important question then is what is wrong with the lambda calculus right so there are of course seceral things wrong with it and if you if you look at it sort of more carefully and look back on what ever we have done the some of the things that are really with it are as follows one is that if you actually apply the lambda calculus on to some other formulas after all the whole idea of the lambda calculus is that you should have an independent formulation of the notion of functions which is applicable to any other domain where you want to build higher order functions from the existing functions right so the first thing that happens of course is both that that is in the pure lambda calculus there is alreay a type confusion problem i mean in the sense that at any time if i take this lambda term does it represent the combinator k which is supposed to give me the first of two arguments so it acts like a projection function or does it represent the predicate true i mean so if i take the pure lambda calculus and i construct all my data structures my booleans my integers and so on and so forth then i can i can do arbitary kind of applications there is nothing in the calculus itself which prevents me for example from from asking whether a natural number is true so i can actually true to a natural number and the most unfortunate thing is that it will actually give me some answer which which i may not be able to interpret either in the booleans or in the natural numbers yeah  refer slide time 14  42  so and secondly of course there are there is this problem of the various constructors and deconstructors that we have are not inverses of each other i mean while you are guaranteed for example for the pairing function that if you actually explit explicitly constructed a pair from two terms m and n and then you deconstruct them you will get back the two terms m and n in the separate deconstructors but nothing guarantees that if you take an arbitary term p and first deconstruct it and then form a pair you wont get back the original term yeah so what normally we like to we like to think of constructors and deconstructors as being some are inverses of each other and if there were no type confusion then there would be no problem with this see for example if there was if there was some typing then this peak could not this this deconstruction operation could not have been applied on any arbitary term p it could have only been applied on a term for which these deconstruction operations are actually defined okay and if if you can some how put in those restrictions then what it means is that these constructors and deconstructors should also be inverses of each other so what we require are really typed guarantees in the lambda calculus and especially in the applied lambda calculus  student  sir whats meant by k here  k is this combinator right so so infact so whats and what really i mean and when you actually apply the lambda calculus on to some other domain then you know the whole thing becomes completely birsel you don t know now what what is actually happening when some faulty i mean constructions and deconstructions are are applied on those terms and more over those the applied lambda calculus itself might be applied on a domain which actually is a combination of two diamonds when the booleans and integers integers and reals and booleans and so on and so forth and then type distinctions become important and the pure lambda calculus when applied directly on those on those calculi is not going to give you the benefit of those type distinctions right so so that s that s was one thing that was important so can we some how preserve types when we apply the lambda calculus on to some existing domain that s one important question right then the second thing is with this the beta reduction essentially and the non terminating computations in the beta reduction  refer slide time 18  44  so essentially all our examples had to do with these two okay but these two are just the simplest possible examples i mean after all i could construct other equally bad examples like like i don t know may be i could i could construct some a new combinator omega of this form may be lambda x x applied to x applied to x and then i could consider omega applied to omega i could take something that gives me four or five applications i could give i could even take more perverse things like x y y y applied to x x and i could consider a whole lot of perverse things which when you finally ball down to it when you finally ball it down to its basics you go back to the fundamental question what was the lambda calculus originally intended to do the lambda calculus was originally intended to some how account for functions the basic notion of function and function application were really the main parts of its agenda then in the light of that basic question do these combinators make any sense right just because my context free grammar for the language allows the generation of such terms i still have to ask this basic question do these things make any sense to either man or beast yeah so and then what happens is that you really have to go back to your basic mathematics and when you look at this these combinators and lets say try to interpret them as their effect after all the whole idea was that we define function application and function abstraction in such a way that its some how its some how supposed to intitutively capture the notion of functions unnamed functions the construction of higher order functions and generally unary and certain isomorphism properties assure us that its enough to consider only unary functions because any unary function can be carried into a sequence of unary functions where the arguments are taken one at a time so all that is fine all that intrusion is fine but whats wrong is do these things really do would they be meaningful in an actual application and then we come to a basic property mathematics is always followed right  refer slide time 21  42  so so the first thing is that these first thing is that its not at all clear that they are really useful combinators the second thing is that from the examples that we have seen these things actually prevent meanings from being computation independent all right and in particular what i am talking about are these kinds of applications there what is the speciality of these kinds of applications instead of being applications they are more like replications right i mean if you look at all these perverse combinators they are more like symbolic replications of some free terms we just i mean its like a population explosion problem i mean its its hardly functional application because no mathematician would ever make this kind of an application right and so its there is a replication problem where its not even clear that that replication is meaningful there is there are replications which are meaningful like the y combinator or the turing fixed point combinator but here its not clear that these replications are really meaningful yeah so so the so the next question is supposing we can some how get rid of these kinds of arbitary replications then if you if you get rid of these kinds of arbitary replications then you you will be guaranteed beta normal forms then what would it mean is all computations would terminate and given that the the beta reduction basis is church rosser you would always get beta normal forms right  refer slide time 23  11  so the important thing is that meaning should some how be independent of computation and this kind of replication operator actually prevents the meanings from computation dependent yeah and whats more of course they complicate matters i mean they yield infinite computations even when i mean i don t mind if you yield infinite computations always you know if a certain term regardless of how you apply it i mean regardless of how it is beta reduced it always yields an infinite computation perfectly fine then it s intrinsic meaning is that of something that is undefined i can interpret it that way if a term always if sometimes it i mean when it s a bright and sunny day it gives me a beta normal form and when it s a stormy weather it gives me a infinite computation then there is something wrong with what i can ascribe to it as a meaning the meaning becomes dependent on the computation rather than on the function itself as an object right so the meaning should not be either computation or weather dependent yeah and so when atleast when beta normal forms do exist i should i should be able to guarantee some how that the infinite computations i mean that there are no infinite computations and i i might be willing to relax this provided these things actually had some meaning but its not clear to me that in any mathematical domain these these combinators actually have some meaning right so we get back to the drawing board and we look at some simple typing schemes right so what so what we do is we go back and look at our school mathematics textbooks yeah so we get back to the basic mathematical notions of functions and function application and looking at these functions and function applications in in my nineth class mathematics textbook i get this  refer slide time 25  46  if a function is defined when this is this is what my mathematics textbook says and this is what i have abstracted out of it and that is if a function is defined from a set a to say b and x belongs to a then f of x belongs to b and this right from nineth class upwards in all mathematics textbooks i have abstracted this is the common behaviour of all the functions remember that the lambda calculus is supposed to abstract common behaviour of all functions across disciplines across i mean in any in any discipline that uses mathematics so going through my physics chemistry and even my biology texts i find that functions when used as mathematical functions they satisfy this property right so which means now what is this of course but i still have to think of functions as i mean functions are as good values as other in that way i differ from those textbooks right i mean they they regard functions functions as being some separate kind of object and these values is being some separate kind of objects some how distinguished between the two and say that f applied to x is a value its not a function i mean no no no don t ever get confused but the point is that i don t want to make those distinctions as far as possible right so now what do i do so then i look at this and i decide that this f is really as much an element of this set of all possible functions from a to b as this x is an element of a i mean when there is really no distinction between the two so what i and and the same is true about this so if a uniform functions and values what i get is an inference rule like this i create all functions as values so then i treat f as just being member of this domain and i treat x as being a member of this domain and i treat function application therefore as being a member of this domain right  refer slide time 28  06  okay so that s as far as i mean and of course i have cleaned up function application and so on i mean this instead of this standard mathematical text i now use this this is my function application right so that s the first thing the second thing is that the lambda abstraction somehow must be a function that is ready to be applied yeah so while still trying to reconcile functions as values i still have to keep in mind that eventually my lambda abstraction is a method for defining unnamed functions which on application yields some values right so the lambda absraction must still be a function that is ready to be applied right so then my mathematics text actually this is this i went beyond college okay this is not something that i find in any school or college mathematics textbook but if you go into this higher maths textbooks very often when they draw these mappings from one topological space to another they often write a function in this fashion its actually very close to the lambda lambda abstraction notation and they have done it without knowing it yeah and what it says is lets i mean you consider pull backs from open sets to open sets or some such thing and what you you loosely say that it takes x to some two x or some such thing yeah i mean so the fact that this function takes x to two x this is the function that takes x to two x for any x which is a real and what you infer from that mathematical text is that therefore this function must be a function from reals to reals right and here actually the mathematician has come closest to define a lambda abstraction yeah without confusing it with application right upto msc level they have always confused lambda abstraction the abstract notion of a function from its application just because very often they had to in in order to define functions they had to define the effect of its application on something and and so they very often have confused functions with their applications so we will clear that so what we have is this  refer slide time 31  08  so if i have some x belonging to a and the effect of f applied to x which i have inverted these two by the way is this corresponds to this yeah the effect of f applied to x gives me a value in b then this abstraction this this thing abstracted out is really like a lambda expression lambda abstraction of this form then this whole thing must represent a function from a to b okay and that s the simplest possible typing you can think of in the lambda calculus so these so essentially what we do at now is we formalize this again into a language so these notions are what we are going to formalize into the simply typed lambda calculus right so here is the language of types and so we will assume initially that okay we will assume that there is a finite collection of base types and if you want to look at its parallel with m l actually its no longer just a finite collection of base types i mean there are there are constructors and deconstructors in m l for example which allow you to produce int list int list int list list list and so on and so forth bool list bool list list int star bool int star bool star int star bool and so on and so forth so the base types are not really a finite collection because they they are types they are new collections of data right so and the since you have constructor operations for the data for forming tuples for forming records for forming lists and so on from simpler types what you actually have in terms of just all the base types in terms of looped when again when you distinguish data from functions what you have is actually an infinite collection of different base types i mean you start with a finite collection of really primitive base types and you apply this data construction operations to construct complex data from simpler data and you actually have an infinite collection of base data types okay if you like but that i mean data types is another technical word which you shouldn t confuse with this and then and then what you do is you construct functions on this data types yeah okay so we will just assume a finite collection of base data types for the purposes of examples just int and bool and then what we have is this simple notion of function types right so if if b is a base type lets say integer or bool then well it s a type its it s a form of function type and given two types tou one and tou two tou one arrow tou two is also a type yeah so this is the simplest language of types function types that i can think of right so what happens is that if you actually have a nice constructors and deconstructors for the data types you can relax this condition of finite collection to infinite collections which are generated from finite collection i mean you can give a grammar for this if you like but lets just look at the function types for the moment assuming just these two base types  refer slide time 34  48  so so essentially we have to some how what we are going to do is we are going to some how incorporate this language of types into the pure lambda calculus and i am going to i am not i don t want to i don t want to i would like to study the theory of the pure lambda calculus assuming some base types only then i will know how to apply it to an applied lambda calculus right so don t take this int and bool to be very serious i mean we are not having boolean boolean algebra and pioneer arithmetic all that will just complicate what we want to look upon we still want to look at the lambda calculus assuming just some finite collection of base types lets say we want to be an what is the effect of typing on the lambda calculus right so yeah this is the language it s a context free grammar and essentially all i am saying is every base type is a type if tou one and tou two are types then tou one arrow tou two is another type  student  whats tou one  it s a non terminating symbol here this is the context free grammar right it s a generation rule  student  if tou goes to b and or tou goes to tou applied  i mean the whole point is that you can this grammar allows you to construct types like this right yeah okay so then here if you look at it from the point of view of this grammar then this right hand side tou is bool and this left hand tou is this whole thing which again has some right hand side of type int arrow bool and left hand side of type int arrow bool and so on and so forth it s a generation process right you can generate so many very many different function types okay the something that you can prove without too much problem is really that every type is of this form i mean tou one and tou two and so on might be there but eventually there has to be a base type okay there has to be a base type some where inside here so its usually it has to be something of this form right this thing confirms to this form  refer slide time 37  58  so these things could be various kinds of these things could be various bracketed types with various arrows and nested arrows and brackets and so on and so forth so all these tou one tou two tou one could be any kind of types generated from this thing but eventually they all yield a base type because you are applying the lambda calculus on to an existing something yeah your typing discipline is some how determined by the base types that are of interest to you in your application all right so i wont prove this its something that you can prove by induction on the structure of on the generation process on the context free grammar right and then what i will do is i will define a static semantics so what i am going to do is i am going to say now that the lambda calculus is originally meant to capture the notion of the functions lambda application is supposed to capture the notion of application of functions of appropriate type to appropriate arguments to yield values or functions of appropriate types so i am not going to permit any application that violates thes type constructs right so that is how i would be capturing i would be able to apply this lambda calculus to some existing application of in mathematics yeah so we have our type inferencing rules of this form right so what we will assume of course we can have very complicated expressions so we have to see what happens when we go deep inside some lambda expression in order to find the type right so in general i will assume that there is something called a context available given a whole lambda expression or some application in isolation the context is going to be some empty set is going to be the empty set assume that you go deep into an expression you will be you will be having more and more contextual you you will be building up your context some how and you will be building up this context very much like the way we buit up environments an isolated program executes in an empty environment but as you go through the nestings within the program you have a you have environment you have the environment building up with more and more declarations okay in a very similar manner we are going to call a context a collection of variable to type bindings so the types are like the values or variables except that they are not really values they are now types okay they are not just they are not values of the variables so this is what i would call a static environment as that of the environment that we are already done which is a dynamic environment i mean which actually represented activation records at run time right this is a static environment because all this can be done at compile time at translation time without executing so i mean if its going to be done at translation time remember that there are no values available to the variables yet values become available to variables only at run time so this is a static environment in the sense that so this static environment is typically is this symbol table that is constructed during the process of compilation yeah so if you go through your page zero compiler or some such compiler there is a building up of a symbol table and that constitutes the staic environment so the variables have types but they don t have values right so we will create this variable to type bindings and what we are going to do is since we are doing everything at translation time i mean this is all translation time that means just from the syntax of the of the of the term we should be able to infer types because can not do application an an actual application is like an execution right so the whole point is that we should not permit applications unless you have the right types for those unless you have a function of a type and an argument of type which is which corresponds to the type that the function takes as an argument that checking is on the thing that is done statically at compile time  refer slide time 43  34  so so you have so our context will typically consist of of a set a collection of lots of names basically names to type bindings yeah and we will call this the static environment is too general a term in in the actual process of translation it might actually be bound to values or in the case of identifiers which are constants okay so static environment is a much more all encompassing term what we are interested at this point is a type environment just a type environment just a collection of variable to type bindings right so so now what we will do is remember that we have this language of types on top of what we are going to do we are going to do syntatically sugar up the lambda calculus to use those types some how and that gives us church s original simply typed lambda calculus yeah and here is this here is the grammar for the simply typed lambda calculus  refer slide time 44  54  so what we do here is that we okay all variables are lambda terms i can apply to lambda terms but well we will worry about that here is where here is where the problem is that s something we have to fix i can do abstractions but now in this abstractions what i ensure is i ensure that every variable in the abstraction has a type associated with it in the grammar itself and this is really i mean its this is actually analogous to the way we often define sets for example the set of all x belonging to n such that some properties satisfy this is very much this is very much like that and of course its very much like your variable declarations in pascal procedures right var x colon integer semicolon begin then you have the body right so this notation also is which carries forward are original set theoritic notation which was analogous to procedure procedure declarations in pascal and so on and so forth it carries through here right so so now what we have is is just a simply typed lambda calculus where every abstraction has the bound variable declared to be of a certain type this type in pascal programs this type is one of your base types in the case of the lambda calculus it need not be a base type it could be a type constructed from that grammar of types i mean it could actually be a function type that s how we get a higher order functions yeah so the difference between lets say if simple declarations in pascal and in pascal procedures and here is that this type is really what ever can be generated from that grammar and its not restricted to being a base type so then this way we we assume this way this way we get higher order functions from lower order functions by abstraction  student  sir why havent we taken any base type to x  yeah we have we are  student  only x the first one  which this one uh okay well the syntax of the lambda calculus is such that if x is a term i mean x is anyway a term and its its free x is x is a free variable so which means that it gets its value during execution from some environment in which it is some how applied then it will also get its type from that environment then what is the idea of a free variable the idea of a free variable is that it some how gets its values from some where outside that you don t know if you don t know its value if you are not going to know its value when you look at it as isolation you are also not going to know its type i mean for all you know the type may be determined by by the value that it obtains from the outside world how ever when you have local declarations then you are specifying this x right if this x occurs free and if you actually know the context then actually its going to be bound some where so there will be a declaration which has its type specified yeah right so yeah you said something  student  sir we are not able to always through the we may not be able to find the exact type definition mechanism  a closed program has types assigned to every identifier okay so we we are mostly interested in closed programs only there we really be able to execute right but for the purposes of getting congruences and so on we require to also consider non closed terms so that what ever we say for non closed terms also applies to closed terms later remember that compatible closure of beta reduction and so on and so forth we are taking even non closed terms because when you compatibly close it in context those properties are carried through that s the whole fundamental purpose of having congruences pre congruences and so on you consider its you consider things with free variables and isolation and what ever properties you can prove for them you will also be able to you will also be able to satisfy those properties when they are placed in context and closed i mean this is something that happens in any logical language too right i mean there is an assignment of truth for example in a first sort of logic you assign truth values i mean you have free variables you don t know what truth values they are going to have but you don t care they get their truth values from some where but you what ever you can prove you will be proving for both truth values for the free variables for the bound variables you are not going to do that kind of assignment right what ever you can prove for both possible truth values you can prove for its going to be true when it actually allow properties  refer slide time 52  53  you can prove regardless of what the truth value of that variable is you can also prove when the variable is closed up some how when it occurs in context right i mean its like being able to its like being able to look at a piece of a program segment in isolation from the rest of the program and trying to find some values i mean semantical equivalence semantical equivalence we define as over all possible states in all possible environments if the two constructs yield the same results then they are semantically equivalent  refer slide time 53  33  so you are not considering you are considering all possible bindings right all possible bindings if they are both if you can prove it in a general form then you can prove it any context so then in any context where one one of those program segments occurs you can replace it by the other otherwise your semantical equivalence becomes context dependent its no longer context independent yeah so so lets look at this simply typed so this is the modified language of that lambda calculus and the type inferencing just goes as follows we have the three rules for the syntactical construction of lambda terms so for any variable x if it actually appears in this context then it is typable as being of this what ever is a type assigned to x in this context grammar here is your context coming so the x is really free then you can not assign any type to it when if it does not occur any where in gamma you can not assign any type to it and for application if you can infer in from the context grammar that l has the type sigma arrow tou for some sigma in tou in the language of types and you can also infer from the context that m has the type sigma then you can infer that this application has the type tou and if the one or more of these inferences is invalid then you can not infer any type for the application and this is this is really one of the rules abstracted from our school mathematics and then for the lambda abstraction its very simple the type of an abstraction in a context grammar is obtained by taking adding this to the context temporarily and then trying to find out what is the type of this term l so by temporarily attaching x of type sigma to the context updating the context with this type if i can infer that l has the type tou then i infer that this lambda abstraction has a type sigma arrow tou going back to our agenda you will notice that the whole idea is that a lambda abstraction should really look like a function should behave like a function should have a type of a function so it has to have an arrow in it yeah so we will do some examples which which illustrate this type inference for example why delta omega are not well typed and what it leads to later programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 32 monomorphism  58  04  welcome to lecture thirty two so today i will briefly go through what i did the last time and pick up from there on the simply typed lambda calculus so we just looked at what s wrong with the untyped lambda calculus firstly there is a type confusion often you don t know whether you are applying k to the right argument or not for example i mean you might be asking is true is zero true i mean so then are you applying k or true to zero and one may be right  refer slide time 01  01  so there is some type confusion but that s not so serious slightly more serious problem is that constructors and deconstructors are not inverses so which means that unless you cant apply deconstructors i mean if you apply deconstructors on to some arbitrary term which is not explicitly constructed through a constructor then you you might get some result and you wont know whether the result is wrong so that s that s one thing and so and the question is one problem one thing is to find out where the culprits are i mean who the culprits are one possible thing is to actually look at this very general purpose language definition and see whether everything in it is really meaningful i mean may be what one should do is may be one should restrict the language some how so that only really meaningful things are actually used and this is this is something that s quite natural even in our natural languages i mean so so one thing is that one of the reasons for meaninglessness if you like i mean if you if you were to actually apply this lambda calculus as a calculus of functions to some data type applied some where then one thing that really doesn t seem to make much sense and which mathematicians over thousands of years have are agreed upon is for a function to apply to its or for something like this so what happens with things this is that they complicate matters in more ways than one i mean one thing is that they don t actually seem to mean much secondly if you remember with with such replicating combinators i mean you could have other replicating combinators for example i could define a combinator like this which will just keep making three copies some how applying them or like this which will make four copies and some how apply them pair wise and so on and so forth  refer slide time 03  34  i mean this there is an infinite number of such combinators that one can form and the thing is that it doesn t seem like we can actually give a decent meaning to all of them so one possibility is to actually try to restrict the language so that some how you get only meaningful combinators meaningful lambda expressions which are really which really have the potential for being functions which can be applied in a general sort of fashion on may be some data types or some other system symbol position system and thereby get a decent model of computation so and another thing is of course that because of the non deterministic nature of beta reduction some some of these the even if you were to try to extract a meaning the meaning does not seem to be intrinsic  refer slide time 04  22  it seems to be computation dependent so one example that i gave before was of this right i mean if you take this combinator k and apply it to x and omega in this fashion then you have one in a one step beta normal form which is just x or you can do an infinite number of steps reducing this omega to itself right okay so if you were to take i mean this omega reducing to itself is only the the simplest of complications i mean if if instead of this omega i actually took this other combinator which replicates things three times then i will get an explosive computation where no two terms are identical right  refer slide time 05  49  i mean let me call that little omega let me say little omega is this then i could take this combinator like this and then if i try to do k x and little omega what i will what will happen is i will get k x omega omega omega applied to itself apply so apply it to itself right so let me so if you were to consider something like so if you take omega applied to itself what you get is omega omega omega and this can go in various directions depending on how you want to do the application and various all of them will just keep multiplying copies of this little omega so when you take this k x omega omega then there is one normal form which is directly obtained otherwise you get these expressive computations where k x will be preceding each other and all these don t seem to really have some meaning so one possibility is to actually try to get rid of them so so one complication one other complication they come up with is that they yield infinite computations even when there are actually beta normal forms right so so lets go back to our basic mathematics and we have a simple typing scheme of this form  refer slide time 06  59  right and of course remember that we have to look at values and functions as far as possible equally there is an important result in the theory of computation which says that there is no general algorithm to decide whether two given functions are the same or not but but there is an algorithm to decide whether two given values are the same but how ever not withstanding that as far as possible we would like to give them equal status so so one possibility is to unify the notation in this fashion and actually put a check on the construction of terms so that they are well typed right so and then similarly put a restriction that lambda abstraction must be a function that is in some sense ready to be applied okay and the lambda abstraction is actually something that is that somebody should have discovered probably long ago since mathematicians are using such notation yeah so  refer slide time 07  59  this kind of a typing scheme leads us to firstly a language of types which allows higher order functions so we could for example  refer slide time 08  10  start with a finite collection or even in infinite collection of base types if you allow various kinds of pattern formations like this and lets for simplicity assume that we have a finite collection of base types and we could define a language of type constructions okay so so this this language allows us to construct functions of the form int to int int to bool bool to int bool to bool int to int the whole thing going to int to in tint to int going to bool and so on and so forth it allows all these kinds of constructions yeah and one one easy thing to prove is that every type is really of this form this can be proved by induction on the structure of the induction on the production rules of this grammar on types okay so now what we will do is put this typing on top of the lambda calculus some how so one thing is that we will we will define the simply typed lambda calculus as one in which types of variables especially bound variables  refer slide time 09  26  are specified and if you if you look at a complete program as one which has absolutely no free variables okay so free variables will only appear as sub programs only in sub programs similarly if you take a complete lambda expression as a function then there should be no free variables all the variables should be bound so only sub terms can have free variables right so and of course this notation is analogous to our standard set notation where actually this is like a typing constraint and for example considering if you define the set of all numbers which are even okay i mean even doesn t make any sense with i don t know with real numbers or lets say the complex numbers and so on and so forth so it makes sense with naturals so you put a type constraint on the numbers right i mean this is a very common practice and we just follow that practice and construct the simply typed lambda calculus in this with this two level syntax one level syntax is for the typed language using the base types and the other level syntax is for the actual lambda expressions right and now the important thing is i can actually determine the types of various lambda expressions statically so we will define a context as a collection of variable to type bindings and we will call this a static environment okay the reason for calling it static is that what ever we are going to do something that can be determined at compile time it does not require run time checks whether a certain lambda expression has a well within quotes a good type is something that can be determined by a compiler without executing the program right okay so so that s why the word static when ever in any in anything to do with programming languages and compilers if the word static is used it means it is something that can be done before execution at translation time or at compile time if it s an interpreted language it still translate it you can do it before execution what ever can be done before execution is said to be static otherwise what ever can be done only at run time is dynamic yeah so we can construct a collection of such bindings and essentially this context is what constitutes the symbol table that the compiler i mean the symbol table that a compiler constructs has this context as an essential part of this symbol table i mean the types of the various identifiers and so on and so forth yeah so an essential part of the feature of the symbol table which is really a static environment as opposed to a dynamic environment which is the activation record stack and so on and so forth okay so an essential part of the symbol table is really type checking and its all something that can be done at compile time without actually running the program yeah so this so what ever we do is something that can be done at compile time so lets look at the type environment  refer slide time 13  00  and we can give actually inference rules of this form so we have to look at we should we should look at type inferencing of course in a structurally inductive fashion right going down the syntax tree some how so when you go down this syntax tree even if you started out with a program which had no free variables when you look sub expressions and the sub programs then there are bound free variables in that context so so we will assume that there exists a context so for a fully defined program for a full program especially a program which doesn t use library routines and so on and so forth complete program actually starts with an empty type environment just like often it starts execution in in an empty environment that an empty activations record environment yeah so during the process of compilation you will be collecting a lot of type information about the variables so for so when ever there is a reference to a variable okay you look into a type environment if the type environment has a type specified for it then that s the type of this variable right so its essentially you just access the symbol table and see whether that variable was already declared and if it was declared in all languages which insist on declarations preceding use if it was declared then there must be a type information unless it s a forward reference in which case it has to be 14  40 but in general the symbol table should contain the type information if declaration proceeds use and of course there are implicit declarations and so on but lets not worry about it lets live with this the most of them are otherwise algorithmic aspects which have nothing to do with the system of typing and then now we would say that this application is well typed and actually has a type tou only provided l has a type sigma arrow tou for some sigma and m has a type sigma essentially what we are looking at are something acclaimed in mathematics to domain and range information domain and co domain information so you if if there is a function from natural numbers to natural numbers then it has to get an argument only which is a natural number so you insist that all applications are meaningful only if the argument to an application the operand is of a type that is consistent with the domain of the operator right and if it is so then you you can infer the result the result is what ever you get in the co domain right is as simple as that and in the case of a lambda abstraction you insist that it has to be function because it has a bound variable which is some how replaced by a beta reduction to a function application which means a pure lambda lambda abstraction should actually represent a function i mean its type must be something which contains an arrow it can not just be a base type a lambda abstraction can not be a base type it can not be you cant have a lambda abstraction which is of type int okay it has to be a function from something to something so but how ever if you go about things in a structurally inductive fashion  refer slide time 16  50  assume that the body of the lambda abstraction has a type tou then this lambda abstraction is really a function and if x is been declared as being of type sigma then this lambda abstraction represents a function which goes from sigma to tou so which takes arguments in sigma and gives you results in tou and so that s and the important thing about all this is that there are no executions involved its all something that can be done by a compiler as part of the that s why most of the typing information is done at compile time there is absolutely nothing at run time that is necessary for such things yeah so having done that so and and and the static environment is very much like the dynamic environment you are doing temporary updations because you will have newer and newer declarations with newer with bound variables with bound variables being re declared i mean you will have newer declarations may be for the same identifiers so you have to temporarily update your environment so that you know what are the what is local and what is global and these things are all meant to be local so you the static scoping rules are in force so you require this temporary updation because of static scoping rules yeah so so these rules actually tell you what a well typed term is so i would say so we would say a term is well typed in a context lambda in a context gamma provided its possible to infer its type by the application of rules t one t two and t three a finite number of types remember that these are rules and they have to be applied only a finite number of times otherwise your compilation will be an non terminating process yeah if it requires an infinitary proof then your compilation itself is going to be non terminating forget about executions even your compilation is going to be non terminating which you don t want right okay so so the next question is have we actually achieved the purpose i mean have we banned combinators like delta  refer slide time 19  19  yeah and so lets look at delta itself and try to see whether by applying these rules t one t two and t three we can actually we can actually infer a type or what can what actually happens so so lets lets assume given an arbitrary context gamma and so supposing delta were of type since it s a lambda abstraction it could quite well be of a type sigma arrow tou for some sigma and tou in terms of the base types right so but delta could be of types sigma arrow tou if and only if the body of delta which is x applied to x is of type tou okay now x applied to x could be of type tou only if this x were of type row arrow tou and this x were of type row i mean otherwise how would the application be meaningful right so so now what so what you are embarking on is going down as deeply as possible in order to infer a type i mean you are trying desperately hard to give delta a type and the conditions the constraints you are getting are this so i can infer a type for delta only provided i have x of type row arrow tou and x of type row both in the same static environment right now this thing is possible only if this sigma is equal to row arrow tou and sigma equals row so which means that  refer slide time 21  13  see after all its possible that these things look deceptively different but they might be actually solvable as being equal i mean how do you know they are not solvable okay but when you when you do this when you get it in this form what you see is that when you do the substitution essentially what you are trying to do is you are trying to unify these two terms in order to find a solution in fact a most general unifier so when you try to do this what happens is that you just keep expanding out infinitely  refer slide time 22  03  and if you if and this keeps on expanding in this fashion infinitely and your tou has is not yet been fixed to a base type int or bool remember that our language of type so such that every type that is in that is valid is something which will have last thing which is a base type you have not yet been able to infer the what is the base type tou which you are getting every time in in every unfolding of this equation okay now so therefore this inference is going to go on infinitely so without an infinite proof one can not infer a type for data okay and of course even after an infinite proof i don t know what the type is okay so this which is impossible here which i is really to be read as something that what ever may be its type its not inferable in a finite proof right so so now assume that so the point about the point about unification of course is that i went about it rather simplistic manner but the unification algorithm is deterministic and it will clearly point out that this is impossible equating these two is actually impossible any unification algorithm will be able to point that out right so it wont even go out through an unfolding it wont even look upon it as a recursive definition which is to be unfolded it will it will look upon it as an equation which is to be solved by a most general unifier and it s impossible to get that most general unifier okay because the disagreements sets are such that one is the subset of another okay so when the disagreement sets are like that then you know that you are not going to be able to solve it that s how a unifier would actually look at it so it is actually compile time feasible to detect that delta can not have a type assigned to it its not necessary to go through an infinite unfolding process but logically speaking from the point of view of rules what it really means is that you can not infer in a finite proof the type of delta yeah so so that s how the simply typed lambda calculus goes and what we will do is so we will just look upon the language now that that the grammar for the types itself puts the appropriate restriction the inference rules put the restriction on the kind of terms which can be executed  refer slide time 24  30  so what it means is that your if you put in a typing inferences engine with a unification algorithm in your compiler the compiler will just throw out all those terms and say impossible the type can not be determined and therefore it will not even permit execution it will not generate code to execute so we will just look upon the simply typed lambda terms as all the well typed terms of generated by the two level grammar of types and and the lambda calculus so and of course we just have to complete a few formalities we have to define what is beta reduction and we should define beta reduction in such a way that  refer slide time 25  18  its well typed so if now this this portion is not part of the language but this is what the compiler has inferred what ever is in light blue is really part of the language okay of the simply typed lambda calculus but by by an application of the rules of inference assuming that l is a term of type tou what your what your type inference system will produce is that this lambda abstraction is of type sigma arrow tou and then assume that it can it also and only if it can the argument that you give to this application only if that is of type sigma will it actually perform a beta reduction so what ever is in dark blue here is really something that is that is actually not part of the language but is something that s part of your type inferencing system which is again a part of your compiler or translator for the language okay so the actual terms are those that are given in library so given an application of the form lambda x colon sigma bar l applied to m and assuming that the type inferencing system can assign these types to them through the application of rules t one to t three then a beta reduction is possible which will give you a term of type tou yeah and what we can do is we can carry these carry these definitions forward just as we did before i mean you can define a many step beta reduction equality on beta now when you do equality on beta its its guaranteed that you can never equalize two terms which do not have the same type okay after all they should both be beta reducible to a common term and so all the all the terms in your beta reduction should have the same type all the steps in your beta reduction should have the same type so that is so that is implicitly guaranteed once you go through a type inferencing system yeah so and of course and the interesting thing is that since delta and omega are not well typed and they are thrown out by a type inferencing system you are also not going to get these horrible infinite beta computations okay actually there is there is a caveat there that does not mean that with a type inferencing system you can guarantee that every program terminates you can not guarantee that every program is an algorithm its only guaranteed for the typed lambda calculus with base types which are not actually applied some things like numbers the moment you bring in numbers we can we can we can really sit together and design a really lousy definition which will run for ever okay but the but the thing is if you remove functions or numbers and you look at only the typed the simply typed lambda calculus with this beta reduction with all these replicating terms out there are going to be no infinite beta reduction infinite computations do come in the moment you apply it on to some other domain like numbers but with just using this base types int and bool or what ever as patterns and not actually using any integers or booleans just looking at the lambda terms without actually doing number computations i mean don t include don t bring in pioneer arithmetic nothing just use these int and bool as patterns for possible values just look at pure lambda terms there will be no infinite computations because you you out load all these kinds of terms which have the potential for replication okay so so that s actually so what happens so in the in the typed lambda calculus what what it means is that then beta reduction is strongly normalisable they are always guaranteed coupled with the fact that coupled with the fact that beta reduction is church rosser what it also means that there are unique normal forms if something is church rosser and it has two different terms then they should they should be able to meet at a common term by the diamond property which means you will have a unique normal form because if you have two different distinct normal forms which are not mutually alpha convertible then by the church rosser property it says that there is still there is a common computation which they should both meet so you can not have two distinct normal forms this is some of the nice properties that come out of type checking and type inferencing yeah so  refer slide time 32  11  so lets just so lets so one so the lets look at look back what we have done so one thing is that when you do simple types the type inferencing can be done entirely at compile time or translation time in the case of interpreted languages can be done at translation time before you actually perform any kind of executions yeah and the second thing is that there are no replicating or self applicative combinators in this new language and therefore those horrible infinite computations are are outlawed and the type inferencing of course is done by structural induction which in practical terms in terms of a parser and so on means that you will do it through a recursive descent parsing technique if you compiler has implemented by a recursive descent parsing technique then you will do it through that if its if its some other parser like a table driven parser something then in the table you can also incorporate these rules for the appropriate productions yeah so and then so and all these horrible combinators which complicate life are removed on the basis where they are really meaningless things and they are really complicating life and they are all since they can not be typed the unification algorithm will produce failure the unification algorithm within the type inferencing system which is within the compiler will will produce will actually produce a failure so the type inferencing system will actually produce the failure because of that and the compiler can just throw this programs out without generating code yeah so so the simple typing the simply typed lambda calculus that we have looked at basically starts with the assumption that no term which contains either self application or a form of replication can be well typed and therefore anything that is built up on top of self applicative or replicating terms can not be well typed i mean after all by structural induction unless you can type the innermost terms you can not type the outermost term so if this omega and delta and so on are embedded deep inside a huge lambda term your recursive descent parser will go upto the omega lambda and keep coming up from the recursion and produce a failure okay so so which means and then and the other nice thing is once you have put in a type discipline okay you can not apply arbitrary deconstructors to arbitrary objects okay so then your constructors and deconstructors will actually be inverses  refer slide time 34  02  so a deconstructor will be applicable only if its argument is something of a constructor type of this of the corresponding constructor type and constructor will be applicable okay to what ever only provided the elementary objects from which it constructs are of the appropriate types so then you can actually once you have constructed these thins you can keep applying these constructors and deconstructors as long as your compiler allows then you are guaranteed that they are they are meaningful right okay so so now lets look at beta normal forms so beta normal forms since there are no infinite computations unless you actually give really lousy definitions all computations atleast of the pure simply typed lambda calculus have a guarantee to terminate computations of the applied lambda calculus infact any any calculus which incorporates natural numbers as a data type means that it has a potential for infinite computations okay so only in the pure lambda simply typed lambda calculus is it guaranteed that there are going to be no infinite computations the moment you apply it on something like moment you give these definitions over an applied term which are moment you give bad recursive definitions you are going to get infinite computations okay right so and then since beta is church rosser unique normal forms exist and you can always find them for the pure simply typed lambda calculus okay so now lets look at what what is the pure simply typed lambda calculus  refer slide time 36  02  term look like you know what are the what are some of the meaningful terms right so what you will find is okay so lets look at some some of the simple combinators we cant use omega and delta and so on and terms like that but we can use some simple things like you can use k you could use the identity you could use s so lets look at the identity function over these two base types integers and bool right so lets look at this so the identity function in the simply typed lambda calculus over integers will look like this and your type inferencing system will since when it goes down it collects this information that x is of type int then when it gets down to this x this free variable x it gets the information from the context that it is of type int then when it comes up again since it s a lambda abstraction it pronounces this to be of type int arrow int okay so the identity function is an integer identity function so basically what it does is if you give an integer argument to it it will return back the same integer argument that s all yeah but the point about this is supposing you give a boolean argument to it then it will throw it out because it can not because its of wrong type it does not satisfy the conditions of the beta reduction right so if you give remember that even if you use the lambda calculus representation of numbers and booleans you will use different you will have to include type information on all the bound variables there so you can not give a boolean argument to this function and expect to get any answer it wont type check the beta reduction can not be enabled because it expects an argument of type integer and you are giving it an argument of type boolean and it will throw it out right so which means what what the good thing about that is you cant for example ask questions like whether zero is true right so so what does it mean supposing you want an identity function on booleans what it means is you will have to have a different combinatory like this okay what applies to the identity function also applies to other complicated functions point is that they should all be well typed and what happens to the boolean now what if i want an identity higher order function which takes a lower order function and returns me the same function in general what if i want a combinator which transforms one higher order function into another higher order function like like derivatives for example the derivatives are higher order function which takes a function and gives you another function  refer slide time 40  29  so here i am taking identity as the example but it could be any of those functions the point is that they are not going to be type checked unless the arguments and the functions are of appropriate type so supposing you take a higher order function for which you want a higher order identity function but you cant take any arbitrary higher order function you have to take lets say you take a higher order function of type int arrow int then you have to have a special identity function for int arrow int if you have a function from int to bool then require a special identity function from int to bool so for example this this combinator accepts only arguments of the form which have the type it arrow bool that means which are functions which so it accepts as arguments this is a higher order higher order function which accepts another function whose type is int arrow bool and gives you back the same function right so if you want int arrow int you will require another combinator if you want int arrow bool arrow int arrow bool then you will have require yet another combinator and so on and so forth so which means for every type tou and there are an infinite number of such types because it s a context free grammar on the on the language of types starting from even to even from a finite set of base types i can construct an infinite number of types which means even for simple function like the identity function i require an infinite number of identity functions in order that my type checking actually works right so that s that s what happens so which means what what we looked on as in the untyped lambda calculus if you look at all the combinators which can some how be ascribed types with the new with the type system so for every combinator c which for which you can ascribe a type there are actually infinite number of typed versions of that combinator and only the appropriate typed version should be applied to the appropriate argument type so if you had a combinator c like k for example  refer slide time 41  42  the combinator k in the untyped lambda calculus for all types sigma and tou for which k is a valid combinator to be applied on types of on type sigma and tou you will require new combinators one for each sigma tou combination so all these combinators are all going to be so each combinatory of the untyped lambda calculus is going to be multiplied an infinite number of types an infinite number of times to cater to each of the infinite number of types that are now generated but if you look at the code of this combinator and infact what happens is looking we have been we have been up in this type for a long time so may be lets so what happens is that in most programming languages the statically typed languages like pascal and modula actually use this simple typing scheme for their functions and procedures yeah and people claim that c uses it but c has a lot of dangerous things which also do not use it for example this returning void as a in a function is not really is not something that is really statically typable its its actually an untyped form which is why you can do a lot of you can do a lot of manipulation of types using those voids and using pointers in c okay but for what ever is actually declared c does use the simple typing scheme that we have seen in the lambda calculus okay so languages like lisp  refer slide time 43  42  well they do allow integers and so on and so forth as types but mostly lisp is really an untyped language if you remove all the data types from lisp and look upon pure lisp as a version of the lambda calculus then its really untyped there is absolutely no type checking mechanism there is no type inferencing mechanism and there is no worry about whether you are applying some combinator to some argument which you shouldn t be applying so its mostly untyped its just that most of this implicitly once you have typed data the typed data the underlying data type is well typed then that typing often works from most of our run time environment so where as long as the values are from those data types the typing works mainly because of the representation the representations are nicely ensured and the machine which ensure that you get reasonable values so and what holds for lisp also holds for scheme and c with its void construct is actually going into the untyped territory of the lambda calculus yeah so most of these untyped languages simply don t bother about typing though its an important way of catching catching bugs at a very early stage so and its becoming more and more important so the so so most languages though i haven t yet spoken about name functions and procedures i have spoken about named i have spoken about unnamed functions and unnamed blocks so far but essentially if you take all those blocks and give them a name you get your named functions and procedures and of course you should allow parameterization which is something we will do later we will which will start immediately after sometime after this so now the next question is so so what we started out in the simply typed lambda calculus was that self application is really meaningless and no self respecting mathematician will use self application but long long time ago we actually looked at this combinatory twice and if you remember what we did was the firstly so a version of twice for the simply typed lambda calculus would be something like this right i mean for the bound variables i have to specify types so i have done that otherwise i have made no other changes in the definition of twice yeah so this twice of course because of the typing constraint i have to give since x is applied to y i have to give x a type of something arrow sigma but since x is applied to that the result of that i have given it a type sigma arrow sigma and via a type sigma so that so that its well typed so this is actually a well typed expression yeah but now the point is what about twice twice i mean we had actually looked at this application also okay the moment you put these type constraints on the simply typed lambda calculus twice twice is no longer a well typed okay if you remember the fact that this sigma each of these sigma is something of the form arrow arrow arrow arrow arrow which ends up in a bool or an int you at once you at once find that twice applied to twice is not well typed okay and but we actually applied it and we got some nice results the next question is we actually applied it and got some nice results so is twice applied to twice actually meaningful i mean is there something are we being too restrictive i mean is it is it becoming like is it becoming like a dictatorship you know to put in simple typing and this allow things like this and when we apply twice to twice we actually got some results right you remember we got the octupling function or what ever twice is applied several times all that made perfect sense so what it means is that it means that it means all self self applications need not necessarily meaningless i mean its true i mean you cant apply a function from real numbers to real numbers to itself its not going to type but there are enough functions like twice which look meaningful i mean what does what does twice do it just takes any function as an argument and for any argument that that function might have twice applies that function twice i mean that s that s really all it does which is perfectly meaningful i mean after all given given a real number x and f f is a function from real numbers to real numbers f applied to f applied to x is perfectly meaningful right i mean this there is no problem with that and what all i am doing by specifying twice is that i am saying you take any arbitrary function f on real numbers and apply it twice on what ever argument you get i don t care what function on real numbers you are taking but what ever it is you just apply it twice and then give me the result  refer slide time 50  21  so twice is a nice higher order function its and its actually in some sense type independent and there are lots of lots of such functions and the important question that actually arises as part of all this is what is m l s view on types secondly there is another important question i said the identity function you are going to have infinite copies of the identity function i mean just imagine just in order to give you back what you gave me i require an infinite number of copies which check what type it is send it to the appropriate copy and then send you back the same thing right so you are going to i mean so this what about the code that you that is going to be written for something like the identity function regardless of the type of the argument the code is going to remain identical there is no difference at all what is the code essentially what the code says is take it and give it back i mean that s that s really all that the code says take it and give it back even without looking at it but your simple typing scheme actually puts a restriction it says take it look at it and only if it is compatible with you send it back otherwise don t so i will require an array of infinite number of programs which do nothing but take and give back and we are actually got this problem for whole lot of programming problems for example what about the cons  refer slide time 51  39  of integer lists should the cons of integer lists be different from the cons for character lists should the cons for integer lists and character lists be really any different from cons for lists of integer lists or lists of character lists or lists of lists of integer lists lists of lists of character lists and so on and so forth right assuming that a base data type could have such an infinite collection then your type your simple typing does not it only creates more problems it creates a tedium you will have to create copies where only the type name is changed you will have to create an on demand when ever you get a new copy you will have to create a new program in which the type is changed this is the problem with pascal for example whether if you define stacks of integers you cant use that program for stacks of characters you cant use the program for stacks of strings you cant use that program for stacks of records of something of the other though the actual stack operations pop push and empty are going to be identical in all these cases and the reason in pascal and modula and so on you cant do it is because they use a simply typed scheme a simple typing scheme which requires an infinite number of copies where only the types of the bound variables have to be changed okay whereas in m l in lisp you don t require it because its untyped it doesn t care what type you get i mean so that s essentially the difference between the typing in m l and lisp or m l and scheme because m l and scheme are both statically scoped languages they are easy to compare in in scheme you can do cons of for any kind of type but that s because all types are regarded as being just the same type as being type lists so you can do cons of integer the same cons is applicable to integer lists integer integer star integer lists character star character lists integer list star list of integer list and so on and so forth but that s because the cons in scheme is type lists and its essentially like the untyped lambda calculus and so it doesn t care what the argument is in m l the cons is the same except that it is typed its what is known as the polymorphic type so you use the same code but now our base types are type constants what you require are type variables which are going to be instantiated on demand yeah so type variables are required what we are saying is that if you look at the cons operation then for all types as long as they are types of the form something list for all types t such that an argument is of type t and another argument is of type t list its possible to do a cons of the type object of type t with the list of type t list and i require the same piece of code i require only one copy of cons for that i don t require an infinite number of copies yeah so this polymorphism so we will look at the polymorphic lambda calculus where we actually move from we actually move from the simply typed to the parametrically typed and this polymorphism is what is present in c plus plus and aida as generics so in so for example you define the stacks you define stacks in aida you use a type variable which is not going to be instantiated okay and you write all the code for the stack operations pop push checking emptiness so on and the compiler compiles it you call this code for producing stacks of integers stacks of characters stacks of records what ever but at the call to this code the variable the typed variable is initialized to integer to the type typed variables are different from value variables right so you have a notion of typed variables different from value variables which can be instantiated on a call so you produce particular instances of the same code for the same type the simplest implementation of course is that instead of you writing the code for integer stacks real stacks character stacks so on you separately write it as a generic package in aida or c plus plus and the compiler will produce code for for what ever type you are demanding those operations to be used it will actually replicate the code by changing the typed variable and putting the base type that you are entering that that s infact most what most aida compilers do they actually replicate the entire code for that call but its also possible to use reentrant code use the same code with the typed variable instantiated which is what the m l does yeah you can use reentrant code without actually generating new code so we will talk about polymorphism in the next class programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 33 polymorphism  55  59  welcome to lecture thirty three so i will just recaptulate what we did about monomorphism and then go on to polymorphism today right so we defined the language of this simply typed lambda calculus where there is a where there is a there is a notion of some base types and the construction of the construction of higher types from the base types and these base types which for simplicity i took to be integer and boolean are really what might be called type constants they are really i mean if you say something when you give something the name integer its in the in the domain of types it s a constant now the significance of that will become evident slightly later but its important to remember that they are type constants okay so you can construct complex types using the simple language that we gave for constructing higher types from these type constants  refer slide time 1  36  so essentially what it means is that all your types will be of the form lets say if int and bool are your base types then you will have types of this form int arrow int int arrow bool may be you could have you could have for example higher types like int arrow int arrow bull arrow bool and so on i mean what i mean is expressions are going to be of this form all your type expressions will have the names of the base types always occuring in it i mean there is nothing else there is nothing else to it okay so and then we gave type influencing system for for this which from which you can prove that you can prove that every combinator therefore  refer slide time 2  47  or every lambda expression in the simply typed lambda calculus actually has a unique type based on the type expression based on this inference rules yeah so given that a variable has a certain type which the variable could have a higher type also remember that we are treating functions and variables functions and values all as equal objects so for example this x could be of type int arrow int which denotes that it s a function from integers to integers and you could infer therefore the types of all the lambda abstractions all the lambda expressions in the language which are typable and if they are not typable then of course it doesn t belong to the simply typed lambda calculus and the beta reduction is of course modified to take typing into account  refer slide time 4  04  so that only if you have a lambda abstraction which which is typable and something arrow something and as lets say sigma arrow tou and you have an operand which is of type sigma assuming that these types can be inferred and therefore they are well typed terms then you can perform a beta reduction and get a value of type tou right so one of the things was that so we looked at some of the examples dealing with the identity functions and so what what  refer slide time 4  34  we notice at this point is that every every combinator are useful function because because of the type inferencing system because every combinator has a unique type which is built up from the base types therefore for what used to be typeless combinators which could be applied anywhere in the lambda in the untyped lambda calculus for each one of them you have several copies depending on the type of application right so even a simple integer a simple identity function then therefore has a incarnation for integers has an incarnation for booleans has an incarnation for all functions from integers to booleans so for every tou that you can think about that is the separate identity function like tou okay and they all and this is because of the unique typing feature of this simply typed lambda calculus right so for each type tou the combinator c any the analog of the combinator c in the untyped lambda calculus when you move it into the simply typed lambda calculus for each type tou you will get a combinator c tou which respects that typing okay  refer slide time 5  51  so and of course we asked this question whether we are actually being too harsh by using a simple typing scheme and one thing of course is that these simple typing schemes were what are used in some of the older  refer slide time 6  04  programming languages like pascal and modular and so on and so forth and of course there are some questions i mean we we actually went through the computations of twice applied to itself and so on and we found that some of them could actually be given a meaning so not all self application really is meaningless and secondly the the point is that these if you take any combinator c is really too tedious to how so many different copies of it for each you have a combinator c tou for each possible tou the combinator could be some complex program right so its really too tedious to have something like this what we would like when we think of an identity function what we actually are implying really is that it s a higher order function which given a function of any type tou returns you the same function actually but returns you a result of the type tou so the and the so the identity function regardless of the type for which it is meant actually is is one really higher order function which could be parametrized on the type so we can talk about i the identity combinator i being parametrized on a type tou and i tou would therefore and the result should be i tou which is the appropriate combinator for values of type tou by values i also include functions right so so and infact such a such a generalized what i might call a parametrized typing so what we should be able to do is we should be able to take this identity call a general identity function i and parametrize it on the subscript right and that is that is what polymorphism is about then we have an identity function which actually takes a type itself as a parameter and then specializes to that type right so we and as we saw there are lots of lots of functions which are which we use in all our m l programming and so on which really are of that type the head and tail functions the cons function the map function  refer slide time 8  42  they are all in that sense polymorphic i mean in the sense that they the actual function is not very crucially dependent on the underlying base type from which your data type is constructed i mean the function remains more or less unchanged except for the type for all possible kinds of arguments that you might get or atleast for a class of possible arguments for example i mean you cant apply cons on an integer cons for between two integers but there is a class of arguments namely integers and integers lists booleans and boolean lists may be integer integer to integer functions and lists of integer to integer functions and so on for a there is a class of objects for which cons is going to have essentially the same representation and our intutive meaning of cons is just that given some given some argument of type tou and given another argument of a type list of elements of type tou you should be able to perform a cons and the implementation or the meaning should not significantly vary with the with variations in the underlying type tou right so so the monomorphism actually has this real problem  refer slide time 10  15  that you can not adequately parametrize it so we generalize monomorphic types so that they give us this general flexibility and that means that in addition to these type constants which are the base types we also allow for type variables okay so and that s that s intutively even how we look at the lets say even a simple function like the identity function right so what what we are saying by a general identity function is that for any type t and for any value or function x which is of type t the identity combinator i t when applied to x is some how beta equivalent actually it should beta reduce in many steps may be to x itself okay now what we can do if is if you look at the lambda abstraction as we have looked at it i told you the analog of the lambda abstraction which sets the lambda abstraction also has its analog with universally quantified objects okay so and this is essentially a universal quantification  refer slide time 10  50  so if you look at the lambda if you look at a lambda term of the lets look at the identity combinator what we are essentially saying by this bound variable x is that we are saying that for any x return back the value of x okay in the case of sets what we were saying is if you had a predicate here lets say p dependent on x is saying for any x such that the predicate p of x is true so the set notation the lambda abstraction and universal quantification are all very very similar and we will use this fact so if were to actually read what ever we have been saying now a good way to read it is as if it is a universally quantified object for each type t and for any value or function x of time t i t of x should be beta equal to x so what we did is that we stopped here in our lambda calculus in our simply typed lambda calculus we stopped with this abstraction we just translated this abstraction into the combinator i t right but if we go beyond this abstraction and also into this abstraction then what you get is a general combinator i okay so for you can read this is for any t and for any x of type t return x and i am just and since the i mean the universal quantifier and the predicate logic is really like lambda abstraction i am just using the universal quantifier so this identity the generalarized identity combinator that we are really looking for is really has a type which is given by a universally quantified type variable the moment you have got so now the here here you have got a case where there is a variable which is bound by this universal quantifier right so this is like again i mean this is like a local declaration this this this is this is like a predicate if you like so but this local declaration clearly specifies that for any type so the identity combinator is has the type such that for any type t it has a functionality a functionality t arrow t where i dont really care what value you give this t okay if you are generalising from monomorphic types then what you can say is that this variable t may be may take any value from the monomorphic types defined by this by the language of types so for example so this t could be a value like int could be bool could be int arrow bool it could be int arrow int arrow bool and so on and so forth any of the types that we have so far defined in our simple type structure could be a value for t and the identity approximately and that identity combinator will will will appropriately take those values t okay so now what we are looking at is i mean we we started out with variables and constants way back in the in the in the paleolithic period where constants and variables really took values from a underlying domain of values we took then we got functions unnamed functions which are also variables we generalize functions we generalize our notion of variables to functions untyped functions where the functions could take values from well function spaces an underlying domain which is function spaces now we are going further and we are taking the domains themselves types are really those domains and those domains are fixed so far and now we are generalizing them we are saying you take a variable which takes values which are the naming of the particular domains the domain int arrow int could be a particular value of this variable t okay and this universal this is like a universal quantification okay so if this lambda abstraction is like a universal quantification then lambda lambda application or a beta redex is like universal instantiation so i mean you you studied this quantifier elimination in introduction rules so lambda abstraction quantifier introduction and beta reduction is quantifier elimination or universal instantiation so you can instantiate the t here by any particular type constaant that you like or any type expression which is built up only from type constants  refer slide time 19  40  so t could so you can talk of type variables which actually take values and those values are type expressions built up from type constants and that is what polymorphism is about right so this is what is known as a parametric polymorphism and this is the this is the polymorphism that is present in m l yeah so so lets lets look at the function for example twice so the reason twice is meaningful then is really that it is a function of this form if you look at the definition of twice i can say that for any type t for any function f of type t arrow t and for any value or function x of type t twice f twice applied to f applied to x should be beta equal to f applied to f applied to x right so this is this this is the basic fact we know about twice and now go back upwards and do the abstraction so what do you get you get f applied to f applied to x you perform the abstraction on x to be of type t then you perform the abstraction on f to be of type t arrow t and so far its really like the simply typed lambda calculus and then actually we have an yet another abstraction a universal quantification over the type t and so this essentially says for every type t and for every function f which has a type t arrow t and for any argument x which has a type t return the result f applied to f applied to x and so what is the type of twice i will i mean the problem the point is now that now we have two kinds of beta reductions if if universal instantiation if lambda expressions are really like universally quantified predicates then beta reduction is like universal instantiation okay then an universal instantiation is like beta reduction which means when you universally quantify on types and instantiate those types you get a form of beta reduction also for types in addition to the beta reduction that you have already for the lambda expressions so its as you can see things can get a little hairy at this point okay so we have variables we have type variables we have constants if you are applying the lambda calculus on to some domain and you will also have type type constants and then you can have instantiations of those variables instantiations of those values instantiation of value variables instantiation of type variables by type expressions by expressions of the application and you will be inferring types you will be inferring so types and values also look essentially the same you are going to have a beta reduction for type expressions for quantified type expressions which which is like another lambda expression only this lambda is abstraction is on types and its not on values and of course values are the same as functions what ever may be the order right but types are different but still types also follow essentially the same discipline of quantification beta application beta reduction universal generalisation universal instantiation quantifier elimination quantifier introduction and every thing these are the this is the and  refer slide time 24  05  so okay so what is the type of twice so i will just assume that you already know the inference rules you can get the inference rules by analogy so we can actually by structural induction on the inference rules which are predictable now you can actually infer the type of twice in this fashion so given that f is of type t arrow t and x is of type t f applied to x will be of type t f applied to f applied to x will be of type t too because of this f is of type t arrow t and f x is of type t therefore f applied to f x will be of type t then when you perform the abstraction over x i am doing this bottom up of course but that s always easier to understand for the human being that s not necessarily how the machine will do it how your compiler will do it note that all this has to be done by the compiler so it will do it by a structural induction you can assume for practical purposes it will be doing it by recursive descent parsing method as part of the parsing process type determination is part of the parsing before the code generation you do the type determination in order to decide whether the code has to be generated at all and so you will do it in a recursive descent parse parsing fashion and come up so then this abstraction gives you this type then the abstraction over f gives you this type and the abstraction over t gives you this type right so essentially what we are saying is that functions like twice which are actually meaningful they are meaningful in and under self application because when twice is applied to twice the operator twice has a type twice since twice is polymorphic its type is universally quantified we can always choose a type for the operand twice and generalize it so that the operator twice has a higher type than the operand twice and that s infact what normally in mathematics when you apply one function to another when you apply a functionto another the the operator always has a higher type than the operand now since twice this polymorphic this twice when you apply twice to itself this is the operator twice and this is the operand twice the operator twice is of a much higher type than the operand twice  refer slide time 27  19  so you assign if you assign if you assign to twice this the operand twice a type tou and that type tou would be an expression of this form universally quantified a universally quantified type expression of this form then if this has a type tou where tou is lets say some for all sigma such that i i have i have missed out a t here this should be a t here t arrow t goes to t arrow t right for all so for all sigma if this twice has the type for all sigma sigma arrow sigma arrow sigma arrow sigma then this twice has a type so the result of applying this twice to this twice should give you an element of type tou right so then this twice has a type which is really given by tou arroe tou arrow tou arrow tou on application treated in general it has the type when you look at this twice i mean that tou arrow tou arrow tou arrow tou has to come out as this particular case of this sigma by a suitable substitution process i leave that as an exercise to you but essentially polymorphism means that an expression is polymorphic if it can actually have different types depending on the context in which it is applied right so the two so the application of twice to twice is meaningful provided the operator twice has a higher type which is compatible with the operand twice which is compatible with twice of the operand a particular case of this is something that you can you can see in any standard book on polymorphism or programming languages okay so what we will do is so lets formalize this notions so we have the language of what i might call it polytypes okay so now the language of polytypes as supposed to the language of monotypes which is what we did in simply typed lambda calculus is firstly you assume an infinite collection of type variables and a collecion of type constants and these type constants are usually are the usually consist of the base types which you are going to start of with so lets say integer and boolean and then firstly you we build we build up the monotypes in the same fashion that we did for the simply typed lambda calculus if b is a base type then b is also a monotype and if tou one and tou two are monotypes then tou one arrow tou two is a monotype in addition you allow type variables also to be regarded as monotypes  refer slide time 29  57  so type variables actually are going to denote particular instances of monotypes right then we build up polytypes like this any monotype is also a polytype and any mono any polytype quantified over a free type variable the notion of free and bound variables is as before over type expressions if if pi is some type then any variable that occurs in it any type variable that occurs in pi is a free type variable and you can quantify over type variables and then that variable becomes bound okay so so this is how we construct polytypes yeah so this is the kind of when when there is i mean if you just extend this argument further we could also construct super polytypes by a similar grammar given that pi is a polytype you could define a collection of an infinite collection of super type variables a collection of polytpes over polytypes and then super polytypes being defined in a similar fashion yeah so this the type hierarchy actually can go add infinite upwards though lowest part of the type hierarchy are the monotypes yeah right and below the monotypes of course are values and functions right so lets so lets limit ourselves to just this the type hierarchy at two levels so they are just type variables and type variables can vary can take values only from monotypes and monotypes was what ever was were defined in the simply typed lambda calculus right so and then you can construct a polytype by quantifying over the monotypes we are quantifying over the monotypes we are quantifying over the type variables right so now so the polymorphic lambda calculus is defined this fashion so we have the usual syntax of the simply typed lambda calculus remember that this is a monotype remember that the simply typed lambda calculus was very nice in the sense that it gave you every thing that was that could be statically type determinable it could type check statically but the only problem was that the simply typed lambda calculus could not account for generalized combinators like i or twice which are polymorphic i is of course very very general twice is not so general i mean twice can will type check only for certain classes of arguments argument types i mean for example you cant give twice a value from the base type it wont type check  refer slide time 35  42  for example you cant apply twice on a integer twice can be applied only to another function which means it should have a type of the form some t one arrow t two i mean it could only apply to an other object which has an type t one arrow t two it can not apply to a to a to a just a base type has to apply to a function right so but that is essentially like the way we write a a sets right lets take this x such that x is even and then it satisfies some other property i mean you can take sub types of i mean you can take subsets of the naturals and write generalized definitions right which is still quite general right so so the so functions like twice are polymorphic in the sense that they do not range over the entire type hierarchy that is the the combinator i actually ranges over the entire type hierarchy you can give it a value a function a type variable anything and the identity combinator will work it will just give you back what ever you asked what ever you gave it but the twice requires an argument which has a type of a certain form that it should be explicitly a function form it can not be a value form not only that it should explicitly be a function form it should it can not be a function form of type t one arrow t two where t one and t two are completely different it has to have a type t one arrow t one so all for all instantiations of t one such that the types such that you have functions from t one arrow t one the twice can be applied that is the type of twice right so so that s what is obvious from its from the definition of the function abstraction in twice right so so now what we have is in addition to the monotypes we have the type abstraction on on lambda abstarctions on lambda terms right so this is the standard is the type abstraction which i pointed out and this is actually an application of a monotype of a of a lambda abstraction of presumably a lambda term which takes a type as a parameter so if this lambda term were the combinator i were the polymorphic combinator i you could give it any monotype as an argument and it will specialize to that particular i tou okay in the case of i of course this tou could be anything anything in the monotypes in the case of twice this tou would have to be of the form sigma arrow sigma where sigma arrow sigma is constructed constructable form the base types through the monotype context free grammar right so so this is actually a type application this is actually an application of lambda term to a monotype so as to specialise that combinator for that particular type yeah and then well for completeness since this was this was there in ravi shetty s book i decided to add this construct also  refer slide time 39  08  so here is the let expression where so but these are all monotypes where ever i have written tou it s a monotype where ever i have that means it s a restricted part of the type grammar that we have got i mean its they are all simple tou s are monotypes means that it comes from the language simply typed of the simply typed lambda calculus with the language of type types in the simply typed lambda calculus but with with the added construction that you could have variables instead of actual expressions built up of type constants okay but now we can take a polytype okay so if x is supposed to be a polytype then this is this whole let construct is a lambda application in which all free occurences of x in this will be replaced by this i should type check so will look at it we we will look at this constructor little more carefully later but essentially the most important additions are really that are really this that new type application in the type abstraction this type application means that there is a beta reduction for types okay and this is the construction of more complex functions which are polymorphic from the polytypes themselves yeah so this is really this is actually a form of lambda lambda application as you can as you will be able to see right when we when i give the rules it should become clear for the moment let go off this or rather lets keep this because i am going to give an example which illustrates this right so it s a very nice example again drawn from ravi shetty s book so lets take twice so i have this expression which is really let twice which is defined in this fashion okay twices of type this its polymorphic as you can see because it has the universal quantifier over type variables so its not a monotype so consider the polymorphic function twice whose definition is given by this okay in the expression twice int successor successor is a standard successor function written in the lambda calculus we will we will assuming applying it on integers lets say  refer slide time 40  31  so for any x the succesor of x is as defined by pioneer arithmetic right so now what we are saying here is apply first twice on to integers that means particular is twice to integer functions that means functions from integers to integers so what you get when you apply twice to integers is a new function which is particularized to all functions of the type of the monotype integer to integer apply that function on to the successor okay the result of which has to be applying successor twice on to what ever is the argument and if you apply and the result of so the result of this application is to particularize twice to the type int arrow int and having particularized twice to int arrow int you apply it now to successor which is which is a function from int arrow int so its perfectly understandable right so its type compatible okay since successor is a is a function is a since successor is lambda x x prime where x prime is going to be int and therefore this lambda abstraction gives successor of the type int arrow int so this applied to int gives you a function twice subscript int arrow int which applied to this function successor which is of type int arrow int so therefore its applicable i am sorry twice applied to int gives me a function int arrow int arrow int arrow int which applied to a function successor which is of type int arrow int gives me a result which is a function of type int arrow int which given the argument zero gives me a value in int so essentially it will give the value two which seems how ever big mess to get into just in order to get the value two but in principle it s it s a powerful operation so i have i have just showed how the beta reduction works for particular as in so the beta reduction for types for polymorphic types is to really particularize that function for a particular for a certain type right or instantiate the universal quantifier in the type to a particular kind so this twice applied to int is actually what i have written in in if if you if i follow the notation that i used before its actually twice particularized to int so its really i would have given the subscript int arrow int arrow int arrow int right so it this twice  refer slide time 44  04  works only for works only on functions of the form int arrow int of course there is possibilty that i could have applied twice on a type variable too and i could have put that variable may have may be quantified some where later okay if if i can have nested quantifers right for all s for all t and so on and so forth then that variable might get its value might get its type value because of the instantiation of a quantifier that exists some where in the outer scope right and then so then if for so and if i look at this restricted subterm then it would be twice with the subscipt t arrow t arrow t arrow t where t should get its value instantiated some how later right so now we can actually and since beta reduction is so the so so now understand why beta reduction is really important checking whether membership in a set is really a form of beta reduction applying functions is really a form of beta reduction universal instantiation is really a form of beta reduction and constructing sets by abstraction is really a form of lambda abstraction constructing quantified predicates is really a form of lambda abstraction constructing types is really a form of lambda abstraction so applying types and instantiating them is also form of beta reduction so beta reduction in computation is really the most fundamental concept which which is probably evolved over the last forty years almost anything that is constructive by constructive anything that is computationally relevant has beta reduction appearing in some form or the other parameter passing and procedures is a form of beta reduction whether the parameters are passed by value or by reference or by name they are all forms of beta reduction and now types are also forms of beta reduction so the generics that you have in c plus plus an aider are really some very restricted form of beta reduction applied only to some base types and the so far in in terms of implemented programming languages the highest form of the most sophisticated type system that is so far come up is the m l polymorphism which is completely statically determinable where types are completely polymorphic types are statically determinable which means they are determinable at translation time without going into executions yeah so that s one of the reasons for studying m l because its it s a functional its not just that it s a functional language but also that it has a very sophisticated type system the most sophisticated type system in existence in an implemented programming languages is in m l  refer slide time 47  38  so the type inferncing rules now actually by now you might have you would have got a flavour of type inferencing rules that you should have and these are essentially it so given a variable so again given a type context grammar or a type environment grammar the type of a variable x is what ever this x is free right so if so what ever if if the context does not x then it does not type check as simple as that and you throw out that program but so what ever the context we give for x is the type of x and this is the usual application of functions if l is of type sigma arrow tou which are by the way sigma and tou are monotypes this is this is the monomorphic application and m is of type sigma which is again a monotype then l applied to m is of type tou if with the assumption that x is of type sigma added to the context you can infer that l is of type tou then the lambda abstraction x sigma l has the type sigma arrow tou where again all this sigma s and tou s are monotypes right okay so that is as so these the type inferencing rules here are really like the type inferencing rules as the simply typed lambda calculus and there is no difference but what we require now are type inferencing for the polymorphic lambda calculus right so here here goes so if it is determinable that l has that l has the polymorphic type for all t pi where pi could itself be another polymorphic type because you could have quantifiers you could have a sequence of quantifiers right in nested quantifiers so pi could be a polymorphic type then given tou a monotype you can not apply l to another polymorphic type but you can apply it to a monotype that is applying l to a monotype means particularizing l to a certain type applying l to a polymorphic type does not exist in our language so far okay but i mean as i said there is no reason to build up the type hierarchy i mean you have quantifiers over polymorphic type variables too i mean then you would have a particular reason that is actually a reason for doing that that s because its not many of those things are when you go from to types higher than this it turns out that lots of problems about of undesirability crop up this is about the limit that we are currently we have currently reached  refer slide time 54  15  so given the tou is a monotype you are particularizing there are polymorphic lambda expression l so that an incarnation of it for the type tou is created by this application and if an incarnation and so that means what this universal quantifier for all t has to be instantiated has to be eliminated by instantiating t with the value tou and that s what this substitution does so it takes pi and for all free occurences of t in pi it replaces those free occurences by the monotype tou okay so this is the case this is clearly a case of universal instantiation its also a form of beta reduction for the types yeah so this is a type application or an instantiation where pi is a polytype and tou is a monotype right and then of course and so this is like universal instantiation so there should be a corresponding universal generalisation or quantifier introduction rule and that is what this does given that in the context grammar you can show that l is of type a polytype pi then this abstraction over the types over the free variables t over the free type variables t in pi gives you a polymorphic lambda expression which has a type for all t pi so this is type abstraction very its very similar to the lambda abstraction yeah the only thing of course is that when ever we are talking about free variables whether its type variables or value variables and when ever when ever we are talking about binding them we should ensure that there is no capture of free variables because of your because of the binding you are creating because if the introduction of the quantifier you are creating a you are binding this variable t and as a result no t in pi which actually they should have there should not be any t in the context grammar okay already defined because that t if that t occurred in pi then that t would get captured by this quantifier okay so the usual confusion of free variables bound variables alpha conversion with quantifiers and so on i mean i mean of course alpha conversion also exists anywhere where there is anywhere where there is binding or declaration if that is alpha conversion right and so you have to do aplha conversion so a bound variable to ensure that there are no free variable captures right so t should not be free in grammar that means t should not already have been declared in grammar and then the last role is just really a form of typed lambda application for the polymorphic case if l is of type pi one which is a polymorphic type and with the assumption that x is of type pi one if you can infer that m is of type pi two then this let expression this whole let expression really has the type of m because m is really the expression and the meaning of any let expression is the body of expression which is which in this case is m so it has a type pi two so let expressions really are like applications because what you can its the semantics of a let expression is equivalent to substituting all free occurences of x in m by l yeah so its really a form of so now you can look at you can go back and try to determine what the type of this is right and by now so i mean you have really reached the highest levels that types can reach in a desirable fashion programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 34 type checking  53  11  welcome to lecture thirty four so today we will do some backward integration we have already done the hardest parts of type checking so we will just try to integrate it with what ever we already know about programming with our simple functional and imperative programming languages so always keeping in mind that it is possible often to do some static type checking that is at compile time right so when you come to general programming languages the question of what is type checking in the case of lambda calculus of course our type checking was entirely governed by the fact that all lambda functions all all lambda abstractions are unary functions and so therefore you had to do the type checking essentially only for unary functions and since all other kinds of functions were sort of carried forms in the lambda calculus certain issues were overlooked so essentially in in its most general form in an applied lambda calculus or in a programming language type checking is just the fact that you have to check that each operation in the program receives the appropriate number of arguments of the appropriate types and in the appropriate order so the  refer slide time 02  12  appropriate number of arguments was automatically taken care of in the lambda calculus that the fact that it took arguments one at a time and each as and when an argument was obtained it would have to type check of appropriate types was automatically taken care of in the rules for type checking in the lambda calculus in the appropriate order was also automatically taken care of how ever in any applied lambda calculus where there could be nary functions nary operations what it means is that you will have to check all these things so for example if if the operation is not commutative or it is over a multi sorted structure then it all the arguments should come at appropriate times and should come in the appropriate order and they should be all of appropriate types right and the question is of course why do you require type checking and probably the most direct answer is to answer the question what actually happens without type checking so we already have an idea of what happens in the lambda calculus in the untyped lambda calculus what happens there is that well any any lambda term might be applied to any any other lambda term and the result is actually a lambda term but you may not always be able to interpret it right so that s and essentially the same thing happens in hardware so what ever may be what ever may be the operation and what ever may be the arguments the hardware the underlying hardware which is totally untyped of course the difference with the lambda calculus is that the hardware is of course all of untyped data bit strings if you like the hardware always produces some result when types are not respect okay and so so for example you could do integer addition on two arguments  refer slide time 04  32  one of them representing a character and another representing lets say a set and what what actually happens is since the hardware is untyped rather memory locations are untyped registers are untyped they are just bit strings so what can happen is that you will get something and but the problem is that you wont be able to interpret the result and infact this and that the result of one type violation might actually be carried forward as an argument for another operation and so on and so forth it will continue to produce more and more meaningless results till the entire computation actually fails and of course this fails is a judgmental word i mean the hardware doesn t know anything is failed and question of whether its failed or not really depends upon your interpretation right so it will produce something and it will be very hard for any for the programmer to actually detect what exactly went wrong right okay so so this is so this is so this the question of failure is not something that might be immediately detectable too i mean its something that might come that might come up years and years after the software has been sold for a phenomenal price to some unsuspecting  refer slide time 06  03  customer who actually believed that it worked till something happened yeah so and so which means that so while its true that you can not get rid of all programming errors in any fail in any full proof fashion ever what you can do at most is to introduce checks and one important check where four is type checking yeah so so and you since the underlying hardware is untyped its it s a good idea to introduce type checking among other things so type checking actually is a much more general term for example i would when you look at array bounds checking in programming languages that s also a part of type checking because the index set for the array is a sub range type and therefore you have to do type checking so type checking is a fairly general word to catch a whole lot of issues not not necessarily all right and another unfortunate thing is that type errors are very common even among experienced programmers and which means that early detection if you can detect it early then you can save a great deal of time and effort in wasted execution and in debugging right so so the so even though even though what it means is that there is going to be a greater overhead on the compilation it is often better to detect it early so that the production runs are not compromised yeah okay so  refer slide time 08  03  and otherwise what happens is the entire burden of of detecting type errors or type violations and type violations include things like array bounds checks and so on and so forth is is entirely on the programmer during its debugging phase right so early detection implies that what we would like to do is actually detected at compile time and this is called static type checking so static type checking means that you do the detection of part violations as far as possible during compilation or translation time yeah and so what this means is that compilation is slowed down but it also puts in other things i mean for the for the typed lambda calculi you can do the type checking during translation time always but its not in general possible for for other for all features of programming languages for example it is only possible where you have statically created data so that means through the mechanism of explicit declarations declarations either before or after used but declarations which clearly create places for data so dynamically created data can not be for example statically type checked so any kinds of pointer mechanism list mechanisms whose the data of which changes during run time can not requires run time type checking dynamic run time dynamic type checking so but for for example for most pascal data structures which are static i mean they have explicit declarations which give the full length which give the full size of each piece of data so if you leave out the pointer mechanisms every other piece of declaration in pascal really gives you you can compute what ever is stored in the activation stack is is something that is that can be statically type checked what ever is going to be stored in the heap is something that will have to be dynamically type checked right so an early an early type checking strategy means that as far as possible what ever declarations are there and which can be type checked you do the type checking at compile time to say one execution time for dynamic data structures of course you require dynamic or run time type checking and this is something for example pascal actually has in the run time descriptor therefore heaped it up so and of course all this implies that you require what ever you can do you can do static type checking only for statically created data and it is possible only where under where bindings are static where bindings are determined at compile time  refer slide time 11  25  if there are bindings which are determined at run time then for example in languages like lisp and apl and snobol then what happens is that you cant do type checking at translation time right okay and so normally what happens is that all this type information is really what the information is is really the information that is stored in the symbol table during the process of translation yeah and so so type information is usually some sort of attribute which is part of this symbol which is part of the compilation process and and that s how it is implemented right and so on the other hand if you look at dynamic type checking its really something that has to be performed for each operation just before it is executed as i said i mean from the from the point of view of the hardware and operation can always be executed and it will always give you some result but if you want to ensure the type violations do not occur then what it means is that you will have to perform this type checking before each operation is actually executed so there is has to be code generated for checking so what it means just that before you execute an operation just check that there are actually the number of arguments that are required for that operation available each argument is of the right type and they are available in the right that that s really all that means but the point is that a program is full of little little little operations and what it means is that before each before each during execution you will have to do this checking before executing each operation and that can slow down executions considered so this is also one so but how ever what it does is it allows a sort of flexibility in language design  refer slide time 13  39  you know it doesn t force you to do early binding static binding it allows for example well declarations did not i mean you it frees the kind of flexibility is also means possible abuse right i mean that s that s an important thing to realize any kind of flexibility could also imply potential abuse and so which means that no declarations may be required in some of these languages which allow for a flexible typing mechanism whose type checking is done only at run time and then what more importantly what can happen is that typed variable the types of variables may may never remain static throughout the scope i mean the actually the types of variables may change during execution according to some need according to some perceived need but this this has a price i mean there is a price to pay for these things and what it means is that you cant take a print out of the program and expect to try to debug it to see what s wrong with it what it means is that if you want if such programs fail for some reason or give you results unexpected results what it means is that you will have to sit constantly with the debugger and try to determine what exactly is wrong with the program right so so especially the type variables changing during execution can be very confusing for very large software yeah and even for software which is just as large as compiler it can be very confusing and debugging can be very hard right so and then of course the other kind of flexibility abuse that it allows is that it frees the programmer from typing concerns but it makes debugging extremely difficult  refer slide time 15  36  and then what it the extra overhead that it adds when you have dynamic type checking is that which if you have to if you of course you could take the attitude that i don t require any type checking its too slow forget about it no type checking but that means that there could be strange type operations a strange type violations that might occur and they might all actually produce results which on the surface look meaningful okay but if you do want to do some type detection type violation detection then what it means is that if you decide to delay it to run time then what it means is that you will have to also tag every object with its with some type information to enable this kind of type checking attractor so one thing of course is it means us it often means a certain amount of increase in space which may or may not be which may or may not be negligible but what it more often does is that it actually slows down execution right so most functional languages most languages do some form of type checking because even if when you look at even even when you look at them as just forms of the lambda calculus of the untyped lambda calculus still they use operations of the underlying hardware and there are representational differences and so on and so forth so they do they do some type checking in the applied in in the underlying domain that to which they are applied they do some elementary type checking so but when you do these things dynamically what it means is that you you are going to considerably slow down execution yeah so that s an extra overhead right so now what we so so type checking really is something that is not an absolute essential for programming if you like but it s a desirable thing to have in your translator or your run time system the attitude towards types is actually varied tremendously from from the early programming languages but more and more the feeling has come that really type checking is something that should be performed at some time before execution which means either static or dynamic mainly to detect errors in large software right so it so the point so so what has happened historically is that the early languages like fortran had had really no type except boolean except integers and reals but more and more typing information has been gathered so for example with pascal you get a fairly rigid form of static type checking with with certain amount of dynamic type checking to ensure that array accesses are not violated array boundaries are not violated and to ensure that heap data is when ever is created and destroyed is of the appropriate type right in c for example there is an automatic possibility of casting one type into another which which is a type cohesion which is a form of type cohesion which is quite acceptable in many cases you explicitly want to co head certain types but if you look at c plus plus again the rigidity of a pascal typing system has again come which means that they have realized some of that an early detection of type errors is some how important to reduce debugging time to reduce to reduce overheads of run time type checking so a large amount of c plus plus type checking is actually static how ever because c plus plus is a super set of c any type violation and every c program should be compilable and executable in a c plus plus environment i mean that s that s the basic principle so which means that the various kinds of abuse that you can do in c are also possible in c plus plus without any problems so type abuse that you can do in c is also possible in c plus plus but if you were to restrict your c plus plus usage to just the standard libraries that are available with at with the c plus plus and just the c plus plus constructs that are new then what you will find is that there is a tremendous amount of static type checking one of the reasons why compilation in c plus plus is considerably slower than that in c they actually do a tremendous amount of static type checking to ensure some how that type violations do not occur right but the but the overhead typing overhead that c plus plus carries with it is the fact that its got a very type flexible language sub language which is open to enormous amount of abuse right okay so so lets lets carry forward our type inferencing mechanisms or rather carry backward type inferencing mechanisms because we have done the hardest part of type checking i mean we know how to type check higher order functions what we don t know is how to check type check the underlined data so let me just finish that  refer slide time 23  36  so lets assume lets assume some elementary base types for the present i will i will just take the while programming languages to give you a flavor of how type checking is can be performed can be defined in a structural inductive fashion so that what what it means is that you can actually do early type checking in many cases especially for a statically scoped language okay a statically scoped language means that with the binding is early the bindings are also static and so type checking can also be done static memory allocation can be done statically for all except the dynamic data structures so so the type checking can also be done statically with relocatable addresses what ever so so we will just assume that our collection of types which is actually if you go back to the lambda calculus what it means is that we are talking about the collection of base types so its just integers and booleans keeps it simple and right and then we will use some symbol conventions i mean i i am changing one thing i am using i am using this dark brown t to denote one of the boolean values true or false and t is of type bool right then of course i am using m and n as integer values its also even though i claim that the underlying hardware is largely untyped very many architectures came up which during the seventies and eighties which actually did a tagging an automatic hardware tagging of memory locations to do elementary type checking in a in a speedy fashion so which means what it means is that they would actually tag the memory locations to be either floating point or integer or boolean or character we just use these four basic tags and everything else all other type checking especially for example of higher order functions and so on was the compilers responsibility so they did use some form of either tagging or a separation of the elementary data types some how to to enable type checking to be performed right to enable at least basic type checking to be performed not very sophisticated type checking right so and of course we will have things like integer variables may be we will also have boolean variables what ever  refer slide time 24  52  right and we will for the for the moment we will just assume that the there is some way of determining types for the most basic constructs in the language so which means variables i am not actually explicitly introducing declarations here but ideally what you would do is you would introduce declarations and that would be the explicit way of determining the types of variables right at compile time and using them some how to do the type checking for the rest of the program but but since since i am since that is the trivial thing which i am not too bothered about now i just wanted to give a structurally inductive definition of type checking without without worrying too much how the base types are determined as they could be determined in several ways right so and then we will so what i have what i am doing the way i am changing the language now is that i have i previously defined two expression languages an expression an integer expression language and a boolean expression language and then a language of commands infact what i did was i did the type separation in the syntax so what i will do now is i will mix up the syntax of expressions one cant mix up the syntax of expressions with commands but one can mix up the syntax of boolean and integer expressions and see how type checking can be done  refer slide time 26  44  so we will assume that the that these boolean operators are i mean that these binary operators are available and i will go ahead and also look for equality as being either between integers or booleans these these operations are integer operations this is the only other boolean binary operation its enough to have one representative of each kind actually so its otherwise more than anything else it becomes boring to and repetitive so so we will just assume that this is a sweet of operations and we will define the language and will define what is known as a static semantics yeah right so so our expression language is some how some how consists of all these expressions so essentially t is the truth value m is an integer may be so some how lets assume that we can distinguish between t and m i am not really bothered how x is a variable it could be a boolean variable or an integer variable then this is a binary operation on expressions it could be any of the binary operations we have seen there is a unary operation the only unary operation in expressions the commands remain as they are it is nothing much to the only thing that i have done now is from the previous incarnation of the while language is that i have collapsed the boolean expression language and the arithmetic expression language into a single language which is one set of production rules to enable i mean to partly to partly to ensure that we can actually do the type checking that s that s about it its not necessary to separate them very early in the at the level of syntax possible to do the type checking and separate out what is well typed from what is not well typed yeah as far as commands are concerned expressions denote values so they have a type so depending on what value it s supposed to be written it will have a type whereas when you are looking at commands commands just state change state okay so commands really do not have a type except that we should ensure that the underlying expressions are well typed okay and we should ensure for example here since there is no separation between boolean and arithmetic expressions you should ensure that you you only have the boolean expression here right and similarly the while do right so the point is so commands themselves will not have a type we will just assume that there is a unary predicate of well formed ness which allows us to  refer slide time 29  51  check whether a command is been properly formed expressions of the appropriate type an of course a command at the top does not type check or rather its not well typed i mean this is a standard thing is to say when when something is well typed it is you say that it is it type checks okay a command does not type check provided if any of these arms do not type check or if this is for example not a boolean expression that says this might type check to an integer expression in which case of course this the command is not well formed so so these are the elementary considerations and which which we will use for type checking right so so and the and principally we have to some how be able to extract information about when an expression is a boolean expression and when it is a an integer expression so that s fine now we will assume that some how the base types are some either representation distinction or actually may be through some declarations some where there is some there is some how some way of distinguishing the base types which may not be entirely true on a bear machine yeah right so how does one type check expressions lets lets take the unary operation if e is a boolean expression so which what it means is that those where those type axioms like m is of type int and t is of type bool and so on and so forth already available  refer slide time 31  38  and if e is of type bool then nought e must be of type bool i mean the point is that this can be done in a recursive descent fashion right and after the kind of complicated things we have done all this looks actually really trivial except that it seems more and more essential in programming languages to do this kind of type checking as early as possible yeah even though its appears to be totally trivial  refer slide time 32  26  so for any binary operation what i am going to have is that if e one is of type tou one and e two is of type tou two then e one binary operation e two is of type tou three except that now i have to worry about what tou one tou two and tou three are i have circled tou one in brown tou two in sky blue and tou three in red okay and now this rule is subject to these tables  refer slide time 32  39  which actually give you the type rules right that s really so if you take the arithmetic operations plus minus and star if tou one is so what happens is an arithmetic operation type checks so tou three has to be integer only if each of e one and e two is of type integer so here is where the type checks in all other cases there is a type error right so this is this is this is the only non error type other ways there is always some type error and and this is compile time detect time right so if you look at a quality of course either they should either you are comparing two booleans or you are comparing two integers in all other cases there is a type error yeah and in the case of or its only when the two operations are boolean that you actually type check in all other cases you have a right or has meant to be a boolean operation so that s all so the this is all there is to type check it once you have dealt with polymorphism and so on and so forth the hardest parts of type checking are really type checking higher order functions for which you require the kind of heavy inference rules that we had for the typed lambda calculus  student  noise  yeah the yeah the the red entries are tou three  student  quality of integer and integer gives boolean  i am sorry boolean sorry this has to be a boolean right i made a mistake there right so this has to be a boolean and and otherwise it s actually extremely trivial the really hard part is type checking higher order functions and once you have declarations available to you you carry the carry forward the type information in a static environment and you perform the type checking just like you had a dynamic environment in which you did the execution in which you wrote rules for the execution so you can look at p l zero compiler it essentially does similar forms of elementary type checking and in the case of commands all that we require is that we just have to go down deep into the command that s it right and in a recursive descent fashion we should just ensure that the commands are well formed right so i would say that command c is well formed i will use the predicate w f c to indicate that a command type checks if it satisfies these rules i mean so the basic thing is that the assignment should be between things of compatible types okay now in most programming languages you have type cohesion mechanisms that means casting mechanisms for so that you can either explicitly or implicitly cohere lets say an integer value to be real value okay or a or truncate a real value in order to give you an integer value  refer slide time 36  34  what ever so in in many programming languages including pascal actually the integer to real conversion is is implicit i mean if if any if the left hand side is real variable and there are integers on the right hand side then there is a implicit function which converts the integers to reals and then does the assignment in in languages like m l they have actually made it explicit right which is which is actually well good idea i mean in the sense that if its explicit then you know that the programmer actually knows what he is doing whereas if it is implicit it is not clear whether the programmer is aware of what he is doing and extreme case is in the case of fortran where you do the division you have the same symbol for division and the same symbol is used for integer division and real division and its not clear when the programmer will make a mistake when we will get a quotient which is a real number when is when and especially with variables not necessarily being declared with symbolic conventions and so on in fortran it is a complete its actually a complete mess in terms of readability whereas the most later languages actually allow for some form of implicit or explicit type cohesion but then you know the type cohesion function lets assume its explicit is really a function from lets say integers to reals or reals to integers truncation is a function from reals to integers and given an argument which is a real truncation and then by the standard application rules of the lambda calculus you get an answer which is an integer and you can write out write out those rules for both implicit and explicit type cohesions right and in the case of other commands you just follow the normal forms of structural induction  refer slide time 39  16  and if these two commands individually type check then their sequencing also type checks and the if then else and the while just require this kind of checking their individual bodies have to type check and it should be ensured that the expression is a boolean then the individual commands also type check right so so type checking rules are usually quite trivial and its because they are so trivial its because they can be done so simply that you always recommend early preferably static type checking that will that will that will isolate errors early in the program its possible to do it most of the time except for dynamically created data if you have a language which in which you insist some declarations before use you can do a large amount of the type checking can be done at compile time and errors can be pointed out right and so it says on execution time it saves on having large run time descriptors saves on generating code to do the type checking at run time and it saves on execution time so more and more languages actually try to do type checking early so the most recent languages actually do all the type checking and also that s also the reason type checking is one important reason why many languages have moved from dynamic binding mechanisms to static binding mechanisms because then you can do the type checking at compile time at translation time you don t need to wait for run time type checking yeah so the introduction of declarations brings in a certain amount of complication so lets just assume here now i will just get rid of commands and lets look at a purely functional fragment an m l like function fragment so then what it means is that you will you will some how have to make a distinction between what is being declared now and what was free what what could be re declared now for example in a fresh declaration so a typical typed declaration of lets say a value could be like this lets assume that the other expressions in the language are the same there is this one extra construct a let construct right and then of course we have the normal declaration mechanisms right  refer slide time 42  03  the sequencing of declarations or rather if you like the pipelining of declarations the parallel evaluation of declaration or rather independence of declarations and nesting of declarations right so now the only the only sort of complication it raises is to actually know what are the free variables in a expression what are the bound variables the moment you introduce declarations that means you have a concept of free variables and bound variables you have you have a concept of a variable being re declared and therefore i mean and in particular re declare to be of a different type therefore creating a whole in the scope and therefore it has to be taken into account during the type checking process right so so with with the language with declarations like this then what it means is that you have to give some structurally inductive definitions for what exactly is being declared and what exactly is free right so so lets look at free and defined variables in a functional language some how they don t use the word declaration they say its a definition so i will so i will use these two terms interchangeably in a in an imperative language its called a declaration okay so the declared variable in an in an elementary value expression of this form is just x the free variables in this definitions or all the free variables of e yeah so x itself could occur in e but we know from our run time semantics that that x which might occur in e refers to an x that was previously declared and this is the re declaration of that x is a re declaration of x so so e may or may not contain x but what ever x it contains its not the same x as this okay which is being freshly declared right so and in the case of sequencing of declarations the declared variables are just the unions of the declared variables and free variables of d one semicolon d two are just the free variables of d one union of course you could use the variables that were declared in d one in order to do in order to have the declarations in d two right i mean in expressions within the declarations in d two so which means you just exclude those variables which were declared in d one from the free variables of d two yeah so and then the in d one and d two there is the condition that there is a condition of disjointness right that the same variable can not be declared in both d one and d two and can not be used so the declared variables are just or the defined variables are just the unions of the individual defined variables in the declarations and the free variables are also just the union  refer slide time 45  48  and in the case of and in the case of nested declarations what you have is that in a nested declaration of the form d one within d two what you are actually declaring is only what ever is in d two if you remember i said that you since the declarations in d two could be very complex you abstract out some of the sub expressions give them new names in d one and use them but essentially at the end of this declaration only the environment created by d two is available so the declared variables of this declaration are just the variables of d two and the free variables here of course are all the free variables of d one union all the free variables of d two which are not free in d one yeah right finally when the all this of course is meant to define the notion of free variables in the expression language so for all other expressions the the notion of free variables remains unchanged and what i mean by all other expressions i mean expressions of the form e one binary operation e two are not e so on and so forth and in the case of  refer slide time 47  27  a let construct what you have is just that the free variables of this are just the free variables of e excluding all the all the free variables of e which have been declared in d and including all the free variables that are already in d yeah  student  sir d one within d two d minus declared variable d one  yeah yes yes yes yes i was i was wondering what was wrong with it yes right yeah so and now essentially we require just this much and this notion of free and declared variables is what is going to be used as in  refer slide time 48  43  processing declarations as a context okay so if you remember about what i said about contexts a context is just a collection of variable to type bindings okay so given given a program for which you want to do static type checking all the free variables in the program should have types available in the context otherwise you can not do type checking of the program inductively speaking of course a complete program does not have any free variables but inductively speaking if you go through a program segment within a larger program all the free variables lets say this expression e is lets say an expression is a particular program segment in a functional programming language all the free variables in that expression should have bindings in the context and the bindings defined by that context are exactly what you require in order to do the type checking for these expressions okay i have already pointed out that this the notion of context is very similar to the notion of a run time environment okay i mean you use the same updation you use the fact that you have to look up the run time environment each time in order to find values during run time because of static in the case of a context you have to look at the static you have to look at the context in order to determine types you cant determine values necessarily because you are not running the program yet but you can determine types okay and it has exactly the same structure and you can reproduce using the notion of what ever is free and what ever is declared rules for type checking statically which are very similar to well rules for type checking in the lambda calculus and which are which are very similar in some sense to the run time environment mechanisms which we used so this whole type checking is since its going to be done statically i mean what ever we have done is supposed to be static so this whole kind of semantics which deals not with values but it only types is called the static semantics for a programming language which is something i have delayed a lot in coming to but essentially if you were to look at in any modern treatment of a of programming languages there is syntax pragmatics static semantics and dynamic semantics so what ever we have done using the environments row and so on represents the dynamic or run time environment and all that semantics is the dynamic semantics and what ever and a similar semantics that we define for type checking is static semantics so a complete language document a language document is really complete only if it defines the syntax the static semantics and the dynamic semantics completely right so i will stop here and next time i will start on procedures programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 35 contexts  53  49  welcome to lecture thirty five so contrary to what i said last time it wont be actually possible to start on abstraction today so i will complete what ever i had to do on type checking in the in the context of contexts and type inferencing and type checking that with contexts something that we had not specified last time we have just assumed some how that information was available in some form especially for example especially for language of for the while language the implicit assumption is that just as in the run time case we assume that there was a there was a notion of a state we are never starting with an undefined state and that the notion of state that we assumed there in the run time system was that there was a variable to value mapping available for every variable okay similarly in the case of this in the case of type checking are implicit assumption and since there are no declarations in the while language as  refer slide time 1  41  we have defined it the implicit assumption is that there is there is a variable to type binding implicitly available at the start and given that that is always there and of course since there are no declarations we were forced to assume that there is a infinite collection of variables all of them had some value assigned to them in the run time environment and similarly now we will assume in the case of while that we have an infinite collection of variables and they all have some type associated with them may be one of the base types yeah so with with those assumptions in mind we actually gave this simple rules for type checking and one reason for giving such type checking rules  refer slide time 2  33  is that they are very elementary and lets dispose off expressions and get on to context get on to contexts so that we get to know how exactly type inferencing is done how type inferencing and type checking are related so  refer slide time 3  37  so here was the lets quickly run through this so we assume some how that a context or in type information is available for the basic elements in the language namely variables and if there are constants there are constants too and we had these basic type checking rules so that if you can some how infer that e one is of type tou one then and e two is of type tou two then for any binary operation e one binary operation e two is of type tou three where tou one tou two and tou three are listed through these tables right and as far as commands were concerned commands do not have type how ever commands are made up of expressions so we have to type commands in a way which which guarantees syntactically that a command is some how is well well formed out of the right kinds of expressions so with commands since they are just state transformers we just assume a predicate called well formed  refer slide time 4  18  and we will so so the type checking rules for the assignment command a that you should you should assign to a command only a value expression of the same type to assign you should assign to a variable only a value expression of the same type and that you recursively go through the commands based on the structure of the command and you type check complex commands in terms of the type checking rules for the simpler commands so if any of these components does not type check is not well formed for some reason then the whole command is ill formed right the whole then the whole command does not type check and so similarly for the for the rest of the complex commands this is just one extra type checking rule we require is that you actually have boolean conditions and not arbitary expressions okay so now lets actually look at it in the in the context of a functional programming language with declarations i don t want to mix up i don t want to make the problem too complex where also having an imperative command structure essentially this is to illustrate the use of contexts where you do not have an infinite collection of variables you have only finite collections of variables available at any instant and there are declarations which specify the types in which case how type inferencing is done so if you were to extend this language to include a language of commands essentially the type checking or type inferencing for the declarations and commands and expressions would produce would proceed in the same way in addition commands would have to be well formed by appropriate rules right so so we will not worry about commands so we will just look at a simple functional language which is usually which is like m l if you like follows a similar syntax so we have this declarations  refer slide time 6  24  you can hav a declaration of this form in m l it would be val x equals something in m l also you don t always need to give the type tou okay that s because m l is not a type checking system its more than a type checking system it s a type inferencing system so if the type of e is is clear then the then what m l will do is it will automatically assign that s that type to this variable variable x so will come to that distinction later so right now we will assume the declarations are all well typed so which means we are actually imposing pascal like declaration discipline in the language for the present for the present moment yeah and then you can of course compose declarations in the usual fashion and just for variety i have added an extra form of an expression an m l m l type expression the if then else which is an important form from the if then else its also easy to derive options where you have options on the structure options on pattern matching and so on but that s that need not address too much so lets look at this so we have this expression with a declaration in it and what holds for this expression with the declaration in it also holds in case you extend this to commands for commands with declarations in them right so so pascal like declarations are very easily taken care of and then of course you have this literal constants right in the case of declarations the moment you bring in declarations you are bringing in bound variables or free variables the notion of free variables becomes important and we have this definition of free and defined variables right so this is in a so the the base case is something of this form the only defined variable here is x and every variable that s free in e is free in this entire declaration and when you compose declarations you get you get similar structurally defined sets of defined variables and free variables its important to realize that yeah  noise  right so the question is whether x can occur in e in the first declaration now the way declarations work in m l is that the x that occurs in e is a different x previously declared in some out of scope from the x that is now being redeclared okay that x is still free so its still a free variable of this declaration okay so if you took all x s in e lets assume you give them a different colour the free variable x in the free variable set it will be of that colour whereas in the defined variable set it will be of this colour right i mean that s so this natural static scoping rules apply and so any occurrence of any free occurrence of x in this refers to an x declared in some outer scope right and it has its it derives its type also just as it derives its value from that from that outer scope it also derives its type from that outer scope where the declaration for x occurs right so and d one semicolon d two is similarly has all these defined variables  refer slide time 10  32  the free variables of d one semicolon d two are that well anything that s free in d one is free in d one semicolon d two and what ever is declared in d one could be could be used in d two so anything that s free in d two except that those that have been declared in d one this union constitutes the free variables of this declaration similar things apply here for d one and d two right and of course there is a disjointness condition for d one and d two the same name can not be declared in both d one and d two that disjointness condition is made clear in a rules of inference okay and then in the case of the nested declarations the nested declaration is really a declaration really refers to the declaration d two which might be very complex so you might require additional names to abstract out some of the sub expresions and give them new names so you have the declaration d one to aid essentially what is required is the declaration d two so the only the only defined variables in this declaration are those that are defined in d two and the free variables in this are all those variables that are free in d one and as usual the variables that are declared in d one might be used excuse me in the declaration d two so the free variables of d two excepting those that have been declared in d one this union actually constitutes the free variables of this declaration so far all other expressions that we have already seen before the notion of free variables remains unchanged and in the case of this let expression we need to specify free variables and declare a defined variables in declarations in order to enable a definition for the let construct essentially so all this machinaries precisely for this so the free variables in this are exactly the free variables in e of course the free variables in e could include some variables that have been declared in d so remove those because they get bound union the declaration d involves expressions which contain variables previously declared in a scope in the scope in which this let construct occurs or in some outer scope in which this let construct occurs  refer slide time 13  28  so the free variables of d are also free in this expression right and here of course its very trivial its just the union of all the free variables that occur in each of these expressions right so a type environment or a context so now we are going away from infinite collections of variables since we have declarations we can we can restrict ourselves to actually the variables or the names or the identifiers that you actually have in your program you don t need to assume an assignment of types or of values to the infinite collections of identifiers that you have so the we have the notion of a type environment or what is called what we have called in the lambda calculus a context right so a type environment or a context over some finite collection v of variables is this would be now a finite collection is just a variable to types binding yeah this types this this this types is meant to denote what ever are the types generated through some context free grammar in the language of types yeah so assume so so for all identifiers by the way when we say variables they could be i mean they are not really value variables they could be they could be identifiers which denotes functions right a higher order functions but for the present we will just restrict ourselves to this what ever what ever types means yeah so in the type environment over a given set of variables is just a set of all possible contexts that you can define over the set of no at the present moment i am not considering type variables yeah so there are no type variables there are only value variables but we have a set of types generated by some context free grammar yeah infact for most of the examples here this this this word types will denote only the base types i don t want to go into the into polymorphic types type variables are really required when you are dealing with polymorphism but we have already dealt with polymorphism and we are doing a backward integration into lower levels so lets restrict ourselves to this at the moment yeah so just as in the case of our environments we require the notion of a temporary updation on this static environment by the way there is static environment is another word that is used interchangeably with context and type environment so temporary updation of the type environment so given that capital gamma and delta are both contexts over some specified over some specified sets of variables this updation of gamma by delta is such that for every identifier x the type of x is given by delta if x is  refer slide time 17  00  a defined variable of delta okay so if x is some how a defined variable of delta what i have done is i have stealthily extended the notion of defined variables from syntax to the contexts so essentially if you think of a context as a collection of this form as a collection of this form and so on and so forth may be z of type int arrow int or some such thing if you if you consider context to be a collection of this form then the defined variables are x y z etcetera right so i have actually done that without explicitly mentioning it so we have the concept of what are all the defined variables in a context and so this updation is such that if x does not occur in the defined variables of delta then the type of x is what ever is given by gamma right  student  d tou delta  well he wants me to explain this well gamma is a context delta is a context and gamma updated with delta is a new context okay and how is this new context defined its defined in terms of the contexts gamma and delta as follows take any for any identifier x in the in the collections of variables over which gamma and delta are defined the type of x is what you are interested in a context and this type is what ever is the type of x in delta if x occurs in delta if x occurs as a defined variable in delta if x does not occur as a defined variable in delta then the type of x is what ever is the type of x in the context gamma okay so this takes into account redefinition of a variable right over right so lets so how do we process declarations for type checking we have so we have the rules of inference which are very similar to the rules that we had for dynamic semantics in the presence of environments run time environments so we are considering a collection of variables v and essentially this context gamma has this collection of variables v as defined variables right so given this amount with this assumption of the types of the variables given in gamma if you can some how infer that the expression e is of type tou then this declaration of x this is a declaration of x is well typed okay and what it means is that this declaration creates a new little context just like we had new little environments this creates a new little context which consists of a type binding type for x and that type binding type for x is tou so the type of x is tou and it creates this new so this this this rule actually does not do any inferencing it does only type checking so it it creates this little environment only if e has some how been inferred to be of type tou otherwise i mean otherwise this rule is not applicable which means that your your declaration does not type check okay i mean the same kind of default rules that we had for the dynamic environments also apply here i mean what is not specified is not allowed right so so if if e were of some other type some type other than tou then what it means is that this does this this this rule does not apply because this declaration does not type check and no and therefore therefore you no new little context is created and essentially the translation stops right there right no no further translation takes place i mean because remember that this is this is something that happens at translation time as supposed to our run time environment in the run time environment if none of the rules applies then what you can say is that essentially the run time environment the execution stops there aborts it does not give you any result okay similarly the translation stops here if something does not type check okay so now more complex declarations so now declarations produce little type environments  refer slide time 23  18  so in the context gamma okay which means some how all the free variables in d one have type bindings in gamma assume that all the free variables in d one have type bindings in gamma assume that all the free variables in d one have type bindings in gamma then since each of those declarations in d one may be something like this for example each of them they create a collection of bindings of so this processing assume that we can some how create a collection a little type environment which creates bindings for all the defined variables in d one then since the defined variables of d one could be free in d two in this updated environment where delta one is the little environment created by processing d one and of course it might contain some new variables v one so in this updated type environment if you can infer that d two creates a little context delta two then in the original context gamma this declarations d one semicolon d two actually creates the little environment delta one updated by delta two note that this this allows for the fact that the same variable might occur in both d one and d two okay this is not allowed in languages like pascal but its certainly allowed in m l okay and so so what happens is that the updation takes so within this in the context of this composite declaration then the most recent declaration of that identifier is what is what is used throughout and this is the same thing also happens in the run time environment the value of the most recent declaration of the identifier is what its effect right so so this is how contexts are created and now for the other two declaration mechanisms we have this so of course here we have this disjoint ness condition on this side that the declared variables of d one and d two should be disjoint okay and given that they are going to be disjoint both d one and d two are type evaluated in the same context gamma within which they occur and if each of them produces a new type environment delta one and delta two respectively then this composite declaration produces a new type environment delta one union delta two and this disjoint ness condition ensures that delta one and delta two are also disjoint and so the same variable does not occur in both yeah and finally for the within construct  refer slide time 26  46  as i said d one is nested i mean d one is used in order to essentially create the environment create the environment d two the type environment defined by d two so so what you have to do is and that and of course the defined variables of d one might occur within d two as free variables so which means assume that so we first process d one in the context gamma and lets assume that creates a little environment delta one with of course a new collection of variables v one now in this updated type environment gamma delta one process d two to obtain a new little environment type environment delta two and the net result of this declaration is to create this new type environment delta two yeah right so now we have seen how actually declarations create contexts and now the type checking becomes quite easy so we can we can look at type checking with contexts so right so if you have so if you have if you assume given a collection of assume a given context gamma which means the collection of variable to identifier to type bindings then that is the context we will carry through throughout in the case of these integers and so on and so forth assume that there is some syntactic mechanism by which even even without explicitly specifying it may be you can you can infer the type so in which case or in other cases you might actually require an explicit declaration of this form so in either case any constants some how are their their types are obtained by pattern matching of the forms for any variable of course for any free variable its type is really what ever is defined in the context yeah and for lets say these boolean variables we have the same old rules except that they are all in the presence of a context right so we have  refer slide time 29  29  this trivial type checking rules e one is of type tou one e two is of type tou two then e one circle e two is of type tou three where again of course the type tou one tou two and tou three are as defined by these tables here for integers and booleans right and so what it means is you carry forward these tables for type checking here and lastly the i mean the if then else is just it should ensure that in m l type inferencing actually means that in the case of commands as i said well formedness is enough there is no question of command having a type but in the case of if then else used in an expression language the whole expression since the whole expression denotes a value it denotes a value of a certain type okay whole expression denotes a value so that it denotes a value of a certain type which means both arms of this conditional should have the same type the result of this expression is going to be either the result of evaluating e one or the result of evaluating e nought depending on whether e is true or false so essentially this whole expression is of some type tou which means that its eventual value that it returns in execution will be a value of type tou provided each of these arms e one and e nought is of type tou is of the same type tou and of course this has to be a boolean otherwise its not a conditional at all right so if this is a boolean and each of the arms is of the same type which is not necessarily ensured in if then else in a typeless language in an untyped language what can happen is that this e one and e nought could be of different types and you might and the result of this if e then e one else e nought may not be compile time determinable as a type it could actually have different types and at run time after the evaluation of booleans you will actually when you get the value you will also get its type but in an m l type environment where we insist m l is strongly typed in the sense that at translation time the type of every expression is determined then this expression must have a type and it can have that type only if both e one and e nought are of the same type right so so that s how the if then else works here right and as far as let expressions are concerned we will just serve you have you have a declaration which creates a new type little type environment delta then you evaluate the type of this expression in the context of this new type environment delta so you look at it in this updated context determine the type of this expression and then where of course the side condition is that this delta might use new identifiers so this v prime is the set of all defined variables identifiers in the declaration d then in the context gamma the type of this let expression is really the type of this expression e  refer slide time 34  04  right so so so let so so this is how so in a completely closed program you will start just as you started with an empty run time environment you will start with an empty type environment and as you go through the declarations you will be building up contexts as you get as you go into inner and inner scopes your context will keep getting updated okay as you exit scopes your context will also keep shrinking just like the run time environment keeps shrinking as you exit more and more calls till at the end again you are left with an empty environment right and that s at run time this is at compile time that s the only difference that since its at run time it specifies values this is at compile time it specifies only types yeah so there is an interest so this is an interesting analogy between the structure of typing and the structure of for example the run time environment there are other very very interesting analogies too but we will look at that may be later so lets lets briefly look at typing i initially said that typing is an important an important concept because it allows you to save on badly typed run time executions so if you do type checking you can throw out a program early in the game without even bothering to execute it right so and as and it is been noticed that typing is even experienced programmers may type mistakes and so it s a good idea to have type checking the type inferencing so the difference between type checking and type inferencing is merely this that actually they are very both closely very related so if you look at languages like m l and caml what happens is that  refer slide time 35  47  now in any m l session or a caml session you might be declaring a large number of identifiers i mean one does not necessarily need to think of an m l program as a one complete whole after all the fact that you can go through an interactive session means that you can keep changing things and you so as a result in a really long session you will be declaring you will be using a large number of identifiers some of them useful some of them useless as time goes on but what ever may be the what ever may be the case what m l or caml requires is only that you specify just enough type information so that the m l typing system can deduce what is called the principal type forget about the word principal for the moment it can deduce the type of the expression that s why its not necessary for example to for every new identifier to introduce the declaration like you do in pascal right you can keep introducing new identifiers and as long as the m l type system accepts them and assigns them a type you are playing safe so it actually does a form of deduction and this deduction is a form of equation solving using unification so we will look at a small example of that but essentially it does deductions so it does type inferencing from very little type information  refer slide time 37  47  so it means that you require to have very few explicit type specifications for identifiers and the types of all other identifiers and expressions are inferred especially since m l uses pattern matching the use of pattern matching means firstly that it can use unification to do type inferencing secondly it means that since pattern matching is also a facility in the language it means programmers are going to use a large number of identifiers and they don t have to be explicitly they don t all have to be explicitly declared right so m l actually infers the type of every identifier and then type checking the expressions is no problem okay so type checking and type inferencing go on at the same time and further for identifiers which do not have which do not have explicit base type definitions so for example i can define this function lets say head for a list in m l as normally i would so head of i am i am using incomplete pattern matching okay right so so i will just define this pattern h cons t and i will say that this is equal to right now the the point about m l is that there are now atleast there are three new identifiers one is head another is h and another is t the types of none of them have been explicitly specified except that i use this reserved word fun and therefore this its clear that this head is not actually a base type that s all it specifies but this application says that i can this head is actually a unary function lets assume that there is some precedence so for example if i put brackets if i don t assume precedence then i put this brackets then what so all that you can really look at and infer from here is that head must be a unary function okay of well from this cons you actually infer that which applies on lists of some kind i don t know lists of what kind and it and if you have this if that list satisfies this pattern h cons t then i return h right so what m l does is it actually assigns a type variable since the types of h and t are are not explicitly specified all that m l so m l says that this function is really a function from some alpha list where alpha is to the type alpha where alpha is a type variable okay and since m l is polymorphic what it means is you can for example apply head to the list consisting of one two and three okay from the very syntax of this m l gathers that this is of type int list and it actually tries to apply head to this and sees whether it can do it and it can apply head to this only if it can equalize this alpha list with int list again it uses its own pattern matching facility and infers that alpha therefore must be int here and it assigns for this particular context it assigns head  refer slide time 42  11  it assigns alpha the value of int it i mean this is like a trivial equation solving when can what is the value of alpha such that alpha list equals int list well the value of alpha is int right i mean its most elementary form of equation solving it does that and it gives you an int and then it actually types check this so this is int so if alpha is int then this type checks so you could for example apply this head again to something else like true comma false and then it actually does this alpha list equals so this it knows is a bool list and then it does ad equation solving and for this application it does the type checking right so but m l actually is capable of  refer slide time 43  03  very complicated type checking type inferencing equation solving with the unification so it uses all these as this is words like list as function constructors in the unification algorithm arrow is also a function constructor in the unification algorithm arrow is also a function constructor in the unification algorithm okay the base types are may be the int bool real what ever there is a type language all other things are function constructors over which unification has to be performed so for example you could have you could have alpha arrow alpha list i mean you could have a list of functions right so so arrow list even the record constructors the tuple constructor they are all the star for example for tuple constructor for the tuple type constructor they are all function symbols to be applied to be used in a unification algorithm so for example list is a postfix this this pattern this this this not this string i shouldn t call this this constructor is a postfix function constructor over the language of types this arrow is an infix constructor over the language of types the star is an infix constructor over the language of types and it uses all this and does the unification algorithm runs the unification algorithm to do equation solving right and having done the equation solving and having inferred the values of all these variables not all variables will will if if i stop my m l session here alpha will not have an explicit value its still a type variable okay but the point is that it assigns it assigns a principal type that means it assigns it finds the most general unifier of that set of equations the most general unifier is usually expressed is not expressed in terms of a monotype always if this is my m l session okay and i use head over undefined kinds of list lets say i use head over list of functions of type alpha arrow alpha and so on and so forth then that alpha is never actually given an explicit base type definition in the language of types it remains always a type variable okay so it generates new type variables and does unification and finally has a list of type variables which might actually be in terms of which all other type variables are expressed yeah  refer slide time 47  16  and that s how it solves it now but whether its simply typed in the case of if its simply typed if it s a simply typed language then what it means is that at the end of a program this at the end of something this alpha should have an expression i mean alpha should solve to an equation of the form int arrow int or something in terms of the base type it should be an expression it should be a type expression in terms of the base types but if its polymorphic i just leave it as a variable and i do the type checking assuming that all the type variables that i have got have some how been expressed in terms of some minimal set of type variables and base types in the type language right so so if you look at so lets lets quickly look at this example here is a here is a example of sorted okay now if you look if you do not give this declarations so there are lots of identifiers here h h one h two t sorted they are all there is no declaration for any of them but there is a declaration for this h one and this declaration is required for example because you are going to use a less than or equal to relation and this less than or equal to relation is not defined over all types its not a polymorphic relation it is a relation that is defined in the m l environment only for integers reals may be characters for example its not defined on strings in your own environment you might have defined something like a less than or equal to for some other data types but then you are still being absolutely specific about it so this less than or equal to is not a relation that s freely available over all types and therefore the only way m l can type check this is if it knows which less than or equal to are you using then what this declaration actually explicitly specifies and determines the type of the entire thing for example from this from this declaration its it follows that this this has to be an int list and therefore h two has to be an int therefore t has to be an int list okay therefore this empty list is the empty list of int lists and this h therefore must be a int list and this nil is also the empty int list and then of course then this less than or equal to type checks for integers it doesn t type check for many other types and this this whole thing is a boolean so sorted is has a type which is int list to bool right  refer slide time 50  03  so all this so the programmers are lazy and would like the convenience of not explicitly specifying types but then when they do not explicitly specify type they also make type mistakes so the best way to combine convenience with typing is to do type inferencing which is what m l does the other extreme of course is something like pascal which do only type checking they expect that every identifier is has an explicit type specifier and after that they only do type checking they do not do any type inferencing the type inferencing is restricted to expressions but then that is for but then that is for checking consistency and that that kind of type checking can be done trivially by just matching parities and so on it doesn t really require a sophisticated type constructor collection of type constructors to do it so languages like pascal do only type checking and actually even though i have written no inferencing they actually do some inferencing after all expressions come without predefined types every expression is not type defined by the program they do that amount of type inferencing which is already given by the rules and they do the type checking to essentially to maintain consistency of types right so so and essentially that s that s how type checking goes on so type checking and type inferencing are i mean they go together if every every if every identifier has been explicitly declared then you require to do only type checking but you use the same set of inference rules  refer slide time 50  58  for both if not every identifier is declared then you have to do type inferencing and if and you might require sometimes in that example of sorted if you did not explicitly specify some where that something is that its an integer list either in an implied form or in an explicit form if you do not specify in some place by which the inferencing system can determine what is the type then it automatically gives you a message unresolved type or some such type or ambiguous type unable to resolve type yes that s that s the message it gives so so both m l and pascal are actually statically typed languages in the sense that the types are determined at compile time that s what static means they are strong actually pascal is quite strong but i have written weak here for for a specific reason and that s to do with variant records where you can do type mismatches okay but otherwise both m l and pascal are strongly typed pascal has other weaknesses also like for example in procedures which have functions as parameters the types are not clearly determined and they are not determinable at really at compile time except at the call but but more or less pascal assumes but more or less pascal is strongly typed most types are determinable apl and snobol and so on do not actually have a notion of compilation they are very very highly interactive so they have a very weak dynamic typing dynamic type checking facility just before the operation is applied in the execution they actually check the consistency of the types and see the operation can be applied lisp and scheme are mostly untyped except for the underlined base types which for which type tags are already available in the hardware or in the for through firmware or through assembly or some such thing right so that s how typing goes right so i will start on abstraction next time  refer slide time 52  04  programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 36 abstracts  57  27  okay welcome to lecture thirty six today so today we will start abstracts and well its not a very common name for what might be called an application of the principle of abstraction which again is not a very common principle okay essentially according to tenant for example in his book he defines the principle of abstraction as just any semantically meaningful syntactic category can be used as the body of an abstract  refer slide time 1  00  right and actually what what we mean by an abstract we have actually done some abstraction in a certain way and that is the declarations in the m l let construct so essentially the whole point about abstraction is this and we should lets lets not confuse this word abstraction with the abstraction in the lambda calculus though they are some what related so whn we talk about abstraction we are really saying that there is some complicated object or a group of objects and in general i am talking about any objects it does not necessarily mean code okay it does not mean necessarily data or code any object infact for which which we would like to group together either for some logical reason because they belong together or we would like to group them together for some convenience so one form of abstraction is for example your the fact that i mean a department within an institute is a form of abstraction okay where there is a there is a group of people equipment furniture rooms what ever which some how are grouped together and regarded as one consistent whole i mean so records for example in programming languages also form some part of an abstraction but the notion of an abstraction can be generalized to essentially not just data but any logical grouping of objects and i don t necessarily refer to objects in in the sense of programming objects i am saying that we are use this principle essentially throughout our lives in a in almost every field in every field of activity right so one natural abstraction is is the are the declarations in a let constuct in m l so there is a the the main the main body of a let construct is the expression which is finally evaluated but there is enough reason  refer slide time 6  15  to take sub expressions from that expression abstract them out by giving them a new name which is what you do in a declaration and use that name where ever that sub expression is intended okay so that is a form of a abstraction you have some complicated expression you split up into subexpessions which for various reasons either for purely logical reasons or because you want to frquently use that subexpression for various reasons which might be determined by a programmer he removes that sub expression may puts a declaration and gives that sub expression in name and uses that name where ever that sub expression is intended right so essentially what the programmer has done there is that he has performed an abstraction in the sense that as far as the main expression in a let construct is concerned he does not really he would like it to some how stand on its own and so you have some declarations like let x equals i don t know some complicated expression okay in some expression may be okay so essentially what what has been done is that this expression three star y plus z should have should have occurred here where ever x occurs and for some reason best known to the programmer he has deemed it not exactly necessary but helpful atleast convenient atleast to abstract out that some expression give it a name and use that name throughout okay so essentially what what this structure what this abstraction gives us is that it gives us this expression the main body of this expression where if we if we choose we do not really require to look deeply into what exists okay it provides a form of hiding which which you need to look into only if you really want okay so that that s what an abstraction does an abstraction takes some complicated object or group of objects and some how performs a certain hiding on them so that the so that what might be perceived as irrelevant or unnecessary detail can be hidden or can be eliminated or can be skipped and only the main overview of that group of objects can be viewed okay and this abstraction is something that that is actually that is actually very common throughout our lives i mean if if you look at all your pieces of equipment essentially what you are saying is that there is an internal you have got a black box view and the black box view itself is a form of abstraction you got an interface which is all that is visible to you the internal details are hidden the internal mechanisms are hidden and all that you are interested in is the interface so you don t need to open the black box if you don t want to know the internal details okay so the internal details are hidden so that a form of hiding that we have already seen is that of scope rules but that s very rigid form of hiding in the sense that its if you are inside the scope you can see what ever is there in the scope if you are outside the scope you really cant see anything okay so that that s a form of abstraction scoping is also a form of abstraction but what we are looking at more now is what might be called named abstractions we have looked at abstractions in their unnamed forms either as unnamed blocks or lambda expressions but what we are looking for now are named blocks okay so a named abstractions and naming despite what despite what anybody might say is actually very important despite what shakspeare might say it is actually very important if the flower rose did not have a name shakspeare wouldn t have been able to give his famous quote about it and he would have puzzled what to do about it right so naming is actually an important  refer slide time 9  50  aspect of all forms of abstractions right so the major syntactic categories which are for us semantically meaningful throughout i mean a unified view of programming languages would say that there are really only three syntactic categories expression expressions commands and declarations and all of them have abstractions available to them you can create abstractions with all of them and what way will primarily concern ourselves now are with what might be called the command abstracts so which are really procedures in imperative languages right so these we have already looked at expression abstracts though i did not explicitly mention that they are abstracts we we will look now at procedures procedural abstracts which is not exactly the same as the notion of procedural abstraction which which ables and smith give in their book on scheme but its similar and related so we are looking at abstraction in imperative languages and then there are abstractions possible over declarartions also declarations or definitions and they are the abstractions which are called modules or classes okay modules are just names of name unnamed collection of declarations grouped together for some reason right a class is well a module parametrized to include various subtyping and inheritance properties and so on so its its really so these these are so these are the declaration abstracts that we we will not necessarily look at in great detail but lets look at procedural abstracts so so as i said one thing that is important about the most important features of an abstract are firstly its naming okay so the fact that you give a name to an abstract means firstly that even if that name is inappropriate incongruous or plain ridiculous it still stands for some collection of properties or objects which it names and naming therefore allows a repeated use of the same kind of objects in an abbreviated form a naming actually provides an abbreviation which can be used repeatedly right so our use of names in even even in natural language are use of pronouns are really forms of local declarations so that you give it an give something the name it and you can use the word it repeatedly for that something and that something could be some complicated object which may not be capable of being described very conveniently and it get forms a logical hole right so so the use of words like he ate she and so on in our natural language is really a form of local declaration and very often let me tell you most people use them ambiguously but if they are not used ambiguously they actually form a local declaration to allow for repeated use so pronouns are really names for abstracts even in natural language right  refer slide time 16  44  so naming is an important thing which in the sense that it allows the repeated use of the same well in the case of programming languages same computations at several different control points in the program right the thing with unnamed blocks is that that s infact true of this expression abstract too there are atleast two occurences of this x and if i did not if this x were something really complicated  refer slide time 6  15  then i would have to replace this x by that complicated expression so its important to have naming so that you you allow you allow an abbreviated an abbreviated abstract name which can be used repeatedly in different contexts okay so which essentially for the same kind of computations that you want to be want to be performed at different contexts by the use of a name you can abbreviate that but the moment you the moment semantically speaking the moment you introduce a name you are also it also means you have to introduce a binding and pragmatically speaking the moment you introduce a name you have to generate some code which actually interprets that name in something that s that s consistent with the semantical use risght so both semantically and pragmatically names create fresh bindings and we have to we have to do something about those fresh bindings okay and but they provide this convenience to the user of repeated use of performing of of actually standing for some complicated object or a group of objects and so can be referenced by single name and further if you parametrize the names then instead of just being able to duplicate the same computations by using their name you can duplicate many similar computations by varying the parameters right so a name and a collection of parameters allows you to actually perform this abstraction so that the parameters form what you might call the interface to the black box what is actually externally available for viewing without looking inside the black box right  refer slide time 18  53  so the the name and the parameters together what they can do is they can actually provide an when when you look at many similar computations to be parametrized you are performing another form of abstraction where you are emphasizing the similarities of the computations and deemphasizing or hiding the differences in that in those computations right so abstraction as a form of hiding essentially with with parameters means precisely that you are trying to emphasize or highlight the similarity of different users of that name rather than emphasize the dissimilarities or the differences between the various users in the various contexts okay so despite everything in especially the issue of naming has cropped up recently also in the whole question of mobile computations with cellular phones cord phones the specification of such things and it turns out that naming turns is one of well both philosophically linguistically and computationally one of the most important objects which we have overlooked treated quite shabbily in the last forty years of programming languages yeah right so so the main features of an abstract therefore are its name parameters and of course what what is actually what it actually represents and that s its body right so and the body of an abstract of course as i said since abstracts can be you can have expression abstracts that means the expressions can be evaluated you can have command abstracts which means those commands can be executed you can have declaration abstracts which means those declarations can be eleborated right and so so the so the three features of an abstract are really this issue of naming the issue of parameters and the body and we will not worry really too much about the body because we have already done unnamed blocks and abstractions and we know how those bodies are executed or elaborated or evaluated okay now its just a question of looking at naming and parametrizing okay if you look at parametrizing then the question of parametrizing again balls down to substitutions in a calling environment right it s a matter of performing certain appropriate substitutions in a calling environment the issue of naming is also a question of performing a substitution in an appropriate environment eventually all forms of computation are really forms of substitution in appropriate environments appropriate substitutions in appropriate environments  refer slide time 23  05  so so that s one of the reasons why we had to do the lambda calculus in some detail so that you appreciate that everything really has to do with substitution yeah so so we have this so these are the features of an abstract and we will concentrate on naming mainly in naming and control abstraction for for some time now okay so lets look at control abstraction most programming languages are what might be called a command abstraction right so a command abstraction in most programming languages most imperative programming languages is really in the form in the form of procedures okay and though again so so and what are these procedures they are really an encapsulation of commands of of a command group of commands and the encapsulation i mean essentially gift wrapping it so that you don t see the internal details you might name them and parametrize them right so of course you can have naming of course is compulsory in many languages your parametrization is optional right in languages like algol sixty even naming is not compulsory and that s how you get unnamed blocks in algol sixty c you can use unnamed blocks so that naming is not compulsory and essentially what happens in such languages is that naming is used only when there are repeated uses in different contexts if this is going to be only single use but its still an abstract you just define a unnamed block right so so its encapsulation of comands is really what command abstraction is all about and most programming languages okay this is these this is some terminalogy which i have taken from tenant its not necessarily standard its not necesaarily widely used but it actually captures the essential meaning that you have expression procedures okay which means which basically corresponds to functions in pascal functions is a greatly abused word so it s a good idea to use the term expression procedures they are procedures because they transform state because they are commands their bodies are commands so they are a command abstraction okay they are expression procedures because they return values of expressions too okay they are procedures because they are a command abstraction and therefore they are they change state or change configuration but we prefix them with the word expressions because they also return values and then you just have what might be called command procedures which are the normal procedures in pascal right  refer slide time 27  00  so so basically entities in pascal with the reserved word function are expression procedures entities in pascal with the reserved word procedure are command procedures yeah and procedures in general the way we will be using the word is to denote any form of command abstraction which is named which has a name which has a name is important because it means it also has bindings there are bindings to their name and you have to decide what exactly is being bound to that name okay what is the type of object that a command procedure is what kind of a what kind of a creature is it i mean that s that s an important question and the name should denote that the should classify the kind of beast that this command abstraction is right right so so what we will do is lets play around with a few permutations so this issue of naming so lets look at an abused pascal like syntax and i will tell you the reason for the abuse so you have lets lets look at our pascal like procedure for the moment i will forget about the parameters they are optional so the typical procedure structure is reserved word procedure some are named a procedure identifier parameters may be semicolon some local declarations begin a command end right so now what what are we done by this naming is that we are claiming that this is the name of an semantical object which represents this command abstraction right so the abstraction the syntax for performing the command abstraction is the reserved word procedure okay so so essentially what we are saying is let p be the abstraction procedure parameters local declarations begin c end or the let p be the name of this particular abstraction that these parameters yeah similarly for functions you have the syntax and what we are essentially saying is let f be the expression procedure defined by function parameters colon some type semicolon local declarations command may be and i have for convenience i have just written result e okay and the reason for that is as follows the way pascal works is that it uses the function name itself as a variable local to the function so instead of this what you would have is f is assigned e right but then that f has a that so but then there are two f s in the same scope one f is is this beast which we still don t know what it is its an abstract the other f is a local variable which has which just has its type specified here right there are other there are other problems with it i mean with in a language in which you have side effects global variables can be used can be modified within the function so on and so forth and in a language which does not allow more than more than a simple type to be returned as function values supposing i want a huge collection of values to be returned record to be returned or even a file to be returned after the evaluation of a function what do i do well one possibility is to to actually return a pointer to that object which is what for example most c programmers do they return pointer to structures okay which you can do in pascal too other possibility is to actually treat this if its some complicated type to treat it as a global and perform the side effects and make it a parameterless procedure or in case there are problems about whether its it was succesfully executed or not just return a boolean value but the actual side effects that you are interested the actual effects that you are interested or the values that you are interested are are stored through side effects on globals for example on files yeah so now supposing there were no parameters supposing it is a parameterless function and i had f is assigned some expression involving f itself okay then there is a resulting ambiguity is that f a recursive call to this function f or is it does it refer to the local variable f which was created as part of the elaboration of this function right so supposing i had this statement if i followed rigid pascal syntax i had this statement like this for a function f which is parameterless okay then the use of the same function name as a local variable i still don t know what the type of an abstract is but what i do know is that the type of this regarded as a local variable is what ever is the type specified here okay the use of this therefore and this since this this type is involved inside here its not its not at all clear that this function itself has the abstract the abstract the expression abstract f has the same type as this local variable f right and why should it it may not in which case does this f refer to the local variable f or does it refer to a recursive call to the function f itself okay that s an ambiguity that is not really been addressed many implementations take their own view they they actually for example turbo pascal actually refers refuses to recognize a recursive parameterless function it assumes automatically that if f occurs if the name of the function occurs on the right hand side of an asignment then that f must be the local variable f which is created as a part of the function right so that s one reason why its perhaps not a very good idea too of course there are very simple syntactic ways of changing i mean of of rectifying this ambiguity and that is to insist that all functions whether they have parameters or not have a pair of braces around them in their declaration and if you are using it as a local variable just call it just use f if you are actually making a recursive call make it this way i mean there are simple patches of doing of changing this but that s not the point the point is that what you are interested in firstly is that this object which is being called f whose type i still don t know all i know that its an it s a command abstract whose most important things are the creation of some side effects because it uses commands changing state and finally finally returning a value of the type specified here right that s really what this object is yeah and so the issue of naming therefore so essentially we look at so the intutive meaning of what might be called procedures are that so i will this p and f are  refer slide time 34  06  with reference to the the previous example lets say so so essentially the intutive meaning of a procedure in a imperative languages is that a procedure p is a function which well will take which might take some parameters and a store and gives me a fresh store right and an expression procedure or a pascal function is is a mapping which might take some parameters and a store give me a value which is actually specified by the body e of the expression procedure and give me a change store which is actually defined by the command c and since the language allows for side effects even in expression evaluation this e itself could change the store the evaluation of this e itself could change the store right so basically what we are saying is so therefore an expression procedure or pascal like function is just is just something that takes parameters of appropriate types and the current and the current store and returns a value and a new store and a procedure is something that takes appropriate parameters and a current store and returns the new store and the abstraction that you are interested in what does what intutively does the abstraction mean under such a semantic setting it means that if you are lets lets just consider this lets just consider procedures it means that this the change from one store lets say sigma one to our final store sigma two actually goes through a sequence of changes whose intermediate changes we are not interested in we are hiding them right you are only interested in what is the final store that is reached given this initial store if you did not have a named procedure then what you would have had to do is you would have had to take that body c and place it inline in your code in which in your main code in which case you would have you would it means that you are essentially interested in every state that is produced by that body c and the semantic abstraction that you provide by this is that you hide all the intermediate states that are going from the intial state to the final state right so that is what we will essentially look at it that way we will postpone the semantics for a bit lets look at some other issues since we are talking about naming and identifiers so we have to look at the issue of scope again so the normal scope rules that we are all used to us what are known as the static scoping rules so which means that given what in programming is called a non local identifier in a procedure or what to be consistent with what ever we have done its actually a free identifier of a procedure so the free identifiers of a procedure are just those which are neither parameters nor have been locally declared so every identifier that is that is neither locally declared nor which has a nor which is a parameter by the way are also declared is a free identifier or what might be called the non local identifier and it is statically bound  refer slide time 38  25  in the environment right so which means that the binding occurences for such non local references are determined statically that means they are detremined at compile time before any before any thought of execution of the program occurs and the binding occurences are determined by by the innermost enclosing scope rule okay so given any reference to an identifier now this identifier also includes procedure identifiers function identifiers what ever you anything which has a name it actually for anyhing that has a name it refers to the innermost lexically enclosing scope in which that name has been declared and that is something that is compile time determined so this this is known as static scoping or lexical scoping and the binding that is created by that well is naturally called static binding  refer slide time 41  20  so this is the binding that is created by the compiler as it reads through your code right and most languages including including scheme important including scheme actually use static scoping rules and as supposed to static scoping what you have is what you the other possibility is what is known as dynamic scoping right so now what happens in in the case of a dynamic scope if you have dynamic scoping rules what it means is any non local reference for any non local reference the appropriate binding occurrence can not be really determined at compile time okay and the dynamic means that what ever you do has to be can only be is most likely possible only at run time so which means that you actually so what ever are the free or the non local identifiers in the abstract they are bound not statically but they are bound in the environment in the calling environment what might be called the calling environment right so this is so these are so for so what it means is that different points in the program if there are different calls then different at different calls the same identifier might refer to different things the binding occurences can not be determined by looking at the text of the program the binding occurences can only be determined by looking at the run time stack at that point right so and this is what is used in lisp and apl and what and what it falls what it follows is really the innermost enclosing call rule rather than the innermost enclosing block rule okay a block is a piece of text a call is a run time object okay there is a nest there is a nesting of calls and the innermost enclosing call which actually contains a declaration for that object is the reference to that object is the is the binding occurrence that you are looking for yeah so lets lets just look at this lets just look at these differences some what more pictorially so lets so what i will do is as usual to me all languages are either either have m l syntax or they have pascal syntax so but right now we don t even need to worry too much about syntax we just have to worry about boundaries and so on and so forth scope boundaries and so on so assume there is a program p okay which has a declaration of of x so when i write x colon dot dot dot it means that this is a declaration for x okay and then within the program p there is a a procedure lets say p one okay and there is within procedure p one there is a procedure p one one okay and this is what i might call the static structure of the program i mean the program as you read it on a print out no machine anywhere just read the printout if you read the printout and look at sort of block boundaries i wont say scope boundaries lets look at block boundaries then the program p is this entire boundary purple object and the procedure p one is red object lexically nested inside the program p the procedure p one one is this blue object dark blue object lexically nested textually nested within the procedure p one and after p one there is another procedure p two okay so you can assume that p two follows p one if you like i didn t have enough space  refer slide time 44  00  so p two and within p two well there is a black procedure p two one p two contains a declaration for x right so now the point is this if this were a pascal program and there is a reference to x inside this p one one what is the binding occurrence for that refernce to x and that binding occurrence follows normal textual rules so you take the innermost textually enclosing block which contains a declaration for x and therefore this x refers to this x i am assuming that p one does not have any declaration of x yeah so then the innermost enclosing block which contains the declaration of x this purple x so therefore this this blue x is actually this purple x so all references to x within this blue procedure are really references to this purple x right assuming of course that neither p one nor p one one has a local declaration of x okay how ever if this were not a pascal program but for a list for a apl program then its not clear what this x refers to you can not make out from the text of the program that this x refers to either this or this what this x refers to really depends upon the execution time behaviour so lets look at an execution time behaviour lets assume that this is a this is the body of the main program p and there is a call to procedure p two okay so which means you have this green x here you are executing the body of the procedure p two and within it there is a call to procedure p one so which means you go through this and within p one there is a call to procedure p one one and you find this reference to x and now the question is what does this x represent what is binding occurrence of this x okay there is an x in the global environment because you called it from the global environment there is an x also in an enclosing calling environment namely in p two now the now which x does this refer to if you use the innermost call rule then the closest declaration of x in the calling environment is this green x and therefore this x refers to this p two okay so and the reason i am saying its important to realize that it is the part of the calling environment is that sometime later in the main program if i had actually called p one okay then the calling chain assuming that it s a similar calling chain then p one is called pone calls p one one and again this you refer the same reference to this x now in the calling environment is of this purple x so at different points depending upon what the calling environment is the binding occurences can change  refer slide time 48  19  so which means that at compile time you can not make any commitments about the reference to this x you do not know at all at compile time at compile time what does it mean you would have read the declarations of p you would have read the declarations of p one you would have read the declarations of p one one you will be processing the body of p one one and you encounter x and you can not jump to the conclusion that since you have this x here it it refers to this x here because at the point when you read this you have no notion what are the calls and what is the sequence of calls that are actually going to be performed at run time when you come down to the calls even if you chase the sequence of calls you have by the time this has become a black box which is not available to you the information in p one one is no longer available to the compiler when you are processing the body of the program p okay so which means is that at compile time in a dynamic scoping environment at compile time no commitments can be made about non local references right and if no commitments can be made that means that the very act of compiling itself is actually a useless activity you cant create any bindings you cant do any memory allocations because you don t know what x is you don t know what x it refers to you cant do any so you cant do type checking you cant do memory allocation you cant do any storage representation so all of them have to be deferred postponed till you start executing the program which means you might as well dispensive the compiler and just interpret the program directly and that s why most lisp and apl systems are interpreters they do have compiling features these days i mean but the original lisp and apl systems were purely interpretive programming systems yeah which and the reason for using dynamic scoping rules there were was more pragmatic than planned i think yeah so lets lets quickly look at this so this this call that i looked at actually has this there is a starting there might be an initial environment consisting of globals libraries and so on which might be loaded then there is this call from p to p two p two to p one pone to p one one and there is a refrence to x and if you work backwards along the chain the innermost call rule tells you that this x under a dynamic scoping under dynamic scoping rules refers to this green x whereas if you maintain some how this the static nesting structure of the program even in the calling environment then what you get is that this as you will have to go backwards along this static nesting chain and you will never hit this green chain x you will only hit this purple x and so under static scoping rules you have to i mean it it should give you this the purple x for the same sequence of calls yeah and so what it means is that actually what this means is that static scoping though its textually very nice and allows for debugging allows for compiling allows for compile time type checking allows for compile time cogeneration and so on and so forth what it means is that there is an overhead associated with it that overhead is that you have to maintain the static structure of the program you have to capture the static structure of the program the textual structure the lexical scoping structure of the program some how in your run time environment so that your references are statically determined on the other hand the dynamic scoping does not require this extra overhead of maintaining that information you just traverse down the run time stack and find the take the first x that you can see period and your dynamic scoping rules are implemented  refer slide time 54  21  so dynamic scoping is not convenient for the purpose of reading a program or debugging it from a listing but its it s a simple environment i mean at that at that point you take a decision to go down the stack and find the first x that you encounter and that s the x that you are refering to whereas in a static scope what it means is that you have to maintain this information after all static scoping is not preserved really in the run time structure because as you can see i mean there is a call from p two to p one and p two to p one are at the same level the same nesting level and they are independent right so which which means that your run time stack does not necessarily maintain all this information textual information right so so and how do how do this so a typical static environment so at compile time what you have for the same program is that at a time at the time when you refer when you reach the blue x you actually have a static environment which includes type environment and so on nesting depths and so on and so forth with all this information in the symbol table not necessarily in this fashion because symbol tables for fast since their references are very frequent symbol tables are usually organized as through as hash tables through hashing but the logical structure of the symbol table if you assumed a linear search for refrences would be that you have this kind of a stack with well the you have to look at environment i mean you might there is a global environment of reserved words keywords environment variables library identifiers and so on and so forth then your main program the symbol table for your main program where type information storage information and so on and so forth then as you go through you come to the declaration of p on so you have the symbol table for the local declarations of p one the parameters of p one and so on and so forth and you keep incrementing the nesting depth at each point and maintaining the nesting depth then you will reach p one you would you would not at all touched p two yet okay so there is no question of looking at the green reference to x the green declaration of x so and therefore what you look at in the static environment is you will just go down this well logical stack because its not really organized that way and find the first x and that gives you the innermost enclosing block which contains the binding occurrence and if you maintain this nesting depths then the refrerence to any x is just the position the reference to any x is just how many how many nesting depths down you have to go followed by what ever relative address has been assigned to that x right and that s the reference to x so you can you can substitute all references non references to x by this ordered pair of nesting depth and relative address except that this nesting depth has to be maintained in the run time environment some how and for that we have what is known as static change point okay we will continue next programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 37 procedures  54  12  welcome to lecture thirty seven so we will continue with procedural abstraction so as i said one thing is that the whole purpose of abstraction is that  refer slide time 00  36  you can take any semantically meaningful syntactic category and you can use it as a body of an abstract and and important part of using it as a body of an abstract is to give it a name so essentially you when shakspeare asked whats in a name the answer should have been an abstract but he apparently didn t get that so so you have various kinds of abstracts  refer slide time 01  02  expression abstracts which we have already actually seen partly procedural abstracts which are commands command abstracts and then you have definition or declaration abstracts as in modula m l c plus plus simula small talk etcetera right so the most important features of an abstract are really its name and of course we have to answer the question what exactly is an abstract that s an important semantic issue where we have not yet addressed and and of course the parameters to capture similarity of computations  refer slide time 01  50  and so on and so forth and a type checkable body which is elaborated when ever the name is so it is to be called with appropriate parameters right so we were concentrating on control abstraction and what and what i say for controlabstraction i mean namely procedures more or less also holds for these expression abstracts where the expression abstracts denotes functions anyway when we do the semantics of m l like functions we will come to that but lets look at this form of control abstraction that we have got  refer slide time 02  28  which is basically like functions in pascal or procedures which change state in some fashion and of course when it comes to these kinds of abstracts normally there are two kinds of scope rules that you might use one that we have been using mostly in languages like scheme m l and pascal c ada algol sixty most of the most of these languages actually most of the languages which are compiled rather than interpreted though of course the m l environment is an interpretive environment but most languages which are compiled prefer they use a static scoping mechanism which is which means that which means that it is easier to debug look at non local references from the text of the program rather than from the run time environment so the bindings are all compile time or the identifiers are all statically bound so and the main difference between and in the case of dynamic scoping what it means is that non local references as in this case where you have a chain of calls assuming this large program p has two procedures p one and p two which are at the same level of nesting and independent of each other and within p two there is a call to a nested procedure p two one within p two there is a call to its own nested procedure p one one which has lets say a non local reference so if it were a statically scoped language this non local reference would refer to a binding occurrence that occurs in the innermost textually enclosing block on the other hand in a dynamically scoped language this reference would actually depend on this calling sequence and would refer to the most recent calling block most recent invocation of block which actually contains a binding occurrence so that would be the difference between static and dynamically scoped languages and the reason i said that dynamic scoping is mostly used in an interpreter style rather than in a compiled style  refer slide time 05  16  is basically this so is basically because of the run time environment so lets lets look at that lets look at this program executed in which is which is lets say which is lets say at this point at some control point here after this chain of calls so if you look at the run time stack of this program run time run time i am not talking of compile time so if you look at the run time stack the activation records actually look like this so firstly of course you have you have a global environment this is sort of normal since no program usually works without a global environment you you usually require some global environment which contains standard io procedures may be libraries and so on and so forth so lets assume that there is a global environment in which every program executes and of course then there is this call to procedure p one there is this program p which which is which has a pink border or a purple border if you like this program p so the activation record for this program p which actually well it has a local declaration of a variable x and it has of course the procedures p one and p two declared locally so in the process of compiling what would actually happen is this p one and p two would would refer to i mean would refer to the jump addresses to the code segments of the appropriate procedures okay so so the fact that p one and p two are local to the program p means that you have to some how create a jump address the address of the code segment and in case and then p one and p two could have parameters so in in any call within p to p one and p two you will have to type check parameters so so there will be also part of this would also be some some information on kinds of parameters in the order in which they appear and their types i mean basically their types so that any call to p one or p two when you are actually compiling this main program should should type check right so anyway so in the in the run time environment this p one and p two would refer mainly to the the code segments of p one and p two where they are where the jump addresses where the codes start from okay some so from within p of course there is a call to p two which is this green block and so and so there is an activation record for p two p two one is of course nested within p two so therefore there has to be some address to its code segment and there is a local variable x and there are of course parameters but this is the main program so it doesn t have any parameters strictly speaking main programs also have parameters which which usually refer to something in the library routines like you have like a pascal program actually refers to the files that is going to access but lets assume that there are no such things so a procedure usually has some parameters it has local variables may be locally defined procedures whose the address of whose code segment you require to maintain and then within from p two there is a call to p two so there is an activation record for p one which is similar right and from p one there is a call to p one one so there has to be an activation record for p one one so the typical run time environment at the point at the point when you reach this control point is that there are  refer slide time 09  26  four activation records calling chains of length four beside the global environment so now what happens is this everytime the moment for example you the moment you stop executing p one p one one supposing the execution of p one one is completed then what happens is you are back in the environment of p one which means that the current this is the current environment pointer so you have to know at each stage what is the new environment pointer that you have to jump to when you exit this scope during execution so when the scope p one one is exited during execution then this this top block this top blue block should go and your current environment pointer should point to the bottom of this right so the fact that you are creating a new little environment at each stage means that you have to have what is known as a return pointer which will return you to the current to the new current environment pointer each time you exit a block at execution time during exection the moment the block p one one is exited you require a return pointer which gives you the base address of the previous activation record okay so this r p is called the return pointer and and this whether your language is a dynamically scoped language or a statically scoped language you require these return pointers each time right so so what is called the dynamic chain which is really the calling sequence looked at backwards okay if you remember the calling sequence here so this is so this is really the dynamic chain right so there is a calling sequence so you have a dynamic chain which points from p one one to p one p one to p two p two to p and p to the global environment if necessary but i have not shown that so essentially you require this dynamic chain which is maintained at execution time so now in an language that is interpreted it is easier to implement a dynamic scoping mechanism for the present lets assume that this is not there in a language that is dynamically scoped hat it means is that every time you see a full phrase you translate it and execute it befor you look at the next syntactic phrase okay so now what happens is supposing you see this phrase containing this non local reference to this blue reference to x then when you have to translate it you have to check out whether you have it you have you can just traverse down the dynamic chain to find out some reference where find out the first reference where x was actually declared or where x has a binding occurs since translation and execution both take place simultaneously at the same time essentially the most of the basic information about identifiers and their bindings and so on will still have to be maintained in a symbol table at run time since there is no compile time each phrase is textually read translated code is created and executed so when ever you read a new non local reference you don t know anything about it till you traverse down the dynamic chain either you have it in the current environment or you have it in the previous environment or you have it in the previous environment which ever is available you just which ever is the first that comes you just bind it to that right so under the dynamic scoping rule or the dynamic bindings that means all the translation is done at execution time i mean there is no separate compilation phase then what happens is that with just this dynamic chain these innermost enclosing invocation which contains a declaration of that non local reference can be found by just traversing down this chain and searching through the through the subsequent activation records so if you have a non local reference what makes it a non local reference is just the fact that there is absolutely nothing corresponding to this activation record which contains a declaration of x or a specification of its type so what do you do you you go through the previous invocation which is which is p one you check whether p one in the activations of in the activation record of p one there is some identifier with a declaration for x if there isnt then you just traverse down from the return pointer of p one to the previous thing and check and and you will find may be this x there is an x declared here okay remember that all the symbol table information identifier name its type everything has to be stored since it is interpreted it has to be stored some where at run time so so identifying that its an identifier with a certain name is possible at run time provided you do everything at run time in a compiled language identifiers and names no longer exist after compilation they have all been translated into addresses okay so there is no question of string matching identifiers so on and so forth so so what in a dynamically scoped environment you just keep traversing down this dynamic chain in order to find the most recent binding occurrence that was encountered in the chain of calls okay right on the other hand so this this dynamic chain serves all this purpose which is the reason early implementations of lisp and apl which wanted to make it interpretive it is also an easier thing to write actually use this dynamic scoping mechanism but what it means textually when debugging a program is that you can not look at an abstract you can not look at a procedural abstract in isolation and hope to get anything out of its non local references the only way you can understand that abstract is by essentially hand executing and producing the appropriate calling chain by hand execution and then determining what would have been that reference at that point what would have been the binding occurrence for that non local reference at any point yeah so debugging is interactive debugging may be easy but a planned debugging of the text is usually quite hard with with a dynamic binding mechanism right so so most languages which allow for compilation and m l scheme because they actually would like to encourage debugging from a text they have they implement what is known as a static static binding mechanism which means essentially that kind of binding mechanism that one sees in pascal right but then what does this mean this means that in the run time environment in the run time environment you have to maintain what happens in a compiled language this is specific to compiled languages  refer slide time 19  26  at compile time a symbol table is created and an address a relative address relative to some base for each identifier is created and what is the what is the translation of an identifier lets say of a simple variable an identifier which denotes a simple variable is just the translation of replacement of that name by its appropriate relative address with a reference to the scope in which it was bound right so which means that now under a so so in the proces of compilation i can actually create a i can actually create an address for this with the depth of nesting some lets say one and a relative address which basically says that this has been allocated so much of memories so many bytes from the top of the base from where this scope starts would start in activation record in the run time right at so now any non local reference x here would therefore refer to just this address with the with the appropriate nesting depth with the same relative address that is actually a reference to this x right so as you read through as the compiler actually scans the scans the token file and it encounters this identifier x it just hashes on to some binding occurrence and translates this x by the appropriate address okay so at the code that is actually executed onlly has addresses relative addresses which at execution time the bases will be filled up as and when activation records are created there will be a base address stored some where which you have seen in your p l zero compiler i mean there is a base and the stack for the current activation record and all addresses are relative to how much above that base should you go before you reach this variable right so what all what this also means is that since you have addresses which are relative to a base in the case of the run time environment you have to maintain the static structure of the program some how i mean you have to maintain for example the fact that both p one and p two are nested within p so p one is this red activation record it has a pointer to the pink one this green p two is this green one which has a pointer to this pink one and this blue p one one is nested within p one so there is a pointer from the blue to the red to the base of the red so that now  refer slide time 09  26  if you can maintain this information then all non local references which have a nesting depth comma relative address given to them means that you go down the appropriate chain appropriate this appropriate chain rather than this chain i mean you have a current nesting depth of lets say two and you have a non local reference x which you have a current nesting depth of lets say three p one one is at nesting depth three and you have a non local reference which has been translated as having nesting depth of one and has so and so address relative to that base of that nesting depth one so which means that you have to traverse down two steps and go up to the appropriate translation above the relative address in order to access x right so this chain of nesting textual nestings is what has to be maintained in the run time if you have to have a statically scoped language implement especially a compiled language right so a typical dynamically typical language with dynamic binding will just have this kind of will just have a dynamic chain if it implements static chain if it implements static binding then it also has to have a static chain point which gives which essentially captures the nesting levels and the textual containments of the various blocks right so so so essentially the difference is that in a dynamically scoped language  refer slide time 23  54  the free or the non local identifiers are bound in the environments of invocations calling environments at run time and of course dynamic binding is much more easily implemented in interpreters you just have to follow the return pointer chains so this going back to this diagram lets just this this scp stands for static chain pointer this rp stands for return pointer once this invocation is over what is going to be the current environment pointer or the base of the what is going to be the new current activation record and what is its going to be its base address so that you get from here so essentially when you exit this block this return pointer the value in this location is going to be copied into the current environment pointer the value in this is just the address to this base and so your current environment pointer will be pointing to this and all addresses will that you will be refering to will will refer to addresses which are either which are any way less than this so you don t need to explicitly garbage collected the further invocations all the previous activation records which are really not supposed to be active but have been left behind as dead wood will get overwritten right  student  p one and p two are the pointers to the p one and p two in the main program but where we are elaborating p one and p two  you have under either a under either a static or dynamic scoping mechanism when you actually read p one you will be actually creating its code segment some where and its going to be a pointer to that code segment  refer slide time 19  26  mostly its going to be a pointer to that code segment very often if they are procedures with parameters what happens is that you often for type checking purposes you don t want to put all the information about parameters basically what you want is type information and the order of the parameters right you may not want them all to be all located here what you might do is instead is have each procedure since different procedures have different numbers the parameters with different kinds of types what you might do is you might standardize it to two pointers one is a address to a code segment another is a pointer to some place where the sequence of parameters and their types and what ever information you might want to include about their types is maintained so that you can do the type change right so if there is some record which basically allows you to find the code segment of that procedure if you want right right so what it means is that under a static environment under a static binding mechanism as you compile which is  refer slide time 27  19  normally a compiled language for a compiled language as you compile as you read the token string after lexical analysis basically it s a token string you actually maintain for each each block that you have created a nesting depth it s the addresses for each of its variables variables are going to be translated by the addresses right so the symbol table variables are going to be translated so the symbol table contains type information address size this that such things right so at compile time you have this symbol tables for each of these and as you textually read the the program you actually create all this information that is essential the moment you exit after compiling p one one the moment you have exited p one one this part of the symbol table is no longer necessary and you work with this part when you especially when you are reading the body of p one right and once you have finished reading the body of pone then this part also goes away the next textually available thing is the procedure p two so a new symbol table for p two is created lets say green in color and then within p two there is a procedure p two one lets say black in color and as you read p two one you create another symbol for p two one the moment you exit p two one the symbol table for p two one is removed and you are left with symbol table for p two the moment you have finished elaborating generating the code for p two which essentially means you have finished reading it and generating code p two goes away and then you you are back with just this which is the main body of p of the program and you can elaborate the program and all references in the program could of course the symbol table here should also contain should also contain some information saying that there are these two procedures p one and p two right because in the main body of p you can have in the main body of p you can have calls to p one and p two which have to be some how type checked and similarly in p one you should have some reference to the procedure p one one because you can have cause to p one one and you would require a type checking right so so and the way you would create addresses is that at every point in the symbol table you will actually maintain what is the current nesting depth that you are looking at and for each identifier essentially something the current nesting depth is going to be equal to the length of the static chain which is going to be maintained at run time right so its just any reference to a non local variable or even to a local variable will be the chain position and a relative address corresponding to the base of that activation record right so so these static pointer chains will have to be explicitly created and  refer slide time 30  56  maintained in order to capture this innermost textually enclosing block information to resolve non local references to resolve both local and non local references its it s a uniform procedure and of course what happens in the in a dynamic environment is that sometimes so what it means is that lets get back to this diagram so what it means actually lets get back to the diagrams so now at this if you look at static chain pointers if you look at this there are actually two different static chain pointers which of course meet at the main program they are two completely independent one is this green single pointer chain and the other is this double pointer chain right okay so at any point during the execution of the program the only references i am assuming a statically scoped language the only references you can have to identifiers are identifiers in the current activation record identifiers in the activation record immediately pointed by the static chain pointer are identifiers of the activation record pointed down by this by the static chain pointer of this static chain pointer so what i mean is the only references you can have in this blue procedure or to identifiers in the blue procedure in the red procedure or in the pink procedure you can not have any references to identifiers in the green procedure under a statically bound under a static binding mechanism okay now you can think of it if you just generalize this diagram so that you have lots and lots of procedures p one p two p three p four p five all at the same level and all of them having very deep levels of nesting with more and more procedures embedded in them then your run time stack in some in when its executing some piece of code in the innermost nesting of some procedure can actually have a whole lot of pointers a whole lot of static chain pointers which are all independent except for the fact that they meet at the main program okay and at any time at the from the top of this stack in the activation corresponding to the block which is being executed at any time the only non local references are so that you actually have a collection of disjoint chains i mean they are not disjoint because they all meet at the main program but otherwise they are all disjoint so at any point you actually have a huge you might have a huge collection of static chain pointer static chains of which at any instant only one static chain is really necessary for your access in order to access non local references okay you could have a huge number of independent static chains but at any instant there is only one static chain that is actually useful to you for executing that block right is it clear i mean here the example is that there are two independent static chains and at this point this is this static chain is the only thing that is necessary right if you are consider a nesting mechanism lets if you consider a nesting mechanism a calling mechanism for this same program such that you actually you call you call p two and then you call from p two there is a call to p two one and from p two one of course there could be a call to p one and there could be a call to p one one okay then you would have to independent static chains okay and depending on which block you are executing you require only one of the static chains available for that block that s the only static chain that is useful for that block the moment you exit the block you might know into a difference block which is not textually nested which means you will require a different static chain right and in languages where there is a huge amount of side effects like most imperative languages what it means is that lot of people instead of having a huge number of parameters for their procedures they tend to use a large number of globals okay in that case for a really complicated program what it means is that traversing down the static chain for each and every non local reference can become expensive and can slow down execution right so at any so at any point what happens is so very often some speed up mechanisms are used and one speed up mechanism is the use of a display right so so a display is just a collection of registers high speed registers some how organised logically in the form of a stack which we use in some order okay and since out of a vast collection of a disjoint static chains at any point you require only one complete static chain pointing down upto the main program these static chain pointer addresses are all stored in the display in a last in first out fashion okay so the address the base address of the pink block will be stored here and then the base address of well the green is not important the base address of the red one would be stored here and and the base address of the blue one would be stored here right so as long you are executing this blue block  refer slide time 38  19  these are the only base addresses you require in order to resolve non local references fast okay you know the nesting level textual nesting level here is three if you find a non local reference at nesting level one or some such thing then all you have to do is look at the base address given here to down this display stack and look at the relative address starting from this base address just imagine if the stack actually grows and you have a reference to a global variable at nesting level twenty that means you will be doing twenty hops down the static chain before you can locate before you can access that non local variable and you don t want to do the twenty hops you might the entire procedure or function might might have no parameters nothing it might just be doing some task which is completely modifying global variables which means for each global variable occurrence that is there either for modifying it or for reading its value for each of those global variables you will have to make twenty hops down this static chain and then find the address and either modify it or read the value you don t want to do that it can slow down your execution tremendously so by the use of this display what you can actually do is you can just subtract nesting levels from the current nesting level go to the appropriate go to the display to find the base address and find the relative address from that base right so essentially what we are saying is that this this pink actually contains the address of this base address this pink base this the address of this base is actually given here similarly the address of the red base is given here the address of the blue base is given here and these are high speed registers right which make it which make accesses fast any way that s a that s a sort of an optimization or a speed up if you like its nothing more than that so the act of naming essentially does this and the which means what it means is that in the case of a dynamically scoped language there are enough overheads just translating it at run time translating the phrases at run time and accessing them but you don t have this extra overhead of a static chain you just keep following the return pointers which form the dynamic chain every time you want to get to resolve non local reference yeah so there is an extra overhead with static binding but then that extra overhead means that debugging your program is easy its possible to compile programs use them for production run whereas an interpreted programming language basically means that you will be doing on the fly not really debugging you will be doing only on the fly redefining on the fly development which may not be systematic which could be quite adhoc so the extra overhead is worth it provided we can some how speed up global accesses and speeding up global accesses means using some some method or which pointer hopping can be saved some how using a fast access so supposing you have got a nesting level of i don t know sixty twenty or something you cant you may not have actually that many registers but lets assume that you have cash which is a fairly high speed memory mechanism then you can access the cash but the point is that you are at a nesting level of sixteen you got a global variable at nesting level of one and you want to find out its address you don t have to do sixteen hops down the static chain you just have to go to the cash fifteen places down random access because this is going to be organised as an array if you are going to organize it in cash  refer slide time 45  24  so its all going to be this is all just addresses so they are all uniform right so you don t have to you can actually randomly access the corresponding base address go back to the run time stack to that base address and take the relative address from that so that your access speeds are substantially improved when you use a display mechanism and hopefully your execution will improve  student  how do you find that that x what is the address of that x  that x at compile time the x has been translated into something that gives the nesting level and relative address from that from what ever is corresponding base so you have the nesting level that s what that s what that s the job you are doing at compiling while compiling what are you doing about non local references you are replacing all all identifiers by a nesting level cum relative address with respect to the base of that block okay so your main problem at run time is finding the base of the block your relative address is there and how do you find that they are the base of the block it so happens that the difference of the nesting levels is just going to be the length of the static chain pointer the number of hops down the static chain that you have to perform so instead of doing that number of hops you actually organize those that that static chain itself as an array in some high speed memory so that you can randomly access that you so each time when your static chain changes there is extra overhead that the new static chain will have to be copied out into this display right for example here the moment i exit the moment i exit this blue block okay on exiting this blue block i am not travelling i am not going back to an activation record which is along the same static chain path i am going into an activation record which belongs to a different independent static chain so the extra overhead when i lop of this when i exit this blue block and come to this red block is that i have to completely erase out my display and now before executing this red block before continuing with my execution of this red block what i have to do is i have to copy out this static chain into the display to enable quick accesses to non local references in this red block right in the code segment of this red block  student  sir in the birth case it could narrow down to actually traversing the thing  in each case its it could  refer slide time 47  40  for example yeah when you go from here to here it just works out to that but i cant be i mean that s not something you can that can be really be guaranteed right i mean you have to atleast do one traversal down this the appropriate static chain fully before you can decide what to do and its easy to do that blindly by just copying out the display you every time exit the block and enter a new block so every time when i exit this blue block and come here i look at the static chain pointer copy it out into a display in reverse fashion and then traverse this copy out its static chain pointer and so on yeah that s that s really what i am going to do but every time i exit a block and get back into some previous invocation i have to do a complete traversal of the static chain atleast once but what i am claiming is that so all you have to do it right then you can save on subsequent hops in through a static chain by using a display right so and now of course when it comes to and the thing is that we have actually we are we are now looking at pragmatics before semantics because a lot of the pragmatics is very easy i mean the act of naming has not significantly changed implementations and in the case of recursion its even trivial i mean one important thing about naming i said was to was to provide a form of abstraction and important feature of that kind of naming as a form of abstraction is also that it allows you to call recursively automatically right so for example in an unnamed block except with explicit goto statement you cant go back to the beginning of the block from within the block with naming you can automatically make recursive calls and the whole point about implementations once you have organized your run time stack in this fashion okay you you should through your mind back to the the run time environment in languages like pascal and m l as supposed to run time environment in languages like fortran okay in if so this this briefly recalling what we did in fortran in fortran every block was independent of each other and they were all statically allocated in memory okay and they all contained persistent data and any reference was had an absolute static address okay and one consequence of that so for example if you were to take a diagram in a in a in a fortran like environment with static allocation what it means is that there is going to be a pink block code segment cum data separate from i have forgotten the color of p one red p one is going to be code segment and data p two the question of nesting of course never arose so lets assume p one one and p two one don t exist okay p two would have code segment and data and they would all be fixed for life and by life i mean the lifetime of the program  refer slide time 50  04  right so what it meant was that all references whether local or non local would have a fixed absolute address which is relative to the address given at loading time of the program relative to some base given at the loading time of the program so what that means is that you cant have recursive calls because recursive calls means having different copies of the same activation record with different values and so on and so forth so you just cant have recursion on the other hand with the kind of run time stack environment but what flip side of the coin is that is also very fast having fixed at all fortran programs execute extremely fast because they don t have these complications of allocating fresh at run time deallocating traversings pointers none of these has its so the executions are very very fast one reason why scientific programs are still written in fortran the other important reason of course is that lots of them were written in fortran and nobody wanted people are feeling too lazy to change it okay but the but the moment you allow for recursion the moment you have organized your run time environment as in the form of stack with a dynamic allocation of memory of activation records and so on and so forth you directly open up make it flexible for the purpose of implementing recursion so what it means now is if there is a recursive call  refer slide time 53  34  so lets if there is a recursive call you have this program here and then there is another sub program here which is which is recursive then what it means is that your activation record would look like would really look like something like this for over n calls over n calls to this this subroutine what you will have is essentially one activation record for well here are the globals lets say the global environment will have one activation record for the main program which with a return pointer well it s the main program which with with a return pointer well no it s the main program so there are no return pointers then no static chain pointers and then you have several activations of this recursive call with a static chain pointer pointing here and a return pointer pointing here and then lets say there are three activations of this three you are assuming the case when the recursion has been called thrice then you will just duplicate these static pointers like this and your return pointers will be like this so you can have several incarnations of the variables local references will be locally resolved non local references will go down the static chain when you exit one invocation you automatically have from the return pointer the address of the next where the invocation that you are return to right so there is absolutely no problem with the implementation of recursion in this case of course then the next the next important question is what exactly do we mean by meanings when you have functions and we will worry about that next  refer slide time 53  49  programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 38 meanings  55  08  so lets look at abstracts about functional and procedural abstracts and today i will try to define the meanings of abstracts the age old and fundamental questions still remains which we will only partly answer today and that is so whats in a name well an abstract and we still havent answered the question what what really is an abstract right so today we will answer it and i know a lot of people who read agatha christie books and who after the intial chapter when the murder is committed will go to the last chapter where the murder resolved in then they start reading the rest of the book so lets do the same thing so we have done the initial chapter of this chilling suspense and now there is this chilling suspense so the next question is who did it in our case its rather who done it in our case whats in an abstract what is an abstract and amazingly the answer is that it is an applied lambda abstraction well applied appear depends on rest of the application and we will define the notion of a closure and we will try to define the meaning of abstracts so any abstract especially parametrized ones are really going to be just lambda abstractions yeah and so this so this so the distinction between an abstract and an abstraction is then then becomes negligible yeah so so lets go about it so what we will do is something that we havent done is is that we will consider since in a functional settings these things are always easier to define  refer slide time 4  00  so we will look at the semantics of m l like functions right so so lets look at functions what ever we did we only considered mostly values and i will make the assumption that it suffices to consider unary functions and i mean there is a there is a view point in which every function is a unary function not obtained by carry okay is just that any n ary function can be regarded as a unary function applied to a n tuple and n tuple construction so in that sense infact in that sense the conventional mathematical notation for a function is also really that this is really f as a unary function applied to an n tuple right so so its we do not loose too much by considering considering all functions to be essentially unary functions so there is an important aspect of a n tuple construction before you actually apply the apply the function but even otherwise whether carried or not carried we can think of all functions as being unary right so and so then what happens is with this it also makes it more convenient and less messy to talk about functions the parameters and so on right so so lets look at the syntax of functions of course now now that we have done something about type checking what i will do is i will define the syntax so essentially there are just two constructs we have to worry about one is function declarations so f is a function symbol with a parameter x of type tou nought and e is an expression of type tou one and if you remove if you remove this type information then essentially its like a m l declaration of a function right and of course even in m l if the type inferencing has to succeed always you might have to put atleast some type information some some where in order to make sure that these are operations inside e are meaningful right so we will we will assume that we have we have fully typed information right and then of course the function call is just f applied to some expression e right i mean so this is a sort of m l syntax so we will consider mainly non recursive functions the extension to recursive functions is quite simple if you clean up the syntax a bit  refer slide time 7  14  okay so then as we call declarations and expressions we have to look at what are the free variables of a declaration so the free variables of a declaration are are just all the free variables of e except of course x which is a parameter of f right so so in the so if its non recursive then that s really what it is if its recursive what it means is that you will have to use some form of disambiguation to make sure that f is not included in the free variables of e remember that functions are also values and i mean there is a great unity operating all that so if e is if if you have some way of recognizing that its a recursive definition then what it means is that you will have to remove f also from the free variables of e okay there are many languages which actually in the case of m l what happens is this this the question of whether f is whether this if foccurs in e the question of whether its recursive or that f actually refers to some previous declaration of f is resolved in m l by by a very simple mechanism it takes the innermost reference and decides on that right so if if the name of so it looks through this and if there is an f here then it assumes that it must be a recursive definition okay so so m l has a very simplistic view of that so in which case in the in in actual m l programs this this this distion between non recursive and recursive is just is just is not very syntactic its rather implied rather than being direct but many languages like camel for example which is really a variant of m l explicitly requires that if you intend a function to be recursive then you should put a reserved world called rec in front of the definition of the function right so for example if you have a definition like this so if you have a definition like this f of x equals if x equals zero then zero else f of x plus one f of x minus one plus one lets say so if you have a definition like this in the case of m l it automatically takes this to be a recursive function okay on the other hand this might be a part of m l session in which you may have previously declared so you would have actually you would actually have something like this you may have previously declared f to be of type explicit okay so now now what m l does is it just it just basically disregards this okay and all occurences of f here refer to this most recent most recent occurrence syntactic most recent syntactic occurrence in the as part of the interpretation  refer slide time 10  00  how ever camel which is very much like m l and has a very similar syntax and is based on the same kind of principles type system type inferencing and so on actually specifies that you should if you do not if you do not include a reserved word rec here okay so this rec is a reserved word which is supposed to indicate that a function is recursive so then if you actually include this reserved word rec here then and only then camel will take this f to be recursive call to the same function if this rec were not there then what would happen is camel would be looking in the environment for a previous declaration of f and would use that as the meaning of this f okay in some sense in m l things are very implicit even though even though the designers of m l were among the first to point out that there is a ambiguity in programming languages in which you do not use which most programming languages do not use this reserved word called rec or some kind of a syntactic time which makes it clear whether the function is whether the function or a procedure is recursive in spite of that they actually have this sort of implicit implicit binding mechanism which binds this f to this whereas in camel its actually has to be made explicit by means of this reserved word rec yeah so so how ever i mean in that sense camel is some what more explicit and perhaps better so because we do not know really what kinds of ambiguities the lack of this recursive keyword can create i mean we already know how how it can create problems in for example pascal functions right so anyway so we will we will for the present assume that its these are non recursive things if the only difference in our semantics  refer slide time 12  23  if it becomes recursive is that what it means that you have to have a y combinator some form of recursion combinator just like we found for for while loops right that s that s going to be all the difference otherwise the difference between recursive and non recursive is really question of identifying what are the free variables and what are the defined variables  student  sir if e is non recursive then is f a free variable  if f occurs in e yes  student  then why should we bring since x is also been declared at the same place where f does  no the syntactic ambiguity that i was talking about if if you claim e is not so lets lets lets be explicit lets take camel like syntax where if you intend it to be recursive then you give a recursive you give this keyword rec otherwise you do not give the keyword rec so if you do not give this keyword rec here okay then what it means is that the elaboration of this dec  laration  this expression means that your environment the environment in which this expression is going to be evaluated should already have some definition for f if it does not have a definition for f what a camel interpreter will do is it will say that f is a undefined variable okay this is the non recursive case in the recursive case this reserved word rec will actually help it to identify that therefore in within the scope of this definition all occurences of f are the same as this occurrence of f and since f is a defined variable here it is not a free variable okay all this is not explicit in m l its implicit  student  excuse me sir is it necessary to have the type of x same in both the functions  in these two  student  yes sir  no no no not at all not necessary  student  sir but if we do not give this recursive rec then this f of x is going to be bound to the first function  yes it is bound to the first function and if the types of x minus the x here and the x here are different then you are going to be at a type error  student  i think sir it should be there but they are not  so if you do not intend it to be recursive and you actually intend this f to refer to something that is already available in the environment then the type checking problem has to has to be satisfactorily adddressed that s all right right so otherwise so in the case of free variables  refer slide time 12  23  so essentially all the variables in e except x are free okay if i include this explicit rec here then the free variables of this declaration are so essentially the way i am looking at now looking at it now is that the free variables of free variables of rec f of type tou nought the whole function of type tou one equals e equals e is equal to is equal to all the free variables o e minus x and f okay and the free variables of if f were not recursive then any occurrence of f in e refers to some previous binding so that f is a free variable of e okay its as simple as that right the defined vriables so of course so what it means is that if you did not have the keyword rec the reserved word rec when f could be both a free variable and a defined variable of this definition so the defined variable of this definition is just the symbol f and in so this is as far as a definitions are concerned and in the case of a function call the free variables of are called to f with an actual parameter which is an expression e is just f union the free variables of e right right so  refer slide time 19  36  so lets just go through this so firstly of course we have to look at the static semantics of these definitions so lets look at the static semantics or functions now our type environment as usual a type environment over a set of variables or a set of identifiers is is just some identifier to type binding okay so now the types here previously the types we had were were just wer just base types like integer and bool and so on and so forth but of course after having spent so much of time on the typed lambda calculus its clear that now our our types actually expand out in addition to the base types you have all the type constructions that are possible well either in the simply typed lambda calculus depending upon what your language is or the polymorphic lambda calculus depending upon what your language is right so i am leaving this unspecified but essentially it it includes something more than just the base types it includes all kinds of functions that you can create on base types and if you are considering a polymorphic case it also includes all type variables and type constructors including the for all quantifier on type variables right so what ever can be what ever can be defined from the base types using the language of the using the typed language of the polymorphic lambda calculus is included in this and of course since our functional language is an applied language is an applied lambda calculus essentially all the types of the underlined application and so on and the higher types created by the lambda abstractions on those applications all of them are included in this types so that s how you get higher order functions from a simple application right so we thought much i do lets just go through the type definitions so if e is of type tou one and then this definition of this function f with x as a parameter of type tou nought is that so the type checking rule is just this that you should be you should make sure that the body of the definition is of the same type tou nought tou one and as is given in the declaration and then what does and this definition creates a little type environment which associates with f the type tou nought arrow tou one right so it takes that so essentially the function takes an argument of type tou nought and gives you a result of type tou one what ever tou nought and tou one might be they might be polymorphic types they might be monotypes they might just be base types what ever what ever is representable right and then in the case of a function call  refer slide time 19  36  of course the type checking that is that if this expression e the argument should if it is so if your if your static type environment gives you the type information gives you the type of f as being something of type tou nought arrow tou one then this function call type checks only provided the argument to the function call is of type tou nought right so if the argument to the function call is of type tou nought then this function call is of type tou one and it type checks so essentially that s the type checking for function calls you just have to evaluate the type of argument remember that all this is done at translation time its got nothing to do with executions its all done at translation time so essentially you look at the expression from the static environment that you have already created when you are looking at this expression you evaluate the type of this expression and if this expression has a type tou nought and your environment assuming a declaration before used strategy your environment already has created for f the binding the type binding tou nought arrow tou one then this type checks this function call type checks otherwise it does not right why i have written that as a side condition side uh side conditions are usually of this form that in the main premises and the hypothesis we only use what is directly relevant to the syntax of that phrase side conditions are all the information that could some how be gathered or have been earlier gathered in through some definitions or some declarations or through applications of other rules right so this is given the type environment gamma as an assumption this is a piece of information that is inside gamma and is not part of the current syntactic phrase of our rules so the current syntactic phrase is phrase is just f applied to e which has two which has one sub expression e so the premise has to be in terms of just that e right so given this assumption gamma if you can prove this is the provability symbol that e has type tou nought e green remember remember my coding green is a part of the programming language blue is some higher something else some other pieces of information derivations inferred information and so on and so forth  refer slide time 27  28  so if you had inferred that this e given this type context gamma if you have inferred that this expression e is of type tou nought then you can infer that this call f of e one is of type tou one provided inside this grammar you have this binding for f that it is of type tou nought arrow tou one that s that s how all our rules are being going that s how side conditions are given the premises and the hypothesis the premises and the conclusion of any rule are just the syntactic sub expressions consist of just the syntactic sub expressions of the phrase that is under consideration yeah right so lets look at the dynamic semantics of course what what in in a dynamic semantics what what we often do is we transform the syntactic phrase its part of the symbol manipulation of the run time right so in the dynamic semantics we have the notion of a run time environment row right which of course is a binding from identifiers to values and now values take on a new meaning values are not just values in the base types of your language values include also values of higher types constructed through various functions okay so this values is now a loaded word i mean its loaded with as i said functions are also values functions of functions are also values and all higher order functions are values right so and of course looking at it the other way from the lambda calculus everything is a function whether it s a value or function right so this values includes all these values of higher types that you might have starting from the construction of your base types right okay and then so and the so the if you look at values originally we had just this if you go back to some of the age old semantics that we gave we had only the natural numbers but of course we showed how you could construct other structured constants from this and we also include what are called closures  refer slide time 28  11  okay and closures are just so that the other structured constants could be records tuples lists etcetera this base values could be something other than int two i mean it could have int and bool and real and character and so on and so forth but in addition we will have this thing called closures okay we will look at a closure and a closure is really nothing more than a lambda abstraction right so right so lets look at this so lets look at the semantics now so given a run time environment row over a collection of identifiers v and a type environment v because you might also have to do run time type checking in certain cases so we will always look at now we will look at the semantics in terms of both a run time environment and a static environment or a context gamma and both the run time environment and the context are over the same set of variables right so what ever a basic constraint which has to be satisfied by these two environments is is this that the run time environment should some how be type consistent with the static environment  refer slide time 29  23  so so unfortunately there is a pleasure of colons here and colons is a very highly overloaded operator in what ever i am doing this colon here is a new symbol okay which which essentially says that the run time environment over v type checks is type checked by the context gamma over v provided this colon of course is a colon that i always use when i write a logical predicate with quantifiers its its something that separates the variables inside a quantifier bound to a quantifier this colon is the colon of the type right so what we are saying here is this the run time environment is type checked by the type environment gamma provided every variable in the run time environment has a value right every variable in the run time environment has a value which is given by row v applied to that variable right and that value should have the same type that gamma gives it right so this is this is the constraint that we are working we are always going to work under i mean this ensures that things also run time check if necessary so so this new colon is really an extension of this old colon this colon this this light blue so these two this dark blue colons are really essentially the same this colon is over particular variables or values this colon is over collections of variables or values i mean its just an extension from single variables or values to collections of variables and values so these two dark blue colons one this the left one is actually an extension of the right one this light blue colon of course is just part of this my symbolism for writing predicates that s all there is nothing more to it yeah so this is the basic constraint under which we can define the semantics and so the so lets look at function definitions so function definitions so what happens at execution time you have i am assuming i have removed this capital v as a subscript given a run time environment row implicitly over some collection of identifiers say lets say capital v such that it type checks whether type environment gamma which is again over the same set of identifiers v this is a this is a function definition which which goes so this is a function declarations so it creates a new little environment okay so it creates a new little environment and what is that new little environment that new little environment which is a run time environment so functions have also all identifiers have values actual values and what is the value of the function f in this new little environment it is a lambda abstraction over some expression containing e okay such that this huge expression is of type tou one so one constraint of course is that this tou one and tou nought are specified here right so now this what does this huge expression do basically you have to evaluate you will have to give e has what are the kinds of variables that are there in e e could have bound variables namely x since we are considering only unary functions e could have x is a bound variable e could be a m l like expression so it could have let and local and so on and so forth and more and more declarations in it which which all are bound with variables okay if it is an since lets assume for simplicity now that e is non recursive so it has no occurrence of f okay then what what are the other identifiers that could be there in e those are all the non local references so e has to be evaluated assuming that all those non local references have the have values given currently by the environment row okay so now i want to look at this function declaration in isolation okay so what do i do i am saying that this function has a value which is given by a lambda x which is the bound which is the which is the which is the argument of this function so essentially parameters are going to be bound variables in a lambda abstraction so if you have several parameters then there going to be that many lambda abstractions either in a single lambda abstraction over a tuple or that many carried lambda abstractions you take you can take you re pick the two things are isomorphic  refer slide time 36  57  okay but since we are considering unary functions for the present for simplicity so it s a lambda abstraction over x and the expression that it has can not just be e because e has too many free variables which are essentially non local references so e could have all kinds of bound variables in it in addition it could have some non local references which have values in the current environment given by row so essentially if i want to specify this function declaration in isolation that means if i want to remove it from its current m l session this is a function declaration that s part of lets say a larger m l program now if i want to take this funtion declaration out of that m l session and put it lets say in a separate m l program as a stand alone m l program by itself then what should i do i should then in the current m l session extract the values of all the free identifiers that occur in e assign them the same values and then i am ready to take this take this definition out and look at it in isolation and this is what like this extract for all the free identifiers in e extract their values as given in the current environment right and create fresh let declarations which give you those values in the current environment so essentially what i am saying is if i have an m l session a simple m l session in which i first defined lets say val y equals three and then now i define a function f x lets say of type integer and lets say this is also of type integer let me be completely explicit and this is equal to just x plus y okay i am not for simplicity i am not considering any non local references then what i am saying is what is the meaning of this function this function has the meaning in this current environment and what is the meaning of this function the meaning of this function is the same as replacing this by a new lambda abstraction okay x of type int such that let the only non local reference here is y let y equal three in x plus y so essentially that s what happens  refer slide time 38  55  right it s a closure because now there are going to be no free variables in this lambda abstraction the lambda abstraction that you now get is a completely closed lambda expression with no free variables right so that s why its called a closure the reason we have to look at all abstractions as closures is precisely is also has a pragmatic basis and that is that when you look at a function or procedure as an abstraction then what you are trying to say is that its something its something that stands by itself i mean i can take that function from you for for the same purpose for which you defined the abstraction for a similar computation and i can call that which means i can pull it out of your program or from a library of programs and use it some how the only problem that is created is then there are non local references okay and how do i resolve those non local references if you had non local references in your function i extract all the values that are expected at that time and use them only then i am guaranteed that to do work right and that s essentially what i am doing by this right so so an abstraction is really that i mean its really an abstract is really an abstraction in that sense right so and and that is infact the purpose of an abstraction too i mean its something that should stand out in isolation unfortunately we have a lot of environmental variables and so on and so forth which prevent us from taking it directly so in order to give an abstraction a meaning in isolation you have to also include the information that of what ever is relevant from the environment that is that should go into the meaning of the abstraction and that s really all that we have to yeah so so lets look at function calls so that s the function definition so then as a closure of function definition then stands out by itself as the lambda abstraction and whats a function call well a function call is just this a function call if a function definition is a lambda abstraction then a function call is just an application for that lambda abstraction so it s a beta reduction okay and of course in a m l like language we in a m l like language we don t directly have beta reduction but we have beta reductions in in another form that is in the form of lets so this let if you look at let carefully its really like an application if you remember if you look at the semantics of let what we have what we what we gave as the semantics was that you create a new little environment where this x has some has this value and then in that environment you evaluate e which means that all occurences of all free occurences of x in e will take that value from the new little environment that is created okay but that s another way of saying forget about this x replace x completely by this by the expression that it denotes and keep the environment the same don t create a new little environment  refer slide time 44  46  so our semantics of let could have been alternately written out as a pure form of beta reduction with no notion of an environment okay unfortunately in in imperative languages i mean so in a purely functional language we could actually do that every variable can be replaced by its body and often out of context but in imperative language because of the possibility of side effects and various because of the fact that there is a huge amount of environment that really needs to be maintained its necessary to keep that environment information some how but otherwise the pure m l like language that we we are defining we could have defined without an environment by just using the notion of a substitution which which makes a let abstract a let construct in m l just a beta reduction a let construction in m l is just a beta redex so let some variable let x equals e nought in e is just lambda x e applied to e nought okay so any so this let actually is just is just like an applied lambda abstraction assume that there are no local references in e okay then this would be just this and we wouldn t require any environments you you could actually do syntactic replacements but we require environments because there are redeclarations of variables and so on and so forth so we used it is more convenient so but essentially a let construct is just a lambda is just a beta reduction is just a beta redex and so a function call is just a beta reduction since a function is just a lambda abstraction a function applied to some argument is just the lambda abstraction applied to that argument its just a beta reduction right so the kind of the kind mechanism that we have been looking at so far really is what is known as call by value  refer slide time 45  43  okay so essentially what what we are doing in call by value i mean i am redoing that function call i can rewrite that function call more explicitly in this fashion so that it models our further discussion on parameter passing so you got the function is called with an expression e nought then you evaluate e nought in the expression language so essentially you don t touch the function till you have evaluated e nought completely and e nought has finally got reduced to some value m now here again this m could be a i mean could be could be any higher order value okay i mean it could be a function value or what ever it could be anything okay but i am using m in order to keep it uniform with what ever we have done before as something as a as a value as a hard value that is some how available then the application of f over e nought is just the application of f on m which means that it is just this net construct where f is of course just this lambda abstraction where and of course i have to her again i have to worry about those non local references and so on and so forth right so there is a let inside let if you like what ever and having done that its just its just this yeah so this don t take this e totally a very seriously i mean this e might have non local references which means again you will have to write a let row again you will have to write this kind of an expression okay so may be i will just give it a different name i will just give it the name e one where e one equals let row over the free over the non local references of e in e in in what e yeah right where of course the function definition is of the form f of x colon tou nought equals e right so so then this is what so here what you what are you doing in call by value you are evaluating the argument completely and function call is applied only after you have got a perfect value so if the evaluation of this argument is in infinite computation what it means is that you can never return from this function call which is something we have all experienced so that brings us essentially to what is known as i mean to various parameter transmission mechanisms application in the lambda calculus in functional languages and also other imperative languages so let me just quickly go through what happens in the lambda calculus in the lambda calculus you have two mechanisms one is called the call by name these are the two principle mechanisms which have applications also in other programming languages  refer slide time 49  01  so if i take and what is what happens in the lambda calculus actually also happens in other programming languages to some extent so if you remember we considered this infinite computation we said that we said that because of replicating operators in the untyped lambda calculus you can get infinite computations even though by taking a different reduction route you could get a normal form okay now there is a theorem in the lambda calculus in the untyped lambda calculus that if you take the leftmost outermost beta redex always and reduce it and you having reduced that you will get a new lambda expression again choose the leftmost outermost beta redex i mean that means don t go deep inside to choose the redex look at the outermost levels of paranthesis to find the redex from left to right read from left to right and do this if you do a leftmost outermost reduction always which is deterministic okay remember that the lambda calculus operation semantics is not deterministic but now if i make it deterministic by choosing always the leftmost outermost beta redex to be reduced then there is a theorem in the lambda calculus which i am not going to do is that if there is a normal form beta normal form then this order of reduction is guaranteed to produce it okay and this is an this is clearly a case of leftmost outermost beta reduction and this is called call by name okay whereas if you have a call by value then you evaluate an operand  refer slide time 50  58  completely and try to reduce it completely before actually doing a beta redex so you go deep inside so essentially while you are reading the lambda expression you go you look at all the operands of beta redexes and try to reduce them an operand of a beta redex could itself contain beta redexes inside so which means you go inside and try to reduce those operands within there and so on and so forth so you actually going in inwards but you are not going too deep in in the sense that you are not going deep inside the lambda abstractions you are only going deep inside the operands in an attempt to first reduce the operands to their normal forms and then apply the beta reduction so you will always so you will always so given an application of this form you wont go deep into l you will go deep into m you will want to reduce m in order to reduce m you take you find out whether there are any beta redexes in it and in in which case you take their operands and reduce them and so on and so and try to reduce them and so on and so forth right so you always look at the right you always look at the operand of an application and try to reduce it first okay that s really what we did in call by value given an application you take the operand and try to reduce it first and then do the application unfortunately in the untyped lambda calculus this can give you an infinite computation so its not guaranteed to produce normal forms even if they exist yeah  student  excuse me sir sir if this is the why at all we use call by value  yeah because call by value can converge faster right so once you have typed lambda calculi there are no infinite computations normal forms are guaranteed so now call by name and call by value should both yield the same results but in a typed lambda calculus there is a polymorphic type you can have operators which are replicating okay so if you have a replicating operator l applied to an operand m then a call by name can produce copies lots of copies of m which means that you will have to reduce each of those copies of m individually separately later on the other hand if you decide to reduce m first itself and then you do the application when you when you even if you even if l is a replicating operator you still have a normal form without doing extra reduction on copies of m okay so its also the easiest thing to implement a call by value reduction and it converges faster especially in the presence of replicating operators so that s why an untyped lambda calculus is completely of no use to man or beast because the semantics requires that you will have to always use a call by name implementation if you want to get normal forms so therefore most functional languages use call by value right so we will start parameter passing in this context next time and we will look at other parameter mechanisms in imperative languages programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 39 parameters  58  13  welcome to lecture thirty nine so today we will discuss parameter passing so since most of the issues are pragmatic though it is really and it is also tremendously complicated to give a semantics so what will do is we will discuss most of the basic parameter passing mechanisms from a purely pragmatic view point and its possible to give a semantics but then that s the semantics of of an after thought and its not a predefined semantics in the sense that so as a result especially for imperative languages it becomes very messy and rather complicated and actually does not provide too much intrusion atleast in my opinion so but what it can do of course is to accurately pin down exactly how it should be implemented what are the considerations but how ever we will look at it from a purely pragmatic point of view its most elegantly discussed only in a functional setting or in a lambda calculus setting with the toy language  refer slide time 1  36  so as i said most functional languages use call by value which is which is that given an operator and an operand you completely evaluate the operand before you actually perform the application right now thw whole question of and as i said in in the particular case of the applied lambda calculi whether typed in the case typed applied lambda calculi which is what most functional languages are call by value and call by name give the same results but call by value is likely to give you a faster convergence to the normal form in the untyped case call by name is always guaranteed to give you a normal form if it exists but call by value might go off into an infinite computation so so that s so that s one important reason why you should introduce typing so that then you are guaranteed that you will always get normal forms secondly from a purely pragmatic view point you can use call by value which is an easy implementation compared to a call by name implementation anyway coming to procedural languages or imperative languages the whole question of parameter passing is closely is closely related to what might be called an attitudinal problem problem right so essentially given a function or a procedure call the question is and this in general could be an expression this e which is why i have put it in dark green but very often it is just a single name of a variable or or of an array component or record field so very often it is just a name of of some object and in imperative languages if you remember objects could have even simple variables actually have different interpretations for example depending on whether they occur on the left hand side of an assignment or or on the right hand side of an assignment i mean assignement in some form of the other is the most important construct in an imperative language right it and a name has a different meaning depending upon whether it has whether it occurs in the left hand side or at the right hand side of an assignment so when it occurs in the left hand side you are essentially referring to its location or its address and a variable is really like a container a container which can contain a value and if it occurs in the right hand side you are really looking at not the location but the value contained in a particular location so i mean so i made this distinction between l values and r values long ago and essentially that confusion resides here  refer slide time 5  13  so the attitude so or the interpretation that you can give an expression if it s a simple arithmetic expression or some such thing there is really nothing you can do i mean its actually still confusion there too but as we will see but so the attitude that you can take is if the parameter e denotes an r value then what you mean is that e is really a value and therefore the value is what has to be passed as the actual parameter of this procedure call okay and if you regard this expresion as denoting a value then essentially you will be using a call by value mechanism on the other hand you could regard this expression as an l value okay which means that it is it should be an expression which for which there is a possibility of that confusion between an r value and an l value if its just an arithmetic expression of the form two plus three then clearly it has no l value okay so but where it could be a simple variable or an array component or a record field in such cases what happens is that that then that expression is open to two different interpretations either as the value of the value containe in a location or the location itself okay so then here what you are saying is should i pass the location itself as the parameter after all that that expression if it contains if it s a if it could be interpreted as an l value then its just the name of a certain object right and i am passing i am passing that which object am i passing that object itself or am i passing the value contained inside that object okay so if i am passing that object itself then its really an n l value i am looking at and what you get if you take that attitude is what is known as the call by reference mechanism and there is actually a fair attitude if there is if its some expression then whether it might have an if its an expression then it certainly has an r value lets say it may or may not have an l value but then can i say that when i am calling this procedure i am not passing either the l value or the r value but i am passing the expression itself as a parameter in large parts of mathematics actually this confusion never arose okay and that s because and that s because whether you pass the expression itself or you pas the value of the expression as a parameter you are always guaranteed the same results in mathematics okay and that really has to do with the fact that whether you whether you do a call by value reduction of a lambda term or a leftmost outermost reduction of a lambda term in either case the results if they are not an infinite computation will yield the same normal form and that is the call by value that s the call by name implementation  refer slide time 8  41  you i can look upon the parameter itself as just a plane piece of text which is what i am interested in passing i am not interested in it passing its value i am not passing interested in passing its location i am really passing just a plane piece of text and this is really what happens in a beta reduction in the lambda calculus and because the lambda calculus is such that especially the type lambda calculus regardless of whether you pass it as a value or you pass it as a piece of text you get the same normal forms therefore in mathematics in mathematics there is no question of an l value therefore so the only confusion that could have been is whether you are passing when you when you do a function application whether you are pasing a value or an expression as regarded as a piece of text okay but that confusion in the typed lambda calculi since its guaranteed that you will always get normal forms and you will get the same normal form always it did not matter a call by value or a call by name implementation how ever in any programming language in which locations are also closely inter interwoven into the language what happens is that this call by value and call by name can produce different results yeah okay so so so the call by name or what what in the lambda calculus would be the leftmost outermost beta reduction rule that means you scan a lambda term the moment you find a lambda application take the take the take the operator and the operand do not reduce the operand apply the left starting from the left the outermost application has to be done first then in that case the operand itself could be something that might is capable of being reduced but you do not reduce it and when you do the beta reduction what what you are doing is substituting the entire text of the operand in place of a free variable in the body of the lambda abstraction right that s a call by name method of passing yeah okay so lets so lets look at i mean these three are the most important for these reasons actually therefore call by name is really not being used too much but you can not really justify a lot of things especially in the functional languages without actually proving for example that they will give you the same semantics as call by name so on so how ever there are  refer slide time 12  41  so the call by value is something that s implemented in pascal c and most languages most imperative languages call by name the only imperative language in which i know it was implemented was algol sixty and certain derivatives of algol sixty which well like the stanford artificial intelligence language sail i think you used call by name right but it s a very very it s a very very dicy kind of thing to implement so its quite unpopular call by refernce is of course is there in pascal and fortran and then there is also something called call by value result and i say ada question mark because its not clear what ada wants you to implement okay so its not absolutely clear we we will see the reason for that okay so lets look at this so when since call by value is if you remember the activation records and stack and so on there was a place for parameters so i wont go back to that just keep that in mind so in a call by value in a call by value implementation what you do is you evaluate the actual parameter to obtain a value so the actual parameter could be an expression including lets say an arithmetic expression containing just constants if you like what ever but you actually do all the possible reductions that are possible till you get a value right and having done that there is a formal parameter lets lets look at the general structure of procedures you will have lets say a procedure text which has some formal parameters okay lets lets call it x lets just assume one one parameter and then you might have a call to this procedure procedure has the name lets say p and you might have a call to this procedure this procedure could also be a pascal function what i mean is there is no fundamental difference between functions and procedures in pascal or in fortran because all it means is having an extra local variable to store the result otherwise they are both absolutely the same right so we will just look at them as procedures and there is a actual parameter okay and of course you have to draw the formal to actual correspondence and the whole question of interpretation now reduces to in this call should x denote the value the r value of a should x denote the l value of a or should x be the name for this piece of text  refer slide time 15  07  just a name for this piece of text but that s really all that those are the three mechanisms that we are we are really interested in looking at right so so then what do you do in a call by value implementation you just you evaluate the actual parameter down to an r value okay and assign the formal parameter this value that means you are explicitly you are performing a implicit assignment operation okay where the formal parameter is initialized to this value right  refer slide time 15  47  and then you execute the procedure or function with this initialization of the formal parameters right so now because of this particular way of going about things supposing the actual parameter were an expression that could be an l value either the actual parameter is an simple variable an array component or lets say a record field then what happens is this this process does not for example change the value of values of the i mean so within the procedure or function if you change the value of the formal parameter that is not reflected in the value of the actual parameter okay so this call by value implementation if you take a broad pragmatic view is really for doing things like value i have got a value a and i want to find sine a or cos a you know elementary mathematical functions where you are not really interested in producing side effects of course what happens with most of our programs is that within the function there are side effects due to various reasons very often the reasons are lie in pascal or the appropriate example for example pascal has a restriction that a pascal function can return only an scalar data type value supposing i have a function which actually has to return two different values what do i do so i might do some some silly thing like making one of the values global returning one value and producing a side effect within the function that s that s one possible way of doing so we do such things but what ever are the changes in the values of the formal parameter in the execution of the procedure will not be reflected back unless you actually use that actual parameter also as you modify that actual parameter also in the procedure or function so changes in the formal will not be reflected back in the actual in general in general but there are specific instances where it could right so so now so what kinds of variations you can have for example you could have so your actual parameter could be as i said its it could be a value of a base type yeah so which means that there will be no side effects possible i am i am talking of these side effects possible when ever i talk of no side effects possible i don t mean i mean side effects through modification of the formal parameter and not side effects created by explicit assignments of assignments to global variables within the function or the procedure right how ever the r the r value of the actual parameter could be a location after all a location is also locations addresses are also r values so it could be a location in which case what happens is that you are essentially dealing with pointers and then all assignments to formal parameters are immediately reflected in actual  refer slide time 19  20  and infact if i were to write various lisp lisp based programs in pascal for example i want to take all these scheme functions elementary scheme functions and programmed in pascal so what would i do well i would define a function call const which takes lets say a value and a pointer to a list and i would actually const it and do things so and that pointer to the list would could just be a call by value parameter but since it s a pointer and i am going to go through this process of first creating a new location and then modifying the pointer and so on and so forth i will actually get side effects the end result that i get would actually be a new list with that new element const to it so under cases where the r value is actually an address you could you could actually so what it means is that there is a global there is a heap based data structure which you are modifying some where we are not actually modifying the value of the location in the formal okay but you are modifying some some large structure that is pointed to by this r value and once you have modified that you have essentially got you essentially changed your global environment on your return from the procedure you have got a modified global environment right so for example lopping off the head of a list i mean i could just pass the list as a pointer okay lop off the head by deallocating the head putting the pointers in some particular way and then just get back there is ostensibly there are no side effects because of a call by value implementation but the new list that i get is really the tail of the original list and infact this is what is used mostly in c to do to produce side effects c uses only a single parameter passing mechanism and that s call by value and the way it uses call by value even though it for normal variables call by value produces no side effects the way it produces call by value is that it uses value parameters through explicit referencing and dereferencing operations so you can actually pass the value which is the pointer to something okay dereference that pointer change the value through an explicit call by value mechanism and get back and you would have produced side effects though the actual parameter would not have changed its value right it could still be a pointer to the same location but you have changed the contents of that location and so you have produced a side effect yes so that s how c for example does not allow for any other parameter passing mechanism it allows only for call by value but since it has primitives for referencing and dereferencing you can produce side effects through the call by value mechanism right so so there is another variation of the call by value parameter mechanism and that s called the call by value result right and that s very simple you just proceed exactly as in call by value okay the only extra constraint is that your actual parameter should also have an l value  refer slide time 23  20  right so if if you if your actual parameter is some lets say some arithmetic expression two plus three is what comes to the mind always then obviously call by value result is i mean its not going to i mean it doesn t satisfy this constraint but if the actual parameter is something which also has an l value then what i can do is proceed exactly as in call by value and after the and just as there was a there is a copying in phase in a call by value mechanism you first find the r value of the expression and you find the r value of the expression and then you actually do this assignment to the formal parameter right so this is the copying in phase so just like you have a copying in phase after executing the procedure you have a copy out phase okay so where you will actually copy back values from the formals to the actuals right so a call by value result is just call by value plus this extra step which is firstly with this constraint that the actual parameter should have should be a should also be a location and after executing i mean its its because you have to copy back values from the formal to the actual and you cant do that in a memory based system without the actual parameter actually also denoting a location also having an l value so then this copying back is done and essentially you have created a side effect but in a clean fashion i mean in the sense that there is no really there is no great interference between the globals and the body of the procedure i mean the procedure still remains as an abstract but you are copying back the values at the end of the execution procedure all intermediate values of the that the formals take within the procedure will not be reflected in the actual parameter okay so that s that s the call by the call by value result is as i said its not actually used in any programming language except that aida has three kinds of parameter mechanisms okay so in a typical aida procedure could have what are known as in parameters okay which are quite faithfully reproduced by the call by value mechanism okay then there are in out parameters which means there is a copying in phase of these parameters and at the execution of the at the end of the execution procedure there is a copying out phase also okay and then there are also plane out parameters okay so you can think of this plane out parameters as a form of what might be called just call by result i mean there is no copying in phase for these out parameters they stand lets say essentially uninitialized when the procedure begins execution but at the end of the execution there is a copy out of values and you have essentially i mean so you don t have a copying in phase but you have a copy out phase so you can call that a call by result mechanism if you like but the whole point about the aida language is that it does not specify how these things have to be implemented it is just a generally accepted conjucture that this is what the designers meant okay so that s why there is a question mark there now what actually most most languages from fortran always tried to implement was always provided was what is known as a call by reference mechanism which is what is which is the variable  refer slide time 27  48  parameters in pascal so what happens is you actually assign to the formal the l value of the actual parameter and then you execute the procedure with this proviso so that when ever the formal when ever the r value of the formal parameter is required you take the r value of the corresponding actual parameter and when ever you want to change the value of the formal parameter you actually change the value of the actual the corresponding actual parameter so the formal parameter or what ever is regarded what ever is the space reserved for the formal parameter in the activations record is just an address to the actual parameter and within the procedure when ever it s a formal parameter that is that is being assigned or being used whose value is being used actually the value is not taken from the formal parameter but you go go down that pointer to the to the location where the actual parameter is and either use the value inside that or modify that okay and this is a this is so the call by reference mechanism is what was used in fortran a long time ago it is used in pascal through the val declaration is there in algol sixty is there is there in so many languages and its there in c by virtue of the fact that c allows referencing and dereferencing and but it is not explicitly provided by the c language the only explicitly provided mechanism is the call by value mechanism right so so lets look at a few examples of these things so supposing you have so i will use a pascal like syntax throughout so lets lets consider just this trivial example taken from an elementary programming course on pascal so i have this procedure swap with two with two formal parameters x and y decalare integer i have an integer an intermediate variable t also declared to be an integer and then i have this standard swap procedure right and now i call this procedure with  refer slide time 30  35  two actual parameters a and b okay and now by default if its if this is a pascal program then this x and y are implemented as value parameters so what it means is that first there is a copying in phase so the values of a and b the r values of a and b are stored in the locations corresponding the locations for x and y and then the values of x and y are actually swapped okay and when you get out the values of a and b are still the same old values right so that s okay so there are no changes to the values of a and b so the standard thing to do this so what you could do is you could implement it by a call by value result mechanism supposing we extended this the pascal parameter passing mechanisms to to value result then what it means is that i would assume that the body of the swapped procedure is the same except that i have made this change i have declaraed x and y to be value result then what happens is that in the copying in phase x and y are assigned the values of a and b then the values of x and y are swapped in the procedure body and then there is a copy out phase where the actual parameters are assigned the values of the formals so that s what i mean by a clean interface i mean you can look upon this i mean  refer slide time 32  21  if you look at the procedure as a single entry single exit black box okay with the parameters forming the interface then it s a clean interface in the sense that there is a copying in which corresponds so the the transmission of the parameters is through the through the interface for through the copying in position and then there is a output through the copy out phase okay and the rest of the program while this procedure is executing essentially if there are if globals are not referenced globals are not modified and so on and so forth then what it means is that this procedure stands alone and can be used by others as a purely as a distinct complete functional unit in itself as an abstraction in the true sense of the word right okay on the other hand of cousre what we normally do since the value result mechanism is not available in pascal what you do is you usually implement this swap by reference right so you actually define this these two formal parameters to be var parameters or reference parameters and then the mechanism goes something as follows so yeah so x is assigned the value of a pointer to a which means the address of a y is assigned the address of b okay so this type information here is only refers to the type of the actual parameter and not to the type of the formal var okay all that you are saying is that it should x can be a pointer to an to an integer actual parameter right and then after that what happens is that the body of the procedure executes actually in this fashion you dereference x and take that value and put it in t and the act of dereferencing x means going to the variable a taking its value and assigning it to t then then dereference x that means go to a and to it assign the value of the dereferenced value contained in y so dereference y which go to the location b and copy that value into the location pointer two by x and then of course copy the value of t into the location pointer two by y okay so the side effects are immediate right so the side effects are immediate unlike lets say the call by value result mechanism so  refer slide time 35  15  so so essentially if you look at these three mechanisms then as long as you have referencing dereferencing primitives available what you could do is you could all effects of call by reference can be captured by call by value and which is what c does and which is what a lot of which is what many people do in pascal because of pascals peculiar constraints on what functions can return  refer slide time 35  50  okay either you define it as a procedure or define it as a function return a pointer parameter to some structure right i mean then you will get the same result yeah so so so essentially all the efffects of call by reference may be captured by call by value but provided you have these referencing and dereferencing primitives already in the language otherwise of course you cant right now in the absence of referencing and dereferencing primitives what we have is of course that the call by value can produce no side effects except explicit assignments to globals and so on and so forth and but all call by reference side effects can actually be captured by call by value result and the only difference is  refer slide time 36  57  this effects are not immediate in a call by value result mechanism the effects are delayed i mean they are delayed till you actually are exiting the procedure right but how ever there is there is a catch to this which actually most books do not point out okay and the catch is here and this you can capture all effects of the call by reference all side effects of call by reference by call by value result only provided all the actual parameters in your procedure call are distinct if there is if you have an elliasing problem okay you have you have actually used okay lets look at this so if you have an elliasing problem then there is no guarantee that the effects of call by reference will be captured by call by value result and vice versa there is absolutely no guarantee that the effects of call by value result will be captured by call by reference okay and so here is this here is a simple example which actually does this the important thing here i mean here so look at this so this is this procedure presumably takes values x and y which are lets say integers and then it it gives the quotient and remainder of the input values x and y and stores them also in x and y because assuming some how that you don t require the x original x and y and you want only the quotient and remainder then what this procedure does is it some how does this but there is nothing which prevents me from calling this procedure with x and y being the same being the same l value right then actually what will happen is this order of evaluation and the last statement which modified a will is what is going to take effect okay and since the effects are immediate well in order to buffer the effects being immediate i have used this intermediate variable t and used it throughout  refer slide time 40  07  otherwise i might have used y here but so essentially what it means is the result of this procedure call is going to be that a will have the quotient which means one if a is not zero okay at the end of this procedure or it wont have that may be it will have a a a is equal to x x is zero yeah it will have right it will have one i have luckily programmed it right okay now i under the value result implementation what happens is that you here at the end of this procedure x actually contains the quotient of the original a and a which is one y actually contains the value zero but depending on in what order you do the copy out phase it will be reflected in this actual parameter so which means supposing you assume that all sensible compilers and run time systems do the copy out phase in the order because parameters order are important always so if they do it in this order then essentially what it means is that a will have the value of y which means it will have zero in this case you will have a equals zero and in this case you will have a equals one right which is an excellent reason when not doing this kind of programming  student  sir there will be another problem suppose inside the procedure we use the actual parameter also then also  once you use the actual parameter inside the procedure then there is confusion confirmed okay so then lets not get into those complications this is just to point out the difference between these two in effects which is not usually pointed out in most books on programming languages most books actually claim that the effects of call by value result can be captured the effects of call by reference can be captured by call by value result and its not entirely true yeah okay how ever so since but call by value result is a very clean mechanism in the sense that it doesn t give you unnecessary headaches how ever the point is the problem really is what happens when you have to pass really large structures i mean whereas call by reference actually can be can make things quite complicated especially with array referencing array index computations array index modifications inside the procedure and so on and so forth it can it can make things quite complicated and reasoning about in a call by reference environment can be quite tricky it means debugging also could be tricky right so how ever call by reference is actually the most  refer slide time 43  57  economic way of passing large structures huge arrays a bitmap of yourself for example to be transformed some how into may be to make you look handsome or something but what i mean is you want if you have if you are going to pass large structures then you would also like to be economical about your usage of store of the store so what you don t want is what you don t want is to create an output which is another large structure so very often you just want to create the side effects on that structure itself and how those side effects available to you at the end of execution of the procedure right so when you if you have really large structures like a million bits by million bits passed as a parameter then the copy in phase and the copy out phase are going to be extremely expensive so even though you might get the get your results through a call by value you might get correct results through a call by value result mechanism its actually faster and more space efficient to use a call by reference mechanism to produce the required side effects right so so that s what happens with call by reference so this is one of the reasons why call by reference is most popular because even for large structures all you require is a single address that s it as a parameter so your parameter will just contain a single address to the start of the structure and that s it so whether its large large list structures or large array structures or record structures it does not matter one address is all that you require a single pointer okay so in so then lets look at the call by name mechanism so in a call by name mechanism essentially what we are saying is that a formal parameter is just a name for the piece of text which comprises the actual which comprises the actual parameter right so what you do is you go back to the lambda calculus and try to model the behaviour of the beta reduction there exactly in your programming language so how would you do that so that means before executing the procedure i replace all occurences of the formal parameter in the procedure body by the text which is the actual parameter okay so essentially the effect of a call by name is to produce a macro expansion okay you produce a macro expansion except that just as in the case of a beta reduction you have to be careful there should be no free variable captured okay so essentially if you look at so in the of course in a compiled language this may not be such a serious problem because whether whether so every variable will have a would have a would have an address determined by its base of its activation record and the relative address with respect to the base of its activation record this problem may not be so serious at run time for a compiled language but looked at as a form of a beta reduction being modeled in an imperative programming language what it means is that so you have to replace all occurences and of the formal in the procedure body by the text of the actual and to avoid  refer slide time 47  05  free variable capture what you have to do is you have to rename all local variables in the procedure by an alpha conversion mechanism right but in a compiled language that part of it could be automatically done but what happens is that in the case of a essentially the resulting procedure when you replace all these formals by the text of the actual expression the resulting procedure should be executed as an unnamed block in the calling environment that means the procedure call is just an abbreviation for taking the entire body of the procedure after replacing all the occurences of the formal parameter by the text of the actual parameter so the execution of the procedure call just has the meaning that it is an unnamed block a procedure call is just an abbreviation a parametrized abbreviation for a unnamed block that has to be executed at that point with a substitution of the parameters by the text of the actual parameter and of course ensuring that there are no free variable captures so so the so this so this aspect is what really a beta reduction in the lambda calculus and that is what is obtained by a call by name mechanism and since all the execution is in the calling environment in the environment of the procedure call all side effects are immediate okay and you can actually pass large structures because you will just be giving the large structure a name  refer slide time 49  05  and passing it that s it so depending on how your run time environment deals with large structures i mean for example arrays regardless of what language you are looking at arrays are still just a pointer okay so your side effects will be immediate and you don t have this extra overhead of what happens in i mean you don t have this copying in copying out all that the effects on large structures are also captured immediately array dereferencing or array indexing and so on the only problem with this is that reasoning about the program in about the program from its text can be quite a problem right and but of course so that is so this is so and there is of course another effect which call by name and of course there is no guarantee that call by name is equivalent to call by reference or any other any other mechanism for that matter all these mechanisms are different because what happens is that its different from call by value because in call by value the parameter is evaluated once to a value and that value is used throughout all references of the formal within the procedure in the call by name because of the textual replacement that actual parameters evaluate it every time that formal is referenced inside the procedure so what it means is an expression like a plus b passes an actual parameter to the procedure so that a plus b can mean different things in for different references of x in the procedure because you may have modified a between two diff  erent  two successive references to the formal parameter okay so so in general for imperative languages this is this is not the same as using a call by value mechanism it means at every it means repeatedly evaluating that expression every time that formal is encountered you execute the text corresponding to the expression and essentially reevaluate that expression but you reevaluate that expression in the calling environment further this call by name mechanism can produce different effects depending upon whether your language is a statically scoped language or a dynamically scoped language because then you are then your references because your textual the text of your parameter really is a consists of non local references so then depending upon where it is being called those non local references have different bindings okay if you call it if you have it in the environment of the procedure call if you have it in the environment of the call then in a statically scoped language the parameters refer to the innermost enclosing text innermost textually enclosing block whereas in a dynamically scoped language they refer to the binding in the innermost calling block so the two so even call by name depending upon whether you are talking about a statically scoped language or a dynamically scoped language can produce different effects right okay lastly i will just there is just one more thing and that is that many languages like pascal have procedures as parameters yeah and here there is no question of whether its call by name or call by value or anything of this sought so supposing you have this set of procedures this this main body where of course the main body has this call to the  refer slide time 53  05  procedure p which is blue the main body has two procedures q and p q is black and p is blue the blue procedure p has a function inside it the black procedure q is a procedure which calls a function as a r as a parameter okay and this r is also a just a formal name right and then lets say there is a call to this r i here r i is used here and now what happens is if you look at this call there is a call to p and then within within p there is a call to q with the function f passed as a parameter so that means this formal parameter r now refers to this function deeply nested inside this p okay so the question is  refer slide time 54  23  what is the environment of non local references inside the function okay the point is that any in a pascal in a statically scoped language this x should refer to this pink x here in the main body any reference to i here should be this blue i here q itself has an x and an i this i her this black i here refers to this black i here but when you call this function with this i here then you are actually using this blue i within q so you have to some how create the static environment suitable for f in order for f to be executed at this point and that is very simple what you do is at the point of this call for a function or a procedure parameter you also send along with the parameters and so on you send the static chain pointer address for the current base so that all references to i and x within this function would refer to the non local references in keeping with the statically scoped structure of the language okay i mean this is a separate but this is a very weak parameter passing mechanism and actually creates more confusion than it solves but its an excellent way of for example programming higher order functions like map atleast first level higher order functions like map and what ever list based functions which have to be applied over the entire list or for example solving you can have a newton ralphsons method in which the actual method is called as through a parameter the actual function could be a trignometric function or a hyperbolic function or could be a polynomial or mixtures of these things you could call that as a parameter and that that s the reason it was introduced in pascal for doing such kinds of computations independent of actually what the value of the function is so you can call newton ralphsons with this function f and may be find a fixed point solution but what it means is along with a function name and the type checking becomes problematic pascal is the the rules of pascal are inadequate to do to adequately type check this mechanism because pascal tries to save on your effort by saying that you don t need to declare all the variables all the parameters but then if you don t declare all the parameters how are you going to type check pascal thinks its being more flexible by not allowing parameters so that now you can actually call it a different places with a unary function or a binary function or a ternary function but then you cant type check those things and it violates the static type checking rules of pascal that that at compile time every thing should be type checkable it violates that basic requirement so anyway its not a very popular thing but it is a way of programming a higher order functions in an imperative language programming languages dr.s.arun kumar deptt of comp sc & engg i.i.t delhi lecture 40 the future  52  34  welcome to lecture forty and lets look at the future or rather lets look at all that you have not learnt so far so as i said the field of programming languages were broadly divided into three syntax semantics and pragmatics so lets look at each of them the story of syntax which actually started with chompsky finally ends with having found the murderer so you can get linear time deterministic parsing algorithms for any language which consists of regular or context free and or context free productions and boiling down to basics what that means is that as long as you don t have anything more complex than paranthesis matching you are safe with and you can get linear deterministic linear time deterministic algorithms that s that s really what it all boils around to despite all the complex normal forms that might be that might have been invented and so far and so far it doesn t really matter and what is so you have looked at parsing algorithms for example the p l zero compiler and so on and so forth and the next extension which is of course already available in most unix systems is that of a parser generator and that which uses the fact  refer slide time 1  06  that any firstly given any notation my first my first aim would be to see if can just convert it into grammatical rules and the extended bnf notation is one that is directly generated by a context free grammar so where you can you can actually so you can so the lex and yacc programs which are available on any unix system the essentially are parsers for the extended bnf notation so you take any grammar so take any grammar given its production rules using braces and square brackets and so on and so forth its possible to generate a parser  refer slide time 2  50  automatically from the grammar rules of course the extended bnf notation is just one possibility the other possibility is to use pascal like syntax diagrams its possible to use one i mean there is a one to one correspondence between the extended bnf notation and pascal like syntax diagrams and what it means therefore for to construct what it means to construct a parser generator therefore is to write a parser for the extended bnf notation itself regarded as a language which generates syntax diagrams as graphs okay and that is automatically possible and so that s really what lex and yacc do mainly what yacc does that this specification of lex for lexical analysis of token generation if you like can also be done automatically and that requires no more power than that of a context free grammar so you can use the same notation for both token generation and for parser generation right so what it means is that you just give as inputs the production rules in an extended bnf notation and the parser generator like the lex will produce tokens for the individual for the individual syntactic categories elements and yacc will take that and produce a syntax tree syntax tree generator yeah so then you just have to introduce code generation and so on and so forth there are methods of doing automatic code generation which are not very perfect but these two are fine and are actually used so the story of syntax is more or less finished its well understood that for context sensitive grammars or higher grammars of which are which are more which are more powerful in a certain sense which are more powerful not in the sense that they can generate a large number of sentences but in the sense that they can generate a large number of sentences but in the sense that they can generate restricted class of sentences after all grammars are a means of control which allow you fine levels of restriction so for anything like a context sensitive grammar or a type zero grammar what it its more or less understood within the community that probably you wont get such good algorithms as you have for context free grammars there was an attempt in algol sixty eight to what are known as fan one grammars but it it did not it did not proceed very far okay so the next thing is semantics so what we have looked at in terms of semantics is rather than give algorithms after for every algorithm that you give i can give a thousand variations of the same algorithm rather than give algorithms give a simple collection of rules which are some how syntactically motivated which provide the minimal framework on which a algorithm should be based right so you can use this for both static and dynamic semantics and if if your semantics if your static semantics is structurally inductive then what it also means is that all context sensitive grammatical  refer slide time 6  27  and syntactic information of which types is one can also be specified by a static semantics there have been methods for specifying semantics in a more within the domain of a context free grammar itself and an important contribution in that respect is due to don kunit called atrribute grammars where he took context free grammars as a bases as a framework and with each production you associate a semantical rule very much like something we do but he encoded it in the form of code generation rules to generate code and idea was that now that you have a parser generators you should automatically also do code generation by using those attributes so a lot of what kunits kunits work on attribute grammars is actually used in his software for text formatting called tech which is really like a massive compiler it generates code for in a device independent fashion and he is used to a whole lot of including his own parsing algorithms most of the table driven parsers the best known parsing algorithm with one step look ahead is due to kunit for context free grammars it s a bottom up parser he has used all this so an excellent application of programming languages compiler concepts in something that is really not got anything to do with programming languages and compilers are text formatting programs so tech is one examples cribe is another the eqn on the unix systems is the third they are all methods of coding coding notation into context free grammars parsing them and then generating code which is the formatted output i mean which will give you the formatted output so there is a higher level form of using these languages and grammars and semantical rules attributes which you can use for applications outside just the domain of the programming languages so these are not these are not these are very general methods transition systems are very general you can use it to describe anything the notion of grammars and syntactic syntax directed translation or syntax directed semantics is also very general notion which is going to be important when ever you are trying to automate any piece of software so whether it be automating symbolic computations in mathematics automating proof s or doing just plane text formatting or doing hypertext translations or trying to map graphic images onto something one good mechanism which people are followed with fruitful results is to some how transform the whole problem into a grammatical problem and then into a semantical problem and use the principles of compilers construction to actually solve the problem in some satisfactory fashion of course what in order to find unit you might have to introduce heuristics and so on and so forth but what it means is that what ever we have done is not very very restrictive when its something that has a wider applicability and has been used by several people actually to do for example the image processing text formatting infact the design of all user interfaces for all kinds of software means first encoding the interface into a language writing a translator or an interpreter for the language and interpreting and executing it right so  refer slide time 12  01  so in terms of application it goes quite far and the restricting as is to semantics itself what we have what we have specified is what might be called operational semantics essentially the fact that we didn t have to describe algorithms we just gave the minimal amount of information in terms of rules and then you can construct your algorithms based on that makes it an operational semantics because it gives execution time behaviour in a step by step fashion we had one step transitions we had many step transitions right so it is really operational the fundamental what makes it operational is that you are actually considering a step by step transformation of some notion of a configuration so you are looking upon the program as a transducer you are looking upon each construct of the program as a little transducer and a program itself as a complex transducer made up of little transducers which provides transformations on the input right so its operational in that sense the other story is that you might you can require semantics as being denotation that means you can look upon every program itself as a function from some domain to another purely as a mathematical function which means you are not looking at it step by step transitions but you are looking at just one feature what is the input to the output relationship of this program by passing all the intermediate information that might be available right so what is an her again it will have to we would like to do it in a syntax directed fashion so what you want to look upon is each language construct is denoting a function and a complex language and a program therefore which consists of language constructs some how some how connected together we will look upon them we would like to look upon them as functions which some how are connected together to give you one large function right so we would like to express the meaning of a program as a function in terms of the functional meanings of its components so again in a structurally inductive fashion in particular what this means is that we have to be able to account for the semantics of loops and recursion in a perfectly syntax directed fashion purely as functions essentially as functions which compute a fixed point functions which yield a fixed point right so this is this is a functional semantics or mathematical semantics a denotational denoatational comes from the fact  refer slide time 14  50  that you are talking of a language construct as being a syntactic object which actually denotes some abstract object just like a numeral denotes a number in the same way you want a language construct as just a syntactic representation of an abstract function in our mind right the other the other thing which is of some importance is is what is known as axiomatic semantics and there are several flavours of axiomatic sematics but principally what you have looking at in axiomatic semantics is that you want logical rules of inference for reasoning about programs in a language so you here again you want a syntax directed logical rules of inference the various flavours of axiomatic semantics are that these logical rules of inference so when you are talking about reasoning about a about programs then you are talking about reasoning about the behaviour of programs and you require a language in which to express the reasons express your reasoning about the program one possibility of course is first order logic and infact a large part of pascal was axiomatized by hoare and wirth in nineteen seventy in nineteen seventy five or so and it and this their logical rules also influence back the design of the language in order to make it clean infact the problematic constructs in the language are those that they did not axiomatize like variant records types its clear that at that time they didn t have a much of a clue as to how to take care of those and those indeed are the problematic constructs right the other possibility is to use what is known as equational logic but we will not worry too much about it but we but its important for something later and the fundamental thing here is that the fundamental tool here is the use of invariant properties  refer slide time 16  10  to develop prove verify correctness against a specification where this specification is also in the logical language which you are going to use to reason about programs right so whether its recursion or loops or anything what you want to develop our rules of inference for reasoning about the correctness we are not at this point we are not interested necessarily in specific functions we are interested may be in broad properties that the program should satisfy so express the broad properties as predicates in some language it turns out that first order logic is not a sufficiently powerful mechanism for example you will have to have first order logic augmented with mathematical induction in order to do reasoning but the moment you introduce induction then you automatically become you automatically get into the domain of a higher order logic because mathematical induction is not a first order logic specified predicate so there are problems about expressivity of the properties that you are interested in many of the properties that you are interested in may not be first order and they might require higher order predicates there is an extra complexity by introducing another language ecen though it s a logical language the other possibility is to actually do an axiomatic semantics within a single language framework have a specification language which is a superset of your implementation language have the notion of semantic equivalence as defined from an operational or a denotational view point and do the reasoning as equations within the same language that is another possibility that is being explored right and so but this the use of invariant properties for reasoning essentially about imperative programs that means reasoning about control which can change state is perhaps the most important reason why you have used an axiomatic semantics method right and of course the moment you have two or three different kinds of semantics there is of course another problem of mismatch of the individual semantics so then you have the extra added obligation that you have to prove that the three semantics are mutually consistent right and you have the extra constraint that the presence of all kinds of strange properties that you might have in your operational semantics the other semantics actually give you all the properties you are looking for otherwise you may be never able to prove a program is correct there might be certain properties which which are so inrinsically operational so its not just consistency that you require that you require between the various semantical formalisms you also require a completeness that every property that is expressable in operation can be captured in the denotational framework or in the operational framework if i am to be able to prove all the properties correct if i have to prove about all properties of the program that i am interested in then they have to be some how expressive they have to be mutually expressive and that is what is known as a fully abstractal it s a full abstraction problem so you have to prove not only that the semantics are consistent but there isnt so much information hiding in the operational semantics that you are not even able to prove certain properties in your axiomatic or a denotational framework right so so  refer slide time 21  13  so there is a vast body of knowledge on semantics and then when you look at pragmatics what we have essentially looked what we have essentially seen is that you have where various dynamic and static storage allocation mechanisms you have dynamic and static scope and binding mechanisms which we know how to deal with well you have essentially heap stack heap and stack management management of the run time environment which essentially consists of the heaps stack and the code segment may be how is the data attached to the code segment right then we also know about symbol table management at translation time and it so happens that this is infact all you require as a basis for implementation it s a matter of deciding now for any few atleast for the language construct for the language construct that we have not studied so far and for the language constructs that have evolved over the last twenty years it seems largely a matter of decision making for a given data item given this properties of the language whether you should store the data item in the stack in the heap or with the code segment  refer slide time 22  00  given the nature of any construct how how much information is available at compile time therefore what other information is lacking which should therefore be checked at run time how many what things can be checked at compile time and therefore they don t need to be checked at run time these are the basic implementation issues which we have looked at and which actually govern the extra what ever new language constructs probably may come up right so essentially you have to look at the nature of the language whether it is statistically whether it s a static language or a dynamic language look at whether there is recursion in it in some form or in more than one form for example the while loop is a form of recursion but implementationally it doesn t matter the while loop can be regarded as being different from recursion because it does not mean creating new activation records but semantically the while loop can be regarded as being another form of recursion infact it s a tail it s a form of tail recursion right so by recursion in a at a in a pragmatic sense we actually mean recursion syntactically determinable recursion and the and essentially from a given construct and from from the given nature from the given language what is the kind of information that you can obtain at compile time or translation time based on that you can also you should you can also decide is it most suitable to have an interpreter for that language than a compiler but the point is these these days for any language you will have to have both an interpreter and a compiler essentially when when you go into debugging mode of a compiled language like pascal you are essentially interpreting the language but you are interpreting the language after that after the after all the information that is translation time extracted all the information that can be extracted a translation time has actually been extracted if you take if you decide if you take a language like m l or lisp which is usually interpreted eventually if you are going to productionize it you can not afford to run it in an interpreter mode if there is some large piece of software which has to be run repeatedly it can not be run interpreted because what what it means is this manual intervention and there is manual intervention where its not necessary what you would like to do is compile the language compile the program after having developed it so we use the interpreter mode for developing the program correctly and testing it out and after that you compile it into an executable or an object code and run that object code so essentially a part of our programming environment for any language is that there is that fine mix at development time you want an interpretive mode to be readily available at production time you just want a object code you want a compiled version of the program to be executed right so so that s based on the nature of the language what it therefore means is that what what part of the language can be readily interpreted what part of the language so essentially what parts of the language give you a confirmation at compile time what do they with hold from you at compile time and therefore what has to be obtained at run time right it should be the symbol table for example be present at run time it s a important question a pascal symbol table is never present at run time and its its an important question a pascal symbol table is never present at run time but in a in a very in a dynamic language like this you would probably have to maintain the symbol table at run time right so if its if its going to be a language which does not allow for a static type checking mechanism requires only dynamic type checking then you will have to maintain all that information at run time right so  refer slide time 29  26  and then as far as the nature of data is concerned essentially your your your the basic design decisions are going to be is it static statically determinable data can i determine types sizes bounds at compile time is it dynamically created data or is it data that is persistent depending on that depending on these classifications i essentially decide whether to store it on the stack the heap or the code segment right so and infact the pragmatic point possibilities are not so high except when you move from our essential foint normain architecture to a different architecture the other possible architecture is that you might want to move it into or that of parallel architectures where you have a little foint normain machine with its own local memory and connected through networks of connections right or a completely non foint normain architecture may be a data flow architecture in which case you actually create new pieces of automation dynamically may be okay but those are the those are the other possibilies that essentially within the framework of this of a single or multiple cpu sharing some memory essentially these rae the only things that you can do it s a matter of deciding between these possibilities right lastly we have to look at language features and that s where most of the development has been in the last twenty years in the last fifteen years may be so if you look at language features there are two important features so if you look at language features well we have looked at basic language constructs basic data and data structures basic notions of control in imperative and functional languages and we have looked at essential abstractions in expression and command language and we have looked at scope issues right infact scope is a sort of overriding under current throughout the discussion ever since definitions and declarations came in ever since the issue of naming comes scope becomes an important issue and scope is actually a very if you look at scope its actually fairly clued in the sense that  refer slide time 31  06  it either provides you visibility direct visibility and complete freedom to deal with a name or it completely hides the name and allows you no access to the name and a name of course represents some object either a data object or a control object right so one thing that you want to do is when you look at it from several view points just like you have control abstraction the other possibility is to have data abstraction which means you look upon you group together declarations regarded as a single unit as an abstract of definitions and there are there are good reasons to to deal with this one thing is that actually this is a contribution which originates the language simula sixty seven which is a descendent of algol sixty but simula distinguishes itself in two important for two important features one is that it s the originative of the class concept okay and a standard biline in any decton implementation which had simula sixty seven was a poster which said simula has class and this was there in seventies the whole idea is that you actually you group together structured data and all the operations that can that are defined on that structured data in one single logical unit okay and that was the simula class except that it did not provide too much difference in visibility they use a standard scope rules but now when you encapsulate it with a name you get the module facility of modula and you get the classes of c plus plus or smalltalk which actually which actually elaborated on that similar concept on the on the class concept of simula and provided the necessary abstraction in pragmatically what all that they did in smalltalk was that they provided the necessary abstraction by allowing you not a direct access but an indirect access through pointers with with permission encoded in the access through pointers so pragmatically it  refer slide time 32  20  was not a very greatly what it meant was that everything the whole but the philosophy was important in the sense that you have when we talk about an integer we are not talking only about the piece of data which is an integer we are also talking about all the allowable operations on integers for example you can not exort two integers well in c you can but what i mean is you can not logically exort two integers and so along with integers comes the operations that are associated with integers addition subtraction multiplication division excluding division by zero and so on and so forth so when you there is absolutely no reason why we cant lift the basic notion of a data type from basic scalar data types to higher data types to structured data to data structures and when you bring in the abstraction what it means is that you regard a data structure primarily as an instance of an abstract data type so an abstract data type is just a collection of is just some structured data with with the operations associated with that structured data grouped together as a single unit right and pragmatically what if you look at the classes of c plus plus all that they do is that the the the struct construct of the c or the record structure of pascal has just been elevated to deal with classes so since there is a fundamental unity between data and control there is absolutely no reason why i can not generalize a record structure in pascal where a field of the record is a function okay and the record field specification gives me exactly the kind of visibility that i am looking for the moment i specify the record name i get access into the fields of the record so similarly the moment i specify the data abstract the abstract data type name i get access to the functions inside that abstract data type but before that i do not have any access to it okay so now for any abstract data type which you can record atleast pragmatically you can regard it as a generalization of pascal records where there is a unity between data and functions so a record field could be a function right when you look upon it that way then if you insist that every data structure data type also has among the operations that are associated with the data type are also creation and destruction operations then what it brings about is a fine interface by which there is no way of creating that data creating an instance of that data type unless you use a creation function inside that abstract data type there is no way of destroying an instance of that data unless you use the destroying function inside that abstract data type there is no way of manipulating several instances of the same data struct data type unless you use the functions inside that abstract data type which allow you manipulation okay so now what happens is that the interface that i have is name of the abstract data type and what it also means is that i can not do indisciplined or indiscriminate changing of structure or manipulation of data without the permission of that abstract data type once i have done that what it also means is that i can clearly separate out the specification or the interface of that abstract data of that abstract data type what is the interface of a procedure the name and the parameters what is the interface of an abstract data type this set of fields inside which means the names of the data that can be created the names of the functions that you can use right so i can separate out that interface from the implementation which means now i can change the implementation since all operations on the creation destruction and manipulation of all objects created by a data type are all resident within it i can separate out the interface from the body of the abstract and i can change representations and therefore algorithms in the body of the abstract without affecting the interface that s really what c plus plus classes are about i can not go into an instance of a class without essentially taking permission of the class the representation of an object in the class is not directly available to me the only way i can get to the representation of the class is by the only way i can manipulate that instance of a class is by using the operations defined in that class so you can use different representations you can change implementations a new fancy algorithm with a new fancy representation has come for some complex data structuring mechanism b trees or grid files what ever what it means is that i throw out my old implementation and write a completely new implementation with new representations new functions new algorithms for defining the operations on it but i keep the interface intact right so that all programs which use that old data type will still run with the new representation as long as the interface does not change there is absolutely no reason why old programs should not run right so and that form of abstraction is what is is what comes into so these are all largely methodological issues so new features are all guided by new methodologies so the modules of modula two the classes and objects of smalltalk and c plus plus the signatures of m l if you look at the signature structure in m l and the implementation structure you have two separate units which are mutual which such that the signature forms the interface to the to any m l program which uses that data type which creates objects in the data type and manipulates them and even destroys them and there is a separate implementation unit which is which is hidden which is not available so what it means is that i can separately compile programs with an abstract data type which means i do not have the either the representation information or the algorithms available to me for that abstract data type but i can still use that abstract data type in my program and compile my program i can compile the specification the signature file separately i can compile the implementation separately provided the compiled version of the signature is available for the implementation in order to do type checking in order to check out that the same operations are available i can have lots of hidden operations which are not accessible from outside just like i can have local variables in a procedure which are not accesible from outside only the operations that are in the interface specified in the signature or in the module specification are actually available for manipulation and they use the representation information right so that was an so so pragmatically speaking its no big deal but when you look at it from the point of view of developing large libraries in a representation independent fashion and providing a certain fine control of visibility and information hiding then its actually an important step for right  refer slide time 42  05  right so this directly generalizes to libraies and then the therefore the field of data structures goes out of the window and what you have is the field of data abstraction right the last and probably the most one of the most vigorous areas of research currently is is concurrency it starts with just here again the first possible represent language representation of parallelism probably came through the coroutine concept in simula where they actually wanted to simulate the fact that there is a cpu with several several in a in a time sharing system with multiprocessing capability where a job is executed for some time throw into suspension and another job is executed for some time and so on and so forth  refer slide time 42  35  they wanted to use it as a simulation facility to study lets say operating system concepts they also brought down the opearting system concept through the coroutine method into a language into the language to study this method and that gave a new essentially a new method of control which is different from procedural abstraction in the sense that now a procedure from a main program is an asymmetric relationship you call the procedure and return at the end of the procedure to the main program two coroutines have a symmetric relationship you execute part of one coroutine with a resume command you move into the other coroutine starting from where you left off or if it was a first call then you start from the beginning till you resume back so you pass control mutually between different coroutines and that essentially simulates the behaviour of jobs on a single processor system with time sharing okay now you can generalize it of course to multi processor systems with time sharing memory sharing what ever you can generalize it further to distribute its systems with shared memory distributed systems with local memory and no sharing or mixtures of these and what you get as a general logical notion is a is concurrent systems and when you boil all this down to its basics when you look at a concurrent system it could be multiprocessing it could be time sharing it could be distributed it could be memory sharing it could be not memory sharing what ever when you look at the fundamental problems of concurrency then essentially it reduces to three important things independence causality and conflict and how do these three concepts interact with each other so what you can model the nature of distributed computations or time shared computations or mixtures of these by creating a new language construct which looks at these three problems and their mutual interactions right and what it gives you therefore is once you have decided independence causality and conflict is that independence and conflict together actually give you another form of non determinism you can import non determinism also that s infact its found concurrency from the elementary studies of concurrency the coroutine concept in simula was a purely deterministic construct but when you analyse the large scale behaviour of an operating system with respect to various jobs without knowing anything about the scheduler then you are forced to introduce into your simulation language a method of non determinism which is not just probability based you want to be able to claim that those jobs execute correctly or fairly inspite of what ever may be the scheduling mechanism you would like to prove your programs correct regardless of whether lightening or thunder strikes them and then what you have is that as an under current you have non determinism infact that s how non determinism came as a construct into programming languages the study of operating systems in bringing down operating system structures to languages or providing language support to operating system design for multiprocessor or time sharing operating systems right and then what you can do is once you have concurrency as a very general notion regardless of the underlying architecture you can actually exploit fine grain parallelism by making clear making clear what exactly are dependent events and what exactly are independent events what exactly are conflicting events which is a form of non determinism right and you can actually look at localized computations you can look upon the notion of a process or you can look upon parallelism in the abstract as a pure programming language construct completely devoid of its reality when you want to get back to reality of course what you do is you map the parallelism into multiprocessor architecture by looking at dependence causality and conflict relationships right  refer slide time 48  27  so it s a very important and vigorous means of study vigorous subject of study and what happens in this is that you can boil it down even further to its basics and regard communication and parallelism as the main primitives for computation control and express all possible computations in terms of communication and parallelism right so and of course lets not forget one important thing there is a fundamental unity between data and control which means that control which means that under such a model firstly i can express all data as through processes i can express all processes also as data if i wanted to do it but essentially i can express all data and control as processes some which some how communicate and interact i can even look upon the assignment statement as a form of communication between between a process which is one memory cell another process which is that expression and the act of reading and writing are communications between two very small processes right and once you have this fundamental unity of course you shouldn t forget the lambda calculus right  refer slide time 50  07  if you look at communication then its just a form of beta reduction the act of reading or writing the act of assignment is a form of beta reduction the control abstraction is a lambda abstraction data abstraction is a lambda abstraction communication is a beta reduction parallelism is a is an lambda application parameterisation is a lambda abstraction parameter passing or ins or instantiation is a form of beta reduction and finally everything boils down to that can you actually look at all these kinds of behaviours as forms of beta reduction what are the abstraction mechanisms that you can impose on top of concurrency what are the type checking mechanisms you can put in how can you do how can you do higher types over communications can you define higher order processes just like you did higher order functions what is lambda abstraction over higher order processes mean what is parameterisation do how do you map process to processor how do you map the real life situation which is a geographic distribution of some sites to an existing abstraction and the importance of that abstraction is that if you were to change the architecture of your distributed system your abstractions still stands and you can do a fresh mapping of process to processor without changing your original arch it s a new methodology method of programming which looks a fine grained parallelism fine grained independence looking at essential conflict relations essential causal relations and based on that you do a process to processor mapping yeah and that s what the future holds in the light of the lambda calculus so the most important thing to do is to study zen and the art of the lambda calculus transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 21 structured data  time 1  00 min  iit delhi welcome to lecture twenty one so last time we looked various storage allocations strategies for simple data so let just briefly summarize that firstly if we had constants as said that lots of important questions of policy that one should answer before gets on to the implementation should allow structure constants should allow simple constants  refer slide time 00  44  um what happens if you take various decisions concerning these should allow um should insist on compile time evaluation of a constant value should be done at runtime if so um should you do it just before program execution now should you allow it to be more flexible and each time the block was entered in which case each time the block is entered it has a its a constant only for its particular life time and not a constant for a entire program so constants should be allowed it should be should constants just be names of some literals or should they be allowed expression values and then if you allow expression values then would you would you insist on compile time evaluation or a run time if it s a run time evaluation it would just before program execution begins or each time the block is entered in which the constant is declared as far as variables are concerned should you allow initialization  refer slide time 1  36  should you insist on initialization always which is what happens to initialization of large structure variables what kind of language allow what kinds of initialization for variables and finally we also classify variables in various kinds those which are automatic in the sense they created a block entry and and um destroyed at block exit  refer slide time 2  12  the own variables or the static variables which are created during programming which are created at entry to the main program and they have a life time extending right through the execution of the main program the area in which it should be most natural to allocate this variables and then of course there are controlled programmer controlled variables which are explicitly programmer controlled and a usually allocated on a heap  refer slide time 2  43  so they have a life time um which is different from their scope um in the sense their actually their explicitly created or destroyed by the programmer and so today we will look at structure data in some greater detail so so what ever we said previously holds for um let us say simple variables simples constants and so on we should not which means which means the when we when i talk about simple variables or simple constants what i mean is that the underlying virtual machine already supports those types as data types so there are predefined operations predefined sets of values and of course then we are talking um and so um so um so they are simple in that aspect if you had an underlying virtual machine which only dealt with matrices i mean than that would be a simple data type for that machine right but in general most in in most general purposes languages the simple data types usually are scalers out of form integers um reals constants um strings um booleans and so on so now we are talking of structuring data  noise  simple data into more compound data so it means that you have to some how combine simpler pieces of data scalar data in in the view of the virtual machine on which you are implementing into compound units to be regarded as a single data item so in any um in any kind of structuring um operation there are what are know as constructors and what i have called but which no other programming language text calls deconstructors so when you um when you when you structure when you put together pieces of data and structure them into larger units then you also require methods of exploring that large unit and obtaining the simpler components so this i am calling them deconstructors  refer slide time 04  43  but in the case of particular structuring operations they have particular names right um so i would um um i wouldn t call it destructor because that that means completely destroying the data but deconstructor is that is you split it up into its components again so you take some simpler pieces of data combine them together but later you might want to access the individual components for some reason may be to construct new structure data and so you would like to deconstruct a compound unit into its individual um components right so the most so most we should look at structuring data in terms of the underlying what does what are the most common um structuring mechanisms that mathematics provides and how does one um how does one model those structuring concepts in a in our programming so lets lets start with the simplest possible um structuring mechanisms in mathematics and that is of tuples correct so the the construction the constructor in this case is just a um cartesian product right so so you can take cartesian products of two sets for example and you get ordered pairs you get n tuples by taking cartesian products and n fold cartesian product of n sets and you get n tuples of this form and give an any tuple what you require or corresponding projection functions right so um given a tuple the ith component of the tuple should be extractable remember that all these individual sets may be of different types they may all be really different sets so what you are looking for our forms of projections projection functions projection functions are deconstructors for a cartesian product  refer slide time 8  10  so please note that deconstructors are really my name i mean it really not available in anybody in any other programming language text and deconstructor is actually a philosophical word dealing with some modern concepts in linguistic philosophy but any way i think it s a it s a good names to coins so we will use it in for this purpose so projection functions are the most common or what you require for exploring a cartesian product and extracting individual components and so um and so now lets look at how these these these constructors and deconstructors are provided in some languages  refer slide time 9  28  so in most languages in most imperative languages you have records with names fields as the most as a form of as only tuple formation construct okay and the fact that you got named fields means that you use a field selection operation which uses the name so field selection um a field selection in a record is the major operation to explode or to extract an individual component of a record in ml of course um what they have is there is an um explicit tuple construction mechanism and an explicit deconstruction which you can which which which is which is through a pattern matching rule yeah so since ml is quite um is quite clear about a kinds of pattern is a tuple must poses um what you have is you could have a typical um you could you could go through a typical session an ml session like this in which you you construct a tuple a which has these values and the ml type inferencing mechanism automatically gives this a type which is defined by this cartesian product i mean of course the individual patterns actually make it clear what what constitute what what is the construction operation or  noise  what cartesian product what domains are involved in the cartesian product and in what order and the most common way of deconstructing is give a declaration like this and the um i mean note that b c and d are names which are which are not pre declared i mean it is not it is not that well it is not that while constructing this tuple you gave these field names these field names are absolutely new but the pattern matching facility of ml clearly indicates that if there is some new name and you declaring this in this fashion then you have to do an appropriate mapping of these what might be what are ml variables to their appropriate values  refer slide time 12  03  so you get a response which actually is some thing like this and so the deconstruction is done through very often through pure pattern matching and this is not true just of the tuples but it is true of any data type in ml that you can deconstruction through such a pattern matching mechanism so whether they are ml records are records of records to any depth records of tuples  noise  of lists of so and so forth you can do deconstruction just by using the pattern matching facility right so in various most other languages while ml has taken a very strict mathematical view towards data types the constructions are such that they modeled an element in mathematical text what kinds of constructions and deconstruction mechanisms should be provided and how should they be um um how how should they be implemented where as most languages were actually did not have not considered this um this problem of have providing a separate construction or deconstruction mechanism for many for many mathematical operations they have usually they have usually um confused the abstract operation with a representation equality for example the pascal record is one such case it s a is one thing but most commonly are these are they kinds of mechanisms used in lisp and scheme a cartesian product is a clearly a different operation from a lisp forming operation from a sequence forming operation the fact that cartesian products are isomorphic to certain subsets of lisp formation is a different matter altogether but it is fundamentally a different operation where as most of these other the older functional languages um mainly lisp and scheme which is of course just daughter of lisp um with a simpler structure only use lisp as only use lisp construction as a main operation and lisp operation themselves as a deconstructors so um so an ordered pair in a lisp or scheme is no difference from a list of twelve elements an ordered triplet is no difference from list of three elements okay where as logically they are two there are two different things i mean list is meant to model a sequence and not a tuple so how ever since they are isomorphic since lisps are more powerful  noise  construction operation its assume that you can some how since you can program tuples so on through lisps its assumes that its not necessary to have our fresh data type called lisps or called tuples so so in fact most of these languages actually do this um what ever they they have in the pass they have confused what constitutes and abstract construction or a deconstruction operation from the fact that it is easily implementable and they have confuse these two issues okay so the other important operation is what might be called the sum or disjoint union or its also called a co product the cartesian product is a product and this is a co product valence some category of domains but lets not worry about that so its its what i might called a sum so you have you have the sum of two sets and i um what i said loosely was that this is really like maintaining the identities of the elements in the individual sets these two sets may not disjoint but you have to some how maintain the identities of that elements drawn from these two sets and so the natural thing to do is to have a tag or discriminant which will clearly give an identity to the elements in the in this union okay  refer slide time 16  30  so when we are talking about a sum or a disjoint union we are really talking about two injection functions which are which are of this form they they there is an injection function um in one which takes an element from a and gives you an element of a plus b and there is an injection function in two which takes an element from b and gives you an element of a plus b right  refer slide time 17  05  so actually these two injection functions are really what constitute the constructors for the disjoint union operation and a but most um and so a if you look at it as and as a construction which is obtained through injection functions it is not necessarily the same as having a tag or a colored to distinguish between the lms i mean this is only a representational mechanism it is not a logical mechanism logical operations is just a plus b so i have use this to indicate that in in my past lectures um when ever i have used the disjoint union i may have used an equality but here what i am saying is it is not really an equality it is an isomorphism okay which means that the two sides are not exactly identical there is some logical information in this plus which is made explicit through a tag or a discriminant and this is really representational rather than the logical um so how ever what is happened is that just as in my past lectures actually identified this with this most languages have also identified these two and they used a tag in some form or the other right so most languages actually have records with variants with with variant fields um and what should do is you have to you in order to identify individual fields or rather to take the identity of the individual elements in this co product a plus b  refer slide time 19  00  they have they use this tag and um the operations that come with it that come with appropriate tag so you do a case analysis on the variant so for ex so this is the most common thing that is been used in all languages starting from starting after algol sixty so pl one for example was a cobol has a struc mechanism which is similar to the records of pascal but the variant mechanism really comes from um probably the first language to use it to spl one and then algol sixty eight and then pascal and so on and so forth so most languages actually have a form of records with variant as a and the variant filed is a tag tag field is used to  noise  disambiguate to provide an identity for where a certain element comes from right so how ever the problem is that the variant records for example lets look at the pascal variant records there is a certain problem a certain in security which actually is was exploited even by nicholas wirth in his own original compiler on um own original pascal compiler so and but how would the constructor even though it was very useful for its compiler the constructor came and tremendous criticism because of this reason so i can have a record declaration like this normally um i mean very often what you doing is this variant is often apart of larger record declaration which means that you are taking some cartesian products um initially and then you are taking a disjoint union of two different kinds of cartesian products so that s that s really how you should look at it but lets usually look at it in look at the variant in isolation so there is a tag filed which may be is a boolean and if the tag is true then it what it has is an underlying integer and false is and if the tag is false in its character it s a what you are taking is in this particular declaration you just taking a disjoint sum of integers and characters right now this of course what is there within this parenthesis could be more complicated i mean it could be another cartesian product or some such some such thing  refer slide time 21  34  but so now what you can do is now this declaration is a variable and there fore its and since it s a record and you use field selection what you can do is the normal idea of designing such a record is that you do a case analysis on the tag and do appropriate operations may be integer operations or character operations remember that when you take a disjoint union of two sets the injection function is such that the injection functions are such that once the identity of the individual components of set are known the operations of that area type become applicable when you import also the operations of that data type so for example if you taking the disjoint union of integers and characters and you know that there is an element which actually came from the integers then you could do for example addition subtraction when you could take two different records which both belong to um whose parent in the in the parent component integer and you could add those values if their characters are there for example you could look at you could you could you could do some thing like you could compare the two characters are take some successor of the character what ever predices what ever but the point is that if you have two different records p and q with the um same declaration and one has a tag which is true that and the other has a tag which is of which is false then theoretically for example you can not add the integer component of one to the character component of the other i mean the operations the operations remains distinct right how ever what happens is that since the record is just a variable and its individual components its individual fields are also just variables what one could do is one could actually assign the tagger value like this and then one could change the value of the tag and then look at an operation from the other other data type right so  noise  you started of with p being from the characters and you assigned it a character and then you change the type so that now p became part of the integers since so now you can right out an integer value so this is some thing that if if done if this this is some thing that leads to for example a type and runtime insecurities are certain are as a pride of pascal implementation is that every type is compile time determinable now with the variant records you have some thing that is not compile time determinable there is not even really run time dependable and um in the sense  refer slide time 25  44  that you can not introduce checks can not introduce run time checks because you do not know how the tag is going to vary after all i could change this i could change this to some complicated boolean expression so it has a type which it has a skids of running type which keeps changing depending on the the actual value doesn t change of the individual components but there is a side effect on their type of their individual component by changing the value of the tag field right so um this is the reason this construct came and a severe criticism because it leads to firstly an abuse of the type structure of a of the language and then it makes runtime checking type checking really impossible after all there are other ways of doing the same thing and this should be used and it should not be able it should not able to abuse the a flexibility that is provided right so so what happens so what happened was that many many languages actually avoided this problem in various ways so one is the language of the euclid um let let me first say that i will always use pascal or ml type syntax and i don t swear by that syntax i don t know the syntax of euclid or of the many other languages that i would be discussing so what i will write them is in a sought of pascal like fashion the whole idea is that you should worry about the semantics rather than the syntax so so this syntax is definitely not euclid compilable so what i have used is a pascal like syntax here so now what euclid did was that it um one thing you could have done in that pascal declaration is actually defined that record variant record as a separate type and then define and then declare that variable p has being of that type so you can give that type and name and then declare the variable ps of that type that some thing that could have been done but the point is that them the insec that does not solve the problem of those runtime insecurities so what euclid did was that it actually parameterize the tag in the declaration itself so if you declare some variable to be of this type then since this is a variant record type you had to give it give an initialization for that tag and the compile check was introduced to ensure that there was no way that the tag was changed in the program so the identity of the disjoint union was preserved by putting the putting an initialization for the tag field in the declaration of a variable itself so you could not mix two variables very easily right so you could how ever so you define two um two different variables of the form p and q like this so note that so p comes from the integer component of the disjoint union and q comes from the character component of the disjoint union so the injection the properties of the injection function are rigidly maintained but you could also very often you require temporary variables so um you could also differ you could also declare some thing like this so you declare some variable like r in which there was a new reserve word called any and what it meant was that you could assign any type to this  refer slide time 28  30  so the natural the natural question is so could you have for example so can i do this can i first in the main program can i assign a value to p assign that p to r and assigned that r to q thereby i would have again violated i would have moved from one one component of the disjoint union to the other so what euclid did was it banned all forms of such assignments so if you are going to use r as a temporary variable you could assign p to r but you could never assign r to q okay so how ever you could assign once r is been declared to of same type as p you could assign r to some other um variable let us say s which is of the same type sp okay so there is a runtime type checking so based on the tag value you could do appropriate assignments  refer slide time 30  33  so if this if this position were inside a loop then occasionally it depending on the value of some  noise  the variables that you are using are might be either of the type integer or it might be of the type character and you could a case analysis but could never make such an assignment so this is so in this case so no note that in some in some sense this r is of a type is of a super type compared to either of the individuals p or q so by assigning p to r you are widening um the type assigning a p which is of narrower type to a wider to a variable of a wider type so that s called widening you could do widening but you could not do narrowing which means taking an assignment taking a value of super type and assigning it to a narrower type you could not do this arbitrarily unless you actually did some i mean um it was very difficult to do this so you had to so there was some predefined tags but what it means it is also complicates matters allot when you want to do explicit type quotient um so but there there single purpose in life was to avoid the pit walls of the variant records in pascal and so they took this view the language algol sixty eight actually took a more pragmatic view first we this is probably i am mentioning algol sixty eight let me first mention that algol sixty eight has got no relation whats ever to algol sixty and its not its not like fortran four fortran seventy seven fortran ninety no it is not like that algol sixty eight is a completely new language um which was which um the main purpose the main design issues in algol sixty eight were to not only specify algol sixty was a first language which used to be nf notation to specify syntax the contextry syntax and the idea in algol sixty eight was not only you should specify contextry syntax you should also specify context sensitive syntax so they had um they had grammar rules which was complicated that i doubt if anybody other than the language designers are actually written substantial programs in this language  refer slide time 33  12  so um but so but algol sixty eight had a very interesting method which was just that it they followed they actually gave a union of types in this fashion so you could actually a define a union like this and do a case analysis there was no tag field and there was therefore was no there was no possibility of abusing the tag field right so you could look at any particular variable of this co product type and you could do a case analysis based on whether it comes from here or it comes from here there was absolutely no tag field and therefore there was no tag field assignments there was no way you could change the tag field assignments there was no question of widening or narrowing and they so they thought they had they had put everything in watertight compartments things could be compile time checked and but you could do you could do assignments you could do assignments between various variables of the type int char and but but along with along with that information there is actual value of the assignment the parent type which came from injection function those also carried through so you had to always do a case analysis on finding what was the injection function used in order to do what ever manipulations you want right so more more in languages like ada for some reason just used exactly what pascal has done in spider knowing about all the insecurities of the pascal variant record right so the only difference was that ada gave it a forcelable name called a descriminant and which sounds more grandiose and but what they did in order to avoid the insecurities was that they said that any time you are trying to change the tag field you can not select duly update the tag field what you have to do is you have to change the entire record you have to re assign the entire record so this way they allowed for a flexible core cohesion of types but then that cohesion included the cohesion meant that the programmer by having to change the entire record including the tag knew that what exactly he was doing so why so you first say change the so you do the entire thing in a block the changing of the entire record either in a block but since ada also allowed various kinds of compound initializations since ada allowed mechanisms even syntactical mechanisms for doing initializations of um aggregate objects or compound objects you could do the entire thing also in a single command  refer slide time 36  00  you could change the entire record in a single command through various syntactic means by the use of various kinds of packets so if you had records within a record so the components of a record would be within would be written in parenthesis very much like the ml tuples and if they records within records then you would have um more and more parenthesis nested parenthesis inside and so a large record could be changed in a single assignment command by using an aggregate so the large record including the tag could be changed by in a single command by using an appropriate aggregate but then that only means that that only meant that the programmer knew explicitly that he was changing the tag field and he was doing the change in the tag field at the same time he was changing the other components they are not being done in a selective or distributed fashion i mean it s the code for changing the tag field and changing other areas of the record were not distributed along the in the various places within that program resolved done in one single command so so it localized the problem of the variant record problem of pascal so which they found is the was the most acceptable to them um there are there is a co product construct also in ml but that is at the level of data types so what a typical ml co product would like is that what is if you look at a disjoint union it is not just the sum of two sets its actually a sum of two different data types see if you have integers and characters if you are taking the um disjoint union of integers and characters then you are also importing for appropriate components the appropriate operations available with integers or the appropriate operations with characters so you are actually defining not just a new set but a new set with a new set of operations where the where each operations comes fro either of the two injection functions right so so what it means is that so if you look at so so if you look at the operations of int so so if you look at a construction of an int char data type it is not just a construction of a new set but it also has summation multiplication and so on and so forth ordinal numbers of characters um getting a character from an integer and it has all these operations except that all these operations are conditional they are undefined so for example ordinal number of an integer or rather obtaining a char um obtaining um or adding two integers is not available on the in the character domain so its conditional and so what you are actually doing is here defining a new data type of this form so by data type we mean a set with a quit with some with its operations so an integers is not it is just a set either integers is not it is just a set but there are data type which have associated operations with that so you could define a  noise  a data type like this  noise  my ml syntax is also quite informal so i don t guarantee that all this is compilable so you actually define the injection functions in ml so you would actually would define something like in one of int in two of char right and each time you would carry this injections function itself as the tag for each element right and so so what you would do is you would always whenever you are assigning a value to this data type whenever you are assigning a value to this data type you would actually right some thing like this in one space five lets say right so this clearly spaces so the injection function itself acts as a tag and they have two distinct tags it solves that problem of i mean of course since it is a functional language of course everything has a constant value so you actually have two different tags for this data type if you did an n fold summation you had n different data types because because the injection functions were always tagged on as patterns with the component and the injection ensured that you had type compatibility when ever you perform the operations what ever so you could not you could not just you could not do strange cohesions unless your explicitly conscious of it unless you took into of into of two into of some character unless you took that took its ordinal value you could not treat it is an integer rightso so ml actually used  noise  follows um thus um follows the mathematical definition of a co product or a summation um in a very strict fashion um so um lastly we have to look at storage allocation issues and there is also this thing that given an arbitrary um um variable of this type you could do a case analysis just as so if you could have a function some f okay applied to p which would actually be defined by  noise  you could actually define it as f of in one some x equals some thing f of into some y equals some thing so you could do the case analysis standard case analysis that is available in ml through the patter  noise  pattern matching is also available to do the case analysis for an arbitrary variable of this summation type right  refer slide time 42  35  so um so finally we have to look at storage allocation issues  noise  so what do you do we just when you look at variant records um so lets first look at ordinary  refer slide time 43  20  records without variants um by a records of course here i don t mean ml records but um in general pascal like records or pl one cobol like structures okay so you could have and what you do is um you just you just allocate as you you know you know how much space you require for each of these components so you allocate a contiguous block of memory equivalent to the sum of the space is required for each of this at runtime right so all that you do is you you just allocate it all contiguously and you don t require and part of the so you just place the individual fields in contiguous locations and in the in the order of appearance in the record declaration which also brings in another types issue which will come to later another of this various insecurities in this imperative languages which have records  refer slide time 44  09  so one thing is that you don t really if you do this then you don t really require any runtime descriptor anyway if it s a variant record um in pascal then you don t really know what kind of runtime descriptor you can keep for it um so um and the address of any particular field which you require for deconstruction is just some offset plus some of the sizes of the individual fields upto that and since the order of appearances in the declaration is important that s order in which they are also stored so you can you can just take the size of individual fields and contiguously allocated um you can you can you can randomly access an individual field of a record by a simple runtime calculation in fact by a simple compile time calculation right this the offset is the only thing there is going to be known at runtime  noise  part of offset consists of a relative at relative start of the block in the runtime start the other is the absolute position of where that block is going to start in the runtime environment and in that depends on its life time right so it depends on the number of procedures calls may be number of block under it so on and so forth so the offset consists of an absolute component which is not known excepted runtime and a fix relative component which is known at compile time so that and the sizes of each of these fields is know at the compile time this calculation of the of the fixed offset from the current base of the activation record to the appropriate field is all compile time calculateable is all compile time determinable and so um so you can do random access quite easily with such a structure  refer slide time 46  15  of course what might happen is since a record is a structured data type in using individual components you might require some runtime descriptors in order to do various kinds of checkings after all for example you might have a record whose one component has an array with certain fixed bounds then you have to ensure that the array index doesn t go out of bounds and so on and so forth so the runtime descriptors are usually necessary for arrays we will come to that when we do arrays um next time so so but for the record as a whole you are not going to have any runtime descriptors you are just going to have run descriptors as appropriate for the individual components right and in the case of a variant record you really have nothing to do except allocate some maximum value computed over all possible variants of the record so you just look at possible variants of the record find out which one requires them maximum amount of storage the fixed part of the record anyway is going to use only that much that s one of the reason why one of the reasons why pascal insists that the variant the variant the tag field and the variant component should always be the last declaration in a record if its present so that the fixed parts of the record anyway have fixed um fixed memory allocation and the variant part can um could variable amounts of memory allocation but normally most pascal implementation you just take the view that will just take maximum over all possible variants that you have and just allocate that much space and if its if you are looking at a smaller variant than that is just that just occupies a space that is apart of this larger space that you have allocated and the initial position of that larger space is all that they occupy right  refer slide time 48  12  so so that s how these records structures are um implemented and that s how they allow also random access to the fields of the record so next time we will we will do arrays as and some some other data types um and then we will get on to control structures thank you transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 22 sequences  time 1  00 min  iit delhi welcome to lecture twentytwo last time we started on structured data and we looked at essentially cartesian products and they variants so it corresponded approximately to records and variant records so cartesian products and disjoint unions um today we will look at sequences so let me just briefly recapitulate what we will what we will have to do while structuring data as i said one important  noise  one important is to find constructors to combine simpler pieces of data into compound units  refer slide time 01  10  and then find deconstructors which explode those compound units which explode a compound unit into its simpler components  noise  right so looking at sequences the main constructor um actually we should look at it this way  noise  long long ago the mathematician and philosopher rene descartes said that after addition and multiplication the most important human intellectual activity is equation solving it is so in fact that is also how school curricula and so on are designed but of course equations solving also means finding inverses of additions and multiplications so you also have subtractions and divisions but equation solving is really a most fundamental activity so after these four operations you could think of equation solving as the fifth operation it is part of any branch of mathematics right so the main constructor is that of is that of an equational definition right  noise  so given some data domain d i can define this equation in terms of the unknown s so s is the unknown and the question that you are asking is what is the solution of s for such and equation right um you might be asking it you might be asking actually what are the possible solutions of s which satisfy this equation right so if you look at it that way  noise  one um ones one solution or what for what we might call the least solution is that of d star and what is d star d star is defined as lets say the union if you want you can take this also to be the disjoint union which i don t know how may be i put i could write it like this so in this case the disjoint union would work just is where so you can take this as this union of sn where s zero is the m is the singleton set consisting of the empty sequence and sk plus one is s cartesian product sk right  noise  so this solution to this equation is actually the what might be called the least solution  noise  it is in fact the smallest set s which when you substitute of both sides the s would yield the um any quality right so and since its it s a its actually the construction of the ordered pairs right you construct so sk plus one is an ordered pair consisting of two elements the element is an element of d and the second element is an element of sk means that s that s way you should look at it  refer slide time 5  39  so this is so this is in fact the natural motivation why most of these functional languages actually provide some form of head and tail functions because a sequence is to really a cartesian um is really an ordered pair of whose second component might be another is another ordered pair and so on and so forth right so so the deconstructors are the just head tail and may be the i don t know the so the constructor the constructor um really the cons operation if you like which comes from here other wise and the deconstructors are just the head and tail fucntions now if you look upon at just as equation solving then this is not the only solution okay there are other solutions to this equation for example you could have infinite sequences also a solutions to this equation right  refer slide time 6  15  so if you take if you consider d star plus d infinity where d infinity is that collection of all infinite sequences okay for the moment lets look at it then this way then this d star plus d infinity is also a solution to this equation and it is in fact this solution is in fact the larger solution to this equation um right and as far as infinite sequences are concerned a way of looking at you can you could look at infinite sequences also as an infinite sequence can be could be regarded as ordered pair whose first element is an element of d and whose second element is another infinite sequence so i may so the same head tail and cons functions are equally um those who the constructors deconstructors that were used for this are equally applicable also for infinite um infinite sequences um infinite sequences though we will not be looking at them in some detail are actually present in some form in ml okay the lazy evaluation mechanism of um ml actually has these infinite sequences these some domain some data domain some thing that you already know s s is the unknown variable s is equal to d star um this is the empty sequence this is the empty sequence this is d cross s okay where s so a solution for this equation i said was d star where i defined d star as the disjoint union of all i am sorry may be i should i should have call this may be i should have used dn um i should have used dn so this should be dn okay where d not d not is the empty sequence d k plus one is d cross dk um so i hope that clarifies the matter right  refer slide time 8  42  so  noise  i used s through out though of course what i had written was also not wrong let me clarify that um so that is also equally correct so for any solution if you have done this done these operations you would have got that also to be a solution of this equation okay so anyway thats thats immaterial the least solution but how that is not the least solution the least solution is that which is obtained by using d right um as if we could just as well um there are other solutions for example which which could like infinite sequences so if you add infinite sequences also then you get a solution you get another solution which might be called the greater solution so this equation really has only two solutions there are there are really no other solutions and so these are you can loosely look upon them as this is some thing as a set definition normally you don t see a set definition which is recursive oaky but in fact computer science is full of set definitions which are recursive um in the in the in the early jurassic period of this course we looked at context free grammars right so if you take a context free grammar of this form lets say um so the question that you are you can ask is what is the language generated by this grammar um so if you ask the question  refer slide time 9  41  what is the language generated by this grammar you are also you might equally well ask the question what is the solutions what is the least solution to the to the equation given by this okay um so so let me re write this properly what is the least solution to this equation thats that what you are really asking so thats why equation solving is absolutely basic so where of course um i have used a very loose notation this zero and one or this s is presumably supposed to be a set and this zero and one i have used them around s to denote that every element of s is pyrite on the left with a zero and a right with a one okay so its a its a it s the normal prefixing of suffixing operation on strings i have just generalized that to sets a strings so you can ask this question what is the least solution to this equation and in fact that is the language generated by the graph right so equation solving is really fundamental is absolutely fundamental and um so all most all things that we talk about computationally or or instances of equation solving in fact most of the equation solving where we do in the computer science is of the is of finding what might be called fixpoints um so all recursive definitions any definition which is recursive in particular this grammar is also a recursive definition this set is defined in terms of itself and so its a recursive definition all recursive definitions are essentially this are essentially which have to be solved and what is the meaning of finding in solution to a recursive um to equation is to find a non recursive definition which will satisfy the equation right  refer slide time 13  39  so and what is that mean that means finding a fixpoint um everybody knows what a fixpoint is know if you look at if you look at your standard methods like um let let f be a continuous differentiable function over this interval on the reals so than f on the reals you want to find a root you want to find where this curve intersects the x axis okay so what you are going to do what you often do is you are essentially solving the equation f of x equals zero okay  refer slide time 15  48  and in order to solve this equation f of x equals zero which is finding the root of this function what you often do is you transfer the you transform you manipulate thing so that you get an equation of the form x equals g of x and then way back in the sixteen th century some body said that the solution to this equation is defined by the recurrence right okay so this is this is a so so you can look up on this finding the root you some how you do some transformations and get an equation of this form which is very much like giving an recursive definition to x okay so i could look upon this as giving a recursive definition to x and given a recursive definition to x i give an iterative solution which is a recurrence so i can use recurrences in order to give iterative solutions to what what might be called fixpoint equations um this is standard on real numbers there is absolutely no reason why one shouldn t it on sets right so this solution that i am talking about these solutions are in fact also recurrences in the same form in fact what i have given so if you look at it analogously this solution this is a recurrence which defines an iterative solution to this equation to this fixpoint equation  refer slide time 8  42  right so this clearly an iterative solution and most iterative solutions will give you solutions to fixpoint equations um the language generated when we are talking about the language generated we are we are talking about the least solution that will satisfy this equation the least solution that will satisfy this equation in the sense that it satisfies we want the least solution which is close under the operation of prefixing and suffixing of zero and one respectively right so what we are saying is what is the least set s such that zero one belongs to s and two if s belongs to s then zero x one also belongs to s so there is a closure there is a closure property that has to be satisfied so any solution which does not preserve that containment this containment problem namely that zero one should belong to s and for all x which belongs to s if x for all x that belongs to s zero x one also should belong to s this is an important closure to um given by this definition and the least solution that you can think of are all the possible purely nested bracket matchings if you like zero ones you know so this so the solutions the language generated by this grammar if you go through an iterative solution is just zero raised n one raised n where n is greater than are equal to one this is the set s it is the least solution because i am not for example permitting infinite sequences okay if you permit infinite sequences there is there is possibility of getting um a cardinality that is greater than i left not you are actually um you are actually finding accumulation points of or limit points of sequences i don t know whether you have done all these sequences but its its its a its a topology which is which is completed by some thing anyway lets not worry about that but the point is that there are um okay so this is the least solution because the least solution in the sense that it has to satisfy these two closure properties  refer slide time 20  34  you take any you take any set smaller than this okay i can find that there is an element in that set which does not satisfy one of these properties okay so we are usually talking always of least solutions not necessarily um not necessarily always now but we are always talking of least solutions which satisfies this such closure um such closure properties so if you if you were to take this if you were to take this constructor for example such a domain equation then um no set smaller than d star will actually do for you  refer slide time 8  42  if you take any set smaller than d star either by removing some elements or just by putting a bound some bound n then i can find um i can find a sequence which does not satisfies this closure properties for example i can take an n element sequence okay for any n element sequence it does not satisfy the property that if i take that n element sequence and an element of d as an ordered pair than that ordered pair belongs to the set s because it is an n plus one sequence so the smallest solution that you can get is just only the set of all finite sequences of elements of d you cant get anything smaller than this because there is a closure property to be satisfied right so very much as  noise  very much as newton um gave a solution to this fix point equation you can give solutions to such fix point equations and in fact most of our recursive definitions in computer science are really um are really equations to be solved and they are fixpoint equations to be solved and um what is most surprising is with though if you did a course on semantics of programming languages you will see that in fact the way you obtain solutions to those fix point equations is also very much like the way newton did it for continuous differentiable functions okay  noise  this is a very little the i mean the analogy is absolutely striking right so you can have also infinite sequences as solutions to this equation right we might look upon um so you might just look upon an infinite sequence loosely but then more particularly when we talk about an infinite sequence or at least a sequence that is infinite on one side not necessarily infinite on both sides okay when you talk about a sequence that is infinite on one side we can look up on that is a function from um the natural numbers to the domain i mean after all when you when you consider a sequence the most natural thing is to ride it ride a sequence as a one a two a three a four a five and so on and so forth which essentially means you are drawing a mapping from the natural numbers to elements of the domain you are saying that the first element is a is a one and the second element is a two and so on and so forth right so  noise  this this d infinity itself is a solution to this equation and is in fact it is it is not the least solution because the empty set least solution for this equation but it is a least solution other than the empty set right so  noise  so when ever when ever you think of recursion think of equation solving when ever you think of equation solving think of equation solving by um solving a fix point equation and when you think of fix point equation solve it by a recurrence i mean find a suitable recurrence which were um satisfy it and that s  noise  that s the basic that s the basic idea so if you look at any sequence so as i said we could look upon infinite sequences as just each infinite sequence i can um regard it as just said a structured data element containing an um an infinite number of components in some order or i can look upon that merely as a function from the natural numbers um to my data domain okay so this basically tells this function basically tells you what is the first element what is the second element what is the third element so on and so forth so you can think of that infinite um you can think of that infinite sequence as a function defined by enumeration um by an infinite enumeration so what we could also do is we could also think of  noise  other sequences as functions so even though we defined though sequences as ordered pairs um we defined a k plus one length sequence as an ordered pair consisting of an element of d and a k length sequence as a second component right how ever we could just collapse all that and we can look upon every finite sequence as being isomorphic to a function defined by enumeration right so every k length sequence you can look upon that either as that just a k length um sequence of data elements or as a function from the set one to k to the elements um to d right where of course  noise  the empty sequence is the unique function from the empty set to d right um the domain is empty so it doesn t matter really what the co domain is when there is only one possible function from the empty set to any domain d okay so you can look upon the empty sequences as just such a function and you can look upon any k length sequence for k greater than one as  noise  a function from one to k to d um right  refer slide time 25  30  so the set of all k length sequences is just the set of all such functions isomorphic i mean this is isomorphism right i mean all i am saying is that there is a one to one correspondence between the definition of sk as we have given before and this and this set of functions right  noise  and this is the basic idea behind arrays um  noise  lets lets take a k length sequence of integers so i have the sequence eight three four eight two one okay the first element what is the first element its eight okay so this is function f of one is eight f of two is three f of four is also eight and f of six so this is the sixth length sequence f of six is one um what was happened is that a long years of writing functions in closed form have made you forget that you can actually write a function by enumeration i mean why not right i mean um you can look at the graph of the function right so a sequence is really a function in that sense it s a function not defined by not defined in this form  noise  f of x equals x square this is this is a definition which is not by enumeration it s a definition by a predicate right if you like okay but you can also have functions define after all the definition of a function just says that  noise  for each element in the co domain in the domain there exists a unique domain in the co domain which means that all it means that you cant have an arrow of this form i mean you cant have two arrows from the same element of the domain thats all otherwise so you could write a function out by enumeration that s as a good a function as any other right so  noise  so sequences could be regarded as functions its important to know that a data and control control is really a functions when we do the lambda calculus you will see that there is no really essential difference between data and control but then that you already know in the case of you are a computer architecture course both data and controller are represented as data right i mean the fact that you got opcodes for every instruction you are not really doing much control you are just representing program as data okay there is an alternate view in which we can look upon all data as functions and all control is also functions and that s what we will study in the lambda calculus when we come to but in the mean while the fact that the finite sequences are this set is isomorphic to the set of all functions from this from this closed interval one k to d also gives us the reason why we might look upon arrays in programming languages as functions so arrays are really functions defined by enumeration um its important for us to look upon sequences in their generalities so that we understand that there is an underlined unified whole called the sequences we should not be well we should not be mislead by the fact that arrays look different from files and lists and so on but fundamentally they are all sequences okay so the only thing different is that arrays are functions by enumeration and they are functions over a finite index set um of some component type this is of type d so you are very basic declaration in a pascal like language is really that of a function specification if you like a is an array some thing index set of type component which is really specifying a domain and a co domain and specifying the name of function to be a okay it s a function defined by enumeration as supposed to function defined by a  refer slide time 31  59  closed form and the deconstructor for such a function is just function application so how do you get a component out of the array by applying the function well the function a to the component i to get the ith component its as simple as that right so this is the deconstructor and just because there are reasons why you might want looked upon them as but many languages for example do not distinguish between array component reference and function application at all okay they used the standard parenthesis for array component reference or subscripting if you like so this so this subscripting operation on arrays is really a function application um so now is  noise  so if you lets look at arrays the one important reason why arrays are useful um in programming is that they provide a form of direct access what is what is known as random access so so if you look at the pragmatics of the array um array allocation what you would find is that firstly these arrays especially since they are bounds  noise  in a language like pascal the bounds are known at translation time okay so which means that you can fix you the type the type of the components is also known at compile time which means that you can give an accurate calculation of the space that is needed to store the entire array okay so if you can give an accurate calculation then all your address calculations can be based on the stack you do not need to go into the heap right so which means that all you all um so the only thing that i require during in my storage allocation is that i have to get know um i have to i have to be careful about one thing and that is this index set right so an array has a is a is a function from the index set to a component type so this index set is by definition an index set um is is any finite an index set is any finite set is any finite ordered set so its ordered by its ordered either by a predicate or more more usually its an index un an index set is a finite set with the with the operation of with the operations of successor and predecessor define for all elements except the extremes okay so so thats thats also the reason why any enumeration type can be regarded as um you can take any enumeration type or a sub range type in pascal and call that the index set of the array right so the successor and predecessor functions are defined for all the other for all the basic data types except except the reals so for example um um um a dense set what is a dense set a dense set in which is one in which between any two distinct elements there is guarantee to be another element right so any dense set is um is going to be infinite so dense sets can not be index sets okay so which means logically the reals and the rationals are route so all you can have is some if and if you are talking about any finite set which is ordered by a successor and predecessor relation then this index set is isomorphic to some set  noise  some ordered set of natural num natural numbers for some k right so this index set so in fact so you can if you look at if you look at an array definition of this form array lets say a to z of integer then you are really draw you are looking at a function that is a composition between this isomorphism from index set to this um this set where k is k happens to be twenty six and integers this is an ordered finite ordered set with a successor and predecessor defined on it you can also have um you can also have an array of array of for example it has to be false true um um false is less than true so um so you could have arrays of this form too  refer slide time 36  57  so as far as long as a um as a successor and predecessor function are defined on the index set and it is ordered in some way the index set can be placed in one to one correspondence with the closed natural interval one to k for some k and therefore any array could be defined in terms of any such index set right so  noise  so in fact the these sub rings types and these enumerated types in pascal are actually implemented by this one to one correspondence this is there i mean what you do is actually create a table with an order actually create in fact what you do is actually create an array um and put the elements in that order so that you you have you have you um already perform the mapping by an enumeration mechanism right so  noise  um so we are looking at storage allocation so the only other problem that you have to worry about is a is a is really that of bounds checking bounds at runtime and for that you require what is known as a runtime descriptor so here i have a given a picture of um a single dimension array so its  noise  so i its called a vector by a pract but anyway thats not its not important so what you have is in order to be able to perform runtime checks on indexes um you required to know the lower bound upper bound and the type of the component and thus the amount of space each component is going to occupy and you store since you have to do runtime checks on expressions which are going to access components of the array you require store this descriptor along with the array on the stack as and when you enter a block which declares this array right so the array is actually the the actual the data of the array is actually stored down here  refer slide time 39  30  so this is how  noise  most of these most of these languages which involved runtime checks on um on array bounds actually perform these bound because they keep a descriptor which is created this descriptor template is created at compile time and there is enough information in the declaration of the array to give you this info  noise  to have this stored some where and each time on entry into a block during execution time you first store this descriptor and the stack and then store the store the actual array elements after that and so you you can do a direct um address calculation which is where which is quite review um so there is  noise  so if you look at array access whether for l value or r value you still have to get together get to that component get to that location of the address of high so then there is a base address which of course is runtime which is not compile time determinable so relative to this base address there is a descriptor size which is compile time determinable there is of course an i this i need not be an index it might be an expression on the data type of the index set and therefore this i is not compile time determinable how ever the lower bound and size of the elements are compile time determinable so which means that which means that you have to do a certain amount of compile time computation as part of your array creation mechanism and the rest can be a runtime determinable um runtime computation okay so the actual direct access and the meaning of direct access are its a its more wrongly called and more often called random access its not its what you mean is direct access and the direct access means that the direct access does not have to go through the structure in sequence to find an appropriate element but that you can do an address computation outside that structure and hit upon the um hit upon the absolute address of that component directly um  refer slide time 41  10  so  noise  and that of course involves this very trivial runtime computation um so how ever what happens is that most languages um most machine architectures usually um usually specify some byte length or some word length um very many architectures starting from the ibm three sixty actually specified that while integer should be in half word boundaries all floating point should only be in full word boundaries where a word consisted of four bytes some machines like that deck system ten had a had a word consisting of five bytes and then they had all these um complicated things like um can store data only starting from a word address or a half word address so some such thing so  noise  the whole point was that um they underlying assembly language provided operations which worked fastest on word boundaries or half word boundaries and works much slower um when you had to pull a apart and the individual the individual bits of a byte one at a time okay they could this um this parallel boolean operations on words they could do parallel boolean operations on half words but um but sequentialyzing all those operations bit by bit would slow down the machine extremely so they always recommended that you store um you stored data in such a way that they occupied word boundary so that they might exploit the parallel boolean operations to the fullest extent to give you the maximum throughput right so how was so what happens is that as a result when you take this when you take a when you store arrays you will be storing them mostly according to some architecture specified boundary so that the computations are fastest how ever the storage is at a premium then what you might want to do is you might want to use a packed representation right so then if you are going to use a packed representation what it means is that you are disregarding storage is more important to you than actual computation time and so you are disregarding all those  noise  all the advice that the architect of the machine gave you regarding fast operations its more important for you to able to pack as much data as possible into a single word rather than to do faster computations okay may be because you got tremendous amount of data so then when you have packed representation then what you are usually doing is you are not going to stick to the discipline of word boundaries or byte boundaries or half word boundaries what you are going to do is you are going to do move things to as small to occupy as smaller place as possible to which means that a certain data item for example if you have integers only from one to hundred then it is not necessary to store an entire to allocate one word to each integer you could allocate one word to four integers if its um so if you have just if you have  noise  if a integers are all going to be less than to raise to eight lets say to raise to  noise  to raise to four or some such thing then you are going to going pack as many integers into one word as possible and if you are going to pack that many then what it means that you can not exploit the parallel explanation of those logical operations at the machine architecture provides you which means in order to do them fast you have to unpack these integers and store them a fresh in some place do the computation and store them back in this pack fash right  refer slide time 46  13  so the um so one thing is that so any kind of packing mechanism means that your access to individual components will no longer be direct and secondly if you want faster computation you will have to unpack that representation so you might use more intermediate storage in order to gain faster computation but you are going to lose some time because of unpacking and packing again right because thats an extra loading and storing packing and um unpacking and packing also means going to accessing um accessing individual components which whose addresses are integral multiples of what ever is the basic unit in the machine memory  noise  right so packed representations are likely to be slower and more expensive and should and are going to be used only when the data is so huge and it is possible to save on storage spaces storage spaces at a premium is possible to save one storage space but you don t at but at the expense of slower access as slower computations right  refer slide time 46  59  so this this notion or variance can be easily generalized to multi dimensional arrays all it means is that your runtime descriptor should also include information about the dimensions and the dimensions for each dimensions of course you have a lower bound and a upper bound specification so you have to specify them  noise  in a particular order which is also going to be reflected in the representation of the entire array for example a two dimensional array could be represented either in row major order or column major order by row major order what do i mean i just mean this um i think you must all studied there in data structures but so if you have a matrix of this form a one one to a nm a two dimensional matrix is going to be represented linearly in the in memory if its represented in row major order then then you are going to have a one m to a one n followed by a two one to a two n and so on up to a nm so this is row major that means its matrix is regarded as a vector of columns um as a vector of rows as an array of rows if you like the other alternative is a column major order which means that you would a one one a two one and so on up to a n one and then  refer slide time 48  48  a one two and then so on up to a n two and so on up to a n m this many of the initial fortran are and even now many fortran compilers actually use this column major ordering that means they represent um they represent the two dimensional matrix as a vector of columns where as most languages now a days pascal and so on and ada would represent a two dimensional array in row major order which means they would represent a matrix as a vector of rows um so they generalize the um so in the case of multi dimensions again you can write an easy formula to do that do a direct access calculation of the address separate it out into translation time computable  noise  term and runtime determinable term the other kind of sequences is that we might have an let be that we use a strings so normally this strings are stored in heap sometimes the heap is divided into a separate space string space aware all strings are stored in order to gain access um or stored contiguously with just a single descriptor which gives the length of the string um and the most  noise  very often stored contiguously because each character in a string doesn t require more than seven bits so you could you could effort to store them in an  noise  in an eight byte word contiguously for example or of course you actually use the heap for dynamic data structures and you use it as part of the dynamic data structuring which is that but this means pointer travels um it can make things slow  refer slide time 49  41  so a language like snow ball four which is meant specifically for string processing would try to store all the strings contiguously with out having to use pointers right without having to use too many pointers there are languages like pl one which allow fixed size strings so which means they could be store in the stack because their length is calculatable or fixed bound but variables sized strings which means you specify an upper bound of the length in the string but you are not um but you dont guarantee to use all of it so which means um the runtime descriptor for that kind of a string would contain both the upper bound and current length of the string so that you know exactly how much you are using you are not reading more and more garbage that is there in the end of that allocated amount so then in such cases these strings could be stored directly on the runtime stack since their sizes are compile time determinable lastly you have files this thing that looks like an idly pan to make idlies in a pressure cooker is actually the disk drive or some secondary storage device um so in the case of files there is a logical difference between the program read and write operations and actual transfer to and from the secondary storage which is this idly pan um so um so usually the transfer is on um is um block and there is a file information table which is stored which usually contains various information firstly um what area of the disk did this piece come from or what area should it go to um secondly it should also the actual read and write from the programs i mean they pointed to the actual area in the buffer where the current pointer is right and so very often the actual transfer data takes place at an ideal time for processor or it just handles directly by the io processes which means you have to have interaction with the operating system on which you are implementing your language um there is  refer slide time 51  35  these are only sequential files languages like cobol allow direct access and index sequential which means that essentially the structure of directories has to be pull down into data components within the file and you will have store addresses disk addresses cylinder sector addresses in some files so that you can do a direct access on to the disk for the block of storage which contains the data you are looking for but then that means a lot more intensive computation lists well there is nothing to say about lists  refer slide time 53  25  so they are just stored in the heap right and either be controlled they are usually be programmer controlled they automatic in functional languages the allocation and deallocation depends upon whether what kind of a lists it is going to be um so next time we will start on control structures transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 23 control  time 1  00 min  iit delhi welcome to lecture twenty three so today we will start on control so we have gone through what might be called usual data structures there are normally available in programming languages and its now the turn of control and through out what ever i say about control i will not worry too much about control structure and expressions because i think we have defined a runtime semantics and dynamic semantics which makes all that very clear so the important therefore in control is for actually look at the look at some important features and important variations may be and look at some pragmatics of the implementation of certain features right so um the interesting thing about control and though um interesting thing about both data and control is that we are not very different from each other in many ways first you can look upon you can look upon for example um if you look at the fondo main architecture both data and control have the same representation which is part of fundamentals of such an architecture the stored program concept so to speak essentially and we suggest representing data and control in the same with a same kind of representation that s what also makes self modifying programs possible and so on and so forth um there is also another way of looking at it i mean so that you can look upon that fondo main architecture of an actual machine as one which essentially says that control is really nothing but data all control is really data okay there is an alternative view which we will look at in the lambda calculus and what that says is really of all data is just controlled i mean this you need not have a separate so you can look upon all data as just functions so um and that s what the lambda calculus propose to do and it actually does it quite successfully so so how ever in our mental mind set there is a clear distinction between data and control but the fact that these two are actually mutually into changeable um mutually um convertible also means that its its like what polio i said once true mathematical genius is not in finding in differences between structures but in finding similarities between widely difference structures so and one of those things that um for example lead to the design of pascal is exactly the similarity between data structures and control structures and there is an interesting analogy that which points out um which motivated the way he design both the data structures of the pascal and the control structures and which is an interesting analogy first we of course you have scalar types and scalar types assigned to the assignment i mean so in the sense that they are both atomic they found the basis for other structure types are other structure commands then he looks at record types and essentially it says that record types which consists of enumerations of types in sequence correspond to a compound statement may be bracketed by a begin and end end right so the fact that the fact that i have record consists of firstly record consists of sequential um of a sequential enumeration and secondly of a sequential enumeration of of widely different of heterogeneous data right so the record being a sequential enumeration of heterogeneous data um a compound statement which is really represented by a sequential composition really represents the fact that you can you can you can sequentially compose heterogeneous kinds of commands okay and you can sequentially enumerate that right so and then if you look at if you look at array types the array types actually are a fix number of repetitions of a homogenous type each array type as a structure which is a fix number of repetitions replication of a homogeneous type of a of a single type and a for loop iteration if you look at that whether it is for counting up or counting down it doesn t really matter its really fix number of iterations of the um fix number of repetitions of of the same underlying command and you goes further and  refer slide time 6  50  says file types really correspond to indefinite iteration right so a file type is just a sequence remember that a file type is um is homogeneous in the sense that its a sequence of um its a sequence of elements of the same type and indef of unbound of unbounded size and an indefinite iteration in the form of a while loop or a repeat until is really a form of um indefinite number of repetitions of this same kind of command right  refer slide time 07  16  so and so the while and the repeat actually like or like the file type and they represent essentially indefinite number of repetitions or some homogeneous kind of element whether it be a command or a or a type right and to you can actually carry these things further and you could look at variant records variant records if you look at variant records they correspond more or less o conditionals mean there is a case analysis and  noise  a particular case of case analysis when there are only two possibilities so if for if then else all kinds of conditional statements really or like variants in a record and so there is a very close analogy between variant records and conditionals and that also motivates why he is used similar reserve words in in all these data and control structures that he has designed and lastly of course we can look up on procedures and functions mainly if you look at recursion then you also have a corresponding recursive data type which is a pointer type right so the main main thing about recursive procedures and functions is that recursion is an important control structure and its also an important data structure in faciliting the pascal right and with actually stops there but one could go further if you look at non recursive data types if you look at non recursive procedures and functions what they essentially give you is an abstraction of control and corresponding to that you have abstract data types which are an abstraction of data so they provide when you look at when you when you um unlike unlike unlike previously when procedures were procedures and functions were considered to be forms of avoiding repetition of the same piece of code or may be parameterized forms of the same piece of code the fundamental  refer slide time 9  11  reason for having procedures is that they have they give you a level of abstraction control abstraction  refer slide time 9  33  so that you dont need to look deeper into the procedure you just need to know a specification now what the procedure does and you can use it right so they provide essentially a form of control abstraction and what we will see later is that corresponding to this control abstraction there is also a data abstraction that s possible which in fact in modular two wirth actually implemented this this analogy too i mean so he his analogy in pascal of course stops here but in modular two it goes further into abstract data types of course this is not his um its not his own original idea so its its its an analogy that was beam that has been pointed out um the head beam pointed out some time after the design of pascal and it was developed in various other languages like clue alpha um modular two there is a form of modules also in ml um both the moscow ml and in standard ml which is actually a very high very highly abstract entity and so you could actually carry this analogy right after this portion um  noise  how does a how does a typical pointer type look like what is the most important characteristic of a pointer type you would define you would declare ptr could be a pointer let say to some data type called a node and then you would  noise  define node to be some record which has may be various fields and finally may be there is some thing lets say single linked lists there is something called next which is of type pointer right so now if you look at this this pointer is defined in terms of node and node is defined in terms of pointer okay okay  refer slide time 13  32  now if you unfold so this is the case of mutual recursion if you unfold the recursion you can express essentially structurally at least a node in terms of itself wait a min that is the fundamental purpose you you you want to use you wanted to define a data structure recursively so that you don t have to give um fix any bounds and so on and so forth so this is actually a case of mutual recursion and i mean you could pull it apart unfold it merge this definitions together and you get a definition of one node in terms of itself that can be that s really how a singly linked list works for example or any other kind of recursive data structure i mean we have lots of all these dynamic structures are recursive in natures tree structure list structure they are recursive data structures um so  noise  and so lets look at lets look at one particular form of the analogy which has to do it the file types and indefinite iteration right so if you look at file types and um indefinite iteration what i said was the most important thing when you look at file types in the last in the last time um we said that a file has been real nothing but a sequence i mean right so um except that it s a finite sequence finite but unbounded sequence right so if it s a finite but unbounded sequence essentially its s solution of a of a definition of an equation of this form  noise  right so now if you look at this equation and look at the solution what do you find you find an inductive  noise  the solution has been defined in inductively and that forms of the part of the iteration that forms the part of the iterative solution  refer slide time 14  12  so you have an inductive solution for essentially um recursive fix point equation um essentially a recursive definition or a fix point equation and what i said was there is a the solutions to such equations is actually very similar in all all branches of mathematics so so for example um another thing is that um that i showed was that in fact any grammar the production rule the production rules of any grammar can be looked upon as um though i took a context free grammar it is not necessarily any grammar which has a set of production rules you can look upon those production rules as defining a um a set of mutually recursive um definitions of the non terminals of which one non terminal has to be solved usually the start symbol right  refer slide time 15  05  and you can you can now you can take those definitions and go through an iterative process and obtain a solution which is a fix point of that equation um right and the similar and a similar method was what was adopted for example in the newton raphson method for or in any kind of fix point  noise  fix point definition  refer slide time 15  52  fix point equation solving even on real numbers and so on and so forth so if you look at the numerical methods were solving for fix point equations i mean you actually have an iterative or recurrence so you actually have a recurrence which gives you a solution which gives you closure and closure approximation to this solution provided certain conditions are satisfied i mean the derivative should no where b zero and things like that so um if you follow standard convergence criteria and so on and so forth and smoothness of functions and so on and so forth what this gives you really is inductively or through a recurrence it defines closure and closure approximations to the actual to at least one of the actual rule that you might be interested in so that in the limit you actually get that root only in the limit right you can look upon this this union for example as also computing a limit i mean this unions is limits is it clear unions are limit limits no okay take this sequence um okay unions are not limits okay wait okay may be lets unions are limits look at this way take the definition of real number to if you if you go from a rationals to reals okay so what are you adding now you are adding irrational numbers also to assign so if you look at how can we define a real number in general means that means two things first how can you define an irrational number in general and how can you ensure that all the reals which are also rationals satisfies that definition then um two centuries ago dedekind solve that problem by declaring that an irrational number is not just a number it is actually the set of all rationals that are less than it okay so any irrational number is is actually the set of all rationals so lets take root two is actually the set of all rationals such that is less than root two i mean you can of course express this otherwise by writing p square plus q square less than p square by q square less than two okay so um that is not a serious problem so what he said was um an irrational number is really a set in the sense that if you take all rationals less than it then in the limit you approach that irrational number so the notion of a limit itself is that of a set okay now if you take this and apply this definition lets say to the rationals themselves okay the rationals are reals so a similar definitions should apply so you can take every rational to be set of all rationals every rational number can be defined as being isomorphic to the set of all rationals that are less than or equal to it so that there is a limit of set is the rational itself okay so now you have a completion of rationals into the reals every real number is really a set of all the rationals there are less than or equal to it no it doesn t matter whether the real whether the real number is a rational or irrational it is still a set okay further that set can be regarded also as if you look upon a real number um as being isomorphic to the set of all rationals that are less than or equal to right okay i can also look upon the real number as being isomorphic to the union of all the reals which are  noise  the union of all the reals which i will write as which are less than or equal to it okay so what is the concept of limit the concept of a limit is just union or least step of bounds right this is what you get from the mathematics that you understood so now what a similar process actually is is what is what happens in this kind of any questions solving right so this unions are really so if you look at the we can look at successive approximations so if i if i define dn in this case as let me define a new dn as being the union of all dns super script n i am sorry dms super script m such that m is less than or equal to n okay in okay then i have a sequence of approximations to d star in this fashion d not is of course just going to be equal to d zero it is just going to be the empty sequence  refer slide time 21  16  d one is going to be equal to d zero plus d one and so on and so forth right dk plus one is going to be equal to um d not plus d one plus and so up to dk right so what we get is a chain of increasing approximations d not subset of d one subset of and so on and so forth dk subset of dk plus one up and so on and so forth in the limit what you get you get union of all these which is the least step of bound of this chain so if you take the least step of bound of this chain you are really computing the limit of an infinite sequence and d star is just is just the union of these dks i mean a i took a short cut here by writing d star to be the unions of these dns but i could equally well regard d star is being the union of these dks okay so d star is a limit that is reached by successive approximation right so now okay so now what what we are going to show what we are going to have is a similar piece of reasoning for while loops right  refer slide time 23  23  so um so what so one of things that it that we that every that every body gives us an operational semantics is essentially defines operational semantics of the while loop in terms of itself but what they usually mean when you define when you define  noise  the meaning of the while loop in fact if you look at our operational semantics in terms of those two rules it essentially says nothing more than this right essentially what you are saying is the um the solution to the what does the if you ask the question  refer slide time 24  35  what does the while loop do then what you are asking is a solution for this equation which is a fix point equation i mean you are looking for a fix point of this equation and what we like to do is to define the notion of successive approximations to obtain a solution to this fix point equation right and just as in the case of finite sequences i can define the successive approximations like this so firstly like we have an undefined value an expressions i can define an undefined command in the sense that any looping command is an undefined command okay so the notion of approximations that i am going to have is going to be relative to this notion of undefinedness okay now so what do i do as in the case of sequences as in the case of newton raphson as in the case of i don t know context free grammar was what ever we define a solution to the fix point equation in terms of successive iterations in a in terms of a recurrence and he has a recurrence  refer slide time 26  06  so let me define wh not to be this looping construct and whk k plus one is defined in terms of whk so its no longer recursion but it is inductive so i mean this is like a recurrence so essentially what we are saying is i will define whk plus one to be this construct if b then begin c followed by whk okay and what i am essentially saying by this construction is is that supposing i have a while loop and supposing it um for the given state it requires some k iterations to terminate okay then its behavior is exactly captured by whk plus one okay if it requires k iterations before it terminates then whk plus one is also equivalent to do k iterations because of that same while loop um so what i have um my construction of wh k and whk plus one and so on really depends upon um the two components of this while loop same boolean expression and the same commands which forms the body of the while loop so if you look at this way then whk plus just represents the same behavior as the while loop for all states from which you require to do only you require to do at most k iterations in order to terminate and get an answer the moment the number iterations becomes is k k plus one or more then this whk plus one goes into an infinite loop because it will it will hit this omega here i mean note that this is inductive so which means after after k iterations if the boolean is checked out to be true then it will do one iteration um it will execute this body and then go into omega which means it will never come out of the loop so the so the main difference between the actual while loop and this whk plus one is that where is that actual while loop might actually might terminate some time after k plus some iterations this whk plus one will never terminate um  refer slide time 28  41  so so whk plus one is the so the the sequence wh not wh one wh two and so on and so forth is an increasing sequence of approximations to be actual while loop right in the sense in the sense that wh not will never terminate wh one will do all the computations of the original while loop which require no iterations whk will do all the computations of the while loop which require less than k iterations but no more and each of these therefore is an is an approximation along the definedness along the definedness ordering okay and in the limit now this definedness ordering its its its um look a little too square may be but it quite similar to this it is quite similar to this sequence  refer slide time 23  23  this is of course well rounded but um what i mean is so its actually a sequence of approximations and infinite chain of so you can define the while loop iteratively as an infinite chain of an approximation they come and closure to the behavior of a original while and this limit where k tends to infinity i mean this is a loose way right i mean dedekind would not agreed to this but what i mean is corresponding to this corresponding to this less than or equal to its possible to show blab blab blab that this ordering on programs is the partial order its not just a partial order but least steps of bounds are well defined so there is there is an operation there is a both are binary and least step of bounds are of course commutative and associative so there is an operation of finding least step of bounds which looks like a squarish form of the big union big set union and that least step of bounds always exist  refer slide time 31  38  once those least step of bounds always exists what you have is that is a limit is defined as the least step of bound very much like the limit of a increasing chain of sets is defined as a union of those sets right so this while loop and of course for details will have to do a course on semantics of programming languages but um but this while loop is really expressed as in a way in a form very similar to the newton raphson um method of computations hence exactly the same kind of thing you have a fix point equation you define recurrences you define a recurrence in such a way that you get closure and closure approximants to the solution the limit of closure and closure approximants is the solution to a fix point equation as simple as that unifies the whole of mathematics computer science and logic um um we can prove that if you really want me go into all that that s a full course in itself a half a course in itself the way at the speed which i go um so um so so lets not get into that um lets lets leave some thing for you to forward to in your future and its in after know that there are similarities polio style similarities between programs and other branches of mathematics um and finally after all the desperately interesting stuff um about least fix points about fix points and all that we come to the pragmatics of while loops and we find that its really too simple for words um so essentially what you do is you implement this fix point equation you replace left hand side by the right hand side so if you are replacing left hand side by the right hand side then what you do is you you implement it as an if then with the while loop again in between and and the boring thing about basic architecture is that loops are just jump statements i mean at some point so you sought of evaluate the code evaluate the boolean expression this is the code to evaluate lets say this is a block of code to evaluate the boolean expression if and of course the result is stored some where in a condition code or in a register or some such thing and you look at that register and you if its if its zero then you get out of the loop  refer slide time 34  08  if it is non zero then you execute this code corresponding to the body of the while loop and after having executed it you jump back to the beginning of the loop right so um so that s so that s really what um so that is the analogy between the data structures and the control structures and the the while loop is um i mean is very close to recursion but it easily the most interesting control structure of variations in the while loop and or easily the most interesting control structure and you can see the similarity between that and the data structure mechanism of sequences if you look at further at our approximations actually what you what you see is also that withs analogies get carried for example if you this um this approximation uses the sequential composition whose analogy in the case of sequence was the record which is a tuple formation right so if you look at the corresponding solution for um for sequences you have wits analogies exactly being carried through right and then um we have a case analysis here which is which corresponds to variant records which really correspond to a summation disjoint unions and so wits analogy carries exactly from from data structures to control structures um so even right after the solution of these equations um so so i wont go too much into the other control structures except that there are lots i mean you can do a similar analysis of the repeat until statement and you can do analysis of the false statement so on and so forth but we have defined i mean we are um both of us some of you have defined the semantics of the false statements and and other other other forms along the line so there is no problem really with with talking about the pragmatics of these control structures  refer slide time 36  49  so what i would like to do now is quickly go through the alternation um so other variations of the um control structures you will find in the next tutorial sheet um so that s so that s saves me a lot of hazzle okay so if then else of course has this straight forward syntax directed translation the if then as a similar straight forward syntax directed translation you can look the any compiler like the pl zero to see that um the only the only thing is if if you are implementing things in in a non recursive fashion for example if you are not using a recursive design parts in technique if you are using a recursive design parts in technique then there is no there is really no problem in the code generation of the translation because as part of the parsing the moment you are recognized if then or an if then else statements you you would have you would generate the code for it the code that is generated actually by the if statement block if statement procedure is really what ever is given in black the code that is generated by the procedures which are called from the if because after all the whole parsing the whole parsing technique is mutually recursive so on and so forth so when you see a boolean condition or when you see an if then you transfer control to the boolean condition um recognizer and that also generates the code for evaluating boolean expressions and so on and so forth and then since it is recursive you return to back to the if statement and then you generate this code this instruction and then you have to call command again to generate the code for the body of the if then and then of course you return again you can actually you actually get back to it if you are in the if you are doing a parsing by methods other than recursive design recursive design is is a simple is a simple parsing technique which but it require recursion to be available in the programming language right so if you are using some thing other than  noise  recursion in your parsing technique then what you normally do is well you well you normally back patch and what is back patching you you after you generated this code i mean there is a forward reference to this um label right you don t know what is this what is going to be the instruction number corresponding to this out when you generate this you don t really know what is going to be the instruction number corresponding to this right so after generating c after generating the code for c only then you know what is going to be the instruction corresponding to this right so what you do is you you keep this blank and a make parse pack wards and fill up this address at all these appropriate um occurrences so what you do is you actually maintain a list of blanks which have to be filled up for each different for each different label you actually maintain that list and keep keep filling it up backwards as you go um so as you get a new reference to the i mean this is something that is useful for example when you programming an assembly language you have to know it um right otherwise you are going to get into the some deep trouble so you would chain all references to the same label especially forward references backward references there is no problem because you can always see the code reference you can you know you know the number of the instruction which is label with the backward reference but forward reference is you collect you create a chain of all the forward references and when the forward references address finally got resolved you traverse backward on the chain and fill up all gap so that aggress means called back patching or forward chaining um similarly if again similarly in the case of a if then else what it means is that you have to you have to generate two jobs um to the um after some outer portion and you could generate these codes recursively um right the most important thing um that as far as conditionals are concerned otherwise things are not very not particularly interesting except that there is some thing that is often used which i though one should look at and that is the case statement so if you look at a standard pascal like case statement um then um one way of looking at this case statement is that is actually equivalent to semantically equivalent to this sequence of statements right i mean look upon this either as a sequence of a booleans which equate e with each of this integers and then executes the corresponding block um so either at i can look up on it as this piece of code or i could look up on it as an if then else a huge nested if then else which checks for each of this case in sequence but the whole point about the case statement is that it s a very highly symmetrical statement and what one would like is that the order of occurrence should not should not matter i mean for example its um the alternative should commute and associate in any order right and but the more serious pragmatic problem is that if the evaluation of e itself can create side effects then what it means is that this equivalence this semantic equivalence is no longer valid right  refer slide time 42  57  this semantic equivalence so the case statement is really some thing you evaluate e so if you look at the semantics of the case statement which actually wanted to be an exercise in semantics for you but essentially for the semantics of the case statement is that a if e goes in okay lets lets be a little more rigorous about it given a state sigma if e goes to on zero or more evaluations a value m then this case which i will leave unspecified end sigma goes in one step to what cm coma sigma right this would be my rendering of the semantic of the case statement and this rendering means that the expression e is evaluated exactly once is not evaluated only if only if you specify this as a meaning of the case statement can you ensure some how that there are no arbitrary number of iterations possible there are no arbitrary number of evaluations of the expressions e possible right and this is a simple enough rendering and so what we would like ensure even in the case of side effects if this definition has to be if this rendering has to be true then what it means is that e should be evaluated only once right  refer slide time 44  02  so what most implementations of the case statement actually do is that they look upon the case as a form of a switch right so so what they do is they look upon the cases of as a sought of switch and so so um so what you do is you evaluate e only once and lets assume that code for evaluating e stored the value of e in some register r right so what you do is you create what is known as a jump table so if there are n cases to be considered then you generate n jump instructions in sequence so and what you do is you also generate an instruction with jumps to what ever is the current instruction pointed plus what ever is the value of e i mean for the um present moment i have taken e to be an integer expression okay but if it something else enumerated data type as long as its some enumerated data type can always map that expression to an integer expression from one to n right so then what it means is that i jump directly to um an appropriate instruction in this jump table there is only one possible jump and that itself gives the jump code to an appropriate code so what happens is that if if i have the labels l one to ln in successive instructions and if the if if this expressions evaluates to some m then i would be jumping to this the m th instruction in this table so this is the jump table okay its actually like an array of okay so its actually like an array of jump statements and you jump to lm and that in turn is a jump to at the actual piece of code which constitutes the body of that okay so this ensure for example that e is evaluated exactly once and its evaluated very fast  refer slide time 47  00  i mean since you are not doing success repeated evaluations of e e could be a very complicated expression but it means is that um you are making a very efficient implementation of the cases by using a switch rather than by um rather than by using a sort of equivalent um shady equivalents it is not always equivalent if there are side effects using if thens or if then elses right how ever of course what all these means if you if this implementation has to be successful what it means is that this e can not be anything other than enumerated linearly ordered type with a well defined successor predecessor function okay except for the extremal points of course secondly if the case analysis is not exhaustive yours your automatic code generation for a case that is not listed in the case statement what it will give is a no op i mean that s there is going to be a blank instruction if it your if the case analysis in statement is not exhaustive then what it means is that you would have generated this label that label lm you would also generated a corresponding lm but there might be nothing in that lm so what actually is going to happen is quite unpredictable which is also the reason why but um which is also one reason why the case analysis should be exhaustive and the i also standard pascal actually specifies that it should be exhaustive right but how ever what you can do is that at translation time itself you can actually point out an error for each case statement to determine whether the case analysis is exhaustive or not okay except now um and so um so what it means is that since the runtime checking is going to be very difficult it should all this analysis is that the translation time which of course most pascal compilers do not right they do not and further what is happened is that most um most implementations of pascal also they they assume that you are going to have such a large number of cases that you may not want to exhaustively um do case analysis  refer slide time 48  52  so what they have is an ls closure and other closure which acts as a default in case none of those cases true is true okay now problem with that is that i may have just forgotten one case when without my intending to it actually goes into the ls closure of the others clause and executes something and gives me wrong results where as if you did a translation time check if the compiler actually did a translation time check and found the exhaust found the case analysis to be non exhaustive then what it could do is it could point out a compile time error which will solves the problem if its a matter of repeating code that is not a problem because for example pascal allows you to give lots of cases as um lots of case labels for the same block of statement right so it is not a matter of repeating code but the fact that you might have forgotten a case is more important and should be pointed out and especially since it is runtime check since it is compile time checkable there is absolutely no reason why you should have an l s closure and others closure um exact that is the main problem in this right it should be some enumerated type but if it is some enumerated type like just integer i mean which can range from minus max int to plus max int and if this is going to be how you case analysis is going to be proceed then just think of it you are going to have two times max int number of jump statements which is a phenomenal explosion in the code in the code length in the code that is generated might so of course but it is in the programmers own interest in such cases to use an if then else and rather than a case statement if you have a any number of cases in your enumerated data type but there are similarities um but there are going to be a very small number of actually distinct cases um then you are better of using an if um nested if then else than a case statement  refer slide time 52  10  which intimated case minutes um its a its a decision that you are the programmer have to take right is it better for me to use a case statement in this case um in this kind of a situation or it is better this is one extreme others you actually have supposing e actually belongs to some finite enumerated data type user defined enumerated data type then there is absolutely no reason why you can not use the case statement okay but if it belongs to a sub range type it is going to be huge then it s a programmers decision to decide is it more efficient to use a case statement or to use an if then else nested the compilers decision it is not going to take it is not going to take a decision in fact one of the aims of introducing a case statement in most languages the case statement is very much like the computed go to fortran in which which may be a parents would know about um okay and its its its not its not a decision that is going to be taken by the compiler it is a the case as a switch is meant to provide an efficient switching mechanism okay so all implementations of the case unless extremely foolish are going to provide it as a switch mechanism because implicitly there is an assumption in the semantics that there expression e is evaluated only once okay now the compiler is not going to take any decisions if it is case it is going implement as a switch thats thats what any compiler write about ensure the decision is a programmers decision do you want this inefficient case like this or do you would you rather use an if than else knowing fully well how the case in the if then else are implemented or knowing fully well what the semantics of the case and the if then else are it s a programmers decision and not the compilers writers decision well that is also a possibility but the case as a switch actually is what is the most common implementation of case statement because you really want to look upon it as a multi way tree of n cases so you don t want to worry about if then elses right i mean you you would like to be faithful to the syntactical structure of the statement itself so the um the point is any programmer who writes code like this is just foolish and probably deserves what he gets um so thats thats a whole point um so it is a programmers decision and its its interesting to know that and this this decision of a lot of compilers to provide this extra feature of others closure is actually completely misplaced because it can only create more errors due to forgetfulness carelessness than it can solve problems right okay so so the standard pascal does not allow for such thing though most other versions of implementations of pascal that have seen actually allow for an others clause for a default clause in case that when when the case analysis is not exhaustive transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 24 non-determinacy  time 1  00 min  iit delhi welcome to lecture twenty four so today we will have a short talk of non determinacy um well there are actually there are there is a lot of confusion about this word there is non determinism which is as oppose to determinism and determinism is an ism so and determinism is an ism so its philosophy which talks about the eventual which talks about destinity destiny fertility and so on and so forth um determinacy is a property of a system the predictability so that they closely related but they are not the same um there is of course correspondingly there is also indeterminacy which often means you don t know it might be deterministic so indeterminacy usually it just means that you don t know non determinacy means that there are several possible choices and you can not predict which choice actually be taken um so indeterminacy so so for example division by zero is indeterminant right so non determinacy is a property of systems by which by which there are several possible alternatives may be an infinite number but or a finite number but there is usually at least more than one possibility and determinacy the property of determinacy itself is an degenerate case of non determinacy where there is exactly one option right okay so we were looking at the case statement and this non determinacy or choice is really a generalization of the case statement so but lets lets motivated by by an example right so lets take this example so the problem is lets say given an arbitrary positive integer what you have to find is you have to find the largest m and n such that two raise to m multiplied by three raised to n divides lets say the number a fine so um so you have to find i mean um its its its its it s a small part of lets say unique prime factorization where you are restricting yourself to just the powers of two n to three right so um so lets just look at this so given an arbitrary positive integer of of course when a is zero m and n are well infinite indeterminate what ever um but when a is positive of course you will have some definite number m and n right so so here is a simple pascal like program segment which just counts the numbers um the number of twos and threes is found so far so what what you would do is the you would well one possibility is just that while you found that two is still a divisor of a just keep incrementing twos by one and keep dividing a by two right and once you are through with all divisors of all the divisions by two you start with the divisions by three and you find that all the number of divisors of three the number of times three divides um this number right  refer slide time 4  30  so now if you look at this there is an equivalent program of course in which i could have flipped these two so i could have flipped these two while loops and it could have create it would have um and it would not a matted at all but there is also another equally viable alternative in which i do not care to first find all the devices of all on the number of times two divides the number the number of times three divides the number after all the two things are really quite independent so i could decide to divided twice by two if it does divide if four is in fact divisor of the number and then start at a few threes and then to come back to looking at a few more twos and then come back to looking at a few more threes till process ends till it is impossible for me to find the number is till it till the number is no longer divisible lets say by two three or six right so now this is a form of i mean this is a perfectly valid computation right i mean its its a fine days so today i divide by twos it s a cloudy day so i divide by threes i mean  noise  and i continues this process after why is that in any sense less come um less important as a computation than going about it in in such a rigid and straight jacketed manner right so what what dykstra ones coined is that he coined a non a non deterministic choice mechanism which works as follows so here is a non determinate solution so the point is this here the way the control goes is as follows you take you have this choice between you have this choice between these two alternatives you can look upon this okay as checking whether this um whether this boolean condition is true this boolean condition is true and what makes it non determinate is that both boolean conditions could be true at the same time right i mean it is not its not necessarily that only one boolean condition will be true at the same at one time both could be true at the same time and so what i do is i take all these choices and put then out together i just separate them by this well on a keyboard you just probably be a um a square brackets placed close together but so what i can do is this actually models this phenomenon of i mean um i feel like dividing by twos divide by two then i feel like dividing by three um without exhausting the twos um i do these things in any order so if you actually looks at the computation defined by this program then what it means is that there are several possible orders i mean this is a multiplicity of orders in fact um there are there are if m and n are the number of twos and threes that divide the number then its its shuffling of all those it s a shuffling of m plus n um lets say the twos are colored red and un and red marks text and threes are colored with black marks text then how you shuffle them all the different orders it is an exponential number by the way okay its its exponential in the number of m and n right in how many number different ways can you take m card of one color and intersperse them um with n cards of another color right  refer slide time 9  10  so the number of choices therefore is the number of different possible computations that you can have is actually phenomenally large but the whole thing is the whole program itself is deterministic in the sense there is a single number m and a single number n which the program computes right so if the program were wrong then and in general in the general case the fact that you have lots and lots of different options open at every stage means that you could also have lots and lots of different answers coming coming at the end of program if it ever ask terminate right so but what this actually does is that it generalizes what what it actually shows is that a programmer can at many times not want to be bothered about imposing an artificial sequentiality in this computations artificial i mean this the this the program the sequentiality that it imposes because of the language because of the design of the languages entirely artificial right its got nothing to do with the notion of computation itself there is nothing there is nothing dictated by the notion of the computation which necessarily says that it should be done deterministically only in this order or in the other order so this is a freedom which a programmer might want for various reasons which i will come to later lets take a look at another solution which is actually a more famous solution um for this for this i need to coined by david greece um from in his and it is described in his book the science of programming okay it is called a welfare crook problem i have stated a very prosaic and dull version of the problem but the original problem is actually that um well there is a there are three lists of employees at ibm thomas j watson research center um students at cornell university and people in new york state who are drawing dole welfare you were drawing government welfare and unfortunately the lost they have in the country are such that you cant be a full time student and an employee at the same time you cant be an employee and draw dole  refer slide time 12  30  you cant be a full time student and draw dole and so on and so forth but apparently there is one guy um that country and that city really breed such people  noise  who who is actually in all the three lists there is at least one guy who is who is in all the three lists and the idea is just a find find lets say the first one the first such person so bringing the problem down to earth is um always saying that there are three let us say three sorted sequences of numbers with at east one number in common in all the three sequences and what you want to do is to find the least number that is common um to all the three sequences so um lets just and i am not particularly bothered about the bounds and the sequences are put a question mark here but lets assume for simplicity where we have got some large arrays right oaky so again if you were to take if you were to program this in um in any of these standard programming languages then you will get a deterministic problem of this you will get a deterministic solution of this kind as long as you have not found the common element which is what is which is what is represented by this not okay do um while ai is less than bj do i is assigned i plus one while bj is less than ck do j is assigned j plus one while ck is less than ai do k is assigned k plus one right and of course these three while loops could be written in any order right and the whole point is that  refer slide time 13  39  when you terminate from this while loop what you can claim is that for the current values of i and j ai is greater than or equal to bj aj um bj um bj is greater than or equal to ck and ck is greater than or equal to ai and therefore it follows that ai must be equal to bj must be equal to ck i mean you cant have a cyclic greater than or equal to relation unless they are all equal right so so any computation of this would do and of course one problem is that and there are too many while loops in this program so may be what we should do is we should locate another program right so if you look at another program so um is this clear so if you look at another program the what i could is i could convert i could transform all those while loops into just if then statements right hand after all there is this loop this condition that guards the entry into the loop and so i can not enter the loop um if if i have in deed found an i j and k such that ai is equal to bj is equal to ck right as long as and um and of course what happened is that in this case this is allows for  noise  a far more number of possibilities then the previous one um previous one was really straight jacketed within one iteration of the main loop  refer slide time 14  17  you had to first find an i such that ai is greater than or equal to aj then you have to find j such that um bj is greater than or equal to ck and you have to find a k um ck is greater than or equal to ai and then repeat the whole process till you have found an i j and k of that time this one allows for more possibilities right so it allows for actually well may be not but it allows for may be changing i j and k  noise  in a sequential fashion i mean the first chain i if it necessary then you change j if its necessary you change k if its necessary and go through the process all over again but it is still total deterministic right it is totally deterministic and therefore also it is easier to reason  noise  with such a deterministic programs so a non deterministic programs for this problem would just would just be the following um right so there are three conditions that outermost complicated condition is not necessary because um if all these conditions if all these three conditions are false it means that ai is greater than or equal to bj is greater than or equal to ck is greater than or equal to ai and therefore they are all equal right i mean um when does the loop exit when all these conditions are false when its impossible to execute any of these when it is impossible to go past these guards these are called guards so these things are called guards so if you go can not pass these guards then the loop terminates so if there is at least one guard that is true then you should be able to go passed it um  noise  the square bracket is just um is just a syntax a piece of syntax to separate out the guard i mean um if you like its okay will square bracket is just syntactic notion but its it also means that its symmetric operation um we will come to that when we do the operations semantics of this constructor um no this square brackets indicate that there are so many different choices so if there are m square brackets then there are m plus one different choices okay i mean its like supposing you are doing of addition of a large  noise  i mean um supposing you are doing um adding m plus numbers then you have well m plus signs in dipping right in in the infix notation so you can think of this is an infix notation to separate out the various choices right in particular so lets lets look at this syntax there are two there are two analogous kind of constructs for non determinate choices one is the if construct and this if construct is just says that there are all these boolean conditions they are all these boolean choices corresponding to each boolean choice there is an appropriate command to be executed right  refer slide time 18  32  in in the case when when control enters this this if guarded command all these booleans um one of these booleans which is true is executed you go you can go passed a guard if that boolean is true otherwise you can not go pass the guard how we don t know and the moment but if bi is true then ci can be executed right and in order to ensure that since its its since its much harder supposing since its much harder to um visualize all the different possibilities what it means is that the programmer has to be careful in a to ensure in the case of this if that he has to done in a exhaustive case analysis you can look upon this construct as a generalization of the pascal case and the if then else put together if you were generalize both the if then else and the case in order to get a new construct what would you do the if then else is an asymmetric construct right it it specifies a clear um line of control right if you had to come the case is a nice symmetric construct in the sense that in the sense that  noise  of course it  noise  but the case is restricted to enumerated values so you choose um and of course pascal and what ever from all the semantics that we have done so far we know that they are all deterministic so there is only one possibility in the case which might actually be which can actually be executed i mean an expression from our expression language semantics it is clear where it can have only one value for a given state and if it is going to have just one value then there is only one possibility of the case which is which is true right so if you generalize the case and the if then else then what it means is that you get something like this which has the symmetry of the case statement but it is not necessarily restricted to enumerations it is not necessarily restricted to um ordinal values its its um its a full blown boolean condition as you would use in any conditional statement and the fact that you are using full blown conditionals they um they could be any they could be any boolean expressions means that more than one could be true at in a in a given state if more than once could be true then it means that more than one of this ci could be executed but the if stays that um only states that you can execute only one of them so which means that if more than one is true there are several different possibilities so if you execute that if on a friday may be the ci will be taken if you execute it on a sunday you may be you will get or a if lightening strikes the earth may be you will take some cj for some bj which is true what i mean it is really non deterministic i mean um its an act of god um which god is going to be taken right its its really and when i say an act of god it means that there are there are reasons which are unpredictable because of which one particular execution might be chosen right in the case of the do of course what happens is that as long as at least of these guards is true the loop keeps executing and it exits the moment all the guards are false right so lets just look at the operations of the semantics of these two things at the moment i will assume that i will assume that if stands for this thing lets guards bone to bm and commands cone to cm and they do similarly has guards bone to bm and command cone to cm right so the operational rule for the if is as follows and its and this is what makes this is first instance of a rule which actually makes a transition system i mean we can we can add these rules to the while um we can add these constructs to the while language for example for which we have defined a deterministic set of rules right the moment you add these constructs to the while language and give this set of rules you get the first instance of a non deterministic transition system right all these side conditions says is that all this rule says is a if there is at least one if there is some bi which is true then this statement this if can um can transfer control to the command ci period so if there are two um two or more different values i and j for which the bi bj are all true it doesn t say which one is going to be execute um it only says that um if both bi and bj are true since there are i mean what i mean there are m different rules for the construct okay and all those m different rules one for each i and all those m different rules only tell you only tell you m different possibilities okay so if every guard is true for example then all that this rules say is that all that this these m rules says tell you is that well one of them can be executed that s it one of these ci s can be executed it does not say anything more not is it say anything less okay and just in order to ensure now the moment you introduce a non deterministic set of possibilities what happens is that reasoning about the program depends far more difficult i mean after all you do not know what implementation you will choose will use what policy to choose which guard which the command corresponding to which guard is to be executed you look upon all these guards as forming not a sequence sequence is um there return in sequence only because well textually you have write things in sequence they actually form a set of possibilities so any any possibility from the set could be picked up and the corresponding command could be executed right  refer slide time 27  04  so in which case so okay and just in since it s the reasoning process is much harder in this case because of the fact there is an explosion in the possibilities the whole transition system now becomes non determinate non deterministic and so what you have to ensure is that your if statement if it is non exhaustive for some reason then it should come out to be an error okay so note that i once in the medieval ages said that we never have any negative rules this is well as i was explaining it it looks like it is going to be a negative rule that if none of the gods is true then the if statement gives an error but actually this it can be expresses the positive rule right so um so if this this junction is false that means none of these possibilities is true then this leads to an error um this should ensure um this this kind of thing should ensure that the any programmer has to be careful to um to make an exhaustive list of all the possibilities it is not necessary that bone or btwo or up to that the disjunction of all the guards has to be necessarily a tautology its not necessary but what is necessary is for the set of possible states it should be possible to prove that only those states are possible which make at least one of these guards true and the two things are different right i mean if you look at this is a logical formula it need not necessarily be a tautology but i am not be able prove above the program before entering the if statement i might be able prove that there are only these many possibilities that there are that the state has to satisfy one of these properties and so what it means is that then this disjunction need not be a tautology in the sense that it need not always be true but for that set of states alone it should all it should always be true right that means there are only a certain assignments are values possible for those set of states um right so for example in the case of this twos and threes problem what it means is in the case of the twos and threes problem supposing i were i require this condition  noise  the meaning of the do is similar except that its a matter of exiting right i mean its in the case of the loop in the um it s a matter of an infinite computation that is the only difference right in the case of this if i know that this state is always one in which a is going to be greater than zero i mean this supposing right if i know that the state is going to be one in which a is always greater than zero then um and i have an if then else um of the form of some thing then if it might be just necessary for me to show um then this itself is sufficient conditions so that these things the guards need not form a tautology i mean lets take a negation of these guards also you can like right they need not form a tautology i mean they it could just be that for that set of states in which a is guaranteed to be initially um is always guaranteed to be greater than zero for an invariant property in which a is always be greater than zero if the states satisfies that invariant property than i am guaranteed that lets say the loop will exit without any problem um the loop will exit only when all these conditions are true so um so it is not necessary to have this forming a tautology it is only necessary to ensure that its its its satisfies an invariant property of this possible states that will actually be encountered before entering this if condition so that it does not exit in an array  noise  so it is exhaustive in that sense but it need not be a tautology right so the  noise  the rule for the loop is um is very similar to our while loop and it just tells you that as long as some bi is true um to execute the body and repeat the loop and of course this in the case of the do we don t um there is no question of error you have to exit from loop so if all the guards are false okay then by um by fact that there is no rule specifies that it follows that the loop will be exit right they do is actually equivalent to this while loop and of course the conditions in all these cases are that there is there are no side effects cause for the evaluation of booleans i mean its modulo that um if boolean evaluation produces side effects then it matters how many times you evaluate the boolean in the case of the do which ever boolean you choose to evaluate and more over if the boolean evaluation cause side effects than that adds it soon it soon complexion to the program i mean that its own non deterministic to the program which is often unpredictable which is often implied rather than explicit where as the do construct makes a non determinism explicit what ever and you don t have a side effects um then it does not matter how many times you evaluate these booleans they are going to yield the same value  refer slide time 33  08  note that you are going to evaluate some boolean inside this if so which means it you are actually evaluating some boolean at lease twice in this in this equivalence where as in an actual definition of the do all you are doing is you are evaluating one boolean you are evaluating booleans as long as there false you keep evaluating booleans may be till one is true and then you follow that guard right so side effects are always nasty business especially if you have to prove the correctness of your program and amazing thing is for a long time when dykstra actually coined this produce these things for a long time this is used for constructing perfectly correct toy programs but now this is actually being used in the ada language okay so lets lets summarize the properties of the of non determinacy in generals if in particular may be so it s a generalization of the case you can use arbitrary boolean expressions as guards more than one guard could be true at a time it um it aborts if the guard is if no guard is true  refer slide time 34  40  this is for the if and what it allows you is that what it allows is the specification mechanism right so it is an excellent specification mechanism because you do not really very often the programmer does not want to be bothered about an nitty gritty s details of how exactly a computation should be performed and what most of the languages do is that they force you to specify a certain sequence in which computations have to be performed where is often in many cases there is absolutely no reason why the computation should be performed in that order so so you can look upon the if we as a symmetric generalization of the if then else the if then else is itself is a deterministic construct and its also a generalization of the because it allows arbitrary boolean expressions and what it does is it freeze the programmer from imposing an unnecessary sequencing of operations but it also makes life more difficult  refer slide time 34  58  in the sense it opens a variety of possibilities so unless your program is proved correct but proved i mean proved in a mathematical sense of a proof unless it is proved correct there usually no guarantee that a program will actually work as you indented to right and for example it is not um it is no it is no where we specify  refer slide time 36  01  to the programmer in what order the boolean expressions are to be evaluate okay one possibility is that one possible implementation is that i take all the boolean expression i evaluate all the boolean expression values and out of the once i have created in the set of indices of the true booleans then i run a random number generator program and pick out one okay that s one possibility and other um very simplified possibility is that i just go through the boolean expressions in sequence till one is true but i can but they can be any kind of complicated scheduling mechanisms and the whole idea is that um its specification mechanism in the sense that you since the programmer is not forced to impose an artificial ordering it means also that the actual implementation is to be hidden from the program how exactly is this if statement going to be implement implemented is not something that s going to be known to the program and if you wants the programs written in this language to be portable what it means is that it should work under all possible what might be called scheduling policies how you schedule the evaluations of the booleans in a guard in an if or a do statement so its um if you want so so which means if if it is going to be if it is going to work under all possible scheduling assumptions and it does not use any other kind of mechanisms that is why its it is a symmetric it is a symmetric construct both both the constructs are symmetric means that if you if you change the order the program should not change if instead of writing them in the order b one arrow to c one to bm arrow cm if i reverse order or shuffle the order in any way the meaning of the program does not change and if the program was proved correct then it should not matter either in what order these guards are written or in what order they are taken by the runtime system to execute right so it s a specification mechanism which freeze the program which freeze you from um imposing an artificial constraint but then it also imposes um it also imposes um its also brings in its own unpredictability of the result so if a program is wrong i mean different implementations could give you different wrong results i mean and and amazing as it might seem one of them might actually give the right result because the implementation took a certain decision which happened to match with the programmers intention of  noise  or his presumption of how the guards are going to be evaluated oaky so  noise  which all goes to show it doesn t mean a damn thing if you if you actually gets the right results from an execution um  noise  so it is necessary therefore  noise  the obligation for proof then becomes absolutely essential in such a situation the moment you have non determinacy what it means is that you have to have a proof and how can they proof be if the proof is going to be of the form which tries out each and every possibility then its not much of a proof its just a hand execution so what should the proof be like the proof is some thing that should exploit the invariant properties regardless of what choice you take you should be able to specify one single general invariant property of the loop and regardless of what choice is taken should be possible to show that your eventual goal is going to obtained through that invariant property only then the proof has some meaning so this construct was actually used by was created by dykstra um to to be able to develop programs in such a way that if program and proof develop side by side and the proof leads the way to development of the program right i mean so the whole the whole philosophy is that um its not that you should write a program and then prove it correct that s an after thought and a proof after the fact is really um is really of no importance except of an academic importance only but what is essentially is to use the proof as a guiding tool to develop the program so you have to specify the invariant properties you have to do a case analysis at each time um they should be amitating a programs and then based on that a program should be developed which is some thing that we tried to teach in the early programming courses doesnt sync in though so  noise  so the whole point is that you have to exploit the property of invariants invariant properties so that that the number of options that you have regardless of the number of options you have you are guaranteed that a certain theorem can be proved certain goal can be proved right so all this was actually motivated by dykstra s own experience in the design of multiprocessor multiuser operating systems right  refer slide time 43  00  so now if you look upon bringing down various operating system concepts and encoding them in programming languages which is which is a very useful thing to do in the sense that um then you can write the entire operating system in that programming language you can bootstrap an operating system on top of another for example um an ordinary general purpose operating system you could write a real time operating system right if you um wrote a general purpose operating system using non determinacy then the portability of your programs also increases i mean some body might just decide to attach one more processor to the current machine and your operating system should be take should be able to take care of this extra thing it should not be necessary for example to re write the entire operating system so you be able to write the entire operating system also in high level programming language and not depend on um assembly language all the time which is what is being done right now right but the area programming language for example designed remember designed bomb in nuclear installations in iraq right so  noise  the whole point is that what motivated the design of ada is that the us department of defense found that it has four hundred five hundred may be a thousand installations and at least a hundred different programming languages being used in those installations its not a joke and what they decided was um as a result what it meant was that um in difference of course you keep getting transfer here and there so what it meant was that when program was transferred from one installation to another even though the system architecture and everything was identical just because of programming language was different they have to spend a few um a months learning the new programming language learning how to use a system and a lot essential time was wasted this is a fact it s a documented fact by the us department of defense and secondly what it meant also was that you could not move port programs across installations in the sense that i am having a new pursing um missile installation some where and i cant move the old programs because well that system was by a different company and it supports different compilers and it does not support any of the compilers for which i have installations right so what it means what it meant was re writing the entire software in a different language and when you transfer personal to and fro what it means is that they spend the first six months just coming to grips with a new installation a new piece of hardware a new piece of software um a new language a new system called conventions a new procedure called conventions and a whole lot of thing i mean it s a whole society in itself in each installation and there is very little interaction between different installations so what they decided was that except for their business data processing i mean not that they do business i mean which a lot of people are accuse them of doing but um in the terms of their accounting and pay roll and so on and so forth this found that cobol was sufficient but for all that scientific i mean they do a lot of research and pure science right otherwise you cant do the some of the things they do right um and for all their control systems and for all their non business applications scientific numerical control um concurrent um distributed you have a wide variety of installations which have to communicate in a um you have to have things working in a distributed fashion parallel may be you require fast computations which with with um parallel computers vector processors and so on and so forth reactive you have control systems which have to react to external stimuli in the form of um in the form of sensors for nuclear reactors special controllers temperature controllers transducers you have um an you have to respond may be to a cosmic ray shower um and so you um you require a wide variety of different kinds of applications and what you have is an explosive variety of programming languages in which all those things are written and so for everything other than their business data processing what they wanted was a single unified language they went further to say that it should be a pascal like language which supports concurrency modules reactiveness responsiveness um and therefore if you are talking of reactive or responsive um software embedded in some hardware like a controller for a nuclear installation or a chemical reactor then what it means is that what that controller gets as signals from various transducers is totally non deterministic its not predictable in a highly decentralized mixture of mechanical electrical electronic hardware and software to actually predict when some thing when some signal might come from some where and therefore it is necessary for these controllers or even client servers in a in a more business like environment even client servers i mean take your railway reservation system and so on and so forth its not really when you look at the design of the server or a controller in isolation all you know is that you want get signals from this um wide variety of systems what you do not know is it what order are you going to get those signals after all you control on the basic purpose of the controller is that um if the pressure goes beyond some limit you have to um you have to activate some mechanism relates some electronic switches in order to bring the pressure down may be you have to keep pressure temperature volumes and some so on and some flow rate so on and some balance within certain tolerances so when one variable increases you do not so do not really know so that controller at in the at the design of the controller all you um all you will know is what kinds of signals are you going to um are you going to get but you do not know in what order are they going to come and what it means is that sitting as a controller monitoring an entire distributed installation what it means is that i am subjected to non deterministic pressure from a variety of um places right and so what it means is that i have to have a facility to deal with this and its not going to done by a human being i mean imagine um in a nuclear installation and a human being sitting there and trying to co ordinate the entire thing its all going to done by a controller written in a mixture of hardware and software i mean you go into hardware there is really no difference between hardware and software except that when you want some faster executions time sensitive responsive when you have the controller will have to activate um various procedures within a given time frame otherwise i mean otherwise the whole chemical reactive might burst right so they have to be time sensitive they have to react to stimuli which come in a non deterministic fashion in non deterministic order and therefore you require a convenient construct which is symmetric and since you can not predict in what order you are going to get these signals what it means is that your guards in addition to being boolean conditions they would also include lets say signal values okay if a certain signal comes then this response has to be sent some where not necessarily back to the same place right you have to activate some thing else so the fact is that any isolated program in a system in a large system the only way to cope with the entire system is not to know the nitty gritty s of the each and every system after all that system can keep changing i might replace something by a faster machine might replace by a some mechanical relays by a electrical relays i might replace some thing with more capabilities so then it s the only way to cope with the entire system is not to know the nitty gritty s of each component of the system but to know only broadly what kinds of stimuli are you going to get and therefore what should be a suitable responsible its an abstraction that you have to perform right so in a real time um that s another word i should have written real time these are all buzz words many of them are closely related um overlap and so on and so forth but they are different words right so in a real time concurrent distributed environment which is you have to be in a reactive system you have to be responsive and you have to do it in real time right so for that you require these um you require such a construct so the only difference is that the construct also includes communications from outside and also it includes timing mechanism which ensures that if something does not happen within a certain amount of time then something else has to be done um about it next transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 26 the lambda calculus  time 1  00 min  iit delhi welcome to lecture twenty six so today we will start the lambda calculus proper at let just briefly recap the whole idea of the lambda calculus was to look upon functions as first class objects to be able say what kind of a value of a function is  refer slide time 00  42  to look upon function spaces themselves as sets of points to give un named functions as characterization and which essentially mean meant that you have to be able to treat functions and also more importantly to give a fundamental theory of computation  noise  and the parallel with sets was something i pointed out that in when it comes to mathematics you can see the sets to be a fundamental object you re formulate they re formulated the whole of mathematics so that everything depended on just set theory and set theory was itself axiomatized in first order logic um similarly church attempted um using functions the concept of a function as the fundamental object of a computation and you could also define sets as functions um the membership predicate and as i said i mean there is a lot of there is lot about the membership predicate which looks like um which can be make to be look like function application if you try hard enough that is um and the main important thing about function application is that you are really doing syntactic substitutions and its symbolic is totally symbolic  refer slide time 01  12   refer slide time 01  56  so even a child an animal or a machine could possibly do it  noise  so when it come to and the so the lambda abstraction so to speak or the lambda calculus emphasizes the difference between functions function definitions where functions are treated as objects and application of functions result of applying a function to a value object which might be either concrete or symbolic right so  noise  we will start with the syntax of the pure calculus so the lambda calculus is in important for one one very um is important because firstly it can be regarded as the mother of all programming languages especially the function programming languages it was the first um first programming language with a cleanly defined syntax and semantics and it was never implemented till very recently um so the pure calculus i will first look at the pure calculus so you as in the case of any logical theory you assume accountably infinite set of variables symbols given to you and you have nothing else right everything else has to come from the lambda um from the lambda calculus itself  refer slide time 04  05  so you don t have any predefined sets of objects unless you are applying the lambda calculus to some existing domain so and then the the grammars are actually very simple so so the any variable symbols is a lambda term and i have the concept of a lambda abstraction which contains a binding occurrence of a variable and a lambda term and since  noise  so i am um trying to maintain the analogy with sets as far as possible and then um so and then within for this binding occurrence there is a certain scope defined by the body of the lambda term so anything of this form is a lambda term and if you have two lambda terms the application of one lambda term um to the other enclosed in parenthesis is also a lambda term right so so it s a simple syntax with essentially just two constructs and as usual we could define the notions of free variables and bound variables um so for the lambda term x x is the only free variable for the lambda term which i will cal this lambda xl okay i do not really like church is original notations though that s what you will find in most text books i like this analogy with sets so i will use this notation so what i will call it lambda xl and this as all variables set occur in occur free in l x set to s as a free variables and in a lambda application um the free variables are the free variables are each of the constituents  refer slide time 05  17  where loosely speaking this lambda term l is like an operator applied on operand how ever they are both lambda terms since you have nothing you are starting essentially from nothing except variables you really can not distinguish between whats the function and what is the value so its um its very general in the sense that well functions might be applied to functions some of them might be meaning less some of them might be meaningful but if you allow this facility then you will also get higher order functions right so i will use capital um i am using blue here though which is against my convention to use if we use for a programming language for a very simple reason then most beginners find the lambda calculus very abstract so um so in thought we should use blue for that anyway um the set capital lambda is the set of all lambda terms and the set of all closed lambda terms that means um those lambda terms which have no free variables is the set of combinators um and so we have defined the syntax now lets define the semantics um so the semantics really is through a notion called beta reduction so a beta reduction has this as the primary axiom if you have a lambda term which is of the syntactic form that is it s a lambda abstraction applied to another lambda term then the result of the application is what ever is in green is the is a substitution the whole point is that this what ever is in green is not part of the lambda calculus language but if you remember um i mean the result of this application i have to some how express in some form in terms of l and m and what i claim is that the result of this application is you take the pattern of l and take all free occurrences of x remove those free occurrences of x from that pattern and substitute that pattern copies of the pattern m okay so this is a syntactic substitution which literally does the pattern matching and replacement okay so so this this thing in green braces this operation in green braces is metasyntactic and its its certain level of abstraction above the syntax of lambda calculus right so you take this pattern l look at all free occurrences of x in it and replace all those single string x patterns by entire patterns m copies of the pattern m right so thats really what this and this is called a beta reduction this such such a pattern which consists of a lambda abstraction applied to another lambda term okay is called a beta redex right and  refer slide time 9  57  um they have some fairly complicated names for all the things but lets lets just look at beta redexes and then after that you have to structurally close all these on in contexts so if m goes on a beta reduction to n then l applied to m goes on a beta reduction to l applied to n so what i mean what it means is i mean its possible that since i said that the what what looks like the operand of an to an operator even though i meant that itself might be an expression which is capable of being reduced right so if that is capable of being reduced in one step to another expression n then the result of this application reduces to the result of this application reduces to this application right similarly its possible that the operator itself the so called operator is capable of being reduced without actually applying it to um an operand so if l itself can go in one beta step to m then the result of applying l to m um moves in a single beta step to n applied to it um and lastly of course um if you have a lambda abstraction the body of the lambda abstraction could be quite complex and might be capable of being reduction um being reduced for example it might contain another abstraction applied to some application and so it is capable of i mean after all this is just a lambda term so this lambda term could itself be an application right so if l could reduce in one step to m then this abstraction reduces to this abstraction okay so these the rules beta two to beta four actually just close the notion of reduction to um to take care of all syntactic contexts so the main rule is really beta one um which gives a one step reduction now as in the case of our other operation semantics we could actually define many step beta reductions okay so um and what what actually church did was that he actually formalized all these also as rules  refer slide time 12  36  so on any step beta reduction which we have mostly taken for granted as a reflexive transitive closure of this is really this i mean this and this three rules just give the reflexive transitive closure so for example if l is capable of being reduced in one step to m then l is capable of being reduced in zero or more steps to m the reflexive closure just says that l can be reduced in zero or more steps to l itself and the transitivity just says that if l can get reduced to m in zero or more steps m can get reduced to n in zero or more steps then l can get reduced to n in zero or more steps right um i will talk about um weak reductions but loosely speaking a weak reduction a weak a weak one step beta reduction is something that does not have beta four so if you just consider the rules beta one beta two to beta three then then what you are saying implicitly is that you can not reduce you can not do any reductions inside a lambda abstraction okay so what it means is that unless its abstraction gets applied to something and therefore this x moves out and inside application can not be reduced okay we will see the reasons for those things later okay  noise  right so and what we could also do is for example we could um we could at an equality generated by a beta reduction right so for example you could take this symmetric transitive relation generated by the many step beta reduction many of course include zero um and what is the meaning of generated by this um well if l goes in zero or more steps to m then l is beta equal to m um notice that this equality is um not the same as syntactic identity it something that it will contain syntactic identity because l can go zero or more steps to l itself and so l is beta equal to l itself okay so its its slightly weaker than syntactic identity and if l is beta equal to m then m is beta equal to l and if l is beta equal to m and m is beta equal to n then l is beta equal to n right so this is the equality relation generated by the many step beta reduction and what it means is that um and this equality is actually very much um very much like the normal equality that we might think of in our algebraic computations i mean these things are all um these things are all motivated by similar notions of computation and considerations which come from algebra right so if you take the simplification of a complicated algebraic expression you go through a process of reduction okay using some rules or some theorems that you already have and in each case the step that you get after a reduction is equal to the step that you had before okay so that s how you simplifying get may be a single value or a an a simple expression right and symbolic computations also use the same thing it doesn t matter whether its values or symbols you still you go through the same forms of reduction except um okay so  noise  so these things are motive so all these concepts really come from your school algebra if you like i mean standard question number one after having studied a plus b whole square is equal to a square two ab plus b square question number one what is ninety nine square or the a minus b whole square equal to a square minus two ab plus b square question number one what is hundred and one the whole square what is the ninety nine the whole square and then you go through a process of reduction step by step reduction and that reduction has an equality that is already generated by the reflexive transitive closure of the notion of reduction and reduction is important in simplification is one directional okay you write ninety nine as hundred minus which is actually an expansion its not really a reduction but then that your school teachers don t know any way that there actually um you are actually expected to expand before you reduce in a different way right i mean after all what is to prevent you from just multiplying ninety nine and ninety nine in your normal fashion in getting the square but the whole point of that exercise was presumably to test whether you understood how to apply the formula a minus b the whole square so the so you go through the process of expansion and then you go through reductions okay where you apply these formulas these formulas the application of these formulas is like rules very much like the beta rules okay anyway we will look at we will look at that so this is the pure calculus and the whole idea of having a pure calculus is that it should be applicable to any other discipline i mean it should be completely independent of and applicable to any other discipline which uses functions okay so every functional programming language can actually be thought of as just an applied version of the lambda calculus and so well at lets look at the applied lambda calculus and see how this is how the pure version is really distinct from the applied version  refer slide time 20  30  so as far as we are concerned from um from the stand point of the lambda calculus itself the applied calculus is really nothing more than the pure calculus with a new production that which which consists of a collection of finite constant symbols and noting the fact that you are doing all this from nothing i mean you don t have the distinction between values and functions functions of functions and functions and so on and so forth these constant symbols could be either values of the underline domain of the application or they could be operators on the underlying domain of the application right now if they if they are operators in the underlying domain of application then what it means is that you also have your own reduction rules for that particular domain right so so the applied calculus is so this is how you would applied to apply the lambda calculus to any other domain which consists of values functions what ever i mean so in in general to any other kind of um algebraic domain if you like i mean if you some how convert domain into some form of an algebra um which is not very difficult to do if you have the will power so um so what it means is that so we will regard this we will regard all the constant symbol so these constant symbol are not just values in the underlined domain they are they include also the operators and the functions that you have predefined in that algebra which have their own forms of reductions okay very often you have equations which do the reduction like for example the distributive property on natural numbers so if you have some thing like a star b plus a star c then there is a reduction step which is a star b plus c or the other way also could be a reduction step i mean the algebraic equations give you two kinds of reductions if you like where reduction is only a name some times the reduction can be an expansion but then its also natural that in a lot of trigonometry in school you must have realize that it was it would have been necessary to expand something before reducing it so reduction is a general term um to denote some goal oriented activity towards some simplifying form you know which can not be simplified any further in the process  noise  the actual strings may actually expand but you know you don t worry about that if they get you to your goal then it s a reduction i mean this there is nothing else to be set about it um so so now um so what are meanings in a um so we will we will look at an application  noise  so what is the notion of a meaning now in a in any expression language you would say this is the meaning of an expression is the value that it some how reduces to i mean you have some complicated arithmetic expression then you would say that the value it reduces to is the meaning right so right okay so now lets look at an application i will i will tell you about the meanings of the lambda terms also but before that lets look at an application and the simple application that i have in mind is is the peano arithmetic now what i can do with peano arithmetic is that i actually if you look at the naturals they are merci stuff in the sense that there are infinite number of symbols if you like i know we don t what that and its horrible to deal with infinite symbols so you want so what you will do is we will simplify the naturals into as a grammar of this form um so there are only two symbols what ever is in light brown or ocar is the symbol of the natural the natural number is being completely down to earth a brown in color um so zero is a natural number and if m is a natural number then m prime which actually denotes the successor of m is also a natural number i mean this is this is what peano set at the turn of the century and we have no reason to doubt him right  noise  so the two constant symbols here are zero and this prime okay which you can look upon it as either an operator like the successor operator or more most specifically in the case of a language you can look upon at as a constructor which allows you to construct um arbitrary elements of a language so the natural numbers here are just a language so an arbitrary natural number wills is going to be zero followed by a number of prime symbols i am using a post fix notation which is quite usual um and what happens is of course this whole thing is tedious so you might define two more constant symbols okay these two constant symbols might be addition and multiplication and what you have the moment you define these two constant symbols is that and i am using a prefix notation here so what you have is reduction rule a new from a one step n reduction rule on the peano on peano arithmetic rule remember all this is completely different from the lambda calculus from the pure lambda calculus its got no relationship at all its got relation to ship to a reality of counting may be that s thats about it so um so what you do is you define these constant symbols by means of reduction rules so i require two reduction rules for lets say addition and one is just says that the sum of m and zero is m and the other says that the sum of m and the successor of n is the sum of the successor of m and n  noise  and its actually reduction rule i mean its look like neither expand nor reduces but actually it s a reduction rule  refer slide time 27  40  because eventually you will get all the primes from here to here and you will get a zero there and say then you will get a value so similarly the product of m and zero is zero and the product of m and successor of n is the sum of its look like an expansion rule but its actually a reduction rule um so this product of m and n prime is just the sum of the product of m and n and m so its look like an expansive rule but actually reduction rule we will see that so just as we have a one step beta reductions you have one step peano reductions okay and remember you are starting from the void there is nothing and you are defined the naturals or if say a genius and now the question is when you have nothing what is the meaning of an expression in peano arithmetic so from nothing you are defined the language of peano arithmetic and now it s a language so it requires to have a meaning and now the question is what is the meaning of peano arithmetic assuming that there is nothing else in the universe the only alternative is that the meaning of peano arithmetic has to be found within peano arithmetic itself right so what we will say and this is in fact what we do in our school i mean it will be it will be amazed you be amazed how much of these things go back to your elementary education so the meaning of expression in peano arithmetic is just the representation of a number using only the two constructors in the language  refer slide time 29  00  so you do this one step peano reduction by the way peano reduction is not standard a name i have just invented it but  noise  but the whole point is you do those reductions defined till you have a string which which is a string of the original language of a peano arithmetic so which means that it can only consist of the constant symbols zero and prime according to the rules of the grammar and what you declare when you have nothing and you have your force to give a meaning you declare there the meaning of um a complicated expression in peano arithmetic um which has possibly plus and star also in it is just what it reduces to eventually till no further reductions are possible and when no further reductions are possible what it means is that the only reductions rules you have a plus and star so when no further reductions are possible what it means is that you should have an element of this language which means zero followed by some primes right so that is the notion of the meaning right so similarly in the lambda calculus similarly in the lambda calculus again we are starting from the void so the meaning of a pure lambda term is a lambda term that contains no more beta redexes so assume that you got some lambda terms so if you take a lambda term which is not itself a beta redex and does not contain any beta redexs in it then what you declare is that its meaning is itself and there is no point going further for a meaning um so so the meaning of a pure lambda term is just a lambda term obtained after sufficient number of beta reductions such that it contains no more beta redexes any where so if it s a string that contain that is that contains absolutely no more beta redexes which means it contains no more applications if you if you recollect if you recollect it for you um so this is the beta redex so if it contains no more sub terms of this form then you just claim that you have reached the absolute nm and there is no more beta reduction possible and that is the meaning of the original lambda term and during the process of reduction of course you have also generated the equality relation so the final lambda term that you get is equal is beta equal to the original lambda term that you started output right okay so lets just quickly go and similarly if you take an applied lambda term lets assume that we are applying the pure um we are taking an applied lambda calculus in which um the constant symbols are the symbols of the peano arithmetic so that means we have take the language of lambda and we have taken the language of peano arithmetic okay so the in the so in the syntax of the applied lambda calculus so in the syntax of the applied lambda calculus this see  noise  anyway in the syntax of the applied lambda calculus you can replace those constants essentially by all possible expressions of the  noise  the peano arithmetic i mean so you can append the two languages so that they can inter mingle right anyway lets lets take this notions of reductions quite seriously um so just to get you familiar with peano arithmetic um the notion of reductions in peano arithmetic lets lets do a simple example um using the rules that we have so here is here is a simple which and for the benefit of those who are not automatance and giving an interpretation of this expression on the right hand side so essentially assume that you have to calculate two star two plus one so which in our language of peano arithmetic um remember that the moment you add those two new constants you are also extending the language you are also extending the language by those by those two constants right i mean by by expressions involving those two constants right so what you are essentially saying is that the new language after having added these constants is of the form um firstly firstly you have this m which which is of this form secondly you have an expression after having um added these these two these two languages these two constants  refer slide time 35  40  you have an expression language e which actually allows any member of this original language and all expressions of the form this and this right and now you add this expression language also to the pure lambda calculus right and then you get an applied lambda calculus right so um in my original def um syntax of the applied lambda calculus i actually did not um did not stick to this format i just gave an extra production for a c you could replace that c by e okay the reason for using that c is rather pedantic but actually that was correct but lets replace it by e in the applied lambda calculus syntax replace that production by e where e is an expression of the application right so um now now lets look at a pure peano arithmetic example so if you have if you have some thing like this then by the reduction rule for by the reduction rule for um multiplication you have that this is actually going to be this so whatever is underlined in black is the redex in peano arithmetic so it is capable of being reduced okay and since we are following prefix notation or order of computation all that is implicit i don t i dont want to go into specifying the orders of computation and so on and so forth but we will assume that um its normal prefix form and so the computation will go in from a in a most operator first right to the outer most operator so this is a redex in peano arithmetic and so this gives me this push right so remember the star m n prime is equal to plus star m n m again right peano rules peano reduction peano reductions so star um star m n prime reduces to plus star m n m right so this is how this reduction goes and this which is equivalent to actually saying that two multiplied by the successor of one is equal to two multiplied by one um plus two right  refer slide time 39  45  so you can follow this the right hand side to see the meaning so again applying the same rule to you get that two multiplied by one is actually equal to um two multiplied by zero plus two right um two multiplied by zero by the first by the basis rule is going to give me a zero so this reduces this reduces to there is something wrong with this you say its right um so um yes here here it reduces to this zero and then there this now this beta redex now i use the addition rules the addition rules says that despite what ever m might b if n is a successor of some thing if this is successor of n then you take the addition of the successor of m with n so this this peano reduction gives me the successor of zero and essentially the predecessor of this and a further peano reduction gives me this and then of course this is the base case of the addition operation so it gives me just this um plus m zero reduces to m right and then i have to take tackle this new plus here is a peano reduction which gives me this and then going on in that fashion um i get i get this for example this beta um this reduction yields this which is equivalent to essentially four plus zero plus one and then this reduction actually is this itself and so now i tackle this reduction which is this which is again the basis and which yields this  refer slide time 41  10  and after desperate number of computations we have actually decided the two multiplied by two and with incremented by one is actually equal to five right but the whole point is if you look at all this they are all symbolic reductions right so lets look at an example of reduction in the pure um lambda calculus and its going to be very very much like this its purely um remember that having defined the language of peano arithmetic and the expression in the language we have what we have done is we have just apply the reduction rules in a pure syntactic substitution don t look at this right hand side look at only this side so its pure symbol substitution which satisfy according to the rules of reduction right and that is really what church considered the basis of all computation function application and reduction so if you lets look at this pure lambda term um so here now you will understand why i use the blue color for the syntax of the lambda calculus you could interpret peano arithmetic very easily but we are not going to be able to interpret this very easily um so um look at this um so i have for your convenience i have marked out all the relevant portion so this term is lambda x lambda y lambda z each time new scopes and this term here is x applied to z okay which is enclosed in parenthesis and what ever this object might be is applied to this object which is y applied to z itself okay and since its an application i have enclosed it in parenthesis and then i close all the brackets right so its its a nested scoping where the body itself an application of essentially unspecified symbols right so um its its an application applied to another application the body itself is an application applied to another application and now this entire now this is this whole thing is a lambda abstraction the whole thing is applied to this lambda term which is which is just which is which is just something very simple convenience um so it s a lambda u lambda v u itself right so now what do you do how do you do beta reduction you scan this string from left to right till you encounter consecutive occurrences of open parenthesis and left square bracket then you know you have a beta reduction for possible there go to the matching square bracket and look for the operand after that and the closing parenthesis now you beta reduction says that once you identify what is the bound variable here and what is the term that is going to be applied that is going to be that is going to act as the operator operand you take this body this entire body and replace all free occurrences of this x in this body if you look at this body in isolation then x is not actually bound any more its free replace all free occurrences of x by this term so i have marked out there is a single free occurrences of x here and that x is going to be through out replace by this so um what you see in green is the effect of the substitution right so what i have done is i have replaced this x by this entire operand and then what i have is now i now this this application of x to z look rather obtuse i mean x is not necessarily a lambda abstraction okay you cant apply unless i mean you cant expect to get anywhere within application unless there is an lambda abstraction as an operator but now this substitution has created a new beta redex by itself um so now scanning from left to right in fact you get this beta redex i marked in red so what now what is happened is this application is actually going to be over this lambda abstraction so all free occurrences of z all free occurrences of u are going to be systematically replaced by z and these pair of parenthesis goes away this abstraction also goes away right so and that yields this right  refer slide time 49  14  now this thing now this thing was even more obtuse because you are performing an application here and now you automatically have that this term y applied to z is actually an operand of this lambda abstraction and therefore is capable of being a beta redex is capable and therefore this body is capable of being reduced right so which means now you take this you take this beta redex replace all free occurrences of v within the scope of v by y applied to z and this z the same as this z god alone knows that but um the whole point is i don t care at this point at least now the of course there are no free occurrences of v here and so the result of this application is just that i get back z right so so and this again is purely symbolic right now we can we could quickly go through um we could quickly go through this supposing i mixed lambda calculus with peano arithmetic there is no tutorial today okay so if i if i mix the lambda calculus with peano arithmetic what do i get i actually apply the  noise  i have chosen to apply the beta rules first so i have this redex and i follow the usual practice and i replace there are two free occurrences of x here so i replace them by this entire term okay and i get this rather colorful object i have always thought of computation is being very colorful otherwise they are confusing so um and the point is that i have chose this orange redex here i could have equally well chosen um this pink redex okay but i have chosen this orange redex and which means that all free occurrences of y of u within this by y and i get this now of course i could have instead of trying to find a beta redex i could have even done a peano reduction here okay but i have decided not to do it i could have equally well chosen this reduction and try to reduce it some point so and when i perform this that means when i perform this beta redex all free occurrences of u in this body are going to replaced by a plus y zero double prime i get this and then now i have a lambda abstraction in which there are no more beta redexes but there are peano redexes  refer slide time 49  42  so i continue with the peano reductions um so i do the peano reductions as as i have illustrated before and i finally get this mixed lambda peano term a symbolic term remember that y was not specified anywhere i get a pure i just say that the result of this is lambda y fourth successor of y um but i could have chosen these other alternate reductions any time i wanted i could have intermingle beta reductions with peano reductions and believe it or not you would have got the same result transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 27 lambda alpha beta  the trinity  time 1  00 min  iit delhi well we are still batting twenty six not out and so today so today we will continue with what we did last time i will just recap the pure lambda calculus  refer slide time 00  44  so here is the syntax for the pure lambda calculus and in the case of applied lambda calculus you just add a constant will come to that um this the standard definition of free variables the notion of a combinator or a closed lambda term  refer slide time 01  03  which is according to our local declarations what ever we know about local declarations its all equally applicable um and here is a operational semantics for beta reductions um so there is of course the important thing to realize that is this syntactic substitution um  refer slide time 01  44  so we have this usual rules so this is the main rule which says that note that this notion is not part of the lambda calculus it is the syntactic act of replacing one pattern by another so it s a its what what might be called a meta syntactic operation um then of course we close these rules again um in various contexts including the abstraction if you don t have the abstraction then you then its becomes what is known as a weak beta reduction um  refer slide time 02  15  such a term which is an application of an abstraction to a lambda term is called a beta redex and you can generalize these two we can take transitive closures of beta redexes or beta reductions and there by get many step beta reductions either strong or weak you can also take the equality that is generated by a beta reduction um by many step beta reduction and you will get well an equivalence reduction and since the beta reduction rules are closed under context actually this this equality is actually a congruence relation um so um so and then here is an exmplae of an applied lambda um um applied lambda reduction um so using peano arithmetic so as i said um so so we went through this last time um as i said we could equally well have chosen other possible beta redexes i mean they could have been several redexes we could have mixed beta reductions with what ever peano reductions um in any order  refer slide time 02  26   refer slide time 02  50  but even if you were to take the pure lambda calculus what it means is that firstly is that the lambda calculus by its very semantics is in inherently non deterministic possess non deterministic execution behavior so there are other possible execution sequences for the same lambda term especially when you have two different beta redexes so what it means is you you there is no hard and fast rule which way your non determinism will which way if you had an executor which way it will take unless you determinise the executor and take up scheduling policy its really non deterministic its essentially non deterministic and when you couple it with in an applied lambda calculus with may be non determinism from the domain applied domain of course our peano arithmetic was not non deterministic if you look at reductions in the peano arithmetic they are purely deterministic um so it could becme even more non deterministic okay so so as far as um the basic computations are concerned its clear the only thing that s not yet cleared is what exactly is all this lambda calculus about so lets look at what how we can interpret lambda terms and which is essentially the original agenda which church at set out to himself right um so lets look at the operator that lets look at this lambda term it s a mixed lambda term with with peano terms in it so what is this do this says lambda x lambda y star x y okay this star x y is clearly a peano term which which denotes multiplication um remember that in um in a normal multiplication which is also what is there in peano arithmetic  refer slide time 03  58  the multiplication is a binary operation which takes two arguments right now here this lambda abstraction makes it a function of a single argument where the arguments are taken in sequence so the effect of um essentially applying this lambda term to to the to the natural number two is to create a new function which is the doubling function okay so given any given any natural number it will double it right so that s that s thats means this x will be substituted by this zero double prime and you get the doubling function of course there are well you could you could define the doubling function independently itself in some such fashion um but the but the point is that this is what is known as a curried form of the multiplication operation we will get into currying a little in in some detail a little later but the effect is that instead of lambda calculus treats every um treats every function as a unary function and the way to program binary functions or ernary functions is to allow us for a sequence of abstraction  refer slide time 07  22  so that the arguments can be taken one at a time okay so so this is so this is essentially a binary function which require two arguments before you can get a value but those two arguments are taken in order one at a time so you first apply to the first argument and get another function which when applied to another argument will give you a value right so this this so this function is obviously unary function and so is this so there is no problem about that now if you look back at this example at this previous example we could actually give a name to this function so so this i am taking this lambda and i am calling it twice right so its lambda x lambda y x applied to x applied to y right um so what is the behavior of this twice i mean what is this twice actually mean um it means it supposing now we we have nothing except just variable symbols and we don t know whether those variables symbols denote values or functions or i mean anything or functions of functions higher order functions anything so now if you were to take this and apply it and lets say to the doubling function or one of the two doubling functions right so take this twice and apply it to one of the two doubling functions right and what you will get is um um so i have used the word double itself to denote this function so when you apply twice to double you follow the normal beta reduction rules and what you get is that you get double y which is applied to double um rather double is applied to double y and which is the quadrupling function which makes perfect sense right  refer slide time 09  00  so if you were to take i mean you could um this is of course a simplistic way of looking at things you could actually took take the expansions out after all remember that one of the purposes of the lambda calculus is to treat un named functions right i mean to look at functions themselves as objects worthy of treatment surgical treatment may be um so um so if you were to actually do this application um what you get is again you get actually a non deterministic choice between two possible beta redexes so this is a twice applied to the double and the effect of that is to replace all these blue xs by this term that s what comes out into red and of course this y is here now either i have the possibility of replacing these two xs note that this is an application and open parenthesis followed by a open square bracket um so i have the possibility of replacing these each of these xs by this entire application or um i am taking an easier way out to avoid confusion is that i am first doing this application to so i am leaving this term as it is and i get this a nice neatly simplified term and then i when i when i apply this when i perform this beta reduction i get this right so this is a quadrupling function which is perfectly understandable and actually if you look at if you look at this definition of double it really doesn t matter what you put here i mean if you if you didn t put double you could have put something else for example you could have put for example the squaring function  refer slide time 09  22  so twice of square for example should give you just well square of the square and the square is very easy defined as a unary function and i could just get square of the square so we could actually put any function in place of double or square and apply twice to it so in particular what i could think of doing in which church did unlike most mathematicians was care of doing such things is that he applied twice to twice this is normally not found in mathematics but if you are talking of a purely symbolic method of computation and functions and this and that  refer slide time 12  15  then what it means is that we should allow for such a possibility and see what you get so when you apply twice to twice you get this you of course get well twice of twice applied to twice applied to y i mean this is just perfectly reasonable so if you in fact if you were to apply it to double again what you would get is um twice of double and twice of that and which is the octupling function if you like and this is by the way this is my name  noise  i am sorry to say that the natural languages don t have enough names for my purpose so this is this is the function which goes eight times lets say and it s a perfectly reasonable function so why a mathematicians scared of applying a function to itself right so so then what happens is that i could look upon i could look this twice okay and what i could do is i could actually look upon this application of twice to twice as actually a lambda abstraction applied to twice right so this is this particular lambda abstraction which if you do a beta reduction each each of these xs will be replaced by twice and of course the abstraction x will go away so you will get twice applied to twice right so so which means that there is a way of getting this twice applied to twice by another lambda abstraction okay so now this becomes as an object of interest in itself so what i could do is i could play the same game applied this function to itself so i applied to itself and what do i get each of these xs is going to replace by this term and i get this  refer slide time 14  13  which except for the colors exactly identical to the previous term right and so you you do this application again you get exactly the same term and so on and so forth so then this was an example of a non terminating computation right so what church going back to church what church essentially did was he said that he said that an algorithm what is an algorithm well algorithm is is an abstract object which is totally meaning less as far as i am concerned if you want to give an logical definition of an algorithm it s a lambda term which has a terminating computation the only concrete representation of an algorithm is of a lambda term which has a terminating computation so of course you should make a diff distinction between this non terminating computation and say at computation of this form say say term of this form itself i mean take this lambda term this lambda term is an algorithm in the sense that all the computation is already terminated i mean there are no more beta reductions possible but this lambda term is not an algorithm because the beta reductions are possible and its an infinite computation right so so essentially church said that so the concept of an algorithm is just so so if you look at it in more modern terms what it means is what church is constructed is a is a full fledged programming language what he claims is that the notion of an algorithm though two thousand years old is not is not an accurate notion in any sense its only a notion that could be understood by intrusion the only concrete objects are and not algorithms but programs and algorithms are simply programs that are guaranteed to terminate there are programs and there are programs which are not guaranteed to terminate um which can have infinite computations so so that essentially settles the issue of the notion of computation the notion of algorithm um get old did not quite believe what church said but um but the point is that um it was what what church was the church is work with subsequently complemented by turings work and the two formulalisms were proved equivalent and so on and so forth which you learn in your theory of computation hopefully okay now we have to worry about some other aspects of a of our theory lets get back to the beta reduction which is well the most important thing in life so you have this you have this syntactic substitution right so which is replacement by a variable another lambda term um  refer slide time 17  34  so now  noise  if you play around with the lambda calculus as a as a sought of toy which of course my children frequently play with lambda calculus as a toy um is that um is that for example i could take this lambda term which really denotes multiplication operation in the lambda calculus in the mixed lambda calculus right all in the rather in the applied lambda calculus mixed lambda peano calculus if you like and then what i do is so essentially i can look upon this lambda term as a box that i acquired from some where typically you acquire box like this from from the us and then you have this hundred and ten volts two hundred twenty volts problem then you go to jackson hytes or kotla mubarakpur or lamington road in bombay and you get another box and you apply one box to the other and hope and pray to god that it works right so here i have another box called z which i have acquired from jackson hytes and now i apply them in the hope of getting something that works right and and actually it does so so i get a new box which is the function multiplied by z what ever z might be then of course since its since z is a jackson hytes product its not very reliable and after some times it comes out having detected it that comes out i go to the local market and ask them to supply me a z but they say well we don t exactly have the z i mean but we have something else which will work equally well we have a y right okay  refer slide time 20  20  so you have a y so now i plug them both together and what do i get i get something a typical chandni chowk product i mean right i wanted multiplied by something and i got squaring right so that is the problem when you play around with these toys especially when you play around with incompatible toys right so um church of course put it up more loftily and so on and so forth he is not aware of chandni chowk and kotla murabarakpur and so on and what he said was that essentially this this y that you acquired from the market is different from the y um it is really different though it looks identical is really different from the y that is inside this box right and the two y s and these two y s are actually different and they should not be treated as this one and that s where the problem is so essentially what is happened is that  noise  this y is a free variable which has a value which presumably has a value in some context um okay may be hundred and ten volts but um but the point is that the moment you perform this beta reduction this y gets captured by this locally declared y okay so free variable gets captured even though its not intended to be captured right so so what it means is that now you will have to you will have to and remember that so which means this notion of substitution in the operational semantics is something that has to be very carefully worded up right i mean you cant you cant just do substitutions like this i mean another another possibility is supposing supposing i have actually a lambda term of this form if i have a lambda term of this form um something blab blab blabla to which i actually apply some substitution this substitution came about as a result of some beta reductions some where i mean this this lambda term is part of some huge um larger lambda term and then i get this x should be replaced by m okay now the point is that this x should be replaced by m this is what an application its it s a syntactic substitution and this x is not the same again as this x this x is very highly local to this box and this x was which was suppose to be replaced with some free x some x that was outside this box right if this whole thing was part of some larger lambda term in which there was some reduction i mean what i mean is supposing i had this lambda term and this lambda term is in turned a sub term of something else which has an ex abstraction to which i actually applied a m right then a beta reduction means that this x has to replaced throughout this term but if there are any local x is there they are different from this x right  refer slide time 24  44  so in the process of doing this replacement i might i might actually encounters this problem that i have a i have to deal with a syntactic substitution of this form and what do i do since this x is different from this x i should not do any replacement at all right so so this is again an another incident of a free variable capture except that this x is actually bound but it has a scope and this x has a is a whole in the scope of this x right to use our standard programming language terminology this x is this scope is actually a whole in the scope of this x and as far as the body of this huge lambda term is concerned the x is free and you can if you if you make this replacement you can you might be capturing as far as as far as just this body is concerned this x is free and you it will get captured right so the first the first principle of doing successful computations on the lambda calculus is not to allow free variable capture so which means we have to define this notion of syntactic substitutions very carefully um and whenever we talk of substitutions its clear from our beta reduction rules then we are talking about only free variable substitutions and no other substitutions no other um bound variables are not to be substituted okay bound variables get substituted in a two step kind of process they get bound variables are like parameters stored procedure you apply a value um you apply the function and you take the body in the of the lambda term which has his bound variable x and then you do the replacement but then the x in the lambda term is free right so all substitutions that we will talk about are only free variable substitutions so unless otherwise mention in fact i wont mention it otherwise to all substitutions are going to be only free variable substitutions you can not replace bound variables because bound variables are logically distinct variables what ever you require to do in the in in the sense of bound variable substitution is something that can be obtained through the normal process of lambda application by first freeing the variable and then doing the free variable substitution so lets defined free variable substitutions in as accurate a manner as possible now this free variable substitutions is had a very rocky history okay most people most books let me tell you a probably wrong um okay they have made a mistake some where or the other in the definitions of the substitutions so don t believe most of these books um but play around with it to see whether there is a bug in my definition right okay so if i have a lambda term x and i am going to replace x by m then the effect of this syntactic replacement just to keep me m which is perfectly obvious if i have the lambda term y where y is different from x then the effect of doing this syntactic replacement um so which essentially means replace all free occurrences of x in this term by m and there are no free occurrences of x in this term and therefore the result is just y right so now if it s a constant in in the case of an applied lambda calculus well a constant is different from anywhere you have by definition right and so the effect of this substitution is just leave the constant unchanged if it s a lambda application um if its an applicate um if its an application of l to n then the effect of this substitution is to replace all free occurrences of x in this application by m and that works out to replacing all free occurrences of x in l by m and applying the resulting lambda term to the term applying by replacing all free occurrences of x by m in n right okay now that s also obvious note that these are all structural definitions structural definitions by some case analysis okay here is where you get an important set of case analysis if i have lambda x l then this syntactic substitution clearly refers to an x which is different from this x and therefore there is no free x in this scope they can not be any free x in the scope l so the effect of this substitution is just to the leave the lambda abstraction unchanged um so if on the other hand y were different from l then the effect of this substitution is to replace all free occurrences of x in l by m right so if and moreover an added constraint here is that m itself should not contain any free variable should not contain y free if m contain y free then then y in m is different from this locally bound y right  refer slide time 30  40  so you can do this locally free variable substitution only if y does not occur free in m if y occurs bound in him there is again no problem because that binding will carried through in this substitution okay y has to be different from x y should not be free in m now the point now what happens so supposing y were occurring free in m what we what would you do then what i would do is this take this lambda y l and call it let let me let me lower this so that i can still retain this so this is a sub this is case very similar to the previous one except that y does occur free in m okay but y is different from x then what i do is i pick out a new fresh variable z and then what i do is i systematically replace since y is a bound variable here it probably does occur free in this term l i systematically replace all free occurrences of y by z note that if there are bound occurrences of y and they don t have to be replaced but all free occurrences of y so i mean as the same variable could occur both free and bound within that thing um within within a lambda term and um and obviously they two different objects right so all free occurrences of y in l are replaced by z  refer slide time 32  50  so i get a new term which is lets say l prime by this and now i dont have any problem i replace all free occurrences of x in this new term by m and i can do this provided z does not occur free in m so that s why i am saying that s why i am saying take z completely fresh freshness is not a virtue that many of these books have emphasized right but it is the safest possibility when in doubt pick out a fresh variable and there is no death of variables because we are covered our cracks by saying initially assume a countably infinite set of variables available to you any particular lambda term is a finite syntactic object and so has only a finite number of variables it can not have a infinite set of variables so therefore you are always guaranteed that there exists a fresh variable that does not occur any where so you take a fresh variable or at least take variable that ensures that that is that is such that it does not occur any where in the terms in which you are performing a substitution right and then first systematically replace this variable y and this is the case when y belongs to the free variables of m systematically replaces all free occurrences of y by z and then in the new term systematically replaces all free occurrences of x by m note that x itself could occur in m either free of bound so these free variables substitutions are perfectly um perfectly um generally if you follow this case analysis the problem is that i might have this is mistake that is there in many books the problem if i recollect right is that you might have if z is fresh there is no problem if z is not fresh but occurs bound in some where deep inside the term and um if z occurs bound then um you are going replace and you have z occurring um bounded m also and if i have some application like this deep inside then the z that occurs bound in m okay that will not effect but if the z occurs free in m then there is going to be a problem okay but the problem with these syntactic substitutions is over sixty years is very difficult to distinguish between what definition is right and what definition is wrong and i am playing safe so i am saying just take a new variable don t get confused don t allow for any possible confusion just pick out a new variable that is never been used and do that replacement first i am just playing safe if you analyze some of the definitions you might find it some of them are really correct but you have to do a large amount of case analysis before you can prove them correct instead i prefer to take the safe way out okay because of such things for example in ravi shetty s book this bug is there it just assumes that z is not free in in z is not free in m um but in the act of replacement they might be confusions between two bound zs okay um the x okay so that  noise  this confusion between two bounds z should not occur mean that some thing that i suspect he is not taking care of right so i would prefer always as a good pragmatic rule to pick out of a fresh variable and do the replacement before in fact what i would advocate is that the moment you see two lambda terms which have a which have one common variables um which have common bound variables i would suggest the you just suggest replace them all by fresh variables and that is some that all students know of hand right so so if you look at this look if you look at this terms so in what way are the following different so all these are i have taken the definition of of twice and what i have done is as just done various kinds of replacements so here is lambda x lambda y x applied to xy which was original definition of twice now what i done from while going from to here to here this as i systematically replaced all occurrences of x not just free i mean what i mean is i am not performed a syntactic the syntactic operation of substitution i am just considered this as a string of symbols and i will just systematically replaced all occurrences of x whether free of bound by you and all occurrences of y whether free of bound by v in this case i have taken this and just replaced all occurrences of y whether free of bound by u in this case what i have done is i have done a simultaneous replacement i have with one hand i have replaced all occurrences of x by y and with other hand simultaneously so that there is no confusion i have replaced all occurrences of y by x okay  refer slide time 39  55  and in what way they are different actually they are not different right so and they are all the same they are all obtained by what might be called renaming of bound of variables right and so essentially in addition so we have if you look at so between so one possibility is that you have this syntactic identity these four terms are not syntactically identical but they are syntactically identical in the sense that only the names of local variables are different otherwise the patterns the structure of the patterns is the same right  refer slide time 40  44  and so we would consider them almost identical and that s called alpha conversion um so these all these terms are really alpha in alpha convertible and what is  noise  my problem now is that i have how do i express this alpha conversion in terms of our notion of syntactic substitutions which we have rigorously defined right so for example how do i get this forth term from the first term its actually very simple in in two steps i get the second term from the first term by two replace by two syntactic replacements using what is what i call alpha conversion which i am going to talk about now and then in another two steps i get i get the forth term from the second term right um so so what is the alpha conversion we have to define the rigorously too so alpha conversion is just this just says that given a term of the form lambda x l it is alpha equivalent it is almost syntactically equivalent to a term in which there is a bound variable set and all free occurrences of x in l are replaced by z where of course of course one thing that we should ensure is that if two terms are syntactically equivalent syntactically identical then they are also alpha equivalent i mean thats that s a trivial thing i mean so this z is either the same as x or again i am playing safe z is a fresh variable different from anything in this term and that s this is the safest game to play and not get mislead by most of these books which only concentrate on free variables it is possible to construct very often powers and convoluted examples in which some of the definitions in most of these books will be proved wrong it is also very hard to construct examples to show that they are wrong but what is safe is always to pick a new variable a brand new variable preferably shining um and do the replacement right so the basics of the lambda calculus which i hope you will spend some time toying around with um what is the basic um what is the calculus mean i mean that is most  noise  we missed that out i mean why is pascal not a calculus where as this language of lambda terms is a calculus why is it a calculus what is a calculus what is a what is a calculus about preposition logic why is it called prepositional calculus why is it called predicate calculus but in the prepositional calculus and the predicate calculus there are no computations unless you impose computations some how no a calculus is a system with a formal language that means defined by something like a context free grammar and whose rules of inference whose axioms and rules of inference are such that there exists and algorithm to decide whether a given application of axiom or a rule of inference is correct a calculus is  refer slide time 43  34  one in which regardless of meaning attributed to a formal language there exists purely syntactic rules symbol pushing rules which allow you to perform inferences that is in fact a property of um your axioms systems are first order logic for prepositional logic for boolean algebra um and lambda calculus the rules have to purely syntactic and involve only symbol processing and by symbol processing usually i mean just pattern matching and replacement that s what make it a calculus transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 28 data as functions  time 1  00 min  iit delhi welcome to lecture twenty eight so today we will i briefly recapitulate what ever we done in the lambda calculus and then i will talk about data as functions right so lets just briefly go through the pure calculus so we have the syntax given a countably infinitely set of variables symbols you have every variable symbol is a lambda term a lambda abstraction is a lambda term and a lambda application is also a lambda term  refer slide time 00  48  we have the usual notion of the free variables we have the closed lambda terms which have no free variables of our combinators it will be looking at some important combinators and the operation semantics of the beta reductions is this follows  refer slide time 01  18  so the main rule is that of um finding an application and replacing the bound variable in the body by the operand which simulates function application and of course there is the notion of syntactic substitution which we have rigorously defined before we can um we can take the reflexive transitive closure of the beta reduction and  refer slide time 01  48  obtain a many step beta reduction um by these rules um and we also looked at um we also look and just to make clear what are notion of free variables substitutions are as i said for our beta redex we require the notion of free variable substitutions which some how has to be very carefully defined and this is our definition  refer slide time 02  16  going from here to on to here right um so  noise  we will we will take this as a definition though it is not completely algorithmic its possible to give this definition entirely in terms of um entirely in terms of um free variables without worrying about these bound variables and then make it algorithmic but then harder to understand we will follow the simpler root here okay  refer slide time 02  54  and we also looked at the notion of um we also looked at some strange kinds of things which um which normally do not happens with functions for example we saw that functions may be applied to them selves  refer slide time 03  12  some thing that is otherwise i am heard of and actually it does make sense for a function to be applied to be itself on the other hand it also does not make any sense for certain functions to be applied to themselves because they lead to infinite non terminating computations um so it means one has to be some how careful  refer slide time 03  53  the difference between these two is that we something look at later one is that this is a polymorphic function and therefore it is not really self application whereas um this is really self application that way we will see that self application by itself has to be banned right and then we also looked at alpha conversion which is essential some how when you have to readability for doing a beta conversion um with out creating collisions um confusions so this definition of alpha conversion here uses the definition of um free variable substitution and also this alpha conversion includes this syntactic identity right  refer slide time 05  03  so  noise  so if l is alpha convertible to itself thats thats the moral of the story and okay um and we will take z to be of fresh variable different from any in this again its possible to redefine alpha conversion such that um its only necessary to look at free variables in certain bodies but we wont worry too much about it um okay now with this notion of alpha conversion what we would like to do is the alpha conversion basically tells you that two terms whose which are different only in terms of bound variables or essentially the same they are almost syntactically identical and that um even you really can not distinguish them on um the basis of any meaning or operational semantics or any such thing so what we will do is we will include alpha conversion in our definition of beta equality so the new beta quality is defined as follows  refer slide time 05  42  if l is alpha convertible to m then l is beta equal to n and if l goes in a many step beta reduction to m then l is beta equal to m if l is beta equal to m then m is beta equal to l if l is beta equal to m and m is beta equal to n then l is beta equal to n these these three were the original rules which really tell you about the reflexive transitive closure of a many step beta reduction and this is a new one this that every now and then while doing beta reductions we might feel compiled to rename some bound variables therefore alpha convert to another term which is not syntactically identical but which is alpha conversion which is alpha equivalent and we will claim them the about beta equal  noise  so our reflexive transitive closure actually includes alpha convertibility too so that now we don t need to worry too much about alpha conversion which is always which is which adds to the confusion and does not add anything new to the meanings of terms right so now getting back to the title of this lecture namely data as functions the important thing to realize is that um there are essentially several models of computation right if you get back to this data as functions what we are trying to say here is that if you if you were to go through an architecture course um what is the main goal of an architecture course firstly it tells you that there is no really difference between program and data program and data both have the same representation namely i don t know various kinds of bit representations and they have the same representation so it s a matter of interpretation whether some bit string is a program instruction out of data or a piece of data i mean the whole the whole idea of a fonnamean architecture which is what you study in most architecture courses is really this that there is nothing really there is nothing fundamentally different between programs and data programs are not any different from data it s a matter of interpretation you can interpret certain bit strings either as data or an instructions so there is no difference between data and control and the important feature about any architecture um course would be that all control and all programs are represented in the form of data right in the sense that you are um you are the machine instructions are all um expressed in some bit strings your jump instructions or bit strings um so they are all coded into data so all so all control is coded as data and data is coded as itself of course so so the and the whole point is that depending on convenience you decide so very often you actually you actually logically partition um lets say areas of memories into a data segment and a code segment and you interpret some of that um what ever is in the code segment as control and what ever is in the data segment as data but its really a large is largely a matter of interpretation and there is um no essential difference between the two now  noise  in the case of lambda calculus what actually church did was that what he showed was that yes programs and data are no different or functions and data are no different and that all data are functions which is reversal as part of the fundamentals of architecture right there is no difference between programs and data or controlling data in his case its functions and data and data all data can represented as functions right so the important thing so in in this um what we are um what we have looked for is not only the pure lambda calculus its its what might be called a pure untyped lambda calculus um so the first thing is that the first thing to realize is that its not really necessary its not really necessary to have to apply the lambda calculus um to have an applied lambda calculus in theory in theory its not its not all essential in theory what it means is that our application like taking the pure lambda syntax and applying it on to something like peano arithmetic is not essential and when you can get rid of peano arithmetic completely because what we will do is since all data are going to be representable as functions peano arithmetic can also be represented as functions and so it is possible to work with just the pure lambda calculus you don t need to apply it okay it s a matter of taste whether you want to apply it or not but largely if you look at functional programming languages they are all applied lambda calculi but they are all applied lambda calculi for reasons of efficiency  noise  the basic thing that is happened in the last fifty years of the existence of computers is really that you should be able to exploit speed of hardware hardware is very fast anything that is programmed in hardware is likely to be very fast so instead of actually coding coding of all data structures as functions its more efficient to use the underlying datas data structuring capabilities of your machine hardware okay so you want the power of the lambda calculus which means higher order functions but you do not want to you do not want to work with your pure lambda calculus simply because even though the pure lambda calculus can represent all the data you require its going to be extremely slow so what you do is you apply it on to an existing virtual machine whose operations are likely to be very fast since underline hardware is going to be very fast and it is getting faster everyday it makes a lot of sense not to try to code every thing in the pure lambda calculus but to use the underline data representations of the underlined hardware and the code only what is very difficult to do with the underlined data representation remember that the whole the whole fonnamean thesis is is not just is not that you can not write higher order functions in in hardware is just that it is extremely hard to do it its extremely complex so what you do is you exploit the hardware to the hilt i mean for example to the integer arithmetic is very fast on hardware no amount of simulation using list is going to get you that speed what ever is programmed in hardware or at most firmware is likely to be five to ten times faster than anything you can um then the best algorithms you can write in software so you write you always  noise  that s um so it s a good pragmatic reason to use only the applied lambda calculi which is why all functional languages or applied lambda calculi they provide this excellent structuring facility for higher order functions but even the pure calculus can structured data just as well as um data itself is its its make a lot of sense to apply it on existing on to an existing virtual machine and develop and so get the benefit of the structuring of mechanism of um the lambda calculus coupled with the speed of the underlying virtual machine um so that s really the main reason why all functional programming languages can just be regarded as applied lambda calculi right but as a matter of academic interest it s a good idea to know that i mean its its goes in parallel with lets say what what you learned in an architecture course in an architecture course essentially what you learn is the representation of all data and functions as data when if if an architecture course as to be summarized in one sentence that is just  noise  it s a representation of all data and functions as data and good idea is to look um the the lambda calculus pure lambda calculus theoretically at least to be able to program all the data themselves as functions and so it provides a parallel okay so some of the we will lets first go through some of the important combinators that we will that we will see and then and then may be we will look at this data structuring um facilities numbers and data structuring facilities so um one of the most so three so the three the three most important combinators are these  noise  what is a combinator a combinator is closed lambda expression so that it has no free variables um so closed lambda expressions um are what provide the capability i mean they they have the status of full programs to which data can be supplied and they can be executed um so this is this i is the identity function so you apply it to any you apply to any object you get that object okay by object i mean that that of since we are talking about that n type lambda calculus that object may be a value object or it might be another function okay so what ever so this combinatory i is polymorphic in the sense that there is nothing specified about this type x the type of x whether this type of x is a value object or a function object or a higher order function you can apply this i to any function and you should get back the function so its if if you have a supposing you are using this i in an applied lambda calculus in which you have a function from some domain a to some um some codomain b then this so the function f a arrow b when i is applied to f then i acquires the status of having a type a arrow b goes to a arrow b right so so if you take um so if you have a so if you look at an object x lets say in integers okay and you apply i to x then i is a function is a combinator function of the type integers to integers on the other hand if you took a function f from integers to integers oaky then and you apply i to f you get back f then i really has the status of having the type integers to integers that means it takes a function integers to integers and gives you another function which actually happens to be the same which is integers to integers okay you can take any function of any type and i applied to that function will give you back the same function in each case if if you have a function f which goes from some type a to some type b then i of f is a function from i applied um i applied to f is equal to f and therefore i has the status of being from a applied to b arrow a applied to b so what i can do is i can parameterize so i is polymorphic in the sense that depending on the argument it has a varying personality um if the argument is a plain integer then i has this personality of a function from integers to integers if it s a plain boolean the i has a personality which is like booleans to boolean if if if i is applied to a function from integers to integers or integers to boolean then i takes a personality which is of which is that it s a type of integers to integers to integers to integers or integers to booleans integers to booleans  refer slide time 21  52  so what it means is that in general instead of looking at these various types formed by constants i can just a second can talk of value variables i can talk of type variables so if then i if alpha is a type variable then i is of type alpha arrow alpha okay and this these types are what you see in your ml interpreter when you define a function it determines the type if it can determine the type by means of a constant i mean if there is a distinguishing constant then um of course the type is very determined in terms of those constants i mean say you can look upon types themselves as a language okay whose constants are things like int bool real and so on and the functions are arrow types formed of these in the case of a um a combinatory like i what its saying is that you take any type alpha so alpha is a variable over the types and i is a um i is a function then of alpha arrow alpha um  noise  so one thing is one thing that is possible we will talk about polymorphism and a type lambda calculus in in a certain um in a little of bit of detail a large part of it still it is a matter of fairly current research um so but there is quite a bit of material which is which is already quite well known especially ml type inferencing and so on right so um similarly most of the other combinators you can assign a type in that way okay in the case of twice the two applic the the applic so twice is also polymorphic in a similar fashion okay in the sense that i can assign type variables to the type of twice and then i can try to solve for this type variables solving for entirely for type variables means expressing one type variable in terms of the other if i can not do that as in the case of omega combinator that i um that i showed um that as in the case of this  refer slide time 24  24  so the self so twice self application is polymorphic where as the omega combinatory which is defined this way is not polymorphic it is possible to do a type inferencing for twice but it is not really possible to do a type inferencing for omega okay it is possible to assign some type variables and some type expression to twice but it is not possible to do that for omega and that s what distinguished just genuine self application from merely polymorphic term application um any way so we will go into the um in some detail later but for the moment just look at these combinators so just keep in mind that we are starting at the beginning right um in the beginning in the prime module mesh there is nothing i mean you are extracting functions values everything from essentially a homogenous mason nothingness right so um here is a combinator k if you look at the combinator k it basically given two arguments x and y it returns the first argument um then there is this combinator s which is actually very important but i don t really want to spent time explaining it its form is such that for three arguments x y and z its of the form x supplied to z the whole thing applied to y applied to z its importance is its really that with s and k you can program all other combinators i mean there the absolute primitives you required um for um the just these two constructs essentially all functions there are programmable can be programmed um  refer slide time 27  30  but that s is the matter of doing something it s a matter of study your finish your theory of computation course but till then you just keep in mind that is in k are very important this this this combinator is just just tells you about how to compose functions together function composition is again again polymorphic right i mean after all regardless of the domain in which you are you can always compose unary functions if um this is a function composition and then lets look at actual representation of data so the first thing is truth values so the truth value true is just the combinatory k so given two um given two functions x and y it chooses the first one and that s true falls is just choosing the second one given two functions x and y um i given two objects x and y if you choose the second one and that s false um so this is a quite this is a bit light i mean you can see the similarity here its functional but in your architecture its really data based i mean you have only two possible values of um data um from which data all formed zero and one so one them false and other is true right so it really doesn t matter which one you take so and then lets look at we will just quickly look the data structuring capability so one thing that we have to do is find is to be able to define various data structuring facilities and what are the data structuring facilities um cartesian products um disjoint unions may be but forget about disjoint unions sequences right um we need to able to be to construct sequences so pairs tuples sequences and if you got them you essentially got everything else thus everything else that is required theoretically but it is not in required pragmatically as far as functions and so on  noise  arrays are anyway functions and functions anyway you have i mean thats is the whole purpose of the lambda calculus right so how do you define um so we will define a pairing function okay so i am so now um i am following the following a convention now i am using dark green for combinators constructed from lambda expressions um some times don t get confused so it will be a good idea if everybody had five six colors to write with um so don t get confused when i used a green square bracket don t confuse it with the blue square bracket which is part of the lambda calculus um language um so here this is a pairing function which takes two arguments and this pairing function is just defined by this combinator okay now in all are data structuring facilities we had essentially two kinds of operations one is the constructor operations which allows you to construct complex data from simpler data and the other is the deconstruction how um how do you get the simpler components from a complex piece of data um so so this is the constructor for pairing and this is the deconstructor so i will call the deconstructor this and what it means is that if i have a lambda expression um p which is actually suppose to denote a pair of elements which which presumably was formed through the construction operation through this construction operation not any other okay then p applied to true will give me the first component of the construction and p applied to false will give me the second component these are things that you can verify by actually doing the lambda application remember that this bracket is the i mean this is this is the my representation of the deconstructor operation okay its in green this blue parenthesis represent actual lambda application so that means you take the combinator p to which you apply the combinator through and see what you get as a beta through beta reductions and of course our beta reductions now include also alpha conversions wherever necessary um so if true anyway is the combinator k right which given two arguments gives you the first argument okay so when you apply for example take this if if p was in deed if p see remember one thing p could be any lambda term okay and strictly speaking you can apply the deconstruction operation on any lambda term its just that if you apply it on any lambda term then the chances are you wont be able to interpret what you are getting in some reasonable fashion but if p had been constructed through two terms m and n using this constructor then when you apply true here what happens you get true applied to m the whole thing applied to n true applied to m true is just k k applied to m applied to n means that it will give you the first component it will give you n right similarly p applied to false will give you the second component and it will give you n but remember that this thing since we are we are in a un typed world you can actually construct any arbitrary lambda expression and say that this some how represents a pair though you may not have constructed it through this fashion you can apply p you can apply that combinator to true and get some result which doesn t make any sense to you okay so this is some thing that this sought of natural i mean what i mean is here again we actually had an inkling of that in this i mean if you say true has same representation as k the n you know can i apply k to some thing else to a pair and claim that i am applying true to some pair and getting some truth value and i mean it s a um the type the type of value you get is the largely a matter of interpretation after all i mean this is this is some thing that as as as a very person i use to do i use to have this fortran programs and then run them through the pascal compiler just to see what kinds of errors come i mean its really a matter of interpretation right i mean you can apply to anything to anything um you can send any file into a pascal compiler i mean what what you get is anybody s guess so similarly you can apply any combinator to any combinator after all the language syntax that does not disallow you from doing that but what you get is undeterminable so these combinators the fixing of the types the fixing of the kinds of values is largely a matter of interpretation after all i mean can take cosmic ray data and try to execute it  noise  i mean i get a whole sequence of bits zeros and ones why cant i regard it as a program and try to execute it right and what will actually happen i don t know some bleaches will take place some where some thing will happen but what i mean is it s a matter of interpretation whether you get something meaningful or not in an untyped world which is what your underlying hardware is um there are no types there is no distinction between programs and data so what prevents me from executing data right so similarly in the untyped of the lambda calculus what prevents you applying from some strange combinators to strange other strange combinators what you get is anybody s guess okay  refer slide time 36  20  so i can interpret this the construction and deconstruction operations only under certain conditions for example what prevents me from incidentally forming an expression which has this syntax why should it why should i interpret it as a pair at all okay similarly there is nothing prevents me from um doing this application to any arbitrary lambda term and seeing what i get what i get is not necessarily the first element of a pair what i get would be the first element of the pair only if that lambda term was obtained by this pairing constructor this is a fact life in the un typed world whether it is program or data it doesn t matter any untyped world there is a problem of interpretation that s these operations are not serjective i mean they don t go back and forth um necessarily always but they go back and forth only if you go through the construction operation and then the deconstruction operation okay so  noise  so that s one of the problems of the untyped world right i mean so you can take an arbitrary lambda term and deconstruct it first and then try to do the construction and see whether you get the original lambda term no if you arbitrary lambda term was not of this form then you are not going to get the original lambda term see if you arbitrary lambda term was of this form then sure you will but your arbitrary lambda term was not of this form then you are not going to get it because when you do the deconstruction you will some strange lambda term m you will do  noise  and a strange lambda term n and then you will apply this construction and you will get a lambda term of this form and which is not necessarily going to be beta equivalent to what you started out with  student   so but when we started with true and false sir don t be expect something like when we when we do it in boolean in the boolean world then true plus false equal to one and something like that  professor   no no i mean that s that s that s the problem you are mixing up with an untyped world i mean why should true plus false equal to one  student   that is if anything possible that is when we take a function applied to true it take an take a function apply false statement  professor   yeah  student   and then in some way try to get it back we always get back the same function  professor   no no the only thing that is guaranteed is if you go through the construcutor first and then do the deconstruction then you get back what you originally had if you do a deconstruction and then try to apply a construction then there is no guarantee what will happen that s how the untyped world is okay what um what barrenreck has proved in way back in nineteen seventeen four is that there is no perfect possible construction and deconstruction operation for pairs which will ensure that construction and deconstruction are in in are some how inversions of each other under all circumstances okay which should answer your question what it means is that if you do a deconstruction and then do the construction from the what you get out the deconstruction you wont necessarily get the original thing um so right so and the most natural thing to do when you i mean and it s a good idea to play around with these things to see what happens you know with your data structuring i mean its its i mean why should why should we play around only with pascal compilers and fortran programs i mean we can we can do that also with combinators um apply their lets say i mean apply the successor function to a sequence and see what you get i mean you can you can do all these kinds of things i mean after all there is nothing that really prevents you from seeing what horribly complicated expression you get um so um so this is how the pairing is goes once once you have the pairing its very simple to construct tuples so the pairing is through cartesian product and tuples are also cartesian products and if i bracket my cartesian product if i take a cartesian product a one cross a two cross up to am i can look upon this as a cartesian product which is obtained by a binary cartesian product take the cartesian product of this with the next and so on and so forth and i keep on bracketing the other possibility of course is look upon this cartesian product as looking at such cartesian products okay so that i get right now so i can look upon an hornery cartesian product so i can i can look upon an hornery cartesian product which i will just write like this okay as being isomorphic to binary cartesian products okay so these things are isomorphic this is isomorphic to this is isomorphic to this so what it means is i can take  noise  an hornery cartesian product and either i can look up on it as  refer slide time 42  15  binary um cartesian products done one way or binary cartesian products done the other way or i can also have mixtures of these right i mean i can choose some k and some how do a one to ak and then ak plus one to am and take it as a binary cartesian product so isomorphic to whole lot of possibilities is actually isomorphic to all possible ways of bracketing um that you get um  noise  so we will just take one of these so we will look upon a tuple as just obtained as an ordered pair a tuple m one to mn is an ordered pair whose first component is m one and whose next component is an n minus one tuple whose first component is m two and whose next component is an n minus one two tuple and so on and so forth  refer slide time 43  48  since we have a pairing construction operation we know how to find tuples essentially what we have what we are using here is one of these isomorphisms um for the representation of tuples right so as as usual we require for tuples we can and we can take since our tuples are constructed from pairs are deconstruction operations for tuples which basically means projection functions for the tuples can also be constructed from the pairs from the deconstructions for pairing right so if you keep on deconstructing the tuple you can in a certain fashion you can get the ith component of the tuple um so which essentially means that p has been obtained by an explicit tuple construction mechanism then  noise  to get the kth component of this tuple what i do is i do the paired construction k minus one times okay if i do it k minus one times so if have this if i have this tuple of this form m one to m two to and so on and so forth mn minus one mn so if have this tuple if i do this pairing um deconstruction for pairing n minus k minus one times then what i have is a tuple whose first element is mk and whose next element is another tuple okay of size i mean of n minus k a tuple of n minus k elements it s a n minus k tuple and now what i do is i take the first component of this and that s what actually happens here so um you take the right hand component each time of the spare do it k minus one times and then take the first component of what you get right and for the nth element you always take the right most element so you do this n times n times perhaps n minus one times n minus one times n minus one times um and you always get that take the right component here so um and that should give you your so these are the deconstructions operations for tuples right derived from deconstructions operation for pairing right um so now lets look at sequences  noise  okay so here so we had the first confusion with these parenthesis because they this parenthesis when in blue are lambda applications parenthesis when in green are deconstruction operation for pairs and tuples um for sequences i have square brackets so square brackets when blue are lambda abstraction um square brackets when green are actually sequence construction um so um simple method of forming sequences um if you actually look at your programming  noise  that the tuple formation is really like your list construction in ml or lisp or what ever um its like having a cons operation so on and so forth  refer slide time 48  09  so cons operation is really like pairing successive pairing i mean there is an alternative way of looking at them um you can look at sequences at this way whole sequences as just lambda z z applied to m one applied to m two applied to and so on and so forth up to mn and um and then right and then you have this deconstruction operation like this which i don t want to go into great detail about it um if you if you actually play around with it you will be able see that this is a deconstructor um this p i n is the deconstructor which extracts the ith component of the sequence the n gives you the fact there its n if that sequence is n components long then the ith component out of n going to be this and we are in the untyped world what it means is firstly you can apply this p i n to any particular any lambda any expression that fancies you that take you fancy it may not even represent a sequence you can still apply it and can get something what you apply it god alone knows what you get after application god alone knows you could even take sequence that is lets say much shorter than n elements long and try to apply p i n where i is greater than the length of that sequence you will get something you will get some other lambda term because a lambda term applied to a lambda term will give you a lambda term but what it means again god alone knows okay here again only when you go through the constructor operation and then you do the deconstruction you are liable to get back your original component if you go through the deconstruction operation and then go through a the constructor the constructor you don t know what you will get and in the untyped world there is no such thing as an error okay i mean after all errors or a logical or a logical consequence of an interpretation in your actual hardware you actually said some condition codes and detect errors and detect certain patterns is errors but otherwise if you just look at the bare hardware what is an error there is no such thing is an error is just bit patterns that s it take it or leave it so similarly here there are no the concept of errors again is a matter of interpretation and you just get lambda expressions take it or leave it and errors are really  noise  a higher level abstraction from the untyped world i mean they are not part of the untyped world right so if you do erroneous applications you will get something but interpret it as an error is is a matter of your taste  refer slide time 49  22  is a matter of what you consider right for example you may not like the idea of a deconstructor followed by a constructor um application giving you some actually something that is meaningful then you call it an error you still get a lambda term whether you like it or not  noise  okay so may be we will stop here and we will do numerals next time and i will do the quite um combinator transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 29 data and fix points  time 1  00 min  iit delhi welcome to lecture twentynine so we will just briefly review what we did last time and continue with fix points so i should use some important combinators yesterday k s omega b there is another important combinator w which i will mention when it comes um okay and then so we will looking at data structuring in the lambda calculus how to represent data as functions so here the truth values um true and false um and of course we are in still we are in untyped world so you can always ask whether a giver i mean whether you talk about numeral representations you can ask for example whether a numeral is true or not i mean  refer slide time 02  12  i mean it is syntactically possible not there it should be allowed okay but where it and you might get and you might get actually some answer but that s the problem with it with the untyped world then we also had pairing the pairing function again with its constructor and deconstructor um you can of course apply the deconstructor if you already apply the constructor and then you are guaranteed that you will get the components which were used for used for the construction of the pair but you can not apply a deconstructor and then a um and then apply a constructor to expect to get the same term back  refer slide time 02  37  so um using pairing inductively we might defined um n tuples and for both the construction and the deconstruction and we looked at this we can also define sequences in this form and for which this is the constructor and you will have deconstructors like this which look like this so you want to be able project out the ith component um and get this and if you use this combinator ui n which has n abstractions and then you put that entry this and make this application then you will get um the ith component of the sequence and of course the last and most important things how do you represent the naturals in in the lambda calculus so we will look at the representation of the numerals the representation of numerals is actually a very important feature of the lambda calculus unfortunate and but the only thing is that there are a lots of different representations of numerals and they were used because as i said church is original agenda for the lambda calculus it was abe to show what exactly is computable and what is not right and so there was a going body of work called recursive function theory which was really set of functions which can be programmed which which which people thought could be um could be mechanizable at that time so and the natural numbers formed an important part of that so to represent the naturals this is not churches original notation this is not this is not churches original representation his original representation is a little more complicated and it does not satisfy some nice properties so you just take the the i combinator the identity function itself as zero okay and this n plus one i am writing n plus one because well i don t know what else to write i mean i could have written n prime may be okay but this this whole thing is not suppose to be addition it suppose to be a single combinator um this single combinator is just the order pair false with the representation of n so essentially since its an inductive definition essentially what it means is that a natural number um other than zero is represented by the combinatory i and any other number is actually an ordered pair of false so there will be some n falses and then i except that they all be nested with in the pairing fucntions so it s a pair containing false and another thing which is a pair containing false and something else which is a pair containing false and something else so on and so forth right and of course what you require is the successor function in our peano arithmetic and this was a successor function is just defined in terms of pairing right so given any x the successor of x is just the ordered pair formed by false and x and we can actually take the predecessor function and predecessor function is just for any x its predecessor is is obtained by applying that x to the combinator false of course zero should not have a predecessor but then there is no question of undefinedness and so on and so forth in this case so the predecessor of zero is taken to be zero itself so if you actually apply i to false if you actually apply i to false what you will get is um you will get false um you will get false this is not right  refer slide time 07  10  so  noise  so this is so this is one of those things which um you know in in untyped world this is one of those expected things it is expected in the sense that you can get unexpected results when you do something you are not supposed to do like finding the predecessor of zero right um and but how ever the predecessor of n plus one would be n right um then there is of course a zero predicate which is just the natural number applied to true okay and you will be able you will be able to get both true and false answers from this and once you have this you can basically define other all other functions and naturals using these right so there is no problem about it so essentially we have looked at data structuring the most important data structuring facilities are you should be able to represent booleans and numbers real numbers are just ordered pairs of natural numbers um integers are just ordered pairs of a sign bit and a natural number so you know that a sign bit can be an boolean or and a natural number so you mean integers reals floating point those are all its all trivials once you have represented numerals those things are trivials and of course we have looked at the basic data structuring facilities forming pairs tuples and sequences and essentially you have the power of  noise  excuse me you have the power of essentially the data structuring facilities in any programming language right but now we will come to an important so let me tell you a story  noise  in a forest of lambda terms there was once a young combinator once upon a time in a forest of lambda terms there was once a young combinatory called y so people ask why y and they got back the reply because why why y so people asked but why why why y and they said well because why why why y why y and it went on okay and that is this young combinator looks like this okay now if you if you if you take take a careful look at this i have of course i have followed structured programming facilities and provided abstraction and so on and so forth so i have give a new name v to this lambda term and y is lambda x v v where v is this now so essentially if i remove if i substitute i mean if i remove this v and write out the full lambda term this combinator looks like this it s a lambda x lambda y a applied to y applied to y this whole thing is applied to x okay  noise  now we have seen a similar combinator that something that looked a bit similar um which was the omega combinator okay anyway so this combinator which i will keep away here so here is y colorfully embellished so that you can distinguish all the terms okay so and the combinator omega that we had was some what similar to this i mean except that there was no none of these x is here and there was in this abstraction over x okay and if you just throw a mind back to what we said about omega omega keeps beta reducing to itself or to an alpha version of itself right okay so its so omega is sought of d the ultimate in undefinedness of course its an infinite computation it which doesn t show any output okay its really like an infinite loop but so if you take this y instant which is not quite that okay  refer slide time 12  50  if you perform this beta reduction you get something else i mean you don t really get what the effect of y but you do get some form of infinite computation okay so if you consider any term m and this by any term m i am saying any term now you can include y if you like i mean the term m could be y itself which is y why y why why y why y why y and so on and so forth okay um now if i take if i apply y to m now what happens um so this a there will be a beta reduction so this this this left bracket comes here and then this m comes here and then there is a closed square bracket um right parenthesis actually and then i can do this beta reduction i i don t do this any of the i don t do this application at all but what i do is i apply i do the beta reduction over this x then when i do this beta reduction over this x what it means is that this abstraction goes this application remains so this blue left parenthesis is not this one but this one okay and lambda y and instead of x i put m y y and then again lambda y m y y right and now this is itself a beta reduction um this is beta redex and i can apply it and i can apply beta reduction to it  noise  and when i do that what i get is this so so this is the beta reduction um so what it means is that each of these light aqua colored wise will be replaced by this orange term right so when i do that replacement i get this m applied to this thing the blue brackets are of course gone away due to the application  noise  but the but this so this aqua bracket is this aqua bracket and this bracket is this aqua bracket and these two aqua brackets are these two aqua brackets right and each of these ys has been replaced by this term which of course i have written in orange and red to distinguish that right now what is this term this whole term which is been underlined in green is just y applied to m right  refer slide time 14  52  so now so what is the moral of the story you get that y applied to m is beta equal to m applied to y applied to m and that is beta equal to m applied to m applied to y applied to because i can taken this ym and do a beta reduction and get another copy of m applied to y applied to m and so on and so forth okay um and of course if i if if instead of m i used y then i get y applied to y y applied to y applied to y y applied to y applied to y applied to y y applied to y applied to and so on and so forth the moral of the story is that there is this this combinator y has a peculiar property and that is that given any term m it automatically gives you a fix point of m okay so this since if you look if if we think of these um lambda terms as being functions then this equation is really equivalent to a function um something which says so if i take this y ym as some m or some x so then what happens is that if x is equal to ym then what i get is that x is equal to f of x where f is m which my definition is a fix point okay i mean when is x a fix point of a function f when x equals f of x right so up to beta equality the y when applied to any combinator when applied to any lambda term automatically gives me a fix point of that lambda term so um right so so what happens is that so i can always find fix points by just applying the combinator um applying the combinatory y on the term so what i mean is i can get a something that satisfies such an equation um and the property about fix points of course is that i mean in normal manner going back to a normal mathematics is that okay if f equals to f of x then i can apply f on both sides and i get f ox x equals to f of f of x but already know that x equals f of x so this equals this and i can go on add infinite terms so in fact i can keep on applying f again and again right thats what we do and that s that s the property of a fixed point  refer slide time 17  01  so um here essentaially the same effect has been obtained right so this y applied to m is x this m is f then we get x equals um fx equals f of f of x equals f of f of f of x and so on and so forth okay  noise  so what is the importance of fix point operators right we have already seen fix point operators right some where but the one important thing to notice okay um we will worry about that so essentially um so this forked tale actually is summarized in this so y is the fix point combinator in such that you can keep getting you can keep getting this sequence add infinite term with out any problem and we have seen other fix points also right  refer slide time 18  04  so lets look at some other fix points you have this um so instead of looking at mathematics we should actually look at programming so we have the standard while loop for example in our imperative programming language and that is semantically equivalent to this in our operational semantics that s what we defined right so this while loop is like some y some combinator y applied to something which includes b and c and the effect of that is to unfold the while loop okay that that some thing this that something um with the m in this case is that something which contains b um b and c and this is exactly like and now i can unfold this while again and i can get an semantically equivalent construction like this and so on so forth and i can keep expand unfolding that while loop again um  refer slide time 19  25  so each inside while loop can be expanded out into this and which is exactly what happens in this case so if you look at if you look at this um if you look at um if you look at if you look at this than that s really what you are doing you got some so  noise  so this body um this context consisting of b and c  noise  is the m here the while is just that y applied to m and the effect is to expand out the inner most while by creating a new copy of the body right so you can just like you can by beta reduction can go in this fashion what we are doing in our operational semantics in our implementations and so on is that we are gradually unfolding the while loop one at a time for each execution of the program here unfolding the while loop in terms of an if then um so this this y actually provides that required unfolding that you want how much ever you like in the case of for loops for example it s a finite number of iterations so which means it is a finite number of unfoldings in the case of while loops its actually an indefinite number of unfoldings right  refer slide time 21  05  so you take this inner most body so this while loop is actually some thing of the form y um y applied to m and it can be unfolded in each case to give you more applications of m on to itself right  noise  and if you look at other fix point equations in for example data and so on we actually have a similar we have a similar situation so take take an alphabet a and take the set of all finite sequences or finite strings on this alphabet so what is the set of finite strings on this alphabet it s a set of its it s the empty string and prefixing any string of the set by a letter of the alphabet okay which is like a one step unfolding of a star okay so this epsilon union a prefix is actually like your m and the star is like y applied to a okay and the result of that is to provide unfoldings of this of this form  refer slide time 22  39  now this a star in turn can be unfolded into this portion into this fashion and so on and so forth right  noise  you take in fact anything which has a fix point this is what is going to be happen so so if you take this collection of all finite and infinite strings on this set a then i can it has a basis which consists all the finite strings and then union um prefixing of all strings in the in the same set a infinity and this is like a finite unfolding you can go on in this fashion for ever this is you can think of this is a set of as a collection of equation which are all satisfied by a fix point by some um by any function which has a fix point how do you get that fix point applying the y um by applying the y combinator to any function um so the y combinator is actually quite interesting right so lets lets go back to this while loop  noise  one of the things that best decide when i was when i was defining the while loop semantics was that i said it is not structurally inductive okay  noise  now actually what we can do is we can define a structurally inductive semantics and that s very simple what do i do i have some thing consisting of the set of states i have some set sigma right capital sigma which is the set of possible states and i can take any mathematical domain including what ever what ever including the set of states and apply a lambda calculus on it i mean in the sense that i can define an applied lambda calculus of the algebra of states i mean i can take any mathematical domain and i can do lambda abstractions and so on the lambda calculus is sufficiently independent to allow for such applications we have already seen how we get an applied lambda calculus on natural um natural number like peano arithmetic and so on do the same thing for states then what it means is that in particular you can apply the y combinator on any function on states and you can get a fix point i mean that s what it guarantees i mean you take any combinator um you take any lambda term and you apply the y combinatory on it you get a fix point and see how this while loops works means that we just have to find a fix point on a function on states that function on states is defined by the boolean condition and the body of the while loop right so here is the semantics so there is some function w which is defined as follows okay so if the boolean for example starting in some state sigma evaluates to false then this wbc on sigma evaluates to sigma and if bc goes to true um if b sigma goes to true and c sigma goes to some state sigma prime and wbc applied to this sigma prime goes to the some sigma double prime then wbc on sigma goes to sigma double prime this wbc we know exists now now we that it exists because you can always have define a applied lambda calculus on the functions of states can always takes this states space and use applied lambda um and include the um and on top of it have the applied lambda calculus the pure lambda calculus sitting on the algebra of the functional states its some function which which satisfies this properties note that these arrow marks for wbc do not have a label okay the label if i have to give them a label that label will be like the label that i defined for the applied lambda calculus on peano arithmetic each reduction is either a beta reduction or a peano reduction in this case this arrow would be either a beta reduction or some reduction defined on the set of states okay  noise  so i do not have subscripts on all arrows which deal with this um with this wbc because this wbc note that its its its not green i mean okay wbc is the highest level of abstraction you can go to so  student   it s a function on states  professor   it s a function on states okay it takes one state to another state and it s a function on states which depends some how on c it s a function on states constructed from behavior of b and c okay i mean that the only way you can apply that um the only way you can get a semantics of the while loop is that it should some how depend on the boolean and a body right what we do from the  noise  by putting the pure lambda calculus on top of what ever may be the calculus  noise  what ever may be the reduction mechanism for states is that we still can get a y combinator we still have the y combinator of the lambda calculus and for any term i am seeing you can there are no constrains at all on that term m that term m could be either pure or applied it doesn t matter the only the point is that when this term is whether its pure or applied we are not looking inside this term m right so we are not using the structure of m or the properties of m in any way these beta reductions the y just allows that what ever may be this it might be just one tone of garbage too i mean what what it means is that it allows for replication of that term okay the behavior of the y combinator is completely independent of this term m this term m can be a pure lambda term it can be an applied lambda term it can be an any it can be an any mathematical domain okay and i am not looking into it i am not manipulating m in any other way all i am doing is this y just creates new copies for the application of m to it to what ever is the rest um so if m is actually a just a data object and therefore its not a function what i will get is um an applied lambda term which i can not interpret okay  refer slide time 30  05  but if this m is actually a function in some in my domain in what ever domain of interest that i have then what i get is an application of this function  noise  i am assuming a unary function um so it means an application of this function repeatedly satisfying this fix point equation right so this wbc is really constructed through the y combinatory and what what does it do wbc it just tells you the it it just gives you this beta reductions actually i mean straight away except that you might also mix beta reductions with the reductions in that in this in this um in this algebra of states an so what ever  noise  so if b is so so the only constraints we are imposing on this function wbc is that if b in state sigma as false then wbc should do nothing and just returns that state if b in this state is true then what should wbc do it should be it should iterate the body c once and then create what ever is the effect of wbc on the new state this is iterating the body c once and on that new state find out what ever wbc can do and thats what wbc can do original state sigma that we started output  refer slide time 31  56  and so now this wbc of course depends only um the rules  noise  remember that um a long long time ago when i spoke about transition systems i was saying that almost anything on earth can be represented as a transition system including functions after all why shouldn t we have functions also represented instead of representing functions as equations i will represent them through this arrow mark through this transition as a transition right so the successor function applied on m goes to a transition which is m plus one right so this wbc is actually a function and what is this function its actually the function which some how takes b and c creates a new function okay its really a function of this form it s a function lets say f bc on sigma right so this function is sigma if b is false and it is a what ever  noise   refer slide time 33  15  what ever is the effect of c applied to fbc when sigma is true or rather if fbc applied to c applied to sigma what ever function c might be okay and this function so from the from the functions of b and c i some how combine up to take and apply a y combinator on to them and i get this function wbc right and just so i have defined this function wbc in order ti be consistent with our transition systems semantics i have defined this wbc also through redcutions and otherwise it is a function um and os now we have a purely structurally inductive definition for the while loop which is not really operational so if you ever do a course on semantics of programming languages one of the most important thing going to be the construction of fix point combinators um  student   in the second row two sigma dash are same in the second row sir  professor   these two sigma primes are same these two sigma double primes are the same if b in the original state sigma evaluates true in our original while language you know the side effects okay right but if supposing you had side effects and so on what it means is that i would just transform this sigma to some tau and work with that tau that s about all that s the only change that will happen in the rule its not going to effect this semantics of the while at all in any way so if b evaluates to true in this state sigma and one execution of the body of the c actually gives me sigma prime this this actually should be a many step a possibly many step evaluation right and if wbc is the function on this new state sigma prime the new sigma prime is obtained after one execution of the body of the while loop if gives me a final state sigma double prime then wbc on the original state sigma also gives me sigma double prime okay and on the other hand if the boolean in the original state evaluated to false then wbc just leaves sate unchanged right and so the semantics of the while loop so this is really like the if then its really the if then so the while loop is really the y combinator applied to an if then which is which is really what are which is really how operational semantics looks so there is this function made up of the function booleans b and the body c which is really an if then function with some sought of a parameters whole for it and the while loop is just the y combinator applied to this um so  noise  i don t want to  noise  be label the point its just that um what i wants to says that its possible to define an inductive a structurally purely structurally inductive semantics for the while lop something that we had left um really undefined um we have given it in an operational flavor in terms of an implementations but if you want to purely structurally inductive definition it is possible not that its very easy to understand i can um i can see that its quite hard to understand but anyway if you ever do a course on semantics of programming languages you will have to deal with fix points a large numbers of them and it so happens that in the lambda calculus this this y was a combinator defined originally by church okay and it satisfies it does not satisfy one very important property so  noise  for any term m y applied to m is beta equal to this m applied to y applied to m but y applied to m does not beta reduce to m applied to y applied to m um lets go back  noise  why doesn t it beta reduce  noise  so um we had this right we can just find it up um here it is here it is why doesn t y applied to m beta reduce to so what ever i done here i have taken y applied to m and with one beta reduction i got this step with another beta reduction i got this step but i inferred the beta equality from the fact that this is equal to y applied to m its going back i mean going backward but y applied to m does not itself reduce to m applied to y applied to m okay does not by itself reduce its just that beta equality since its defined as its defined in fashion um where you can either go forwards away reductions or backwards away reductions it doesn t matter then i would consider the two terms equal it because beta equality is defined in that fashion that y applied to m is actually equal to m applied to y applied to m but y applied to m does not beta reduce to um does not beta reduce to m applied to y applied to m and in fact a nice property that we would like to have is really that it should be possible is it possible to find a combinator which will satisfy the property that when ever its applied to a term if beta reduces to this expansion that s really what happens in our implementations okay  refer slide time 39  44  if you want to accurately modern implementations what you are doing is you are you are actually going through this beta reduction you are not going through an equality you are using the equality may be to reason but the actual execution of a lets say a while loop program is that you reduce that while loop program if you go back to a original operational semantics  refer slide time 40  30  how how did we define the original operational semantics in the while loop especially let forget about the case when b b is false when b is true we wrote it as if b sigma goes to true sigma when we say that the while b do c sigma goes to c semicolon while b do c sigma right so what we are defining here is the reduction okay this transition is like a reduction which actually expands which actually does the unfolding and this is a accurate rendering of how you um how your loop is actually implemented by a code generated um by the code generation procedure in your compiler this is actually the way it is implemented and so what we would like to know is whether it is possible to have a fix point combinator which actually does this unfolding which for example church is y combinator does not and it turns out the turing actually defined one which does  refer slide time 41  30  and you can check it out at home  noise  so the turing fix point combinator t is defined in this fashion um lambda x lambda y x applied to yy applied to x and you can see that at and you can do your beta um do beta reductions to c that it actually directly reduces in this fashion  refer slide time 42  35  so it actually does the unfolding that you require for your implementations um right so lets um you you can try this out yourself i think it s a great deal of effort the whole point is that you have to be careful in order to check check such a thing you have to be careful that you are not using the fact that you have named something before as something you are not using that that s what we did in the churches combinators it does not directly reduces you have to use the rules for many step beta reduction very very rigorously um okay so now lets lets look at the beta equality some of the things we have done over over the past few lectures is that considering the way beta equality is been defined two terms are beta equal in order prove the two terms are beta equal you can do one of the following well one thing is directly reduce the left hand side term to the right hand side term l to m other possibility is you can directly reduce the right hand side to the left hand side and then they would be both the beta equal a third possibility is that you reduce each of them to some common term right i mean so lets look at so lets look at the beta equality from the point of view of our new beta equality which includes the alpha conversion so in in in all these many step beta reductions you might also some alpha conversions in between you can intersperse them any where you like right so we will assume that alpha conversion we wont explicitly mention alpha alpha alpha coverison we will say that c syntactically syntactically equal so strictly speaking what i would says to in order to prove l is equal to l is beta equal to m by the third method it means that you reduce them both independentally to some two terms m and m prime which are mutually alpha convertible if you like but  noise  i will consider the alpha alpha equivalence is same as the syntactic identity and so essentially what it boils down to you is that you can reduce them both to some common term and then when you reduce them both to some common term you can claim that l is beta equal to m there is actually a fourth possibility and that comes from the fact that the operational semantics of the lambda calculus is non deterministic  refer slide time 46  02  what i can do is i can find a term p okay which reduces by one way to l and by another way to m since the operational semantics of the lambda calculus is non deterministic it actually allows for this possibility i mean um so  noise  i am sorry this is something that we um we do not very often use because we are all too concerned with proving things by reductions but there is also a method of proving things by abstractions right i mean right and so that you find a common abstraction from which both of these will reduce to these terms may be in two different ways i haven t yet i mean this is not clear the non determinism of the operational semantics um is there any guarantee i mean that but what i am saying is that that is something that we have to prove how how do you know it will always work i mean no you are saying that if its not true then um then we will not be able to use it which is true i exactly agree with it but there is nothing in the development we have said so far which actually guarantees that you will find this n what is there what guarantees do you have that you will be actually able to find a common m it might sometimes be they might not be a common m always i mean so the fourth one does not necessarily reduce to the um the third one what we are saying here is i look at the structure of l and m and i make a guess about a p which is a lambda abstraction within application may be such that through a sequence of beta reductions i can get p um l through another sequence of beta reductions starting from p because p might have more than one beta redex so so if you want to if l and m are distinct then p is something which which either has more than one beta redex initially or some where along the line through a common beta reduction it expands to a term which has more than one beta redex and by applying them differently by going through the beta reductions in in two different ways i might to get l and m i mean its its a feasibility it s a feasible solution but there are no guarantees that all of them will reduce so so all i am saying is its its just like i mean um given given a theorem to be proved i mean there are many different directions you can follow some of them might lead you to the conclusion others may not right so so um these two are not equivalent these two are not equivalent and there is no guarantee that there are going to be equivalent okay so but this last um this last kind of alternative is something that we have not actually used in our manipulation so far but it s a it s a certainly feasible um it s a certainly a feasible method for example i can take this combinator so i can ask if for a given n the combinator f satisfies this beta equality then what is f well it is one way of looking at f i will just say that f is this combinator and other most natural things but then i decide i don t want this horrible thing i will look for something else  refer slide time 50  07  i will look for a property so i will look for this so if f of x is if f applied x is beta equal to this then this is a self application this is a form of self application which is like which is like so if i look at this structure its like n is applied to x is applied to x which is like a composition of two fucntions so i take the composition operation b and redistribute the parenthesis in this fashion right now i have a guarantee from the structure of b and by lambda reduction that i can actually get this by many step reduction from this okay  refer slide time 51  28  now there is a  noise  one important combinatory which i did not mention at that time was the w combinator which is just a diagonalization w is just a lambda x um lambda y x applied to y x applied to y x applied to y y um right so then if if i look at this w then i can go backwards i know that from w this way i can get this by a beta reduction right so now i have got two structures which are of the same form and f um at the last a single at the last and f applied some thing so this is some thing applied to x and so i claim that f is w b applied to n but then this is again like a function composition so i can look upon this bw applied b applied to n right so this is going backwards by the alternative four right and important question that he has raised is really the last important thing  refer slide time 52  30  is it really true that given a lambda term p and if it reduces in two different ways because of non determinism to l and m is it true that they will both eventually reduced to some common term we are two used to seeing such things being taken for granted in mathematics  refer slide time 53  36  but we are there are as yet no guarantees that this property would actually hold in the lambda calculus right i mean this is a typical school mathematics problem right i mean you can have two different solutions um it shows non deterministic reductions it shows different applications of different formulae one uses that the fact that a square minus b square is equal to a plus b multiplied by a minus b the other uses the fact that a plus b the whole square is equal to a square plus b square plus two ab hence and you get the same answer i mean we used to getting the same answer that we take it for granted but how do you know it is actually going to happen and that is an important property and unless it is guaranteed this does not really modern programming in any way and we have to some how guarantee this um so transcriptor name  m satish proof reader name  programming languages dr s arun kumar department of computer science & engineering lecture 30 normal forms  time 1  00 min  iit delhi welcome to lecture thirty so let me just briefly go through what we have done and continue it in normal forms um so one of the things we did in any kind of computation is that we express the meaning as of un the meaning of an expression as a value it reduces to  refer slide time 00  56  the meaning of an expression in peano arithmetic was just some representation of the number it is the value of the expression in the lambda term um in the lambda calculus is an lambda term that contains beta redexes in a in an applied peano in an applied lamdba terms then it something that it doesnt contain either any beta redexes or any peano redexes  refer slide time 01  10  so lets look at this thing um what might be called a model for meaning in some detail so lets go back to the beta reduction and i will define a basis for a reduction  noise  as just the first axiom right so this is just an axiom um what is an axiom is is just some enary relation in the case of reductions and axiom is a binary relation which as a left hand side going to a right hand side so i can set of all bold betas is essentially this is essentially a rewording of the axiom of the first beta axiom for beta reduction right so for all terms l and m belonging to lambda this ordered pair which s which means that this application can reduce to this substitution is the basis of the reduction um the normal terminology is to call it a notion of beta reduction but i think since we use the word notion in a very general form i will call a basis for beta reduction many books will call it a notion of beta reduction and what we do is and your one step beta reduction is really what might be called the compatible closure of this set right so so what is the compatible closure is just is just that when you get these applications in some context then the whole context reduces that way i mean this is something that we have done before right so the compatible closure is is really really consists of these rules which we gave in the operational semantics right  refer slide time 03  50  so this first beta one is really the basis of the beta reduction and the syntax of the lambda calculus allows only three possible constructs um or rather two possible constructs but then application works o can look upon either forms of the application as two possible constructs and this the one step beta reduction that you get by closing it over these three rules is what is the compatible closure and that is the one step beta reduction right  refer slide time 04  41  so so now so this one step beta reduction is just what might be called the compatible closure and the many step beta reduction is the reflexive transitive closure of the one step beta reduction and the equality is just the symmetric transitive closure of the many step beta reduction so so far so good so the question of what exactly constitutes equality is something that is been subject to a lot of philosophical debate and the main um the consensus is that is that is that is a sort of emerged is when you look at any forms of equality it is important to look at look at it from an extensional point of view okay so by an extensional point of view and then this extensionality is a is a fairly fundamental philosophical concept which goes over starting from mathematics um on to all branches of engineering i mean including electronics for example right so what is extensionality firstly when are two functions are equal so two functions f and g are equal lets for the purposes of argument assume that f and g about unary functions it doesn t mater really if they are enary functions then i will just make this x a vector x okay so if f and g are unary functions then f is equal to g if and only if for all values x f of x gives the same value as g of x okay so this is this is what might be called an extensional meaning of an extensional notion of equality i mean the effect of the function if the if the if you look at the two functions as black boxes with an input port then for all possible input values they give you the same output then essentially the two black boxes are in distinguishable and one can be used for the other right that is that is extensionality okay now this is as far as functions are concerned as far as sets are concerned for example its its exactly the same the basic the basic s that you can perform on a function regarded as a black box is that of supplying inputs and getting outputs the basic tests you can perform on sets regarded as black boxes is to test from my machine give a input and get a yes or no answer right so regardless so  noise  two sets are equal if and only if they contain the same members okay so that s like testing the that s like looking at the set as a black box and not looking inside too deeply for example how exactly is the set represented how exactly is the function representing and so you are taking a black box behavior of the entity in question and you are trying to um you are assuming that you can perform exhaustive number of tests even if the number of tests you have to perform is countably large or even uncountable assuming some how that you can perform the an exhaustive number of tests you decide on that basis when two sets are equal when two functions are equal and when two programs are equal  refer slide time 08  57  in fact the definition of the semantic equality of lets say two programs is really based on in extensionality principle no no extensionality principle is just that you view an entity as a black box and test it exhaustively okay so that means you are by extensionality what we mean is as opposed to as opposed to what might be called its intention right so extensionality is best understood if you understand what intention is and intention is just construction of the black box  refer slide time 09  29  the internal construction supposing you remove the cover on the black box and looked at its internal details so you could have this function f of x its internal details are such that it is some how of the form x square minus one you have the g of x which is of the form x plus one multiplied by x minus one so the internal details in terms of construction are different for the two how ever since on testing they give the same results you consider these two functions to be equal on exhaustive testing for any value of x the black box f and the black box g give you the same results so you consider them to be indistinguishable and that is in fact what is also true of sets its true of um so if i can construct so if you look at the internal construction of the set is just that this is a set consisting of all multiples of thirty where as this is a set which first you construct all even numbers the set of all evens the you construct the set of all multiples of three then you construct the set of all multiples of five and then you take the intersection of all these three states and you get some set and what set you get so the internal construction of this set is different from the internal construction of this set the fact that they are both equally some thing you can prove through number theory set theory and so on and so forth but the essential fact is that there internal constructions are different they are like too different for example two um two different electronic circuits which are some how equivalent right right and um so so the intention is that we should not worry too much about so when we are talking about equality we are really worried about what might be called the extensional behavior so if you i mean so um well the two favorite programs the most favorite programs in semantics are these right so these two i mean they are internal constructions are different but but they are really the same program  noise  that may be let me keep it like this um one computes um lets for the moment lets assume that we are talking about x being a natural number so one computes y as a factorial of x um by counting downwards and the other computes y as factorial of x by counting upwards right i mean so the internal construction of the two programs are different but their behavior in terms of um in terms of the actual output that you get which is why is the same for all natural numbers x right  noise  so so that that is so this is the notion of equality which provides essentially all um all branches of engineering mathematics ad so on right  refer slide time 12  20  so one of the things that happens with the beta reductions so when we are talking about equality one of the things that happens with beta reductions is the following right so consider so consider a term l which does not have x is a free variable and consider any term m right and now i can i can i can look at this term l applied to m okay and i can look at this term which is  noise  l with a lambda abstraction and this lambda abstraction is applied to m okay in both cases so this lambda abstraction when applied to m means that all free occurrences of x in this will get replaced by m and since and l does not have any free occurrences of x what it means is you will just get l applied to m in a single step of the beta reduction okay and from since from here you get here in a single step of the beta reduction this these two are actually equal by by the beta equality principle but there equal for more important reasons there equal for extensional reasons  noise  regardless of what ever might be the argument m both these black boxes l and lambda x lx would give um would give you the same result that s for all possible arguments m right that s the important thing they would both give the same results for all possible values of m but with within beta equality we really can not prove that they have both um we can not we can not prove directly without applying on m we can not prove that l is beta equal to lambda x lx so we infer the equality from extensional um from its extensional behavior  noise  typically a lambda term is mind to denote a function and if the function for all arguments gives the same if if two functions for all possible arguments gives the same values then we have considered them equal but how ever our beta equality is actually is not strong enough to capture that form of equality in isolation only  refer slide time 17  10  so you can prove that so we can only prove that l applied to any argument is beta equal to lambda x lx applied to the same argument but we can not prove that l itself is equal to beta equal to lambda x lx though of course it makes a lot sense to equalize the two so the beta equality is not sought of powerful enough to take extensionality in to account  noise  and so what we normally do is if we looking at lambda calculus as a way of representing functions is that we add an extra notion of reduction or an extra reduction basis and that reduction basis is the eta rule right so what this says is that its important to realize one thing that l applied to m is beta equal to lambda x lx applied to m only if x is not a free variable of l if x is a free variable of l then these two are not going to be equal because the free occurrences of x within l are all going to be replaced by m and you will get something different l applied to m okay for example if x is free in l and you l to m then m is going to replace some other bound variable its not going to replace x right for example if you take  noise  l were defined as lambda y x applied to y okay where x is a free variable of l the l applied to m is going to beta reduce to um to x applied to m however lambda x lx applied to m okay is going to reduce to um well l all free occurrences of x in this term are going to be replace by m so what is going to be happen is that x is going to replaced throughout so you will get lambda y my the whole thing applied to m again okay  noise  right and even though this might be this reduces further may be to um this reduces further to mm m applied to m but these two these two are not equal this x applied m and m applied m are not equal right so its important to realize that only  noise   refer slide time 19  18  whenever x is not a free variable of l and you have a term of this form then the effect of this term on all values of on all arguments m is going to be the same as the effect of this term l on all values of m  noise  so we will define this notion this notion of reduction or rather this basis of reduction eta and we will define its work a one step eta reduction as the compatible closure of eta over all lambda terms the many step eta reduction and equality eta equality as as we did for beta equality okay  noise  and what we could do is on on various so we could also define now supposing we wanted the extensionality principle actually to be used throughout then what we could do is instead of talking of just beta reduction and eta reduction in isolation  noise  we could combine them both after all beta reduction is fundamental to us as function application so you could combine the two so you could take the two basis and take the union and then correspondingly do a give a compatible closure will get a one step beta eta reduction which means that in either take a choose to do a beta reduction or an eta reduction  noise  you can intersperse beta reductions and eta reductions in any way you like and you don t have to follow any strict sequentiality  noise  we can define the many step beta um beta eta reduction has the reflexive transitive closure of the one step beta eta reduction and we could define the beta eta equality and the beta eta equality is really what we would what we would is an equality which is compatible with our notion of extensionality oaky  refer slide time 21  35  you say both function application and extensional equality right so we wont look at  noise  we wont we wont be label this point its just important for us to know that there are various kinds of reductions we might define and in fact this is not something new because the peano arithmetic though i did not actually mention it we actually had two different notions of reduction two different basis for reduction and we had this language of expressions of peano arithmetic and then so  noise  so the basis of  noise  in peano arithmetic were really these two sets and what we actually used in our in the example of peano reduction which i did not mention is that we used first the union of these two and we actually used right  refer slide time 22  38  so this is just these are just the two definitions of addition and multiplication represented as binary relations right i mean um m plus zero is essentially m for all m belonging to n and um m plus successor of n is the successor of m plus n and m multiplied by zero is zero m multiplied by the successor of n is the sum of product of m and n and m right so what i had given is rules um for peano arithmetic and i did not actually specify but this we are um we were essentially using it without actually mentioning it and next and we also we were actually using a compatible closure of this which is a one step plus star reduction and remember that if you gave these rules which we omitted give at that time which because it was all taken for granted  refer slide time 23  25  its actually non deterministic it doesn t matter how i mean they they could be more than one possible reduction and then actually assuming all this implicitly we actually did this derivation without actually using the rules right so  noise  of which we took we took one possible execution sequence right so but essentially in any um in any form of computation including lets say peano arithmetic we have this we have first of all on deterministic sets of rules secondly we have some notion of um combining reduction basis so that you can apply things in arbitrary order and we have the concept of a final value which is a normal which is what is called a normal form right so let me look at so now what when we when we are looking at normal forms which so what we will do is we will look at normal forms as a general notion so remember what we said about meaning the meaning of a term is the value it reduces to so given any any kind of reduction basis a term in that language is an r normal form if it has no r redexes i mean we can define plus redexes star redexes um and we can define for example um in the lamba calculus beta redexes eta redexes beta eta redexes and so on right  refer slide time 26  24  so you can actually  noise  we you can actually talk about so if we look at the original meaning of notion as the value that a term reduces to in the peano in peano arithmetic its really the value represented in that in the form of this grammar numerals right so its its really a every number represented as either a zero or successor of a number right that s the normal form that s the value that of value representation of a value in that language and  noise  given a basis so we um in the lambda calculus two we can talk about beta normal forms essentially as a term which as no beta redexes that s the value of any term so you perform all the beta reductions still you can find no more beta reductions no more beta redexes that s the value similarly you can give talk about eta redexes and eta normal forms as a term in a eta normal form if it has no more eta redexes and beta eta normal form is one you pair you can neither find a beta redex and nor can find an eta redex right and essentially the computation terminates there and that that itself is regarded as a value of the original term you started out with okay so when we look at normal forms the important question is in the lambda calculus is does does every term have a normal form so lets lets just look at beta reduction so does it every term have a beta normal form in the peano arithmetic it terms out that every term does have um does have a plus star normal form because arithmetic expressions not having variables will reduce to a value okay if you allow variables then you should allow variables also in the in the in the basis language of expression and then what what will happen is you will get you will get a normal form which is either a value represented as zero or the successor of some number or you will find it as a sequence of successors of some variable or a variable itself right if you allow variables into peano arithmetic and into the expression language of the peano arithmetic so now in the in the case of peano arithmetic you um you actually have you actually have all terminating computations you all always have a plus star normal form but in the case of lambda calculus we had this horrible term called omega for example which does not have a normal form because at every step of this computation you can always find um a beta redex  student   so there is peano arithmetic we know variable x um plus zero dash x that wont be what even does it do it  professor   plus zero dash x that would reduce to if you go through the beta reduction rules it should finally reduce to plus x dash zero and therefore it will also reduce to x dash so that would be the normal form if you if you follow the rules systematically it will reduce finally to x dash um our rules for plus or asymmetric so i mean what it means you will gradually be get the successor from the right hand side argument to the left hand side argument so you should finally get a zero on the right hand side or if there are two variables then you will get a plus expression in terms of the two variables so for example plus of x y dash would essentially just give you plus of x dash and y i mean that would be your normal form um so in the um but you do have normal forms always i mean you have some place you you have a step where you really you can not do any more reductions where is in in the case of the pure lambda calculus thre are terms which always have beta redexes regardless of how many of your reductions you perform and so therefore not every term has a normal form okay  noise  so then next question is we have non deterministic computations so now supposing instead of pursuing um one form of computation i pursue some other form of computation i mean um i have i have a collection of beta eta redexes and i have to choose one i can choose one instead of the other so i have several possible computational sequences several possible beta reductions possible then does every supposing you have a term which has at least one normal form one beta normal form and it it starts of with several possible beta redexes so that means you have several computation sequences do do all those computation sequences yield the same normal form again if you look at peano arithmetic that is in fact the very basis of our notion of computation i mean you will find that all all computations that you might do finally yield the same normal form meant we have grown up with it and we have taken it for granted right but it is an important question i mean if the lambda calculus is really trying to um trying give you a fundamental picture of computation then does every computation of a term which has a beta normal form does do they all give you the same beta normal form and for this again the answer is no so if you take this term k the combinator k is the projection combinator right i mean if you remember i had the combinator k is this combinator right so it takes two arguments one by one and it gives you the first argument however because of the non deterministic nature of beta reduction there is absolutely no reason why i should apply k to x and omega and get you get back this normal form x and so x is a normal form right i mean there is no there are no beta redexes on the other hand this non determinism allows me to do the to do a reduction on the omega as far as i want and at any point i might decide to do the reduction of kx omega okay so there are a whole lot of finite um finite terminating computations which all terminate in the normal form x but there is also an infinite non terminating computations where i have chosen never to apply k the beta redex kx omega but always apply the beta redex omega itself since omega itself is an um is an application term if i always choose to apply the beta redex omega then i get a non terminating computation right  refer slide time 33  53  so so now what is happened is there are some fundamental reasons there are some fundamental notions in which the lambda calculus is actually the different from our normal understanding of computations in mathematics and so on and so forth right the next thing is so now you had non determinism you had non determinism also in the peano arithmetic there was no problem so for the first thing was that the peano arithmetic always guaranteed a normal form which the lambda calculus does not the next thing is even though there was non determinism the peano arithmetic actually guarantees that every possible computation yields the same normal form which the lambda calculus does not and lastly supposing if the lambda calculus does not guarantee these things then does it at least guarantee then that where two different computation sequences exist and which both yield normal forms is it possible that a term can have more than one beta normal form right in fact if you look at the peano arithmetic since all terminal since all computations terminate and its even though there is non determinism they always terminate in the same normal form in the case of um so you can not have two distinct normal forms okay now can a term have more than one um one beta normal form so the fundamental question we are asking is can a term have more than one meaning depending upon how you execute the term okay so if you look at if you think of you re the notion of meaning as an intrinsic meaning then what we would like what we would like in order for computation to a possible is that a term should not have more than one meaning there should be an intrinsic meaning if it has more than one beta normal form it means that it has two different intrinsic meaning and the meaning is dependent on the order of computations okay which means that your your notion of meaning is no longer a function from syntax to semantics its something very peculiarly concerned with um with the order of execution and this is in fact true of many programming languages which have not looked at this problem very in great detail i mean you take this short circuit boolean evaluation for boolean operators like and and or are are instrinsic meaning of the boolean operation and and or are the they are both commutative but if you had an implementation which short circuit evaluation then there is no guarantee at all that you will actually get the same meaning right the intrinsic meaning is that they are commutative but in programs we know especially in the presence of side effects that if you flip the two the two operands of a nand you can get different results because the meaning is some how not intrinsic enough to be determined by this by the semantics but its some how dependant on the computation sequence um so what is the fundamental property which will guarantee that a give term has only one intrinsic meaning right and that is the church rosser property so this is the finally the only thing in which the lambda calculus actually um does something that we expect so it turns out that firstly your first perception that does every term have a normal form well that s destroyed does if a term has a normal form does every computation yield the same normal form its not clear  refer slide time 38  45  but if the condition is that if your reduction a system of reduction is church rosser that means it satisfies this property which is will expound on then its guaranteed that a term can have more can have more than one can never have more than one normal form and in fact that is a fundamental property of all these reductions systems that we are talking about that he should be church rosser otherwise meaning is no longer a function of the syntax meaning is then very highly dependent on execution um so actually it turns out that eta reductions is also a church rosser lets look at the church rosser property in some detail um the proof is rather long um so i will not actually do the proof here but that is really the fundamental property so so now we can talk about this um the church rosser property for any any kind of reduction for any kind of reduction basis right so i will say the a binary relation r this is not necessarily a reduction relation okay this is any arbitrary binary relation on terms of the language a binary relation r satisfies the diamond property okay if this diamond is satisfied okay that means if so given that for all l m and n if it is true that whenever lm belongs to this relation and ln belongs to this relation if it is guaranteed that you can if it is guaranteed that you can find a p such that m and p are in this relation and n and p are also in this relation then you would say that this relation satisfies the church rosser satisfies the diamond property um so it should be possible is it possible given given now in the in the case of notions of redcutions what we are really looking at is is this  noise  a basis of reduction now supposing in stead of just being an arbitrary binary relation if you binary relation was actually a basis of reduction now this basis of reduction is church rosser if well from the basis of reduction you get the compatible closure which is the one step reduction from the one step reduction you do a reflexive transitive closure you get a many step reduction okay and if um if this many step reduction satisfies the diamond property then you would say that the original basis of reduction is church rosser or satisfy the church rosser property right and this is in fact the fundamental thing you have to prove about any functional programming system that you that you design supposing you come out with a whole set of new operators which have got nothing to do with what is there in existing functional in programming languages then when you have a and you claim that you got a functional programming system its of no use to man or beast unless it satisfies the church rossers property and what it means is that if you take okay so so what it means is that this diamond property should be satisfied by by the many step reduction in that functional paradigm um and so here is a theorem which um which has a symbol inductive proof if if a basis of reduction r satisfies the diamond property then its um reflexive transitive closure r star also satisfies the diamond property okay i mean so if any binary um actually this is not necessarily a basis of reduction any binary relation r satisfies the diamond property then its reflexive transitive closure also satisfies diamond property um i hope by now you know what the reflexive transitive closure of the relations right um its like you take well you take um in the in this case its not its not like construction of we are talking of relation and they are reflexive transitive closure so that is not the same as the a star that we constructed for arbitrary sets okay here the r star is like this so you have a binary relation on on a on a set right so r is a subset lets say of um set a cross a okay so this is a binary relation so it has various kinds of order pairs xy and so on and so forth so now i can define r not to be the identity relation on a what is the identity relation on a this is to ensure reflexitivity so its just set of all ordered pairs a such a belongs to a right and given r not i can define r k plus one as being equal to r composed with r k plus one um with r k right and what is this this is just relational composition so what is relational composition if r and s are both subsets are both binary relations then r composed with s is the set of all ordered pairs a coma c such that there exists a b satisfying the condition a coma b belongs to r and b coma c belongs to s there exists b in this set b belongs to capital a right so now what you r star r star is just the union of r raised to k for all k greater than or equal to zero so what we are saying is so um how is the how does the many step beta reduction go its just you start with a reflexive closure which is a one step beta reduction raised to the power zero which is gives you the same terms okay its reflexitivity condition and then you compose it successively for k steps and having obtained k steps you get k plus one steps by this composition right so this is just relational composition so the star is r star they is a relational composition a finitery relational composition that means you do not allow infinitery compositions right  refer slide time 48  30  if you allow only a finitery composition and if r already satisfies the diamond property then r star also satisfies the diamond property and that that proof is very easy um supposing you had l okay you have to show that l goes to some m and it goes to some n in two different ways okay if l is related to if l related is r star related to m and l related is r star related to n then there exists some p which completes the diamond oaky and if l related is r star related to capital m then what does it mean it means that there exists a finite sequence of composition lets say lets say l related is r star related to m implies that there exists some m such that l the ordered pair lm is in r raised to small m which means that i go through m compositions lm one is in r m one m two is in r m two m three is in r and so on and so forth and when i do the compositions i can find this kind of a path so what it means lm two is an r square lm three is in r cubed lm four is in r raised to four and so on and so forth and lm is in r raised to m similarly ln is in some r raised to n for some small n and if this relation r satisfies this diamond property then what it means is that given l m one and n one which are um which are all in the relation this l m one is in r and l n one is in r i can find a p one one such that m one p one one is in r and n one p one one is in r so i can complete this little diamond and using the same reason inductively what it means is that i can complete this larger diamond first mean let us take this slice so having constructed p one one i can use m one m two p one one to construct a common some thing that completes another small diamond which is lets call it p one two and using m two p one two and um m three i can construct another small diamond which gives me p one three and so on and so forth i can complete this entire slice right so having completed this slice i can now extend this this slice to the next slice so there is an n step induction inside which there is an m step induction okay and i can find a p and m so what what um so what is the moral of the story given this fact we can prove that beta reduction is church rosser that the basis of the basis beta of the reduction of beta reduction is church rosser provide um provided we can prove that one step beta reduction satisfies the diamond property if we can prove that one step beta reduction satisfies the diamond property then the many step beta reduction also satisfies the diamond property and therefore beta satisfies diamond property  refer slide time 51  30  how ever one step beta reduction does not satisfies the diamond property um and here is the problem right so let me take this delta this delta is actually you remember omega that horrible thing which goes on forever yes so delta is that part of that of that omega now you take this delta and you take any term m suppose m goes in one step to n so the basic assumption here is that we have assume that m goes on one step beta to n okay then n i consider this application delta m oaky and in one step it can go to delta n right and then i consider this application delta n um which means that all occurences of x here will be substituted by n by the one step beta reduction rule right so in one step beta reduction this goes by the way there should be a beta here in one step beta reduction this goes to n applied to n how ever if instead of choosing the m goes to n reduction if i apply this delta to m directly then what i get is two ms m applied to m and now i can i um i require at least two steps either way to obtain n applied to n i mean this mm and nn do satisfied that diamond property because you can um you can you can do this m goes to n beta reduction any time we like  refer slide time 53  36  but which m are you going to first reduce to n is again non deterministic right and so in two steps i can go to nn but this i can not do this in one step from mm i can not reduce to nn in a single step and that s where the bottle neck is right so um but so how ever beta reduction is still church rosser um and so what it means is that such a simple fact can not be used to prove that beta reduction is church rosser what it means is that but we can use this example some how we define a new form of beta reduction in which parallel applications of these reductions as possible then okay that means simultaneous you can i mean you can do the reduction across all possible beta redexes that immediately visible in one shot okay and consider that one shot reduction which in a single step beta reduction will go over many steps consider that to be a one step parallel beta reduction oaky then what you can show is that parallel beta reduction when you the star of that is the same as beta reduction is the same as beta reduction star and then you can show that parallel um beta reduction actually satisfies this diamond property and beta reduction also satisfies diamond property so so  noise   refer slide time 55  50  so then that beta star um then that many step beta reduction therefore also satisfies the diamond property  noise  therefore also satisfies the diamond property and so it is church rosser um and the important thing about church rosser as is just this that a term can have at most one normal form for any reduction basis r if that reduction basis is church rosser it may not have any is in different computations you may never get a normal form but if it does have a normal form if there exists a computation which produces a normal form then all computations which terminate would yield the um same normal form provided that basis is church rosser and that s what the parallel beta reduction does um so i will stop here and go on to the type lambda calculus next time 