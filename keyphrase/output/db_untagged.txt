database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture # 1 introduction to database management system welcome to this course on introduction to database systems this is an introductory course usually taught at the first semester of third year under graduate course  is also expected that you should have done course on data structures  file systems probably operating system to better appreciate the concepts covered in this course though the course material may not assume completely that you have understood all the prerequisites  it could also be done probably independently because wherever the concepts are required they would have been covered and some difference to the background material would have been indicated in the appropriate sections it ’ s a 42 lecture module of one hour duration and expected to be covered in a semester long course what we will be doing is the two instructors in this course  when myself will be covering the transacting processing system at the later stage of the course and the other instructor dr srinath will be covering the initial sections on database design er modeling and other basic concepts in the initial sections of the course what i am going to do in today ’ s lecture is give a brief introduction to databases and show the importance of databases and also in the process introduce some of the basic concepts that we will be covering in much more depth as we go in to the course as the term databases indicates ever since data has been digitized and we are able to store data in digital form  we see that the mode of data that is being stored by corporates and other organizations as increased from a few kilobytes over a period of time to now terabytes now databases in that sense have become an integral part of our day today life in that we actually do lot of transactions  our day today transactions whether it is railway reservation or it is an airline ticket reservation or withdrawal of money from a bank in some sense we are actually working with the underlining databases that this organizations have they actually store the costumer data and other information  user information in the form of databases  in the form of databases where it is accessible by a number of other entities so that our regular transactions like withdrawal of money from the bank or reserving a ticket for a train  these are actually are business processes that are working with the underlining data that is stored in this organizations in that sense databases are an essential thing in the business processing world and they become a key entity in terms of developing these applications  business process applications to show what kind of you know  importance database have assumed in recent times  i will just show you an example of a business transaction in how we actually work with these business transactions in our day today life i realized only yesterday that i actually have to pay a premium for one of my lic policies and in normal sense what we do is for paying this policy  the lic actually sends us an reminder which is a paper copy posted through the postal mail and normally look at this remainder and then you will actually send somebody or you go to the lic branch and try paying the premium across through your cheque or something bank cheque or other mechanism  if you carry the cash you will use the cash to actually deposit and then take the receipt back all this requires that you physically now move from one place to the other place and sometimes stand in the queue  if there are more people waiting to do this transaction and then the process you will really have to spend time in doing this transaction  business transactions in your day today life but now with the digitization and the storing of this data into the digital form into the databases  what you start realizing is that it is possible for you to actually access this data online and then see that you have to pay a premium and then see you know if you know actually do this and get on to your bank you know and then pay the premium online through the bank and also get the receipt of this  inaudible  transaction back on your email  in your email box which means the whole transaction can be completed sitting in your office without moving even an inch from your office and fully finish this entire business transaction and this is sometimes what we call as work flow to show how this work flow actually takes place let us see how the scenario which had explained earlier can be managed and i will show you online  how this whole thing actually can be done  refer slide time  08  47  so we will actually go to the lic ’ s database so this is the lic ’ s portal which actually gives the user the ability to login to this lic portal now i basically try login in to this database so once i login here  i should be able to get all my data relating to what are the policies that i have with lic and what are the premiums that i paid earlier with lic and what are the due dates  refer slide time  09  09  and if i actually want to calculate the lone information  i should be able to do all this say for example you know you  inaudible  pay premium which have paid online  so i can say that in this particular year i want to see what are the premium that i have actually paid which shows that these are the premium that i have paid already online in this particular year so this database actually shows you know  for example if you want to see the receipts you can see online so all this is being managed for you by the lic database  refer slide time  00  09  39  so once you logged in and all those informations relating to you is available  this becomes a one single point access for you and now we can say for example  one of the transactions that i want to execute now is i actually want to actually do an online premium payment  refer slide time  00  10  19  and once i want to do this online premium payment  i have to choose the policies for which i want to pay the premium online and it suddenly gives me a lot of options here i can say this is the premium for which i want actually do it online so now what i will do is i will say submit  so this should take me saying that yes this is the premium that i have to confirm which says that would you like to pay for this particular policy  this much amount of premium online  refer slide time  00  10  40  now if i say yes actually it will say you know i can only be paying the premium in my name and all that you know things which are indicated here and it also tells that i am going to get on to more secured way of paying this premium  refer slide time  00  11  05  this is the transaction id automatically generated and it says how do we actually want to make this payment i can say since i have now various ways in which these can be pay probably  you can see that there are now city bank debit card you know  sbi let us say that i have sbi account  so i will now try to do this using the sbi net banking  refer slide time  00  11  37  so this will take me to sbi database now it automatically pushes me to the state bank of india ’ s gateway  payment gateway now this will put me into the  you know database of state bank of india  refer slide time  00  12  03  now i will login into state bank of india 's database  give my details and now say let me submit this which should say yes  now it says i have logged into this  refer slide time  00  12  11  and it says now would you like to confirm paying this money to lic  you say confirm  refer slide time  00  12  25  so it says verify and confirm life insurance con transactions detail  so it gives the client code  it gives the indian rupees and date and you say service charge is 0 and the branch is indian institute of technology chennai then i say confirm  refer slide time  00  12  44  and this should now  it says that i have actually paid  your payment request is being processed  refer slide time  00  12  50  so this will basically  underlying take into the sbi ’ s database now i am gone to lic  looked at the premium that i need to pay and i told this much premium i need to pay to lic and then i have gone to my sbi account now i said this much amount be given to lic for paying my premium and it ’ s automatically done  refer slide time  00  13  15  now you can see i come back to lic ’ s portal and it says thank you for paying premium though the payment gate way where the receipt has been generated for the payment  click here to view or print a confirmation mail with a copy of the receipt has been sent to the mail id provided at the time of registration so you can see that now it will give me the full registration  the receipt which is needed for me if i storing it for future requirements to show that i have paid this premium this is important because at a later point of time  we need to say that yes we have paid this premium  so we need this receipt which tells that this is the receipt  refer slide time  00  14  04  and interestingly if you basically look at it  this receipt is electronically generated and is digitally signed so it doesn ’ t require any more signature of anybody else  the lic can in future confirm that since it is digitally signed by lic it can be confirm and saying that this receipt is generated by lic and you can see the whole details are available for me here and i can store it for my future reference this is what we mean by actually a business transaction and a work flow as you can see that i am moving from one database to the other database and in the process actually i am accessing data that is stored in this organizations databases and doing business transactions with them and please remember this is very important because these are business transactions at a later point of time  the bank can not say that it has not paid this amount to lic and lic at the later point can not deny that i have not received this amount so there are lots of issues involved here like the properties of the transaction that we need to ensure saying that the customer is protected again as double payment for example if he clicks twice  he has to be protected saying that this transaction is actually carried out once and not twice and his money as is available in his bank account is properly reflex this payment so as you can see this has become now an integral part  this is one of the business transactions i could execute this online sitting right now here and you can imagine  it can be done very late in the night when you realize that you have to pay this premium and with no intervention of the bank officials with the no intervention of the lic officials since this data is digitized and exposed  you are able to actually as a customer access all these data and do this online without any hassles and hence this becomes an integral part of our day today life and hence databases are an extremely important subject for us in terms of developing these applications and making them usable by in our day today life by people lot of other examples also exists for example  if you go to clear trip dot com you can similarly make payments and buy your tickets and do so many other things with respect to booking your reservations and things like that so in some sense this course is going to focus you and tell you the underlining examples or principles in developing these kinds of databases and what i am going to do in the next half an hour is to actually show you an example of how i could have developed this example and what are the different components that are available for you when you are actually developing these kind of applications for this what i am going to do is i am going to take a simple application that we have in iit madras and we are going to show how incrementally i can develop a database model and a database application for this business process within iit madras the business processes actually one of trying to allow people to purchase computer systems and peripherals by the faculty of iit madras where there are different vendors who can supply these peripherals at different prices so the idea is actually to allow the faculty to be able to purchase these systems at a very competitive rate and what iit madras does is actually it enters into what is called the rate contract with various vendors for a supply of computer peripherals and computer systems now this process is done every few months to actually take care of the varying rates of this computer items and the vendors can bid at different points of times and then the best competitive vendor is chosen for a given computer system and then that is made available to all the faculty so that they can use that rate contract system to actually obtain their necessary computer items without going through any other purchase procedure and this is done often to actually allow the systems which keep changing in terms of their configuration to be brought in to the rate contract system now one of the requirements here is the vendor should be able to quote at regular intervals into for iit for various computer system configuration they should also be able to see what iit ‘ s requirement is and once these things are chosen  the systems are chosen then they should be made available to the faculty so that they are able to actually choose whatever systems they need and are able to generate the required papers for purchasing those systems now when we initially had a paper based system  all the vendors used to quote  supply their quotes on paper and then they used to be personal in our purchase department who use to enter all the things into a word processing system like microsoft word providing a comparative statement of different vendors for this computer configurations and so it ’ s basically file systems that we are using but then lot of processing  manual processing of entering these data in that which is usually other problem because this involves a tds  a typing of the details provided by the vendors and then circulation of this papers to various members so that they can look at the details and then choose the systems that are most competitive now the problem in this particular case as we can understand is that which used to take nearly 3 to 4 weeks minimum for the entire process starting from the vendors quoting to the selection it used to take 3 weeks to 4 weeks and used to involve lot of manual processing by various people and also circulation of large quantity of papers containing this data two various members involved in choosing these things so obviously this is a very good case where one can think of how this whole processing can be improved as a business case and see how one can bring in an automated processing by choosing databases as an example of introducing data base systems for automating the entire business process or work flow from one end to the other end to just show what are the likely business processes or work flow elements that will be involved in this i will just take you to a site which actually gives a first cut requirement of what was developed to show how this whole thing can be sort of automated in using the database concepts in a nice way  refer slide time  00  22  53  so this actually gives an initial flow of how we could probably introduce the database systems for automating this business process and one of the key things as i was explaining was to actually search through these rate contract details which were approved which could be used by various faculty members in choosing their requirements now as given earlier now i can actually quote through this which actually says that you can search through this rate contract details let us understand this one business process of how exactly one can search through this rate contract  refer slide time  00  23  33  this actually take you to the various other menus that we have  for example one can search through computer system configuration or add on parts or space for computer systems or printers and scanners and note book computers so if i go into computer system configuration then i can search through this computer system configuration  refer slide time  00  23  55  now i can search for example iit specification number wise where each specification will give the different specifications for the computer systems or you can have the search through vendor name or through brand name or processor name or a combination of this for an example let us say i actually click on specification number wise and submit this then i can actually see all the specifications iit has in this particular case there are ten specifications in this case let us say i actually choose the fourth specification and submit that specification  refer slide time  00  24  42  then now i can see all the vendors who actually quoted for this specification right and their prices  refer slide time  24  47  for example i can see here there is a company infotech limited which quoted for the specification 4 at 25000 rupees for g 33 chipset for p 33 p 35 chipset actually it is available at 25500 so what this gives is actually all the vendors and whoever has quoted for the system and they prove to be in the rate contract and what are available in this rate contract can be searched by using this one can in fact do a little  better search by saying for example if you want to actually take a specific processor or a cpu clock speed and also a vendor name then you can actually submit this quote here  refer slide time  00  25  50  now you can say a canonical electronics private limited is a vendor name and i want clock speed 2.8 and i want to see all specifications matching these two requirements then i will basically submit this i get one specification matching this requirement which says that this is the available system for me in the rate contract which i probably put order if that satisfies my requirement now to understand what is happening here as you can see here we need to get this information organized in the back end database and have this information stored properly into our database system  refer slide time  00  26  02  now this is where actually the data model becomes extremely important  what is the data model and how do we store data into that system and how do we access that data as we need it  refer slide time  26  43  for example in this particular case you can see that i am actually accessing the data by actually saying that i actually need to retrieve the data which matches a particular specification number and a particular vendor number so i am actually going into the data and trying to match the required values and retrieving the data as they match that particular value that i have supplied it ’ s quite possible that i actually navigate through data which means that i actually retrieve at one level for example you can see when actually go with specification number i am actually retrieving the data at one level now we can see these are all the data that i have as for a specification number is concerned now i can say for 5 is what i am actually choosing which means that i am actually drilling down  i am navigating using into the database by saying for 5 now i need the information so i get to the second level by saying please get the information on 5 for me this is what we understand as a navigational query where you are navigating through the data  has a post to actually retrieving all the information that satisfies a particular criterion for example one can say i want to find out all the students who have registered my database systems course or my paradigm programming course or my grid computing course or i want to find out for example a further intersection of the sets by saying i want to get all those students who registered for my paradigm programming course and also my course on grid computing  refer slide time  00  27  54  so in that sense we have various ways we retrieve this data and hence the data model becomes extremely important  what is the kind of a data model we have becomes extremely important and also another interesting fact that we should see when we see data models is that here is a case where the data is well organized as a table for example i have an iit serial number and have an iit specification  i have a chipset  have a company name unit price  unit price without monitor  category thing like this this is more like a table organization of the data now lot of information if you see for our business purposes gets very easily organized as flat tables of this nature and that is one kind of a data model that is extensively used while storing the data their other data models that are possible for example you could have data models where you have an object where the information could be unstructured which could mean that you have a field name and you have a corresponding value but then one field in that particular object could actually point to another object and that could point to another object when example for such a kind of data model or real world data model is for an example  if you want to store the information about maruti 800  what are all the parts maruti 800 has  it fits more as a object data model because you are going to specify now what kind of engine maruti 800 has and within that what kind of other parts that engine has so you are going to actually store the information more in terms of objects and the objects linking each other and that becomes an object data model where you have a flat model like this  flat table base model is often also called the relational model and as you go into this course you will see in detail how the relational model can be used for storing data and how the relational database system can be used for developing applications that fit very well into the relational model what i have shown here is a simple example of using the relational model for building this computer rate contract system i will further go down and show how the relational model fits in here in terms of developing the application one of the important things that we should note down here is for example this also enables the vendors to come in and give information of what they have directly once i have enabled this database system  refer slide time  00  31  41  that can be shown here for example by saying that a vendor who wants to quote to me can login here for example you can see here it allows the vendor to login here and in this particular case i can give a guest name  guest is the login and then with the password you can login now which shows that now he is allowed to actually enter my system and he can quote to my various requirements  refer slide time  00  32  06  for example in this particular case i have a computer system configuration now for the computer system configuration there are different kinds of specifications which i need  refer slide time  32  24   refer slide time  00  32  38  so he can say that he wants to quote for the specification one here  he can also click here to see the specifications that i have at the moment for the computer systems so this gives the list of specifications that he has for the computer systems  refer slide time  32  59  now from this he can choose for which specification he wants to quote for the computer system configuration once he decides his specification for which he is going to quote  you can basically allow him to quote here by saying that now he can give this information  in fact he can say that this particular specification one he wants to quote it let us say at 25000 then you can say he can supply that information here unit price and he can say the unit price without monitor could be 18000 in this case assuming that he has some calculation for the monitor  refer slide time  00  33  17  so he basically can give this information and also put some comments saying that extra something here if he wants to say that the additional component  cost component that he is going to use in this once he is decided to quote for this  he can say that this is the information that i can give it to you as for a specification one is concerned then he can say he is actually quoting for this which means that this information gets into my database now  refer slide time  00  34  19  i will show you in a minute where exactly this gets in and gets stored for me and that ’ s where basically the entire relational database system concept makes sense once he is doing this update  this goes in and get stored in an appropriate table in my database system and i will be able to retrieve or use this information later to make my decisions or other business processes or transaction to access this particular data  refer slide time  00  34  58  now you can say that this confirms saying that you have logged in as a guest here and you have quoted from my computer system and the quotation is registered into the database this is extremely useful for various reasons because now the vendor doesn ’ t need to actually come to iit madras he can be anywhere  he can be situated in delhi  he can be in chennai  different parts of chennai  all that he needs to do is he need to registered with me so that he has a id and the password once he has that he just logs into my system and able to quote into my system and once he quotes that quote is actually stored into my database system and other processes and transaction in my database system will be able to actually access and do the necessary processing here now let us understand what are the key challenges are conceptual thing that we should be understanding in building this now for a minute let us understand this application  what we have as different entities in these applications and how do actually store this data or create my tables and my data and then how do actually make the transactions work with this particular data that i have underneath now the most important requirements in this particular case is to actually see that underneath i have actually created a database system in this particular case i am using a mysql relational database system which actually allows me to actually create tables and store the database underneath server for me  refer slide time  00  36  36  now what are the different ways actually i can now put my data into this system ? for example here is the key database system that i am using for actually creating this rate contract system that is called rate contract rc-april ¬ ¬ -6 06 and this has actually got all these tables as you can see i have actually got several tables underneath which are all storing the information for me for example one can see that there is an approved quotations for add on systems  this is one table then i have the approved quotation for computer that is another table and approved quotations for note books and approved quotation for printers now we can see these are all the tables in some sense they form the schema of the database and this is very critical in database design which you are going to study as part of your course here how do i design my database system ? the starting point for design of the database systems is actually understanding the different entities that are there in your domain for example in this particular case there are vendors  there are users  there are people who select based on the what the vendors have quoted so you have actually got different entities and they have some relationship with each other for example vendors quote into your data and then there are people who actually pick information from this and choose so actually you have to understand what are the key entities in your domain and how exactly each entity in terms of what attributes these entities have for example one of the things that you can see here which is one of the tables for example here shown here is the quotations for computers which is something that i have used here  this is one of the tables now this table has different fields  these are entities for example if you see quotation computers has some kind of a table which comes from my entity which we will see in a minute what is the relationship and how do i actually break an entity in to a table now in this particular case you can see serial number  chipset  vendor name these are all the attributes or the fields that i have for this particular table if you take for example vendor name and they all are registered with iit madras then vendors will have a user name and a password and then their address these are all the attributes that a vendor entity will have and when actually i have this vendor as an entity  one way i could possibly create this information in to my database is by actually converting this entity into a table for example in this particular case as you can see a simple thing like users for example has  to the users of the system and they have the information on company user name and password being available in this particular table so if you see user as an entity  these are the fields that will be available as part of that entity so one easy thing when you are designing your database system is to actually convert the entities into tables  relational tables and store this information now there are lots of issues and in terms of converting an entity in to a table because if this information is duplicated for example this company name and id is duplicated in other places for example it is available in other places  the company name is available in other tables then what happens is this duplication of data will create problems when you actually  a company says that now it has changed its name then you have to go and then start changing this name at multiple locations so one of the key things that we understand  when we are designing database is that one fact at one location so you don ’ t want to store the same piece of fact at multiple locations because it causes lot of problems for you so database design concepts will explain how exactly this process of design could be done and how one can come up given an entity relationship diagram  how you can come up with tables like this for example the database design in this particular case has all these 23 tables and this is the essential part when i design my system and this is the key of the whole concept and how do i actually arrive at these 23 tables and each table for example has its own for example in the case of quotations  you can see this is the schema that i have you can see the table here has serial number  chipset  vendor name  unit price  bulk price bulk two prices for example if they quoting for more then some number of systems whether there is a discount additional with monitor  without monitor all these stuff and then whether the monitor is what kind of monitor brand that they are supplying with me and what are the comments for example  whether he wants vat to be added or its included and all this so this is what we understand as the database design now these schemas also changed for example tomorrow i don ’ t want this bulk two price to be quoted because it is no longer my requirement then i drop this particular field from my quotation for computer which means that i no longer need this particular field to be available this is what we understand as schema change in my system  in my database and often these schemas change because as the systems evolve  business processes evolved and systems evolved schema changes are natural and your system should be able to cope up with this schema changes that ’ s other important issue when we actually deal with databases now in this particular case we will go and see the quotations for computers and see what we have done earlier in terms of these quotations whether that is available in this particular case as you can see here i am using the php admin and directly getting into the database for example in the particular case  i can go and then browse for example the values of the database  refer slide time  00  44  30  now you can see that these are the values that are available for this particular field so i can also probably browse the entire database  so you can see that this information that ’ s available in this particular table can be  you can use the browse and you can see what is the information available for example quotations for computers you can see now here  you have dev systems which quoted intel g 33 chipset for this unit price  this is a bulk price they didn ’ t quote bulk two price then it ’ s a category is its a local vendor and the monitor is the viewsonic and then a some comments for people to look at for both viewsonic or acer tft monitors  you probably would like to supply both of them so you can see this this is what we have as the information right now one can even see how this data gets changed when something got updated for example you can go to the  as we saw earlier you can go to the computer system and once you have quoted for that  you probably can see based on here what i had given earlier as a guest has come up now here  refer slide time  00  45  58  you can see that i have inserted earlier as a guest and quoted 25000 and 18000 as the unit price and without monitor i have quoted 18000 so that information is reflected here as you can see here and the default samtel has been picked up and i said vat is extra and that is also available for me here now as far as the work flow is concerned  i can use this information whatever people have quoted and pick whatever quotations that i want  put them into the approved quotations for examples for computers this will basically reflect the approved quotations for the computers now you can see all the approved data is available in this particular case and then this is the approved systems for computer systems and that ’ s how the information can be organized in terms of your database tables i can pick the informations from one table and then you know it can be added on to the other table all this is what basically are called transactions so you are going to look at in this course in detail how transactions operate on the database system on the underlining data and change the state of the data as it goes for example each one of this can be seen to be a transaction for example  one can think of which are the things that are actually transactions in this particular case i am searching through the computer configuration as i have actually shown you earlier now in this particular case i am actually the first level search is not a transaction because it is just giving me all the fields that i can use for searching now you can see once i come here  this is the point where i choose a specification and once i choose a specification or a vendor name for example i am actually trying to make a query into the database it is at this stage actually it goes and picks up this data from the database table that i have underneath for example these are all the vendors that i have in my system and now once i choose one more vendor here and i say  let us say i am actually choosing the one of the vendors that is available here and then submit that thing it again goes and reads the data from the database these are actually called the read only transactions because all that they are doing is they are just going into the database and reading the information that i have and then giving me in a convenient form where i can look at it through my browser so lot of thing will be happening underneath but then ultimately all that it is required here is to go  carry the database  get the required information and show this information for somebody who is asking for that information  refer slide time  00  48  26  so these are actually read only transaction  they are just reading the values that are stored in the database  refer slide time  00  48  53  but as opposed to this kind of transactions  one can even think of the other kind of transactions which we saw here we actually said that one can login as a guest and once he logs in as a guest now  you can sort of quote for the computer system now once he says that for a particular system  3 is quoting now and then he give some data here saying that please take my quote as let us say 12000 and then give the other thing as 8000 then i think what we are doing in this process is  we are actually updating our underneath database once he says quoted what is happening is you will be seeing that the whole quote has been registered into the database which means that the data has been now been written in to the database so these are actually write transaction  they are writing in to my database there are some transactions which can go and both do both read as well as write from my database to summarize what ’ s happening here  what we are trying to do is given an application like this  we have been able to design the underneath tables which are the database models and store the data in that particular model and have our transactions which are actually accessing this data and supporting various business processes that i have in this particular case searching through my rate contract  allowing my vendors to quote into the database and allowing my faculty to choose and then order the related computer systems so the entire business processing of the data is automated using an underlining database system and hence database systems become extremely critical in terms of supporting automating this processes and eliminating manual processing and manual production of the papers so that the whole work flow gets completely automated and we have a simple and elegant and nice way of actually handling our business processing systems now what you are going to see in this course as part of this course is you are going to see how this database design can be done  what is the meaning of transactions and how they operate and how do they actually maintain the consistency of the underlining data and provides certain properties when we develop these systems and make it available for day to day use for the people so these issues are going to be covered in depth in the next 41 lectures that we have in this course thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 1a conceptual design hello in today session we would be looking at an important concept in the design of databases namely what is called as the conceptual design of databases using model called the entity relationship or the er model  refer slide time  00  01  24  now you might have heard somewhere that databases today are based around the relational model where the fundamental aspect of the building block of a database are tables so where does this er model come in to the picture and what is its relevants and why do we need something like an er modeling for databases ? so in order to answer that question let us begin with looking at a small description of what a typical database design process looks like this is just a simplified representation of what a database design process looks like  refer slide time  00  01  57  however this captures the gist of most database design process whenever we talk about databases  we should always remember that databases are always embedded within some kind of an application context and this is called the information system within which the database is embedded so databases could be embedded within for example for a banking solution  the databases could be embedded within railway reservation solution  there could be embedded within a university setup  a company whatever now whatever this setup is  this context or the information system context is what is called as a uod or the universe of discourse the universe of discourse basically tells us what are the building blocks that make up this context within which this database is going to be run now generally when we talk about designing a database system  we first talk about looking in to the uod and collecting requirements from the uod we have to make an analysis of the uod saying for example  suppose you want to build databases for a bank  we have to first identify or we have to first understand how does the bank conduct its operations  what kinds of operations do they have and what are their rules  what are their norms and what identifies a correct operations from an incorrect operation and so on and that is going to give us two kinds of requirements the first is what is called as database requirements or you might also term it as data requirements and the second is what is called a functional requirement now as you might have imagined that database requirements are handled by dbms or the database management system however a dbms by itself is not a complete solution to give you an analogy dbms is something like an engine now we can ’ t just have an engine  we have to build a car around it or we have to build a bus or whatever it is around it so this layering around the database system is what is called as the application programming or the information system so the functional requirements go to design in the application program or the larger information system with in which the database is going to be  refer slide time  00  04  24  now continuing with this process  we take the database requirements and come out with a first level or what we called as a high-level conceptual design we have to first understand how this database requirements look like  what are the different data elements and how are they related ? we need to first understand in a way or rather represent it in a way that is understandable by human beings rather than computers we have to first understand the problems first before we give it to the computer so database requirements go into what is called as the conceptual design and the output of the conceptual design is what is called as the conceptual schema now this conceptual schema is what we are going to be taking up today in more detail but going on with the process itself  let us see what happens with the functional requirements so functional requirements also go through a similar process where you do functional analysis  you analyze what are the different functions if it is a bank  what are the different functions ? opening an account is a function or making a bank transfer is a function now what are the constraints that holdup these functions what is a correct function  what is an incorrect function and so on so once these functional analyses are made  we get what are called as high-level transaction specifications in fact we will be revisiting this topic called transactions as a separate module where we will see that many times  we need to club different operations of the database into one logical unit or what we called as a transaction now transactions are what are going are the building blocks of the functional requirements or the functional analysis of the information system now you can also see down here that all of these  until now all of these are dbms independent process so what is meant by dbms independent process ? it means that doesn ’ t matter whether you are using an oracle database or an ibm db 2 database or a mysql or postgres or whatever it is you will have to look at the uod  you will have to understand your uod  you have to come out with a conceptual design which has in no way relevant to the actual implementation which the dbms does and you have to do a functional analysis which is completely independent of the dbms that we are using next comes the dbms dependent processes so we ended up with the conceptual schema previously in the database stream you can say that there are two different streams of a processes happening one is the database stream and the other is the functional stream or the application stream  refer slide time  00  06  50  so in the database stream  we had ended up with the conceptual schema now the conceptual schema in turn should be transferred in to what is called as a physical design now what is physical design ? the physical design actually tells you how the data itself resides on the database  on the computer so obviously this is a dbms dependent process so the way  even though there are many standards that exists now the way that let us say oracle stores its data would be different from the way db 2 stores its data even though there due to some standards there are going to be slight variations so this physical design results in what is called as an internal schema or the schema which actually should goes into the dbms and similarly the high-level transactions specifications that we are talked about are also going to impact the physical design of the database i mean suppose certain kinds of operation have to be performed together it makes sense to design them near to one another so to say let me just use the word in slightly lose fashion here to one another so that it becomes easier to handle transactions on so you get inputs both from the conceptual schema and from the high-level transaction specs in order to make your physical design of the database similarly the application stream goes off into application program design and transaction implementation  finally the application software so under  from all this now we are going concern ourselves in this session with just one aspect of this entire process that we looked at namely the conceptual schema now what is a conceptual schema and why is it important for database design ?  refer slide time  00  09  06  so let us have some observations about what is a conceptual schema we just now saw that the conceptual schema is an outcome of the high-level conceptual design of a database so this is one of the first things we are going to do we are going to understand our uod or the universe of discourse it tells us what kinds of data that the uod is going to handle and based on this  we are going to come out with a high-level design so it ’ s a very concise description of the data requirements  what kinds of requirements do users need and how are the data related between one another and so on again let me take a brief example suppose you are building banking solution one of the first things that you are going to note is that in a bank there should be accounts  there should be customers  there should be customers or account holders or whatever you call them and there should be of course monitory transactions and there should be some kinds of logs and ledgers and account books and so on so all of these represent some kinds of data elements and a very high-level conceptual design would identify each of these data elements and say this is how they are related so it includes the description of what you called as entity types these are the entities that makeup the uod in a bank the customer is an entity  an account is an entity and a ledger is an entity similarly in any other uods  there would be several different entities like that whether it is a railway reservation system or a university or whatever  they ’ re going to be different kinds of entity types and there are relationships across the entity types so customer holds an account  an account transfers to another account and so on and so far so there are relationships between different entity types and there may also be certain constraints that are imposed either by physics or by the uod itself in a bank for example one might say that in a sbi account  balance may not go below 50 rupees or whatever that ’ s a constraints that is imposed by the uod that says you can not have less than 50 rupees in your account in order to make a term  in order to make a term working on the other hand there could be other constraints that are imposed by physics itself  the physical reality for example if you are modeling a railway reservation  you can ’ t obviously have two trains starting at the same time from the same place  from the same platform so it is simply because those two can not exit at the same time so there are different kinds of constraints that exist among data elements and this is what we are going to capture during the conceptual design and another thing to note here is a conceptual design has no implementation details here note that it earlier we had noted that a conceptual design is a dbms independent process so that means we are not really worried here about how this conceptual schema is going to be implemented within a database systems and the main idea of a conceptual schema is ease of understanding we should be able to say to your end user this is what we have understood from your description  this is what we have understood of your needs and your end user should be able to understand your understanding so to say so he should be able to understand your understanding  even though the end user may be a non-technical user so the main idea for behind the conceptual schema is ease of understanding and it is used mainly for communication with end users who are of non-technical nature  refer slide time  00  13  08  so here is a small example an example  company database so suppose we look at a company  what are some of the first things that we can identify first some of the first things we identify are a company of course has employees  we can see them then company has departments  each employee works in different departments and so on then once we start speaking to employees  we find that there are also different projects and each department has some one or more projects and a project may span different departments and so on and once we start to the hr  talk to the hr department  we note that each employee also has some dependence and there are also covered in some way or supported in some way from the company and so on now if you look at this slide carefully  here we see that there are two different kinds of things that we are noting here all these things before the colon  we see the things like departments  projects  employee  dependence and so on and after the colon you have a terms like name  department id  manager or name  project id  location or name  pan number  address and so on so what this says is that all this names before the colon like departments  projects are entity types they are different entities say department is an entity  project is an entity  employee is an entity and so on but once after the colon like name or manager or location and so on are what are called as attributes of this entity that means they belong to this entity for example the name occurring after department here belongs to this department  the department id belongs to this department similarly the project id belongs to project and address belongs to employee  date of birth belongs to employee and so on so it not only belongs to employee  it also tells us or it describes something about the employee so if there is an employee  an employee has a name  an employee has a pan number  if he is paying income taxes regularly that is he has an address  he has a salary  he has well  he or she has a gender and date of birth and so on so all of these attributes define what is an entity type and what are the characteristics of that entity type  refer slide time  00  15  35  so let us formulate things a little bit now in the er model or the entity relationship model  we have standards notation of representing entity types and their attributes as you can see in the slide here  entity types are represented by rectangles so department is an entity and attributes of the department like department name  department identifier  manager of the department and so on are represented in the form of ovals or something like balloons hanging over this entity types so there are some definition here so if you review this  an entity represents an object of the real world that has an independent existence note that term independent existence that is you should be able to identify an entity independent of every other entity in the system address is not an entity because address belongs to some entity called employee or department or whatever however the employee itself is an entity because it is an independently occurring object in the uod and an entity has attributes or which are properties that describe the characteristic of this entity  refer slide time  00  16  53  now what are the kinds of attributes or what are the different kinds of attributes that could exist ? we saw name and age and date of birth and so on but can we classify this attributes into different kinds of attributes and it can tell us something about characteristic of the entities so there are several different classifications of attributes  we are going to look at small number of such classification one of the first classifications of attributes is simple versus composite so what is a simple attribute ? a simple attribute is let us say something like age you are 30 years old  30 age equal to 30 so that ’ s a simple attribute that is just one value associated with an attribute so when you say age  you get back one value called 30 on the other hand suppose you say name  in some countries we say name is name  you just talk about one name for an employee but in several different places when you talk about name  you have to specify a first name and middle name and last name and title and nick names or whatever so so basically a name is composed of several other attributes within it  so that is what is called as a composite attribute versus a simple attribute which has just one value and there could be a something like a single-valued attribute or a multi-valued attribute now what is a single-valued attribute ? again age is a single-valued attribute at any point in time at least you have just one age that is at this point in time let us say you are 30 years old or 35 years old or whatever on the other hand at any given point in time  an attribute may actually have multiple values now take the color of a peacock for example no matter when you are going to measure this  no matter at what time you are going to take a snap shot of a peacock  you are always going to find many colors so this is a multi-valued attribute  that is the attribute itself is defined by multiple values how is this different from a composite attribute ? let me pass here for a little while so that you can ask this question to yourself again how is a multi-valued attribute different from a composite attribute ? now composite attribute if you saw  if you notice closely is made up of different sub attributes for example name is made up of first name  middle name  last name and so on and initials for example or title and so on all of these need not be of the same type i can say title can be either mister  doctor  misses  whatever and so on only these three types on the other hand name could be anything  any string and so on on the other hand when we look at multi-valued attributes  all the different values that it takes  color of a peacock for example it might have different attributes  many attributes but all of them denote color blue  green  red  white  whatever all of them are color  all of them are of the same types so multi-valued attribute are different values of the same attribute type but a composite attribute is a collection of different smaller attributes in itself  refer slide time  00  20  19  and then there  what are called as stored attributes versus derived attributes now stored attributes is something which you just store in the database  your date of birth for example so when you ask what is your date of birth  you just give some date and then store it in the database however suppose you were to ask what is the age of an employee now suppose you know today 's date and you know the date of birth of the employee  you can always derive the age of the employee so age in a sense is a derived attribute  a good database design would put age as a derived attribute because a database hopefully is going to be used for a long time it ’ s not just today  today an employee might be 30 years old but 5 years later he still won ’ t be 30 years old  he would be 35 years old so it ’ s always best to have age as a derived attribute because you always know what is the current date and you always know the stored attribute called the date of the birth similarly there are what are called as null attributes or null values for attributes sometimes an attribute may not have any associated value for a particular case and let me give an example suppose a student can take at most two elective courses in a semester and at least one elective course and at most two elective courses now let us say there is a student who has taken just one elective course now because he can take at most two elective courses there are four different  there are two different slots  elective one and elective two now let us say after  at the end of the course is give a grade for each of the courses  that each of the courses he has taken up now he gets a grade for the first course but he doesn ’ t get a grade for the second course because he didn ’ t take the second course at all  elective two has no grade but this grade is not we can ’ t say that he has scored zero in elective two so a null value is different from zero or unknown or missing value this is not a missing value this is not a  this not zero or this is not an unknown value  this is a null value that means that something like when you say n a  when you write in application forms this is a null attribute so let us come back to entities and define a few more terms here with respect to entity types so we saw that an entity type  i have been using the word entity type but have never really formally defined this term so an entity type is a definition or defines a collection of different entities of the same type  refer slide time  00  22  47  for example in this slide  we give this example of department so a company could have different departments but when you say an entity type department it means that it represents all departments in the company so it is a definition of a real world object type so it ’ s not this department or that department  it is just department it is a type that the entity is going to define now any specific collection of entities of a particular type is what is called as an entity set for example if you take supplies departments plus accounting department now suppose we take both of them in some collection we call this as an entity set  these are two different departments of the same entity type so we also say that an entity type as opposed to an entity set so an entity type is set to describe the schema or the intension of an entity set what is that mean ? it basically says that how should the structure of this entity or entities of this type look like so suppose we say that department should contain a department id  it should have an address  it should have a manager  it should have several other attributes that means that no matter whether it is the supplies department or the accounting department or the systems programming department or whichever department it is  they should be defined by these attributes that is they should have a department identifier  they should have a clearly defined address  they should have one % designated as manager and so on so an entity type describes what is called as an intension or the schema for an entity set or different entities of the same type so here are again a review of the representations entities types are defined by boxes and attributes are defined by ovals and different kinds of attributes have different kinds of ovals may be its not completely visible here the multi-valued attributes are defined by an oval having a double line  refer slide time  00  25  03  and similarly derived attributes are those which can be derived from other attributes are defined by a dotted line instead of a solid line  refer slide time  00  25  38  and we now come to another key issue when we are talking about entity types and namely this is about key attributes now what is a key attribute ? now if you notice carefully when you said that each department has to have a department identifier and manager and address and so on now why do we need a department identifier for each department ? this is to be able to identify each department uniquely now if i say department number 10  it means just one department no matter what its name is department name could be ambiguous  it could mean supplies and accounts and suppose we might have a department called administration  it might mean both supplies and accounts or whatever so a name could be ambiguous but when we say an id or an identifier  it ’ s a something like what you have a pin code in post letters it uniquely identifies each entity of this entity set so the department id attribute is unique for each department that belonging to this entity set so such attributes which can uniquely identify entities are what are called as key attributes or keys so the key attribute as you can see here is specified by an underlined definition so the department id is underlined here to denote that it is a key attribute  refer slide time  00  27  14  now in some cases it need not always be just one and that is the key attribute there could be more than one attributes which are key attributes in which case all attributes which form part of the key are all showed underlined now what is meant by a key attribute ? so an attribute or a set of attributes that uniquely identify entities in an entity set so is what is called as a key attribute ? now like i said it need not be just one attribute that is a key attribute  there could be more than one attributes which is a key so in which case the key attribute is called a composite key so the key is made by combining two or more attributes together a composite key should be minimal what is it mean to say a composite key is minimal let me first give you the definition of what is mean by minimal and then we can see an example so no subset of a composite key should be a key itself for example let me take the attributes department id and department name is it key attribute ? of course it ’ s a key attribute because if i take department id  department number 10 supplies  it uniquely identifies one particular department however the second attribute here  the name called supplies is redundant we don ’ t need to have this attribute here in order to uniquely identify a department it ’ s sufficient enough if you say department number 10 so that ’ s what is meant by a composite key that is department id and name is not a composite key because it is not minimal in nature no subset of a key attribute should be a key attribute in itself in which case this is not so so  key attributes are shown underlined in the er diagram  refer slide time  00  29  14  there are also certain other properties of keys which we are going to revisit again when we are going to look into the enhanced er model what is called as the retention of key attributes in all extensions ? so let me not take this property right now and we will come back to this again that the key attribute should be retained for all extensions of a particular entity type now it may so happen that a key attribute should uniquely identify an element but an element need not have just one key attribute of course i can have two keys to my house  key from the front door and the key from the back door now it is fine but the only thing is this key should open only my house door not somebody else ’ s house door so that ’ s the idea that is a key should uniquely identify a house but a house could be identified by more than one key  you could either enter through the front door or the back door similarly i can have a let us say on a computer network  you can uniquely identify a machine by its ip address or so to say an ethernet address on a lan and so on but a machine could have more than one ethernet cards in which case both of them uniquely identify the machine so there is no restriction on how many keys that you are going to have but usually we are going to use one key  so what is called as a default key usually we are going to enter through the front door not always through the back door so usually we are going to use just one key but you may have more than one keys that define a particular entity type and there could be some entity types which have no key attributes we are going to take a look at one such example little later on and it may not be possible to define any kind of a key attribute for such entity types and such entity types are called weak entity types or entities of such type are called weak entities so they have to what we called as totally participate in some relationship in order to define themselves now we come to the next definition of  definition in the er model what is called as a domain  what is meant by domain of attributes now a domain to put it in a very informal fashion is going to show you the space in which an attribute is going to be defined  refer slide time  00  31  25  for example if i say age of an employee and there are some set of rules that you can not have employees lesser than 18 years and greater than 65 years the domain is basically a number between 18 and 65 obviously it can be  there are also some physical limitations in the sense that the domain of an age can never be negative so that ’ s the nature imposed restriction but there could be other restrictions and which basically defines the space within which any value of this attribute can reside now if a composite attribute has different sub attributes or like we saw name has different sub attributes first name  middle name  last name and so on and each of them have the room domains let us say d ¬ ¬ ¬ ¬ 1 d2 dn ¬  so we see that the domain of the composite attribute is cartesian product of the domains of individual attributes so the first attribute can take a value between 18 and 65 and the second attribute can take a value between 0 and 10 and so on so the domain of the entire attribute is just a cartesian product of all of these terms  refer slide time  00  33  01  so let us briefly come back to this company example and see likewise whatever we have seen until now so we can think of different kinds of entity types like department  project  employee and so on and there are different kinds of attributes for each kind  each of these entity types and there are certain key attributes here which are underlined and then there are certain composite attributes which are shown like this that is name is composed of first name  middle name  last name and so on and like that and several other entity types that we can identify during our analysis the next concept that important concept that we are going to be looking at in er modeling is the notion of relationships like the modeling itself says it ’ s about entities and relationships so we just until now we looked into what are called as entity types so we defined as an entity type  an entity set  attributes  different kinds of attributes and keys and domains and so on now let us see how can we relate entities of different types so a relationship are to be precise relationship type defines a relationship between two or more entity types so the slide here shows a relationship type between an entity type department and an entity type employee and it says managed by  refer slide time  34  35  so it basically says a relationship type specifies that any department or any entity of type department should be managed by some employee or some entity of type employee  refer slide time  00  34  54  so let us go further and make some definitions first of all  let us first defined what is meant by relationship type so relationship type r is an association among different entity types  it need not be just two entities it can be any number of entities so there could be n number of entities and defines  it basically defines a set of associations  what is called as a relationship type so if you look at it a little bit carefully  a relationship type is just a subset of the cartesian product of all of the entity types so that means an entity instance or entity or entity of this type could be related to some entity of this type and some other entity of the next type and some other of the next type and so on and all of this define one instance of this relationship  refer slide time  00  35  49  so let us dwell deeper into what are meant by relationships and what are some of the characteristics that define relationships firstly  the notion of the degree of a relationship type so what is a degree of a relationship type ? it ’ s simply the number of entities or that are involved in this relation  it is how many  between how many relations is this relationship going to establish an association we saw earlier a relationship type of degree two that is a department is managed by an employee so there are two entities that participate in this managed by relationship such kinds of relationships are called binary relationship  binary basically because there are two entity types involved similarly there could be unary relationships  there could be ternary relationships and so on and there could be n array relationships  n different types of relationships  refer slide time  00  36  49  now we come to kind of a tricky problem here now have a look at this slide carefully the first picture above shows the relationship type  it shows employee works for department and the second relationship and the second picture below shows employee as an entity and department in which the employee works in as an attribute of employee so which is correct ? is department an attribute of employee that is does the department in which an employee works in describe the characteristics of an employee or is it that departments and employees have separate existences or separate entities and there is a relationship type between them so let me confuse this problem even further  take a look at this slide here so in the earlier slide  you could have probably said that no department can not be an attribute because department has an independent existence we already saw that each entity type should have an independent existence so because departments have an independent existence they ca n't be an attribute  refer slide time  00  37  50  but have a look at this slide here the first picture shows employee works for department as a relation and then department has a department id as one of its attributes now i am going to take this department id attribute and put it in to employee and say employee works in this department  so this employee is associated with this department id so now which is correct or which is wrong ? so is the first one correct or the second correct ? the answer to this obviously is it depends on the particular situation in which you are looking into now the first one where we show that employee works for department as a separate relation depicts the relationship between employee and department and the second one just depicts what are all the different characteristic that describe or what are the different attributes that describe the characteristics of employee that means if i am talking about an employee entity and if i say that an employee is or in some sense an employee is very closely tighed to his department so he has no identity without his department id in which case you have to say department id is a part of the attribute or is one of the attributes of the employee entity type  refer slide time  00  39  46  so relationships versus attributes  it ’ s a slightly a tricky problem always  so in order to determine whether a particular thing is actually a relationship or it is an attributes and in some kinds of data models like what are called as functional data models or in object databases  relationships are almost always refer to as in the form of attributes for example in object databases  you talk about relationship by storing an object reference let us say you have a relationship between employee and department  so there is a reference to a department object within the entity object and vise versa and a reference to an employee object in the department object and so on similarly in relational databases which we are going to study much deeper  we see that relationships are established by what are called as foreign keys that is there is one table that describes one kind of an entity and a relationship between this table and the other table is described by a foreign key attribute which says that this entity of this type is related to some other entity of the other type now we come to another important issue in the when you are talking about relationship  this is the notion of constraints  refer slide time  00  40  59  so when we talk about relationships almost always relationships are defined by certain kinds of constraints take the examples of employee managing a department one of the most simplest constraints that we can think of is department should be managed by at most just one employee we ca n't have two mangers or two heads of a department  we should have a just one head of a department and you might also establish a constraint that one person can manage at most one department at a time in some cases there could be allowed to manage more than one department at the same time so that means in that case there is no constraints from employee to department but there is a constraint from department to employee so when you are talking about constraints and relationship types  we are mainly concerned with two kinds of constraints what you call as cardinality ratios and participation constraints so what are these cardinality ratios and participation constraints let ’ s have a look at them  refer slide time  00  42  15  so take a look at this relationship again here now i have replaced this managed by with another relationship called works for now works for is slightly more general than managed by in which ways it more general then managed by a department can have many employees working for it however sometimes we might have to mandate a requirement that an employees can work for only one department not for multiple departments so this is shown in one of two different ways either like this that is n  1 which says that n employees to one department or something like this  so which says it could be n employees working in one department and so on so on the other hand what happens if this were to be m  n something like 4  3 or 2  1 or 2  5 or something like that so that means that let us say 4  3 so that means that a department can have at most 4 employees and an employee can work in at most 3 departments and so on so basically you can represent a cardinality constraint or cardinality ratios in this case that says participations in this constraint is defined or constrained in this  by this cardinality or number of entity types the second kind of constraints that ’ s important is what are called as participation constraints what is a participation constraint ?  refer slide time  00  43  49  take a look at this slide here now this slide shows another relationship type which says department handles project now a department may handle several different projects suppose there is a restriction that a project has no existence unless it is associated with a department now suppose i have a project for developing some kind of software now this project does not exist if it is not associated with some department  if there is no department which is in charge of this project basically what this says is that if every project has to be associated with a department then the very existence of project depends on this relation so only if an instance of this relationship exists only then can a project entity type exist or such kinds of constraints are what are called as participation constraints they are defined by a double line here may be it ’ s not fully visible but it is a double line here so that means that the very existence of this project is dependent upon the existence of a relationship of this kind going on further in to participation constraints  we say that the entity type in this case project is said to totally participate in to this relationship because if it doesn ’ t participate it doesn ’ t exist anymore  refer slide time  00  45  16  so a participation constraint in which an entity type existence is dependent upon the existence of this entity type of this relationship type is called as total participation that is the entity type or participates completely or totally in this relationship we can also think about attributes for relationships and not for entities themselves  just like we saw attributes could be associated with entities we could also allocate attributes for relationships have a look at this figure here it shows department as an entity type and says that department handles certain projects and project totally participated in this relationship and then an attribute called budget is placed for this relationship so what is this mean ? it basically means that this budget or the budget that is specified here is allocated for this project associated with this department or is specifically allocated for this relationship type  refer slide time  00  45  50  so in cases where let us  the project is handled by more than one department that means that this budget is not allocated to the project this is allocated only for this project for working on this department and so on it is not allocated to the department as well that means the department can not use this budget for anything else ; it has to use it for this project only so this budget actually is a constraint on the relationship type  so it belongs neither to department nor to the project in its entirety however there is certain kind of relationships where we can actually move the attributes from the relationship type to one of the entities what are those kinds of relationships ? take a look at this slide here the first figure shows a 1  n relationship that is a project can be associated with at most one department and there is an attribute called budget that is allocated to this relationship  refer slide time  00  47  23  now if you see carefully  we do not do correctness if we move this budget attribute from the relationship to the project site so if i say that this budget is allocated to this project  it does not lose any semantics because a project can be associated with just one department in the previous slide  a project could be associated with m different departments that is it could be associated with many departments so here since project can be associated with only one department  it does not lose semantics if the budget is allocated to the project itself so one of the last things definitions that we are going to be looking at today are is what is called as a identifying relationships have a look at this slide here  this slide here shows a figure where an employee is identified by pan number so assuming that every employee is a tax payer and has received pan number from the government  an employee is uniquely identified by his pan number now let us say we prepare  the company prepare some kind of an insurance record for each employee  refer slide time  50  00  now let us say this entity called insurance record which contains certain parameters or whatever is an entity by itself because it has an independent existence you can see an insurance record and note that it ’ s an entity by itself however you see that an insurance record has no existence  has no meaning unless it is associated with somebody unless it is associated with some employee or some department you want or whatever so this is an example of an entity type which is a weak entity type that means it has no keys the key for the insurance record is of course the pan number itself or pan number of the employee so the key of the employee which defines an insurance record forms the key for the insurance record as well and the relationship that defines this  such a kind of association is what is called as an identifying relationship so this relationship here in this slide which is called as insurance details identifies insurance record with an employee it basically ties in insurance records with employee so that an insurance record also gets independent existence by themselves so as you can see insurance record totally participates in this relationship obviously it has to totally participate in this relationship but on the other hand not every total participation may mean an identifying relationship  we actually saw an example earlier where we saw two different entity types department and project in participation but which is not a weak entity type so  identifying relationships are again defined by double lines on the relationship types itself so let us briefly pursue a summary of the different notations we saw the first thing that we saw was an entity type and defined by a box and a weak entity type which is defined by a dotted line which is not clearly visible here  refer slide time  00  51  10  then relationships types are defined by rhombus and the identifying relationship types are defined by double lines  refer slide time  00  51  38  similarly attributes are defined in different ways that is normal attribute is an oval  a key attribute has an underline there and a multi-valued attribute has double line and a derived attribute has a dotted line so with this we come to the end of this session where we had a brief look at what is meant by conceptual design of a database so to briefly summarize that a conceptual design of a database is meant for non-technical users  it ’ s a high-level design and mainly composed of diagrammatic notations like entities and attributes and so on and within this diagrammatic notations  we saw that there are several different characteristic  there could be entities  entity type  entity sets attributes  multi-valued attributes and key attributes and relationships  an identifying relationships and weak entity types and so on so one of the first step that we do in database design is to be able to identify this entity types and relationships and to be able to build this er schema so that brings to the end of this first secession on conceptual modeling of database systems thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 2 conceptual design greetings to you all we have been talking about conceptual modeling of databases in previous session so in this session let us continue with this subject into some more detail and before we continue or before we start with today 's session  let us have a brief review of what we looked into conceptual modeling in the previous session if you remember we first talked about what is a typical database process or design and development process looks like we first start with analyzing the uod or the universe of discourse and the analysis of the uod reveals us two different kinds of requirements one is the data requirements and the second is the set of process or application requirements so the database runs with in an application context like i gave the metaphor in the previous session  a database is like an engine and the application context is the overall body or the car or bus or whatever that you build on around the engine  so both are important when we are trying to analyze the uod so coming to a database analysis or design of the database system  we first start with a high-level description of what the database should handle and this high-level description should not include any dbms specific terms or dbms specific issues it is mainly meant for the end users  its mainly meant for us to show the end user saying this is what we have analyzed or this is what we have understood by analyzing your universe of discourse and it is mainly meant as a conceptualization of what are all the data requirement that exist in this domain so one of the most popular models for performing this conceptualization or building a conceptual schema is what is called as the er diagram or the er model  the entity relationship model we saw that entity relationship modeling are made up of essentially two kinds of building blocks  entities and relationships so we looked at different nuances of entities and relationships and several issues that affect them like constraints and attributes and so on so let us briefly describe some notation  this is not exhaustive review but review of some of the main points what we looked at in the entity relationship diagram  refer slide time  00  03  46  an entity type is described by a rectangle like this  a simple rectangle and an entity type is something which represents a class of entities or objects that have an independent existence like a customer is an entity or a staff is an entity  account is an entity in a bank or in a company a department could be an entity  manager is an entity and so on any logical u-net that has an independent existence is called an entity and an entity type is an intension or some kind of a schema for a class of entities or for a set of entities there could be a set of different managers but all of them share the same attributes or the same properties of the entity type called manager similarly there could be several departments but all department share the same properties of the entity type called department and then entities are associated with attributes which describe the characteristic of the entity and we saw that attributes are in turn defined by the domains for example the age of an employee is defined by a domain that it should be greater than or equal to 18 years and less than or equal to 65 years typically so a domain represents a space within which an attribute lies and even within attributes we saw that there are several kinds of attributes  you could have a normal attribute  a simple attribute  an attributes which takes just one value or you could have a multi-valued attribute  we gave the example of the color of a peacock it doesn ’ t have one color  it has many colors and so it could have different values for the same attribute an attribute could be a composite attribute that is it could contain many sub attributes like first name  last name  middle name and so on it could be a derived attribute like the age of a person which can be derived  if you know the date of birth of the person and the present date so there are several different of attributes  so all entities of a given type have the same set of attributes now we also saw what are called as key attributes or what are called as keys i have not depicted this here in these diagrams but key is some set of attributes that can uniquely identify an entity within an entity type so if i have a set of employees and suppose employees are given an employee identification number  the employee identification number becomes the key if they don ’ t have an identification number  we usually have something like the income tax permanent account number called the pan number or something which forms the key  it uniquely identifies a person on the other hand we ca n't use something like name or age as the key two persons can have the same name and of course two or more people are very likely to have the same age so it can not uniquely identify a particular entity whereas something like the identification number identifies the entity uniquely so key attributes identifies this entity types uniquely and there need not be just one key attribute  there could be more than one key attributes and like i gave the example of i need not have just one key to my house  i can have two keys  one for the front door and one for the back door but we usually use just one of the keys which the default basis and sometimes use the other key frequent and then we talked about what is meant by a weak entity type a weak entity type is a kind of entity type which does not have a key attribute we also saw an example yesterday  the example of an employee and his or her insurance record so an employee has a key attribute  the employee number or pan number or whatever but the insurance record does not have any existence without a corresponding employee or without a corresponding person to be more general without being associated with a corresponding person now once an insurance record is associated with a particular person whatever way  whatever we use to identify the person becomes the key to identify the insurance record as well so for example identify an insurance record with a person having a particular pan number  i can as well identify the insurance record with the same pan number as i am identifying the person and we saw the second building block which are called relationships between attributes so a relationship basically ties in or brings in an association between two or more entity types so even within relationship there are several different kinds of relationships and we saw certain constraints that identify some kinds of relationships one of the first thing we saw was the cardinality constraints  a relationship which has a cardinality constraint says that  says how many entities of a particular type can participate in a relationship for example we saw that department managed by manager so it could be  there could be a constraint that a department may have just one manager or one head and one person may manage at most one department at a time so there is a cardinality constraint that exactly one  one department may be managed by exactly one manager on the other hand if i have something like works in  an employee works in department so it ’ s rather than a 1  1 relationship  it is a 1  n relationship one department may have n employees and there could be a constraint that an employee may be associated with just one department so that for n employees there are just one department so on and we also saw what is called as an identifying relationship an identifying relationship is the one  is a relationship that identifies a weak entity type with a strong entity type that is if i have an employee and says insurance has this insurance record using some kind of a relationship  it means that this relationship is giving an identification or an identity for this weak entity type called insurance record so such relationships are called identifying relationships and we are also saw what is called as a total participation within a relationship so the same thing here  an insurance record will have no existence without its corresponding employee or without a corresponding person so this insurance record is set to totally participate in this identifying relationship however we also saw that just because there is a total participation does not mean that the entity type is a weak entity type how do we  what is an example for that ? we saw an example of department managers project let us say that a project has to be associated with a department otherwise well  it will not get funding  it wo n't get off the ground or something like that so in this case this is a total participation that is the existence of project  the existence of projects will depend upon this relationship that is the existence of some department that is willing to manage this part however a project by itself need not be a weak entity type that means it may have a separate key by itself  a project will have its own separate project identification number which may have no relationship with the department number not every total participation implies identifying relationships but an identifying relationship implies total participation so we will be continuing further on today with some more notations and which can give us greater expressiveness to express what we perceive as relationships and associations between data elements in our uod  refer slide time  00  11  47  now these kinds of  these sets of notations that we are going to see today are what are called as enhanced er notations or sometime also called as a extended er notations and abbreviated as eer notations  refer slide time  00  11  56  one of the first relationships or associations that we are going to see today is the notation of subclassing or inheritance subclassing essentially face what is called as an is-a relationship as you can see in the slide here an entity class b is said to be a subclass of another entity class a if it shares an is-a relationship with a what is meant by an is-a relationship ? have a look at these examples a car is-a vehicle  a monkey is-a primate and a primate is-a animal or is an animal whatever a manager is-a employee so if you have noticed here an is-a relationship identifies a specialization of some particular entity type a car is-a vehicle but not all vehicles are cars  there could be trucks  there could be bicycles  there could be scooters and so on so vehicle is a more general class of cars similarly if you say a maruti 800 is a car but not all cars are maruti 800 ’ s so car is a generalization of maruti 800 and maruti 800 is a specialization of the entity type called car similarly a monkey is-a primate and a primate is-a animal but not the other way so the entity class b that is at the left hand side of the is a relationship is set to be a specialization of entity class a or on the other hand entities of class a are set to be generalizations of entities of class b so what are the properties of generalization and specialization ? have a look at this slide and there are some interesting properties when we are talking about a generalization and a specialization relationship now suppose i have an entity of type car so let us say this entity with a particular registration number is a car that exists in the database now in the database i also have some entities of type vehicles now should this car entity type belong in the vehicle entity type ? if you think about it carefully  you will say that the answer is yes because the simple reason is that a car is a vehicle  refer slide time  00  13  46  if i have an entity that exists in the set of cars  the same entity should also exist in the set of vehicles so an entity can not exist in the database that merely belongs to a subclass  it also has to belong to the super classes and subclasses undergo type inheritance of the super class what is meant by type inheritance of the super class ? again notice carefully here now let us say that we are going to describe some properties of vehicles now what kinds of properties can we think of vehicles ? vehicles will have wheels vehicles will have i mean depending on what you call vehicle i mean you could also called a rocket as a vehicle so assuming that we are only looking at road vehicles suppose i say that a vehicle is represented by wheels and it should have some kind of controlling mechanism  it should have a driver seat or something of that sort and it should move now you see that all of these characteristics apply to all subclasses whether it is a car  a bicycle or a truck or a van or whatever all of these have to move  they have to have wheels  they have to have some kind of a control mechanism whether it ’ s a handle or a steering wheel or anything like that and so on so  all attributes that describe a general class has to be inherited by the special classes  that the specialized class each member of a subclass has the same attributes as that of the super class entities and participates in the same relationship types the second aspect is also important  if a general class entity participates in a particular relationship type  a specialized class entity should also be able to participate in the same relationship type that is if i can use a vehicle to go from point a to point b  i should be able to use a car to go from point a to point b or i should be able to use a truck or a bicycle or whatever so if there is a relationship that exists between vehicles and let us say an employee using that vehicle  you should be able to replace this vehicle with any of the subclasses and the semantics should not change the semantics of the entire database system should not become incorrect that ’ s why said we are looking at road vehicles i mean it ’ s a matter of naming the particular entity  a car is a road vehicle obviously if i also consider rockets and airplanes as vehicles  this doesn ’ t hold anymore using a rocket let us say i can go from here to the moon but i ca n't do that using a car so you won ’ t be able to replace a subclass  subclass entity wherever a super class entity arises so when you are coming out with inheritances and special generalization and specializations in your database  you should be aware of the fact that this type of  this kind of replacements of general class entities with special class entities should be possible that is what establishes a correct inheritance relationship with a incorrect inheritance relationship so here is another example suppose i say that manager is an employee or a manager could also be an employer so as you can see here the inheritance relationship is depicted by a u in the relationship type that is there is a straight line with a u which represents an inheritance relationship or a specialization relationship  refer slide time  00  17  56  if you note that if this is an employee here and an employee is uniquely identified by the pan number  the same pan number can uniquely identify the manager as well so what is that mean ? it means that any key attributes that uniquely identifies  that uniquely identify entities of a super class or a more general class can also identify attributes of the special classes however special classes may have some more attributes in addition to the key attributes that the general classes have for example a manager may also have one more identification which says for which department is a manager or what is a scale or whatever now in order to be able to identify one manager from another uniquely  you may have to combine that attribute with a pan attributes here so there may be other attributes that form the key for the special classes but all the key attributes of the general class has to be retained in the special classes so another example of type inheritance  in this case it ’ s more of a key inheritance the key has to be inherited directly the process of creating subclasses out of a given entity type is called specialization  refer slide time  00  19  42  that is suppose i have a particular entity type  suppose i have identified that the uod here requires vehicles now out of these vehicles i identify that they require vans  they require cars  they require trucks and so on and then i also identify that van is a vehicle and a truck is a vehicle and a car is a vehicle and so on so i should be able to form what is called as an inheritance tree so this process is called specialization on the other hand its also possible that we go in the reverse fashion we first look at the uod  we first go through the company  talk to people and see what is happening and then we identify different entities we see that the company uses cars  the company uses buses  the company uses motorbikes  the company uses trucks and so on the company uses vans and then once we have listed all of these we start seeing relationships among them  we say that all of these are vehicles and all of these share the same attributes as far as the company is concern and then we put all of them in an inheritance tree and this is what is called as a generalization now before we go to the next slide  let me inter check here to note that this is not such a straight forward process  which entity is a special class and which entity is a general class is not such a straight forward process sometimes depending on the usage context which becomes a general class and which becomes a special class  a specialized class may change from one context to the other let me give a particular example take two entity types an airplane and a glider now which is correct ? the first one which says a glider is an airplane without engines or whatever  a glider is a airplane or an airplane is a glider which is correct ? so if you look at it carefully  let us go back to what are the properties of specialization and generalization classes the first property of specialization classes is that wherever i am planning to use the generalized class objects  i should be able to use a specialized class objects so is that always true  wherever i use airplanes can i use gliders ? maybe or maybe not  i mean depends  it depends on the context and secondly whatever attributes that the generalized class has  has to be inherited by the specialized class again this seems to say that let us say an airplane has several different attributes  it has engines  it has wings  it has wheels  it has controls and so on a glider also has all of them except that it doesn ’ t have an engine so it seems to suggest that an airplane is a glider is a correct one so glider is more general and an airplane is more specific  so because a glider has a smaller number of attributes and an airplane has a larger number of attributes however look at it in the context of learning how to fly a glider or learning how to fly an airplane now if you see that let us say i have different paragraphs about or different kinds of skills that i have to learn for flying an airplane and for flying a glider it could well be the case that depending on the sophistication of the airplane  there are some airplanes here where you do n't have to do anything  you just have to go and plan your journey and push a button and it will take you there with all auto pilots and so on and so forth and so on so depending on the context  you may actually have to learn more to fly a glider than to learn than to fly an airplane so if the number of attributes or the different kinds of skills i need to fly this  you see that the opposite inheritance tree is valid that is an airplane that is a glider is an airplane that is an airplane requires smaller number of skill sets to fly while a glider requires a larger number of skill sets to fly therefore a glider is a subclass of airplane so as you can see that it is not such a straight forward thing to identify is-a relationship so it depends on the application context and we should not ignore the application context like we saw in the previous session  a database can not ignore the information system context within which it is going to be run is the application context about building an airplane or a glider or is a application context about flying an airplane or a glider now that much change the inheritance semantics in our conceptual schema  refer slide time  00  24  57  so coming back to specialization and generalization processes  let us take a small generalization example and see how we go about it let us say we have identified two entity types in our  back to our company database so let us say we have identified an entity type called secretary and the secretary is identified by a pan and salary and the kinds of skills the secretary has typing short hand or whatever so on and so far similarly we have identified manager and we see that manager also has a pan number a manager also has a salary and there is an experience field saying what kind of experience the manager has now when we see two or more entity types sharing the same kind of attributes for a large extent that is out of three attributes two are similar here it gives us reason to believe that probably these two are special cases of the same general class so we can generalize them something like that  refer slide time  00  25  59  so we can create a more general class let us say called employee and then say secretary is a employee and manager is a employee so note that the attributes that where first a part of secretary and manager have gone here that is only the common attributes between secretary and manager have moved up the hierarchy to go to the employee class and all those attributes which are specific to this specialized classes remain in the specialized classes that is skills remain here and experience remains here now in some cases it maybe able to  we maybe able to identify precisely how to distinguish one special class to specialized class to another specialized class  have a look at this slide here  refer slide time  00  26  41  this slide show an entity type called employee which is defined by attributes called pan and salary and one more attribute called job type which is not actually shown here now there are two specialized classes secretary and professor now suppose we identify a property that every professor has a job type as academy and every secretary has a job type called admin so each professor belongs to a category of academic jobs and each secretary belongs to a category of administrative jobs so we know exactly how entities of one specialized classes can be distinguished form entities of another specialized classes so this is how we identify this here  we say job type and then we say admin is secretary and academic is professor so such kinds of definitions are what are called as predicate-defined subclass these subclasses are defined by the values of one or more predicates that exist in the er schema next we go to an example where we see that in some cases not all subclasses maybe unique now let us take back the example of secretary and professor you see here that while denoting this subclasses  we have drawn a circle with a small d here now what is this d denote ? this d denotes the fact that these two subclasses are disjoined what is meant by disjoint here ? that is they are mutually exclusive  no secretary is a professor and no professor is a secretary because all secretaries have to have a job type as admin and all professors have to have a job type of academic so the set of all secretary is a disjoined set from the set of all professors  refer slide time  00  28  59  but this need not always be the case  sometimes two or more specialized classes or specialized entity types may actually overlap they need not be mutually exclusive from one another for example suppose in some university there are notions of chair professors  chair professors are usually supported from external sources of  external funding sources but for all practical purposes they work as any other professors here now it could well be the case that some professors are chair professors and some chair professors are normal professors that is they need not be supported by a project but they also work in other activities and so on so such kinds of inheritance trees or subclasses are set to be overlapping subclasses so these two subclasses need not be disjoined from one another and this overlap  this kind of overlapping maybe either partial or even total overlapping now you might have  you might notice that every chair professor is a professor but not the other way around and so on so it ’ s a total overlap as far as chair professor is concerned on the other hand if there are some chair professors who are not teaching  let us say who are not doing the normal activities of a professor here then the overlap is partial overlap  refer slide time  00  30  33  the next kind of generalization technique that we are going to do see is what is called as a union type or this is also called as a category now have a look at this slide a little more carefully now this slide shows entity type called account holder in a banking scenario now when you ask a banker who or what is an account holder  he will probably tell you that the account holder is just an abstraction  it is an entity  it does not necessarily represent a person because it may actually represent an institution an institution maybe an account holder or an individual maybe an account holder or sometimes in some cases accounts maybe held by families or sometimes dynasties and so on so as far as the bank is concern  all of them are just account holders and they are just abstractions but all of them share  all of them have their own set of attributes and have different sets of characteristics obviously an individual is different from an institution institution has characteristics like number of employees and so on which an individual which may not make sense for an individual entity type so each of these entities here in the top most in the top run here may have their own sets of attributes which may not be in common with one another but all of them are account holders here so each individual may have its own different key for example a pan number for an institution and address for a family or some kind of registration number for an institution but all of them are account holders as far as a bank is concerned so such a kind of relationship is what is called as a union type if you are familiar with programming  in c programming you have this notion of unions which has very similar co-notations that is an account holder is either an individual or a family or an institution  a union type is also called a category  refer slide time  00  32  51  and just like we saw in the case of inheritance where in the case of subclassing where a subclassing  subclass could be either disjoined or overlapping we can have what is called as a partial union or a complete union now for example here  not every individual that exist in the database could be an account holder an individual is an entity and an entity is something which has its own independent existence so we may be keep in track of individuals for our own purposes but some individuals in the database could be account holders similarly we maybe keep in track of institutions for some other purposes but some institutions in our database could be account holders so when only a part of the entity set of individuals form or participate in this relationship  we call this as a partial union on the other hand if every individual that we hold in our database is an account holder or participates in this union relationship then it ’ s a full union relationship  refer slide time  00  34  04  so that was briefly about generalizations and specializations and in fact this is a very crucial concept in being able to obtain the notion of abstraction that is to be able to abstract away unnecessary details from a special class and go to the general class so if you are able say that we are going to use an entity type of a general class  it means that it has just enough details that is necessary for this relationship to exist that means if i say that i need a car for this particular activity  i do n't need to worry about what kind of a car is that  what color of the car is that or what is the horse power of that car or whatever it ’ s all this attributes are specific to particular kinds of cars but for this particular activity any car would do so we are essentially abstracting a way or covering up all the unnecessary details and looking at only the necessary detail what is required for a relationship to exist the next concept that we are going to be looking at here is the concept of higher order relationships until now we have been considering relationships with a degree of two recall that the degree of a relationship is the number of entity types that participate in this relationship now this slide shown here shows a relationship with a degree three have a look at this relationship carefully it says that the relationship is called supplies and it relates three different entity types the supplier  part and project what is the relationship say or what is a semantics of this relationship again ? there are three different entity types supplier  project and part so basically it means that the supplier supplies this particular part for this particular project now if you think carefully it is not possible to reduce this ternary relationship or a relationship of degree 3 to any number of relationships of degree 2 a supplier may supply some parts but not all parts maybe designated for this particular project a supplier may supply for a project but you may not supply all parts that are required for the project now a project may use a certain parts but not all parts that are used by a project or may be supplied by just one supplier  there could be any number suppliers so we can not reduce it to three binary relationships without losing meaning let us try to do that and see what happens now the closest possible binary relationship that tries to simulate this ternary relationship is something like this a supplier supplies to a project  a supplier stocks some parts and a part is required by a project or a project requires certain kinds of parts  refer slide time  00  37  57  and to be fair  to be sure we also note that a part is a weak entity type  it has no existence by itself it has to be either associated with a supplier or with a project so even when you do that this is probably the closest we can come to simulating the entity relationship but not quiet close as you can see it can still  it just because suppler stocks certain parts doesn ’ t mean that this part will be required by this project or vice versa let us take another example of higher-order relationships and see whether we can reduce it to lower order relationships without losing meaning so this slide here shows a relationship which is a ternary relationship called offers so it says that instructor offers a course during a semester now there are also other relationships that we have identified in the database and which says that instructor taught_during certain semester or instructor can_teach a particular course or a courses offered_in a particular semester now note that if i have an instance of this relationship that is offers i s c what is i s c means ? if there is an instance of this relationship called offers for a particular instructor in a particular semester for a particular course  this implies that the instructor has taught_during this semester and the instructor can_teach this course and the courses offered_in this semester  refer slide time  00  40  26  that is taught_during i s and can_teach i c and offered_in c s that is an existence of this ternary relationship implies the existence of all this binary relationships however the converse need not be true  the converse that is suppose i have instructor can_teach  instructor i can_teach course c and instructor i taught_during semester s and course c was offered during semester s but that doesn ’ t mean that the same instructor has offered this course during the semester is instructor i can_teach this course but and the courses offered during a semester and the instructor taught_during that semester but that still doesn ’ t say that the instructor taught the same course during the semester  he could have taught some other course and this course could have been taught by somebody else so while this is true the converse is not true  reducing if instances of binary ration relationships exist  we can not be sure that the instance of a ternary relationship also exists cardinality constraints on higher higher-order relationships so what is it mean when we say when we put cardinality constraints on higher-order relationship ? here is an example this examples shows again the instructor semester course example  so it has put a 1 here and a n here and a n here  refer slide time  40  50  so that means that at given course-semester combination should have only one instructor that is in a particular course for a particular semester there has to be only one instructor on the other hand a given instructor may have any number of course-semester relationships that is given instructor can teach in any number of semesters and any number of courses so  again if you think about this carefully  if i have a set of all this relationship types instructor course-semester and so on how do i identify an instance of this relationship type uniquely ? the key here is the course-semester pair so if i take an instructor  an instructor may offer any number of courses in any number of semesters however if i take particular semester and a course you see that it can uniquely identify an instructor that is because every course and semester pair should have just one instructor associated with it the last concept that we are going to look at in this session is the notion of aggregation this concept is usually used in what is called as knowledge management or km in the concept of ontology ’ s and so on  refer slide time  00  41  51  so an aggregation basically aggregates a particular er schema and makes into an entity at a higher level of abstraction note the certain difference or and very important difference between the kind of abstraction introduced by aggregation and the kind of abstraction introduced by inheritance or specialization aggregation brings about the concept of composition or contains relationships so here this slide show an aggregated entity called offering which contains one or more instances of the relationship called instructor  course and semester so an instructor offering a particular course in a particular semester is called a course offering or an entity type called offering so the relationship between the offering entity type and this relationship called offers is that of contains offering contains offers on the other hand the relationship between generalized and specialized classes is that of is-a relationship or rather between specialized and generalized classes a car is-a vehicle and a bus is-a vehicle or monkey is-a primate and so on but aggregation offers the concept of containment this contains this contains this and so on so even aggregation brings about a kind of abstraction that is you are covering up unnecessary details  if i am not really required to know what is the structure this offering i do n't need to really worry about that so this slide here shows the relationship between offering and offering that is one course offering requires another course offering so let us say course number a requires or has a prerequisite that some other course lets it said has to be taken up by the student so a course offering of a requires a course offering of z  so without z being in the database i can not have a course offering of a so here the abstraction basically throws away all details which that talks about what exactly an offering is about  refer slide time  00  44  44  so with that we have covered the major parts in the enhanced er notation or eer notation so before we conclude the session let us take up a small example of a university database and see how or in which kinds of situations do we get these inheritance and generalization and specialization how do we go about identifying that we might probably generalize here or even probably specialize here and so on now take up a small university database now this example here is by no means exhaustive i mean we can not build a complete database in the course of a session like this but we just trying to see what kinds of typical problems or typical kinds of issues that can arise here  refer slide time  00  45  38  so some basic entity types  so each universities has a student and of course several other entity types again i am abstracting away unnecessarily details that is students  faculty members and staff and so on and so on so let us say we have identified a basic as entity type called student now we then go about adding some attributes  for students we note that each student is given a roll number which uniquely identifies the students as long as the student is in the university each student has a name  each student has a gender  a date of birth  address and so on and then we go about looking at other entity types let us say we say that faculty is an entity type and then we talk about what are the attributes that characterizes a faculty member then we come out with some more attributes like this let ’ s say each faculty has an employee number  each faculty has a name and a gender and date of birth and address and so on  refer slide time  00  45  57   refer slide time  00  46  16  now if you see student and faculty they look quiet similar all ready then we identify let us say some kind of non teaching staff and then we see that even they have the same kinds of attributes that is employee number  name  gender  date of birth  address and so on  refer slide time  00  46  51  so which tells is that we are actually looking at different entities of the same generalized class so what kinds of generalization can we make out of these three different classes ? if you see staff and faculty  there is hardly any difference between the two entity types but between faculty  staff and student there are certain differences so how do we identify these differences here ? this brings us to a generalization and specialization tree we see that faculty and staff can both be categorized as employees and they have the same key called employee number on the other hand the same  the key called employee number can not identify a student  a student is given a roll number so employee and student do not belong to the same level as a faculty but they belong to the same entity type called person now what is the property of this person entity type everything else that was common between the three entity types  refer slide time  00  47  41  what are the common attributes between the three entity types name  gender  date of birth  address ? so the attributes that going to person would be all of these attributes name  gender  date of birth  address and so on which all of them share whether it ’ s a student or a faculty or staff all of them share similarly we start looking at certain association let us say a faculty works in department and a faculty heads department and we identify certain kinds of association constraints that says that n number of faculty member may work in a department while only one faculty member may head a department and we also identify some more associations which says that n number of students maybe registered in a particular department and we can also find some aggregations which says that a project involves particular department or project is headed by a faculty member and a faculty member belongs to a particular department and we see that this whole thing can be aggregated into an entity type called sponsored project so a sponsored project means that there has to be a project entity which is involving a particular department and is headed by a particular faculty member and so on so how well schema is aggregated into the sponsored project entity type ?  refer slide time  00  48  40   refer slide time  00  49  02  we can also see a certain higher-order relationships for example let us say some foundation  some organization or non-governmental organization or whatever supports a particular project and a particular department so again we see here that foundation supports department on this project and we can see that we can not reduce it to binary relationships  foundations may support sponsored project and may support department but the ternary relationship says that for this project and for this department this foundation is supporting  refer slide time  00  49  02   refer slide time  00  49  53  so have a look at this higher-order relation relationship here let us say i have a relationship that says a faculty member collaborates with some other faculty member on a particular project so note the double use of this faculty entity type that is this faculty member collaborates with this faculty member on a particular project  refer slide time  00  50  30  and so one faculty member probably  the head of the project may collaborate with n other faculty members on n other on n different projects now you can reduce it like this that is a faculty member collaborates with other faculty members and the same collaboration extends to project  refer slide time  00  51  09  however you ca n't reduce it like this that is a faculty member collaborates with another faculty member and works on a project because we are going to lose semantics  refer slide time  00  51  27  so that brings us to the end of the second session of enhanced entity relationship concepts so before we conclude let us briefly go through the different concepts that we learn today the first concept that we learned was about generalization and specialization where you achieve abstraction using an is-a relationship so in a generalization and specialization relationship  for any entity of the general class can be replaced by any entity of the special class or the specialization without losing semantics only then will you be able to say that my generalization is correct it need not always be correct just because something looks like is-a would hold doesn ’ t mean that the generalization is correct so we saw the notion of inheritance that is each specialized class or a subclass inherits all attributes including key attributes and constraints and relationships from the generalized class and we also saw the notion of overlapping subclasses and disjoined subclasses and how to build a entity type using union types or categories and we saw the notion of higher-order of relationships and how they can not be reduced to lower order relationship without losing semantics and the final concept that we saw today was the notion of aggregation which is again a kind of abstraction relationship but however which establishes the notion of containment rather than is-a that is abstracted by the specialization relationship so that brings us to the end of this second session on enhanced entity relationship concepts database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 3 relational model hello everyone  we have been looking into the process of database design and let us continue with this in this session as well as we saw in the previous sessions  a database design goes through several different phases and we have been mainly looking into the conceptual design of a database a conceptual design essentially means a high-level design of the database or the database system which is mainly meant for targeting the end users that is trying to explain your database model to the end users today we are going to look at another model of data which is called the relational model  refer slide time  00  02  04  and how do we place relational model with respect to the entity relationship model that we have been considering until now in order to answer this question  let us revisit our typical database design process that we saw in one of the previous sessions  refer slide time  00  02  10  as we had seen that a database design process is contained with in a universe of discourse that is a universe essentially is the information system context within which a database is designed whether it is a bank or whether it is railway reservation  whether it is even your mobile phones in many of these different application context databases are usually embedded so it is this application context that makes up the universe of discourse now once we analyze the universe of discourse  we essentially get two kinds of requirements one was what was called as the database requirements as shown in the slide here and the other is what is called as the functional requirements so the database requirements essentially meant  what are the data elements that makeup the system and how are they interrelated and how should we make sense out of the data elements and functionality requirement or functional requirements or the application programming requirements which say what kinds of processes have to run on this databases and what are the semantics of these processing ’ s so these requirements in turn gave rise to two kinds of parallel processes the database requirements gave rise to the conceptual design of the database and from the conceptual design came the conceptual schema and we saw in the previous class that the conceptual schema is usually build using the er model that is an entity relationship diagram  refer slide time  00  03  24  now let us follow this upper stream that you see in the slide here a little bit further and see what happens to the conceptual schema  refer slide time  00  3  57  now the conceptual schema which is typically meant for are essentially meant for communication with the end user is in turn going to give rise to the physical schema what is a physical schema ? the physical schema is essentially the schema that is actually build on the database system on the computer and as you might have imagined  while the conceptual schema is oriented towards human understanding that is communicating your schema or communicating your design with the end user  the physical schema is oriented towards machine understanding or essentially efficiency in terms of storage and retrieval of data elements so the physical schema is optimized towards quick updates  quick inserts  easy searches and so on so it is one of this physical schema or the building blocks of such a physical schema is what we are going to see today and the model that we are going to see today the relational data model is the most widely used data model in most databases today whether it is any kinds of application context  whether it is banks or railways or telephone exchanges or whatever any of these application context typically used relational model to store data as part of the internal schema structure  refer slide time  00  05  22  so what is the background of the relational data model ? the relational data model was introduced in the 1970 ’ s  the early 70 ’ s by ted codd from ibm research and in fact there is you can do a web search for ted codd today and many of his seminar papers that where the database or the relational model was proposed are actually available over the internet and you can actually have a look at them to see why it was so influential before the relational model was proposed there where another models like hierarchical and network model and so on which were not very amenable to internal storage which are not very efficient in terms of storage and retrieval complexity and so on and the relational model was extremely elegant in terms of storage and updates and retrieval searches and all these the main concept behind the relational model is the notion of a mathematical relation you might have studied in course on discrete mathematics and mathematical relation is just a mapping between two or more sets  so where each set constitutes a domain and a mapping between each of these domains forms a mathematical relation in intuitive terms a mathematical relation is no different from what we understand as a relationship in normal english a relationship is similar  is essentially some kind of an association or some kind of linkage between two or more different data elements an employee is associated with a department  an employee has a name associated with him  an employee has a id associated with him  an employee has a salary that ’ s associated with him and so on and so forth so it ’ s this mathematical relation let us say the set of all employees versus a set of all salaries  you can establish a mathematical relation between these two sets it is this set that forms the underlying basis for the relational model so this is the standard database model for most transactional databases today so let us go step by step into the nuances of the relational model so what exactly are the building blocks or the essential concepts that makeup the relation ? so what is a relation in turn ? a relation intuitively represents a table as you can see in the slide here or it ’ s also called a flatfile of records this slide shows here a small table which has three different columns the first column is named as roll number  the second column is named as name and the last column is named as date of registration and there are several different rows and each row corresponds as you might have guessed by now each row corresponds to one particular record or one particular student in this case  refer slide time  00  07  29  so the first row has a student by name adithya and his roll number is 2003-01 and there is a particular date of registration now this is one data element  one set of data elements that are interrelated  so one schematic data element now  this row is independent of the second row which talks about another student called ananth kumar and with his own roll number and with his own date of registration and this in turn is an independent data element that is independent of both the first row and the third row so each row in a table represents  a collection of such related data values are what is called as an instance of the relation the relation in this case is or the schema in this case is a table comprising of three different columns roll number  name and registration an instance of this relation is one of these rows  one which says 2003-01 and name as adithya and a date of registration as 12 8 2003 so each row in a relation is called a tuple and each column is called an attribute of the relation  refer slide time  00  09  32  so let us dwell little bit deeper into the relational model and before we do that we need to define some certain crucial elements in the relational model now the first definition we are going to comeback to it again  the first definition in the slide talks about the notion of an atomic data type now have a look at the slide once again  a data type is said to be atomic if it can not be subdivided into further values remember in one of the previous sessions we talked about attributes in an er schema being either a simple attribute or a composite attribute a composite attribute is a non atomic attribute that is for example name if the name of a person in turn comprises of other attributes like first name  middle name  last name  title  initials and so on this is not an atomic attribute on the other hand the age of a person is an atomic attribute because you can not sub divide this attribute into further sub attributes now this is important because a relation is defined only over atomic attributes as we will see in the next slide and the next definition that we are going to consider is the notion of a domain we have already seen the notion of a domain in the er model and even here the notion of a domain is no different a domain is basically a set of atomic values which defines the space within which an attribute might obtain a value for itself for example we had seen the examples of the age of an employee which may not be lesser than 18 years and greater than 65 years so the age of an employee is a set of all numbers maybe even fractions between 18 and 65 so this constitutes the domain for this attribute called age similarly there are domains for names and dates and so on and so far this slide actually shows some examples like the set of all integers  the set of all valid student roll numbers  the set of all indian cities with population above 6 million or whatever anything that defines a space of possible values is called a domain a relation schema or a relational schema  this is a crucial definition the relational schema is as you can see in the slide has a very specific notation a relational schema is defined by the name of the relation here it is shown as r and a list of attributes  here its shown as a1 a2 extra to until an so this is the name of the relation r  in the previous example the name of the relation was called student record and the attributes are roll number  name and date of registration so each ai shown in this definition  here is the name of certain some attribute or as shown in the slide here is the name of the role played by some domain which is an other way of putting the same thing  name of an attribute or the name of a role played by a particular domain now for example in the previous case their roll number  the domain of roll numbers was the set of all possible valid student roll numbers now what is the role that this domain plays in this relation is the attribute which is the part of the relational schema so few more definitions just like we had that degree of relationship in the er model  we define the degree of a relation in a pretty analogies fashion  it is simply the number of attributes in the relation schema  refer slide time  00  13  08  for example in the previous schema that we took  the degree of the relation was three  it had three attributes roll number  name and date of registration so a relation r or what is even called a set of instances of relational schema  a relation this is different from a relational schema the relation of a relational schema is a set of tuples which belong to the schema or which conforms to the schema along with the schema itself so a set of tuples of the form t1 t2 extra tm were each tuple is of the form of n different attributes or n different values one for each of the attribute so again in the example that we saw earlier  it had three different tuples there were three different student records with three different roll numbers so the entire set that is the schema plus the set of all tuples is called a relation so a relation as you can say  it can also be defined as a subset of a cross product of all this domains so the cross product of the set of all roll numbers times the set of all student names times the set of all registration dates that are possible now a subset of this cross product is what is going to form a relation now what is some of the characteristics of a relation in the relational model ? ordering of tuples  that is one of the first issue that we are going to look at this slide shows here three different characteristics of tuples  refer slide time  00  15  02  ordering of tuples  mathematically the relational model does not have any ordering that is specified over the tuples now it does not matter as far as the mathematical model of the relational schema is concerned whether the set of all tuples are ordered roll number wise or name wise or whatever it is just a set of tuples that are in conformance with the relational schema so but in reality of course they do have some kind of an order which is specifically the order in which they are stored on the disk whether they are stored in sorted order or not  it doesn ’ t matter but they have some kind of an order in reality ordering of attributes  note that a tuple  you might have studied in course on discrete mathematics a list is something where the order is important a list is different from a set so tuple is basically a list of n different elements which means that the order of these n different elements is actually important so if the relational schema says that my relational scheme is called student record and i have these attributes roll number  name and date of registration and if i have a tuple having three different values the first value corresponds to roll number  the second value corresponds to name and the third value corresponds to the date of registration so the ordering is important but that is mathematically speaking in fact  in reality though we can do away with ordering within a tuple as well i can as well dereference a particular attributes by its name  i can as well say what is the roll number value of this relation  what is the name value of this relation  what is the date of registration value of this relation and so on and values of tuples which is the third characteristic that we are going to look at today so like i mentioned before each tuple or each value that makes up a tuple is assumed to be atomic in nature and this is what is called as a first normal form assumption in fact we are going to see in one of later sessions that the first normal form is just the first step in a series of different normal forms in which the database can be optimized for enhanced maintenance of the data what do we mean by enhance maintenance ? easy addition of data elements  easy searching for data elements  easy updation of data elements and so on and this is primarily the reason why it is stipulated that or why it is required that each data value that makes up a relation are to be atomic in nature and atomic as we had seen earlier is something that ’ s not composite that is some value that can not be subdivided into further semantic values so just like entity relationship model  a relational model also are specified by certain kinds of constraints on the data model one of the first constraints that we are going to look at is pretty obvious which is what is called as the domain constraints what is the domain constraints say ? each value of an attribute within a relation has to have a value which lies within the domain which is pretty obvious so the roll number of a particular student has to be a valid roll number that is it has to belong to the set of all possible roll numbers if the set of all possible roll numbers range from 1 to 150  i obviously can not have a roll number which is 200 or i obviously can not have a roll number which says a b c and so on so the domain constraint specification basically states that each data value that makes up a relation has to be or has to belong to the domain in which it is the domain of the attribute in which it utilizes  refer slide time  00  18  16  the second constraint in the relational model is what is called as the key constraint and this is quiet similar to the key constraint that you saw in the er model as well in any relation there could be a subset of attributes that have a property that for every tuple in which those attributes appear  they have a unique value for those tuples such kinds of attributes are called super keys now what is the use of a superkey ? obviously to be able to uniquely identify every tuple in a relation in the previous example where we took that a relation having three different attributes roll number  name and date of joining you can see that roll number forms a superkey because roll number is an attribute whose value is unique for every tuple in the relation however you can also combine roll numbers along with the names and you can see that it ’ s always going to be unique if the roll number is unique roll number plus name is obviously going to be unique so roll number plus name or in fact the entire record can be called the super key for each record which brings us to another property of the relational model in the relational model each tuple is distinct that means in the worst case the entire tuple is the superkey the relation does not allow multi multiple tuples having the same value in the sense that it is a set of tuples and not a multi set of tuples or what is typically called a bag of tuples so each tuple are to be different in value from the other so  a key is a set of tuples in which is defined in a similar fashion as in the er model a key of a relation is a subset of the superkey such that no subset of a key is a key in itself if you remember this was more or less very similar to the way we defined a key in the er model itself so roll number in the case of the student record is a key and if you take away the roll number  you can not identify or you can not distinguish one tuple from the other so however roll number plus name is not a key in itself why ? because you can still take out the name attribute from the key and you can still uniquely identify each tuple using just the roll number because the roll number is sufficient to identify each tuple uniquely in the relation so there are also called minimal superkeys  refer slide time  00  22  10  so this slide shows the talks about what we saw just now that in the student table roll number and name is a super key however it is not a key or it ’ s not a minimal super key because you can still take away name and still be left out with the key that is you can still identify each tuple uniquely in the relation or in the table so  just roll number is a minimal or a minimal super key now  just like we had seen in the example earlier that a house can have more that one keys  the key for the front door and key for the back door any relational scheme or any relation can have more than one key or so on take an employee record and usually employees are given employee identification number and they also have a pan number which is given by the government now using either of these two  you can identify an employee uniquely because each of them are unique for an employee so each such key or each such set of subset of attributes which can uniquely identify tuples in a relation is called a candidate key so either the employee number or the pan number is a candidate key in itself but usually one of those candidate keys are used for identifying tuples in a relation in a company contacts its usually the employee number  we do not usually identify people with that pan number when you are talking about their performance records or salary statements or anything of that sort  we usually talk about their employee number now whichever candidate key is used for the purpose of retrieval in quires and insertions and so on is called the primary key of a relation so in the student relation that we saw earlier usually it ’ s the roll number is what we use for students  so the roll number is a good primary key now just like their entity constraints as part of the er model  there are certain constraints that make up the relational model as well the first constraint is what is called as the entity integrity constraint so what is the entity integrity constraint ? it essentially says that whichever entity the  now look at what we mean by an entity here it ’ s a slight change of nomenclature what we mean by an entity  here is a tuple in the relation  refer slide time  00  24  14  so the primary key of a tuple can never be null obviously because we wo n't be able to identify each tuple uniquely the second integrity constraint that ’ s important in the relational model is what is called as a referential integrity constraint now what is a referential integrity constraint mean ? sometimes some set of attributes in a relation may point to certain other tuples in another relation we had seen such an example in one of the previous sessions as well  when we said that a department is headed by an employee who is the manager or the head of the department so that the headed by is usually contains an employee id of the person who is going to head this department so here what we are and ensure that we are going to get here is what is called as the referential integrity here there is a reference from the department entity to the employee entity now this referential integrity constraint basically says that whenever i make a reference from any tuple to any other tuple  it should make a reference to an existing tuple that means i can appoint somebody  i can appoint some employee with some employee id as manager of the department as long as or only as long as such an employee already exist in the database so such a reference is what is called as a foreign key so in any given relation set of attributes is set to be a foreign key if the following rules hold the first rule basically says that the attribute in foreign key has to be the same as the primary key of the other relation  this is fairly obvious if i am going to say that the department is headed by an employee and i write an employee id here but use employee name as the name of the person who heads  it obviously is not a foreign key it has to be either both the primary key in the employee record and the reference in the department record has to be names or they both have to be employee id ’ s in some sense and for every tuple in the referencing attribute or in the referencing relation like department  the attributes in its foreign key refer to existing tuples that is each department should have a manager who exists  who already exists in the database or there should be null that means there should not have a manager at all so either of these two should hold for the referential integrity constraint to hold  refer slide time  00  27  36  so referential integrity is usually depicted in a diagrammatic fashion as shown in this slide here this slide shows two different schemes or relational schemes  one called the employee schema and the second called the department schema so the employee schema has employee id  name  works_in and reports_to so employee id would be the primary key here and the works_in actually is a foreign key which refers to the department id and reports_to is another foreign key which refers to another employee id within the same relation so note that foreign keys can be from a relationship to itself but the same referential integrity constraints hold that is if an employee a is reporting to employee b  employee b should already exist in the database by the time employee a is being added to the database  refer slide time  00  28  35  and the last kind of constraint over a relational model is what is called as the semantic integrity constraint an semantic integrity constraints is usually more of an application specific constraints something like the age of an employee can not be less than 18 years and greater than 65 years so this is not per say part of the relational model but usually this is important to be implemented within a database context and automatically checked and verified that these integrity constraints are maintained now let us quickly look at what are some of the basic relational algebra operations that make up the relational model now essentially the operations of relational algebra can be categorized into one of two different kinds of operations namely retrieval of data or updates to the database  refer slide time  00  29  07  now updates to the database are usually handled by what are called as insert and delete operations we should not be looking into insert and delete operations in any detail in this session mainly because they do n't have many  very many properties that we can explore at this moment so right now we will be looking mainly at the retrieval operations and retrieval operations are handled by two basic operations called select select is denoted by a sigma as shown in the slide here and the project operation which is denoted by a pi which is also shown in the slide here so let us go to the select operation the select operation is a very simple operation that is used to select a set of tuples from an existing relation so remember a relation is basically a set of different data tuples along with the schema now given this set of tuples and schema  we can use the select operation to select a subset of those tuples now this slide here shows an example which says select salary greater than 3000 from employee so as it shows here sigma salary greater than 3000 as a subscript and employee has the parameter this operation  refer slide time  00  30  11  so as you can see there is a operation here  there is a condition and there is a domain or there is a relation over which the operation is going to be performed so this is going to select the set of all records from the employee relation where the value of the attributes salary is greater than 3000  refer slide time  00  31  15  so the general form of select is shown in this slide  its simply as like this select condition relation so where condition is a conditional expression  i have written a slightly formal grammar of how a condition looks like essentially condition is a logical expression over attributes names  something like select salary greater than 3000 and gender equal to male from employee so which basically says give me all male employees in this relation whose salary is greater than 3000 and so on  refer slide time  00  31  48  now what is some of the properties of the select operation ? the first property which you might have noticed here is that the select operation is unary in nature what is a unary operator ? a unary operator is something which operates on just one operand the select operator operates on just one relation even if my database has many different relations  the select operator operates on just one relation and we have to some how make sure that when we are giving the select operator we have just one relation as the argument of this select relation and  each selection criteria the condition basically that ’ s specified is applied to each tuple separately the condition that we specified here was select salary greater than 3000 from employee now it ’ s going to apply this condition separately to each tuple basically it also means again that each tuple in a relation is independent of the other and the degree of the relation that emerges out of a select operation note that the output of a select operation is a relation in itself because it has just taken a subset of the tuples from the given relation and return them along with the schema so the input to the select operator is a relation and the output is also a relation and the degree of the output relation is the same as the degree of the input relation that is if the employee table in this example here had 4 different attributes  the output of the select operator also has 4 different attributes and in the same order as well however the number of tuples returned by a select operator is bounded by the number tuples that already exist in the relation that is this slide shows this as in a very compact fashion  the cardinality of the select output relation is less than or equal to the cardinality of the select input relation and the last properties shows that select is commutative which is again quiet interesting and important this slide shows here that if i select based on condition c1  an output of a select based on condition c2 for r  i can as well replace c1 by c2 and c2 by c1 and it doesn ’ t matter so you can verify that for yourself that the select operator is commutative whether it doesn ’ t matter in which order i am going to apply the conditions the second operator that we are going to be looking at is what is called as a project operator now select operator if you have absorbed carefully is going to return entire tuples  it is not going to modify the schema of the relation which is given as an input for example if the employee relation is given as an input and the employee relation has four attributes in some particular order  the same set of attributes in the same order is what is going to be returned by the select operator on the other hand what if we can select over columns rather than select over rows and return a different relation with possibly a different scheme in order to do that we are going to use the projector operator  refer slide time  00  34  33  the project operator as shown in the slide here is quiet similar to the select operator in the sense that it has first the command called project which is denoted by pi and a list of attributes  here it shows name  salary from employee so the output of this relation is again another relation however with a different structure from that of employee that means it is going to return just the name and salary attributes or the name and salary columns of the employee table as part of its output so we can also say that it has projected the employee relation on to the selected set of list of operation so the general forms of the project operation is simply of the form project  attribute list and relation so i can just give a list of attributes and the project operator returns relation in the same order that is being presented in the attribute list hence if i gave salary before name  it would also return salary before name in the tuple that is returned as part of the project operation so what are the properties of the project operation ? the first property is that which basically is one of the main properties of the relational model that is a relation may not have many duplicates  refer slide time  00  36  13   refer slide time  00  36  53  that means when i am projecting let us say i am just projecting name and salary attributes from the employee relation  it may so happen that there maybe two employees with the same name and the same salary but with of course different employee identification or employee numbers so the project operator actually would start forming duplicates in the relation that emerges out of this however duplicates are not alone so the project operations remove duplicates from its results when it returns results and the number of tuples returned by project is less than or equal to the number of tuples in the specified relation how can you verify that the number of tuples is less than or equal to  why not equal to because i am just asking for certain columns in the relation the answer to this lies in the first point that is there are no duplicates so when i selected just the name and salary attributes from the employee table it may so happen that their maybe certain duplicates that exist in the output relation now because the duplicates are removed  the number of tuples in the output relation is actually less than the number of tuples that forms the database  that was in the relation in the first place so when the attribute list of project includes the superkey then the number of tuples is same as the number of tuples as in the database which again follow from the first two points and the last point is again important  the project is not commutative so have a look at the slide once more  it gives an example project l1 and project l2 out of r if this were the case then this would become  this would be equivalent to just saying project l1 from r if and only if l1 is a substring of l2 so for example if l1 is just name and l2 is name  salary then i could just say project name from r instead of saying project name  salary from r on the other hand if i try to do it the other way around then it becomes an incorrect expression so i ca n't project name  salary after projecting just name from the relation composition  now this is another property of the relational model if you notice again carefully  we have mentioned this point in passing when we looked at both select and project but this is going to be very important now the input for the relational operator select as well as project is a relation and the output is also a relation the select operators return a relation containing only a subset of the tuples of the input relation similarly the project operator returns a relation which contains only a subset of the attributes of the input relation however both select and project returns a relation now this brings us to a very important property that a relation can be dynamically defined  it need not actually statically exist in the database that means to say that i can put a project operator to the output of a select operator and it would still make sense because the output is a relation and the project operator also expects a relation so this is what is called as composability of the relational operators so relational operators can be composed as shown in the slide here that which shows project name  salary as the outermost operator and there is an innermost operator called select  refer slide time  00  41  01  so if you can look at the slide again  there is a project operator here the project operator is operating obviously on a relation now what is this relation ? the relation doesn ’ t exist when the project operator is performed  in fact it exist only when the select operator is performed that is once the select operator finishes  it basically brings out a relation which is the set of all tuples where the salary field is greater than 3000 from the employee record or the employee relation and this set of tuples which is dynamically created forms the input for the project operator which is going to be another relation now because this is going to be another relation  it can be very well assigned to a relation called salary statements for example here so which says salary statement equal to project name  salary from where select salary greater than 3000 from the set of all employees now before we conclude today  we will just look at one major question which i am sure you would be asking yourself now the question is both project and select operators expect only one relation now does it mean to say that i can not ask any queries that span more than one relation  should i ask every query over just the employee record  over just the department record i mean the department relation or employee relation or so on can i not ask any question that spans employee and department relations together and so on  refer slide time  00  42  56  so the answer to this is the cartesian join obviously so essentially what we have to do here is because both select and project operators require just one relation  we have to some how ensure that even if you have more than one relation they all fall back or they all combine to form just one relation so we are going to look at one such very rudimentary operator to make just one relation which is namely the cartesian join in fact there are other much more efficient ways of combining two or more relations which we are going to explore in the next session so the cartesian join as you might have imagined is very similar to the cartesian product between two sets what is a cartesian product between two sets ? you just take each element of one set and combine it with each other elements of the other set so in the case of relations  you just take each tuple of one relation and combine it with every possible tuple of the other relation so the slide here shows such an example there are two tables here  one table is called student and the other table is called lab now student table has three different attributes roll number  name and lab and lab table itself has three different attributes that is name faculty and department that is the name of the lab  the faculty heading the lab and the department in which the lab belongs to now consider the relational query here shown below the tables  select student  lab equal to lab  name so that means to say that lab attribute from the student table equal to name attribute from the lab table from a cartesian product of student and lab that is combine student and lab to get it so what is the output of this relation or how is this relation or how is this query evaluated ? let us first straight away compute the cartesian join or the cartesian product of the two relations student and lab now what i have done here is i have take the student relation that is the student relation had 4 different students so for each student i have combined it with each possible lab tuple to form one big relation  refer slide time  00  45  16  so here it says note how the attribute names are changed that is it becomes student dot roll number  student dot name  student dot lab  lab dot name  lab dot faculty and lab dot department now the query that we require to match was student dot lab equal to lab dot name now all these quires that match are shown here in pink now it is these tuples that are going to be returned so the result of the query would be something like this that is where the student working in a particular lab is same as the name of the lab for whose record that we are maintaining so essentially what we have done here is that from two different attributes or two different relations student and lab  we have made just one relation by their cartesian product and then given it as just any other input to a select operator so it has become just one relation as seen in this slide here the last slide that should be looking here today would be what exactly are the properties of this cartesian join operator  just like we saw the properties of project and select  refer slide time  00  46  14   refer slide time  00  46  59  now the cartesian join represents what might be termed as a canonical join between two relations that is it just joins every relation from the first tuple to every other relation in the second tuple in the example here it just joined every student record with every lab record whether it made sense or not it was if a human being read the two tables  he would have noted that many of these joints do not make sense that is even though it says that the student works in a particular lab that record is joined with some other lab which has no relationship with what the student is doing so essentially if the number of tuples that come out of a cartesian join is actually the product of the number of tuples that exist in each of the relations that make up the join so hence cartesian join as you might have imagined is actually two inefficient for joining tables  especially if you note that one table has 10000 records and the other table has 1 million records and the cartesian product would be 1 million times 10000 records and probably the output would be something which maybe 10 records or so which is not clearly worth it so cartesian join is mainly of theoretical interest in the sense that this is a canonical form of join operator by which we can join two or more relation to form just one relation because each operator requires just one relation as its input so in the next session we are going to be looking at several other join operators especially what is called as the theta join operator and see how it is actually computed and how its going to be more efficient than the cartesian join operator we are also going to look at certain more  certain other relational algebra constructs and see how we can express the several different quires using the relational algebra so to summarize what we saw today the relational model is a data model for the internal schema of a database and the internal schema is something which is oriented towards optimized performance on a computer rather than human consumption that is something that is meant for human beings to see and understand and so on and the relational algebra  an algebra is some kind of formalism over which we can build sound software that is something that ’ s based on a mathematical formulism can be used to build sound software so comprises of operators that very elegantly take relations as input and produce relations as output and then you can start combining relations from one another and so on and we also just started to see how we can combine relations so that we can give just one relation as an input that is required by each of the relational operators in the next session we are going to be looking at certain more sophisticated forms of combining relations which are much more efficient both in terms of space and time required to compute these joins so this brings us to the end of today 's session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 4 relational model hello and greetings in today 's session we shall be exploring relational algebra into some more depth we started with relational algebra in the previous session with the definition that relational algebra is a data model that is based around the mathematical concept of a relation let us briefly review what we have learned about relational algebra before going further on into some more concepts in the relational algebra so just to revise what we have already seen  the relational algebra is based up upon the notion of a mathematical relation  refer slide time  00  01  48  a mathematical relation as we saw before represents a mapping between two or more domains essentially one relation represents a mapping between two domains that relates each element of particular domain to some other element of another domain so for example i might have the set of all student names being related to the set of all roll numbers and a mathematical relation is a subset of this mapping that is the set of all valid assignments between students and roll numbers so this forms underpinnings or mathematical underpinnings behind which relational model is based upon and as we also saw yesterday  the relational model is mainly meant for the design of the internal schema of a database an internal schema is meant or is meant or optimized towards machine consumption that is its optimized towards efficient storage  retrieval and queries of the data rather that human consumption that is trying to look at the data model and trying to understand what the data modal does or what the schema is all about so the building block of a relational model as we saw yesterday and it ’ s also shown in the slide here is a set of relations that contains a set of attributes and each attributes belongs in a certain domain for example the set of all students belongs to the domain of the set of all valid student names similarly the set of all age attributes belongs to the set of all valid age for a given employee or student or whatever so we also saw the notion of a relational schema which describes how this relation looks like that is what are all the attributes that makes up these relation and what are the domains of the attributes that make up these relation an each relation is dereferenced by a relation name and every instance of a relation that is every data element that conforms to this relation is called a tuple and each tuple in a relation is independent of every other tuple in the relation hence for example if i have one record of a student compressing of the roll number  name  date of registration  date of birth and so on that constitutes a tuple in the relation called the student relation and each tuple that is each record about student is independent of the other records of other students and a relation by itself is a combination of tuples plus schema that is given a particular schema and a set of tuples that conform to this particular schema is what is called as a relation as you saw in the previous class  a relation is the input and the output for most of the relational algebra expressions like select and project which we had seen and what are the properties of relations ? the first property is about the ordering or the lack of it that is tuples in a relational schema or in a relation need not have any order  they need not be placed in any particular order there relation is just a set of tuples that conform to a particular schema and the second property was about duplicates traditionally or the pure relational model does not allow for duplicates of a tuple which means to say that the each tuple is unique when you take a tuple in its entirety so we also made a statement that by default the entire tuples forms the super key for a tuple that is using the contents of a tuple  you can identify each tuple uniquely in the relation we are going to actually generalized on this concept today to see how things would change  if you can allow for duplicates of the tuples and whether it is required or whether it is desirable to allow for duplicates to exist in a relation we also saw that the relational model is defined by certain kinds of constraints and one of the first of which is the key constraint so we defined a super key as something which can identify a tuple uniquely and we also defined a minimal super key or a key which says that no subset of which is also a super key in itself and we also saw the notion of a candidate key where two or more sets of minimal super keys can be used as keys and the notion of the primary key and a foreign key when it comes to referential integrity constraints we also looked at entity constraints which says that the primary key of a relation may never be null so you can not have a tuple in which the primary key is null because null is not a valid value for an attribute it basically says that the attribute is not applicable that there is no value associated with null and we also saw basic retrieval operators select and project operators represented by the greek letter sigma and pi and select is an operator which selects a subset of tuples from a given relation without changing the schema of the relation that is the input relation and the output relation from a select operator have the same schema while a project operator selects in a sense  specific columns of the input relation that is it changes the schema of the relation without changing the data in the tuple it does it or it may change the number of tuples  if we mandate the fact that the output of the project relation has to be a set that is it can not be a bag or a multi-set that is each tuple from the output relation has to be unique so  in which case the number of tuples that are returned would be less than the number of tuples that already exists in the relation we also saw that both select and project operators require specific relation that is exactly one relation has input and provide exactly one relation as output now whatever it do when we have more than one relations on which we have to answer a query so we saw one possible solution to this that is to use the cartesian join or the cartesian product now how do we define the cartesian product over relations ? remember what is meant by the cartesian product over sets if you have two sets a and b  a cartesian product a times b or a cross b is the set of all mappings from all elements of a to all elements this is the same definition for a cartesian product for relations as well when we consider relations as simply set of tuples so a cartesian join between two or more relations is the combination of set of all tuples from every relation to every other relation so  a cartesian join as we saw yesterday is unnecessarily expensive in the sense that if i have m tuples in the first relation and n tuples in the second relation it needs to first compute m times n or m n number of tuples to generate a table which in turn goes as input to the select and project operators and from where selection has to be made clearly this is very inefficient for most operations on involving two or more tables so let us move on today to look at other forms of join operators which generate for lesser number of tuples than the canonical join that is represented by the cartesian join the first join operator that we are going to see today is what is called as theta join operator this as shown in the slide here  a theta join operator shows a join symbol which has essentially something like cartesian product symbol with two parallel lines and which has a subscript called theta which is again shown in the title here  refer slide time  00  09  26  so a theta join combines two or more relations or combines the tuples of two or more relations in a way that is specified by a join condition and the join condition is specified by the operator theta so the slide here shows a specific example where the same relation that we took yesterday the student and lab relation is computed using a theta join operator that is the slide shows student in a join operations with the lab condition that is the student relation theta join lab such that student  lab equal to lab  name so student  lab equal to lab  name is the joint condition and the theta join operator is the operator that is going to combine student and lab relations so this is the relation that is shown here and as you can see the output of this relation is the same as the select operator that we saw in the previous session that is select student  lab equal to lab  name from student time ’ s lab that is the cartesian join between student and lab the only difference here is that the condition for this cartesian join is specified as part of the join operator itself now how many tuples does this generate ?  refer slide time  00  11  20  let us have a look at the student and lab tables from the previous example the student table has four different tuples and the lab table has three different tuples now the cartesian join operator initially generated 4 times 3 that is 12 different tuples as input to the select operator  refer slide time  00  11  39  on the other hand  the theta join operator starts with the condition that student  lab equal to lab  name is a prerequisite for computing the join between the two tables so in this slide here such tuples where student  lab equal to lab  name is shown in a different color  are shown in pink so  as you can see there are only 4 such tuples that match this condition hence the number of tuples that are generated as input to this select condition is just 4 instead of 12  refer slide time  00  12  13  so how is the theta join computed ? well  the general answer is it depends it would be note that in this case  it would be most efficient to compute this theta join operator if lab dot name were to be a primary key of lab that is the lab is being referenced by the name and student  lab is a foreign key in to the lab relation and of course referential integrity is maintained so because we have to compute the equality student  lab equal to lab  name  all i need to do is take up each student  lab attribute and search for the corresponding tuple in the lab relation because it is a foreign key and lab  name is a primary key this search can be uniquely done efficiently using several techniques which we are going to see later like indexing or hashing and then you compute the join between the two relations  refer slide time  00  13  25  so the general form of the theta join relation is shown in this table or is shown in this slide it ’ s simply two relations with the join operator with a subscript join conditions  so the join condition is simply a logical expression over the attributes of r and s  which in the previous case we saw was the equality condition that is student  lab equal to lab  name it need not necessarily be equality condition  it could actually be any other logical condition like less than or greater than or so on now it may so happen that in some cases the join attributes may be null which is also true even if there is referential integrity that is there may be a lab  name which is missing from a student record now in such cases those tuples do not appear in the result that is whenever i can not combine two or more relations or whenever a particular tuple can not be combined with a corresponding tuple from the other relation  such tuples do not appear in the final result so some more properties of the theta join operator we saw a join condition now which sets student  lab equal to lab  name now the condition here or the logical operator here is the equality condition or the equal to condition  refer slide time  00  14  34  now theta join operators in which the only comparison operator that is used is equality condition is called an equijoin operator or an equijoin condition so what we saw actually was an equijoin and theta join is more general in the sense that it could mean any other kind of attributes now a special kind of equijoin is of particular interest and this is what is called as natural join between two or more relations so  natural join is denoted by star as shown in the slide here it is an equijoin where some of the attribute names between the relations that are participating in this join are the same now consider this example  consider again the student and lab example now let the student relation be modified like this that is the student relation has attributes name roll number  name and lab name instead of saying just lab similarly we have modified the lab relation as lab name faculty and department instead of just name now as you can see here  the lab name attribute or the name of this attribute is the same between the student and lab relations now if i say student star lab which denotes a natural join between student and lab relations  it returns me a relation which is of this structure that is the first three are the attributes of student and the last three are the attributes of lab and the middle attribute here lab name is the common attribute between student and lab now it is going to join only those tuples which where this common attributes match so  this is a special kind of equijoin operator where not only equality condition is assumed but its also assumed which are the attributes on which the equality condition is operated upon so the natural join simply takes attribute names which are the same in the two relations and then computes an equijoin over them  refer slide time  00  17  13  so the next operator that we are going to be looking at is what is called as a renaming operator we just saw here now that suppose we modify student as so and so or suppose we modify lab as so and so  we can just use natural join now can we formalize this notion of modification into the relational algebra itself ? can we introduce a notion into relational algebra by which we can say this relation is modified as this relation and then used in this expression and so on now one way to achieve modification is by assignment yesterday we saw that the input and output of select and project operators are both relations therefore the output of these operators can be assigned to a new relation name and then this becomes a relation by itself now this assignment can be further generalized such that we not only assign to a new relation name  we also assign the attribute names of the new relation so this slide shows the idea here  the first expression here projects the following statements that is it takes the student relation and then projects roll number and lab attributes of the relation now once you get a relation as an output  it is in turn assign to another relation called ta or may be teaching assistant with the roll number attribute name replaced by id and lab replaced by lab name so the output of this relation is another relation with its own name in this case ta and with its own attribute name that are different from the attributes names of the incoming relation now this renaming operator or this kind of renaming can be implicitly achieved without an assignment statement by using the rename operator which is identified by the greek symbol row which is shown below in the slide here so the statement here  the relational expression here shows that the same thing that as the assignment statement above that is it projects roll number and lab from the student relation and then computes a rename or gives them as part of a rename expression that says rename it as ta id and lab name so how does the rename operator work in general ? in general the rename operator contains or may be defined by the following properties this is shown in this slide here the first form of the rename operator shows that the entire relation  the input for the rename operator is of course a relation and of course the output is also a relation so the entire relation is being renamed that is it is given a name s and the attributes are given names b1 b2 extra until bn the second kind of rename expression that is row subscript s operated upon relation r  we will rename only the name of the relation that is the output relation is called s in this case and in the third case where row has the subscript of b1 to bn within braces or rather within parenthesis and the input is the same relation r  the renaming happens only on the attribute names and not on the table name itself  refer slide time  00  19  51  that is the name of the table or the name of the relation remains the same and the attributes are renamed to be called as b1 to bn in this case  refer slide time  00  21  08  the next set of operators that we will be looking here are the set theoretic operators again the set theoretic operators here operate on relations rather than specific sets as such now a relation is also a set but it could be multi-set or it could have certain differences when we are talking about joins and so on and so far so the set theoretic operations like unions  intersection  set difference they can be applied in the relational model as well now this set theoretic operation can be applied only to what are called compatible relations now what is meant by compatible relation between when we are considering two or more relations ? that is can i compute a union operator between let us say a student relation and an employee relation or a student relation and a project relation now some of these union operators may make sense and some of these may not make sense now that is why we formally define the notion of compatibility or union compatibility between two or more relations now what do we mean by compatibility ? the formal definition is shown in this slide here suppose we consider two relations a and b or in this case r and s and suppose they have a set of attributes let us say a1 to an and b1 to bn  they are compatible if and only if first of all you might have already noticed that the number of attributes are the same that is the r has set of attributes a1 to an and s has a set of attributes b1 to bn so firstly the number of attributes are the same  if they have to be compatible and the domain of every corresponding attribute is also the same that is the domain of a1 is the same as the domain of b1 if a1 can span the set of all valid roll numbers  b1 should also span the set of all valid roll numbers and so on similarly for a2 to until an  b2 until bn so the set of all corresponding domains are the same note that there is nothing here about the names of each operators roll number could be called roll number in relation a and could be called id in relation b  it doesn ’ t matter as long as the domains of each of these attributes are the same  we should be able to compute the set theoretic operations like union intersection and set difference now assuming that we have two or more relations which are compatible  how do we compute set theoretic expressions ? the union operator r union s which is shown in the slide here it simply returns the set of all tuples that are present in either r or s or both and of course without any duplicates that is any tuple note here that entire tuples are compared between r and s  the entire tuples where the corresponding elements have to be same or compared that is suppose i have one student relation or one record about a student in r and another record about a student in s  it just combines both of them that is the output of r union s is the set of tuples that lie either in r or in s or both and of course without duplicates similarly the intersection operator returns the set of all tuples that are present in both r and s which is same as this intersection operator on sets and similarly the set difference operator r minus s returns a set of all tuples that are in r but not in s so the set of all tuples that are unique to r but and not present in s would be the output of r minus s so we can note that this standard properties of set theoretic operations also apply here that is union and intersection are commutative r union s equal to s union r and r intersection s equal to s intersection r however the set difference is not commutative  s minus r is not the same as r minus s so we shall be coming back to these set theoretic operators again when we relax the fact that a relation may not contain duplicates  now what happens if you allow for duplicates in the tuples the next operator that we are going to be looking at is the division operator the division is a slightly unintuitive operator in the sense that it needs a little bit of explanation to understand what or where a division is going to be used a division operator is essentially used in cases where we may have to identify data elements that are associated with some other data element whenever the other data elements occur that is for all properties of the other data elements  refer slide time  00  25  43  have a look at this slide here which shows a particular example now firstly there is the definition here the division operator is used to denote conditions where a given relation r is to be split based on its association with every tuple in another relation s let me go straight to the example here and then go back to the explanation of division the example shows two relations r and s the r relation has two attributes a and b and the s relation has just one attribute a firstly the division of r and s is going to return the attribute b that is the r divided by s is the set of all attributes b such that there is some relation between the attributes in a so what is that relation ? the set of all attributes b contained in r that are associated with all values of attribute a of s that is suppose let us take the example of b1 is b1 associated with a1 in r ? yes it is  a1 b1 is here is b1 associated with a2 ? yes it is  a2 b1 is also here and these are the only two values in s hence b1 is a valid result in t equal to r divide by s that is every data element here that is associated with every other data element in the other relation is what is going to be returned as part of this relation consider an example something like which employee has worked with some other employee on all projects that he has worked let us say which employee has worked with some employee named arun or something on all projects that arun has worked so suppose these were all the project that arun has worked and there is this employee b1 who has also worked in all these projects  you are going to get b1 as an output consider the case of b2 here  now b2 is associated with a1 but it is not associated with a2 there is some a3 with which it is associated hence b2 is not part of the result but b3 which is again associated with a1 and a2 it is part of the result so the division operator in some sense divides the first relation based on which data elements in the second relation in some way completely divides that  that is associated with all data elements of the first relation we are going to look at an example later on where division operator is going to be used and what is the power of the division operator  refer slide time  00  29  12  there are also other relational operators which are also called additional relational operators which we shall briefly mention and have a look at their properties we are going to mainly look at two such operators namely the outer join and the outer union operator now  going back to the notion of join  recall that join or a theta join operator takes a join condition as one of its input now suppose any of the attributes which match condition is null then such tuples are not further processed at all that is they are just thrown away from the relation however in some cases it may be required to compute all possible joints even when the join attributes are null and this is what is called as outer join so consider two relations r and s  now suppose let us say for the sake of simplicity we have defined a natural join between r and s now every attribute in r which is participating in the joint should point to or should refer to an existing attribute in s that is it should not be null and the attribute that it refers to should exist in s  only then the natural join can be processed however there can be two possible scenarios  the left outer join that is suppose the attributes that are participating in join the attributes of r that are participating in join are null the left outer join includes such tuples even when the attributes that are or the referencing attributes are null similarly the right outer join includes tuples even when the referenced attributes does not exist so it basically replaces them with nulls and then just includes that part of the tuples that is that part of the tuple from r which has data in it and the rest of the data elements would be null so it is some kind of a union or a canonical  well i shouldn ’ t say canonical but some kind of a union operator where you include every tuples any way  its some kind of an inclusive operator or inclusive join operator where it includes tuples any way even when the corresponding attributes are null on the lines of outer join we can also define outer union operator now we saw that the union operator that is union between or union or intersection or any set theoretic operators between two or more relations can be performed only when they are compatible now what did we define as compatibility as ? the compatibility was that both of these relations should have the same number of attributes and the domains of the corresponding attributes should be the same in both these relations now the outer union is basically a relaxation of this constraint and the example shows two relations r x and s z that is x is a set of all or the list of all attributes of r and z is the list of all attributes of s now suppose x and z are not compatible  however a subset of x and z are compatible that is w a subset of x and y are subset of z are compatible now a union or an outer union operator computes the union based on this compatible subset of these relations and then simply includes all other relations or all other attributes as they are in the relation so similarly the outer union operator is simply some kind of an inclusive union operator that includes tuples or works on relations even when they are not union compatible or even when they are not perfectly compatible between them so theoretically speaking there are  do we need all these operators or can we express one operators from other or has a basis of using other operators this slide shows what is called as the complete set of relational operators  refer slide time  00  33  49  now as you can see in the slide here  there are the following operators select  project  union  set difference and the cartesian product now this set of operators is called the complete set of relational operators because every other kind of relational operator can be expressed as a sequence of the above operators i am just giving two examples here but you can verify that for yourself taking each operator and trying to express it as a sequence of the other  one of the complete set of relational operators for example r intersection s can be or is equivalent to the r union s and the difference of r minus s union s minus r so let us not go into the set theoretic operation to prove this equality but i am sure this is quiet obvious that you can express the intersection using union and set difference similarly joins can be expressed theoretically using a select condition over cartesian products we already saw that in the example there were student  lab equal to lab  name or in this case here r join condition s is the same as select condition over r times s the first one here should be the join symbol and r join condition s is equivalent to select condition over r times s  refer slide time  00  35  25  now we are going to look at making one generalization over or trying to relax particular constraint on relational expression or on relations now until now we have been saying that relational operators should take relations that are sets and not multi-sets and return relations that are also sets and not multi-sets however in some cases it might be necessary or even desirable to allow for the existence of duplicate or duplicate tuples in in relations both in the input and output relations now first of all such relations or such sets where elements can occur more than once is called a bag or it ’ s also called a multi-set so bag or a multi-set is a set that may have multiple occurrences of a given element as we can say it ’ s a generalization over the present notion of a set that is every set is a multi-set in which each element occurs exactly once however a multi-set is something where an element can occur more than once now in some cases bags are actually necessary in the relational models not only desirable but also necessary when are these conditions ? consider the case that we are querying the database to compute the average marks obtained by all students in a particular semester so we have a student relation in which there is one of the attributes which is called as marks now we project this attributes saying project marks based on student now based on the set of all marks that it has projected  we compute the average mark by computing the sum of all these marks and divided by the total number of entries that are there now in this case if duplicates were actually to be removed  we can not compute the average in a correct fashion we will actually be losing information when we change the multi-set or when we remove duplicates and make it into a normal set so when we are computing aggregate relations like sum and average  we actually need multi-sets or duplicates in the relation  the duplicate should not be removed from the output relation similarly if we can tolerate duplicates in the relations somewhere while evaluating relational expressions  it may make things much faster for example every time i return the output of a project operations  i may have to spend considerable amounts of time trying to looking for each tuple and seeing whether there are duplicates of this tuple in the output relation so especially when computing projects and unions duplicate removal may take a significant amount of time now if i have a relational query that has several project operations and union operations and are embedded somewhere deep in the query  it may be terribly inefficient or to be computing or to be eliminating duplicates every time we compute project and union operator so  sometimes it may be necessary or it may be desirable to tolerate bags or to tolerate multi-sets as part of a relation as part of an intermediate output in a relational expression now how does this generalization from sets to multi-sets affect relational operator ?  refer slide time  00  39  09  now consider two bags r and s now in the first bag let a tuple t occur n number of times and the same tuple t occur m number of times in r in s so how do we define set-theoretic operations based on these bags ? the first operation that we define is called the union of bags or it ’ s also called the disjoined union of sets and it is denoted by r plus s you can also denote it by r union s when we are sure that r and s are bags and not sets so its simply denoted by r plus s and its simply contains r plus s that is the cardinality of r plus the cardinality of s number of tuples that is both bags are simply combined  every tuple in s is combined with every tuple in r and that ’ s it that is it has m plus n occurrences of this tuple t which has been repeating now when you are compute the intersection of bags  r intersection s what happens to tuples that occur multiple times ? so tuple t which occurs n number of times in r and m number of times in s occurs only a minimum of m and n number of times in r intersection s so as you can see the generalization here that is in sets  a tuple will appear in r intersection s only if it appears in both r and s here it will appear the minimum number of times it appears in both r and s similarly the set difference between bags  the set difference between r and s  r minus s is where the tuple t occurs n minus m times that is n number of times it had occurred in r and m number of times it had occurred in s so the number of tuples that is going to occur in r minus s is n minus m  if and only if n is greater than or equal to m if n is less than m then the number of times is going to appear is zero  of course it ca n't appear negative number of times so tuple appears does not appear  if the number of tuples in r does not out number the number of tuples in s you can think of it has something like canceling out tuples from r and s so for every tuple in r  we cancel out ever tuple in s and then see how many tuple are remaining in r after we have cancelled out all tuples in s and that is the number of tuples that we are going to take in r minus s  refer slide time  00  41  45  now what is the other operators on bags ? the select operator does not change  the select operator operates the same whether it is on sets or bags you just take a select condition and apply it to each tuple in the input relation regardless of whether the relation is a set or a bag similarly the project operator becomes simpler it does change but it becomes simpler that is a project operator simply takes the requested columns or requested attributes and gives them out  there is no need to eliminate duplicates similarly the cartesian product of bags  it ’ s also the same thing it is if a tuple t occurs m times in r and n times in s as before then the tuple t s that is t combined when with s occurs mn number of times that is every tuple in r is combined with every tuple in s regardless of how many times they appear in r m  in each of these relations  refer slide time  00  42  45  however there are certain algebraic expressions on bags that do not hold when the relations are tuples have a look at these algebraic expressions that are shown in the slide here the first algebraic expression is the distributivity over the set difference operator that is r union s minus of t is the same as and is equivalent to r minus t union s minus t you can easily verify that this expression is true when r s and t are sets however this expression is not true or does not hold when r s and t are bags and not sets  when they are multi-sets now why is it not true ? let us take an example  this is called the 1-1-1 principle now consider that a tuple t occurs exactly once in r s and t there are some particular tuple or some particular data element that occurs exactly once in r s and t now when we compute r union s  in the resulting set this tuple occurs twice if it is a multi-set or if it is a bag now once we consider and then once we compute r union s minus t  this tuple which we are considering now would have occurred once that is 2 minus 1 number of times  once it could have occurred on the other hand look at the right hand side here when we compute r minus t  this common tuple which had occurred exactly once is not going to occur in r minus t  it ’ s going to occur zero number of times similarly in s minus t this tuple vanishes  it occurs zero number of times so in the union between r minus t and s minus t  this tuple does not exist at all whereas there is one occurrence of this tuple in the left hand side of this expression so this expression does not work when r s and t are bags and not tuples and not sets take this second expression the distributivity over intersection and union that is r intersection s union t is equal to r intersection s union r intersection t this is of course easily verified when r s and t are sets however this does not hold when r s and t are bags and this can again be easily verified by considering a specific counter example which is called the 2-1-2 principle i will not be going into detail into the 2-1-2 principle  you can use the same argument as we have used in the first case where we took the 1-1-1 principle that is consider a tuple that occurs exactly once in r s and t here consider a tuple that occurs exactly twice in r  once in s and twice in t and see what happens and see if the left hand side of this expression is equal to the right hand side of the expression as far as this tuple is concerned and you can see why this relation does not exist  does not hold or this equality does not hold the third expression is also significant when we are considering bags and not sets expression gives a select operator that is select c or d  c and d are some conditions over attributes of over r  so i am selecting c or d over r that is select any tuple where either c or d or both holds and give me all those tuples now if it were a set that is if r where to be a set  i can rewrite this as select c union select d that is select c over r and union it with select d over r but the same thing does not work when r is a bag and not tuples now again you can take a very simple counter example to show that this is the case now consider a particular tuple where both c and d are true now that tuple is going to be return only once in the left hand side of this relation but this tuple is going to occur twice in the right hand side of this relation because its going to be returned once from c and once from d and when we are taking a union or disjoint union  we are going to just add up both of them and its going occur twice in this relation hence this does not work when r is a bag so tolerating bags is not only desirable but sometimes also necessary however bags pose their own unique problems  unique issues when we are considering set theoretic operations and algebraic expressions over bags and which we have to keep in mind when we say that when we either decide to tolerate bags or not tolerate bags in a nut shell we have covered quiet a few of quiet a significant part of what constitutes relational algebra expressions and what constitutes or how to write queries in relational algebra in the next three slides let me give a small example of relational algebra queries and how queries can be composed from one another so this slides shows a very small database schema comprising of 5 different relations employee  department  department locations  projects and works_on so employee is a relation that talks about details of an employee  it has the first name  middle  initials  last name  the employees pan number  date of birth  address  gender  salary  the supervisor of that employee and the department number where the employee works and the department contains department name  department number which is the key here  all primary keys are shown underlined and the pan number of the manager and the start date of the manager  refer slide time  00  48  18  department location only shows each department number and the location where it is located and project shows project name  project number  location and the department number where the project is working and similarly works_on talks about this employee works on this project for this number of hours and so on so let us take some typical queries very quickly and go through how we can answer these queries query one  the first query which we are going to consider says that retrieve the name and address of all employees who work for the research department so how do we answer this query first of all we take the set of all tuples that form the research department that is select dname equal to research from department  the set of all tuples which are the research department now compute which are the set of all employees who works in the research department ? how do we compute that ? compute a join between research department and employee where the department number is the dnumber recall that in the employee record there was a dnumber here which showed which is the department number where the employee work  refer slide time  00  49  24  so compute a join  an equijoin where this is this now from this we have got all details of employees who work in the research department  from that we need only the first name last name and address because that ’ s what the query asked that is the name and address for employees  so project as a last query query two  find the names of all employees who work on all projects controlled by department number 5 so have a look at this query again we want the names of employees who work on all projects that are handled by this department so how do we go about answering this ? first of all let us find out what are all the projects that are being handled by department number 5 so department 5 project is the name of the relation which says project  the project number and select from project where department number equal to 5 and project only the project number then which are all projects that employees work on ? take the pan number and the project number and then work on this now what we have to do is that we just have to compute a division between employee project and department 5 project which basically gives us the set of all pan numbers of employees who work on all projects or who are associated with all projects of department 5 which is the result  refer slide time  00  50  39  that is we in turn use that to and combine it with the employee record to return the first name and last name of employees so in this way we can  as you can see here for any given query we usually need to perform a series of operation  series of relational algebra operations before we get to the final result  refer slide time  00  52  13  so let us summarize what we have learnt today in a brief fashion so we saw the definition of relational schema  the notion of a relation  domains and attributes and the characteristics of relations especially with considering duplicates and ordering of tuples and so on we also saw the basic relational algebra retrieval operations that is select and project and so on and set theoretic operations and relations and also how this set theoretic operations change  when we relax the notion of the relation from being a set of tuples to a bag of tuples when we can allow for duplicates we also saw why in some cases  it ’ s not only desirable but also necessary to use bags we also saw how we can  given a particular user requirement how we can go about formulating a relational algebra query in a step by step fashion so that brings us to the end of this session on relational algebra thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 5 structured query language hello and greetings in the ongoing saga of management of data  we have covered the aspects of high level schema design using entity relationship modeling and one kind of a data model called the relational model which is mainly used for designing of low level schemas today we are going to look at something more concrete that is something which you are likely to be using on a day today basis  if ever you are going to be working in databases especially if you are a database user or database administrator  this is the sql language or the structured query language  refer slide time  00  01  54  the structured query language is the standard query language that is used by most database management systems that are available today and this is a language that is definitely required to be known by anybody who is going to be using a database management system so let us move into structured query language today  refer slide time  00  2  19  now what is meant by a standard for query languages ? why do we need a standard for query languages ? i just mentioned that the structured query language is the de facto standard or it ’ s rather these standard for relational databases that all databases or most of the database users today use sql in some form or the other why do we a standard query language for database management ? now today at this point in time  in the database arena there are large number of database management systems that are currently available some of them are commercially available  some of them are freely available and some of them have been implemented in academic institution  some of them have been implemented in companies  industries and so on we have oracle  we have ibm db two  we have sybase  microsoft sequel server  mysql  postgres and what not there are so many different varieties of databases now suppose we were to have a different query language being adopted by each different database management system  it becomes extremely difficult if not impossible to be able to port application program from one database to the other we will have to write software that specifically uses let us say mysql or software that specifically uses oracle and so on however with a common query language like sql  all that the application programs need to know is sql they should be able to speak sql and using sql we should be able to connect or use any database management system so this is what is depicted in this in this slide here suppose you have several different database systems  in fact in some application context you may have several dbms within the same application context a huge company for example may use several different kinds of dbms systems in their different branches  while all of them have to deal with the same application  same work flow procedures or same business logic or business processes and so on so suppose we have several different databases or dbms as shown in the slide here and suppose all of them are able to speak a query language like sql  all that the applications need to do when there are different application says that they should be able to speak sql and they should  it doesn ’ t matter which dbms they are going to be working but reality obviously is much more complex than these and while this was the reason why a standard for database query was introduced  it is nowhere near to achieving its objectives that is in reality however we still see applications that say this application is meant for mysql  this is meant for oracle database  this is meant for db two and so on because there are number of additions  there are number of other features in addition to sql that are unique to each different applications and so federating or working across different  connecting different database system is a completely different question all together now and data integration is a completely different question which we would be exploring in much more detail in a later session so coming back to sql  sql stands for structured query language and this was introduced the genesis of sql was at ibm research for database system that they had built called system r just like seminal paper for cord  you can give an internet search for a system r and there are of course some very good papers that are been  that are available on the net that talks about earlier days of sql as you will see today  sql contains a lot of constructs like select and set operators and cartesian products which are quite similar to relational algebra that we saw in an earlier session and sql also works on a relational data model very similar to relational algebra  refer slide time  00  05  57  however sql has no relation to relational algebra or very little relation to relational algebra in itself the mathematical foundations for sql is another data model called the tuple relational calculus which is also another data model based on the relational data modal that is the notion of mathematical relations but there are some similarities in terms of terminology between what we have seen in relational algebra and sql and sql is a database independent language and in this session  we would be looking at construct from the sql two standard and sql three standard has some more facilities that are provided and sql is both a data definition language and a data manipulation language what is it mean ? a data definition language or a ddl essentially defines data elements that means what are the data elements in a relational model ? of course a relation that is relation  attributes  domains  values  constraints  keys and so on so all of these can be defined using sql using sql you can define a relation  you can define its attributes  you can define its domains  you can define constraints across attribute  relationships  key constraints and entity constraints and so on and it ’ s also data manipulation language in the sense that you should be able to add more data into a database  you should be search for some data  you should be able to retrieve some data  you should be able to modify data elements and so on so it ’ s both ddl and a dml  refer slide time  00  8  30  so  in terms of terminology while we use the terms relation  tuple and attribute in relational algebra usually the terms table  rows and columns are used for the corresponding terms we shall be using either of these terms interchangeably as they mean the same thing whether it is a relation or a table  we saw in the session on relational algebra that relation can be represented in the form of a table where each attribute of the relation is a column which is what is returned by the project operator and each tuple in the relation is actually a row which is what is returned by the select operator the sql two standard defines methodologies for introducing schematic structures into our databases a schema is created by what is shown here as the create schema command so the slide here shows a small example which says create schema univ authorization dean  so it creates a schema called univ which is owned by the user with a user id called dean  refer slide time  09  21  we shall be looking into this authorization in much more detail when we are talking about security and authorization in databases where different users of database management system are usually given certain privileges and certain authorizations which authorizes them to do certain kinds of data manipulation operations on the database so hence if the owner of this schema is this user id called dean then a dean is given certain kinds of authorization that is defined by some default values which can also change here  which would typically include adding a table  deleting a table  adding rows deleting and so on i mean any kind of activities that a typical owner of a database would do a catalog in sql terminology is a named collection of schemas so suppose i have a collection of different relations and combine them within particular name  this is called a catalog and a catalog is required or it is required for different tables or different relations to be within the same catalog if i have to be able to enforce referential integrity on my schema so using a scale you are able to enforce referential integrity only within tables that lie within the same catalog  refer slide time  00  11  19  creation of a table  as you know table is another term for relation and a table can be created with the command create table command  just like create schema for schema so the slide here shows an example create table command of the form create table name and some kind of column descriptions the parts of the syntax enclosed within box braces are optional structures that means you can refer to the table name when the context is clear or otherwise you need to identify the schema within which the table belongs you can say something like create table univ.department that means create table called department under the schema called univ there are also certain column descriptions which we are going to see shortly which can say what are the different attributes that form the table and what are the domains of each of these attributes and what kinds of constraints that each of these attributes have so before we look into column descriptions  let us say what kinds of data types are also domains that sql two supports there are several different kinds of domains that sql two supports and some of the most commonly used domains are as shown here there is the numeric domain which is identified by different data types called int  small int  float  real and double precision  refer slide time  00  12  30  each of these refer to different sizes that are of the data word that is being stored as part of this domain there are formatted numbers like decimal i  j which basically means that this number has i number of digits of which j number of digits occur after the decimal point hence if i say a decimal 8  2 it means that there are a total of 8 digit in this decimal of which two digits  the last two digits occur after the decimal point you can also define character strings of either fixed length which is shown by char of n or of varying length which is defined by varchar of n so i can say that this attribute name is a varchar of say 60  so that means that this attribute can store a string whose length can vary anywhere up to a maximum of 60 characters then there are also big strings that you can store either a fixed length or varying length and similarly a special data types like date  time and time stamp and text and several other data types like binary objects and so on but typically we would generally be working with numeric or character string in a typical transactional database system like employee records or railway reservation or whatever the standard transactional databases that we would be considering here  refer slide time  00  14  34  you can also create your own domain  name domain by using the create domain construct the slide here shows an example which says create domain roll number type as int of 6 if i know that the roll numbers that i give to students in our institute would be an integer having 6 digits  i can as well use roll number type instead of int of 6 wherever i need to store student roll numbers the advantage with this is that tomorrow if this domain definition is changed  i need to change it at only one place where i am defining the domain called roll number type there are also certain constraints and default values that you can specify using the create table command which you can specify on an attributes for example the not null constraint  the first constraint is that we are going to take up is a not null constraint which says that this attribute name for which i am placing this constraint can never have a value called null so it disallows null as a valid value for this constraint for example i can specify that the age of an employee may never be null that means i need to always have a valid age for an employee record that is entered in this data value  if i have to insert a particular row into the database  refer slide time  00  15  17  similarly i can use the default construct  the default construct shows what would be the default value for a particular data item so if the not null is specified and i also specify a default value  whenever let us say i specify that age of an employee not null and default 18 so whenever the age of an employee is not known  the default value namely 18 in this example is put in its place then there is the primary key constructs  the third constraint in this slide which specifies one or more attributes of this tables as the primary key of this record similarly there is the constraint called unique  i can say for one or more attributes i can say unique which essentially says that this attribute has to have unique values or distinct values for each row of this table in other words it means that it is an alternate key or a secondary key note that a key is something which can uniquely identify every tuple hence the key has to have a unique value for each tuple in the relation similarly i can also specify a foreign key construct that ensures referential integrity especially when some part of a table is actually referring to some other parts of another table  i can specify them as foreign key in order to ensure that they refer to existing aspects of the other table so just to brush up what is referential integrity  if i have some aspects of my table referring to another table it should either refer to an existing tuple or existing row in the other table or it should be null that is i should not refer to any row in the other table or i should refer to an existing row in formal terms this is what is meant by referential integrity  refer slide time  00  18  16  so here is an example of table creation this slide shows an example table which says a create table employee  the name of the table or the name of the relation is called employee and then there is a set of attribute descriptions the first attribute called pan number  the permanent account number of an employee is given a varchar that is variable character string of 16 letters and it is shown as not null and it is shown as unique that is this pan number may never be null and it has to be unique across each employee that is each employee has to have a distinct pan number if you go down the slide you can see that there is another construct called primary key and emp number that is the second attribute employee number which is also shown as not null is termed as a primary key hence the pan number in this case because it ’ s unique can form a secondary key or an alternate key of this relation then there are other attributes like name  gender  date of birth  address  salary and reports to which basically shows the employee number of the manager or the person to which those employee reports to so you also say another constraint called foreign key which says that reports to  the field called reports to is actually a reference to another record belonging to the same relation called employee and referring to the employee number in this record therefore whenever i enter a table and i enter a particular employee number for reports to  suppose i enter details of an employee and enter the details of the manager by specifying the employee number of the manager to whom these employee reports to then because of the foreign key constraint that is specified here the database management system will verify whether a record for the manager already exists if the manager record does not exist that is the employee with that reports to employee number does not exist then addition of this employee record will fail as part of the database management deletion of tables  tables can be deleted  the terminology used for table deletion is called drop so tables can be dropped using the drop table command  this is shown in this slide here  refer slide time  00  21  05  you can use  the syntax is something like this drop table and name of the table and there are certain optional attributes which says dependent cascade or restrict so the first three terms are obvious  drop table name that is the table name by the given name should be drop what is the dependent clause or what is it do ? so if dependent is termed as cascade if i say drop table employee dependent cascade  then any foreign key constraints that the table holds or views that reference the table  well we have not come to views as yet but for the moment let us just not consider that but when i drop a table let us say employee  any foreign key constrains will also be dropped so the dropping is in some way a cascading process because of table employee  any foreign keys that the table references to or all going to be dropped in a cascading fashion on the other hand if the restrict option is specified then a table is dropped only if it does not have any references  incoming references that is only if nobody references the table only then will a table be allowed to drop just like table deletion  you can also delete an entire schema using the drop schema command this is shown in the slide here the drop schema command also has a very similar syntax  it is like drop schema name and either cascade or restrict so drop schema and name is obvious that is you have to drop a schema by the given name and if the cascade option is provided then all tables that are there in the schema will be automatically dropped on the other hand if restrict option is specified  then a schema is dropped only if it does not have any table or if it does not have any elements how do we modify tables ? how do we alter an existing table definition ? so table definitions can be modified using what is called as a alter table command note that alter table or modification of a table is modification of the schema of the table not the data in the table that is we are not modifying existing data elements or adding or deleting data elements from and to the table  we are actually modifying the table definition or the table schema  refer slide time  00  23  04   refer slide time  00  23  12  the table schema is simply a definition of the set of all attributes that form the table and their domains so the slide here shows an example which says alter table company dot employee add job varchar 20  so it essentially adds a new column to the employee table with the name called job and domain varchar that is a variable character string with a maximum size of 20 characters now suppose the table that i am altering has already containing is already containing some data that is i have created a table  i have added certain data elements and i have used the database for sometime and then suddenly i give an alter table command and say add a new column like job now what happens to  what value should job get in all of these tables ? because i have not specified any value as part of the alter table command you might have guessed it that it is going to be given a default value of null so a new column called job is going to be created with a value of null but what happens if i specify a constraint called not null ? suppose the slide here reads alter table company dot employee add job varchar of 20 not null now what should be the  what should be the value that has to be filled in for this new column ?  refer slide time  00  25  28  the answer to this is in the next slide here now unless a default condition is specified  unless a default value is specified you can not use the not null constraint  as simple as that that is if i do n't use a not null constraint and the query is just like this that is which says alter table company dot employee add job varchar 20  it just adds this new column with all null attributes on the other hand if i wanted to specify not null then i should also use a default value which is what is going to be filled for all the data elements in this new column so i can say default employee or default shop floor or something like this so the default value called the shop floor is going to be filled for all of the elements in this new column which should later be change for specific rows using some other command which we are going to see later on  refer slide time  00  26  31  now we just saw how to modify a table by adding a column what if we need to delete a specific column ? the syntax is again quiet similar to that of deleting tables we use the key word called drop  so this table here shows an example alter table company dot employee and as it to drop the column called pan number and there is an option called cascade now drop pan number is obvious that is the column called pan number is going to be dropped if the cascade option is used then all constraints that refer to this column are also dropped automatically that is if some other column refers to this column as in the form of a foreign key or a view or so on then they are all going to be dropped automatically  it ’ s a cascading process similarly if restrict is used then a column is dropped only if there is no incoming references to this particular column  refer slide time  00  27  41  it is also possible to alter a column definition rather than just adding and deleting new columns column also has a particular domain and a particular constraint set of constraints that are associated with it now it is also possible to add and drop these domains or constraints that refer to a column and this slide here gives certain examples the first example shows alter table company dot employee and in turn says alter reports to that is alter the column called reports to set default as 007 so what is this do ? this basically says that wherever the reports to  wherever the reports to column is null set it with the default value called 07 so whoever whichever employee does not have any manager  assign him to the manager called 007 which is what is being set by this command the second command shows alter table company dot employee  alter reports to drop default which is basically the other way around that is suppose it already has a default value then all those rows where this column has the default value are set to null and the default value is going to be dropped for this column  refer slide time  00  29  11  we now come to the main operation in sql  the most frequently used operation for retrieval of data elements from um from tables which is called the select operation we have not really seen how to add data into a table as yet but let us first see how to retrieve data from a table and then we are going to consider how to add or modify data elements to and from a table so the select operation is a most detailed operation in sql and is the most frequently used operation and it has the variety of forms which we are going to see in a step by step fashion so the sql operation is the basic retrieval operation  the select operation is the basic retrieval operation in sql  it has no relationship with the select operation in relational algebra just to reemphasis the point that sql is actually based on tuple relational calculus and we are going to see here that the select operation of sql can perform both select and project that are defined in relational algebra and sql select one major difference between sql select and that of relational algebra is that it considers relations as a bag we saw in the session on relational algebra that by default relational algebra expects relation to be sets and when we convert them to bags we have to take care of certain algebraic conditions with sets which does not necessarily hold for bags but sql by default considers tables to be bags and not sets there may be multiple occurrences of the same tuple  refer slide time  00  31  05  the basic syntax of a select operation is shown in this slide here it is very simple  it says select attribute list from table list where a given condition so there is an example which shows here select employee number  name that is the list of attributes emp number  name from employee which is the name of the table where reports to equal to 007 which essentially means that show me all employees  that is give me the employee numbers and names of all the employees who reports to a manager whose employee number is 007 the select from where is the basic operation that we saw here and it can also act on multiple tables  it need not act on a single table until now we have being considering one single table called the employee table now let us work with two tables just to show that select can act on multiple tables  refer slide time  00  32  03  now let us first define a new table called department so this slide shows the definition of department that is create table department and which says where the first attribute is called dnumber which is also the primary key  the department number which is int of 6 and not null and name address and head that is name of the department  address of the department and the head  the employee number of the person who heads the department now it also retrace the fact that head is a foreign key that refers to employee number from the employee database or from the employee table consider the following query what is the name of the person who heads the supply department ? if you look back at the definition of department  we have seen that the department contains department name  address and head which is the employee number it does not contain the employee name but the query here requires the name of the person who heads the supplies department so the name of the department is supplies and we require the name of the person  so this can be specified by small sql statement like this  select employee dot name from employee and department where employee number equal to head and department name equal to supplies  refer slide time  00  32  51  as you might have imagined this is quiet similar to performing a relational algebra select on a cartesian product of two tables in this case there is a cartesian product of two tables employee and department and we are stipulating the fact that employee number equal to head in this cartesian product that is considering only those tuples where the head of the department corresponds to the employee number of record in an employee and the department name equal to supplies and also note the use of that table name in order to disambiguate attributes having the same name now even the employee table  in the definition of the employee table the name of the employee is specified by an attribute called name but in the same way in the department record as well  in the department table the name of the department is also de referenced by an attribute called name now when we say select name  which do we mean ? do we mean the employee name or the department name ? in order to disambiguate this we can prepend the name of the attribute with the name of the table so the query here says select employee dot name rather than saying just name and then also in the where condition where department dot name equal to supplies so for some strange reason if some employee is called supplies that should not be matched  its only the department name which should be matched against supplies however this disambiguating attributes by prepending them with the table name is not always sufficient  refer slide time  00  35  40  consider the next query here now the query here says what is the name of the person to whom arvind kulkarni reports to ? now here is an employee with name called arvind kulkarni and he reports to some person now we need to know the name of the person note that in the employee table  we only have the employee number of the person to whom each employee reports to so obviously we need to have a join of the employee table on the employee table itself that is you have to have a self join for the employee table so suppose we write a query like this that is suppose we try to write or we try to disambiguate attribute names by putting the table names before them so such a query shown here that is select employee dot name  so we need employee dot name from employee  employee because both employee and manager are both employees so it ’ s a select employee dot name from employee  employee where employee dot name equal to arvind kulkarni and employee dot reports to equal to employee dot emp number obviously you see that there is something  there is quiet a bit that ’ s wrong here you do n't know which employee table are you referring to  is it the first employee table or the second employee table ?  refer slide time  00  37  14  so this is still ambiguous so in order to disambiguate this attributes names in such a situation  sql provides as with the opportunity of using what are called as aliasing so aliasing can be used as follows now consider the same query shown in this slide here so this slide for the time being concentrate only on the second line of this slide where it says from employee  employee as boss so the entire query is like select boss dot name from employee and employee as boss so essentially what it saying here is that take the first table employee and the second table employee  however use an alias called boss for the second table so we know whether we are talking about an employee or his boss and then we say employee dot name where employee dot name equal to arvind kulkarni and boss employee number is the same as the employee reports to number  refer slide time  00  37  36  so employee dot reports to equal to boss dot employee number so in this case we will be able to identify which name are we referring to from which relation  refer slide time  00  38  44  suppose we omit the where clause in the select from and where syntax and we just give a query of the form that is shown here that is select name  pan number from employee what is going to be the output of this ? as you might have imagined this  such a select statement is similar to the project operation in relational logic so what this statement does is it returns all rows in the table called employee  however only the columns name and pan number so it is similar to saying project name and pan number from employee and what happens when the where clauses omitted and instead of saying just one table name  we actually specify more than one table name this is shown in the query here it says select employee dot name  department dot name from employee  department that ’ s it  refer slide time  00  39  43  now what happens here in this case of course we get only two columns as output that is employee dot name and department dot name  however we get all possible combinations of employee dot name and department dot name in other words we have computed a cartesian join or a cartesian product between employee and department with this operation  refer slide time  00  40  07  suppose we want to select all columns of particular table that is similar to the select operation in relational algebra we want to select all or entire tuples and based on certain conditions  in such a case you can use the term called star as shown in this query here it says select star from employee where name equal to bhadriah so  essentially this query is similar to the relational algebra expression which says select or sigma name equal to bhadriah from employee that means the entire row or the set of all attributes of relations where the name attribute is called bhadriah is going to be returned similarly  if i say select star from employee  department in the second query that is shown in the slide here  it computes the complete cartesian product between employee and department we now come to the fact that how tables are treated in sql in relational algebra we have seen that by default relations or tables are considered to be sets on the other hand in sql  tables are considered to be multi-sets or bags that is multiple tuples having the same values are tolerated we have also seen why this is sometimes not only desirable but also necessary  it is desirable because it is expensive to remove duplicates  refer slide time  00  41  13  suppose i return a query with 10000 records of which there may be hundreds of duplicates i need to perform  i need to first sort each of these  this whole set of records and then remove duplicates and then reorder the records in whatever order that the user has asked for therefore it is very difficult or it ’ s an unnecessary over head to remove duplicates  therefore it is desirable in many cases to tolerate duplicates and in some cases it ’ s actually necessary to tolerate duplicates we have also seen examples of these suppose i want to compute the average marks of all students in a particular course  it is not only desirable but it is actually necessary that i retain the duplicates because the duplicates all contribute to the total number of marks which have to divide by the total number of occurrences to find out the average marks so for computing any aggregate properties  i need to have duplicates however in some cases if i want to remove duplicates explicitly from the output of a query  in sql you can give the clause called distinct as part of your select statement  the table  the slide here shows such an example  it says select distinct name from employee so which simply says that show me the set of all distinct names that the employees have so if two or more employees have the same name then they are shown only once as part of this query  refer slide time  00  43  32  similarly one can perform several set theoretic operations like union  intersection and set difference using sql so union is operated by using the clause called union and intersection by the clause called intersection and set difference by the clause called except as shown in the slide here now  by default union  intersection and except assume that the sets that they are operating upon are actually sets and not multi-sets so note the use of distinct in this example the example here says that select distinct name from employee where salary is greater than 3 lakhs union select distinct name from employee where salary is less than 24000 so essentially what its doing is that it is selecting the set of all names of employees who are earning more than 3 lakh and combining them with the set of all names of employees who are earning less than 24000 and duplicates are removed in these sets of name so the union operator assumes that duplicates are removed  when it is performed in the union of these two sets  refer slide time  00  44  55  on the other hand if i want to tolerate duplicates or if i want to specify that the sets are actually bags and not pure sets  ttherefore i need to perform a disjoint union or a disjoint intersection remember what is the disjoint intersection of two sets  if a tuple or if the data item occurs multiple times in an intersection  for an example it has to occur the minimum of the two number of times so if i have to specify that i am actually working on bags and not sets  i need to specify that with the key word called all which is shown in the slide here so if i have not specified the distinct construct in my select statements as in my previous examples  i should use the term union all for disjoint union and intersection all for bag intersection and except all for difference or set difference between bags we can also perform comparisons over character attributes especially string attributes by comparing partial strings or comparing wild cards so this slide here shows two such examples the first examples says select star from employee where name like percent  arun percent so note the use of firstly the key word like and secondly the use of the percentage symbol so a percent symbol matches any number of characters wherever it occurs  therefore this query here matches employee name where the employee name contains arun as a substring  a r u n as a substring and where it may be preceded by any number of characters and succeeded by any number of characters  refer slide time  00  45  53  so while the percent symbol matches any number of characters  a single character can be matched with the underscore symbol so suppose if i had said where name like underscore arun underscore underscore  so it essentially looks for one character before arun and two characters after arun so any kind of character  any kind of name where arun occurs as a substring with exactly one character before it and two characters after it one can also specify arithmetic operators like addition  subtraction  multiplication and division so i can say where salary plus perks not greater than 50000 or so on and i can also use these arithmetic operator not only in the where clause but also in the select clause so have a look at the example shown in the slide  this slide shows a query which says select 1.1 times salary that is 1.1 into salary from employee where salary greater than 3 lakh  so which basically says that show me what would be the figures  salary figures of employees  if salaries where to be raised by 10 % effectively  refer slide time  00  47  22  that is i am multiplying existing values of salary by 1.1 and showing that as the result  refer slide time  00  48  21  so this brings us to the end of the first session on the structured query language where we have looked in to the basics of what makes up the structured query language and we have seen how to create a schema using sql and what is a catalog that is a collection of tables and how to specify the structure of a table by specifying the name  the attribute names  the attribute domains  the constrains on the attributes like not null  unique  default values and so on and the key constraints like the primary key which identifies what is the primary key in this and also referential integrity constraints like using the foreign key constraint we have also seen how to alter schemas and table constructs or table structures and what are the implications of these constraints on these modifications ? that is what happens if i drop a particular column name but that column name is actually referred to as a foreign key from some other table now in such cases i can also specify whether to drop all foreign key references or to drop this column only if there is no foreign key reference coming into the table so using either the notation of cascade or restrict we also saw the most widely used operation in relational algebra namely the select operation however we have not finished looking into the different forms of select operators in the next session we would be looking at some more features of select as and how to nest select operators and how do we dereference or how do we disambiguate attribute names in the nested select operators however as we have seen because select is the most widely used sql operator or sql statement  it has varied number of forms and several different notations and one of the main properties that we have to remember about the select operator is that the select operator treats tables as bags or as multi-sets rather than as sets that is it tolerates duplicates in the sets unless we specify explicitly that we do not want duplicates and this is specified by the distinct clause so to summarize  the slide here shows the summary of what we have seen today we have seen an introduction to the sql standard tables  attributes and values and we saw how schemata are created and tables can be created and constraints and essentially the select operation in its different forms that brings us to the end of this session thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 6 structured query language ii hello and greetings in our ongoing explorations of databases  we had started out exploring the default or the standard for database querying namely the structured query language or sql  refer slide time  00  01  36  so let us continue with sql in this session today and look at some of the more advanced aspects and what kinds of queries that we can express within simple sql statements so before we begin let us summarize what we have studied until now in terms of sql now we looked into an introduction to the sql standard and what are the building blocks of sql namely tables  attributes and values which or tables  rows and columns and we also saw how to create schema in sql and what is meant by a schema and how do we create tables and what kinds of column descriptions can we give when we create tables and what kinds of constraints can we specify when creating tables and so on  refer slide time  00  01  53  we also saw how to specify entity constraints specifying what is a primary key and what are the secondary keys in a relation or a table we also saw how to specify constraints like something should not be null or something should have a default value and we also saw how to specify foreign key constraints so that referential integrity is maintained and we also started by looking into the different forms of the select operation which is perhaps the most frequently used sql operation for retrieving tuples from the database so the most generic form of select operation is the select from where clause where you say select attribute list and say from table list where a given condition holds true we also saw how we can disambiguate attribute names especially when we would be using multiple tables and two or more tables have attributes with the same name one simple way of disambiguating attributes names is to prepened the name of the table before the attribute name so we saw something like instead of saying name  we say employee dot name or department dot name and so on so  on the other hand we can also use aliasing for disambiguating names mainly because when table names are also the same  when we saw the example of a join between employee and employee tables  refer slide time  00  03  01  so we basically use the notion of aliasing where you say use the term or use the table employee and call it as some other name like say e or boss or whatever then we also saw how we can select from multiple tables  the cartesian products of multiple tables and what happens when we omit the where clause and so on and we also saw the different kinds of set operations on tables by default in sql and very unlike relational algebra  in sql tables are treated as multi-sets or bags that means it can tolerate or it is valid to have different tuples of the same having the same data in a sql table which is not valid in or which is not valid by default in relational algebra so we can make a table into a set by using the keyword called distinct so when we say select distinct some query  a query is returned with all duplicates removed from the query we also saw how to perform set operations on tables and sets by default  these operations by default assume that table are sets like union  intersection and except for set difference if we have to specify that the tables are not sets but instead they are bags  we have to specify  we have to use the qualifier called all so we say union all whatever  so we say employee union all department and so on or employee union all managers and so on and we also saw how to compare substrings using the percentage symbol or the underscore symbol which matches a single character and we also saw some arithmetic operators like addition  subtraction  multiplication and so on which need not just be used within the where clause but they can also be used right after the select clause itself  refer slide time  00  06  21  so let us move on further today and in today 's talk  i will be using two example tables in order to illustrate several features of sql so let us look into these tables once again the first table that we are going to be considering is the employee table so the employee table contains employee number which is the second field shown in the slide here as the primary key it also has a secondary key called pan number of the employee which obviously has to be unique and non null and employee is given a name  gender  date of birth  address  salary and reports to which is a foreign key that is reports to contains another employee number which refers to the manager to which this particular employee reports to if reports to is null then it means that this employee is the head of the company or that is he doesn ’ t report to any other employee then there is a department number or the dnumber which shows where the employee works in the second table that we are going to be considering today for our examples is the department table  refer slide time  00  07  39  the department table as shown in the slide here is indexed by or uses as primary key the field called department number or dnumber which is the same as the dnumber used in the employee relation that means the same domain department also has the name  address and head which is again a foreign key which points to the employee number of the employee who heads a particular department so here are the specific declarations which says primary key is dnumber and head is a foreign key which references employee number in the employee table so we had started out with looking at arithmetic operator  so let us continue from that point on and we had seen how you can use plus  minus  star and slashes to perform arithmetic operations namely addition  subtraction  multiplication and division similarly you can use the operator called or you can use the keyword called between to check for a range that is to check whether a given parameter lies within a given range  refer slide time  00  08  17  the slide here shows an example which is of the form select star from employee where salary between 3 lakh and 4 lakh therefore this query returns all employee records select star in this case note that the star here is not the multiplication operator but star here refers to the entire tuple in this when star occurs by itself therefore this query selects the entire set of tuples  the entire set of columns for all tuples where the salary field or the salary attribute is between 3 lakhs and 4 lakhs it is also possible to sort the output of a given query using one or more parameters the sorting is achieved by a new construct called order by  this construct is shown in this slide here this slide shows another small query which says select star from employee where salary is greater than 3 lakhs and order by name and employee number therefore this query returns the same result as the previous query that is the set of all tuples  the complete tuples for all employees well not exactly the same of course for all employees whose salary is greater than 3 lakh but not necessarily less than 4 lakhs and the output here is ordered first by name and next by employee number so that means that it is first ordered  the records are first ordered by name and wherever there is a tie that is two or more employees having the same name  it then orders those tuples using employee number as the ordering attribute  refer slide time  00  09  41   refer slide time  00  10  39  we now come to the next aspect of sql querying perhaps what gives it compositionality so to say that is by which you can compose queries  bigger queries from small queries this is what is called as the nested query note that just like in relational algebra  the output of an sql statement is a table especially the select statement the output of a select statement is a table and the input is also a table it can be multiple tables which are treated as a cartesian product in which case therefore the output of a particular query can be used as part of another query to perform further searches so the slide here shows such an example where a query has two different parts in it for the sake of convenience they are shown in two different colors white and yellow there is an outer queries so called outer query which says select name from department that is select the names of departments where dnumber in  so in is a new keyword that we are also introducing here which essentially stands for set membership  that is where the dnumber or the department number belongs to the set of all tuples that are returned by the query which is shown in yellow that is where dnumber in the query called select dnumber from employee where salary is greater than 3 lakhs so let us analyze this query what does this query do ? firstly take a look at the inner query the inner query says select department numbers of all employees whose salary is greater than 3 lakhs and the outer query is saying select the names of those departments which are returned by the inner query so essentially this query is asking the database show me all departments which pays greater than 3 lakhs as a salary so the highlighted query  the yellow part of the query here is called the nested query and the non-highlighted query is called the outer query in this case so let us continue with nested queries and note that a nested query contains two different queries and the inner query or the nested query in this case returns a complete relation or returns a complete table for all practical purposes we can consider the table as sets or bags comprising of different tuples and the in clause that we used in the previous slide performed exactly that  that is did exactly that that is it considered the table that is returned by the inner query as set and essentially performed a test for set membership  refer slide time  00  13  11  that is whether the dnumbers specified in the outer query belongs to the set that is run in the inner query similarly one can do a check for a particular attribute against all elements in a set using the keyword called all this is shown in the slide here the outer query says select name from employee where salary is greater than and then the inner query begins this is shown in the slide here so the inner query here says select salary from employee where supervisor equal to 007 so let us try to analyze the query step by step  first take the nested query or the inner query here it says select salary from employee that is give me the salaries of all employees where it should actually be reports to there is a small bug in this slide where reports to equal to 007 that is give me the salaries of all employees who report to 007 so we just want to know what is this employee number 007 or agent 007 pays all of his subordinates and then the outer query says select name from employee that is give me the names of all employees where salary greater than all of this inner query that is whose salary is greater than all of the salary that is paid by 007 that is it is greater than the maximum salary that is paid by 007 so it returns the names of all employees whose salaries are more than the salaries of all those who report to 007  refer slide time  00  15  45  now what happens to disambiguation of attribute names when we are considering nested queries ? so this slide here shows such an example firstly  the rule which says that any unqualified attribute name in a nested query applies to the inner most block of the query so what does it mean ? have a look at the query here there are again two levels to this query  the outer query says select e name and then i say from employee as e that is i am aliasing employee table as e where e.employee number in and the inner query begins which says selects reports to from employee where e.name note that i am using the alias called e inside the inner query as well  while the alias is actually defined in the outer query and then i say e.name equal to name so what is this query do actually ? so if you notice closely  the outer query as well as the inner query works on the same table called employee the outer query calls itself as e and the inner query does not change the name of the table  it is still called employee therefore in the inner query when i say select reports to that is give me the set of all employee ids of the bosses of all employees where the name of the employee is the same as the name of the boss that is e.name here would be the boss here because i am looking for e.employee number to occur within this set of all employee numbers who are bosses so the query essentially returns all employee names who have the same name as their boss  refer slide time  00  17  49  so let us look at few more definitions pertaining to nested queries the kind of query which we just saw now that is where a particular alias is defined in an outer query and then used inside an inner query  such kinds of queries are called correlated nested queries that is there is a greater correlation other than the fact that an inner query or a nested query occurs within a condition  there is a greater correlation between these two queries now how do we understand or how do we analyze the behavior of a nested query now to understand how a nested query begins  it should be noted that or it is sufficient if you just note that every select query is performed or every select condition is performed exactly once on each tuple that is specified in the table that  for the table specified in the query therefore let us have a look at the previous slide once again in the previous slide for the time being consider just the outer query assume that there is just one select statement which is the outer query so this statement says select some attribute that is e.name from the set of all tuples in employee where some condition that is e.employee number in whatever so let us not worry about what the condition is let us just assume that there is some condition called c now this condition that is in this case the nested query and the set membership function that is e.employee number in that nested query  this whole condition is checked once for every tuple that forms the employee record that is that forms the employee table so for every employee  this particular condition is checked that is for every employee we are checking whether his employee number occurs in the set of all employee ids of people who have the same name as one of the subordinates so in that way as long as we remember this fact that select query or the select condition is checked once for each tuple  its easy to understand nested queries in some kind of recursive fashion that is you have to understand this for each level in a given nested query  refer slide time  00  20  25  so continuing further with next nested queries we can use a term or the keyword called exists or of course not exists to check whether the output of a given nested query is empty or not so if the output is not empty then exists returns true that is there exists some results from this query and if the output is empty then exist returns false or not exists returns true so there is an example given in this slide regarding this which just says select d.head where from department as d of course that is alias department as d where not exists where the following condition does not exist what is the following condition which is the inner query  select employee number from employee where reportsto equal to d.employee number so that means essentially i am looking at the head of all departments that is i am looking at d.head  so the employee numbers of the heads of all departments where this does not exist what is this ? select employee number from employee where reportsto equal to  this should actually be a  there is again another bug here  this should actually be employee.employee number that is or d.head rather so where where reportsto equal to this head that means give me the set of all heads of departments who do not have anything  who do not have anyone working under them because this condition should not exist is what we are checking for  refer slide time  00  22  15  we can also specify explicit sets until now we have been looking at implicitly defined sets  we have looked at set membership in the form of in condition  we have looked at set comparison in the form of all condition and we have also checked for empty sets using the exist condition  refer slide time  00  22  33  but all of these sets where actually specified in the form of a query that is the nested query we can actually specify sets in a more explicit fashion using just parentheses that is we just parenthesize and enumerate all elements of the set explicitly the slide here shows one such example which says select name from employee where reportsto in 007  008  009 therefore what it says is that give me the names of all employees who report to either 007  008 or 009 or that is whether the reportsto field or the reportsto attribute holds one of these values in this one can check for null values in an sql table using an sql command called null or a sql keyword called null capital n u l l and you can also check for whether something is null or is not null so remember what constitutes a null value for a particular attribute a null value is a value which is not applicable or an attribute which has no value or no semantic value associated with for a particular tuple this is different from saying that the value the attribute is zero or unknown so the example here shows a query which says select e.name e.salary from employee has e  so that is select the names and salaries of employees where something exists what is that exists or that is where the following set is not null what is the set which should not be null ? the set which says select employee number from employee where e.reports to equal to employee number and reportsto is null so let us analyze the interior query or the inner query again  refer slide time  00  23  18  it says select or give me the set of all employee numbers from the employee table where e.reportsto that is the employee from the outer query is a subordinate of me that is the employee in the inner query and reportsto is null that is i do n't report to anybody else so essentially the query returns the names and salaries of all people who report directly to the head of the company that is this the name and salary of all people where the following condition exists the following condition here is the set of all their bosses who also happens to be the head of the company that is who doesn ’ t have any other supervisor himself or other self so note that here the use of the condition is null that is whether reportsto is null this is different from saying reportsto is 000 suppose 000 is the valid employee number  this is different from null that it means that this employee has a boss but in this case this is looking for the fact where this employee does not have a boss or does not have anyone to report to  refer slide time  00  26  04  just like we have been using aliasing for renaming table names  the as key word or the as clause can be used to rename attribute names as well so the slide here shows such an example it says select name as employee underscore name from employee so what does it do ? it simply selects the set of all employee attributes from this table  however while returning it when the table is returned the name of this attribute is changed from name to employee underscore name now this in turn may probably be as part of a larger query were this would matter or even if it is just printed out on the output  the name of the attribute would have changed from name to employee name or employee underscore name note that when a select query is run over multiple tables that is the from clause contains multiple tables select name  salary from employee  department by default it assumes that we are having a cartesian product between the tables that are specified therefore if we have to compute a join between employee and department for example we have to identify  we have to explicitly identify which is the attribute and which has to be compared therefore in a simple select operation  let us say we have to join  we have to compute a join between employee and department we would say something like select star from employee  department where employee.dnumber dno  refer slide time  00  26  58  remember what the employee record look like the employee record had one of the fields as the dnumber or the department number of the employee where he is working in  so where employee.dnumber equal to department.dnumber so we have to explicitly equate these two attributes to perform a join otherwise it is constitute as a cartesian join between employee and department on the other hand we can specify a join that is rather than a cartesian product using the keyword called join  the slide here shows such an example the slide shows a query which says select name  address from employee join department now when we just say join without any further qualifiers  there is still not enough information to identify which attributes to use for join condition what is the theta or what is the join condition here ? therefore we specify that condition explicitly so we say as shown in the slide here select name  address from employee join department on dno equal to dnumber note that dno is an attribute of employee and dnumber is an attribute of department so this is specifying an equijoin condition that is it is equating dnumber dno to dnumber and of course there is a where condition which says department.name equal to research therefore it is computing the join between employee and department and selecting only those tuples where department name is called research and then printing the set of all names and addresses of employees who work in this department  refer slide time  00  29  53  it is also possible to specify natural join directly without having to specify the equality condition note that in a natural join if we have two tables  at least one of the attribute names should be the same or should be common between the two tables so natural join which in relational algebra was depicted by the star operator just performs in equijoin between two relations where the set of all  where it has some subsets of attributes having the same name so the same condition also holds true in sql that is if you are performing a natural join  you have to have at least one attribute name which is the same you can always change the name of a table and its attributes names using the as clause which we have already seen so a natural join may be specified using the natural join construct it automatically finds attributes having the same names for performing the join and of course relations can be renamed  relations as well as attribute names can be renamed in order to accommodate natural join so there is a specific example here which says select employee.name  department.name from employee natural join department in our specific example though  the only attribute name which was common between these two was the name clause itself therefore for our particular example schemas that we have taken  this query may not make sense but the syntax of the query is illustrated the main idea behind the example is to illustrate the syntax of the query which just says select employee.name  department.name from a natural join between employee and department that is to identify all attributes which have common names between employee and department  we compute a natural join and then project the set of required set of attributes from them  refer slide time  00  32  04  similarly other kinds of joins in which we saw in the session on relational algebra can also be specified using the appropriate keywords in sql just like you have natural joins  you can also specify left outer join  right outer join and full outer join as part of any sql statement we next come to the notion of aggregate functions in sql until now we have been working on generating a set of tuples or the set of all tuples that match a particular criteria  refer slide time  00  32  26  sometimes we may need aggregate properties of a query rather than the set of all query results so there are number of aggregate function in sql  some of them are shown here there for example count which counts the number of tuples in the query result  sum which computes the sum of the set of all values in the query result in which case the query result should return single numeric attribute average which computes the average of all values in the query result again the query should return numeric attributes  max and min which computes the maximum and minimum values of all the query results and of course max and min will work if the query results are numeric or they are ordinal which basically means that they have some kinds of total ordering that is specified among them for example date  date has a total ordering you can always compare two or more dates the query can result the maximum and minimum among dates and of course char  you can always compare characters using their ascii equivalent so the maximum and minimum can be returned here is a small example query shown in the slide which just says select count of salary min of salary max of salary sum of salary and average of salary from employee which is obvious it just counts the number of salary elements  as you can see i could have as well set count star  it does not make any difference because it just counts the number of tuples in the relation and min of salary  min of the salary field max of the salary  salary attributes  sum and average of all the salaries  refer slide time  00  34  40  since aggregate functions return a single value and not a table  they can actually be used as a part of a logical operator within a where clause until now when we have been talking about nested queries  we have been treating the nested query as the set and we have only been applying set theoretic operators like in and all and exist and not exist and so on but once we use aggregation function and reduce our query result to a single value  i might as well use it as part of a logical operator the example in this slide shows such a case which says the outer query says select e name from employee as e where the inner query says select count star that is select the count of all  the set of all tuples that are returned by this query and what is the query ? from department as d where department head equal to e.employee number that is it is selecting the set of all employee numbers who heads departments and counting the set of all such queries that is it is count  it is for each employee that is search in the outer query it is first searching the set of all departments that the employee heads and return in their count and this count is compared with two here that is greater than or equal to two therefore the semantics of this query is return the names  note that we are finally retuning e.name return the names of all employees who head two or more departments  refer slide time  00  36  32  in some cases when we are talking about aggregate functions  it is not really desirable to apply the aggregate function to the set of all query results  set of all tuples that have been returned by the query it may be more desirable to apply this  the query or the aggregation functions two different sub groups of the query results now you can specify such sub groups using what is called as the group by clause the group by clause is shown in this query here in this slide the slide shows a small example  select dnumber dno  count star  average salary from employee group by dnumber what it does is for each department that is it first checks the or it first scans the set of all tuples in the employee table and then groups the set of tuples according to dnumber or dno so for each department that are contained in the employee table count the number of people working in it  that is because we are counting the set of all employee records which have this dno so count the number of people working in this department and also count the average salary of the people who are working in this department  refer slide time  00  38  04  but note that the group by function is computed after computing the results of a query that is the query first scans the set of all employee records as in the previous example and then groups those records or groups those tuples based on dno and then performs the select operation however if you want to select certain tuples based on some aggregate property  not an individual property note the important difference here suppose they want to select a set of tuples from a table based on some aggregate property  we can use different keyword called having clause the slide here shows such an example  it says select dno that is select the department number count and average salary from employee group by dnumber that is it ’ s the same query as earlier that is for each department return me the count and number of people working in it and average of salary having count greater than 20 that is there is a further constraint here for all departments having more than 20 people working  show me the department number  the number of people that are working there and their average salaries note the difference between having and where clause you might you might be wondering we could have just used the where clause  select from where  where actually specifies a particular condition and having is also specifying a particular condition now what is the difference between having and where ?  refer slide time  00  39  49  having versus where  where conditions apply to individual tuples independently that is the where condition where i say something like salary greater than 3 lakhs is applied to each tuple independently however the having condition applies to groups of tuples  its an aggregate property that is having count greater than 20 that is the number of tuples is greater than 20 or the average of salary is greater than something else and so on so having is specifies a condition that applies to a group of tuples  whereas where specifies conditions that apply only to individual tuples so let us look back at select now how does select look like after going through all this different variants of select ? now once i look back looking at all this different variants  we see that the select condition has the following constructs select attribute and or function list i can also give a function remember  i can always say select 10 times 10 star salary that is what happens if my salary increases by 10 times and so on so select attribute list or function list from table list and the rest are all optional  where condition is an option if you do n't specify where then every tuple is checked group by grouping attributes how should the tuples or how should the output be grouped ? having group condition that is some kind of aggregate properties that we can check then finally order by which is the sorting condition for the output  refer slide time  00  40  30   refer slide time  00  41  44  we now move into other operations within sql  select obviously is the most widely used operation and hence the most detailed in terms of some its syntax but we still need operations for inserting or adding data into tables and modifying tables  deleting data from tables and so on by modifying tables here i mean modifying data in tables not modifying the structure of tables which can be done using the alter table command so insertion of data into tables can be performed using the insert command the insert command is shown in the slide just like we have select from  we have insert into so we say insert into employee values and i give the entire record for the employee within parenthesis so i am inserting a complete record where the first 1002 stands for the pan number  the second 002 stands for employee id  the third field bharath kumar stands for the name then m stands for the gender  9-5-1973 stands for date of birth and so on so we can specify the entire tuple in line into the query itself and corresponding fields and the corresponding attributes are matched so inserts an entire employee record with corresponding values  refer slide time  00  43  16  on the other hand i can insert or i can specify only partial set of attributes within the within the tuple so this slide shows such an example which says select into employee or rather insert into employee name  address and dnumber values arun k ysehwanthpur and 5 therefore it returns or it inserts only those fields or only those attributes called name  address and department number or dno what happens to the other attributes ? the other attributes will either get a null value or the default value if a default is specified therefore you can see that there is an implicit constraint in this select operation that is i can not leave out  when i am inserting a tuple i can not leave out any attribute name which contains a not null constraint and does not have a default value if it does not have a default value and it is not allowed to be null then i have to specify a value during insertion or else insertion is going to fail  refer slide time  00  44  26  an insert or insertion of more than one tuples to a table can also be performed using a select operation note that select actually returns a table now if i return and table is nothing but a set of tuples now if a select operation returns a set of tuples and these tuples are in a format that a ready to be inserted into another table i can directly specify the select command within the insert command this is shown in this example here first i just create a table called emd which contains just three attributes employee  manager and dno that is the department number then i give an insert command from the employee and department table using the following syntax i just say insert into emd that is insert into the new table name and what should i insert ? the output of the following select operation then i just give the select command that is select e.employee number as a employee  e.reports to as manager and d dnumber as dno and then from employee as e join department and so on so basically i am getting a set of employee number  manager number and department number which is what is going into the new table  refer slide time  00  45  08   refer slide time  00  45  49  so what are the properties of the insert command ? let us summarize the insert command once again so insert tuples that are specified as part of the command and all attributes which are not null and do not have a default value have to be specified as part of the insert statement and what happens if i give the same insert statement twice ? that is the same insert statement with the same set of data elements and it is actually performed twice it doesn ’ t  insert does not do any checks that is remember that tables are treated as multi-sets rather than sets therefore insert just goes and inserts the tuple again that is second occurrence of the same tuple this is done as long as the second insertion does not violate any unique constructs that is if i give a set of data values in the first tuple and give the same set of data values in the second and one of the attributes has to be unique then the condition  then the constraint fails and in turn insert also fails so as long as the unique construct is not violated  insert will just insert multiple tuples into the table insert also fails if referential integrity is violated  this if your dbms supports it of course that is if i try to insert an employee number as manager where which refers to a manager entity which does not exist  referential integrity fails multiple tuples can be inserted within a single insert command  first of course by using the select statement or by just giving multiple tuples one after the other separated by commas and each tuple is enclosed within parenthesis  refer slide time  00  47  42  deletion of tuples  how do we delete tuples from a table ? deletion of tuples is very similar to the select statement and has the same structure which is of the form delete from where it says delete from employee where some condition that is employee number equal to 007  the first one which just deletes one tuple note that employee number is the primary key therefore it is unique and therefore it just deletes one tuple on the other hand the second tuple says delete from employee where department number in the set of all departments that are headed by 007  so it can delete possibly more than one tuple in this statement on the other hand if i remove the condition and i just say delete from employee  it deletes all tuples note that deleting all tuples is different from dropping the table here deleting all tuples corresponds to truncating the table that is the table exists but it has no tuples in it whereas  if i drop the table the table itself does not exist in the database  refer slide time  00  48  54  updation of tuples  how do we modify tuples ? you can update tuples by using the update operation and the update operation also has a very simple syntax which is of the form update set where that is update employee as shown in this example here so update employee set whatever updation i need to make that is set salary equal to salary times 1.1 where reportsto equal to 007 therefore what i am doing here is that for all people working under 007  i am updating their salary by or i am increasing their salary by 10 %  i am giving them a 10 % rise by this update statement here  refer slide time  00  49  37  we now come to the last leg of this session where we take up the notion of views or virtual tables this is again an important concept when designing large databases view as you can typically or intuitively understand is one particular view of the database that is one particular projection of the database which is suitable for certain kinds of work that is let us say a hr manager needs to know only the employee that is the hr related details of an employee he doesn ’ t need to know the technical details of an employee or the project related details of an employee therefore we have basically created a view of the employee record for the hr manager which is different from the view created for let us say the project manager so table or a view is also called a virtual table  this is a table that is derived from other tables in contrast a table that exists in the database is called a base table so a view can be derived from either other base tables or other views views are need not be stored  it ’ s not that they are not stored but views need not be stored in the database that is the data contained in the views need not be stored in the database but they are typically stored as queries and not as tables and update operations for views are limited  this is as the result of storing them as queries but querying is not  you can query but updation is limited in terms of views  refer slide time  00  51  24  so views can be created in sql using the create view command this is again pretty simple construct which is shown in the slide here which says create view and emd which is the name of the view and there are three fields empl  mgr and department so this is the structure of the view that is the name and the attribute list and this view is created as this query that is whenever i need to compute this view  i just need to run this or execute this query this query is going to return a table which will populate the data that is required by this view what are some of the properties of views ? because a view is stored as a query  a view is always up to date i do n't need to modify a view  when i modify some data elements in my table because there is no data element that is stored in a view hence characterization of a view is not done during view definition time that is when i define a view i just leave it like this when i define a view like this  it is just left like that it is not characterized  it is not computed  refer slide time  00  52  06  but this computation is done during the time of query that is when i give a query on a view this characterization is done views can be deleted using the drop view command just like we can delete a table the efficient implementation of a view is a pretty tricky problem and we shall be addressing this view maintenance as a separate session in itself because how do we efficiently maintain a view and execute queries over so that brings us to the end of this this session so let us briefly summarize or look at the titles of all the different topics that we have studied we looked into a sql two standards tables  attributes  values and constraints  entity constraints  foreign key constraints and so on we looked at several different kinds of select operations select from where and disambiguation  aliasing  selecting from multiple tables  set operations  multi-set operations  substring operations  arithmetic operations  existence checks and null checks nested queries and aliasing and scope in nested queries and group by constructs and having constructs and so on  refer slide time  00  53  14   refer slide time  00  03  01  and then we also looked at sql statements for insertion  deletion and updation of tuples from database finally we looked at the notion of views or virtual tables or rather i should say that we have just scratch the surface of views or virtual tables and saw how we can specify a view in sql using the create view command so that brings us to the end of this session thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 7 er model to relational mapping hello and welcome to the next lecture in the dbms course until now we have seen two main kinds of data models for representing data or managing data in several different application context like i say whether it is insurance or banking or railway reservations or company databases or so on  refer slide time  00  01  18  these two models where  the first was the entity relationship model or what is called as the er model and the second was the relational data model we also saw a typical database design process and placed these models into appropriate positions in the process the entity relationship model or the er model is essentially meant for human comprehension it basically is meant for creating a conceptual database or conceptual schema or what is termed as a logical schema of the database system and the relational data model is used for the physical schema or something that is implemented in the dbms  refer slide time  00  01  46  so both are dbms independent models that is no matter which companies database that you are going to use  you can still use the same er model for representing your data and even the same relational model for representing the schema that goes onto your dbms now in this session we are going to address one important issue now we are going to ask a question  are these two different data models er and relational model completely independent of each other or are they the same or is there some way i can map between the er schema and the relation schema without having to break my head too much essentially that means to say or to put it  to take it to its logical extreme  can i design some tools or can i design some kinds of software that takes an er diagram of a given system and generates appropriate relational schemata for the system and we have also seen in the session on functional dependences  we have seen how we can optimize a given relational schema up to a certain extent using some kinds of automated techniques we have seen how to take a relation to a bcnf or third normal form or the fourth normal form and so on so suppose we want to build a tool to automate this process  we need to be able to first map between an er database schema and a relation schema and then use techniques from functional dependencies to optimize this relational schema so that we can build a database application around it so what we are going to study today form the underpinnings of what are called as lifecycle tools or database lifecycle tools there are several different lifecycle tools which provide support for the entire lifecycle of a database systems  starting from the conceptualization of the problem to the actual implementation of the application and maintenance of the database and so on  refer slide time  00  04  25  so before we begin let us briefly summarize what we have learned about er models and relational models the er models as we already know is used for conceptual modeling of the database schema  conceptual modeling or to create the logical database schemata this is meant for human comprehension  this is essentially used to show end users what you have understood about their problem domain this is a high-level database design and there are no implementation details about that are included as part of the er model and of course the er model is a dbms independent and it is made of building blocks like entities  relationships and attributes which can be attributed to both entities and relationships in contrast a relational data model is the data model that is most popularly used for physical schema design a physical schema is the schema that is actually implemented on the computer  therefore the relational data model is meant for or optimized towards machine conception that is how do we efficiently store data in my database  how do i efficiently search for a given data element  how do i efficiently update a given data element so that it does not create anomalies  how do i efficiently delete data elements again without creating any anomalies and so on  refer slide time  00  05  10  and of course the relational data model is also dbms independent that is no matter what kind of database that you use  you can still use the same data model as far as the database that you are using is a relational database you can use the same data model to represent your data on the dbms of course reality is quite different from the concept of dbms independent and some dbms systems may include more features than traditionally what is supported by the relational model the relational model also supports some kinds of automated optimization techniques which we have seen in the session on functional dependencies where you can optimize a given relational schema  you can reason whether a given relational schema is optimal or not whether it ’ s going to create redundant data in its dbms or whether it ’ s going to create some kind of anomalies during updation and deletion or so on and  how you can systematically change the database schema without changing the correctness but increase in the overall efficiency in terms of retrieval and updates and what are the building blocks of the relational model ? we have relations which comprises of several different attributes and the notion of keys forms  a very crucial role or place a very crucial role in the relational database model  refer slide time  00  07  23  let us come back to the er model and look at some of the notations which we will require if we have to study translation into er models the entities are represented using rectangles and a strong entity type that is an entity type which has its own key attribute and which represents a physical or which represents some kind of a logical entity of the of the real life is represented by a rectangle with solid lines surrounded for example the slide shows this entity type called employee which depicts all objects of type employee which are present in the current system on the other hand we also have what are called as weak entities weak entities are those which do not have an existence of their own or without being associated with a strong entity type the slide shows the example of an insurance record an insurance record doesn ’ t mean anything unless it is associated with some person in a company for example an employee therefore when we talk about insurance record we have to say  whose insurance record and so on so that is the general idea and more specifically the insurance record entity type does not have any key attribute  it has to be associated with a strong entity type called employee which in turn has a key attribute therefore such weak entity types are depicted using dashed lines or dotted lines for the rectangle we then have the relationship type for example a relationship called handles  so employee handles project or something like that which is represented by a diamond and a normal relationship type is represented by a diamond using solid lines whereas  what are called as identifying relationship types that is the relationship types that identify a weak entity or provide an identity for weak entities by associating them with strong entities  they are shown with double lines in the diamond  refer slide time  00  09  36  entities and attributes are associated or entities and relationships are associated with attributes which are some values in a given domain attributes are depicted using ovals and normal attribute or a simple attribute is depicted by an oval with a solid line and key attributes in this example an attribute called pan or pan number which uniquely identifies each income tax payer is shown as a key attribute and it is shown underlined saying that this attribute is a key attribute for this entity type and then there are multi-valued attributes which can have several values for the same attribute we took an example of the color of a peacock now the color of a peacock is actually given by several different colors and all of which in combination form the color of the bird such kinds of attributes are depicted using double lines as shown in the slide here and then there are derived attributes that is attributes whose values can be derived from other attributes and these are shown using dotted lines we took an example of the age of an employee that is if you know the date of birth of an employee and the current date  we can derive the age of an employee  refer slide time  00  10  58  let us look at some definitions from the relational model the relational model is based around the notion of a mathematical relation now a mathematical relation is set to comprise of atomic values or atomic data values and what is atomic data value ? a data value is called atomic if it can not be sub divided into smaller values for example the age of a person similarly each data value is set to reside in a domain  in the er model a domain is also called a value set which is term that generally used by several people and in the relational model usually the term domain is used which is going to specify the range of values that a particular attribute can take similarly a relation schema or a relational schema is denoted by a schema name that is in this example is shown by the name r and a set of attributes in this example shown by a1 a2 until an and each attribute has a specific value that lies within the domain specified as domain of ai  refer slide time  00  12  13  we have also defined what is known as the degree of a particular relationship the degree of the relationship is simply the number of attributes in its relation schema if you remember the same definition of the degree of a relationship also applied to the er model that is a relationship diamond can be a binary relationship or a ternary relationship  unary relationship or a n-ary relationship that is it can be associated with 1  2  3 or any number of entity types and the slide shows that the relation is actually a subset of the cartesian product of all of the domains that form the attributes in the relational model the notion of keys play a very crucial role especially we saw in the notion in the process of decomposing relational schema in order to make them normalized or conform into let us say bcnf or third normal form or fourth normal form and so on  refer slide time  00  12  59  so let us revisit the notion of keys in a little more detail and keys are again very important when we translate from an er model to a relational model we have to be aware which attributes are the key and which attributes are the foreign key and so on so a key constraint in the relational modal essentially defines the notion of a superkey which is a set of attributes of a relation which can uniquely identify each tuple in the relation that is each instance of the relation and a key or a minimal superkey is something which is minimal in a sense that if you remove any element of the minimal superkey  it seizes to be a superkey anymore and there is also the well-known entity integrity constraint in the relational model which says that the primary key of a given tuple may never be null the primary key is the minimal superkey that is going to be used to uniquely to identify a given tuple in the relation and we also saw the notion of referential integrity which is again an important issue in the relational data model and the referential integrity constraint says that if a tuple of one relation refers to another tuple of another relation  it should refer to an existing tuple that means foreign keys that is primary keys of another relation embedded into the tuple of yet another relation should refer to tuples that already exist in the first relation  refer slide time  15  09  so the foreign key constraints are shown in the slide here that is first of all the attribute of the foreign key or the domain of the foreign should be the same as the domain of the primary key of the other relation and they have to refer to existing tuples in the other relation  refer slide time  00  15  27  and we also saw that relations can be or popularly viewed as tables and which is what is a notion used in sql that is a relation of the form student with three attributes roll number  name and lab can be re specified in the form of a table with the name student and three kinds of columns called roll number  name and lab  refer slide time  00  15  53  so let us now come to the issue of mapping between a given er model under a relational database model now i had said in the beginning of this session  why such a mapping is important there are several different commercial tools that are available which are called as lifecycle tools of dbms design a lifecycle tool provides support in several or in most of the phases of a typical database lifecycle that means the tool should be able to or using the tool you should be able to create a logical schema  talk to your end users saying this is what i have understood by your requirements of your system  these are the different data elements  these are the different functionality requirements that form your system and so on and then using the same tool you should be able to create physical schemata from the logical schema by automatically translating them to whatever extent possible in practice it ’ s not possible to completely automate this process that is automatically generate a relational model and optimize it sometimes some kind of human intervention is necessary  in order when the human knows some domain knowledge can not be captured into the er model but there are several such tools  an example is the tool called erwin from computer associates which provide such a support for automatically translating between er and the relational model  refer slide time  00  17  34  so let us see how we can go about such a translation the first case that we are going to take is the case of a simple relation or a simple entity so the slide here shows a simple entity type called department and it has three different attributes department name  department id and manager and the department id obviously is the key or the key attribute of this department now given such a relation  it is fairly obvious to  given such an entity type it is fairly obvious to see that it can be translated into a relation of the type department that is also shown in the slide here just below the figure so this er model can be translated into such a relation where the name is called department and which has three different attributes department id  department name and manager and also note that the key attribute is retained that is the department id which is the key attribute of this entity here becomes the primary key of the relation  that ’ s found so this is straight forward that is as long as we have a simple entity type with simple attributes  note that the attributes are also simple  there are no multi-valued attributes or composite attributes and so on and it can be translated in a straight forward fashion to the relational model  refer slide time  00  18  57  what happens if we have a composite attribute ? remember that a composite attribute is something that is made up of sub attributes a composite attributes is different from a multi-valued attribute that is a multi-valued attribute is something which can have many values for the same attribute  the color of a bird can have several different colors on the other hand a composite attribute is made up of two or more other attributes each having its own domain for example the slide shows a composite attribute called department id for the same example of a department entity type  so the department id is a attribute here which has which is a composite attribute which in turn is made up of two other attributes called location id and region number so on now a location id could have a different domain  let us say location ids are given alphabets like a b c d and so on and region number are given numbers 1  2  3  4 and so on so both of them may have different domains and they combine to form the attribute called department id which in turn is also the primary key of this department therefore we are considering two different aspects here  one is how to deal with composite attributes and the second is what happens if the composite attribute is the key attribute of the entity type so the slide here shows the example of and shows how we can translate this into relational model firstly  the name department of the entity type becomes the name of the relation called department and all the other simple attributes are retained  department name is retained as department name  manager is retained as manager and only here for the composite attribute  the department id never appears here it ’ s just that all the simple components of the composite attribute are straight away loaded into this relation that is location id comes here and region number comes here and in fact if either of these two let us say location id or region number is again a composite attribute and it has some more attributes  just take all the simple components of the composite attributes so do n't take region number and just take whatever is the simple component of this region number and add it to the relation here and all of these simple components which form the composite attribute which is the key becomes the primary key that is the primary key here is a composite key made up of two or more different attributes which combinedly identify or help in identifying a tuple of the relation the next example that we are going to see is the example of how to map relationships first of all let us look at the following relationship that is shown in this slide here what characteristics can we ascribe to the relationship that is shown here firstly  we notice that the relationship here is a one is to one relationship that is one employee is associated with one insurance record or rather the other way around in this case  that is one insurance record is associated with one employee and have a look at the association as well  refer slide time  00  21  51  the association or the relationship type is an identifying relationship type that means the relationship type called insurance details shown in the slide here is used to identify or provide an identity for the insurance record by associating it with an employee and also the insurance record has a total participation in this relationship that is insurance record has no existence without this relationship now how do we translate such entity types ? so essentially the idea here is how do we translate insurance record into the relational model ? for weak entity types the translation is shown here in the slide below just create a relationship or a just create a relation of the same name as the entity type but since it does not have a key to because weak entity types do not have key attributes since it does not have a key attribute  use the key attribute from employee with which it is associated with and take that key attribute and make it into the key attribute of insurance record however note that since pan number here is also the primary key for employee  this has to be made as a foreign key of insurance record that means whenever we are updating or altering the table  we have to use the cascade option whenever let us say the employee type is updated or deleted that means to say that if the employee relation is deleted from the database  this has to in turn delete all the insurance record relations from the database itself because insurance records do not have any existence without the employee records so the three step here in order to translate a weak entity type is to first identify or is to first locate the identifying relationship and see which entity type is this weak entity type associated with and use the primary key of that entity type as the key for the weak entity type or the record of or the relation for the weak entity type and make it into a foreign key of this entity type and use cascade options whenever updations or deletions or performed on the strong entity type  refer slide time  00  25  20  let us move on with translating relationships so how do we translate  let us take the simplest form of relationship again the 1  1 relationship we saw what happens or how do we translate 1  1 relationships  when weak entity types are considered now let us consider an entity type which is not weak but still is involved in a total participation  this is shown in the figure here the figure shows a relationship type called managed by which relates two different entity types that is a department and manager and there are attributes  relevant attributes are shown for each of them that is the department has a key attribute called department id and a manager has a key attribute called employee number and the relationship itself has a key attribute called secretary that is a secretary is assigned for a department that is managed by a manager that is the secretary attribute does not have an existence without an existence of this relationship that is if a department is not managed by a manager then there is no secretary that is associated with this now how do we translate this ? the translation is again shown in the slide below so first create an entity type or create a relationship  create a relation called manager let me repeat this again  for this relationship create a relation in the rdbms model called manager with the following attributes now you might be wondering why should we create a relation called manager  why not department now let us think about it a little further see in this slide here that manager is a strong entity type  it is not a weak entity type however it is involved in a total participation in this relationship type what is a total participation ? the total participation is that the entity type does not have any existence without being participating in a relationship type of this kind so what it essentially means here is that a manager has no existence that is a manager would probably be just an employee  so a manager would have no existence unless here she is associated or is given a department to manage it is the entity type that is involved in the total participation is taken as a primary entity type or the base entity type  primary relation called manager and then employee number becomes the key here that is the key for manager and the department id which is the primary key for department becomes a foreign key in manager and whatever attributes are associated with the relationship itself become attributes of this relation here that is of the manager relation here so therefore the manager relation has a primary called primary key called employee number and a foreign key called department id note that this makes sense  when we note that manager does not have any existence without this relationship that is referred to the problem of referential integrity in relational data model what is the referential integrity stipulate ? whenever a foreign key refers to a tuple in another relation  the tuple should exist that is it should refer to an existing tuple in the other relation now if we had department as the base entity or the base relation here  we can not use employee number as a foreign key because the manager relation wo n't even exist before this relation that is managed by is formed on the other hand department has an independent existence without whether or not a manager is associated with it therefore that forms a rational behind  why we choose the entity type which is involved in total participation as a base entity type for the translation  refer slide time  00  30  03  so let us summarize this previous slide once again so in any 1  1 binary relationship between types s and t  choose one of them as a base relation in case one of them is involved in a total participation choose that as the base if neither department nor manager where to be involved in total participation  it doesn ’ t matter which you are going to choose as the base relation include the primary key of the other entity type as a foreign key in the base relation and include any relationship attributes as attributes of the base relation  refer slide time  00  30  42  consider the example shown in the slide here what happens if in a 1  1 relationship both entity types that is both entity types that are participating in this 1  1 binary relationship are involved in a total participation take a look at the slide here the slide shows two entity types project and consultant and each project is uniquely identified by a project id and each consultant is uniquely identified by his or her pan number and there is a relationship type called consultation which has its own attribute called secretary and it ’ s a 1  1 relationship and both of them are involved in a total participation that is a project has no existence unless it is being consulted by a consultant and a consultant has no existence unless here or she is associated with a project so neither of them will have independent existence without the other in such cases we can not identify any relation as the base relation if we identify project as a base relation and try to use pan number of the consultant as the foreign key then referential integrity could be violated it ’ s the same in the other way around as well if we use consultant as the base relation and try to use project id as the foreign key  again there is a chance of violating the referential integrity in such cases the simplest way is to take the relationship type  in this case the consultation as the base relation that is form a relation called consultation and use project id and pan as the primary key of consultation and then all of the attributes from both of them will become attributes of this relation  refer slide time  00  32  40  so in case both entities in a 1  1 binary relationship are both in total participation then we merge both of the entity types into one usually in the name of the relationship that is in the name of the relationship called consultation in the previous example now let us see how do we map one to n relationships what is a 1  n relationship ? that is n entities of one of the entity types could be associated with one entity of the other entity type  that is it form some kind of a tree relationship that is one entity being associated with n different entities of the other type so the slide here shows such an example that is employees works in department that is employee is an entity type  so n different employees can work in one department that is one department may have several employees but each employee is associated with only one department and of course there are keys called employee number for employee and department id for department the slide also shows how we can reduce this to a relation the simplest way is to take the entity type on the n side of a relationship  refer slide time  00  33  02  so in this case the employee  so take this as the base that is translate it into a relation called employee and the primary key of employee becomes the key of the employee entity type here and the department id becomes a foreign key here  refer slide time  00  34  19  so this is as simple as that that is for each binary one is to n relationships identify the relation s that represents the entity type on the n side why is this so ? because each entity type on the n side uniquely identifies a department that is uniquely identifies the entity type on the other side therefore we can use the primary key of the entity type on the other side as a foreign key in the base relation therefore use this as the base relation and create a relation including the key of the other entity type as the foreign key  refer slide time  00  35  03  how do we map m n relationships ? now what is an m n relationship ? an m n relationship essentially says that m different entities of the first type can be associated with n different entities of the second type therefore there is no unique identification that is given an employee in the previous case  one could uniquely identify the department with which the employee is working in because each employee can work in at most one department on the other hand here let us say a relationship called deputed to  so an employee could be deputed to several departments let us say so m different employees can be deputed to n different departments and of course employee has employee number as the key and department has department id as the key and so on and deputed to also has an attribute called record number which maintains a record of which employee is deputed where and so on so in order to translate such relationships  note the steps that are shown in the slide here there are three different relations that are formed  one is the employee relation that is one of the entity types of this relationship type so the employee relation is formed with employee number as the primary key and all the other attributes that form the employee entity type similarly department relation is found with department id as the primary key and all other relations and then a separate relation is created for the relationship type itself so deputed to becomes a separate relation by itself and then uses employee number and department ids as foreign keys and also as the primary keys of this relation and whatever attributes that belong to the relation becomes part of or belong to the relationship type becomes part of the relation that is created here that is the record number attributes become one of the attributes of deputed to relation note that we can not move this record number that is the attribute of this relation either the employee or a department because it does not uniquely identify either employee or department each employee could be associated with n different record numbers because they could be associated with n different departments and similarly each department could be associated with m different record numbers because m different employees could be working in that department so this slide shows summarizes  how m is to n relationships are translated in a m  n binary relationship  it is not possible to collapse the relationship into one of the entity types because neither of the entity types uniquely identifies the other entity type therefore a separate relation is required and usually this is in the name of the relationship type itself so a separate relation is required in order to complete the mapping and of course the cascade option should be used whenever updates are performed on any of the relations pertaining to the entity types  refer slide time  00  37  57  that is whenever the entity type called employee or department is updated or deleted then these changes should be cascaded so that they are reflected in the deputedto relationship as well that is if employee entity type is deleted then of course the deputed to relationship should also be deleted now one might ask the question is it possible to use the strategy that is use the relationship type as the base relation rather than any of the participating entity types can we use this strategy for mapping 1  1 and 1  n relations as well because m is to n is simply a generalization of 1  1 and 1  n relationships so of course it is possible that is take the example of employee works in department that is n different employees working in one department we can still create a separate relation called works in where it can use the employee number and the department id as the foreign keys of this relation however it just creates an extra relationship or extra relation in the database that is totally unnecessary but this is sometimes actually attractive to use than collapsing the relationship into one of the relations especially where  especially if we have to avoid null values  refer slide time  00  39  06  that is especially if we have to  especially if we have cases where there are some employees who do not work in any department if n different  if an employee can be associated with at most one department it means that an employee can also be associated with zero departments so in that case the department id field of employee would be null it does not violate referential integrity because remember that referential integrity says that a foreign key should refer to an existing tuple or else should be null therefore it does not violate referential integrity but creates a lot of null values in the database schema  in the database itself so if we have to avoid null values  it is actually preferable to use the relationship type as the base relation when performing the translation how do we map multi-valued attributes ? we have seen how to map composite attributes and a simple attributes and keys and so on but what happens if there are multi-valued attributes composite attributes are different from multi-valued attributes in the sense that each of them can have several different domains that is it is just a combination of several simple attributes so we just open up the combination when we are translating a composite attribute and then include all the simple attributes that form part of the composite attribute  refer slide time  00  41  11  on the other hand a multi-valued attribute is not composition of several sub attributes instead it is an attribute that can take on several values instead of one value and the example  the slide shows is that of a bird so a bird has a multi-valued attribute called color so what is the color of this bird ? a bird could have several colors it need not have just one color and of course there is a primary key called species which identifies each bird uniquely so in order to translate multi-valued attributes  take a look at the lower half of the slide which shows two different relations which make up this translation the first relation shows a relation called bird with species as a primary key and all other attributes except the color attribute  all other attributes of the entity type called bird and then a separate relation is created called bird colors where species and color are both included and are both of part of the key that is combinedly define the key of this bird therefore we say that birds species eggs has color y  eggs has color z  eggs has color a and so on so the color attribute may be repeated in several or several tuples or rather the species attribute may be repeated in several tuples  once for each different color that the bird can take and both of them that is species and color become the primary key for bird colors  refer slide time  00  43  34  so for each multi-valued attributes of a given entity type  we have to create a separate relation that has a primary key of s paired with all possible values that the multi-valued attribute can take and of course the cascade options should be used for referential integrity on the bird relations that is whenever the bird relation is deleted or updated  the corresponding changes has to be made in bird colors as well how do we map n-ary relationships ? until now we have been looking at binary relationships  what happens when there are n-ary relationships and different entities forming part of the relationship the slide here shows such an example that is the standard example of suppliers supplies part to project so there is a supplier who is uniquely identified by the supplier or the sales tax registration number or something streg number and there is a project that is uniquely identified with project id and there is a part that is uniquely identified by part id and then the supplies relation which relates all of these three different entity types so the simple way of translating this is to use a separate relation called supplies as the base and of course separate relations each for supplier  project and part with their corresponding primary keys and the supplies relation which has the primary keys of each of these relations has the foreign key of this relation  refer slide time  00  44  05   refer slide time  00  45  15  so for each for each n-ary relationship of any type r where n is greater than 2  we have to create a new relation s to represent this relationship  to represent this relationship type r and of course the primary keys of the participating relations become foreign keys in this new relation and the cascade option should be used for all of the relations that correspond to the entity types that participate in this relationship type what happens if one of the relations in an n-ary relationship type is a weak entity type ? that is let us say part is a weak entity type  there is no existence for part unless it is associated with a supplier supplying it to some project in that case we have to identify  we have to first identify the entity type which gives an existence to part and the part relation here has only foreign keys it does not have any primary keys but the supplies relationship does not change that is it doesn ’ t have any part id  it just has a  it simply has the supplier primary key and the project primary key without the part primary key therefore we get two different relations that have foreign keys and they do n't have their own primary keys  refer slide time  00  46  56  so that bring us to the end of this session that talked about  that gave a  that talked about how we can map er models into relational database models using several different rules and of course this is not a comprehensive set of rules because there are several other sets of rules used for example in derived attributes or enhanced er models like generalization and specialization which are not covered here but all of them in totality are used to create the basis for any kind of a tool  software tool that can translate between given er schema and its corresponding relational schema so this slide shows a summary of each of this mappings that is it gives the set of thumb rules saying if this is what is given in the er model then what happens in the relational model so in an er model if an entity type is given then a corresponding entity relation that is a relation in the name of the entity type is created if a 1  1 or a 1  n binary relationship is given then we create corresponding foreign keys from the n side to the one side or from the weak entity type to the strong entity type in this relation so we create a relation and create appropriate foreign keys from them if a m  n relationship type is given as shown in the slide here then we create a relation with the name of the relationship type  so there is the within quotes which shown as relationship relation with two foreign keys that is one for each entity type that participates in this relation  refer slide time  00  46  57  if an n-ary relationship type is given  it is still the same strategy that is we create a relationship relation with n different foreign keys that is one for each entity type or rather n different foreign keys as long as these entity types are strong entity types if there is a simple attribute in a er model that simply becomes an attributes in one of the relations in the relational model if it is a composite attribute in the er model then it becomes the set of simple attributes that is you take the simple part of all composite attributes that is just go on finding the simple attributes that form the composite attribute and then make all of them as part of this relation  refer slide time  00  49  47  if it is a multi-valued attribute then we need to create a separate relation and a foreign key that is you have to associate the primary key of the base relation with each possible value of the multi-valued attribute if it is a value set it becomes a domain  value set in the er model it becomes the domain and a key attribute in the primary in the er model will become either a primary or a secondary key in the relational model so that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 8 functional dependencies and normal forms two different kinds of database models now  when we are talking about how to design a database system the first data model that we saw was the entity relationship model which is especially used for building conceptual schema  refer slide time  00  01  31  so as we have defined a conceptual schema is something that is meant for human consumption that is it is meant for communication with end users so that they can understand what the database designers have really understood about their domain  what kinds of entities exist  what kinds of relationship exist and so on but an entity relationship schema is not really the best data model for storing data in a computer and for storing data that is the internal schema or the physical schema it is usually the relational model that is used and the relational model in some sense has some kind of common terms with the entity schema like we have seen the concept of keys  foreign keys and referential integrity and so on which hold both for entity schema and the relational schema however the relational schema is mainly meant for the physical schema or the internal schema while the conceptual schema is meant for human consumption  while the physical schema is meant for the computer now usually what happens in a database design process is that we use some kind of tools or some kind of case tools to build an er model or of the information system that we are trying to design and there are number of automated techniques by which or number of automated tools which can take a er representation of a schema and then convert it into a relational schema however while this is automatic  this need not always result in the most optimal form of the relational schema and what do we mean by optimal ? we are going to try to formulize this notion today and try to see which kinds of relational schema or schemas are better than which other kinds and are there some kind of techniques  formal techniques that i can use by which without trying to understand the semantics  just given a relational schema doesn ’ t matter in what context can i try to optimize it into a fashion that can help in efficient storage and retrieval of data so today in this session we are going to look at an important concept of relational database design namely the idea of functional dependencies the notion of functional dependencies is fundamental to the design of different normal forms which we are going to see in a relational database schema so what are functional dependencies ? functional dependencies are a frame work for systematic design and optimization of relational schemas of course there are many more nonsystematic options for optimization for example if we know something extra about the operational domain or the specific database in use  we can up always optimize something more than what we can do with the relational  with functional dependencies  refer slide time  00  04  20  however functional dependencies are some kinds of techniques that are database independent and can be used in a formal fashion in order to achieve some level of optimization or some level of efficiency in terms of database design we are going to see how we can define keys we have looked at the notion of super keys  primary keys and so on how we can define the notion of keys using functional dependencies ? in fact functional dependencies are more generalization or is a generalization over the concept of keys and they are crucial in obtaining the correct normalized schemas when we are trying to design database systems  refer slide time  00  05  33  so let us first define the notion of a functional dependency let me first take the definition here  the formal definition here and try to explain it in formal terms the definition reads in any relation r  if there exists a set of attributes a1 to an and an attribute b such that if any tuples in the relation have the same value for a1 to an then they also have the same value for b so essentially what it means ? this is a formal way of saying that these set of attributes a1 to an uniquely determine the value of b that means if two or more tuples have the same value of a1 to an  it ’ s not possible essentially that means to say that is they should have the same value of b they ca n't have different values of b because these sets of attributes a1 to an uniquely determine the value of b so this is called a functional dependency and a functional dependences is written as shown in the slide  here it is written as a1 a2 extra an and there is a right arrow to b so an important thing to note here is that functional dependencies define properties of the schema and not of the tuples in the schema what is that mean ? essentially this kind of dependency that exists between a1 to an and this attribute b should be a property of the entire set of tuples that are there in the schema for example i might say that the employee number uniquely determines the name of the employee in any relational database schema however we can not say that let us say the employee name uniquely determines the age of an employee because there could be two or more employees having the same name but different age but it could well be possible that there is certain employee with very rare name and there is nobody else who has that name and therefore for this particular employee  the name uniquely determines the age  uniquely determines his or her age but this is not a property of the schema in itself  this is a property of that particular tuple of this particular employee who has this rare name so that there is nobody else with that name therefore we can uniquely identify the person and the age of that person so when we are talking about functional dependencies  we are talking about properties of the schema which holds for all relations or all tuples in the schema  refer slide time  00  08  21  in any given schema there could be many functional dependencies  let us say the set of attributes a1 to an can uniquely define  it ’ s also called uniquely determine b or defines b or so on so a1 to an can uniquely determine b1 or b2 or b3 until bm so this is written as shown in the slide here that is a1 to an and right arrow b1 to bm now why is this called a functional dependencies  what is so functional about functional dependency ? if you are familiar with the mathematical notion of a function which is a special kind of mapping or a special kind of relation between sets  you can see here that the notation that is used for example a1 to an or uniquely determining some attribute b is precisely the notation used for a function it basically determines because a1 to an uniquely determines a particular attribute b  it acts as a mapping or it act as a function which maps this set of attributes a1 to an uniquely to each b that it defines this precisely acts as the mathematical notion of a function however there need not be any computation that determines this unique definition for example the employee number may uniquely determine name  however just given the employee number i mean i wo n't be able to compute  i wo n't be able to determine what is the name it ’ s just that each employee number has a different or identifies an employee with a different name or identifies uniquely each employee however  it is not possible to determine what is the name of the employee given just the employee number  we still have to access a database to do that  refer slide time  00  10  24  so let us revisit the notion of keys now when we say functional dependencies as you might have noticed the notion of functional dependency is actually a generalization over the notion of keys that is in the previous slide here if a1 to an uniquely determine the set of all attributes which form the relation let us say a1 to an combined with b1 to bm forms the entire relation then we see that a1 to an is nothing but a key for this relation that is for the entire relation comprising of a1 to an and b1 to bm and similarly minimal super keys or candidate keys can be defined analogously let us take some examples now consider a relation of the form that is shown in the slide here the slide shows a relation called movies which has the following set of attributes  it has title  year  length  film type  studio and star so  going by the standard definitions of what is a movie and what is title and what is the year of its release  the length of the movie and so on we can from common sense reasoning  identify some kinds of functional dependencies  refer slide time  00  11  11  for example given the title of a movie and the year in which it was released  i can probably uniquely determine what is the length of that movie and similarly given there is a bug in the second slide here  second functional dependency here given a title and the year of release of a particular movie  i can probably determine what is the film type given a movie of a particular type  of a particular name and the year in which it was released  i can probably say well this is an art movie  this is a commercial movie  this is social documentary  this is a comedy tragedy whatever similarly i can probably also say title  year uniquely determines say film studio however title  year may not uniquely determine the star that is the actors who star in that movie because there could be more than one actor who have stared in that movie and you can not uniquely determine given the title and the year you can not uniquely say well this is the movie star who acted in that movie because it need not be complete  there could be other stars as well  refer slide time  00  13  03  so let us see what kinds of properties we can identify about functional dependencies and what we can do with these properties note that if in any given relation r  if i have a functional dependency of the form a to b where a is a set of attributes now i am using the term a that is without a subscript to determine actually a set of attributes so if the set of attributes a defines a set of attributes b and the set of attributes b uniquely define another set of attribute c then we can easily identify the functional dependency saying a also defines c so there is an example here  if the employee number is given in such a way that it also defines the job of the employee let us say all employees who do supplies work are given numbers between 100 to 200 and all employees who do administrative works or a clerical job is given numbers between 201 to 300 and so on so employee number suppose uniquely defines job and let us say job also uniquely define salary that is every employee of a particular job has the same salary then we can easily say that the employee number actually defines salary that is given just the employee number  i will be able to determine what is the salary for that employee some more definitions suppose we have two sets of functional dependencies a and b now again a and b are sets here and we have two sets of functional dependencies a and b and that is a defining b and c defining d now these functional dependencies are said to be equivalent  if the set of all relations that satisfy the first functional dependency is the same as the set of all relations satisfying the second functional dependencies similarly a small generalization over this definition  we say that a functional dependency s follows another fd called t  if the set of all relation instances satisfying t also satisfies s  refer slide time  00  14  44  so we will take up examples here  the follows property of a functional dependency is extremely important by which we will be able to compute what is called as the closure of a functional dependencies that is suppose i give some kind of functional dependencies saying a defines b what else can i say about that ? what else can i infer from this functional dependencies is what we are going to take up before we move on to inferring functional dependencies  we need to complete a few more definitions the next definition that we are going to take up is the notion of a trivial functional dependency now a trivial functional dependency may look of course trivial but it is quiet important when we are trying to define let us say normal forms  refer slide time  00  16  07  now have a look at the example in the slide here this slide shows a functional dependency of the form title  year defines title which is obviously true every attributes defines itself that is every attribute uniquely identifies itself  therefore any functional dependency in which the right hand side is contained within the left hand side that is the antecedent or the right hand side of a rule is a subset of the left hand side of a rule then it is said to be a trivial functional dependency if there is at least one element in the right hand side which is not part of the left hand side then it is called a non-trivial functional dependency and usually we are interested in what are called as completely non-trivial functional dependencies which says that none of the elements in the right hand side belong to the left hand side we will see that this notion of triviality is not so simple because essentially when there are circular dependencies  it becomes quiet tricky to handle functional dependencies which are removed triviality from functional dependencies we now come to an important aspect of functional dependencies  the notion of closure of fds what do we intuitively understand by the term closure ? if we are familiar with the notion of closure and say discrete mathematics  you would probably recall the definition that the closure we say that a particular algebra is closed when i say  when i take certain elements from or certain operands from a set that defines the universe and then perform an operator that performs some kind of a function on these operands and the result of this function also goes back into this set  refer slide time  00  17  42  for example if i take two integers and add them  the result is again an integer if i take two integers and subtract them  the result is again an integer now this is what is called as closure that is i take certain operands or take certain elements from a universe  perform a function on them and the result of that function also belongs to the same universe so let us define the function  the notion of closure on functional dependencies in an analogies fashion the closure of functional dependencies defined by a that is a is a set of attributes in any given relation r is the set of all attributes that are eventually defined by a that is how we can eventually define or uniquely determine what all attributes can be uniquely determined eventually by a for example let in a given relation  let the set of attributes a uniquely define a set of attributes b similarly let the set of attributes b uniquely define two other sets of attributes c and d now let the combined sets of attributes b and d uniquely determine another set of attributes called c then we say that the closure of a contains all of these attributes that is a union b union c union d union e that is all of that and actually because all of these attributes can be eventually defined from a from a we can eventually define b  from b we can uniquely define d and combining b and d we can uniquely define e so all of these attributes contained in all of these sets can be eventually uniquely defined by a what about elements that lie within a  can they be uniquely define by a ? the answer is yes this is because of the trivial property of functional dependencies that is every attribute in a uniquely defines itself which is a triviality rule however closure also just a union b union c union d union e that is shown in the slide does not complete the closure of a  refer slide time  00  20  52  if for example there is a subset of a or subset of any element of closure of a and if that subset of attributes defines some other sets of attributes f such that f is not part of the closure of a then we also add f to the closure of a  refer slide time  00  21  13  so  let us first see look at an algorithm by which we can compute the closure of a functional dependencies then we look at a few examples of closures so given a relation r and a set of attributes a  how do we compute closure of a ? so initially we start with the set saying the closure of a equal to a that is equal to all elements of a this is true because of the trivial functional dependencies  every element of a trivially defines itself now for every a prime that is a subset of a  if there exists a functional dependency of the form a prime defines b and b does not belong to a that is b is a set of attributes which does not belong to a as of now then just add b to the set of elements in the closure of a and repeat step two until no more attributes can be added to closure of a so the closure of a set of attributes a is also denoted by a with a superscript of plus that is a plus and also note that suppose we run this algorithm and a plus contains the set of all attributes of r  what can we say about the set of elements in a ? a is of course a super key of r because using a  we can eventually uniquely define all elements of r so we now come to the next property of functional dependencies  the notion of inferring functional dependencies or following that is how do we find out functional dependencies which follow from one another  refer slide time  00  22  53  given a set of functional dependencies and given no more knowledge about the domain in which a relation exists  what can we infer about any more functional dependencies that exist in r so  we have already seen one rule for inferring which is a transitivity rule which we will revisit again now and before that note another rule that is shown here suppose a b c and d are sets of attributes of r such that the following functional dependencies exist that is a uniquely defines b  b defines c and c defines d now based on the closure property and transitivity  we can easily say that a defines d however there could be some elements of d which are actually contained in a that is d need not be completely disjoined from a so let d subscript a that is d a be the set of all attributes in d such that they belong to a that is they are actually a subset of a now take away these attributes that is let d prime be defined as d minus d a that is shown in the slide here once we do this  we see that we have actually inferred a non-trivial functional dependency that is a defines d prime that is we started by saying that because of the transitivity rule a defines d however there is some triviality that is there in this definition and once we remove that triviality  we have actually encountered or we have actually inferred a new functional dependencies which is non-trivial that is which is of the form a defines d prime so functional dependencies which are specified or which are given to begin with are called stated functional dependencies and fds which are derived are called inferred functional dependencies  refer slide time  00  25  18  now  given a set of relation  suppose we have a set of functional dependencies that are stated this set of functional dependencies is called the bases of the relation for example in the movie relation that we saw where a movie is defined by title  year  film type  star and so on the functional dependency is the title and year define the length of a movie was given similarly the title and year define the film type was also given  so all these functional dependencies that are given are called the basis of the relation now if the basis of functional dependencies are such that no subset of this basis is also a basis that is none of these functional dependencies that form the basis can be derived from one another then it is said to be a minimal basis for the relation  refer slide time  00  26  18  so how do we go about inferring functional dependencies that is computing the closure or getting inferred functional dependencies and so on how do we go about inferring functional dependencies  given a basis that is given a set of functional dependencies for this  there are set of axioms or set of rules that define how functional dependencies behave these are called armstrong 's axiom which are quiet useful when we are talking about properties of functional dependencies the first property is the notion of reflexivity which is like saying  which is easily obvious based on the triviality rule that is if set of attributes b is a subset of the attributes a then a functionally defines b for example  theater and year or rather the title and year of a movie functionally defines title that is if i have a set of attributes  these sets of attributes functionally define every possible subset of this attributes similarly  the second role is that of argumentation that is if i know that there exists a functional dependency of the form a defines b that is theatre  year defines length then i can add a set of attributes to both sides of the functional dependencies without violating the dependency for example i can say that if title and year of a movie determines the length of the movie then i can add an attribute called star or studio to both sides of the functional dependencies without altering the semantics that is i can say title  year and studio of a movie uniquely determines the studio and the length of the movie also note that i need not add studio to the right hand side because it forms a triviality rule that is there is some triviality that entails from adding the same attribute in both sides i can as well add  i can just add a new attribute just to the left hand side and still the functional dependency holds take the same example again suppose i say title and year uniquely determines the length of a movie then i can as well say that the title of the movie  the year of the movie and the studio in which the movie was shot uniquely determines the length of the movie which is fine because adding new information to some set of attributes that uniquely determine something does not alter the dependency similarly the last rule is that of transitivity which we have already seen that is if a uniquely defines b and b uniquely defines c then we can infer that a defines c the next concept that we learn here is the notion of projecting functional dependencies what happens if i compute a project operation on a relation you know what a project operation is in standard relational algebra a project operation takes certain columns or certain attributes of a relation and produces a new relation out of these attributes that means it actually throws away certain attributes from the original relation now suppose let r be a relation and f of r be the set of all functional dependencies in r now suppose i project another relation s from this new relation r by throwing away certain attributes now what can we say about f of s ? what kinds of functional dependencies exist in f of s ?  refer slide time  00  29  23  obviously for example if i say that from the movie relation  if i throw away the year attribute of this relation then obviously i can not have the functional dependencies which says title  year uniquely determines length because the year doesn ’ t exist in the new relation at all therefore at this point  at the first step we can say that functional dependencies in the new relation s should satisfy the following properties what are these properties ? firstly they should follow from the functional dependencies in r that is we should be able to infer them from the functional dependencies that existed in the earlier relation secondly and  which is also quite obvious that they should involve only attributes of s obviously they can not involve attributes of r which do n't exist in s a functional dependency only has to involve attributes that exist in the current relation so how do we compute the functional dependencies on a projection ? now consider this example rather than going through a formal set of rules let me explain it by an example now given a relation r containing the following attributes a b c and d and let us say the following functional dependencies that is a defines b  b defines c and c defines d  refer slide time  00  31  24  now suppose s is projected from r and b is thrown away from r in getting s so s has just three attributes a c and d now what should be f of s ? that is what should be the set of functional dependencies that lie in s obviously we can not have a defines b because b doesn ’ t exist at all in this relation and we also can not have b define c because again b doesn ’ t exists in this relation so but can we say anything else about these dependencies now going back to r  let us see what happens when we compute the closure of a let us compute the closures of each attributes so  i have not included the trivial functional dependencies in computing the closures that is firstly we start from a defines a obviously then we have a given rule which says a define b  a basis rule from which we can infer that a define c because there is another basis rule which says b define c and from which we can still infer a defines d because there is another basis rule called c defines d so in r  the closure of a is a defines b  a defines c and a defines d in s  all we need to do is take away all these or throw away all this functional dependencies that contain attributes which do not exist in s so here a defines b is one such attribute  is one such functional dependencies it contains b which does not which is not part of s therefore the set of  the closure of a in s is essentially a defines c and a defines d or a c d similarly the closure of c in s is c defines d that is c  d in this case and the closure of d is just d now since the closure of a contains all attributes of s that is a c and d  we do n't need to compute anymore closures that is we do n't need to compute the closures of a c or a d or a c d because we already have all attributes of a  we already know how to uniquely determine each attributes of a therefore the functional dependencies that exist in s are a defines c  a defines d and c defines d and of course there are trivial functional dependencies like d defines d and c c defines c and a defines a which are not shown as part of this set here so a simple way to compute the closure or compute functional dependencies in a projection is to first compute the closure of the set of all functional dependencies in the original relation and then throw away everything which contains attributes that do not belong to the new relation  refer slide time  00  35  10  so what is a use of all this functional dependencies and closure and transitivity and axioms and so on these sets of underpinnings are used for normalizing or optimizing relational schemas for better performance what do we mean by optimizing or what are we optimizing against  what is the property that we are trying to remove  what is the undesirable property that we are trying to remove this undesirable property is the property of redundancies in relational schemas if a relational schema is badly designed then it is going to contain several redundant information which results in a number of anomalies what are these kinds of anomalies and what kinds of redundancies are these ? let us have a look at them redundancy of course means that some kind of information is repeated across different tuples the same information saying the title is this one or the year is this one or the star is that one and so on so if the same information is repeated across several different tuples then we encounter two kinds of anomalies what are these anomalies ? the first anomaly is that of updation suppose i need to update a data element using let us say the update clause of sql i can not update in just one tuple  especially if this particular information is repeated across several different tuples if the title of a movie is repeated across several different tuples in the database and later on i see that the title of the movie is entered incorrectly  there is a spelling mistake so i need to change the title of the movie in every tuple there in which it occurs similarly  the notion of deletion anomalies suppose i delete a tuple which suppose i need to delete a tuple about a particular movie  given a particular title i need to delete all tuples which contain this title as its attributes so i need to essentially search the entire database and delete it in several different places  otherwise it again creates anomalies so how do we design relational schema  so that we can remove redundancies and remove these kinds of anomalies  refer slide time  00  37  37  now have a look at the movie relation once again the movie relation has the following attributes that is title  year  length  studio and star and we also have the functional dependencies which says title and year uniquely determine length and title and year of a movie uniquely determine the studio in which the movie was shot however title and year does not uniquely determines star because there could be more than one actor who have stared in the movie therefore for every actor who has stared in this particular movie  you have to repeat the information of length and studio across these tuples let us say some movie has two or three different actor say shahrukh khan  hrithik roshan or whatever now in a given movie let us say some movie x y z that is shot in the same year in the same studio and has the same length  it ’ s only the last field the star which changes from say shahrukh khan to hrithik roshan and whatever so for each actor who was stared in the movie  i need to repeat all the other information that forms part of this tuple which is what is the notion of redundancy and suppose after doing all this  i find that i have entered the length of this movie incorrectly and i need to change the length of the movie  i need to change it in all of these different tuples where this is stored and similarly the problem of deletion  refer slide time  00  39  22  so anomalies are removed from a relation by the process of decomposition now what is meant by decomposition ? given a relation r containing a set of attributes a  you decompose the set of relations into two different relations s and t such that the sets of attributes in of s and t are a subsets of the attributes of a and there are no anomalies in s and t now a decomposition that does not contain any anomalies is said to be in what is called as boyce-codd normal form or it ’ s also abbreviated as bcnf  so a bcnf has the following property suppose given a relation r with the set of attributes a  it is said to be in bcnf if there is a non-trivial note the emphasis on the word non-trivial here  if there is any non-trivial functional dependency of the form a prime defines a double prime that means a double prime is not a subset of a prime so if such a non-trivial functional dependency exists then it means that a prime is a super key of r that is there is no functional dependency of the form a prime defines a double prime in which a prime is not the key if that is the case then the relation is not said to be in bcnf so we will take up examples of this once we complete the notion of decomposition into bcnf and when it becomes more clearer  refer slide time  00  40  51  now how do we decompose a relation such that it forms or it becomes comply into bcnf now suppose in a given relation r  let there be a functional dependency of the form a prime defines a double prime which violates bcnf what do we meant by violating bcnf ? it means that a prime defines a double prime is firstly non-trivial that is a double prime is not a subset of a prime and a prime is not the super key of r that is it is some other sets of attribute in order to bring r into bcnf  we decompose r as follows first take the set of all attributes that are defined by a prime now a prime defines a double prime  it may define something else and so on now let b be the set of all attributes that lie in the right hand side of any functional dependencies that are defined by a prime now remove the set of all attributes a prime along with b and form a separate relation and retain the remaining set of attributes along with a prime to form the other part of the decomposed relation r let us have an example which makes this very clear  refer slide time  00  42  22  consider the movies example once again let us have a look at this relation movies having the following attributes title  year  length  studio and star now here the following functional dependency holds that is title and year uniquely determines length title and year uniquely determines studio and title and year uniquely determines  it does not determine star actually so this is actually a bcnf violating functional dependency because title and year obviously can not be the key for this relation because it does not uniquely determine star so title and year is not a super key as the star attribute is not in the closure of title and year so to decompose this relation  just remove title and year along with length and studio so there is no star here  just remove title and year along with length and studio that it define and put them in a separate relation and retain title and year along with star to form the other relation therefore we get two kinds of relations that is movies is divided into title  year  length and studio because title and year define length and title and year define studio so we have separated them from movies and whatever is left out that is star is combined with a prime which is title and year and retained as it is  refer slide time  00  43  33  so movies one and movies two are decomposed forms of movies and it is also easy to verify that movies one and movies two are bcnf complaint  refer slide time  00  44  08  there is one property of bcnf relations  the notion of two attribute relations suppose i have any relation that has just two attributes  we do n't need to do anything  it is always bcnf complaint how do we ensure or how do we prove that a two attribute relation is always bcnf complaint consider these four cases  let us say there is a relation r which contains just two attributes a and b now there are four possible scenarios the first one is there is no functional dependencies between a and b that is there exists no non-trivial functional dependencies only trivial functional dependencies are there  a defines a and b defines b in which case r is definitely in bcnf secondly there is a non-trivial functional dependency that is a defines b but there is no functional dependency of the form b defines a which is also no problem because in this case a becomes the key because there are just two attributes and a is defining b and b is not defining a so a is the key of the relation and the relation is in bcnf similarly if b defines a and a does not define b then b becomes the key and suppose a defines b and b defines a which is also no problem because both a and b are keys and both a and b are candidate keys and we can use one of them as super key or the primary keys which is fine and the relation is also in bcnf  refer slide time  00  45  46  we now come to another form of normalization which is called the third normal form of a relation the third normal form is useful because in some cases it is not possible to decompose the relation such that the decomposed relations are bcnf complaint  it is not possible to decompose without losing some information now let us understand this by an example now consider another relation of the form which is shown in the slide here let us say we have a relation called drama and it has the attributes title  theater and city that is there is a particular drama troop that is performing a drama having a particular title in a particular city and in a particular theater in the city so therefore we can identify the following functional dependencies  the first one is title and city the title of the drama and the city in which it is being played will probably determine the theater as well so we can say that in this drama played in this city is being played in this theater only suppose there exist a functional dependencies of this form and let us also assume for the sake of argument in this case that given the name of a theater  we can uniquely identify where the theater is given the name of a drama theater  let ’ s say something like kalamandira or guru nanak bhavan or whatever  some kind of theater name we immediately know which city contains the theater now fd2 violates bcnf  as you can see here because theater is uniquely defining city but theater is not the key  in fact title and city form the key  title and city define theater so given a title  drama title and a city we can identify theater therefore this is a bcnf violation in violating fd however  based on the decomposition rule if we decompose drama into two relations title  theater and theater  city base based on this rule here because theater defines city now based on this rule if we decompose drama like this into drama one and drama two  it will actually be incorrect why ? because once we perform the join  once when we decompose relations and we join back the relations  we should retain all the properties of the original relation now when we perform the join between drama one and drama two then the key constraint will no longer hold let us look at this by an example  refer slide time  00  48  03   refer slide time  00  48  46  let us say drama one contains title and theater entries like this  there is a particular drama with a title say yugant and it is in a particular theater now there is another drama with a different title but in the same theater maybe at some other time now this theater uniquely identify city  let us say given a theater name i can uniquely identify city now if i join  if i perform a natural join between theater that is drama one and drama two based on the attribute theater what are we going to get  refer slide time  00  49  23  we will get a table like this that is a title  theater  city and title  theater  city now if you see given a title and a theater  there is no unique title and title and city is no longer the key that is we do n't know we can have possibly different  let us say we can possibly have different sets of attributes that are defined by the same title and city combination so it does not uniquely determine the title that is theater and city does not uniquely determine the title of the drama such dependencies or such discrepancies occur because of a particular property and which is what we are going to see in this slide here so we are going to define the notion of third normal form which is essentially relaxation of the second normal of the rather the bcnf or the boyce-codd normal form assumption what is a relaxation that we are doing here ? if you notice in the bcnf violating constraint that is theater defines city  the right hand side of the relation that is right hand side of the functional dependency called city is actually part of the primary key that is title and city was the primary key  refer slide time  00  50  05  so in order to accommodate such cases we use a third normal form assumption which says that any relation r is said to be in third normal form  if there exist any non-trivial fd of the form a defines b either a is a super key which was all the condition we need for bcnf and here we have an extra condition which says or b is a member of some key so an attribute that is a member of a key is called a prime attribute so therefore either b should be a prime attribute or a should be a super key which is what makes the relation into a third normal form relation we shall not be going into details of how to prove properties of third normal form relations however we will suffice it to note that it is a slightly general form of the bcnf or the boyce-codd normal form let us quickly visit the last kinds of dependencies and the next normal form that results from it  which is known as the multi-valued dependencies now even in bcnf  bcnf is a strict form of decomposition so even in bcnf we have not fully removed all possible redundancies consider this following example here the slide shows an example relation called drama which has the following attributes title  theater  director and genre of the drama and we note that drama is in bcnf because let us say title is the key  let us say each drama is played in precisely one theater for the sake of argument therefore title uniquely determines theater and director and the genre of the drama  refer slide time  00  51  33  however it may well be possible that a given title may be classified into two or more genres that is a given drama could be classified as comedy and it could also be classified as a social commentary now because it is classified as under two or more categories  there exists what is called as this one to many relationships between title and genre so every time we identify the set of all comedy dramas  the theater and director has to be repeated similarly every time we identify social commentaries  the theater and director has to be repeated because it is the same drama so this is what is called as a multi-valued dependency now what is a multi-valued dependency in a more formal fashion ? now suppose in a given relation a  there is a non-trivial functional dependency of the form a prime defines b and suppose a prime is also a key for because it ’ s in bcnf now suppose if b is completely independent of all other attributes of the relation then we say that there is a multi-valued dependency in this case the attribute theater and director are completely independent of the category or the genre of the drama it has no relationship between  a theater may play any kinds of drama and a director may direct any kinds of drama himself  refer slide time  00  53  37  so it ’ s completely independent of that  so we say that there is a multi-valued dependency  refer slide time  00  54  37  so we defined the notion of a non-trivial multi-valued dependency in order to remove them so firstly what is the notion of a non-trivial multi-valued dependency ? a multi-valued dependency of the form a  define a prime defines b is non-trivial if b is not a subset of a prime which is the non-trivial property for bcnf as well and there exist certain other attributes in addition to a prime and b that is a prime union b is a proper subset of a  the set of all attributes that is there exist some more attributes in addition to b so a relation r of a is said to be in fourth normal form  if for every non-trivial functional dependency of this form that is a multi-valued dependency b  a prime is the super key  refer slide time  00  55  31  so we shall not be going into more details of the fourth normal form and for this session we will suffice it to say that fourth normal form is an even most stringent criterion for removing duplicates or removing redundancy in relations so essentially the normal forms can be categorized like this third normal form is the most lenient among the three and next is bcnf and finally the most stringent is the fourth normal form so if any relation that is complaint to fourth normal form is automatically complaint to bcnf which is in turn automatically complaint to third normal form  refer slide time  00  56  15  so we now come to the end of this session and let us briefly summarize what all we have studied here so we studied the notion of functional dependencies which is a generalization over keys and properties of functional dependencies like transitivity  reflexivity and augmentation  extra we also saw the notions of trivial and non-trivial functional dependencies and how they affect bcnf and how we can decompose relations into bcnf so that we can remove redundancies however we see that not all relations can be decomposed into bcnf without losing information because of which we have also  we also need the concept of a third normal form and we finally saw the notion of multi-valued dependencies which can remove redundancies that exist in bcnf and the fourth normal form which remove multi-valued dependencies so that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 9 er model to relational model mapping entity relationship model are what is called as the er model and the second was the relational data model we also saw a typical database design process and placed these models into appropriate positions in the process the entity relationship model or the er model is essentially meant for human comprehension it basically is meant for creating a conceptual database or conceptual schema or what is termed as a logical schema of the database system and the relational data model is used for the physical schema or something that is implemented in the dbms and both the dbms independent models that is no matter which companies database that you are going to use  you can still use the same er model for representing your data and even the same relational model for representing the schema that goes on to your dbms now in this session we are going to address one important issue now we are going to ask the question  are these two different data models er and relational model completely independent of each other or are they the same or is there some way i can map between the er schema and the relational schema without having to break my head too much essentially that means to say or to take it to its logical extreme  can i design some tools or can i design some kinds of software that takes an er diagram of a given system and generates appropriate relational schemata for the system and we have also seen in the session on functional dependencies  we have seen how we can optimize a given relational schema up to a certain extent using some kinds of automated techniques we have seen how to take a relation to bcnf or third normal form or the forth normal form and so on so suppose we want to build a tool to automate this process we need to be able to first map between an er data database schema and a relational schema and then use techniques from functional dependencies to optimize this relational schema so that we can build a database application around it so what we are going to study today form the underpinnings of what are called as lifecycle tools or database life cycle tools there are several different lifecycle tools which provide support for the entire life cycle of a database systems starting from the conceptualization of the problem to the actual implementation of the application and maintenance of the database and so on so before we begin  let us briefly summarize what we have learned about er models and relational models the er models as you already know is used for conceptual modeling of the database schema  conceptual modeling or to create the logical database schema this is meant for human comprehension this is essentially used to show end users what you have understood about their problem domain this is a high level database design and there are no implementation details about  that are included as part of the er model  refer slide time  00  04  11  and of course the er model is a dbms independent and it is made of building blocks like entities  relationships and attributes which can be attributed to both entities and relationships  refer slide time  00  04  56  in contrast a relational data model is the data model that is most popularly used for physical schema design a physical schema is the schema that is actually implemented on the computer  therefore the relational data model is meant for or optimized towards machine consumption that is how do we efficiently store data in my database  how do i efficiently search for a given data element  how do i efficiently update a given data element so that it does not create anomalies  how do i efficiently delete data elements again without creating any anomalies and so on and of course the relational data model is also dbms independent that is no matter what kind of database that you use  you can still use the same data model as far as the database that you are using is a relational database you can use the same data model to represent your data on the dbms of course reality is quiet different from the concept of dbms independent and some dbms systems may include more features than traditionally what is supported by the relational model the relational model also supports some kinds of automated optimization techniques which we have seen in the session on functional dependencies where you can optimize a given relational schema  you can reason whether a given relational schema is optimal or not whether its going to create a redundant data in its dbms or whether its going to create some kind of anomalies during updation and deletion or so on and how we can systematically change the database schema without changing the correctness but increasing the overall efficiency in terms of retrieval and updates what are the building blocks of the relational model ? we have relations which comprises of several different attributes and the notion of keys forms a very crucial role or place a very crucial role in the relational database model  refer slide time  00  07  09  let us come back to the er model and look at some of the notations which will require if i have to study translation into er models the entities are represented using rectangles and a strong entity type that is an entity type which has its own key attribute and which represents a physical or which represents some kind of logical entity of the real life is represented by a rectangle with solid line surrounded for example the slide shows this entity type called employee which depicts all objects of type employee which are present in the current system on the other hand we also have what are called as weak entities weak entities are those which do not have an existence of their own or without being associated with a strong entity type the slide shows the example of an insurance record an insurance record doesn ’ t mean anything unless it is associated with some person in a company for example an employee  therefore when we talk about insurance record  we have to say whose insurance record and so on so that is the general idea  more specifically the insurance record entity type does not have any key attribute  it has to be associated with a strong entity type called employee which in turn has the key attribute therefore such weak entity types are depicted using dashed lines or dotted lines for the rectangle we then have the relationship type for example a relationship called handles  so employee handles project or something like that which is represented by a diamond and or normal relation type is represented by a diamond using solid lines whereas what are called as identifying relationship types that is the relationship types that identify a weak entity or provide an identity for weak entities by associating them with strong entities they are shown with double lines in the diamond entities and attributes are associated or entities and relationships are associated with attributes which are some values in a given domain attributes are depicted using ovals and a normal attribute or a simple attribute is depicted by a oval with a solid line and key attributes  in this example an attribute called pan or pan number which uniquely identifies each income tax payer is shown as a key attribute and it is shown underlined saying that this attribute is a key attribute for this entity type  refer slide time  00  09  22  and then there are multi-valued attributes which can have several values for the same attributes we took an example of the color of a peacock now the color of a peacock is actually given by several different colors and all of which in combination  form the color of the bird such kinds of attributes are depicted using double lines as shown in the slide here and then there are derived attributes that is attributes who ’ s values can be derived from other attributes and these are shown using dotted lines we took an example of the age of an employee that is if you know the date of birth of an employee and the current date  we can derive the age of an employee let us also look at some definitions from the relational model the relational model is based around the notion of a mathematical relation now a mathematical relation is said to comprise of atomic values or atomic data values and what is atomic data value ? a data value is called atomic if it can not be subdivided into smaller values  for example the age of a person similarly each data value is said to reside in a domain in the er model  a domain is also called a value set which is a term that is generally used by several people and in the relational model usually the term domain is used which is going to specify the range of values that a particular attribute can take  refer slide time  00  10  44  similarly a relation schema or a relational schema is denoted by a schema name that is in this example shown by the name r and a set of attributes in this example shown by a1 a2 until an and each attribute has a specific value that lies within the domain specified as domain of ai we have also defined what is known as the degree of a particular relationship  refer slide time  00  12  07  the degree of the relationship is simply the number of attributes in its relation schema if you remember the same definition of the degree of a relationship also apply to the er model that is a relationship diamond can be a binary relationship or a ternary relationship  unary relationship or a n-ary relationship that is it can be associated with 1 2 3 or any number of entity types and the slide shows that the relation is actually a subset of the cartesian product of all of the domains that form the attributes in the relational model  the notion of keys play a very crucial role  especially we saw in the notion  in the process of decomposing relational schema in order to make them normalized or conform into let us say bcnf or third normal form or forth normal form and so on so let us revisit the notion of keys in a little more detail and keys are again very important when we translate from an er model to a relational model we have to be aware which attributes are the key and which attributes are the foreign key and so on so a key constraint in the relational model essentially defines the notion of a superkey which is a set of attributes of a relation which can uniquely identify each tuple in the relation that is each instance of the relation and a key or a minimal superkey is something which is minimal in a sense that if you remove any element of the minimal superkey  it seizes to be a superkey anymore  refer slide time  00  12  44  and there is also the well known entity integrity constraint in the relational model which says that the primary key of a given tuple may never be null the primary key is the minimal superkey that is going to be used to uniquely identify a given tuple in the relation and we also saw the notion of referential integrity which is again an important issue in the relational data model and the referential integrity constraint says that if a tuple of one relation refers to another tuple of another relation  it should refer to an existing tuple that means foreign keys that is keys of  primary keys of another relation embedded into the tuple of yet another relation should refer to tuples that already exist in the first relation so the foreign key constraints are shown in the slide here that is first of all the attribute of the foreign key or the domain of the foreign key should be the same as the domain of the primary key of the other relation and they have to refer to existing tuples in the other relation and we also saw that relations can be or popularly viewed as tables and which is what is a notion used in sql that is a relation of the form student with three attributes roll number  name and lab can be specified in the form of a table with the name student and three kinds of columns called roll number  name and lab  refer slide time  00  14  54   refer slide time  00  15  13  so let us not come to the issue of mapping between given er model and a relational database model now i had said in the beginning of this session why such a mapping is important  refer slide time  00  15  39  there are several different commercial tools that are available which are called as lifecycle tools of a dbms design a lifecycle tool provides support in several or in most of the phases of a typical database lifecycle that means the tool should be able to or using the tool you should be able to create a logical schema  talk to your end user saying this is what i have understood by your requirements of your systems  these are the different data elements  these are the different functionality requirements that form er system and so on and then using the same tool  you should be able to create physical schemata from the logical schema by automatically translating them to whatever extent possible in practice it ’ s not possible to completely automate this process that is automatically generate a relational model and optimize it sometimes some kind of human intervention is necessary  when the human knows some domain knowledge that can not be captured into the er model but there are several such tools an example is the tool called erwin from computer associates which provide such a support for automatically translating between er and the relational model so let us see how we can go about such a translation the first case that we are going to take is the case of a simple relation or a simple entity so the slide here shows a simple entity type called department and it has three different attributes department name  department id and manager and the department id obviously is the key or the key attribute of this department  refer slide time  00  17  20  now given such a relation it is fairly obvious to see  given such an entity type it is fairly obvious to see that it can be translated into a relation of the type department that is also shown in the slide here just below the figure so this er model can be translated into such a relation where the name is called department and which has three different attributes department id  department name and manager and also note that the key attribute is retained that is the department id which is the key attribute of this entity here becomes the primary key of the relation that ’ s formed so this is straight forward that is as long as we have a simple entity type with simple attributes  note that the attributes are also simple  there no multi-valued attributes or composite attributes and so on and it can be translated in a straight forward fashion to the relational model what happens if we have a composite attribute ? remember that a composite attribute is something that is made up of sub attributes a composite attribute is different from a multi-valued attribute that is a multi-valued attribute is something which can have many values for the same attribute  the color of a bird can have several different colors  refer slide time  00  18  41  on the other hand a composite attribute is made up of two or more other attributes each having its own domain for example the slide shows a composite attribute called department id for the same example of a department entity type  so the department id is the attribute here which is a composite attribute which in turn is made up of two other attributes called location id and region number  so on now a location id could have a different domain let us say location ids are given alphabets like a b c d and so on and region number are given numbers 1 2 3 4 and so on so both of them may have different domains and they combine to form the attribute called department id which in turn is also the primary key of this department therefore we are considering two different aspects here  one is how to deal with composite attributes and the second is what happens if the composite attribute is the key attribute of the entity type so the slide here shows the example of and shows how we can translate this into relational model firstly  the name department of the entity type becomes the name of the relation called department and all the other simple attributes are retained department name is retained as department name  manager is retained as manager and only here for the composite attribute  the department id never appears here it ’ s just that all the simple components of the composite attribute are straight away loaded into this relation that is location id comes here and region number comes here and in fact if either of these two  let us say location id or region number is again a composite attribute and it has some more attributes just take all the simple components of the composite attributes so do n't take region number and just take whatever is the simple component of this region number and add it to the relation here and all of these simple components which form the composite attribute which is the key becomes the primary key that is the primary key here is a composite key made up of two or two or more different attributes which combinedly identify or help in identifying a tuple of the relation  refer slide time  00  21  37  the next example that we are going to see is the example of how to map relationships first of all  let us look at the following relationship that is shown in the slide here what characteristics can we ascribe to the relationship that is shown here ? firstly  we notice that the relationship here is a 1  1 relationship that is one employee is associated with one insurance record or rather the other way around in this case that is one insurance record is associated with one employee and have a look at the association as well the association or the relationship type is an identifying relationship type that means the relationship type called insurance details shown in the slide here is used to identify or provide an identity for the insurance record by associating it with an employee and also the insurance record has a total participation in this relationship that is insurance record has no existence without this relationship now how do we translate such entity types ? so essentially the idea here is what how do we translate insurance record into the relational model ? so for weak entity types  the translation is shown here in the slide below  just create a relationship or a just create a relation of the same name as the entity type but since it does not have a key  because weak entity types do not have key attributes so  since it does not have a key attribute  use the key attribute from employee with which it is associated with and take that key attribute and make it into the key attribute of insurance record however note that since pan number here is also the primary key for employee  this has to be made as a foreign key of insurance record that means whenever we are updating or altering the table  we have to use the cascade option whenever let us say the employee type is updated or deleted that means to say that if the employee relation is deleted from the database  this in turn delete all the insurance record relations from the database itself because insurance records do not have any existence without the employee records so the three steps here in order to translate a weak entity type is to first identify or is to first to locate the identifying relationship and see which entity type is this weak entity type associated with and use the primary key of that entity type as the key for the weak entity type or the record of or the relation for the weak entity type and make it into a foreign key of this entity type and use cascade options whenever updations or deletions are performed on the strong entity type  refer slide time  00  25  05  let us move on with translating relationships so how do we translate ? let us take the simplest form of relationship again the 1  1 relationship we saw what happens or how do we translate 1  1 relationships when weak entity types are considered now let us consider an entity type which is not weak but still is involved in a total participation that this is shown in the figure here the figure shows a relationship type called managedby which relates two different entity types that is department and manager and there are attributes  relevant attributes are shown for each of them that is the department has a key attribute called department id and a manager has a key attribute called employee number and the relationship itself has a key attribute called secretary that is a secretary is assigned for a department that is managed by a manager that is the secretary attribute does not have an existence without an existence of this relationship that is if a department is not managed by a manager then there is no secretary that is associated with this now how do we translate this ? the translation is again shown in the slide below so first create an entity type or create a relationship  create a relation called manager let me repeat this again for this relationship  create a relation in the rdbms model called manager with the following attributes now you might be wondering why should we create a relation called manager  why not department now let us think about it a little further see in this slide here that manager is a strong entity type ; it is not a weak entity type however it is involved in a total participation in this relationship type what is the total participation ? the total participation is that the entity type does not have any existence without being participating in a relationship type of this kind so what it essentially means here is that a manager has no existence that is a manager would probably be just an employee so a manager would have no existence unless here she is associated or is given a department to manage so it is the entity type that is involved in the total participation is taken as the primary entity type or the base entity type  primary relation called manager and then employee number becomes the key here that is the key for manager and the department id which is the primary key for department becomes a foreign key in manager and whatever attributes are associated with the relationship itself become attributes of this relation here that is of the manager relation here so therefore the manager relation has a primary key called employee number and a foreign key called department id note that this makes sense when we note that manager does not have any existence without this relationship that is  refer to the problem of referential integrity in relational data model what is the referential integrity stipulate ? whenever a foreign key refers to a tuple in another relation  the tuple should exist that is it should refer to an existing tuple in the other relation now if we had made department as the base entity or the base relation here  we can not use employee number as a foreign key because the manager relation wo n't even exist before this relation that is managedby is formed on the other hand department has an independent existence without whether or not a manager is associated within therefore that forms the rational behind why we choose the entity type which is involved in total participation as a base entity type for the translation  refer slide time  00  29  49  so let us summarize this  the previous slide once again so in any 1  1 binary relationship between types s and t  choose one of them as a base relation in case one of them is involved in a total participation  choose that as the base if neither department nor manager where to be involved in total participation  it doesn ’ t matter which you are going to choose as the base relation include the primary key of the other entity type as a foreign key in the base relation and include any relationship attributes as attributes of the base relation  refer slide time  00  30  29  consider the example shown in the slide here what happens if in a 1  1 relationship both entity types that is both entity types that are participating in this 1  1 binary relationship are involved in a total participation take a look at the slide here the slide shows two entity types  project and consultant and each project is uniquely identified by a project id and each consultant is uniquely identified by his or her pan number and there is a relationship called  relationship type called consultation which has its own attribute called secretary and it ’ s a 1  1 relationship and both of them are involved in a total participation that is a project has no existence unless it is being consulted by a consultant and a consultant has no existence unless he or she is associated with a project so  neither of them will have independent existence without the other in such cases we can not identify any relation as the base relation if we identify project as a base relation and try to use pan number of the consultant as the foreign key then referential integrity could be violated it ’ s the same in the other way around as well if we use consultant as the base relation and try to use project id as the foreign key  again there is a chance of violating the referential integrity in such cases the simplest way is to take the relationship type in this case the consultation has the base relation that is form a relation called consultation and use project id and pan as the primary key of consultation and then all of the attributes from both of them will become attributes of this relation  refer slide time  00  32  27  so incase both entities in a 1  1 binary relationship or both in total participation then we merge both of the entity types into one  usually in the name of the relationship that is in the name of the relationship called consultation in the previous example now let us see how do we map 1 to n relationship what is a 1  n relationship ? that is n entities of one of the entity types could be associated with one entity of the other entity types that is it forms some kind of a tree relationship that is one entity being associated with n different entities of the other type so the slide here shows such an example that is employee works in department that is employee is an entity type  so n different employees can work in one department that is one department may have several employees but each employee is associated with only one department  refer slide time  00  32  48  and of course there are keys called employee number for employee and department id for department the slide also shows how we can reduce this to a relation the simplest way is to take the entity type on the n side of a relationship so in this case the employee  so take this as the base that is translate it into a relation called employee and the primary key of employee becomes the key of the employee entity type here and the department id becomes a foreign key here so this is as simple as that  that is for each binary 1  n relationships identify the relation s that represents the entity type on the n side why is this ? so because each entity type on the n side uniquely identifies a department that is uniquely identifies the entity type on the other side therefore we can use the primary key of the entity type on the other side as a foreign key in the base relation therefore use this as the base relation and create a relation including the key of the other entity type as the foreign key  refer slide time  00  34  05   refer slide time  00  34  36  how do we map m n relationships ? now what is an m n relationship ? an m n relationship essentially says that m different entities of the first type can be associated with n different entities of the second type therefore there is no unique identification that is given an employee in the previous case  one could uniquely identify the department with which the employee is working in because each employee can work in at most one department  refer slide time  00  34  50  on the other hand here let us say a relationship called deputedto so an employee could be deputedto several departments let us say so m different employees can be deputedto n different departments and of course employee has employee number as the key and department has department id as the key and so on and deputedto also has an attribute called record number which maintains a record of which employee is deputed where and so on so in order to translate such relationships  note the steps that are shown in the slide here there are three different relations that are formed  one is the employee relation that is one of the entity types of this relationship type so the employee relation is formed with employee number as the primary key and all the other attributes that form the employee entity type similarly a department relation is found with department id as the primary key and all other relations and then a separate relation is created for the relationship type itself so deputedto becomes a separate relation by itself and then uses employee number and department ids as foreign keys and also as the primary keys of this relation and whatever attributes that belong to the relation becomes part of or belong to the relationship type becomes part of the relations that is created here that is the record number attribute becomes one of the attributes of deputedto relation note that we can not move this record number that is the attribute of this relation to either the employee or a department because it does not uniquely identify either employee or department each employee could be associated with n different record numbers because they could be associated with n different departments and similarly each department could be associated with m different record number because m different employees could be working in that department  refer slide time  00  37  43  so this slide shows  summarizes how m is to n relationships are translated in a m  n binary relationship  it is not possible to collapse the relationship into one of the entity types because neither of the entity types uniquely identifies the other entity type therefore a separate relation is required and usually this is in the name of the relationship type itself so separate relation is required in order to complete the mapping and of course the cascade option should be used whenever updates are performed on any of the relations pertaining to the entity types that is whenever the whenever the entity type called employee or department is updated or deleted then these changes should be cascaded  so that they are reflected in the deputedto relationship as well that is if employees employee entity type is deleted then of course the deputed to relationship should also be deleted  refer slide time  00  38  53  now one might ask the question is it possible to use the strategy that is use the relationship type as the base relation rather than any of the participating entity types can we use the strategy for mapping 1  1 and 1  n relations as well because m  n is simply a generalization of 1  1 and 1  n relationships so of course it is possible that is take the example of employee works in department that is n different employees working in one department we can still create a separate relation called works in where it can use the employee number and the department id as the foreign keys of this relation however it just creates an extra relationship or extra relation in the database that is totally unnecessary but this is sometimes actually attractive to use than collapsing the relationship into one of the relations especially where  especially if you have to avoid null values that is especially if we have to  especially if we have cases where there are some employees who do not work in any department if n different  if an employee can be associated with at most one department it means that an employee can also be associated with zero department so in that case the department id field of employee would be null  it does not violate referential integrity because remember that referential integrity says that a foreign key should refer to an existing tuples or else should be null therefore it does not violate referential integrity but creates a lot of null values in the database schema  in the database itself so if we have to avoid null values  it is actually preferable to use the relationship type as the base relation when performing the translation  refer slide time  00  40  57  how do we map multi-valued attributes ? we have seen how to map composite attributes and simple attributes and keys and so on but what happens if there are multi-valued attributes composite attributes are different from multi-valued attributes in the sense that each of them can have several different domains that is it is just a combination of several simple attributes so we just open up the combination  when we are translating a composite attribute and then include all the simple attributes that form part of the composite attribute on the other hand a multi-valued attribute is not a composition of several sub attributes instead it is an attribute that can take on several values instead of one value and the example  the slide show is that of a bird so a bird has a multi-valued attribute called color  so what is the color of this bird a bird could have several colors it need not have just one color and of course there is a primary key called species which identifies each bird uniquely so in order to translate multi-valued attributes  take a look at the lower half of the slide which shows two different relations which make up this translation the first relation shows a relation called bird with species as a primary key and all other attributes except the color attribute  all other attributes of the entity type called bird and then a separate relation is created called bird colors where species and color are both included and are both part of the key that is combinedly defined the key of this bird therefore we say that bird species eggs has color y  eggs has color z  eggs has color a and so on so the color attribute may be repeated in several or several tuples or rather the species attribute may be repeated in several tuples  once for each different color that the bird can take and both of them that is species and color become the primary key for bird colors so for each multi-valued attributes of a given entity type  we have to create a separate relation that has a primary key of s paired with all possible values that the multi-valued attribute can take and of course the cascade option should be used for referential integrity on the bird relation that is whenever the bird relation is deleted or updated  the corresponding changes has to be made in bird colors as well  refer slide time  00  43  20   refer slide time  00  43  51  how do we map n-ary relationships ? until now we have been looking at binary relationships what happens when there are n-ary relationships and different entities forming part of the relationship ? the slide here shows such an example that is the standard example of supplier supplies part to project so there is a supplier who is uniquely identified by the supplier or the sales tax registration number or something streg number and there is a project that is uniquely identified with project id and there is a part that is uniquely identified by part id and then the supplies relation which relates all of these three different entity types so the simple way of translating this is to use a separate relation called supplies as the base and of course separate relations each for supplier project and part with their corresponding primary keys and the supplies relation which has the primary keys of each of these relations has the foreign key of this relation  refer slide time  00  45  02  so for each n-ary relationship of any type r where n is greater than 2  we have to create a new relation s to represent this relationship type r and of course the primary keys of the participating relations become foreign keys in this new relation and the cascade option should be used for all of the relations that correspond to the entity types that participate in this relationship type what happens if one of the relations in an n-ary relationship type is a weak entity type ? that is let us say part is a weak entity type  there is no existence for part unless it is associated with a supplier supplying it to some project in that case we have to first identify the entity type which gives an existence to part and the part relation here has only foreign keys  it does not have any primary keys but the supplies relationship does not change that is it doesn ’ t have any part id  it just simply has the supplier primary key and the project primary key without the part primary key therefore we get two different relations that have foreign keys and they do n't have their own primary keys  refer slide time  00  46  42  so that brings us to the end of this session that talked about how we can map er models into relation relational database models using several different rules and of course this is not a comprehensive set of rules because there are several other sets of rules used for example in derived attributes or enhanced er models like generalization and specialization which are not covered here but all of them in totality are used to create the basis for any kind of a tool  software tool that can translate between given er schema and its corresponding relational schema so  this slide shows a summary of each of this mappings that is it gives a set of thumb rules saying if this is what is given in the er model then what happens in the relational model so in an er model if an entity type is given then a corresponding entity relation that is a relation in the name of the entity type is created if a 1  1 or a 1  n binary relationship is given then we create corresponding foreign keys from the n side to the one side or from the weak entity type to the strong entity type in this relation so we create a relation and create appropriate foreign keys from them if a m  n relationship type is given as shown in the slide here then we create a relation with the name of the relationship type so  within quotes which is shown as relationship relation with two foreign keys that is one for each entity type that participates in this relation  refer slide time  00  48  44  if an n-ary relationship type is given  it is still the same strategy that is we create a relationship relation with n different foreign keys that is one for each entity type or rather n different foreign keys as long as these entity types are strong entity types if there is a simple attribute in a er model that simply becomes an attribute in one of the relations in the relational model if it is a composite attribute in the er model then it becomes a set of simple attributes that is you take the simple part of all composite attributes that is just go on finding the simple attributes that forms the composite attributes and then make all of them as part of the this relation  refer slide time  00  49  33  if it is a multi-valued attribute then we need to create a separate relation and a foreign key that is you have to associate the primary key of the base relation with each possible value of the multi-valued attribute if it is a value set  it becomes a domain  value set in the er model it becomes a domain and a key attribute in the er model will become either a primary or a secondary key in the relational model so that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 10 storage structures looking at what might be termed as the logical aspects of database design  we looked at different data models  how data can be represented and how the relationships between them can be represented we saw the er schema which is meant primarily for human comprehension and we also saw the relational schema which we claimed is meant for the physical schema that is meant for machine comprehension but when we say physical schema  it is still a kind of misnomer because the relational data model does not say anything about how data is actually stored on computers or storage device like disks or whatever  wherever databases are implemented so in this session and the next few sessions  we are going to actually ripe a part in a sense look inside a dbms or inside an implementation of a dbms and see how are these data elements actually stored on computers and what is it mean when we say that we have stored a table ? how is a table actually residing on disks or any other kind of storage device ? so that is what we are going to be concerned with in the next few sessions so let us begin the session on storage structures in order to be understanding storage structures  one of the first things we need to understand is what might be termed as the memory hierarchy  refer slide time  00  02  39  when we talk about storage or storing data  the first question we need to ask is where is data stored there are different kinds of devices which are capable of storing data you might have obviously come across hard disks  i mean data that are stored in hard disks  floppy disks  cdrom and even the computer ram that is the random access memory in the computer  the cache memory within the machine and even the registers are within the cpu  all of them are meant for storing data and all of these can be organized  these kinds of  different kinds of memory devices can be organized in the form of a hierarchy which is termed as a memory hierarchy this slide shows such a memory hierarchy but it divides this hierarchy into two kinds of storage devices what are called as primary storage and what are called as secondary storage if we were to draw a hierarchy  primary storages devices would appear up in the hierarchy and below them would be the secondary storage devices what is a difference between the primary and secondary storage devices ? primary storage devices  some examples are shown in the slide here like cpu registers  cache memory  ram  dram  sram and so on all of them are extremely fast memory devices  you can address or retrieve data elements extremely fast from these devices however all of these are volatile memory devices that means once the power is switched off  they no longer can hold data data that are stored in primary storage devices can not be persistent in nature on the other hand secondary storage devices of which some examples are magnetic disks like your hard disk on your pc  magnetic tape which is primarily used in many locations for archiving data or taking backups of data then there are cdroms  there are read-only cdroms  there are write-once cdroms  there are even some kinds of read-write cdroms that are being available today and there are also what is a more recent phenomenon what is called as the flash memory flash memory is a kind of  is made of what are called as eeproms that is electrically erasable programmable read only memories which can store data persistently even after the power is switched off and they can perform or they can perform data transfer in a rate that is much faster than existing storage devices like say magnetic disks or tapes and so on so the common theme in secondary storage devices is that data can be stored in these devices in a persistent fashion and usually secondary storage devices are much cheaper than primary storage devices and they can store much more data than can be stored in a primary storage device however usually secondary storage devices are much slower to access  they inquire much more over heads during access than accessing a primary storage device like say ram or cache memory or so on  refer slide time  00  06  06  now for the most part when implementing a database management system  we shall be concerned mainly with secondary storage devices we do not concern hassles with what are called as main memory database which are databases that are completely held in main memory there are not many implementations of main memory databases simply because it ’ s much more expensive to have large main memories which can implement databases of sizable of a pretty large size so secondary storage devices have certain kinds  certain characteristics which are important or which influence the kind of storage structure that we are going to use to access and store and access data in these devices we can either categorize secondary storage devices either random access device or a sequential device something like the pc hard disk or magnetic disk or random access devices that is you can access any given block of data in a magnetic disk  they are called sectors so you can access any particular sector directly  it is not purely random access because it does performs some kind of sequential searches however for most practical purposes or magnetic disk is a random access device where you can address any block directly and move to that block read or write to that block directly on the other hand something like a magnetic tape is a sequential access device if you have to access the hundredth block and the tape is rewound  you have to run through the first 99 block before being able to access the hundredth block therefore it ’ s important what kinds of  how efficient can data access be when we are using a device which is either random access or sequential for example we can not implement a storage structure that has to perform a lot of pauses on the data mainly because especially when we are using a sequential device similarly we can classify devices as either read-write devices  write-once devices or read-only devices read-write devices are those where you can read and write data any number of times hard disks  floppy disks  magnetic tapes and so on are examples on the other hand there are write-once devices where you can write data once but you can not erase it once it is written  it becomes a read-only device so such kinds of devices can not incorporate data structures which need to be modified during runtime for example similarly there are read-only devices where data is stored during the manufacture of the device itself and it can never be altered and so new data can not be stored on such devices again so there are devices especially in embedded systems which stores small databases within read-only devices and it ’ s extremely important that such storage is performed correctly the first time because there is no scope for any kind of modifications  once data is written on to these devices then there are devices that can be classified as either character devices or block devices character devices are those where you have to read data character by character which can be extremely inefficient when we are dealing with large amounts of data some kinds of tape devices and so on are character devices on the other hand there are block data access devices where usually the unit of data transfer is a set of characters called a block so every time any read request is given  a read request reads an entire block of data into memory and writes back an entire block of data on to the device and usually there block access is coupled with what are called as read aheads that means its not just one block  it ’ s a set of blocks that are read into memory at a given point in time in order to increase data transfer efficiency  refer slide time  00  10  40  what are the requirements for storing databases  what kinds of storage requirements do databases pose ? firstly we note that almost all databases required data to be stored permanently or what is called as persistently for longer periods of time it should not be the case that once the computer is switched off  all the data in the database is lost  it has to be stored permanently or in a persistent fashion usually databases are far too big to fit in main memory it is not realistic to be able to search a database by loading the entire database into memory and then use such techniques that are mainly useful for main memory we have to store the database on disk and involve strategies that can search data on to the disk and use memory for this purpose over the years the cost of storage has dropped drastically  in fact there are many claims that the progress in storage has beaten what is called as moore ’ s law that is the amount of storage that can be packed into a given cost  for a given cost or on a given square area of physical dimensions today we can have gigabytes of data stores in a very small device that one can store within your in pockets or on in a very small area we have gigabytes of data stores that are embedded within watches for example and within pens and so on so the amount of the data store or the storage available per person has increased dramatically over the years and one of the byproduct of this is the changing definition of what might be termed as very large databases in fact the term very large databases when it was coined was meant to refer to databases which are of hundreds of megabytes large slowly very large databases came to mean several gigabytes of a data and now we have databases that are several hundreds of gigabytes or terabytes which are 10 to the power of 12 bytes and then even petabytes of data petabytes are 10 power 15 bytes of data where especially databases that work on web related data like google or altavista or search engines which collect data from all over the web actually work on petabytes of data so the definition of very large databases has been changing continuously to include more and more data storage requirements therefore what is being  what has become imperative today is to design extremely agile data structures that can store and manage data between main memory and secondary storage devices in an efficient fashion when we store data on to secondary memory  we usually distinguish between two kinds of data storage ; the one is what is called as primary data storage the primary data storage talks about how we store the data itself  how data itself is organized on to disks or any storage device and how are they accessed secondary file structures are those are also called as auxiliary or augmenting file structures are those sets of files that are used to speedup access to these data elements and this is especially important  secondary file organizations become especially important when the size of the database starts growing by leaps and bounds when we have terabytes or petabytes of data  it is the role of auxiliary files or secondary files that provide pointers that help us in locating the required data element becomes more and more important  refer slide time  00  15  04  usually when we talk about data storage on secondary devices  we are talking about what is termed as file organizations data is stored in logical structures called files on disks the way files are organized on disk is called the file organizations usually files are stored as a sequence of records and a record is analogous to the notion of a tuple in a relation in relational algebra or a row in sql parlance so  a file is stored as a sequence of these physical or logical records and are stored in terms of what are called as physical blocks as we saw before in previous slide  the block is the unit of data transfer between main memory and the storage device so there are two different things here within a file  one is the logical ordering of data which is in the form of records and then the physical storage of data which is in the terms of blocks there could be 1  1 correspondence between records and blocks which is very rare which means to say that each block is one record or there could be many records per block or many blocks per record depending on how we define our record structure the term file organization refers to the way in which records are stored in terms of blocks and in the way blocks are placed on the storage medium and are interlinked so that they can be accessed from wherever there are three different kinds of file organizations that we are going to see in this session today the first one is what is called as the unsorted or the pile file or organization and the second kind of organization is what is called as the sorted file organization and lastly we are going to look at hashing file organization  refer slide time  0017  09  let us briefly look at the notion of record and blocks which is important for us to understand these different kinds of file organizations a record like we have mentioned earlier represents a tuple in any relation it is a logical unit of data which of inter related data which is of interest to the user or the database management system a file is defined as a sequence of records and records could either be fixed length or variable length remember in sql for example you can use variable length strings and variable length integers and so on so the length of a given record could also be variable or fixed and records comprise of a sequence of different fields and fields is the same as  a field is the same as a column in sql parlance or in attribute in relational parlance  refer slide time  00  18  09  blocks  so blocks like we mentioned earlier is the physical unit of data transfer or data storage in storage devices they correspond to for example sectors in hard disk or page in virtual memory systems and so on they store  usually a block stores records from a single file but it need not necessarily be such a case it depends on the file system structure blocks are usually of fixed length  blocks can not be of varying length unlike records and the length of a block is dependent upon physical characteristics of the storage device and also of the operating system and the ram and so on so many times the database management system itself does not have much control over the size of a block a storage device is termed to be either defragmented or fragmented depending on whether contiguous sets of blocks on the storage device belong to the same file or to different files we now define a term called the blocking factor which is important to determine how records are packed within blocks the blocking factor is the number of records that are stored in a block on a given storage device  refer slide time  00  19  19  it is constant across blocks  this blocking factor is constant across blocks if record length is fixed on the other hand if record length is variable then the blocking factor is also variable because the number of records per block may vary from block to block blocking factor is simply defined as the number of blocks divided by or the size of block divided by the size of the total number of records so bfr as shown in the slide is b divided by r where b is block size and r is record size and this is a floor function that is which takes a lower integer value of this division since the record size may not exactly divide block size  there would be some amount of wastage which is given by this formula that is in each block there is a wastage of this much amount of bytes that is blocking factor times the record size number of bytes so how is this wastage managed ? there are two kinds of approaches to managing this wasted block area when records are stored within blocks the first is to do nothing that is do n't use  let wasted spaces be such kinds of techniques are used in what are called as un spanned records that is a record may not span multiple blocks  refer slide time  00  20  41  this slide shows such an example the slide shows one block which spans from here to here comprising of three different records and there is no space for a fourth record however there is some extra space that is left in the block which is left unused  refer slide time  00  21  28  on the other hand there are what are called as spanned records a spanned record is something that can span across different blocks this slide shows such an example  this block contains three records and there is not enough space for the fourth record however part of the fourth record is placed in the remaining space leaving a small amount of space for a pointer to point to the next block in the logical sequence of record and wherever the next block begins  the remaining part of the fourth record is stored and then the next records are stored here so such kinds of record organizations are termed to be spanning organizations where a record can span across multiple blocks i am sure you would have noticed that if record size is bigger than the block size  we have to necessarily use spanning organization for storing records because we can not store records into blocks otherwise  refer slide time  00  22  30  so this is what this slide says that is when record size is greater than the block size that is r is greater than b then usage of spanned records is compulsory when we have variable record sizes or when we use spanned record allocation  we can term what is called as the average number of blocks that are required per or we can compute what is called as the average number of blocks required for storing a collection of records so this can be  in order to compute this we first simply compute the blocking factor that is the block size divided by the average size of each record  refer slide time  00  22  41  then the following formula where r is the number of records divided by the blocking factor will give the number of blocks that is required for storing a particular database that is particular set of records  refer slide time  00  23  30  let us now start with the different kinds of file organization techniques the first kind of file organization technique that we are going to see today is what is called as the unordered file organization this is also termed as a pile file where the term meaning that records are just stored as a pile inside the file this slide shows such an example that is records are coming into the system and they are just being appended to the file record 1  record 2  record 3  record 4 without request to what data that are contained in these records and how they are going to be searched and of course these data  these records eventually go into different blocks and they are managed in some fashion by the operating system underneath in the machine  refer slide time  00  24  23  a pile file is the simplest form of file organization  we do n't have to do anything for organizing this file and insertion of records is the simplest that is records are inserted in the order of their arrival and on the other hand if we have to search this file  we usually need some kinds of auxiliary files or we require some kind of help in order to efficiently search these files for a given data element therefore insertion is very easy however searching is extremely expensive because we will have to do a linear search  we have to just search through the entire file in order to find the data element that we require  refer slide time  00  25  07  what about deletion in pile files ? deletion possesses yet another tricky problem in pile files and which can create certain kinds of fragmentation problems have a look at the slide here  the slide shows a pile file containing three records and some part of the block is empty and there are other blocks as well in the file now suppose that record two has to be deleted now once we delete record two and empty the space  we can not reclaim back this space because the insertion algorithm for a pile file is not cognizant of this extra space that it can use the insertion algorithm simply inserts records at the end of the file  it just appends records to the file therefore such a kind of deletion strategy is inefficient in terms of space usage when we are using variable length records in a pile file  we encounter another unique problem and this is of record modification whenever some data is modified in a record as long as the record is of fixed length  it does not matter we can make the modification in place and write it back into the file however if we allow for records to have variable size and the modification results in the size of the record to grow  there may not be enough space to write back the record this slide shows such an example  refer slide time  00  26  06  there are three records in this file or rather four records in the file record 1  record 2  record 3 and record 4 and record 2 is modified now the modification is such that the size of record 2 increases now we can not write back this record at the same place where it was earlier therefore we will have to mark this  mark the earlier record as deleted or unused or something like that and write back record 2 at the end of the file  this is shown in the figure here and of course we need to update any kind of auxiliary data structures that point to record 2  so that it points to the new location in the file the second kind of file organization that we are going to consider are what are called as sorted files sorted files are those files which are physically sorted on the disk based on some field called the ordering field so the file is actually or rather physically sorted on the disk so when you read the file on disk in a particular order  it provides or it returns back records which are sorted based on the ordering field ordering field should be a key field or i should or it is recommended that the ordering key  ordering field should be a key field that is it should be unique for each record and should belong to an ordinal domain  refer slide time  00  27  27  what is an ordinal domain ? an ordinal domain is something where you can establish a total order among elements of the domain for example the set of all integers is an ordinal domain the set of all names for example is not an ordinal domain  we can not place one name with respect to the other unless of course we impose some kind of an ordering like say lexical ordering we say  we order the names as per the lexical rule that is a comes before b  b comes before c and so on in sorted files insertion and deletion are both expensive because we have to ensure that the file remains sorted at all times especially when a new record is inserted with ordering field which has to go somewhere in the middle of the file rather than at the end of the file and updation of a record may actually involve physical migration of the record  especially if the ordering field is modified however searching in a sorted file is made simpler because we can use what is termed as binary search what is binary search ? we shall not go into too much details of binary search  let me just give a small algorithm of what a binary search looks like essentially the binary search technique is a technique where we divide the search space by half that is into in each iteration of the set that is we reduce the search space to half of the previous search space in each iteration  refer slide time  00  29  34  so this slide shows a simple algorithm for binary searches we see in the first step here that we start with two bounds left and or lower and upper bound the lower bound is at one that is the first element or the first record and the upper bound is said to b that is where b is the number of blocks in the file or the last block in the file and in each iteration we are going to compute the mid point of these bounds that is l plus u divided by 2 is the midpoint now suppose we have to search for a given key value k  we say we read the records from form block i that is a midpoint and then see if and compare that with the required key attribute now there could be three different options that is one is the key attribute is equal to the key that is read from the block in which case we have found the record therefore return  we return a success on the other hand a key attribute could be less than the midpoint in which case we have to search the lower half of our search space that is we have to search between 1 and i minus 1 that is l and i minus 1 on the other hand if key attribute is greater than the midpoint  we have to search in the upper half of the database that is we have to search between i plus 1 and u so this series of steps is performed until and unless or as long as u is greater than or equal to l that is the upper bound is greater than or equal to lower bound whenever they cross that is whenever the upper and lower bounds cross without having found the given record  we are able to conclude that the record does not exist in the file and then we say it is not found so binary search  we are not going to detailed analysis of binary search here however one can verify that a binary search technique requires an order of what is termed as log n where n is the number of blocks in the file a binary search requires an order of log n number of disk accesses where as a linear search which actually searches through the file requires an order of n number of block accesses for searching that is on an average n by 2 number of block accesses have to done for a linear search whereas only log n to base 2 number of accesses need to be performed for a binary search  refer slide time  00  32  45  let us now look at one more technique by which sorted files can be made more efficient in terms of insertion and updation note that whenever a sorted file has  whenever a new record has to be inserted into a sorted file  it is always a problem because the file has to be always sorted physically on the disk it ’ s not a logical sorting that is being performed here therefore whenever a new record is being inserted as shown in the slide here  let us say record with key one is already in the file  next key value of 3 is already in the file and key value with 7 is already in the file and now we receive a record whose key value is 4 now what do we do with this record ? in fact this record has to appear between record 3 and record 7 in the file physically on the file that means that we have to physically move record 7 below and then insert record 4 here so that the file remains sorted this is an extremely expensive operation  especially if lots of insertion operations are taking place in order to mitigate this problem another technique what is called as the overflow file is used an overflow file is a secondary file or another file where records are stored in an unsorted fashion whenever a new record is being added to the database  it is just added to the record or the overflow file in the form of a pile file that is it ’ s just appended to the overflow file and periodically that is in a less frequent fashion  let us say once in a month or once in a weak or on a weekends or something like that  the overflow file is merged into the master file or the actual sorted file that is the overflow file is first sorted and there are a number of merge algorithms that can take two sorted files and merge them together in an efficient fashion and using this  the over flow file is merged back into the master file when such a technique is used for sorted files  searching also becomes a little bit different from a pure binary search that is whenever a key has to be searched  we can perform a binary search and the master file and incase the key is not found in the master file  we have to perform a linear search in the overflow file so there are two kinds of searching that has to be done when overflow files are used in sorted file organizations  refer slide time  00  35  35  so to summarize sorted files  we see that the sorted files are more efficient than pile files especially for key base searches however they are suitable mainly for random access devices note that if we have to perform a binary search and binary search over a tape device  we may have to keep moving back and forth in the tape device quiet often which makes it more  which makes it terrible inefficient therefore binary searches are more suitable for random access devices rather than sequential devices it ’ s a better choice where a database is mostly read only because insertion is always a problem  insertion and updation is a problem we have to use overflow files or physically move records and merge and so on and mostly queries are key based retrievals the third kind of file organization that we are going to see today  the third and the last kind of file organization that we are seeing today is what is called as hashing file organization what is meant by hashing ? hashing is a means of providing very fast access to records on certain search conditions and what are these search conditions ? these search conditions are usually the equality condition based on a key field that is whenever i what to search a record having a particular key attribute  note that it is not something like whose keys are less than a particular key attribute are greater than a particular key attribute and so on this is useful only when we are searching for records whose key attributes are equal to the attributes attribute that is given in the query  refer slide time  00  36  28  hashing techniques uses what are called as hashing functions or which are also termed as randomizing functions that map particular keys into buckets for hosting records and just like sorted file techniques  even hashing techniques are primarily suited for random access devices before we go into how hashing is performed on disks  let us have a look at what is called as internal hashing internal hashing is hashing that is performed entirely in main memory and in most database management systems  hashing is used extensively in main memory in order to quickly access a given data element among a set of data elements that have been loaded onto memory so usually such kinds of hashing techniques uses an internal data structure that has something like static array of m different buckets and they are indexed from 0 to m minus 1 and they are several candidates for such a hashing function a simple candidate is to just compute the mod or the remainder of the key with m that is the m where m is the number of buckets for the hashing function  refer slide time  00  37  39  there are also other hashing algorithms called folding the key where a given key attribute is twisted and folded in different ways in order to come out with a number  that is uniformly randomly distributed across the set of all buckets that form the array static array of hash buckets there are also other techniques like sampling the key and so on which we are not going to be seeing here external hashing is the hashing technique that is used for managing data on disks rather than in memory external hashing comprises of blocks on the disks which act as buckets and in turn are augmented by one or more blocks that hold the hashing array itself or the set of buckets that form the hashing array itself so the figure here shows a typical hashing process given a particular record with a key attribute k  it is first put through a hashing function which is called hash of k here and this hashing function maps it onto a particular index entry in an array of buckets each index entry here in turn has an address of one or more blocks which form this bucket  refer slide time  00  39  02  usually one block can be allocated to one bucket or it could also be more than one blocks that are allocated to one bucket once the block is identified  this record is just appended to this block so whenever a search has to be made on this key  we have to make a sequential search within the block however we can reduce a search space drastically especially when there are large number of records on to a single block or a set of blocks that form a given hash bucket so to summarize external hashing again  external hashing uses two levels of indirection that is hashing into buckets and searching within buckets a bucket is usually one disk block or a set of contiguous blocks which is also important here that is there is no point having non-contiguous blocks as part of hashing because we again need to store some information on how to access these blocks from one another a bucket can hold more than one record obviously and also this depends on records size and we have to perform a sequential search within a bucket  refer slide time  00  40  31   refer slide time  00  41  10  hashing has to content with a contentious issue of what is called as overflows what happens if we choose a hashing function that tries to hash every key onto a very small number of buckets it especially  this can especially happen when the dataset itself could be could be skewed even if the hashing function that we have chosen is a reasonable one that is given a set of keys that are uniformly distributed over a given range  this randomizing function uniformly distributes it over the set of all buckets however if a dataset itself is skewed  we have a large number of key values near a particular value rather than all across the range then the hashing function would also be correspondingly skewed in such cases what happens is that buckets could get overflowed that is the number of records stored in a bucket could go beyond the capacity of the bucket itself in such cases there are several techniques that are used for overflow management there are three different techniques that are primarily used ; the first one is what is called as open addressing open addressing simply says that if this bucket is full just use the next available bucket which has some space in it so once hashing function hashes on to a particular bucket and then we find that it is full we start a sequential scan or a linear scan of the bucket space for the next available bucket in which we can store the record the second kind of technique that is used for managing overflows is what was called chaining chaining is the technique of maintaining a link list of different buckets so that when a particular bucket is full  it maintains a pointer to another disk block or another set of disk blocks acting as another bucket which can hold some more data in them and so on and when that becomes full there is another chain and so on however because hashing has to perform sequential searches within buckets  if we encounter or if we end up with a long chain of buckets it becomes terribly inefficient in terms of searching then the third kind of technique is what is called as rehashing where we try to use another hashing function if the first hashing function doesn ’ t work that is maps to an overflow bucket  we use hashing function two and then see if it works and then hashing function three or whatever and then combine it with something like open addressing or chaining in order to manage overflows so this slide here shows the concept of chaining where there are main buckets here which in turn have pointers to overflow buckets and these pointers point to exact records in these overflow buckets and each record here has a next record pointer which points to the next overflow record that are managed by this overflow buckets  refer slide time  00  43  57   refer slide time  00  44  25  until now we have been looking at kinds of hashing where the bucket space or the number of buckets is fixed such kinds of hashing techniques are what are called as static hashing techniques and we have already seen what is the limitation of a static hashing technique a given hashing technique might work generally that is a given hashing function might be good enough so that if the sets of keys are uniformly distributed  the hashing is also more or less uniformly distributed however when the set of keys are skewed  when the data itself is skewed using a static hashing might be terrible inefficient because some amounts of buckets could be overflowing while a large number of other buckets could be more or less empty in order to obviate this need  we use what is called as dynamic hashing dynamic hashing is the process where the number of buckets can change dynamically can grow or shrink with time as and when keys are being added or deleted from the database the overall strategy in dynamic hashing is quite simple and it is shown in the following three steps the first is we just start with a single bucket to begin with and we hash everything on to this bucket once the bucket is full we split the bucket into two separate buckets and then continue with the hashing process this is and then we redistribute the records or the data that is stored within a bucket such that they are more or less uniformly distributed across the two different buckets this process continuous  the process of splitting continuous whenever there is an overflow and then there is the process of merging that happens whenever there is an underflow that happens that is when a bucket becomes empty we now look at a simple dynamic hashing technique where  which was how buckets can be split and merged the slide shows one such technique here we see that we have a small diagram here which shows two different kinds of nodes or data structures this kind of data structure  the circle here is what is called as an internal bucket and the square or the rectangle here is what is called as the leaf bucket or an external bucket the leaf buckets or those which actually store the data initially all data is stored in a given  in a single bucket that is this is the bucket for all records  refer slide time  00  46  17   refer slide time  00  46  19  and assume that now we our keys are made of binary strings and then we are storing all of our data with these keys in these records now suppose there is an overflow that happens here in this bucket now what happens when there is an overflow ?  refer slide time  00  47  19  this slide shows such an technique that is when there is an overflow  the bucket is split into two different buckets and you can notice the labels here for the edges joining these buckets the first bucket is the set of all records whose keys start with 0 and the second bucket is the set of all records whose key start with 1 assuming that our keys are made of binary strings now suppose there is again an overflow in this bucket and there is no overflow in the upper bucket here  what happens to the hash table then the hash table then changes to this following data structure where the overflow bucket is split and a new internal node is created and two different buckets are then created so this bucket now is the set of all  holds the set of all records whose key start with 1 0 whereas this bucket holds the set of all records whose key start with 1 1 so we can trace that starting from the start node here  so 1 0 takes us to this bucket and 1 1 takes us to this bucket what happens now if one of this bucket encounters an under flow that is it becomes empty when records are deleted from the database we just have to merge this bucket with its partner so to say that is seen in the diagram here that this is the bucket who whose edge is labeled as one we have to just merge it with its partner whose edge is labeled zero  refer slide time  00  47  56   refer slide time  00  48  57  so this takes us back to the previous configuration where we had only two buckets in the hash table there is another kind of dynamic hashing what is called as extensible hashing which also uses a similar kind of hashing technique in order to grow and shrink buckets  refer slide time  00  49  03  extensible hashing uses what is termed as a global directory of 2 power n number of bucket addresses where n is called the global depth of the directory and then each bucket is uniquely identified by some set of higher order bits d number of bits which is less than or equal to n which can uniquely identify each bucket and of course buckets are split and merged whenever they overflow or underflow and correspondingly n is changed that is the global depth is either increased or decreased in a corresponding fashion  refer slide time  00  49  50  this slide shows such an example here we have a global depth of 3 that is n equal to 3 and there are several different bucket pointers that shows 000  001 and so on and there are several different buckets each with differing capacity where here this bucket says that d equal to 2 that means this bucket can be uniquely identified with just the top two bits that is all keys starting with 0 0 can go into this bucket here for example d equal to 1 that is all keys starting with 1 go into this bucket but here these two buckets have a large number of data elements that is where d equal to 3 that is this bucket contains all keys starting with 0 1 0 and this one contains all keys starting with 0 1 1  refer slide time  00  50  43  now suppose what happens if the last bucket overflows that is where d equal to 1 ? this bucket is then split so that d becomes 2 and then instead of instead of just one bit  we have to use two bits in order to uniquely identify this bucket therefore the top bucket here  the upper bucket here is the set of all keys which start with 1 0 and the lower bucket here is the set of all keys that start with 1 1  refer slide time  00  51  12  so that brings us to the end of this session where we have looked at several kinds of file organizations and for physically managing records on storage devices let us quickly summarize what we have learnt in this session we first looked at different kinds of storage media and what are their characteristics we can classify storage media into different kind ’ s volatile  non-volatile  primary  secondary and so on in fact they can be placed in a hierarchy and then there are different characteristics like random access  sequential access or read-only versus read-write or write-once and so on and then there could be either character devices or block devices and so on each of them  each of these characteristics impact the kind of data structure that we can use for storing records we then looked at the concepts of records  blocks and files which are the terminology we use for dealing with data that are physically stored on to disks records are the logical unit of data that are stored while blocks are the physical unit of data that is used for data transfer and file is the set of records or a sequential records in which typically a relation is stored we also saw the notion of spanning and un spanning of records in terms of how they affect the blocking factor or the number of blocks that are required to store a particular file of records we then saw three different kinds of file organizations  the first one was the pile file organization which is the simplest where we just append records into a file however which poses problems with insertion or rather with deletion and updation and also with search we then also saw sorted files which are files that are physically sorted on disks based on some ordering attribute sorted files are much more efficient for search because we can use binary search on sorted files however they pose very tricky problems in terms of insertion and updation of records and they become especially tricky when records can be of varying lengths and updations can change the key value of on which it is sorted we also saw the last kind of file organization called hash files where hashing function is used in order to identify the block number or the bucket in which particular record is stored we also saw how or what are the challenges that are faced by hash functions especially when we use static hashing techniques because static hashing techniques can not work efficiently when the dataset is skewed in which case we have to use dynamic hashing where the number of buckets in the hash table can dynamically grow or shrink as and when data is inserted or deleted so that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 11 indexing techniques single level hello and welcome to another session in database management systems in our ongoing exploration of dbms  we have a kind of graduated from looking at a data management from a logical perspective to looking at a data management from a physical perspective we have looked at how we can represent data conceptually using say the er schema or in a more physical form that is in a way that is more friendly to the computer as in the relational data form and so on however all of these models were basically mathematical models that gave us a kind of formalism which told how we can represent data elements and how we can represent relationships among data elements and so on we graduated from there to see how data is actually stored on the computer  what kinds of overheads do we incur when we use one kind of storage method versus another  which kind of storage method is easier in terms of lets us say insertion  easier in terms of updation  easier in terms of maintenance  easier in terms of searching and so on we looked at three basic kinds of file organizations and compared them in terms of their complexity of insertion  updation  search and so on and in our session on storage structures  we have also mentioned that when we talk about storage files there are basically two kinds of files that we are concerned about  what is called as a primary file or the data file contains the actual data that is stored in the database system and then set of one or more secondary files are what are called the auxiliary files which contain metadata which help us in accessing data elements as efficiently as possible in today ’ s session we are going to be concentrating on these secondary files these secondary files are called as index files which provide one or more index structures that can help us in accessing whichever data element we need as efficient enough as possible so let us look into index methods by firstly  briefly surveying or briefly summarizing what we have learnt about storage structures  refer slide time  03  27  first of all what are the storage requirements of databases  what kinds of requirements are we looking at here ? databases need data to be stored in a permanent fashion or what is termed as persistent fashion for long period of time it shouldn ’ t be the case that once power is switched off  all your data is lost it should be there for much longer period of time than the typical user session on a computer and usually the amount of data that we are talking about is too large to fit in memory we saw how the definition of very large databases has been changing over the years initially the term very large databases was used to mean hundreds of megabytes of data and now we are talking about peta bytes of data which is actually 10 to power 15 bytes of data this is especially true in databases like web log  i mean web databases as in web search engines like google  altavista or so on they routinely deal with peta bytes of data and in fact we also saw how the storage technology has been beating moore ’ s law in the sense that larger and larger amounts of storage is now possible in a smaller and smaller surface area and also at lower and lower prices therefore storage by itself is not a big problem storage is cheap  secondary storage is especially is cheap  quite easily available however the bigger problem now is to search for databases we have anyway stored peta bytes of data but how do we search the relevant data items or whatever data that we need in as efficient fashion as possible imagine how would it be if you used a search engine like say goggle and you gave a web search and it gave you a request to come back after two days to look at your search results it is unthinkable  we are looking at response time that is interactive in nature at most a few seconds before which the user gets bored or the user can not wait beyond that therefore a search engine like this has to search potentially a data space of peta bytes of data before giving results for a given request so in order to efficiently handle these data structure or this vast amounts of data  what is getting more important now is the set of secondary or auxiliary file structures using which we can try to efficiently access the data that is stored in primary databases  refer slide time  06  25  let us briefly go through some of the definitions that we studied in storage structures let us review some of them because they are again important when we are talking about indexing methods first  the notation of a record a record is the physical counter part of what could be termed a tuple in a relation or a row in sql parlance and on disk a data is stored in terms of files and file is treated as a sequence of record so  one might analogously say that a file stores a given relation or a given table although it need not exactly be the case because sometimes files are used to store more than one relations or relation is sometimes spread across different files due to some physical considerations like maximum file length that is allowed by the operating system and so on however in a general sense we can consider a file to be representing or to be storing or to be the physical counter part of a relation records could be of either fixed length or of variable length fixed length records are easier to handle in terms of quickly finding their location in a file for example finding the offset of a given record in a file however not all data elements can be amenable to fixed length records especially when we have to store data elements in form of text where a text can range from few words to thousands of words so if we allocate a large amount of memory for the text field  it would be unduly wasteful when we are using fixed length records in which cases we use variable length records and records themselves comprise of a sequence of fields a field is analogous to a column in sql parlance or an attribute in the relational algebra parlance  refer slide time  08  25  we dint saw the concept of blocks a block is a physical unit of storage in storage devices for example sectors in hard disk or page in virtual memory and so on they are the smallest unit of data that are been transferred between the storage device and the computer and usually we deal with block storage devices when we are talking about databases  we rarely deal with character devices where the unit of information transferred is a single character blocks are usually are almost always of fixed length  they are not of variable length and the length of a block is based on several characteristics that are beyond the scope of a typical dbms for example they are based on a consideration that deal with what is the storage capacity of the storage device that we are talking about  what is the operating system that we are using  what is the size of the data bus in the machine and so on therefore the database management system has little or no control over the size of a block and a contiguous block in a storage device may or may not correspond to the same file if they correspond to the same data file then it is well and good  in the sense that there is lesser overheads in accessing a file we don ’ t need too many seeks  seek is the set of operations that is performed on any storage device like disk in order to find the correct block that we want from the device so if contiguous blocks belong to same file  we do not require too many seek operations when accessing a data file on other hand if they do not belong to the same file then we may incur some overheads in the seek time of the storage device so a storage device is said to be defragmented  if contiguous blocks belong to the same file and it said to be fragmented if the blocks are distributed all across the storage device  refer slide time  10  30  when we are taking about blocks  there is an important term that we used the notion of a blocking factor the blocking factor is simply the number of blocks per record that are stored in the database that is if i have a record size of r and i have block size of b  blocking factor is simply b divided by r  the floor of this function b divided by r if b is greater than r then blocking factor is greater than 1  that means there can be more than one records per block however if r does not divide b that is if b is not a pure multiple of r then there is some extra space that is wasted which is given by the reminder of this division so how do you deal with this extra space ?  refer slide time  11  23  there are two varieties of dealing with this extra space we saw that records could be either of the kind of unspanned records or they could be spanned records unspanned records are those which do not span across different blocks  in such cases if we have some extra spaces as shown in the slide here they just are unused we can ’ t do anything about it  that is we just leave that extra space unused which results in certain amount of wastage of space however it helps in easy accessing of records from blocks  refer slide time  12  35  on the other hand we could think of spanned records were records can be split so that they are stored across different blocks this slide shows such a diagram where three records are stored in their entirety in a given block m and the fourth record is split between block m and block p and of course at the end of each block we should have some kind of a pointer that points to the next logical block  next block in logical sequence in the disk  so that we can know which block to access next in order to find the remaining part of the fourth record  refer slide time  12  38  and of course whenever b is less than r that is whenever record size is greater than the block size then we have to use unspanned record storage  refer slide time  12  54  we also saw three different kinds of file organizations and a file organization is simply an organization mechanism by which data is stored in files so that they can be efficiently accessed we saw three different kinds of file organizations which we termed as unsorted files or pile files  sorted files and hashing files unsorted files are those files where you just input records into the file or just append new records at the end of the file without any consideration to the data that is present in the record or in the file unsorted files are very efficient when it comes to inserting new records you don ’ t have to do any kind of searching  you don ’ t have to do any kind of reorganization  you just have to append it to the end of file however it is very inefficient when it comes to either deletion or modification of a data or searching  especially in searching of data in the worst case we may have to search the data file in a sequential fashion and this can be a tremendous overhead when the file is extremely large in size  when it is giga bytes or tera bytes of data stored in one unsorted file a sorted file on the other hand is a file organization where the file is physically sorted on the disk based on the some field in a record which is called the ordering field therefore a physical sorting of records helps us in easy access of the file we can use a search technique called binary search that can reduce or search space by half in each iteration so that usually what is termed as log n order of time  we can find whatever record that we are looking at however the sorted file organization incurs a lot of overhead whenever insertion or deletion happens in the file whenever a new record is inserted  we should ensure that the file remains physically sorted that means we may have to records in order to accommodate the new records in its place and similarly the case for deletion  in order to remove any kind of fragmentation we have may to move records so that the overall file remains sorted the last kind of file organization we saw was the hash files a hash file uses a hashing function which hashes or that is which is a function that can take a value of a key field in a record and transform it into one of several bucket addresses we saw two different kinds of hashing  static hashing and dynamic hashing where in static hashing the number of buckets are fixed and we have to deal with  we have the problem of contending with overflows especially if the data is queued that is if the data set or the distribution of keys is queued  we may have a small number of buckets overflowing and i have to be dealt with techniques like open addressing or chaining and so on which poses terrible overheads during searching and we could have a large number of buckets that are empty because none of the keys were hashed to those functions we also saw a remedy to this problem namely the notion of dynamic hashing where the number of buckets could actually grow or shrink in size whenever records are added or deleted from the database  refer slide time  16  40  let us move on to auxiliary file structures  the main topic of concern today  the notion of indexes let us first go through a few definitions before we look at actual index structures firstly  the notion of an index file an index file is a secondary or auxiliary file that contains meta data or data that helps us in accessing the required data elements from the database an index or an access structure is the data structure that is used inside the auxiliary files that helps us in searching for our data elements as efficiently as possible and of course data structure are augmented with their corresponding search methods in order to search for our data record we can think of two kinds of indexes what are called as single level indexes and multi-level indexes single level indexes have just one level of index structures that is in addition to the primary file there is just one secondary file and the index file maps directly to block or record addresses in the primary file a multi-level index on the other hand has multiple levels of indirection where one level of index structure may point to another level of index structure and so on and finally the last level would point to block addresses or record addresses in the primary file  refer slide time  18  15  let us look at a few more definitions regarding pertaining to index structures the notion of an indexing field or an indexing attribute is the field on which the index structure is built that is searching is efficient whenever a search is given on this field that is on whichever field an index is maintained for example  we saw the notion of the ordering field which is a field based on which records are ordered on disk in sorted file organizations an indexing field is analogous in the sense that this is the field on which an index structure is build usually the ordering field and the indexing field are the same and usually they are also the primary key that is the primary key is usually the ordering filed and by default an index structures is built on the primary key that is used in the records a primary index is an index structure that is defined on the ordering key field that is the field that is used to physically order records on file in sorted file organizations and in many cases the ordering field is the same as the primary key  refer slide time  19  36  we also define a notion of clustering index where these are index structures that are defined on an ordering key field however in cases where the ordering field is not the primary key or is not even a key field that means the ordering filed need not be unique remember that all key fields have the unique constraint  that are posed on them and whenever ordering is performed on a non-key field  it is not unique so an index structure on such a non-unique field is called a clustering index we also define the notion of a secondary index which is an index structure that is defined on a non-ordering field and even not necessary the key field  refer slide time  20  27  let us have a look at primary index to begin with like we mentioned before a primary index is an index structure that is defined on the ordering field of records and that to when the ordering field is a key field usually it is a primary key on which records are ordered on disk a primary index comprises of an ordered file  note that even a primary index is a sorted file and it comprises of fixed length records and has two fields these two fields are shown in the slide as k of i as you can see here that is there is a pair called k of i which is the key for the i th record and a of i is the block address containing the i th record  refer slide time  21  21  the figure in this slide shows an example of a primary index here on the right hand side of the figure  the primary data file is shown where records are divided into several blocks so each block begins with a particular field number and of course this is a sorted file organization in the sense that records are physically sorted in these blocks now if you can notice here  the first record or the first field in each block for example the first field in the first block is 2000 3 0 1 0 1  the roll number of a student and of the second block is 2000 3 01 21 and of some other block here is 2003 02 21 and so on the first field in these blocks are indexed in the index file that is they appear in the index file here these the first field in the index file is k of i which we saw earlier that is the key value that is maintained in the index the second field contains the pointer to the block or the block address that contains this record  refer slide time  22  39  what is some of the properties of primary indexes ? if you have noticed  the number of entries in the index is equal to the number of disk blocks that comprise of the primary file that is the primary or the data file the first record in each block of the file is indexed that is the first record in each block of the file appears as 1 of k of i ’ s in the index file these records are called anchor records because this is the anchor by which other records in the or other key values in the block are accessed therefore if you are searching for a key value of let us say 2003 01 20 as shown in the slide here  we perform a binary search on the index file and we come to a point where we realize that it has to lie in the first block itself because 2003 01 23 is greater than 2003 01 01 and lesser than 2003 01 21 so we have to find that block address which or that anchor address which is lesser than or lesser than or equal to the given key and the next block address would be greater than the given key such an index structure in which not all ordering or key attributes are indexed is called a sparse index the primary index that we saw here is a sparse index a sparse index essentially means that not all attributes or not all possible key values are indexed more specifically in a primary index  we are only indexing one key value per block on the other hand a dense index or is an index structure where each key or each search key value that appears in the primary data file is indexed in the indexed file  refer slide time  24  50  how do we search using a primary index ? search is easy  we saw just a few moments ago that we can search using binary search  so we just have to perform a binary search whenever we have to search using the key value and then find such a record or a key value that is less than or equal to the key value that we are looking for  such that the next key value that appears in the index is greater than the key value that we are looking for what about insertion and deletion ? insertion and deletion is easy if records are of course one fixed length and they are statically allocated to blocks  without block spanning what is this mean ? this means that suppose i allocate  suppose i know the set of all  the entire range of values of a given set of keys and i statically allocate a given sub range or subset of keys to particular blocks for example let us consider that a student roll number can range from 0 1 0 1 to 0 1 5 0 let us say there are 150 students who can range from 0 1 0 1 to 0 1 5 0 and then we say that each block contains exactly 20 records therefore we store 0 1 0 1 to 0 1 1 9 or 0 1 2 0 rather in the first block and 0 1 2 1 to 0 1 4 0 in the second block and so on therefore we statically allocate each record to block address if we do that insertion and deletion are easy however they may result in wasted space  especially if not all records of this data set may be available at any given point in time for example if we have only let us say 0 1 0 1 and 0 1 5 0  we still have to have a number of blocks wasted because each address is stored in a particular block  we can not store any other address in other blocks on the other hand if we don ’ t want this wastage of space then we have to contend with re-computation of blocks that is moving records between blocks and also re-ordering of the index structure whenever insertion and deletion takes place that is this is because primary index is based on a sorted file that is a file on which the entire data set is sorted and it has to remain sorted whenever insertion and deletion take place  refer slide time  27  37  the next kind of index structure that we look at is the clustering index remember the definition of clustering index a clustering index is an index structure that is used when a sorted file organization is used and the ordering key or the key on which a file is sorted is not the key field that is ordering field is not a key field what is the implication of saying that the ordering field is not a key field ? the implication is that when a field is not a key field  it means that there is no unique constraint on the field that means there could be repetitions  that is the same key value in the k of i  a of i model  the given k of i value may point to multiple addresses or multiple block addresses which store data values pertaining to the same key the structure of a clustering index file is similar to that of a primary index file in the sense that even this stores k of i and a of i addresses except that  except to this small change that only distinct values of the ordering field are stored that is suppose we are ordering a primary data file based on the student names rather than roll numbers we just store one index entry for each distinct student name and not every occurrence of student name and we store the block address of the first occurrence of any given student name that is the first occurrence of the particular ordering field is what we are storing rather than all addresses of a given field  refer slide time  29  38  let us look at an example the slide in this example shows a data file which is a sorted data file of course and where the ordering field is the department number and there is no unique constraint on the department number that is department number may repeat over several records we see here in this example that there are three records pertaining to department number 1 and three records pertaining to department number 2 and two records pertaining number 3 and so on and we also see that the record pertaining to department number 2 has spanned or this key or this ordering field department number 2 has spanned over to two different blocks that is the first block and second block the left hand side of the slide shows the clustering index which contains the usual k of i and a of i fields where k of i is the value of the ordering field or the indexed field and a of i is the block address note the first two entries in the indexed file the first entry says that for the value one  department number 1 look up block number one the second record says that or the second indexing record says that for the value of department number 2 look up block number 1 again why is this so ? this is because the value two appears for the first time in block number one what about the other values of two  how do we search for the other values of two note that the file or the primary data file is a sorted file that means to say that if we know the first occurrence  if we know the block number of the first occurrence of a given ordering field  it is sufficient because the file is sorted and we can start looking at the next logical blocks in sequence  until we finish all the set of records having the same ordering key therefore if we are searching for a particular data record having department number 2  we have to first start from block number 1 and proceed in the next logical block in sequence which in this case is the block number 2 until we exhaust all records having department as 2  refer slide time  32  07  a clustering index is also a sparse index why is this so ? this is because only distinct values that are appearing in the ordering field are indexed in the previous slide even though department number 1 appeared three times and the same thing would be for department number 2 only a single entry existed in the clustering index field for cluster indexing file for both of these records insertion and deletion in clustering index may cause problems because of the well-known problem of sorted files that is the files have to be sorted and when we have to retain this sorted mechanism of or sorted form of files  this may impact on the clustering index in the previous example suppose we inserted a new record having department number as 1 then we have to move all the records having department number 2 that is the last record in block number 1 to the next block that changes the corresponding address  the corresponding a of i value in the clustering index field that is a of i can not  can no longer point to the first block but instead it has to point to the second block therefore we need re-orderings  whenever we perform insertions and deletions there is an alternative solution for handling this problem of insertions and deletions that is to allocate blocks for each distinct value of the clustering field this is shown in the example in the next slide  refer slide time  33  53  this slide shows an example  where a clustering index is used on a primary file that is ordered on a non-key attribute however there is a very specific organization of the non key of the primary file in the sense that each distinct value of the non key attribute of the ordering field is allocated a separate block have a look at the right hand side of the figure more carefully in the first block there are three records having department number 1  in case more records are inserted having department number 1  they are not allocated to next logical block in sequence however they are allocated to separate blocks and a separate pointer is maintained to these blocks  so that we can access more records having department number 1 have a look at the third block that is seen in this slide here there are two records having department number 80 and the block can accommodate three records of this particular size however even though other records exist for example department number 89  they are not put into to this block ; they are given a separate block by themselves therefore this kind of block organization results in certain kinds of wasted space because especially since  especially if there are not many repetitions of the non key attribute however insertion and deletion are much more simpler in such an organization this is because we don ’ t have to worry about any changes in either the block structures or in the index structure itself  refer slide time  36  02  the next kind of index that we are exploring today is what is called as the secondary index a secondary structure or a secondary index file is an index file that is used to index fields that are neither ordering fields nor key fields  that is there is no assurance from the primary file that the file is organized or ordered along these fields and they are also not key fields that is there is no assurance that the values in these fields are unique there could be many secondary indexes possible on a single file that is depending on how many fields that are there in a given record a secondary index maintains one index entry for each record in the file that is if you remember the definition of a dense index  a secondary index is a dense index  refer slide time  37  10  in contrast to a sparse or a non-dense index where not all values of the indexed field are indexed this slide shows an example of a secondary index assume that roll number is no longer the key field and it need not even be unique so the left hand side of this slide shows a dense index where each and every field or each and every value of the roll number field that appears in the primary data file also appears in the index file and there is a corresponding pointer from each of these key values to the particular record addresses directly note that we don ’ t have to maintain block addresses here because this is a dense index  we can directly dereference or directly refer to the record address that is the block address and the offset within a block where the record begins and note that the file also need not to be ordered based on this secondary index and the pointers or the record pointers are arbitrarily shaped when they point to the primary data file  refer slide time  38  22  what are some of the properties of this secondary index ? since if i am performing a secondary index on a key field  let us look at some properties of secondary index  maintaining secondary index on a key field in previous example roll number could be a key field in the sense that it need not be an ordering field but it could be a key field the data file could be ordered on some other field let us say name or grade something like that but the index is maintained on the secondary or on the key field which is unique that means that since each field or each key field is unique  there are as many secondary index entries  as there are records in the data file because it is dense index and each index entry or each key field has to be indexed however the data file need not to be sorted on to the disk that is it need not have  there is no need for that because we are directly referencing it ’ s a dense index and directly pointing to the particular record address and because it is key field and because key fields are unique  we can maintain or we can be sure that we can use fixed length records for the secondary index because we know the length of the key field and we know the length of the address of a given record therefore the length of a key field plus the address of the given record in a primary file forms a length of the cluster of the secondary indexes  refer slide time  40  11  what happens if the secondary index is maintained on a non-key field that is where the field that is being indexed may have duplicate values that is it need not be unique and it can have many number of value what is the implication of having duplicate values ? as we saw in the clustering index  a given value of the indexed field may point to multiple records or multiple records in the primary file there are three different techniques for handling the duplicates in secondary index note the difficulty that we encounter in secondary index that is not there in a clustering index in a clustering index we have the assurance that even though the indexing field that we are using is not a key field  the primary file is ordered or is physically sorted according to this ordering field on this non-key field therefore if we just know the first occurrence of a given particular value of this field  it is sufficient using which we can access all other values that are present in the database on the other hand when we are using a secondary index and a secondary index can be used on fields which need not be ordering fields we don ’ t have any assurance of that sort that is this is a non-key field  so therefore there could be duplicates and there is no assurance as to how these records are distributed in the primary file itself so handling duplicates becomes much more difficult in a secondary index rather than in a clustering index there are three different varieties of handling secondary index duplicates  one is to use duplicate index entries duplicate index entries means that we use fixed length records however and of course use a sorted file or physically sorted file for the secondary index file and maintain as many index records as there are different values of a given non-key field the second one  the second approach is to use variable length records that is have one value of k of i and many values of a of i that is more than one values of a of i that point or that in turn point to different record addresses or the third approach is to use extra re direction levels which will see in more detail shortly where the first level points to a block of record addresses and so on  refer slide time  43  04  the first option is to use duplicate index entries  that is index entries are repeated for each duplicate occurrence of the non key attribute the example shows here that the term 2003 01 02 is repeated 4 times and 2003 01 03 is repeated twice in the secondary index file the advantage of such a scheme is that we can still use fixed length records however the searching of this data file becomes a bit more complicated binary search becomes more complicated why is this so ? because remember how binary search works binary search starts with the entire space or the entire set of indexes  as the search space to begin with that is the lower bound for the binary index is to begin with the first record and the upper bound is the last record and then we compute a mid-point of the lower bound and the upper bound and compare our key  the key that we are searching with the mid-point now what happens if we compute a mid-point in a data file or in an index file where key values could be repeating when we compute a mid-point that is when you compute lower plus upper divided by two  we are not sure that or we can not say that all records having this particular key will appear either to the left or to the right that is there could be repetitions or there could be duplicate entries of the particular key at the midpoint on either sides of the midpoint so we have to search both sides in order to retrieve our particular or in order to make the next decision about the next iteration and of course insertion of records would require restructuring of the index table because the index table is always sorted and maintained in a sorted order  refer slide time  45  10  the second approach to handling secondary indexes was to use variable length records in a variable length recording record schemes we use a  given k of i value that is if you look at the previous slide here for a given k of i value  the size of the a of i field is not fixed that is there could be one or several address fields for the given key index what are the problems or advantages of this approach ? one advantage is that binary search  well it is still little bit complicated however it does not suffer from the complications of repeating multiple keys that is if we know exactly the block addresses and block addresses are stored in a way that we can find out the next block address very quickly  we don ’ t have to worry about whether a given entry appears on both either side of the mid-point however it becomes a complicated in the sense that if variable length records are stored in a single index file  the midpoint may not pertain to or may to point to a valid block address or a valid block address of the index file and insertion and deletion of the records may require restructuring of the index table so restructuring in the sense that we may have to add more fields in the a of i or we may have add more addresses  add more values a of i field which in turn may affect the next fields that appear in the database and of course there is also this problem of spanning and non-spanning of records that is we may have to use spanning records in order to in order to allocate them into blocks and associated problems of a searching and retrieval which we saw in the session on storage structures also hold for this kind of indexing scheme  refer slide time  47  41  the third kind of scheme  secondary index scheme in order to handle duplicates or in non-key attributes is to use extra redirection levels have a look at the slide shown in this figure this slide shows three different levels of files that is the right most part of this slide shows the primary data file which contains blocks which in turn holds one or more records there is a label id or a lab id field in this file which is the record or which is the field which is being indexed by the secondary index file the lab id field is neither in sorted form in the primary data file nor is it a key field that is it need not have unique addresses  it can have repetitions because it can have repetitions  each repetition that is each distinct record address for a given value is given a separate block have a look at the left most part of the slide here the left most part is the usual secondary index file comprising of k of i and a of i fields where k of i contains distinct values of each of the distinct values that appear for the indexed fields which is a non-key field now  because this distinct value can pertain to several records or several records in the database  it is first the a of i address first points to a block that is reserved for storing addresses of this key value or of this index value this block is of fixed length note that this is shown in the second field here note that even though a particular is not full  it is not used up for the second address this is the block for the second value is a completely different block from the block for the first value even if it is not full that is a separate block is allocated for each distinct value of the index field and this block contains the set of addresses by which we can using which we can perform a sequential search or on the particular address that we require or retrieve the set of all records pertaining to the given key value that we are searching for so in this session we looked at three kinds of indexed files or three kinds of single level index structures namely the primary index  the clustering index and the secondary index let us briefly summarize before we end this session and look at what are the different kinds of index structures that we saw in this session today  refer slide time  50  59  first of all let us revisit the extra indirection levels and look at some more properties of extra indirection levels before we summarize  the extra indirection level is the most frequently technique for handling duplicate record and the advantage of this is that the index records are of fixed length and it doesn ’ t suffer from complications  binary search complications that we discussed earlier and we can use the usual binary search in order to search for a particular given address however there could be wastage of spaces because a complete block is allocated for a given value even though if it is not completely full and what happens when blocks overflow ? that is there are large number of records of the given index  in such cases block overflows are handled by chaining which is the same technique that we saw in the hashing technique and retrieval requires sequential search within blocks however insertion and deletion of records are straight forward  we don ’ t have worry about restructuring the index or restructuring the data file we just have to insert the corresponding entries in the block file whenever insertion and deletion take place so let us come back and summarize the different kinds of index structures that we saw today  refer slide time  52  31  firstly the type of indexes  if the index structure is on a key field and the ordering field is or rather if the index structure is on key field and the ordering field which is also an ordering field then such and index structure is called a primary index that is usually on the primary key and the keys order so primary index can afford to be sparse and point to and store and index only the anchor records in each blocks on the other hand if the index is on a non-key field however which is a ordering field then it is called a clustering index that means the data file or the primary data file is physically sorted based on the ordering field which is not a key field that means there is no unique constraint for this field  in which case we can use the clustering index where we just store index structures for each distinct value of this field and store the address of the first occurrence of this distinct value because it is ordered  we don ’ t have to worry about accessing the other values that exist in the database because they appear in logical sequence starting from the first occurrence if the key field is or if the indexing field is a key field  however it is not an ordering field then we use a secondary index for a key field we saw that a secondary index on a key field is a dense index with fixed length records on which binary search can be used efficiently if the field on which indexing is performed is not a key field and is also not a ordering field then we have to use secondary index of the non key variety that means we have to deal with duplicates in one of three different fashions either use duplicate index entries or use variable length records or use the most commonly used technique of extra indirection levels in order to handle duplicate address  refer slide time  54  48  and quickly what are some of the properties of the different index structures that we saw ? the primary index  what are the number of index entries the number of index entries for a primary index is the number of disk blocks in the primary file or in the data file it is a non-dense index or a sparse index a clustering index is also a sparse index which stores the number of distinct index field values that is the number of distinct values that appear in the index file and a secondary index on a key attribute contains as many number of index records as there are number of records in the data file itself  it is a dense index and a secondary index on a non-key field may or may not be dense  may be either dense or sparse depending on whether the non key field is unique or not that is the repetition in the non key field or not and the number of records contained in the index file is equal to the number of distinct values that appear in the indexing attribute so this brings us to the end of this session where we saw different kinds of single level indexes in the next session we shall be looking at multi level indexes where index structure can have several different auxiliary files database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 12 indexing techniques multi-level and dynamic indexes hello and welcome in the previous session we had looked into different varieties of index structures index structures are auxiliary files that are used in the database storage that are used to help in accessing data which are stored in the primary files index structures are extremely important especially when database sizes have been growing exponentially in the recent past and the value of index structure is also more important because the main problem in database today is not the storage of the large amounts of data but retrieval of them rather in searching data elements based on some certain criteria  key values and so on we saw different levels of  different kinds of index structures namely primary indexes  clustering indexes  secondary indexes on key attributes and secondary indexes on non-key attributes and so on let us briefly summarize them today in this session before we move on to more complex index structures  refer slide time  02  28  some key definitions in index structures are shown here an index file is a secondary or auxiliary file that is used to help speed up data access that is contained in the primary files or the data file an index or an access structure is the data structure that is used in these secondary files which help in this retrieval process and of course the data structure is associated with its corresponding search methods and algorithms using which we can access these data elements as quickly as possible we said that there are two kinds of index structure primarily  namely this single level index and the multi-level index structures however until now we have just covered this single level index structures a single level index structure is a single auxiliary file that directly maps to addresses in the primary file and these addresses could be either block addresses which stores physical blocks on disk or any other storage medium or they could be recorded results where the address of or a record is directly stored or a record can be directly accessed within a block that is the block address is augmented with the offset value which gives us the record address  refer slide time  03  57  some more definitions which are again important for looking into multi-level indexes which we are going to be exploring in this session the indexing field is the field or the attribute on which the index is maintained and usually the field could be either an ordering field or a non-ordering field it could either be a key field or an non-key field and the corresponding index structure for each of these fields changes depending on what kind of fields or what is the characteristics of the field that we are indexing a primary index is an index that is defined on the ordering key field of the data element that is the field should not only be an ordering field  it should also be a key field of the data element what are the properties of an ordering field and a key field ? key field has a property that it is unique  that it has a uniqueness constraint or that no two fields in the database  no two key fields in the database have the same value similarly if the field is an ordering field we can have an assurance that the primary data file is physically sorted based on this field therefore whenever we have a key of a given value i  we know that for all key values greater than i we have to search forward that is we have to search in the forward direction of the file we don ’ t have to search the reverse of the file or all the key values until now and so therefore these properties the property that the file is ordered based on the ordering field and the field is a key field helps us in building a primary index  there is a sparse index which can help access data in the primary file as efficiently as possible we also look into clustering index which is the index structure that is used when the field that is to be indexed is an ordering field but not a key field if the clustering field is not a key field then it is no longer constrained by the uniqueness constraints that is there is no longer requirement that each of these  each element in this key field has to be unique this poses a particular problem in the sense that a given key value may correspond to more than one addresses the last kind of index that we saw was the secondary index a secondary index is an index that is defined over some non-ordering field  refer slide time  6  45  that is there is no assurance that the primary data file is ordered based on this field if the primary data file is not ordered based on this field then it is not possible for us to store a sparse index this is because we don ’ t know where to search the next record from therefore the index has to be dense index structure however there is still  there is a further dimension to the secondary index data structure that is  is the key field or is the indexing field on which the secondary index is based upon is it a key field or a non-key field if it is a key field then we have a particular kind of index structure and if it is a non key field then the index structure changes  refer slide time  07  27  let us briefly look at some illustrations of the three kinds of index structures that we are covered so far  so that it helps us in understanding the more complex index structures that we are going to cover in this session the primary index structure is shown in this slide here  this slide shown an index file pointing to a different blocks in the data file the data file comprises of different blocks that is records that are organized into different blocks and the blocks or the records are sorted within the blocks that is the indexing attribute is not only a key attribute that is it is not only unique  it is also the set of data records are also sorted based on this indexing field when this is the case  it is enough for us or it is sufficient if we are able to store or if we are able to index just the first attribute or the key value within a given block this is called the anchoring record if you remember so we just store the key value of the anchoring record in the index file and maintain a sparse index the number of entries in this index file is equal to the number of blocks  the physical blocks that make up the primary file or the data file and this index file is a sparse index because it does not store all attributes or all values of the key and this index file can afford to use fixed length records because the value of the record is  value of the key is known and the value of the address  block address is also known and we don ’ t need anything else therefore the primary index file can afford to use fixed length records  refer slide time  09  15  this slide shows an illustration of the clustering indexing structure in a clustering index  the file or the primary data file is ordered based on the clustering field however this clustering field is a non-key field if it is a non key field then there is no guarantee or there is no requirement for the field to be unique so  in this slide there are some records that are shown  ordered on the field called department number and there are repetitions in the department number that is there are three number of ones for a given department number  three number of two ’ s and two number of three ’ s and so on however since the primary file or the data file is ordered based on this field  it is sufficient for us to know where does the first or where is a first occurrence of a given value and that is what we store in the index file that is the index file stores a unique values that the ordering field takes up that is shown in the left hand side of the slide that is values like 1 2 3 and so on and a pointer that points to the first occurrence that is to the block that contains the first occurrence of this particular value because the primary file is sorted  this is sufficient for us however there is a problem with insertion of records which needs  which may need a clustering indexes to be altered and which can be rectified by assigning separate blocks for each distinct value of the ordering field  refer slide time  11  03  we also know that secondary index on key field attributes  if it is a key field note that secondary index or index structures that are maintained on non ordering fields that is a fields on which  fields which do not contribute to the physical ordering of data records in the primary data file because they do not contribute to the physical order of the records in the primary file  this has to be a dense index because we have to for each record we need to know where this index or where this is actually stored therefore all values of the index attribute has to be reflected in the indexed file in the slide here there are two files shown the index file shown in the left side contains each roll number which is the indexing attribute and which is also the key attribute that is the roll number is unique because the roll number is unique that is because the attribute is a key attribute  we don ’ t have to worry about duplicates  we don ’ t have to worry about repetitions and because repetition is not a problem we can afford to use fixed length record sizes for the indexed records that is we know the length of the key attribute and we know the length of a record address and therefore we can afford to use fixed length records in the indexing file however the indexing file is dense and it contains as many records as there are records in the data file itself if secondary index is maintained on a non key attribute then we no longer have the luxury of the uniqueness constraint on the attribute that means this attribute not only  does not contribute to the physical ordering of records in the primary file  it also is not constraint by the uniqueness constraint that is there may be repetitions  there may be several different records having the same key value if this is the case  then a given key value k of i may correspond to multiple addresses in clustering index this was not a problem because the indexing attribute was an ordering attribute that is physically the data records were ordered based on this attribute therefore it was sufficient for us to know where is the first occurrence of this record we do not have such a luxury in secondary indexes because this indexing field is not an ordering attribute in such a case we use extra levels of indirection or extra level levels of redirection in order to reach the data record in the slide here there are three different kinds of files that are shown  refer slide time  14  02  the left most file is the secondary index file which shows k of i and a of i attributes that is key values each and every distinct key value and a block address for each key value disk block address is not the block address of the data file but in fact a block of addresses  a block address containing a block of addresses  many different addresses so this block contains several addresses  one each for each record having this particular value and they are stored block wise and block overflows are handled by chaining so that each different key value occurs in a separate block by itself  refer slide time  14  53  so let us briefly summarize the different characteristics of a single level indexes and see and motivate a need for multilevel indexes if the field or the indexing field is a key field and an ordering field  we can use a primary index as shown in the slide here if the indexing field is a key field and not an ordering field  we can use a secondary index on keys that is a dense index with fixed length records if the indexing field is a non key field but it is an ordering field then we use the clustering index where we can store just the address of the first occurrence of every unique data value if on the other hand  if the indexing attribute is neither a key field nor an ordering field then we have to resort to secondary indexes of non key varieties that is we have to use an extra level of indirection  refer slide time  16  06  there also other properties of the index structures that is primary index is the sparse index where the number of records in the index file is equal to the number of blocks in the primary file similarly a clustering index is also a sparse index where the number of records is equal to the number of distinct values that are present for the attribute a secondary key index is a dense index which contains as many number of records as there are records in the database itself while the secondary non-key index is either a dense or a sparse index depending on weather there are repetitions in the non key attribute and the number of records in the key attribute is simply the number of distinct values that are present in the indexing attribute in a data file  refer slide time  16  46  one of the main advantages of index structures like say primary index or dense secondary indexes and so on is that index files are ordered files that is they are ordered on their key values because they are ordered on their key values ; it is possible to search them based on binary search a binary search is a search technique which reduces the search space by half in each iteration therefore in the average case or in a ideal case  one can reach the particular key or the address in log n number of times  log n to base 2 where n is the number of records in the file on the other hand a linear search requires times of the order of n or n by 2 rather  so n different memory accesses or n different record accesses however we can note that there are three different entities that we are concerned with during physical storage these are the file itself which is a sequence of logical records  a record which is a logical equivalent of tuple in a relational schema and the block which is purely of physical nature that is which is meant for efficient data transfer between the storage media and the computer and whose size is determined by physical characteristics now between block and records  we have defined a notion of the term blocking factor where blocking factor is the number of records per block or how many number of records can we store in the block now if the blocking factor of an index file that is of blocks storing an index file is greater than two that means if a block  if a disk block can store more than two addresses or more than two index records then we can actually come out with a even better method of searching where the method of searching is of the  reduces by the order of blocking factor rather than by the order of 2 which is constant in binary searches these are explored in multi level indexes  refer slide time  19  20  a multi level index is an index file which contains several different levels as the name suggests and each index block at a given level has a factor called the fan out a fan out is typically derived from a blocking factor that which depends on the number of records that one can store in a block so a fan out is the number of different records that or the number of different entries that a given index entry can point to and in a good implementation  block accesses can be reduced from log n or log b to base 2 where b is the number of blocks in the index file  so it can be reduced from log b to base 2 to log b to base fan out so this is useful if fan out is obviously greater than 2 if fan out is greater than 2  we can reduce the number of block access by a tremendous factor  refer slide time  20  31  the figure in this slide shows a two level index structure where index structures are categorized into first level and second level the first level is called the base level and the second level is called the top level and for the sake of clarity  i have also shown how index records are divided into blocks at the base level it is divided into two  here the blocking factor is 2 but usually the blocking factor is more than 2 if you can notice the slide carefully  the way the slide or the way the index stores information is that there are different levels in which information is stored at the top level there are just 2 entries k of i entries 2 and 10 this entry says that everything between 2 and the next number can be found in this index file that is every key value between 2 that is greater than or equal to 2 and less than 10 can be found in this index file similarly every key value that is greater than 10 and there is nothing below  therefore which is just greater than 10 can be found in the index file that is pointed to by this pointer the other  the second level index file is also a replica of the top level index file in the sense that everything greater than or equal to the key value that is specified here and less than the key value of the next record is pointed to by the present address that is this is an indexing scheme that indexes an ordering field and also a key field therefore all records starting from 2 to less than 5 can be found here and all records starting at 5 and less than 10 can be found here and so on  refer slide time  22  39  the first level or the base level is a usual primary index that is maintained on a sorted file the second level is a actually a primary index on the primary index because the index file itself is a sorted file and it is sorted based on the key attribute  we can store another primary index on this index file and we can continue this process to any number of levels depending on the size of our database so we could have a third level that stores an index or that stores a primary index on the second level and so on and at each level  the number of entries in the next level is determined by the fan out or the blocking factor here the fan out was 2 therefore what we ended up seeing was a binary tree structure however in general the fan out is usually much more  a block could contain many number of records  many number of index entries much more than 2 therefore at each level from level 2 to level 3 for example fan out number of records that is a blocking factor number of records can be indexed using just one index structure therefore the number of entries starts reducing exponentially by a factor of fan out so this again depicts the same thing that is at each level  the number of index entries is getting reduced by half  refer slide time  24  19  in order for a multi level index to be efficient enough or to be more efficient than primary index  there is an important consideration of the multi-level index structure if you are familiar with data structures  you might have come across a data structure called tree which is a data structure used for representing hierarchies a multi level index structure is useful only when the tree structure that is specified by the multi level index structure is balanced what is meant by a balance tree ? it is well possible for us to have a multi-level index structure of the kind that is shown in the left hand side of this slide shown in this figure in the left hand side  as you can see there are several levels to the index structure and in the worst case one has to make 4 different 1 2 3 and 4 different traversal ’ s of different index files in order to find a given record on the other hand the right hand side of the figure has the same number of nodes or here each node or each circle here represents an index file and the right side of the figure has the same number of index files  however a smaller number of levels and the load or the number of index files is more or less evenly balanced across the entire tree in such a case in the average  the average behavior of the such a tree or even the worst case behavior of a balance tree is only log n to base fan out while the worst case behavior of an unbalanced tree is of the order of n different block accesses and which is no different from performing a linear search over these different index files therefore an order for multi level indexes to be useful  they have to they have to form a balanced tree  refer slide time  26  42  so whenever insertions and deletions happen in a data file containing a multi level index  the balanced property of the index trees should be maintained and this is especially problematic in multi level indexes because all index files are physically sorted files and we need to make a number of different adjustments at number of different levels  if you are just storing several different primary index structures in order to maintain the balance tree property of the index structure an approach to overcome this is what is called as dynamic multi-level indexes that is an index structure changes itself dynamically by as a little a number of operations as possible so that the balanced property of the index tree is maintained  refer slide time  27  41  the most commonly used dynamic index structures are what are called b-trees and b plus trees let us have a look at these two index structures in a little more detail here a b tree is an index tree which is of course as the name suggest a tree data structure where each node has a pre determined maximum fan out given by p which is of course  when we are implementing it which would be related to bfr that is the blocking factor there are several terminologies we use when we are talking about b tree a given block that is allocated to a b tree is called a node in the b tree as we will see later  a block corresponds to tree node in the logical tree structure that the b tree forms a special kind of node called the root node forms the access to the b tree that is it is the top most node in the tree and each node has a maximum of p children that is the fan out number of children and of course each node has a maximum of one parent i am saying the maximum of one parent because the root node will not have any parent node so it is either 0 or 1 parent depending on whether it is a root node or a non root node and there are what are called as leaf nodes that are the lowest level nodes that is nodes which do not have any children and any node in the tree that is not a root node  that is neither a root node nor a leaf node is called an internal node of course we have already defined the notion of parent and children that is when a node points to some other node then it is said to be a parent of the other node and the other node is said to be the child of the node which is pointing to it  refer slide time  29  41  this slide shows a typical structure of a b tree node this node is nothing but a block when it is implemented on disk as you can see here there are several aspects or several fields to this node in a b tree one can easily notice that there are several pointers in fact there are precisely maximum of p pointers in the b tree each pointer which points to a triangle represents a sub tree that can be pointed to by this node of course a sub tree can be null if this is a leaf node or if a pointer does not exist sub trees are filed in such a way that there are filled leftmost that is you can not have a left most sub tree as null but some internal sub trees to be filled up and and left most sub tree being null and in addition to the sub tree pointers or pointers to other nodes  there are several different key and data pointers that is there are several blocks here that contain a key value under pointer to the record containing this key value similarly there is another key value and and pointer to that record and so on so there are several key values that are present and pointers to where the key values are present in the primary file in addition to maximum of p pointers to other nodes in the tree what are the properties of these  what are the properties of such nodes or what are the constraints that a b tree has to adjure to ?  refer slide time  31  30  for a node containing p minus 1 keys  note that if a node can point to a maximum of p sub trees  it can contain a maximum of p minus 1 keys because as shown in the previous slide  their keys embed or this pointers embed the keys that is there is one pointer on the left most side of the left most key and one at the right most side of the right most key and one pointer between every two keys in the node the keys are always stored in a sorted sequence that is if there are p minus 1 keys then k1 is less than k2 is less than etc until k of p minus 1 for any data element in a sub tree that is pointed to by one of the sub tree pointers  let us say in some sub tree pi for any data element in that sub tree pi that data element should be less than the data element of the left most key that is k of i minus 1 or rather it should be greater than the left most key  the right most left key that is k of i minus 1 and should be less than the leftmost right key that is k of i  refer slide time  33  02  so more constraints of b tree nodes  each node can have at most p tree pointers of course and each node except the root node and the leaf nodes should have at least sealing of p by 2 tree pointers what is sealing of p by 2 ? divide p by 2 and take the upper integer value of this division so they must have at least more than half of their pointers to be filled up as part of the tree building procedure the root node should have at least two tree pointers  unless it is the only node in the tree that is unless root node is also a leaf node  it should have at least two children as part of the tree and all leaf nodes are the same level what is the level of a node in a tree ? the level is simply the distance in terms of the number of hops from the root node all leaf nodes are maintained at the same level in the tree so this is the constraint that has to be maintained and suitable algorithms have to be or suitable algorithms are created so that these constraints are maintained  refer slide time  34  22  we shall be looking at insertion and deletion algorithm for b trees  after we have a look at the b plus trees in fact the insertion and deletion algorithms for both of these tree structures are the same except that b plus tree has greater expressiveness or b plus trees allows for different kinds of accessing  different varieties of accessing the data elements in addition to b trees therefore let us have a look at some definitions of b plus trees and their constraints before we look at insertion algorithms b plus trees is a most common index structure that is found in many of the commercial rdbms it is very similar to a b tree except that the leaf and non leaf nodes have different structures in a b tree  there is no difference between a leaf node and a non leaf node both of them have a same structure that is they have a set of address pointers and the set of key values and data pointers the leaf nodes form a separate kind of index containing each different key value in a sorted form and pointers to the corresponding data elements that is leaf nodes are linked together so has to provide ordered access to the data file records  refer slide time  35  48  a non leaf node of a b plus tree is depicted in the following figure and as you can see it is quite similar to a non leaf node in a or it is quite similar to a node of a b tree that is it contains a two kinds of entities that is sub tree pointers or block pointers and key values but the only difference is that there are no data values here that is there are just key values k1 k2 etc  there are no data pointers as part of this node and of course the same set of constraints hold that is for any x between k1 and k2 the value of all the keys in that sub tree x should be greater than k1 that is the right most left key and less than k2 which is the left most right key  refer slide time  36  41  a leaf node in a b plus tree is shown in the following figure where there are no sub tree pointers because there are no children for the leaf nodes and there are only a set of keys and data pointers that is there is key one and data pointer to the record containing key one  there is key two and the data pointer pointing to record containing key two and so on and at the end of this block  there is a pointer pointing to next logical leaf node or the left logical block in this sequence therefore starting from the left most leaf node  we can access the entire database in a sorted form just by following the leaf nodes and the links to the next leaf node  refer slide time  37  37  what are the properties of leaf nodes ? keys in a leaf node have to be ordered just like the property that we saw in b trees where keys have to be ordered within a leaf node we should be able to access each leaf node in key sequence that is k1 is less than k2 less than kn if there are n different keys in a leaf node and just like the nodes in a b tree each leaf node should have at least half of its keys filled up that is a sealing of p by 2 number of keys should be filled up and all leaf nodes should be at the same level as far as the overall b plus tree is concerned  refer slide time  38  23  let us first look at how we perform search in b trees and b plus trees we are going to be looking at searches and additions and deletions in b plus trees and the corresponding algorithm for b tree can be derived in an analogous fashion in fact the algorithm for b trees are little simpler than that of b plus trees the searching algorithm over b plus trees is a generalization of binary search here it is a peary search in the sense that where p is a fan out of each block so just like binary search we go about with a given key and starting from the root node of the tree  that is given a search key k start form the root node if the key is present in the root node  in the current node itself then we are successful that is the key corresponds to sorry  from the key we are able to end up  find the corresponding leaf node from where we can find a corresponding data pointer however if the current node is a leaf node in the b plus tree and key is not present then we can be sure that the key is not available in the database itself then we return not available or else what we do is we search for the different pointers such that the key value that we are looking for is embedded between the left and the right most keys that is if we are searching for the first left  first sub tree that is p1 then our key pointer should be lying between k1 and k2 if we are searching  if we want to search any pi then we have to search or value  key value should lie between ki and ki + 1 it is a matter of terminology  the slide shows ki-1 and ki it is a matter of terminology whether how we use i minus 1 and i that is we can either say ki-1 and ki or ki and ki + 1 and we continue this search in the left sub tree in a recursive fashion by going back to step two and searching in that node and searching in a sub tree and so on  refer slide time  41  06  what about insertions ? this is the main contribution of b plus trees in the sense that we will be able to insert records while maintaining the balanced property of the trees now we should be illustrating the process of insertion with an example and we shall not be going into the exact algorithm of insertion which can be referred to in any standard text books however the illustration serves to help us clear the or help us to clarify the notion of how insertion happens within a b plus tree  the logic behind insertion in a b plus tree to begin with b plus tree starts with a single node which is the root node and which is also leaf node it has no parents and it has no children and the first key that is inserted into the database goes into the root node and because it is a leaf node  it just points to the corresponding data pointer as and when nodes fill up that is as and when more and more records are inserted and more and more keys have to be incorporated into the tree  nodes get filled up as and when they are filled up  nodes are split and this split nodes are made into children of a newly created node and the key values are split or also split correspondingly across these two nodes and the new parent node is updated accordingly and this split operation is cascaded to levels above so that we end up with just one tree starting from one root node following until the leaf nodes and in a balanced fashion  refer slide time  43  11  let us take an example to illustrate our point let p equal to 2  that is the fan out factor for just for the sake of simplicity we shall be assuming that the fan out factor is two that is each node has only 2  has at most 2 children each internal node or root node has at most two children and let us consider a sequence in which records are different keys are inserted and a possible sequence is shown in the slide here that is keys are inserted in sequence 5 8 3 7 2 9 and so on so they appear in some arbitrary sequence that their sequence need not be ordered and we can not ascribe any particular property in which keys are inserted initially when we insert five  we just have one root node and one key node and data pointer and nothing else in the tree when we insert 8  it is still just one root node that is shown in the figure here and with two key pointers 5 and 8 and 2 data pointers  two corresponding data pointers however when then next key is inserted that is key value 3 is inserted  this node overflows that because p equal to 2  we can not accommodate any more keys in this node therefore we require a spilt in this key node how do we split this node ?  refer slide time  44  47  note how nodes have been split in this slide here initially we started with a root node which was also leaf node when you spit this node  we get one more leaf node that is which is shown in right most side of this slide here and another intermediate node that is a node that points to two leaf nodes in a b plus tree node that the intermediate node has a different structure than the leaf nodes  this has to be incorporated so 5 and 8 were present in the tree and now key value 3 has to be inserted because 3 is less than 5 and note that the keys always  within any leaf nodes the keys always have to be in sorted form therefore the key 3 has to be inserted to the left of 5  to the left of this pointer called 5 therefore we get two different leaf nodes  one containing 3 and other containing 5 and 8 there is a small bug  there is a small error in this slide that is the left most node contains just the key 3 and the right most node contains the pointers 5 and 8 and the non-leaf node or the intermediate node is suitably updated so that 3 appears here that is everything less than or equal to 3 appears in this node and everything greater than 3 appears in this pointer there is nothing else to be placed here because we don ’ t have any other keys to begin therefore assume that we have got keys in the sequence 3 5 and 8  we would end up with a tree as shown in this figure here now suppose 7 has to be inserted now  7 can be inserted into this leaf node without any problem that is 7 gets inserted here and the nodes and the key values are reordered earlier we had just 8 in this node and one 7 was inserted  the key values were reordered so that the keys are always sorted and there is no overflow however the next key that is key value 2 causes another overflow that is key value 2 has to be inserted at the left most left most side here this causes causes an overflow and this overflow has to be cascaded up or has to go up the level in the insertion or in the b tree  refer slide time  47  32  therefore we get a b tree of the following form here that is both key values in the intermediate node now get filled and the intermediate node now points to three different leaf nodes the insertion of 9 that is the next key value again forms a overflow because nine has to be inserted beyond the last block here the algorithm tries to insert 9 into the last block which fails and then a new node is created  refer slide time  48  10  now  because a new node is created and we encounter another overflow this overflow happens at the level above that is because a new node is created  we need 4 different pointers however a node can accommodate only 3 different pointers here therefore we need to spit even the the node above and and introduce a new level into the tree therefore the corresponding tree that gets formed is shown in this figure here that is the next level node is also split so that a third level is created and then the keys are more or less uniformly distributed across the entire tree as you can notice here the property or the balance property of the tree is maintained as and when the insertion is happening and all leaf nodes are at the same level that is the height from the root of the tree is maintained at the same level for every leaf node in the index structure  refer slide time  49  16  so that was a brief illustration of how keys are inserted into a b plus tree and we shall not be going into the exact algorithm in order to insert keys in a b plus tree deletion of keys have to contend with an analogous problem that is the problem of underflow in insertion we had the problem of overflow and in deletion we have the problem of underflow an underflow happens when a node contains less than p by 2  floor of p by 2 keys less than or equal to floor of p by 2 keys that is note that there was a constraint that alteast half of the keys in a node has to be filled up that is more than half rather so if it contains less than half then of keys in a node then there is an underflow whenever there is a underflow a node is merged with its sibling in order to bring down number of levels in a tree we shall not be going into deletion algorithms also in detail here let us move on to the last aspect of indexing structures  namely how do we deal with index structures on multiple attributes until now we have assumed that indexing attributes are the fields on which indexes are maintained or simple indexes that is simple attributes however the indexes could sometimes be maintained on composite attributes that is a set of attributes forming a key attribute that is for example department id and employee number  to combine to form a key attribute is a composite attribute how do we maintain indexes on a composite attribute ? there are several different strategies that are variations or extensions of existing indexing structures and we shall briefly summarize some of the main techniques used for indexing multiple attributes  refer slide time  51  23  one simple way is to index multiple attributes that is maintain a sorted file  a primary index of multiple attributes is to have an ordered index on multiple attributes that is instead of sorting the file on just one attribute  we sort the file on two attributes that is sort it based on first attribute and among them sort based on second attribute which is the simplest solution possible but which can give us only a primary index as we have seen earlier that is it has to be a ordering attribute and a key attribute the second strategy for dealing with composite attributes is to use what is called as partition hashing  refer slide time  52  13  partition hashing is a hashing technique which takes a composite attribute that is n different key elements pertaining to this attribute and returns n different bucket numbers we then transform this into a single bucket address by concatenating all these bucket numbers to form the bucket address so that is another technique for dealing with composite attribute the third technique that is used especially in applications like data warehouses is to use the notion of grid files  refer slide time  52  50  a grid file is a file that establishes grid structure what is a grid structure ? we can use a grid structure when we know the entire range in which a key value can be spread upon for example if we know that the roll number of a student ranges form 01 to 150  we know the entire range of the set of all possible roll number and if you are fairly sure that the distribution of key values in this range is fairly uniform then we can split this range into several different buckets now such splitting of key ranges into buckets and combining this different buckets forms a grid structure and these grid structures forms a matrix or a hypercube which can be stored within a single file using several techniques called row major techniques  column major techniques and other techniques called space filling curves and so on  using which they can be stored within a single file  refer slide time  54  03  this slide shows such an illustration that is there are two different key values roll number and grade roll numbers are ranging from 001 to 125 and they are divided into 5 different ranges  similar 5 different buckets similarly grades are divided into 5 different ranges a b c and d and the combinations of this form a grid and each cell in the grid corresponds to a set of bucket address or a set of block addresses which contain records that satisfy both this constraints of keys so each pool or each cell corresponds to bucket pool  refer slide time  54  42  this brings us to the end of this session let us quickly summarize the main topics that we have covered in this session we covered several kinds of multi level indexes and multi level index has  index structures has several different levels and are usually organized in the form of a tree a tree contains a root node which is the entry to the multi level index structure  several leaf nodes and many internal nodes that form the tree structure and for a tree structure to be efficient  it has to be balanced and balancing or self balancing tree structures are what are implemented in dynamic multi level indexes in which we saw b trees and b plus trees and  we also insertion how insertion and deletions are handled in b plus trees lastly  we looked at some strategies by using which we can maintain index structures on multiple attributes that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 13 constraints & triggers hello and welcome to another session in database managements systems we have covered a quite a bit of ground in the explorations of dbms already we have looked into logical models of data management from the conceptual perspective  from the physical perspective and so on we also looked at physical requirements of data storage although we have only looked at very specific kinds of physical data storage and retrieval strategies for databases we also looked at few kinds of index structures using which we can efficiently access or efficiently search and retrieve for the relevant tuple or the relevant record that we are looking for in a database management system usually when we are taking about databases there is sometimes a physiological question that is raised as to whether any collection of data is a database especially perhaps in the late 90 ’ s there was this ranching debate about whether the web  the world wide web is just a large database now of course one might be tempted to believe that any collection of data is a database and some might argue that is not really true  collection of data is just a collection of data it is something like the difference between a set  a bag and a relation and so on all of them are just collections of something ’ s  a set of tuples or a bag of tuples or relation of tuples all of them are just a collection of tuples however some of them for example sets or bags impose what are called as additional constraints over this collection a bag of tuple is simply a collection of tuples and there is no constraint what so ever on what should be the tuple  what kinds of tuple should be there  what should be the size of each tuples  what should go into or what should define the attributes that make up the tuples and what about duplicates in the tuples  how many different kinds of the same  how many different numbers of the same kinds of tuples can be there and so on there are absolutely no kinds of constraints that are imposed on a bag of tuples however  when we consider a set of tuples there is an implicit constraint that is imposed that a set is a collection of a things of the same kind although not always so but usually we talk about a set as a collection of things of the same kind therefore when we say a set of tuples  we tend to believe that it is a collection of things or collection of tuples of the same kind that is all of the tuples have the same number of attributes and each attribute or each corresponding attribute of each tuple has the same domain and so on and there is also an implicit constraint that there are no repetitions in sets that is two or more tuples can not have the same value for every corresponding attribute that they contain therefore there is an implicit constraint of no duplicates that is imposed on a set of tuples and of course in order to implement this  that is in order to implement a bag of tuples and to implement a set of tuples  the kinds of programming that we have to do  the kinds of logical checks that we have to do  changes implementing a bag of tuples is the easiest we just maintain one collection of tuples using whatever data structure that we can use for maintaining collections  link list  trees  whatever arrays and so on similarly in order to implement a set of tuples we can use the same kind of data structure in order to implement a collection however we have to keep making checks so that the set property of this tuples are maintained that is we have to maintain checks that all of the tuples are of the same kind and there are no duplicates that are there in the tuples moving on  if we consider a relation of tuples  relation of tuples is also a collection of tuples just like a bag or a set  however it imposes even more constraints on the set of tuples that is we have seen some kinds of relational constraints like the entity constraint  the key constraints and referential constraints  no duplicates and so on so a relation also does not allow for duplicates in tuples it also explicitly states that all tuples have to be of the same kind that is they have adhere to the same schema that is defined by the relation and it explicitly forbids any tuple that does not adhere to the schema and there are strong constraints of about how the values of each attribute should be that is what should be the domains and how each what is the domain of each attribute and where can a value lie and so on and there are other constraints like key constraints which can uniquely define a tuple and because there are no repetitions in a relation  by default the set of all attributes of a tuple form its super key and so on and so forth in this session we are going to look at these constraints in much more detail we are going to be concentrating on the concept of constraints and an associated concept of triggers in database systems triggers are concepts that are quite prevalent in what are called as active database systems  refer slide time  07  14  and we are going to be looking at triggers in this context  in this session so let us look at what are the different kinds of constraints that are imposed or either implicitly or explicitly in a typical database management system let us define the term integrity constraints as the term implies integrity constraints are constraints that strive to enforce the integrity of the database system what is an integrity of a database system ? the integrity of a database system or a system of data elements essentially states that the set of all data elements that are stored in this system is a valid set  refer slide time  08  01  note that a valid set or validity is different from correctness validity simply states that or validity is some kind of correctness that is independent of what the user or what the application really considers semantically correct from the usage perspective for example if i state that the marks obtained by a student can range from 0 to 100 then a value of 200 for the attribute called mark is an invalid number it is not at all  it is not valid so this pertains to validity however if i enter a value called let us say 50 for a particular student it may be valid however it need not be correct  it could be an incorrect entry for that field the student may have actually obtained 90 marks whereas i would have entered 50 for the student therefore it is an incorrect value  however it is a valid value integrity constraints as part of the dbms are independent of the application programs they have no idea about what is the application context or what is the usage scenario in which this particular data element is being used therefore they can only talk about or they can only enforce validity of the database values or of the data elements stored in the database they can not obviously ensure correctness of the data that is stored in the database  refer slide time  09  47  one of the first form of key constraints in the relational model  one of the first form of constraints in the relational model is the key constraints we have seen key constraints when we talked about the relational model and also in sql but let us revisit key constraints here again for the sake of completeness  when we are talking about constraints a key constraints is a very fundamental form of relational constraint and it manages entity existence and of referential integrity what is meant by entity existence ? entity existence essentially means that we should be able to deference that is we should be able to identify each entity or each tuple in our database uniquely so that essentially means that each entity should have at least one super key using which it can be uniquely identified or it can be uniquely distinguished from the rest of tuples in the database in the relational model or in the relational algebra since a relation forbids duplicate tuples  the entire tuple in the worst case itself is the super key of the tuple  refer slide time  11  03  so the key constraints therefore states that key attributes should be unique across the relation that is there should be no two tuples having the same key attribute that is if i identify a subset of attributes as a key attribute  there should be no two tuples such that the key attributes are the same that is such that they are indistinguishable as far as the key attribute is concerned the entity constraint stipulates that the key attribute can never be null because all null attributes are the same  no matter where they occur and there is no difference between a null attribute in the name field verses a null attribute in the age field all null attributes are the same  it means there is no value associated with it the entity constraints therefore states that the primary key relation that is the relation  the subset of attributes that can uniquely identify tuples in a relation may never be null and the third kind of key constraint is a referential integrity constraint the referential integrity constraint essentially is a constraint over the foreign keys remember what a foreign is a foreign key is a set of attributes or subset of attributes within a tuple that refer to another tuple in a different relation  mainly the primary key or any key attribute of another tuple in a different relation a foreign key or the referential key integrity constraint states that a foreign key should either be null that is it should not refer to any other tuple or if it refers that is if it is not null then it should refer to an existing tuple we can not refer to a tuple that does not exist ; we can not assign a manger to a department for example which is non-existent we can not assign a manager to a non-existent department on the other hand we can say that a manager is not assigned to any department at all that is set foreign key as null which is quite acceptable as far as a referential integrity constraint is concerned  refer slide time  13  20  how do we specify key constraints using sql ? we have seen this in the session on sql  let us briefly summarize it here again note the creation of a table in sql the following slide shows a table called employee which has two key constraints  one is the employee number which is the primary key which is marked as a primary key and there is a pan number for each employee which is a secondary key that is it is given some constraints called non-null and unique and so on and there is also a foreign key that is the field called reportsto which states who is the supervisor or who is the manager to which this employee reports to so the foreign key references another employee tuple and references the field called or references the attribute called employee number in another employee tuple or in another employee of another tuple in employee relation so these are the circles that are shown in this slide depict how the key constraints are identified the first circle shows that when we stipulate that pan number is not null then we have identified that as a key constraint that is it is one of the keys by which can uniquely identify tuples in this relation the second circle shows that the employee number is a primary key which implicitly states that employee number may not be null so therefore the not null constraint for the employee number is actually spurious but nevertheless it states that employee number is another key which is used as the primary key that is which is used to  which is actually used to deference every tuple in this relation finally we identify foreign keys using the foreign key constructs that is we identify which  first of all which field in this tuple is the foreign key and it refers to which field in which other relation that is it references the relation called employee and the field called or the attribute called employee number  refer slide time  15  53  how can key constraints be enforced ? now  that we have seen some physical aspects of data storage or data management let us go inside the dbms to see how or what could be a possible means by which constrains or key constraints can be enforced a simple way of enforcing constraints is to create an index structure over the field that form the keys for example if we create any kind of index structure  let us say primary index on the declared primary key attribute then we can easily enforce the unique constraint note that primary key has to be unique  all keys have to be unique therefore whenever a new tuple is been inserted into the database system  we just use the index structure to verify whether a key attribute of this value already exist in the database system or not if it already exists then inserting a new tuple with the same value is going to violate the uniqueness constraint so in which case this can be rejected and the integrity constraint can be enforced similarly for all other attributes that is all secondary key attributes  we can maintain secondary indexes such that we can always verify whether before insertion of a tuple whether the corresponding value for that key actually exist in the database or not referential integrity similarly can be verified by using a primary index over the other table that is the over the table that is being referenced therefore whenever i insert a tuple in a relation that contains a foreign key  i first verify whether this tuple in fact really exists in other table whether the foreign key  whether the foreign key references to a tuple that actually exists in the reference table only if this is so then i have to allow the insertion of the tuple in the referencing table  refer slide time  18  03  so let us probe a little further into enforcing referential integrity sql automatically rejects whenever a tuple is being inserted into a relation containing a foreign key such that the foreign key is not null and it refers to a non-existent tuple and this rejection is performed using corresponding index structures and by default sql uses the restrict option for managing alterations or updates in tables for example if a tuple is deleted or if a tuple is asked to be deleted in the referenced table that is let us take an example of a manager working in a department if the department tuple of the corresponding department table is to be deleted then sql rejects such a deletion because there is a foreign key constraint that is from the table called manager which is referencing this tuple unless of course we use the cascade option in which case even the corresponding referencing tuple would be deleted that is the corresponding manager tuple which is referencing the department tuple would also be deleted when the department is deleted similarly sql rejects any updates to the part to tuples that can affect the foreign key constraints for example if i try to update the department id in the manager table such that it now points to a non-existent department  sql rejects this update on the other hand if i try to update the department table and change the department number such that it violates some foreign key constraints that are pointing to it that is it makes some foreign keys dangling then sql would reject such an update  refer slide time  20  21  of course updates and deletes  we can force the dbms to go ahead with the updates and deletes by using the cascade option in which case the overall referential integrity is still maintained however updates and deletes are cascaded that is every referencing tuple is also updated whenever the reference tuple is being updated  refer slide time  20  48  there are also other constructs in sql that instructs the dbms to perform in the different fashion  to act in a different fashion than mere cascade a cascade simply says that whatever changes is being made in the reference tuple make the corresponding changes in the referencing tuple that is if i change my department id from 50 to 102 change all referencing tuples that are referencing to department number 50 to department number 102 that is simple cascade however we may want to do some other operations other than simple cascade there are other options that we can use to instruct the dbms to perform differently one of such option is a set null option have a look at the relation shown in the slide here the slide creates or the relation creates a table called employee where employee number is the primary key and there is a field called name and there is a foreign key called reportsto which is the employee number of the manager or the other employee to which this particular employee reports to therefore  foreign key references employee relation and the employee number field and then in the foreign key relation or in the foreign key specification there are two other constructs the first construct says that on delete set null and then the second relation says or the second construct says that on update cascade what do these things mean ? the first thing  the first construct says that on delete set null essentially it means that if the tuple that i am referencing to is deleted then set null that is set this field to be null note that a foreign key can be null without violating referential integrity therefore this semantics of this statement means that if the person whom i am reporting to is for some reason deleted from the set of employees then set the field as null that is i am not reporting to anyone on the other hand if the person to whom i am reporting to changes his employee number let us say i am reporting to a manager with employee number 102 and employee number is changed to 150 then it says on update cascade that is cascade this new employee number to and make corresponding modifications in the foreign key construct so that referential integrity is maintained  refer slide time  23  43  sql also supports what are called as deferred constraint checks a deferred constraint check essentially says or tells the dbms that when inserting a tuple that don ’ t make a constraint check right now why do we require a deferred constraint checks ? deferred constraints checks are especially useful when we have circular constraints or circular referential integrity have a look at the figure shown in the slide here the figure shows two tables with corresponding referential integrity the table essentially states that the first table is a manager table which contains a primary key called employee number and the name of the manager and the department id that is managed by the manager the department field is a foreign key into the department relation the department relation in turn is having a primary key called department id  it has a department name and a field called manager which in turn is a foreign key onto the employee number consider that this company which is having a schema like this has recently upgraded its database system and now they have installed a new dbms and they are now porting the set of all data that they have from their old database system to the new database system now when they are porting which essentially means that they are adding these tuples in a batch mode  adding a set of tuples in a batch mode there is no guarantee that or it is very difficult to guarantee that the tuples would be added in the order in which they are required that is whenever i am adding a department id  the employee the corresponding employee id for the manager already exist and whenever i am adding a manager tuple  the corresponding department id for which he is a manager already exist this is very difficult if not impossible to sustain or maintain so in such situations it is useful to have a deferred integrity check so if we use the deferred or deferrable option in sql then the dbms defers integrity checks till the present transaction is completed we have not looked into the concept of transaction has yet however let me give a brief intuition about the notion of transaction a transaction simply is a logical unit of database operations for example if i talk about debiting and crediting between two accounts  let us say i am performing wired transfer between my account and my friend ’ s account so a transaction in this case is the set of operations that debits a set of  an amount of money from my account and credits it to my friend ’ s account this entire set of operations belongs to one semantic entity or semantic transaction so to say and either both of them have to be performed or none of them should be performed that is either both debit and credit should happen or none of them should happen it shouldn ’ t be the case that my account is debited but my friend ’ s account is not credited or the case that my friend ’ s account is credited with some money from somewhere but my account is not debited either both of them should happen or none of them should happen the deferrable option essentially defers integrity constraints that is until the end of the transaction in the middle of the transaction we might encounter a situation where the integrity is violated however that is as far as we have at the end of the transaction  we have perfect integrity that is we have maintained the integrity of the overall database system this slide shows how or what is the syntax of the deferrable construct have a look at the slide here this slide creates a table called manager which is shown in the previous slide or which was shown in the previous slide and manager has an employee number attribute which is a primary key and name attribute and a department id which is the foreign key  refer slide time  28  32  of course there is no need to separately specify foreign key if we directly say that department id references some other tuple that is department department id and we say that it is deferrable and also that initially deferred that is it is initially deferred that means to say that deferrable essentially means that it is possible or it is ok if integrity check on this referential integrity is deferred till the end of the transaction and this second one says that initially deferred means explicitly tells the dbms engine that as soon as a tuple is tuple of type manager is inserted  initially defer the referential integrity checks let us to move onto other kinds of constraints from key constraints the next kind of constraints that we are going to look at are constraints on attributes of course we have already seen a few constraints on attributes as some examples are shown in the slide here  refer slide time  29  42  the first constraint that we have seen was the not null constraint which essentially states that null value for this particular attribute is not a valid value and the second kind of constraint which is also key constraint but some sense also an attribute constraint is the unique constraint it essentially checks against duplicate values of the attribute being inserted into the relation  refer slide time  30  13  a more general form of attribute constraints in sql is what is called as the check option we can specify any general kind of attribute constraints using the check option on a relation there is an example or there are two examples shown in this slide here the first example defines a field or an attribute called gender and says that it comprises of a single character char 1 and then imposes an integrity check on the value of this field or the value of this attribute it says that check gender in m or f that is those are the only two valid characters that can be assigned for gender even though it is a single character  it is not just a single character but it is a single character in this set of two characters m and f similarly the second example shows a field called age that contains an integer of two digits which can contain an integer having two digits and there is a check that states that check that age is greater than 18 especially if you are talking about employee records and so on there might be a legal integrity constraint that is a legal minimum age for an employee of 18 years therefore that can be directly included as part of the check constraint that is check age greater than 18 note that the first constraint that is check gender in m or f is a physical constraint that is it is a constraint that is given as part of the physical world around us and the second kind of check that is used that is check age greater than 18 is the normative constraint it is a constraint that is imposed by the local laws or the set of norms which define the set of correct behavior within this system and it can change from system to system in some places probably the minimum working age is let us say 16 or in some other places it could be 21 and so on so that is a normative constraint while the formal constraint is a physical constraint  refer slide time  32  38  when you ask a kind sort of interesting question  can the attribute constraint that is can the check constraint on values of attributes be used to enforce referential integrity referential integrity as you know is a key constraint that is it is a integrity constraint across different relations however have a look at the small declaration here which might seem to say that we can actually use a check as check for enforcing referential integrity the example that shows the declaration of a field called department id and department id shown as integer having 6 digits and then there is a check here the check says that check department id in select department id from department now this is the department id of the manager tuple and when a manager tuple is being inserted there is a check that is made to see that the department id that is being inserted for the manager actually exists in the department relation which looks like or which seems to suggest that we can enforce a referential integrity using  referential integrity using check statement that is if it try to insert a tuple or if we try to insert or even modify a tuple of the manager relation such that it tries to reference to a non-existent department id  this check fails and the updates or insertion is rejected which seems to suggest that referential integrity can be enforced using check however look at the other way around what happens if a department existed when the manager tuple was inserted and then at a later point in time  the corresponding department tuple was deleted there is no way that this check constraint is now enforced because the check doesn ’ t even know that the corresponding tuple from that is being referenced in the department table is being deleted therefore because of this  because such situations can not be handled by check  we say that we can not use the check condition for enforcing referential integrity the check constraint can not only be used on just an attribute basis  it can be used on a tuple wide basis  refer slide time  35  28  that means a check constraint can actually perform checks on several attributes on a given tuple the following example shows such a situation where it says age int of 2 that is the age of an employee let us say is declared as an integer having two digits and a check is performed to see whether age is greater than 18 that is the legal age for a particular employee in a given company setting should be greater than 18 or there could be children employed by the company as long as the job type is based on talent that is job type is based on encouraging the child ’ s talent therefore you can either perform a check on the age field which says age should be greater than 18 or if age is not greater than 18  you can check a separate field or a separate attribute called job type to see whether it is set as talent in this particular tuple  refer slide time  36  39  it is possible to give names to constraints to declare in sql so that they can be referenced by their names rather than by their actual conditions this particular slide shows such an example where the check called the legal age of an employee or the minimum age of an employee  that constraint on this is given a name called legal employee that is age int of 2 and name is given called legal employee which is the constraint called check age greater than 18 or job type equal to talent  refer slide time  37  20  once we have named tuples  we can alter constraints that is once we have named constraints we can alter constraints that is we can add or delete constraints by referring to them by their names for example the following slide shows two different alter commands the first command says that alter table employee drop constraint legal employee if the particular constraint is no longer valid  we can refer to a constraint by name and then say drop it so that constraint is no longer enforced in future additions of or future updates to the table similarly this second alter statement shows how a new named constraint can be added to the table at any later point in time that is alter table manager and add constraints all works that is the constraint name is called allworks and check the department id is not null that is check to see that for all the employee manager tuples in this manager relation  the department id that is the corresponding department which they have to manage is not null that is there is no such manager who is not managing a department now what happens if there are already some fields or already some tuples in the relation whose department id ’ s are null in such a case the addition of this constraint fails that is the constraint can not be added because it can not be enforced on this table because there are already some managers whose department id ’ s are null  refer slide time  39  11  a more powerful kind of constraint on in sql is what are called as schema level constraints until now we have been looking at constraints that acted at the tuple level or at the key level ; tuple or attribute or keys or so on that is which acted on specific instances of a relation a schema level constraint on the other hand acts at the level of a schema that is which is enforced for all tuples on a database wide basis one such powerful constraint  a powerful general purpose constraint is what is called as assertions assertions are general purpose checks that can be performed on the entire database and can be enforced on the entire database for the entire time in which the database is in operation that is if i make up a specific assertion  the assertion must be true when it is made otherwise the assertion fails of course and the assertion must be true when it is made and it must be true throughout the life time of the database or until the assertion is dropped that is once an assertion is made  it will be enforced that is every modifications to the database in whatever table  in whatever tuple would be checked to see that the particular or this specific assertion is not being violated  refer slide time  40  50  this slide shows an example of an assertion an assertion can be created using the create assertion command this slide shows create assertion command that creates an assertion called nobench nobench is a name of an assertion and the assertion essentially checks that is an assertion is a check statement which checks to see that they does not exist any manager whose department id is null that is there is no manager who is on the bench so to say that means who is not assigned any department to manage and this assertion should be true when this is first asserted that is when i execute the create assertion statement  all managers in the database that are listed in the database should be associated with a department and if there are any managers who are not associated with a department then the create assertion fails the assertion can be created only when either the managers who are not assigned any departments are modified to assign departments or those tuples are deleted from the database and once an assertion is created that is once such an assertion is created it is enforced or it is maintained whenever there is a database update that is happened and any update or any addition of a manager tuple whose department id is null is rejected and this set of assertions holds for the life time of the database or until the assertion is dropped or until the assertion is deleted assertions can be dropped using the drop assertion command so we just say drop assertion nobench and then the assertion no longer holds and further on tuples violating this assertion can be inserted into the database the second kind of schema level constraints that we are going to look at are what are called as eca rules or triggers an eca rule expands to an even condition action rule  this is an integral part of an specific kinds of databases is called active databases however eca rules have been incorporated into most commercial databases as of now  that is let us say db  ibm db 2 oracle 9i or 10 g which is the latest all of them incorporate some form of eca rules an eca rule or a trigger is again a database wide constraint but which differs from an assertion in the sense that there are not always active that is they are not always enforced and eca rule is enforced or an eca rule is awakened only when certain specified events occur  refer slide time  44  08  that is note that eca stands for event condition and action so when an event occurs a corresponding eca rule is awakened the rule then performs a or the rule then performs a condition check to check whether the condition holds that is when the event condition  when the event occurs if the condition holds then the rule performs a given set of actions that is the a part in the eca rule and the action could be anything  it could be like preventing the event from proceeding or undoing the event or any other set of database update operations that could be totally unrelated to the event it could actually be something like intimating the user about the event and so on and the action in turn may generate more events which in turn could trigger more eca rules and so on there are several options that sql provides to handle eca rules we can either specify that the action part of a eca rule be executed either before the event occurs or after the event occurs that is before a particular event is going to happen  make a check for any eca rule that is waiting on this event or one could check for performing the action after the event occurs the action part of an eca rule can refer to both old and new values of data elements that are modified by the event  refer slide time  45  34  that is sql or several or most commercial dbms systems would maintain the older values of data elements that were modified by certain update events in case they trigger certain eca rules and the eca rule would want to refer to the older value of the data element for example one might specify an eca rule that says alert the user whenever the current stock price let us say falls below or falls by greater than 5 %  therefore we need to know the older and newer value of this particular stock price in order to see whether the change in one update is greater than 5 %  if that is so then the user is alerted and the action part of an eca rule can be either specified to be performed once for each modified tuple or once for all tuples that are modified during the update event that is during the database operation  refer slide time  46  49  this slide shows a particular example of an eca rule it says all the keywords are shown here in a highlighted form  the keyword to specify an eca rule is the term called create trigger so this statement creates a trigger called overdraft which has to be performed after update that is the action part of this trigger has to be performed after the update note that actions could be performed either before or after the updates  so this has to be performed after update on a relation called pre-paid and referencing each new row that is the new row that were modified has nrow and for each row that was modified  it checks whenever that is when balance that is nrow dot balance attribute is less than or equal to 0 then update that row and set block or set the attribute block equal to true  refer slide time  48  11  that is for every pre-paid account whose balance is less than or equal to 0  the account is automatically marked as blocked as part of this trigger operation what are some of the properties of eca rules ? eca rules are automatically triggered by the dbms the application program need not even be aware of the corresponding eca rules that were triggered it is the rule writers responsibility that is whoever enforces these rules or whoever wrote these rules to enforce these integrity constraint  it is the rule writers responsibility to ensure that the rules terminate and it is also the rule writers responsibility to prevent cascading triggers that is this action would in turn create another cascading action which in turn would trigger some more rules which in turn would create another cascading action triggering more rules and so on so it is a rule writers responsibility to prevent such conditions from happening a rule write can also write a cyclic rule that is rule a triggers rule b and the action of rule b triggers rule a that is rule a tries to undo something  undo an update and which in turn triggers rule b which tries to redo it back and it can obviously go into infinite rules in most dbms systems termination and infinite cycles are simply handled by seeing or just maintaining a count of the nesting level that is how many iterations have been made and if this crosses a maximum threshold then the entire set of operations including the update that triggered this infinite rules are rolled back that is they are all undone and nothing is changed and the database state is taken back to the state before the event that created this infinite looping of rules  refer slide time  50  09  the last kind of integrity checks that we are going to be looking at in an sql based database system are the notion of authorization and privileges authorization and privileges talk about which user is authorized to do what or which user enjoys what kind of privileges users in an sql based system can enjoy certain privileges that grants them certain authorization what do we mean by authorizations ? and this could mean an authorization to read a particular tuple for example is a particular or is a given user authorized to let us say look at the account details of some other user or is a given employee is authorized to look at salary details of some other employee and so on so there could be a read authorization  there could be inset authorization that is a user allowed to insert tuples into the database there is an update authorization  delete authorization  index authorization that is index essentially means that can a user be allowed to create and delete indexes on a particular table why is this security constraint  why is index important  that is why should i use authorize to create and delete indexes this is because what happens if i use a weather intentionally or unintentionally deletes the primary key or the primary index structure on a tuple then the entire database or the entire data file becomes extremely inefficient to access therefore a user used should be authorized to create or delete indexes then alternation and drop authorizations  refer slide time  52  01  what about views  what kinds of authorizations can we impose on views ? most database systems allow authorizations to be specified on views without obtaining the specific authorizations on the base table that is let us say a hr manager would have a read authorization on a hr based view of the employee records that is which contains employee salary details  perks and so on however there may not be or the manager may not enjoy read authorization on the actual employee tuple which may contain some more information like his or her personal details and so on which the manager may not be authorized to access therefore even if the corresponding authorization does not exist on the base table  it is possible to obtain certain authorizations on the view and since views are derived table  whenever a query is given through a view authorization checks should be performed before the query is answered  refer side time  53  09  the general form of providing authorizations is to use a grant command the grant command simply says that grant privilege list on a particular relation or view name to a set of users  refer slide time  53  26  this slide shows such an example which says grant insert update and select privileges on the manager table to public public means that all users that is everybody  every user in the database system  refer slide time  53  42  the privilege names used in the grant command or the same names as the sql command that is used for this privilege that is a select privilege gives an authorization for reads  an insert privilege gives an authorization for inserts and so on an update privilege gives an authorization for modification and so on and all privileges  the keyword called all privileges suppose we say grant all privileges  it means all the above privileges can be granted to the specific user in question  refer slide time  54  16  it is possible to revoke privileges that is by using the revoke command that is revoke privilege list on relation or view name from user list and of course we can use now familiar terms called either restrict or cascade restrict basically means that if i revoke a particular privilege from a set of  from a given user all the privileges that were granted by the user to other users will not be revoked  they stay in place on the other hand a cascade privilege says that if i revoke privilege x from a user and if that privilege x has been passed on or granted by the user to other users  they are also invoked in a cascading fashion  refer slide time  55  04  so that brings us to the end of this session where we have looked into the notion of constraints in the database system essentially we have seen in different kinds of constraints that a typical dbms system provides and they can be classified as either key constraints that act on the key attributes of a particular relation or they could be attribute constraints which can act on any attribute which checks domains or validity of the attribute value with reference to its domain in any given relation and there are tuple constraints which act on the complete tuple itself that is where the value of certain attribute may depend on the value of other attributes on a tuple wide basis and so on and then there are schema level constraints especially assertions which are a general purpose schema level constraint which maintain certain integrity over the entire database for the lifetime of the database or until the assertions are dropped and finally we looked at triggers or eca rules that are not valid always but are awakened whenever certain events happen and a certain condition holds which in turn prompts them to perform certain actions we finally then looked into the notion of privileges and authorization that different users in a database system can hold and they could be granted certain privileges and privileges could be revoked from them and this can happen in a restricted fashion or in a cascading fashion so that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 14 query processing and optimization hello and welcome to another session in database management systems in one of a previous sessions especially when we are talking about a storage structures and index structures  we talked about what is the biggest challenge facing databases yesterday the challenge is no longer the problem of storing data or of designing that can store larger and larger amount of data in fact we have very small devices today that can store huge amounts of data  devices that you can probably put in your pockets are something which you can wear inside your watches and so on  which can store something like 2 giga bytes of data therefore the problem today is not primarily of storage of data  storage of data has become much more  the surface area required for storing data has become  has shrunk in tremendous proportions and the cost of storing data is also fallen tremendously over the years however this fall in cost or this affordability of massive amounts of data storage has resulted in a new problem or a new challenge the challenge is that of retrieval of data  how efficiently can we retrieve data among the huge amounts of data that we have stored we also saw how the definition of very large databases has been changing over the years today in  10 years ago very large databases would probably have meant hundreds of megabytes of data  probably of few giga bytes of data however today when we talk about very large databases  we are talking about databases that are easily into peta bytes of data 10 power 15 bytes of data so we saw  we had mentioned these things when we are trying to motivate the use of index structures or the auxiliary files those are files containing metadata that can quickly point to or that can help the application program or the query to quickly retrieve the required data elements form the database there is however another aspect of the story using index structures alone does not help in efficient retrieval the other crucial element in effective retrieval of data and making a data or making the difference between usability and unusablity of a database is a query execution strategies or the query execution and optimization strategies this is the topic of this session and the next few sessions that we are going to consider query execution and optimization  it goes without saying is an extremely important aspect of database management and this is what is going to determine whether a particular query is going to be useful at all or not if a particular query execution strategy takes enormous amounts of time to retrieve a particular  to retrieve a given data element that can make the difference between whether the query is interactive in nature or whether the query is batch in nature that is whether you have to  whether the database will have to say that please come back after two days for your query results and so on so let us look into what makes up a query execution and query processing and optimization  refer slide time  05  07  query processing  as we said before effective query processing or efficient query processing is crucial for good or even effective operations of database a database can be rendered unusable if a good query execution strategies are not used let us do a quick calculation suppose i have 1 giga bytes of data and 1 giga byte as you know is 1000 megabytes of data even it is safe to assume that for most of the server class pc ’ s today  we have data transfer rates of something like 1 megabytes per second it is usually 1 mega bit per second  let us consider that it is 1 megabyte per second or 8 megabits per second so this  if i have to access or if i have to scan through a relation let us say i have a query that select query which requires me to scan through the entire relation of 1 giga bytes of data that means it would take about thousand seconds for me to scan the entire relation because it is of a one giga bytes of data now consider a query which is given on two different tables  each of one giga byte of data so there are two giga bytes of data that is there in the database now a bad query execution plan would actually try to compute a cartesian product of the two tables before trying to return the results that we required now if i have to compute the cartesian product of 1 giga bytes of data times 1 giga bytes of data where each axis of the table is going to take me 1000 seconds then it is going to easily take me 10 power 6 that is one million seconds just to compute this cartesian product which is clearly unusable and which is clearly ineffective as far as an interactive response time is concerned therefore efficient query execution or trying to rehash a given query in a more effective form for example in this  in the example that we just took up  the query might be able to figure out that what the user really wants is a natural join for example or some kind of an equi join rather than a cartesian product if it is able to figure that out then probably you would get a response in a little more than a 1000 seconds which is much better than a million seconds for query execution and query processing depends on a variety of factors and not all of these factors are under the control of dbms what are this factors that can affect query execution ? let us take a few examples one of the factors that affect query execution is for example whether the storage media is fragmented or defragmented if the storage media is let us say fragmented  if you remember what is meant by fragmented storage  in a fragmented storage contiguous block on the storage media belong to different files now if i have to access a relation if i have to scan through a relation  let us say in response to a select statement and these blocks are divided or distributed all across the storage medium then the response time would be increased considerably for this query so  not all query execution or not all factors that affect queries or query execution times or under the control of the dbms the example that we took right now is not purely outside the control of the dbms also this is because several different  several kinds of database management systems would override the operating system mechanisms and then start to deal with devices directly for example they create their own file system that can ensure that the file system is never fragmented at all and so on therefore some high end database management systems would try to overrule the underlying operating system and try to access the hardware directly in order to speed up query execution and speed up or decrease response times and as we saw earlier insufficient or incorrect information about factors that can  that affect query plans can lead to vastly ineffective queries for example if a query execution plan estimates that the size of a table is let us say few kilobytes but the size of the table is actually a few giga bytes then whatever execution plan that it uses for a few kilobytes will not work for the few giga bytes table because the entire strategy changes when the scale of the problem changes a few megabytes can probably be or few kilobytes can probably be stored in main memory whereas a few giga bytes can not be stored in main memory also and query executions usually use what is called as catalogs we shall be looking into catalogs in more detail in a later session they use what is called as query cataloging that help in estimating several factors or several kinds of information about the database this can include the size in terms of bytes of a particular table  the size in terms of tuples or the number of tuples in a table and estimate of the number of tuples and an estimate of the number of distinct values in a tuple that can help in building indexes for example query catalogs hence play a very crucial role in deciding the query execution plan that is ultimately used on the database management system how does a typical process or query execution process or query processing  process look like ? the typical steps in a query execution process is quite similar to the execution steps in typical compiler  the way a compiler complies a given high level language construct into machine language and executes it  refer slide time  12  08  we start with the user description of the sql query the sql query is then read and parsed that is the sql query is then read by a query complier that performs scanning where lexical analysis is performed that is the sql query is read character by character and then tokens out of the characters are recognized and then a token stream is given to the parser which in turn parses the query  constructs a syntax tree and then the tree is validated for semantic checks for types and interoperability and so on and once this is done  an intermediate form of the query is generated and which is called the logical query plan tree this intermediate form of the query is usually a relational algebra representation of the sql query now once this intermediate query tree is generated  a series of heuristics and cost based searches are used to rewrite this tree or rewrite this query execution tree in order to make it more optimal or in order to make it in order to use a better equivalent query for whatever execution  whatever query has been requested by the user  refer slide time  13  33  this intermediate query representation is then given to the query optimizer which in turn generates the query execution plan that is it rewrites the tree in order to reorder few of the operations and then a final physical query plan tree is created we shall be looking into a query optimization strategies in a later session however there are optimization strategies can be broadly divided into either a heuristics based optimization strategy which are essentially some kinds of thumb rules which have been known to yield better strategies for query execution and then there are what are called as cost based optimization strategies where an estimate of the cost that is required to execute one query plan against the other is used in order to utilize the best  potentially the best query execution plan the physically query execution plan is written in a separate language not necessarily the machine language but there is a separate language that uses its own construct or its own algebra for representing what are called as internal queries that is the query that are actually performed on the storage structures on the physical files that are stored onto disks the query execution plan is then given to the query code generator which either executes the query as it is that is starts giving results directly which is called the interpreted mode of query execution or it generates machine code which is called the complied mode of query execution which can then be used to actually perform the physical operations required to answer the query  refer slide time  15  .31  the code  the machine code that is generated in a compiled mode of query execution is then given to the run time database processor which executes the code and returns the query results  refer slide time  15  49  in these steps of query executions  two aspects are important and interesting these are the intermediate form of the query and the physical query plan the intermediate form of the query as we mentioned before is usually a relation algebra equivalent of the sql query that the user has given the intermediate form of the query is in the form of tree structure which is also called as an expression tree where relational algebra operators are on the non-leaf nodes and the actual domains form the leaf nodes that is the actual relations on which query is executed from the leaf nodes the tree is then rewritten based on a set of rules that are derived from either heuristics or cost based optimization to generate an equivalent tree which produces the same query but preferably in or hopefully in much lesser time with much lesser overheads the physical query plan is written in a separate language which has its own construct and that is either interpreted or complied into machine code let us have a look at what constitutes the physical query execution plans and what are the constructs that make up this physical query execution plan the logical form or the logical query execution plan or the intermediate form  we shall be looking into greater detail in later session  refer slide time  17  24  the physical query plan comprises of a basic set of operators that define the language of physical query execution now what should this operators be ? obviously the operators at the lower level or the inner query or the internal query should contain all the operators of relational algebra itself that is if the relational algebra says select on this condition  the internal language should also be able to support an operator that can select a particular tuple based on a set of tuples on this condition however in addition to the relational algebra operators  there are several other operators which talk about a physically accessing tables and iterating through them or several other physical operations note that relational algebra itself does not concern itself with the physical implementation of the database  refer slide time  18  23  let us have a look at a few candidate physical query plan operators and see how they work that gives us a flavor of how does the physical code actually look like or the query execution code look like the first operator that you are going to see is the table scan operator a table scan operator as the name suggests just simply scans a particular given table that is it scans and returns an entire relation r or the operator can be parameterized in the sense that you can give certain conditions to the table scan operator that scans a given relation r and returns only those tuples that satisfy the given condition or the given predicate the main operations that are performed by table scan is to read all blocks note that blocks is the physical component in terms of which the records are stored so a table scan contains a code or the operator for table scan contains code by which blocks belonging to a particular file or a particular table are read in sequence and the block is and the table is returned there is also an index scan operator that makes use of an index file in order to read all blocks of a data file in sequence  refer slide time  19  50  another operator that is used usually in the physical query plan language is a sort scan operator the sort scan operator scans a relation r and sorts these results before returning it to the higher level whichever called it if the relation is already stored in a sorted form and the sorting is also required on the same ordering attribute  no sorting needs to be done separately by the sort scan operator and if the relation is small enough to fit in memory then sorting can be done directly in memory however if the relation is too big to fit in memory then external sorting and external sort merge techniques have to be used in order to sort the given record  refer slide time  20  41  the next physical query plan operator that is quite frequently used is what is known as the iterator iterator is an important concept in managing or in the physical management of data records if you have probably let us say programmed in a  done programming c plus plus let us say using the standard template libraries on unix environments or the active template libraries on microsoft environments  you would have come across the term iterator in several places what does iterator do ? the iterator is an operator that functions on an operand which is a composite operand for example an iterator operates on a hash table or a linked list or a tree or something like that  so the iterator operates in a way that iterates through each element that forms the composite operand that is it starts from the first element and it comprises of getnext function which can get you the next element  until you reach the end of the operator or the data element so iterators typically contain three different functions as shown in this slide here the first function open will open the iterator object that is the composite object on which the iterator function has to be performed the next function called getnext is going to get the next logical block that or the next logical record or next logical node or whatever it is in this composite object that has to be returned and then the last operator called close  closes control on the object  refer slide time  22  35  this slide shows an example of the iterator function  example of a table scan iterator that is how we can implement the table scan operator using an iterator as the slide shows there are three different functions that have to be implemented open  getnext and close which is shown in the next slide here the open function let us say given relation or a given file let us consider relation to be stored in a file or a table to be stored in a file  the open construct initializes two variables  a variable called b which points to the first block of the relation and a variable called t which points to the first tuple in inside b the getnext function just iterates through this variables that is before we go in to getnext function  let us try to ask ourselves what should the getnext function do ultimately for the program which is calling the table scan iterator  all that is required is the set of tuples one after the other the getnext function however has to deal with two different  two different things the blocks that is the physical data stores and the tuples that is the logical data stores now tuples can be iterated within blocks but when a block is exhausted  the blocks themselves have to be iterated across the files that is the next logical block in the file has to be chosen so the getnext function performs precisely this set of function that is if t is beyond the last tuple in b  that is if the current block b has been exhausted then we increment b that is point b to the next logical block in this sequence in the file and if b is beyond the last block in the file then you return no more data that is it is exhausted in the record or the file or else the else condition here states  is basically t is beyond the last tuple in b but b is not beyond the last block in the file that means set t to the next or set t to the first tuple in b that is the next b is been incremented and set t to the first tuple in b or the new block  refer slide time  25  10  and then increment t and return the old value of t which t was pointing to so return  that is we first assign oldt equal to t and then increment t and then return oldt for close we don ’ t have to do anything because we have already returned  we already returned from the getnext operator if we have reached end of the file therefore close in this example is redundant  however usually the close function performs some kinds of a cleanup operation where if some data structures were opened during the course of the iterator function  these data structures are closed and the corresponding memories freed and so on let us implement or let us look at an example of the iterator function we shall implement the table scan operator that we saw earlier using the iterator function  refer slide time  26  16  as we saw before an iterator has three different functions the open  getnext and close assume that the table is implemented or is contained within a file and the file is organized as a sequence of blocks that is there are several blocks that make up the file so when we open the iterator that is open the table scan iterator  we need to initialize a few things this is shown in the slide here  that is the slide  the open function initializes two different variables b and t where b points to the first block in the file of the record and t points to the first tuple in the block the getnext function  note what the getnext function should return here the table scan operator should return tuples after tuples that is the first tuple  second tuple and so on however at the physical level  we are concerned not only with tuples but also with blocks that is we actually read and write in terms of blocks and in not in terms of tuples therefore initially what we do is we first check to see if the tuple is beyond the last tuple in b if this is the case we have to increment b and if b is beyond the last block in the file then we just return no more data that is we say that is all  there is no more tuples to return or else that is the else is for the inner if  so else t is then set to the first tuple in the next block that is we have incremented b and then we just set t to the first tuple in the next block and if none of these is the case then we just return the next tuple that means we first copy the corresponding tuple to be returned into a new variable which is called oldt in this example and then we increment t and then return oldt  refer slide time  28  10  for the close function in this example  we don ’ t have to do anything because we have already returned in the getnext function when we have reached the end of the file but usually in a close function  we use the close function to clean up whatever mess we have created so to say that is whatever data structures that we have opened  whatever memory we have allocated which are not useful anymore have to freed up and so on so the close operator or the close function is called at the end of the iterator which performs all this cleanup operations let us look at another example using iterators on how to compute the bag union of two relations what is the bag union ?  refer slide time  28  58  remember we have talked about a considering relation as bags rather than sets a bag is just a collection of tuples or collection of elements without regard to whether there are duplicates in the collection so it is also called a multi set a multi set union or a bag union is simply a bag that is made of two different bags that is you just empty contents of one bag into the other and you have got a multi bag union or union over bags so this is denoted by the operator plus or the disjoint union operator r plus s so an iterator for performing the disjoint union  here we are considering that both r and s which represent relations for us are now in the form of iterators themselves that is we are abstracted away a relation form being a file to being an iterator that is we know it is just a data structure which we can open and call the getnext function and then close once we are done with the data structures so as far as we are concerned  both relations are just iterators so in the open function of our disjoint union iterator  we just open one of the relations we say r.open and then we point the current relation to be r in the getnext function we say that if current relation equal to r then we have to call getnext on the current relation that is we just say current relation dot getnext and if getnext returns no more data that is if there is no more data that is returned then start or set current relation as s and then call s.open and then in the subsequent getnext operations  you just call s.getnext instead of r.getnext  refer slide time  30  53  so what we have effectively done is we have exhausted one of the records by calling getnext as many times as possible that is whenever getnext is called on us  we call getnext on the current rel that is the currel relation so once we exhausted one of the relations  we open the other relation and start calling getnext on that function so when s is exhausted it returns no more data which is what should be returned by the getnext as well and in the close function we just close both of these iterators that is we call r.close and s.close well  r.close and s.close don ’ t do anything which we saw in the previous example but in case they do certain cleanup operation it is always a good measure to or it is always a good programming practice to call the corresponding close operators in our close operator  refer slide time  32  13  so we just went through some of the elements of the physical query plan programming language that is it contains elements of table scan  index scans and iterators and so on now let us have a look at some algorithms that are built around these data structures or around this constructs of the physical query plan that can help us in understanding how a given relational algebra operator  let us say like select or project or something of that sort is actually executed inside the database system we can broadly divide algorithms for a data access into one of the three following categories we call them sorting based methods  hash based methods and index based methods these methods as you can see here are typically meant or oriented towards increasing the effectiveness of search in a sorting based method the relations that are scanned using the sort scan operator that is they are sorted as in when they are scanned and because they are sorted the property that the relations are sorted would help in performing certain other relational operators like say join in an efficient fashion similarly hash based methods use some kind of a hash function to quickly search for whatever tuple or data element that is being asked for within this relations an index base methods resort to index structures like trees or balance trees and so on for searching the required data element we can also divide algorithms for data access based on what kinds of data access requirements that they pose we can divide the kinds of data access requirements into one of these three kinds of requirements the first requirement is what is called as a tuple at a time unary operator that means the query requires or requires to contend with one tuple at time for example select and project operator every time select is called  select has to be or the condition for select has to be checked against each tuple in the relation that is tuple after tuple so at a time one tuple is being accessed and this is a unary operator that is it is just one relation on which a particular tuple is being accessed then there are full relation unary operators where the entire relation has to be searched for example if i have to return something based on or return the value of some relation or if i have to compute let us say set theoretic operations like not of something and so on or any kind of set theoretic operation that are unary in nature and the last kind of operations are full relation binary operators these are operators that again have to compute or that again requires a complete relation as their query result and they are not just unary  they are binary that means they have two relations to contend with  refer slide time  35  52  that is some examples or something like set theoretic operators like union  intersection and so on which require two relations and the entire relation has to be scanned the entire relation has to be returned  refer slide time  36  10  let us see how each of these kinds of query execution requirements can be met using some algorithmic strategies in this session we are going to be looking at a kind of strategies what are called as one pass algorithms what is a one pass algorithm ? a one pass algorithm is an algorithm that performs at most one pass over the entire database that is over the entire relation of interest it does not access the relation multiple times very important and many times limiting assumption in most of the one pass algorithms that we are going to see here is that it assumes that the relation that we are looking for is small enough to fit in main memory in many cases this is the reasonable assumptions but in many other cases  it is not a reasonable assumption that is even a single relation could be so huge that it may not fit into main memory so how does  how can we use a single pass or a one pass algorithm to perform a tuple at a time unary operation ? let us take some example like select or project as shown in the slide here let us say select some condition over r or project some condition over r all we have to do is scan through this r that is use the table scan iterator for scanning through this relation tuple after tuple and store this relations or store this tuples that have been scanned in a input buffer  perform a unary operator and output it to the output buffer so this is schematically shown in the diagram here that is this is the relation iterator and this relation iterator returns tuple after tuple which goes into the input buffer and in this case this input buffer can be as small as one tuple long that is we can allocate just enough memory in the input buffer to store just one tuple so each tuple  after tuple is put into the input buffer and checked against the unary operator and either discarded or sent to the output buffer  so as simple as that this is quite simple that is within a simple single pass  we have been able to answer a tuple at a time unary operation  refer slide time  38  49  what about relation at a time unary operations ? what are some examples of relation at a time unary operators ? one example is that of let us say the unique function in the select  in the sql statement suppose i say select name from employee or select unique name from employee that means given me the set of all unique employee name ’ s without repetitions this as you can see is a unary operator that means it operates on just one relation however it is a relation at a time operator that is it requires to have the entire relation  knowledge about the entire relation before being able to return the required value so the general strategy or a general one pass algorithmic strategy for relation at a time unary operators is shown in the slide here r is now familiar table scan iterator which returns to a tuple after tuple which goes into the input buffer now the input buffer is then read into the unary operator whatever be the unary operator  whether it ’ s unique or group by  for example group by is another relation at a time unique operator now this unique operator will either output this tuple into the output buffer  if it is safe to do so or otherwise will put the tuple into a data structure holding the history of whatever relation has been read until now for example in the unique operator  all we need to do here is have a hash table that contains one entry each for each unique entry that we have found until now in the database so whenever i read a new name  let us say whenever we read a new tuple into the input buffer and check out the name attribute  we just check the hash table here the data structure holding history we just check the hash table to see if this name was already encountered  if you are already encountered this thing if you have already encountered this name then we just discard this new tuple otherwise we add this new name into this hash table here and then output the tuple so a single pass algorithm for a relation at a time is also quite simple except that we need to have an augmenting data structure in the form of usually an index tree or a hash table or something like that can hold the history that is required now one more thing that is to be noted here is that suppose the unary operator that we are concerned with is the groupby operator now the groupby operator can not return any output until the entire set of relations or the entire set of tuples in this relation is read and the performing grouping is formed using this data structure that means this space allocated to this data structure should be large enough to hold the entire relation therefore such an algorithm can not be used for relations that are too large to fit in memory because we are concerned only with one pass algorithms in this session here we assume that the relation can be held in memory so that the entire relation or the entire history of what we have read can be held in the data structure  refer slide time  42  35  let us look at one pass algorithms for relation at a time binary operators now one pass algorithmic strategy is vary depending upon on what is the binary operator that we are looking and almost all of the algorithms for binary operators require that at least one of the relation be read completely into memory before we start reading the other relation and obviously if we have two relations and one is much smaller than the other  it makes much more sense to read the smaller relation into memory and iterate over the larger relation so let us look at a few examples and which will make this clear  refer slide time  43  19  let us see what is the strategy what is a one pass algorithmic strategy for computing the set union of two relations r union s i have explicitly used the word set union here instead of just union that means this is not a bag union that means to say that we have to compute r union s without returning any duplicate entries in the result that means we have to remove all duplicates while returning r union s assuming that among r and s  r is the bigger relation  here is a very simple strategy to compute r union s first read s into memory completely using the iterator on s  retrieve all tuples from s and place it into memory and place it in some kind of data structure like an index or hash table by which we can access each tuple of s as efficient of as possible now as and when we are reading s  keep outputting the tuples of s because anyway r union s should contain all tuples of s then once s is completely read into to memory and indexed in a data structure  start reading r that is the next relation and for each tuple of r that is read into memory  check whether it already exist in s if the tuple already exist in s then just discard the tuple because we do not want duplicates in the output result otherwise if it does not exist in r then or if does not exist already in the relation then just output the tuple now here we are also making another implicit assumption that r and s are sets themselves and they are not multi set that means there are no duplicate tuples in r itself therefore it is sufficient for us to check for duplicates against s  otherwise we need to also store tuples in r so as to check the duplicates within r itself if we assume that r and s are sets  the set union operator can be performed using the strategy that we outline just now the next binary operator that we are going to look at is the intersection operator the strategy for the set intersection operator is also quite similar to that of the union operator  refer slide time  45  56  assume that we have to perform the set intersection between r intersection s and assuming that r is the bigger relation  we first read s into memory and then store tuples of s in an in-memory data structure or in memory index or hash tables structure that can help us access the data elements of s quite efficiently then start reading r into the memory tuple by tuple using the iterator for r  then for each tuple of r if and only if the tuple also exist in s  output the tuple of r in to the output buffer otherwise discard the tuple  refer slide time  46  38  what about set difference ? set difference if you see differs depending on whether we are computing r minus s or s minus r because set difference is not a commutative operation now suppose let us say without loss of generality  let us say we are computing r minus s and that r is the bigger relation so we are computing r minus s and the first relation r is the bigger among the two relations that means we read s into memory as usual that is read the relation s using the s iterator into memory and put s into an in-memory index structure or a hash structure and for each tuple of r check to see whether it already exists in s if it already exist in s then discard the tuple or if it does not exist in s then output the tuple as simple as that but what happens if we compute s minus r  that is r is the bigger relation and it is right hand side of the difference that is instead of computing r minus s  we are computing s minus r because r is the bigger relation it is always more efficient to read s into memory rather than r now if we read s into memory  how does the algorithm change ? let us have a look at that so this slide shows set difference s minus r instead of r minus s and assuming that r is the bigger relation now if r is the bigger relation  have a look at the steps closely for this slide here if r is a bigger relation  all we have to do is first read s into memory completely that is read the complete  use a s iterator and read all tuples of s into memory and place them into an index structure or a hash table now for each tuple of r  what should we do ? that is we are computing s minus r that is s minus r is the set of all tuples in s that are not in r so for each tuple of r  check to see if it already exists in s and now what happens if it already exists in s ? if it already exist in s then these tuples should not be there in the final output and the tuples that should be in the final output are those tuples of s that are not in r so what we do here is for each tuple of r  for which we find a matching tuple in s we cancel them out that is we delete the tuple in s from the index structure that is we delete it from the index structure or the hash table that we have been using now once r is exhausted that is once we have finished reading through the relation r and we have deleted all common tuples  whatever is left in the s data structure that we have read into memory is the output that is that we can push them onto the output buffer  refer slide time  49  47  what about cross product ? r times s cross product is simple as far as the algorithm is concerned and quite expensive as far as the performance is concerned that is again assuming that r is the bigger relation just read s into memory and we don ’ t need any data structure here  we can just store s in its contiguous sequence of memory locations and for each tuple of r combined with every tuple of s and start returning it  as simple as that  refer slide time  50  18  the last one that we are going to be looking at is a one pass algorithm for natural join what is a natural join ? a natural join is an equi join on two relations that equates attributes having the same name and domain now assume that r  x  y and s  y  z are being subjected to a natural join that means y is the common set of attributes or subset of attributes between r and s now assuming again r is the bigger relation  read s completely into memory and then index or place x in a hash table or an index  so that it can be searched efficiently now for each tuple of r what we have to do is search through the hash now  this hash table or the indexing structure should be done based on the common attributes that is based on y here  even if y is not the key attribute so  we perform the indexing or the hashing based on the common set of attributes y and then using which we can search for every tuple of r that is read whether there is a matching tuple in s if there is a matching tuple  match the two tuples and output it to the output buffer if not just discard the tuple of r  refer slide time  51  44  so that was a brief overview of the one pass algorithms that can be used by using the physical query plan operators like say iterator and sort  table scan  sort scan  etc using which we can develop a strategy for performing relational algebra operations like r union s  r intersection s  select  project  unique  groupby and so on but what are the constraints of one pass algorithms ? one pass algorithms are applicable especially for binary relation at a time binary operators they are applicable only when one of relation can fit completely into memory and it is not just it fits completely into memory  we should also have extra blocks of memory for the other relation that is let us say if i have m memory blocks available for me  then the smaller relation can be at most m minus 1 in size  m minus 1 blocks in size it can ’ t be m in size because we need at least one more block to perform book keeping for the other relation that we are reading from disc  refer slide time  53  00  and one pass algorithms rely to a great extent on correctly estimating the size of relation if a query execution plan in a dbms engine decides to use a one pass algorithm for performing a particular query  relational query  relational algebra operation then it depends for a large part large part on the estimate of the size of the relation that it has now if the size is wrongly estimated then it can for example if we allocate two few buffers thinking that the size would be small then the one pass algorithms will be unusable  the query execution plan is unusable  we can ’ t use the one pass algorithms at all on the other hand if we allocate too many buffers thinking that the relation size is big then we may end up in a possibility of thrashing where memory blocks have to be swapped on to disk and so on so it is quite crucial to obtain a good estimation in order to use one pass query execution plans  refer slide time  54  15  so let us summarize what we have learnt in this session we have kind of a scratch the surface of an important and crucial area of data base management systems called a query processing and optimization we have seen the different stages in query processing and two important intermediate steps in query processing namely  the logical query plan and the physical query plan and in this session we have concentrated more on the physical query plan that is a physical query plan is a set of language constructs that perform low level operations using  that performs low level operations directly on the storage media that can physically access files into memory for performing any data base related operations we also saw what a physical query plan language would look like or rather what kinds of constructs it would have it would obviously have relational constructs  in addition it would have constructs like iterators  tables scans  sort scans  etc we have looked at the variety of one pass algorithms using these physical query plan constructors using which we can perform a variety of relational algebra operations like select  project  unique  groupby and many other set theoretic operations in the next session we shall be looking into the big question of handling joins in query execution plans and also have a look at the logical query execution plans so this brings us to the end of this session thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 15 query processing and optimization – ii hello and welcome to yet another session in database management systems in the previous session we started looking at a very crucial aspect of dbms design note that i am using the term dbms design and not database design we will look at  we will address this issue in more detail a little later  what is dbms design verses what is database design we started by looking into a very crucial aspect of dbms design namely that of query processing we saw that nowadays or in today ’ s world  the size of the database is no longer a problem we can have huge amounts of data stored in very small amounts of space and being available at pretty cheap cost in fact i have with me a small device which i can show it for you this is the small device which can hold a database of size 256 megabytes of store and this cost roughly about 3000 rupees so you can store data of the order which was not even envisaged before in very small devices at pretty affordable cost and which occupies very small amount of space therefore storage of data is no longer a problem  the problem today is retrieval of data retrieval in the sense it is not just retrieving any data  it is retrieving whatever data that is required by the user therefore we saw the crucial elements today or the crucial technologies of that is going to impact database in the forth coming years are those techniques that can help in retrieving data elements as quickly as possible from databases we saw that auxiliary files in the physical storage world which comprises of index structures and several kinds of hash structures and so on is one crucial element in making this happen  that is making fast data retrieval happen the second crucial element is query processing  given a users query can we process the query in such a way as to return the results extremely fast there are several techniques that are used for making query processing interactive in nature that is remember the example of google if suppose google were to say that when you give a web search query  suppose it were to say come back after two days for your answer it becomes unusable  nobody would use it if the other alternative that is but a search engine like google can search peta bytes of data or the search space of data is peta bytes of data and it can return you relevant results within a few seconds now this is possible  one of the techniques for this is to use very intelligent query processing techniques of course there are  if you throw more hardware into your database  if you throw faster processors it becomes faster and so on however a crucial element is still the algorithm that is used to retrieve data a bad algorithm can make the difference between a database that is very efficient to a database that is unusable for all practical purposes let us briefly review what we learnt in the previous session in query processing and move on further today to look at some more aspects of query processing efficient query processing is crucial for good dbms design and in fact it can make the difference between a database that is operational or effective and database that is ineffective  refer slide time  05  14  a bad query processor can render a database all but useless when the size of the database grows beyond a certain size  beyond a certain limit query processing depends on a variety of factors unfortunately and not everything is under the control of the dbms we saw yesterday that query processing could depend on let us say the memory size or the size of the tuples or the size of the relations that are present in the database and the kind of file system that are stored and whether the file system is fragmented or defragmented and so on so there are there are variety of factors that impact query performance and not all of these factors are within the preview or within the control of the dbms therefore query processing is oriented towards obtaining the best out of whatever is available at the biggest or at the control of the dbms  refer slide time  06  20  what are the typical steps that are handled or that are executed in a query execution process given a query  given an sql query  the query is fist scanned using a lexical analyzer which in turn takes the string which forms a query and returns a set of tokens and then these token stream is then passed to build a syntactic tree or what is called as a parse tree and then syntax analysis and so on semantics checks and etc are all performed which comprises the validating phase of the query complier and once this is done an intermediate query representation is generated this intermediate query representation is also termed as a logical query plan tree now this logical query plan tree is usually a tree data structure that represents the relational algebra equivalent of the sql query using this tree data structure  there are several rules that are employed in order to systematically optimize the tree for better performance the intermediate query representation or the query tree is then given to the query optimizer which generates a query execution plan the execution plan is also called the physical query plan tree and in contrast to the logical query plan tree that means a physical query plan tree comprises of a query plan written in a language that is usually executed by a dbms interpreter or would be complied into machine code for execution this physical query plan language comprises of its own constructs some of which we saw in the previous sessions something like table scan constructs and sort scan constructs and iterators and so on  in addition to all relational algebra constructs like select  project  set union  difference and so on  refer slide time  08  30  the query execution plan if it is an interpreted dbms would be executed directly and the results would be returned to the user if it is a complied dbms engine  the query execution plan is then given to the query code generator and then appropriate machine code is generated from the execution plan  refer slide time  08  53  of course the last step would be the code if it is generated in a complied mode  it is then given to the operating system run time or the database runtime and then query results are given  refer slide time  09  10  now we are primarily interested when we are taking about query processing in two of these stages in the query execution stages the first stage that we are interested in is the intermediate form of query representation in fact that is something that we have deferred to a later section we in fact went straight onto the second stage that is a physical query plan representation we saw some constructs of the language that is used to describe the physical query execution plan and constructs that are or the physical execution plan is then either fed through an interpreter for this language which executes the query or is compiled onto machine code now what are some of the operators that we saw yesterday ? the physical query execution plan operators are those set of operators that define the basic physical operations that need to be performed in order to answer a query these are also sometimes called the internal query operators or the operators that make up the internal query language sql or relational algebra on other hand is an external query language that is it is the query language used by the users or application programs that are using the database but the database itself or the dbms itself uses an internal language for answering queries that accesses the file system or the storage structure used by the dbms in order to retrieve tuples in response to a query this of course comprises of the set of all relational operators plus some additional operators that talk about physical characteristics of retrieval some example operators that we saw yesterday was the table scan operator which can be implemented using an iterator operator that is an iterator objects so to say which opens a table and returns a tuple by tuple that is an iterator if you remember has three different functions an open function  a getnext function and a close function a table scan operator opens a table when the open function is called and returns the next or the top most tuple on every getindex or getnext invocation and starts incrementing a pointer so that the next tuple is returned and so on so the i th getnext invocation would return us the i th tuple in the table and then the close function would close the table and then there was index scan that would use an index to get a particular or get the required tuple then there is a sort scan operator which not only returns all tuples in a table but also sorts them before returning them we then saw some set of one parse algorithms that are built on top of these physical query plan constructs or data structures that perform various operations  refer slide time  12  22  we divided one pass algorithms into three different kinds of categories the first category was tuple at a time algorithm a tuple at a time algorithm is something like select or project which needs to concern or which needs to be concerned only with one tuple at any given point in time tuple at a time algorithms can easily be executed with a single parse regardless of the size of the relation because we assume that the tuple is never large enough so that it can not fit into main memory we always assume that one tuple of the database can always fit into main memories therefore since we are only concerned with single tuple at a time  we can always use a single parse algorithm for tuple at a time functions on the other hand there are relations at a time algorithms that is there are functions that need to be concerned with the entire relation rather than each tuples independently for example the function called unique in sql needs to look at the entire relation in its entirety of course rather than each individual tuple in isolation relation at a time functions can use one parse algorithms only if the relation can fit completely into memory and that ’ s not enough  in fact the relation should not only fit completely into memory but especially for binary relation at a time functions  there should be at least one block left over to read data from the other relation hence if a relation requires m memory buffers or rather if m memory buffers are present for the dbms then the maximum size of one of the relations of the smaller relations can be at most m minus 1 it can ’ t be m  because we need at least one more block for data from the other relations  refer slide time  14  36  and of course that is the limitation of the one parse algorithm in the sense that you can use them only when you know that at least one of the relations can fit completely into memory now how do you know that a relation completely fits into memory ? what if you don ’ t know the size of the relation ? in such cases you need to estimate the size of whatever data relation or whatever table that that you are using hence one parse algorithms rely to a very large extent on procuring good estimates of relation sizes in terms of the number of tuples and if the estimation algorithm is wrong that is if too many buffers are allocated that is if the estimation is too high then there is a possibility of thrashing on the other hand if the estimate is too low then we may not be able to use one parse algorithms at all because the relation won ’ t fit into memory and the strategies that we studied for one pass algorithms will no longer be applicable what do we do in such cases that is  what do we do in cases where one or both of the relations or too big to fit in memory by itself for these we use what are called as multi pass algorithms  refer slide time  16  03  in multi pass algorithms we shall be studying today mainly about two pass algorithms generally many multi pass algorithms are generalizations over two pass algorithms and if you know the general strategy that is used behind two pass algorithms  it is sufficient or it would not be too difficult to generalize it to multi pass algorithms and multi pass algorithms as the slide shows are used when relations can not be read into memory in their entirety only a part of each relation can be read into memory and they usually have an alternation between reading part of a relation into memory and writing it back onto disk that is an alternation between computation and intermediate result generation and retrieval of intermediate results  refer slide time  16  57  the first kind of two pass algorithms that we are going to be looking at are what are called as sorting based two pass algorithms we are going to be looking at three different paradigms or three different strategies of two pass algorithms  sorting  hashing and indexed based two parse algorithms the first one that we are going to be looking at is the simplest which is called the sorting based two pass algorithms the basic idea behind sorting based two pass algorithms is shown in the slide here suppose we are given a relation r of course that is  such that the relation r is too big to fit in memory and of course for the dbms we are given a maximum of m blocks of memory that is m blocks of memory elements are allocated for the dbms  therefore m blocks of data can be read from the relation r by the dbms at any point in time the sorting based two pass algorithms have the following basic structure  refer slide time  18  05  this slide show the basic or the skeletal algorithms for a two pass sorting algorithms essentially as we noted before  the algorithm or any multi pass algorithm alternates between reading intermediate results and writing them back that is reading part of data from the disk and performing computations and writing them back so the basic idea behind two pass algorithms are based on sorting is as follows first start by reading the relations one or two relations that is we are going to concerned only with either unary or binary operators for the time being of course any nary operators can usually be reduced into one of these forms and it ’ s a generalization of considering either unary or binary operators so let us consider that the relation or the pair of relations that we are going to be using are first read block by block so because we have m blocks that are allocated to us  we can read these relations m blocks at a time now m blocks of relations from m blocks of tuples from the relation or pair of relations are read into memory and then they are sorted once they are sorted they are returned back into disk this is the alternating phase that is you read part of data from disk sort them  in this case which in this case is the computation and then write them back to disk now continue steps 1 and 2 until the relation is exhausted i am just assuming here that we are dealing with unary operator that is until relation r is exhausted and it can of course mean until the pair of relation is exhausted or until the set of all relations that this query handles are exhausted and then once these intermediate results are written on to disk that is sets of different m blocks of sorted tuples are written onto disk  use a variety of query merge techniques remember what is a merge technique  if we have taken a course on let us say data structures you would have come across an algorithm for merging which is usually seen in relation with sorting a merging basically means that given two or more lists that are sorted  can i obtain a single list that is also sorted of course as efficient fashion as possible there exist very efficient algorithms for merging especially when the results or especially when the lists are sorted that can produce results in a linear order of time so use a variety of these query merge techniques note here that in the two parse algorithms  it is no longer just merge techniques it is query merge technique that is as and when you do the merging  perform your query or answer your query as and when you are doing the merging on this intermediate results so use a variety of query merge techniques to extract relevant results from all the sorted m blocks of disk  refer slide time  21  51  now let us see some examples of two phase or two pass sortings  two pass sorting based methods for answering different kinds of queries you can notice that all of these examples follow the same skeleton of the basic idea that we presented in the previous slide so let us take first the example of duplicate elimination now as you can see duplicate elimination is or relation at a time unary operator we don ’ t have to worry about tuple at a time operators for two pass algorithms why ? because all tuple at a time operators can be answered using a one pass algorithm because all that we need to be concerned about at any point in time is just a tuple and each tuple can be checked in isolation with the other tuples so let us take the first relation at a time unary operator namely duplicate elimination that is the unique construct  how do we implement the sql unique construct let us assume that it is relation r on which we have to eliminate duplicates and of course we have to also assume that relation r is too big to fit in memory now as we know that because we are given m blocks of memory available to us  start reading r in terms of m blocks of data that is read m blocks of data into memory and sort them and the third step is store the sorted set of m blocks back onto disk now continue from step two that is read the next m blocks of data from r  sort them and put them back on to disk in in a separate file name so keep doing this until r is exhausted  r becomes empty then what is the next phase now ? we have read r relation  sorted them and put them back into memory or rather put them back into disk the next step now is to use one of these so called query merge technique on these intermediate results that we have generated so what is a query here ? the query here is the elimination of duplicates that is we do not want any duplicate tuples to appear in the relation  once it is output to the output buffer so how do we eliminate duplicates from these set of sorted intermediate results ? so the query merge technique for eliminating duplicates is quite simple  refer slide time  24  36  take one block from each sorted sub list there are several sub list of data that have been generated and each list of data is a maximum of m blocks in size that is because we have read data in terms of m blocks  except the last set of data that we wrote every other data would or every other intermediate result would be of size m blocks the last data intermediate element that we wrote would have a maximum size of m blocks  it could also be smaller than m blocks in size now take the top most block from each set of these intermediate results that we have generated and for each tuple that is take the smallest tuple among this for each tuple just go pass the repetitions of this tuple in the other blocks let me illustrate this with an example in the next slide which makes it much more clearer it is as simple as taking one tuple and then moving or rolling forward all other blocks so that they move pass the present tuple that i have taken that is if they contain duplicate tuples if they don ’ t contain duplicate tuples  you don ’ t have to move them and of course once they are all moved  put the first tuple into the output buffer now this is possible or this is possible to be done in an efficient fashion because the tuples are all sorted that is if a tuple t appears at one particular stage then all tuples that are in some way greater than t should appear below them therefore i just need to search each block until i find a tuple that is greater than t  when i am trying to remove duplicates such a kind of merge elimination takes an order of m n b time where m is the number of blocks in a sorted block set that is one of the intermediate result block set that that we have stored and b is the number of tuples in a block  n is the number of such block sets if you remember merging algorithms  a merging algorithm between two list of size m and n would take an order of m plus n time because we have n different lists of blocks of m different blocks that is each list comprising of m blocks of data and each block containing of b tuples of data we just multiply all three of them together so that we get the total time that it takes for eliminating duplicates from let us look at duplicate elimination with an example that makes the process much more clearer  refer slide time  27  50  now let us assume that as shown in the slide here  this is the first set of block set that is first set of m blocks of data that is written that is 1  2 is in the first block  2,2 is in the second block and 2  5 is in the third block now this whole thing is a set of three blocks that were written onto disk and as you can see here this entire thing is sorted  they are in sorted order that is 1 is smaller than 2  is less than or equal to 2 and so on until 5 this is the second set of blocks that were written onto disk 2  3 4  4 4  5 and so on and this is the third set of blocks that were written onto disk we now start by taking just the first blocks of each block set into memory and then considering just the first tuple in the first such block that was read now the first tuple says that it is or reads as 1 when i read 1 here  all i have do to is eliminate duplicates is to eliminate 1 from all other block sets now if 1 has to appear in all the other set of blocks  they have to appear at the top  they can not appear somewhere at the bottom  this is because it is sorted in order therefore one has to appear in the top so all we need to do is start from the top of each block set and start moving forward until we find a tuple which has a value or whose key value is greater than 1 so therefore once we read 1 here  we look at the second block set and we read 2 to begin with and we know that 1 does not exist in this block set when we read the third block set we read a 1 and cancel it out  read the next tuple and this is also 1  we cancel it out and then go on to the next tuple which is a 2 so we know that we have exhausted all 1 ’ s that have occurred in this block set and we can stop this process here and output 1 so after 1 is output  the set of blocks becomes like this that is 2 is in at the front  all the 1 ’ s have gone and the block sets have been left shifted so to say in an appropriate fashion now the next tuple that needs to go out is 2 so you see that the first tuple here reads 2 and you have to eliminate all 2 ’ s from everywhere else that is return this 2 to the output buffer and start moving forward until you have eliminated all 2 ’ s so therefore you cancel the next three 2 ’ s here and end up at 5 in the second block you cancel the first 2 and end up at 3 and the same thing in the third block set as well that is you cancel the first 2 and end up at 3  refer slide time  31  06  this is depicted in the second slide that is after the second tuple is put into the output buffer that is after 2 is put into the output buffer  our sets of blocks looks as it is shown here that is all the 2 ’ s have been eliminated here and 5 has come to the top and all 3 ’ s that is all 2 ’ s are eliminated here and 3 has to come to the top and all 2 ’ s are eliminated here and 3 has come to the top here now if i start to read 5 here then there is a problem because i have not read 3 as yet ; 3 and 4 still exist before 5 so i can not start rolling this block set until i go beyond 5 the simple answer to obviating this problem is to note that the first set of blocks from all the block sets are in memory so we just choose the least element from the first set of blocks and then start rolling the blocks therefore we now choose 3 as the tuple to be output  to the output buffer and then start rolling each of the block beyond 3 therefore 3 would be output from the second block set and we will roll it until we find 4 here and same thing three would be output here and we will roll it until we find 5 that is we roll the block set until we end up at 5 and so on so in this way we can eliminate duplicates by taking the least element of the top most set of buffers of each block set and then rolling blocks until we move beyond the least element  refer slide time  33  00  how do we perform set union using sorting ? remember these kinds of algorithms that we looked at yesterday for relation at a time operators we looked at duplicate elimination  we looked at different kinds of set theoretic operations like union  intersection  set difference and so on so set union is a binary operation it is a relation at a time binary operator and because it is a set union that is i have emphasized the word called set  it means that no duplicates in the result as you might have imagined the strategy for computing set union is very similar to duplicate elimination when we have to compute the set union between two relations r and r and s  let us say r union s we just have to output all tuples from r and s without duplicates  as simple as that therefore the set union algorithm is quite similar to that of the previous algorithm where you read blocks from r and s rather than just from single relation  sort them and store them onto disk now it does not matter for us whether any given tuple that we have read belongs to r or to s we can just consider r and s to be a single relation and then start reading tuples from them  sort them and store them back to disk and use the duplicate elimination merging technique that we just looked into in the previous example for eliminating duplicates so the output of this would be  it is clear to see that the output of this would be r union s  refer slide time  34  49  what about set intersection using sorting techniques ? set intersection between two relations r and s essentially has to return the set of tuples that are common between r and s this set intersection is slightly different from the previous two algorithms why is this so ? this is because here we need to be concerned about or we need to distinguish between tuples that belong to r and tuples that belong to s in duplicate elimination as well as in union  all that we were concerned about is eliminating duplicates that is whenever there are duplicate tuples  you just return it once and then cancel out all others but here you have to return tuples  if and only if they appear in both relation r and relation s therefore we need to have a mechanism of tagging each tuple as to where it belong  does it belong to the relation r or does it belong to relation s so the simple strategy for set intersection is shown in this slide here given two relations r and s  read them in terms of m blocks rather and store the blocks on disk in sorted order that is read relations r and s and sort them and store then back on disk now when reading it  ensure that the intermediate results of r are stored in a separate block set or in a separate buffer pool than the relations of s so that we can easily distinguish between buffers that belong to r and buffers that belongs to s and in order to compute the common elements between r and s  we simply do the following we take each block of r that is from the block set and take the first block in each block set and move them into memory and read the smallest tuple  the first tuple in this means the smallest tuple in the block set that we have just read into memory and try to roll s that is try to roll relation s beyond this tuple that is beyond the first tuple if any that is if this tuple t existed in s then there should be at least one block which gets rolled that is which gets left shifted if there is at least one left shift in s then it means that the tuple t is common between r and s  therefore it can be returned to the buffer pool or to the output pool if no such left shift happens in s then the tuple t is not there in or is not present in s and therefore can be discarded and note that here we are assuming that r and s are sets that is there are no duplicates in r and s for this algorithm if there are duplicates then what we have to do is we first have to eliminate duplicates from r before shifting s that is take the smallest tuple from r  roll all other block list in r until you eliminate duplicates of this tuple and then start looking for occurrences of this tuple in relation s  refer slide time  38  27  the last algorithm that you would be looking at in the sorting strategy is the natural join function the natural join as you know is an equijoin that operates on two relations  it is a binary operator and it is an equijoin on attributes having the same name and domain in the two relations so assume that we have two relations r  x  y  and s  y  z  where x and x  y and z attribute list where y is the common component between r and s and assume that we are computing natural join between r and s now a simple way of computing natural join is shown in this algorithm here we first read blocks of r into memory that is we first read r in terms of m block and sort them on the y attribute now we can not sort them entire tuple contents  we have sort them on the y attribute because this is the y attribute is the one that is common between r and s now sort them on the y attribute and store them back on to the disk and then secondly read relation s block by  that is m blocks by m blocks and again sort it on the y attribute and store them back on to disk now what is that we need to compute the natural join ? now for each relation or each tuple in r that is stored on to disk  we have to find all other tuples of s that are also stored onto disk  that can be combined with this relation we however encounter a small problem here what is a problem ? there could be situations where a given tuple in r can be joined with every tuple in s now if that is the case  especially if we are talking about outer joins where we can tolerate null values for y then we may have to take a tuple from r and join it with every tuple in s and we may not have  of course we do not have enough memory to store every tuple of s into memory and then start outputting it so we have to do something else now that is there could be worst case conditions where even combining the intermediate results can not be done in memory that is there isn ’ t enough ram to perform the intermediate operations in order to tackle such issues what we do is we start with a second level of what may be termed as external merging what we do is for each value of y that appears at the top of the first blocks of r that is read the first blocks from each block set of r into memory  take the value of y that appears in the first block of r and for each value of i that appears in the first block of r  note that here we can not remove duplicates we should have duplicates in the result unless of course it is specified that there should be no duplicates so for each value of i that are at the top of the first block of r  start rolling s that is start reading the first blocks of s into memory and start rolling them until they go beyond the value of y and take all these tuples and store them separately that is let us say you take tuple t1 from r and a set of tuples ts from s and put all of them into another file now once you have put all of them into another file  you just start combining the tuple that you have found in r with each tuple that you have found in s so of course as you can imagine  this is a pretty  this can be a pretty slow operation because there are several amounts of disk accesses or disk writes that are happening here the first set of disk writes are reading or the first set of disk write happens when we write the intermediate sorted relations back onto disk the second set of disk accesses happen when we read each of the blocks  from the block sets of r into memory and then for each tuple we have to read each set of blocks in s and for all matching tuples we have to write them onto some other intermediate file where they can be combined note that we can not delete the tuples from s  after we have done this operation because there could be another tuple in r having the same value of y which can also be joined with all these tuples in s that is for example if we are joining let us say employee and department and let us say the join attribute is the department number that is for each department number attribute in the employee relation  find the department number attribute in the department relation and then join them together now there could be a case where two or more employees are working in a same department and have the same department number so when we have found the first employee with some department number let us say d and assume that an employee relation is r and the department relation is s once we have found the first employee who works in a given department d in s  we take the tuple d is s and write them onto disks so that they can be combined however this tuple can not be deleted from s because there could be another employee who is also working in the same department and whose tuple also has to be combined with the tuple of the department so therefore natural join using sort is a slightly expensive technique  refer slide time  45  35  the next set of algorithms that we are going to see or the next paradigm of algorithms that we are going to see are the hash based algorithms as the name suggests hash based algorithms use a hash table as their underlying data structure and use hashing as a overall paradigm or the overall strategy in which the algorithms are based the basic idea behind hash based algorithms are shown in this slide here we first  given a relation r that is too big to fit in memory of course we read relation r block by block  we don ’ t even have to read it m blocks at a time if it is possible to read it m blocks at a time fine  it makes it even more faster now for after we read relation r block by block  take each tuple in a block and hash it  move it through a hashing function so that it is hashed onto a bucket in a hash file remember what is a bucket in a hash file a bucket is a set of buffers  is a chain of buffers that contain all tuples that are hashed in the same value for a given key that is all tuples having the same key would be hashed onto the same bucket there could be tuples having different key being hashed on to the same bucket and which could well be possible but what we can definitely say is that all tuples that have the same key will be hashed onto the same bucket so all similar tuples should hash to the same bucket then we examine each bucket in isolation  we don ’ t have to work across buckets anymore in sort based techniques note that we had to work across the buffer list that were  sorted buffer list that were generated in hash based technique we can examine each bucket in isolation of the other bucket in order to produce the final result  refer slide time  47  41  so this slide is a reminder of what a hash file organization looks like given a tuple with a particular value key on which hashing is performed  the key then maps onto a set of buckets this one shows static hashing and of course there are also dynamic hashing techniques so the hashing function hashes a key onto a bucket number  the bucket number in turn points to a chain of blocks that form the bucket or where data or tuples that should lie in the bucket are stored let us look quickly at our usual algorithms for duplicate elimination and set theoretic operations like union  intersection and so on using hash based techniques  how can they be performed using hash based techniques  refer slide time  48  57  let us first look at duplicate elimination using hash based techniques this is quite simple as shown in the slide here given a relation r which contains duplicate tuples and of course which is too big to fit in memory  read the relation block by block or m blocks by m blocks and take each tuple in the blocks that are read and hash it to a set of buckets of a hash file and let me reiterate the fact that if there are duplicate tuples in r and important property that we have to note here is that because we are using the same hash function  if there are duplicate tuples they will necessarily be hashed on to the same bucket so we have already cornered in a sense all duplicates into the same bucket and now our job is much simpler that is visit each bucket and eliminate duplicates using either a one pass algorithm for duplicate elimination  if the bucket can fit into memory remember how i did one pass algorithm for duplicate elimination that is we have to maintain a internal hash or a internal index structure in memory and then eliminate all duplicates as and when we read the relation so if the bucket can fit into memory  you can use a internal data structure or in memory data structure to eliminate duplicates or if the bucket is too big to fit in memory then we can consider this bucket as a small relation and use our previous algorithm  the two pass sort based algorithm for removing the duplicates in the buckets  refer slide time  50  31  what about set theoretic operations union  intersection  set difference and so on ? for set theoretic operations involving two relations r and s we maintain two separate hash files  hash based file organization however these two hash files will use the same hash function  hash of k and the buckets are also labeled analogously that is bucket number 0 in the first hash file corresponds or is analogous to bucket number 0 in the second hash file and so on so if tuple t appears in some bucket n in the hash file of r then if it is present in s  it should also appear in bucket number s of the hash file of s and in addition to that the standard property that all duplicate tuples are always hashed to the same bucket in a given hash file  refer slide time  51  34  so using this it is quite simple to perform set theoretic operations using hash based techniques for example if you have to perform union  all we have do is hash each tuple of r into r hash file and hash each tuple of s into s ‘ s hash file and take each corresponding buckets that is bucket 0 of r and bucket 0 of s and eliminate duplicates and then output the results similarly if you want to perform intersection  we just examine corresponding buckets bucket 0 of r and bucket 0 of s that is bucket i of r and bucket i of s and then return only the common elements and the same thing for set difference so that is quiet straight forward the last technique in hashing that we are going to see is what is called as the hash join algorithm  refer slide time  52  29  this is perhaps the most widely used technique for performing natural joins because of it is simplicity and efficiency natural join over  let us compute natural join over relations r of x  y and s of y  z where y is the common component between r and s now all tuples  now we are going to first read r and s and use the y component as the key to perform the hashing rather than the entire tuple now all tuples having the same value of the y component should hash onto the same bucket if we are using the same hash function so if the same hash function is used for r and s  we just take the corresponding buckets that is just take bucket number 0 of r ’ s hash file and for all the tuples that you find  if they can be combined or if they can be joined with any tuple then this tuple has to necessarily exist in bucket number 0 of the s ’ s hash file and so on so given this property  it is enough for us to examine just the corresponding buckets in order to perform the join operation this makes the join operation extremely efficient because we don ’ t have to perform too much of look ups when we are performing the we don ’ t have to do too many look up ’ s when we are performing the join such an algorithm is also called the partition hash join algorithm because the pure hash join algorithm is a slightly different algorithm from this but this is widely used algorithm for computing natural joins in many dbms systems so let us summarize what we have learned today in the two pass algorithms  refer slide time  54  31  so let us summarize query processing in general and then go back to the two pass algorithms that we have studied today we looked into the different stages of query execution to begin with and then we saw that there are two stages namely that of the logical query execution plan and the physical query execution plan that are especially of interest and then we saw what are some of the language constructs that make up the physical query execution plans and then we looked into algorithms that can be built to perform this internal queries or physical  to answer this physical queries using this physical query execution plan construct and these algorithms could be either single pass algorithms which can be used if the relations are small enough or if the query is tuple at a time query and there are multi pass algorithms which can be used if the relations are big among multi pass algorithms or rather in two pass algorithms  we looked at sort based algorithms or sorting based strategies for performing several of these operations and hash based strategies for performing several of these operations we have not looked at index base strategies which is quite analogous to sorting and hashing base strategies we shall briefly visit index base strategies in the next session before we take up the logical query execution plans and ways by which queries can be optimized so that brings us to the end of this session thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 16 query processing and optimization – iii hello and welcome in today ’ s session we shall be continuing with whatever we have being exploring in the past two sessions namely query processing and optimization issues as we have seen query processing is a very crucial element in a dbms design that is this is not about database design like i had mentioned in previous session that is database design is the term that is used to denote activities like schema design normalization and so on that is how to design a database such that a dbms can be used to handle this  the data in the database efficiently as possible on the other hand query processing issues concern design issues of the dbms itself that is how can we built a dbms that can efficiently process a given user query and even if in many cases  even if the query is not formulated in a form that is the slightly to be the most efficient  can the database or can the dbms detect it and rewrite the query in such a way that the query becomes much more efficient and we saw that a query processing is so crucial that it can make the difference between usability and un-usability of the dbms let us briefly have an overview of the different topics that we have studied in query processing before we move on to today ’ s topic that is of query optimization  refer slide time  02  47  a typical query processing  a typical process of query processing takes several different steps when the user gives an sql query  it is first passed through a scanning and parsing phase where the query is first scanned so that the query becomes or query is divided into a stream of tokens and these sets of tokens are then parsed to build a parse tree or a query parse tree that gives the syntactic structure of the formulated query that is given by the user now from this parse tree  a logical query plan is generated that is the parse tree is rewritten based on certain rules  heuristic rules and several different rewritings of this parse tree are possible and one of them is chosen based on some criteria like the cost estimation for this particular query tree and so on so using this  the logical query plan is generated the logical query plan of course is re written like i said and we optimize  the dbms optimizes the query automatically to a certain extent based on rewriting the query then from this the physical query plan is generated the physical query plan is a plan written in an intermediate language that is either interpreted that is executed directly by dbms or is compiled into machine code and in the last two sessions we saw what would be the or what are the typical building blocks of this physical query plan language there are  the physical plan of course should support all kinds of logical query operations like select  project and so on in addition  it should also support some physical aspects of query processing like how to iterate through different tuples in a given relation or things like sort scan  table scan and index scan and so on how do we retrieve based on a particular index and so on we also saw some algorithms that implement the internal queries that is if you remember the internal queries are formulated or all those queries that are formulated by the physical query plan language that is it is the query that the dbms uses to access data from the file system the external query is the query that the users use or the application program uses to access data using the dbms we saw different kinds of access algorithms based on the physical query plan constructs we saw different  we basically divided this algorithm into one parse algorithms and multi parse algorithms one pass algorithms are those algorithms which make at most or exactly one parse over the required relation but one parse algorithms have a limitation in the sense that if we are using a one parse algorithm for a relation at a time operator what is a relation at a time operator ? an operator that requires the entire relation to be present in order to answer the question that is being posed by the query for example operators like removing duplicate that is the unique operator in sql or order by or group by in sql and so on so all this require the entire relation to be available before the query is answered unless the entire relation is processed  even the first tuple of the answer can not be returned even before answering the first or outputting the first tuple of the result  the entire relation must be processed at least once  refer slide time  06  10  in single parse algorithms  we can apply single parse algorithms only if or if we use let us say tuple at a time queries where we need to be concerned only with one tuple at a time rather than the entire relation at a time or we can use it for relation at a time queries as long as at least one of the relations can be fit into memory completely that is it not only fit into memory but there should be more memory space left over for at least one block of the other relation if we are using any binary operator in the previous session we saw what are called as two parse algorithms and we also said that multi pass algorithms are basically generalizations of the two pass algorithms two pass algorithms are used when  of course they are primarily used in relation at a time queries because tuple at a time query do not need two pass algorithms we can just use a one pass algorithm so a two pass algorithms are used for relation at a time queries where the relation size is too big to be able to fit into memory and we saw the basic structure of a two pass algorithm the basic structure of a two pass algorithm has an alternating computation and intermediate storage handling phases that is take one set of blocks from relation  perform some computation on them like sorting or hashing or indexing which we are going to see today and then write them back into disk and then read back all this intermediate results before producing the final result so we divide a two pass algorithms into three different strategies or three different paradigms so to say what are called sorting based methods which we saw in the previous session where the computation that is done when a chunk of blocks is read from a relation is the sorting function that is a chunk of blocks is read from each relation and they are sorted and they are placed on to disk the next kind of algorithms that we saw was the hash based methods where we read a chunk of blocks from the relation and start hashing each tuples based on certain criteria if it is a natural join for example we hash it on the common attribute or if it is a group by or if it is a unique function then we hash based on the entire tuple and so on today we are going to look at the last method or we are going to see just an overview of the last method namely the index based methods that is where the computation that is used is an indexing function that is adding or searching on a index and for our purposes  we are going to assume a sorted index like a b plus tree a sorted index is something like where we can use the index structure not only to retrieve a tuple based on a key value but we can also retrieve tuples based on a sorted order of all the key values that are present in the relation  refer slide time  10  24  so what are index based algorithms ? index based algorithms can be contrasted from sorting and hash based algorithms in the sense that they use a index instead of sorting and hashing for the computation phase and they are useful when tuples are to be extracted on attributes that have been indexed and they are especially useful for selection functions especially of course the  that is when the attributes that are being searched for index and they are fairly effective for joins and other binary operations as you will see there is another join function called zig zag join which is also quite popular as hash join and in terms of efficiently computing joins between two relations  refer slide time  11  19  let us look at the first algorithm in index based methods how do we perform select or how do we perform a select operation using an index ? we are of course assuming here that the select operation involves a condition that is over an attribute that has been indexed of course  if the select is over a condition which is not been indexed then we can not use index base selection  we should either use one of the other methods that we have seen now it is fairly straight forward if the selection condition is an equality condition on an attribute because that is what indexes are meant for let us say searching a b tree based on the value of a key is simply saying that select key equal to this value from the relation on which the b tree is maintained so the simple algorithm is to search for the index for the required set of tuple or tuples in case of inequality conditions that is something like a less than or equal to 10  we have to retrieve a sub tree from the b plus tree index that is we can not retrieve just one node of the index and we are also making another assumption here that the equality and inequality condition involves the key under constant and not another attribute that is we are saying a equal to 10 or a less than or equal to 10 we are not saying something like a equal to b where b is another attribute which has not been indexed but and we have to search for all tuples in such a case where the index indexing indexed attribute is equal to some other attribute that has not been indexed  refer slide time  13  15  we shall not be covering all the other algorithms say set theoretic algorithms on index based methods after we have seen the general pattern of how set base operators like union  intersection and set difference are performed using sorting and hashing  it is fairly straight forward to design algorithms using index based methods also let us look at the last query that we looked at the other algorithms namely joins or natural joins how do we perform natural joins using index based algorithms ? now consider a natural join as usual between two relations r  x  y  and s  y  z  where y is a set of attributes that are common between r and s that is y is the attribute over which the natural join is going to be performed now assume that suppose  that is suppose y is not only indexed that is the y attribute is not only indexed  it is also the primary key in s assume that s has as index over its y component if it is a primary key then it becomes even more simpler but even if it is not  it is just an indexing attribute or set of attributes we can use the following algorithm we just start reading r that is the first relation in this chunk of blocks that is chunk of m blocks so read r block by block and for each tuple that we have read from r extract its y component and search relation s based on the y component that is we start sequentially reading one of the relations and for each block that we read we do a index search and because index search is much faster  we find the set of all relations let us say if it is a primary key then we find exactly one relation and if it is not a primary key we have to use some kind of clustering index and we find a set of tuples that may contain this value of y that we are looking for now all that we have to do is join this tuple with how many ever tuples that we have found in s and we have computed the natural join over r and s so if a corresponding tuple is found then perform the join and push this to the output buffer using index based methods we can also perform another kind of natural join which is called a zig zag join  refer slide time  16  05  the zig zag join can be performed when there is a sorting index that is available for both r and s that is assume that we are performing a natural join over two relations r and s where r is  x  y  and s is  y  z  where y is the common set of attributes between r and s now also suppose not only that y is indexed over s  it is also indexed over r and this index is a sorting index that is it is a something like a b plus tree where i can access all keys available in the relation in a sorted form now all that we have to do here for performing such a join is shown in the set of three steps in this slide we just use or we just keep calling getnext function  note that we can represent relations r and s as iterators so we just open both relations in iterators and let us say we use r and start calling getnext function on the iterator  r iterator now this get next function get the next logical key or the next sorted key in r and for each key that is found in r  we start searching for corresponding tuples or matching tuples in s and if matching tuples are found  perform the join and append the output to buffer such an algorithm is called a zig zag join this is because even though logically we are going in a sorted form physically  the control would actually be going in a zig zag fashion over on a storage disk that is we don ’ t know or we are not sure that the sorted form in which we are accessing the keys is indeed the same form in which records or tuples are stored on disk so such an algorithm is called a zig zag join however it is quiet an efficient algorithm because we are using index on both set of attributes that is both r and s  refer slide time  18  24  let us move on to the main topic of today that of query optimization until now we have been looking at the last step in a query execution process that of managing the physical query plan languages that is how do we perform internal query answering using physical query plan languages let us move on to the next or the upper level before this that is of logical query plans that to primarily that of parse trees that are generated after a query is scanned and parsed by the query complier we are going to see whether or how or whether we can re write a parse tree in a sense that we can make the query answer in a much more efficient fashion than it was formulated by the user or the application program so query optimization techniques that are independent of the semantics of the query or the semantics of the application is what we are going to be looking at here and these techniques are based on rewriting the parse tree representing a relational algebra expression of the query that is the parse tree would actually convert a given sql query into a relational algebra and then represent it in the form of expression tree which can be optimized and there are two kinds of optimization  what might be termed as heuristics based optimization and cost based optimization a heuristics based optimization is essentially a set of thumb rules using which we make assertions that the resulting parse tree is a better or more efficient than the original parse tree however given a parse tree usually there might be more than one parse trees that could be generated based on which heuristics are applied and in what order they are applied now among these different parse trees  we might have to choose one of the parse trees for a particular query instance and we can perform this choosing by assigning trying to estimate cost that each of the parse tree incurs so usually in practice a combination of heuristics and cost based optimization are used before a query execution plan is finally chosen for execution  refer slide time  21  04  now we have been talking about parse trees and rewriting parse trees so what are parse trees ? a parse trees as you might have come across especially in the context of compliers or syntactic structures that of most programming languages that can be expressed in the form of a tree structure or tree data structure a tree data structure represents a hierarchical structure this is also called a syntax tree or a parse tree and execution of a syntactic structure is usually done by what is called as a post order traversal of the parse tree that is a post order traversal simply says that given a tree having a root or 2 or more sub trees or 1 or more sub trees execute the sub tree first and then execute the root and the same rule recursively applies to all of the sub trees we shall not be going into details of how to build a parse tree and a traversals and so on but we shall be concentrating on how to modify a parse tree that is in the context of relational algebra that is context of queries that are expressed relational algebra this slide shows an example of a parse tree the left hand side of the slide shows a small relational algebra query which says project department name from select salary greater than 3 lakhs from again manager join department where the join condition is manager dot d number equal to department dot d number that is it is a natural join between the manager and department relations and from this natural join we are selecting the set of tuples where the salary field is greater than 3 lakhs and then projecting the department name that is we are looking  we are querying for all departments who or which pay their managers a salary greater than 3 lakhs  refer slide time  22  20  the corresponding query can be represented in the form of a tree data structure that is shown in the right hand side of the slide as the tree shows the top most operation project becomes a root node of the tree now the project is a unary operations therefore it has one child which is the select query so the select query becomes the child of the project query the select again is unary operations which is being performed on the join operator here so the join operator becomes a child of select and the join operator is a binary operator where it has two children and these two children are its two arguments that is manager and department relations so  logically as you can see  the queries expressed the hierarchical structure that is inherit in the query is made explicitly by using the parse tree once a parse tree is generated by a query complier  there are several kinds of checks that are performed on the parse tree before it is optimized some of these checks are shown in this slide here  refer slide time  24  31  syntactic check  the syntactic check simply says that is the syntax of every operator correct for example we can not have two children on a project operator  project and select are unary operators and similarly we can not have a single child on a join operator  it is a binary operator and so on entity checks  entity checks basically check whether every relation name that is specified in this query actually exists that is  is it there on disk  that is  is there a relation called manager and department on disk and so on in some cases a relations may not  a relation that is named in a query may not actually exist on disk but it could be derived that is it may not be a base table but it could be a derived view that is it could be a view that is there in the schema in such a case the complier would expand the view remember  a view is again another query and the contents of the view are not actually on the database so the view is again expanded and the parse tree for the view is joined or is hooked to the parse tree of the overall query wherever the view name appears so this is called view expansion and then there are attributes checks that is it does every attribute name refer to valid attributes and then there are type checks that is  does each attribute participating in an expression have a proper type that is we can not say something like salary less than cats and so on i mean it has to be  if salary is numeric then the other attribute should also be numeric and so on so there should be type compatibility between attributes within an expression once these checks are performed  parse trees are rewritten based on certain heuristics so these heuristics are also called as rewrite rules and which specify several conditions and corresponding actions that need to be performed when these conditions hold and a parse tree should be expanded to its maximum extent before rewriting that is for example view should be  view should be replaced by the relevant parse trees when the parse trees is being  before the parse tree can be optimized and some rewrite rules could be situation specific and we are not going to be looking at such rewrite rules here anyway and they work only if certain conditions hold on the data set that is they make certain assumption about the dataset based on which the parse trees can be rewritten  refer slide time  27  35  the first rewrite rule that we are going to see today is what is called as pushing selects the pushing selects basically says that try to push a select operation as low down in a tree as possible without altering the semantics of the parse tree or without altering the correctness of the parse tree why is this important or why is this beneficial ? remember a select or the cardinality of a select operation is less than or equal to the cardinality of the input relation for the select that is a select basically removes certain tuples from a relation before returning the output and one thing to note here is that whenever we talk about relations  always think of very relations that is megabytes or probably giga bytes of tuples present in a relation and if a select operation is such that it requires only a small fraction of the tuples there is a great amount of optimization that is performed already that is huge number of data need not be handled after the select is over and we can start working with a much smaller dataset so  select based optimizations are the most common optimizations that are used in query rewriting so the slide here shows an example of pushing selects based optimization let us look back at the parse tree that we just generated previously that is a project department name from select salary greater than 3 lakhs from natural join between manager and department now here you can see that the select operator here that is salary greater than 3000 is being performed after all possible managers or join with all possible departments now if we are going to anyway look at a managers whose salary is greater than 3 lakhs  we don ’ t have to consider joining all possible manager tuples with all possible department tuples you might as well say that we are going to join tuples of manager where the salary field is greater than 3 lakh so that is what is performed in this tree here  refer slide time  30  06  that is the select operation is pushed down in the tree so that first we select the set of all manager tuples where salary is greater than 3000 and then join this with a greater than 3 lakhs and join this with the corresponding tuples of the department relation and then project just the department name so if  let say if there are about 1000 managers in a company and only about 50 mangers have a salary greater than 3 lakhs then instead of joining or instead of looking at 1000 different manager tuples  we need to content ourselves with only 50 manager tuples  refer slide time  30  58  the second form of rewriting rules is also related to pushing selects and which is called the cascading select or conjunctive selects that is a select which has a conjunction can be split and cascaded into several different select operations and which can progressively start reducing the search space as we go along this slide shows a small example which illustrates this point suppose we have a select operation that says select c and d over r that is select where c and d here are some logical condition now we are selecting those sets of tuples from r where both condition c and condition d holds if this is just the relation here  now this could be  the conjunction could be even more that is it could be c and d and e and f and so on there could be many more and conditions here now if we leave this as it is  then the entire set of relations are searched for this condition that is the entire set of conditions are matched on the entire set of relations on the other hand we can see that the right hand side of the relation or the right hand side of this equivalence condition says that select c from select d from r that is it is a cascading select that is shown in the slide here where instead of using c and d over r  we first say select d over r that is this could be much more efficient if d is an indexed attribute over r so if d is an indexed attribute over r or rather d is an condition over an indexed attribute over r  we can quickly or efficiently retrieve all those tuples that match the condition d from this much smaller set or hopefully much smaller set of tuples that have been extracted  we then perform a select c or we then look for the condition c so the search space for c is much smaller than the search space of d and if d is a condition over an indexed attribute  the search space of d or the inner select can also be very fast that is because we are retrieving based on index searches  refer slide time  33  41  there is an exception however to the pushing select thumb rule that is push selects as far down a relation as possible thumb rule and this exception occurs whenever there are views that are there in a relation when a view is expanded  it might be necessary sometimes that selects are actually pushed up beyond the view before they can be pushed down let us look at an example to illustrate this this slide shows an example having two relations movie and starsin movie is a relation that has the following attributes  the title of the movie  the year of the release of the movie  the director of the movie and the language in which the movie is made and then starsin is an attribute that says which film stars acted in which movie it says it has an attribute title which is the title of the movie the year which is the year in which the movie was released  the name of the film actor and the language of the movie and now of course we can see that among the two relations title  year and language are common that is we can perform a natural join using the three different attributes and now we create view called bengalimovies where which just says select star from movie where language equal to bengali that is we are interested only in those tuples of the movie where language equal to bengali  refer slide time  35  27  now let us see if we give a particular query and what happens let us say we have a query which says which filmstar worked under which director in bengali movies that is we have to bear each film star with a director as long as the language of the movie is bengali so how do we go about or how do we express this query this is quite simple from a sql point of view that is we just say select starname and director from bengalimovies natural join starsin that is we join we perform a natural join between bengalimovies and starsin note that bengali movies also have the same structure as the relation called movies that is you can join based on title  year and language attributes so the corresponding parse tree for this is also shown in the slide here that is we are projecting starname and director from a natural join between bengalimovies and starsin however this relation bengali movies is a view so we have also expanded the view here the view for bengalimovies is basically select language equal to bengali from movie so this view is expanded and starsin is a base table which is kept as it is in the parse tree now  if you see here even though the left hand side of this tree that is the parse tree that talks about bengali movies contains tuples where the language field is bengali it does not contain tuples having any other language field however while performing the natural join we are still considering all tuples in starsin even if it does not contain the language called bengali that is we take a particular actor and we look at the set of all his or her movies regardless of whether the language is bengali or not this is clearly wasteful because after all we are only interested in which star worked under which director in bengali movies so  one way to optimize this is to put a select operator here that is we need to select those tuples from starsin where language equal to bengali therefore this select operation here which is in a view or which defines the view should first be taken up before it is brought down this is what is shown in the next slide here  refer slide time  38  08  that is all tuples of starsin are selected even if they are joined with tuples having language equal to bengali so here we just select all tuples which is not really necessary therefore we need to take this select tuple that is the left hand side of the relation selects a particular or performs a particular selection condition which is also applicable to the right hand side of this sub tree so we move up  we move this selection relation up as much as possible before moving it down and to bring it here that is so this would be the rewritten parse tree that is there are two separate select statements one for movie and one for starsin which finally forms the query  refer slide time  38  58  some more thumb rules involving selects some times in a join function we perform a join blindly and then select a set of tuples from this joined set of tuples based on a condition that applies to only one of the relations we have actually seen such a example earlier but let us revisit again in order to make this  bring out this rule we didn ’ t apply this rule however because we applied some other rule now consider the following query over the movies database again the same movies database comprising of movie starsin and bengali movie relations now the query is we are only looking for which actors or which stars acted under the director satyajit ray in bengali movies we are not just pairing up all sets of actors with all sets of directors under bengali movies  we are just looking for who all acted under the direction of satyajit ray in bengali movies so the sql statement for this again is quite simple we just say select starname from bengalimovies natural join starsin where director equal to satyajit ray  refer slide time  40  30  the corresponding parse tree if you build the corresponding parse tree here this would look like this at the lowest level is the join operator between bengalimovies which is expanded here that is shown in yellow and the starsin relation now after these two relations are join  we select for director equal to satyajit ray and then project starname and director name of course we need we need to only star name  there is no need to project director so what we are doing here is that we are joining two relations and even if let us say that even if we move this select up and bring it here  we are still joining two relations based on just the factor that language equal to bengali however we can note that in the second relation here  we are finally interested in those tuples where the director field equal to satyajit ray we are not interested in any other directors whose records are available in this relation here so we can as well move this director equal to satyajit ray select below the join here that is which is above the join here is now below the join function and of course in addition to this we can also perform the previous optimization which is not shown here that is move this language equal to bengali up and bring it down so that it comes before starsin so there are two different optimizations that are possible in this particular query tree  refer slide time  42  17  the next kind of thumb rule we are going to be  that we going to consider is what is called as inserting projects now note that if the output of a query involves projection of just a few attributes over a large relation containing let us say tens of attributes or different or tens or even sometimes of hundreds of attributes  a large tuple containing many different attributes  there is no need to work with so many tuples or so many attributes when all we require is just what is given by the overall projects and the conditions that of select operations that are given in the query have a look at the examples shown in the slide here the left hand side of the slide shows a query tree which says going back to our manager and department example  project department name from that is select salary greater than 3 lakhs from natural join between manager and department that is this is also optimized that is the select operator which is here is now brought here so that it is optimized now if we notice the output of this tuple is just the department name  the output of this query is just the department name the user or the application program is not concerned about any other field of the database that is anything like the department id or the number of people in the department  the location or whatever else that is there in the department relation that is not really important however we can not just throw away all other attributes other than whatever is requested because these attributes may be required for some conditions now which are the other attributes that are required for some conditions here that is department dot dnumber equal to manager dot dnumber is another condition that is required for the natural join that is shown here so  the only set of attributes from department that we are concerned about here are the department name and dnumber that is the department number similarly the only set of attributes that we are concerned about for the manager relation is just the salary and the department number we are not concerned about the name of the manager  the age of the manager  the date of birth  the employee number  the address  nothing so what we can do is we might as well insert extra projects down in the query tree so that the output relation becomes smaller and more focused to what is required for as part of this query that is we have inserted a project dnumber and salary before manager because those are all the fields that we require above and similarly we have inserted a project dnumber and department name because those are all the attributes that are required for these query from the department relation  refer slide time  45  42  so that was the brief overview of some of the thumb rules that can be used to rewrite query parse trees in order to make them work better let us look at the other aspect of query optimization that of cost based optimization a cost based optimization is essentially used to choose between one or more or choose between two or more different candidate parse trees that is given a query parse tree if i have used the thumb rules and generated several other candidate parse trees  each of which claim to be more efficient which is the one that i have to use in order to do that we assign a cost for each of the different components or a cost estimate for each of the different components of a parse tree and the overall cost of a parse tree is an algebraic expression over each of these cost estimates it is either a sum of all these cost components or multiplication or whatever depending on what exactly is the tree now what are the factors that affect execution cost ? this slide shows some possible candidates that affect execution cost  say access cost is second storage how costly it is to access data from disk or any other storage medium that we are using then what is the storage cost  how much storage do we need especially for storing the intermediate files and what is the computation cost  how much of processor time that we require and so on what is the memory usage cost  how much memory  primary memory or ram does it take and how what is a communication cost especially between the dbms server and the client  if they are situated on different machines and communication cost becomes very profound  if we are considering distributed database that is how many communication sequences are required between the different dbms servers that form this distributed databases so all of these factors affect the overall query execution cost  refer slide time  47  59  a database catalog is a set of value estimates or some kind of meta data or meta information that are stored in a dbms that are useful for cost estimation  refer slide time  48  13  and catalogs are metadata that could be either say table specific metadata like an estimate of the number of tuples in table  the size of the table and the number of blocks that are occupied by table and so on and they could be field specific like the number of distinct values of a particular field  an estimate of those numbers of different distinct values of a field and so on or they could be database wide tools or they could be index specific information and so on so let us look at some typical kinds of information that is stored in a catalog and see how we can estimate the cost of different operators  refer slide time  48  57  some typical information that are stored or depicted here let us say b of r is the notation used for the number of blocks that are taken up by a relation r similarly t of r the number of tuples that exist in a relation r and similarly v of r  a is a field specific attribute that is it is an estimate of the number of distinct values that attributes a has in relation r and of course for example if attribute a is the gender then the number of distinct values is only 2 on the other hand other hand if attribute a is something like distance between something and something else  it could take on a range of several other values and of course v of r  a1 till an is the set of is the number of distinct values of the combined tuple or that is formed by a1 to an  refer slide time  49  57  let us look at some simple cost estimation techniques and there are many more cost estimation techniques other than this but we are only looking at examples that show the bigger picture behind this  the representative example so suppose we have an equality selection that is select  that is s equal to select a equal to c from r where a is an attribute name and c is a constant now a is an attribute name and c is a given constant and we have an estimate of the number of distinct values that the attribute can take which is given by v of r  a then the probability of a equal to c is simply 1 over v of r  a and because the number of tuples is t of r  the slide here shows the estimate that is t of a is equal to t of r divided by v of r  a which is an estimate for the number of tuples that are there in s and this is a good estimate if all values of a have fairly uniform probability or that is in the selection query now if the dataset is queued then it may not really be a good enough estimate  refer slide time  51  27  consider the inequality condition that is select a not equal to c from r which is assigned to s and c is again a constant now this is again quite simple because suppose the number of distinct values of a is given by v of r  a then the probability that a is not equal to c is simply v of r  a minus 1 divided by v of r  a that is 1 over 1 minus 1 over v of r comma a now multiply this with t of r which gives us an estimate of the number of tuples that could be in s what about a composite condition something like select c or d that is condition c or condition d over r and assign it to s now let us first  suppose we have estimated that p tuples in the relation satisfy condition d condition c and q tuples satisfy condition d and there are n number of tuples in the relation now the probability that a given tuple will match c or d is shown here that is 1 minus  the multiplication of 1 minus p over n and 1 minus q over n that is this is the probability of a tuple not satisfying c and this is the probability of tuple not satisfying d and the multiplication of this is the probability of tuple not satisfying both c and d and 1 over this or 1 minus this is the complement of this this is typically the de morgan ’ s law of which says  which gives us the probability of tuples that satisfy c or d now multiply this with n or the number of tuples which gives us the size estimate of the query  refer slide time  53  29  the last estimate that we are going to look at is trying to estimate the size of a natural join consider a natural join between r of x  y and s of y  z initially for simplicity let us assume that y is a single attribute  all though x and z could be composite attributes that is their sets now also let us assume that v of r  y that is the number of distinct values of y that are in r is less than the number of distinct values of y that are in s now each tuple in r can be combined with corresponding tuple in s with the probability of 1 over v of s  r because that is the number of values that is s  y rather than there is a small bug here  v of 1 over s  y now that is the probability of finding given value of y since there are t of s tuples combining with t of r tuples  the size estimate of the total of this join would be t of r times t of s divided by v of s  y here now this is true if the smaller relation is r that is the v of r  y is less than v of s  y  refer slide time  55  04  if the opposite is true then v of r  y appears in the denominator therefore we just take the maximum of v of r  y and v of x  y in the denominator and the numerator remains the same t of r times t of s which gives us the good enough estimate of the number of tuples in the natural join that is if y is a simple attribute if y is a composite attribute that is it contains of many different attributes then we have to consider the max of each corresponding attribute that is common between r and s we shall not be looking into this in more detain here  refer slide time  55  40  so that brings us to the end of this session and also the end of the topic on query processing and optimization that we have studied we have in some sense just scratch the surface of this vast and crucial aspect of dbms design especially  namely query processing and optimization so in this session we looked at index based algorithms for physical query plans and we also looked at different query optimization techniques based on pushing selects  cascading selects and pulling selects out of views and inserting extra project operations we also looked at several kinds of cost estimation techniques which can be used in combination with heuristics of query rewriting in order to select the best or in order to select what is considered to be the best query execution plan that brings us to the end of this session thank you database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no # 17 transaction processing concepts in today ’ s class we are going to look at transaction processing concepts any doubts in this particular class  you can e mail your doubts to the e mail address that is shown in the slide here djram @ iitm.ac.in  refer slide time  1  40  now we will look at what is the meaning of a transaction in databases  refer slide time  1  52  a transaction is essentially an atomic unit of access which is either completely executed or not executed at all typically databases store date of interest and we have applications trying to access the database and modify the data that is stored in the database now one example is banking database where you wish to transfer funds or withdraw amounts from your account in the bank database or you want to debit or credit certain amounts at transfer  amount from one account into another account when the database is operated upon by the applications  we have certain instruction that will be executed and these instructions are not just normal instructions as executed in an operating system but they need to obey something more than the normal instructions what we are going to see is we are going to look at instructions and can see how these instructions need to be taken care of or what properties need to be enforced on these instructions which we call as transactions  refer slide time  03  26  now here are some examples of  typical examples of transaction many enterprises use databases to store information about their state now any occurrence of a real world event that changes the enterprise state requires the execution of a program that changes the database state in a corresponding way for example balance must be updated when you deposit into a banking database when you withdraw an amount  basically need to update again your account by modifying the balance correctly so typically what we say is the transaction is a program that accesses the database in response to real world events they basically going to modify the state of the database transaction essentially modifies the state of the database  refer slide time  04  24  now here is the very simple example of a database transaction as you can see here it ’ s a debit transaction which tries to withdraw a particular amount from a banking database if we show that there is a debit  an account number and debit amount is given to the debit transaction now the begin transaction shows that this is the start of the transaction  execution of the transaction now there are three instructions which are part of this transaction the first one is read the account number and the balance that is there in the database  in the banking database now as part of the read instruction  we are going to see later  what are all the other instructions that need to be executed when this read instructions has to be executed by the transaction now after this read instruction succeeds  you will have your account balance in the variable called balance and now this balance has to be updated correspondingly with the amount that is going to be withdrawn from the account as you can see later after the withdrawal amount  the new balance has to be computed by changing the balance and then the new balance has to be written on to the database back and finally you signal that this transaction is finished by giving an end transaction now what we basically see here is the three instructions id 1  id 2 and id 3 shown in the slide here together constitute what we call as a transaction now normal programs we don ’ t basically distinguish by grouping instructions together and saying that they together constitute a transaction now we will go further deep analyze why these instruction put together will be called as transaction in this particular case  refer slide time  06  41  another example could be a credit transaction as you can see here again we have credit transaction being supplied with an account number and the amount that you would like to credit as in the other case you have a begin transaction and end transaction signaling that all the instruction in between constitute together a transaction now as done in the earlier case the read instruction or read instruction is going to fetch the balance from your bank database  given the account number that particular account number  the balance will be read from the backend database now the new balance is computed by adding the amount that you crediting into the account and now you have to write back the new balance back into the database now these are very simple examples of what is the debit transaction and credit transaction in the case of a banking database  refer slide time  07  43  now we can see how these transactions in reality operate on the database for example it is shown here in terms of the credit process which is shown in the slide shows the credit part of the transaction which we saw earlier now if you look at the debit part of the transaction you can see that the debit process executes the id 1  id 2  id 3 instructions shown earlier as shown in this figure the backend database which is stored on the disc contains the information or the data relating to the bank customers now the credit or debit transaction needs to access the database and retrieve the information and correspondingly modify this information and after modification  they need to write the information back on to the database now let us understand this little more carefully here now we have has part of the debit transaction id 1  id 2 and id 3  id 1 is a read instruction now this read instruction has to go to the backend database management system which takes care of actually now finding out the corresponding data on the disc and move the data back to the local variables of the process  debit process that is now after the id 1 is executed  the balance variable will have correct data relating to the current balance that is there in the account number now since it is a debit transaction  the balance is going to be updated here by withdrawing the amount from the current balance and the new value will be computed which will be stored in the balance variable now at the end of the transaction  debit transaction the value has to be written back onto the backend database and that what signals the end of the transaction now at the beginning of the transaction  the value is read from the bank database and at the end of the transaction  the new value is written back on the database this is what we mean by a transaction a transaction essentially is reading some data from the database processing that data according to the semantics of the transaction and the new changed values are being written back onto the database at the end of the transaction the same thing actually happens in the case of the credit transaction that is shown here except that the new balance is now added with the amount supplied now we are going to see when the transactions are operating on the database  we need to have certain properties that needs to be enforced on these instructions so that the state of the database is in a consistent fashion now let us understand what are those properties and what happens really when the transactions are executing on the database  refer slide time  11  27  now here is the case where we are explaining what happens when the transactions have to read data from the database as shown earlier in the figure  we have to first find the address of the disk block that contains the data item x  x in this particular case can be a balance  it can be an account number so it is basically the data that needs to be fetched from the backend database now once you actually found the disk block  you have to copy the disk block information into a buffer in the main memory that is the local variable that is shown in the figure earlier now once the item has been copied into the buffer  the value of the disk block is copied into the buffer then you have to copy that value into the item x to show that the program variable named x now contains the value that is fetched from the disk block this is what the read operation which is shown in the transaction signifies  refer slide time  12  43  now we have correspondingly a write operation which shows how the write is executed in this particular case you can see that find the address of the disk block that contains x now in this particular case you not only fetch the value of the data  you are going to modify the data and then the updated value is going to be written back onto the buffer as shown here we can understand here  the last step is different from the earlier read operation store the updated block from the buffer back to the disk that is extra instruction that is going to be executed in this particular case now essentially read and write are the two operations that are going to be used by the database transactions and we have seen how the read and write fetch the required information for the database  fetch the required information for the transaction from the database  that ’ s what was explained right now this is similar to the normal process which would have read the value from a backend disk file this is essentially same concept as of the process  trying to open a file and read the information from a file what more has to be done here is it is not just a set of file operation that are been performed by a process but we also need to enforce certain conditions on this instruction so that the database state is always maintained consistently and that is what we mean by a transaction and the transaction has to enforce these properties on the instruction it is executing  refer slide time  14  48  one of the things that will happen for a normal process is when failures occur  the process can leave the files that it has opened in a inconsistent state for example if you understand the operating system and open  if your process opens files and the operating system crashes due to power failure or other reasons  what really happens is the files that are open could be left in an inconsistent fashion similarly if a file is been operated simultaneously by more than one user  again the chance of corruption exist on the file because there is no guarantees on how the file is been accessed simultaneously by multiple users and this has to be prevented in a database because the data that is being stored in a database needs to be in a consistent fashion always that is to be maintained for example if you consider a banking database  whatever happens you wish that you should not lose your money that you are depositing into the bank if the bank come and tells  there is a power failure and we have lost your 1 million rupees that you deposited  you are going to say that this quiet unacceptable and you want the bank to ensure that whatever happens  the data that is stored in the banking database is consistent all the time this is an essential difference between database and file systems file systems could be  they may not be any guarantees associated with file system in operating systems whereas when you take the database transaction there is certain level of guarantee that is given to you regarding the state of the database at the end of the execution of the transactions now let us look at what kind of failures can happen in a system and what are the consequences of those failures they could be different kinds of failures  we have actually listed a few failures here and we will start discussing them in more detail as we go along now you can see here the first kind of failure that can happen is a hardware or software error in the computer during transaction executing this means it is possible that there is a problem in the hardware or the software for example it could be a operating system bug or it could a hardware bug that could have made the computer fail which means that when you are executing this set of instruction that is shown earlier as a transaction  the failure can occur the other kind of failure that can occur is internal to the transaction failure caused by an operation in the transaction  for example you are actually dividing by zero  divide by zero will cause the program to crash  refer slide time  18  23  so this is another kind of error that can happen in the system the other kind of errors is condition that cause translation of a transaction for example data needed for the transaction not found  you are trying to transfer funds from one account to another account then you recognize that the other account doesn ’ t exist this will result in the transaction to be aborted because the fund transfer is not happening correctly the account into which you should be transferring the funds is not found in this particular case the other important issue is concurrency control enforcement which is to be done when multiple transactions are simultaneously operating on the database for example if you really see how people can operate with the accounts  it ’ s possible that two  you could be withdrawing some amount of money using your atm machine but at the same time there could be another transaction trying to transfer funds from your account to another account when this happens it is possible that the state of the database could be corrupted unless there is some kind of a concurrency control that is enforced to ensure that the system is in consistent state we will go and look at the subsequent lectures in detail how the concurrency control is enforced by the database management system on transactions so that the database is in the consistent fashion now it is also possible because of the concurrency control that is enforced  a transaction is aborted because the transaction is started executing and the concurrency control mechanism found that transaction can not proceed anymore then it may also abort the transaction so this is another reason why a transaction could fail  refer slide time  20  37  they could be other reasons as well  something like loss of data in the disk blocks during a transaction  due to let us say the disk head has crashed so then it is possible that you are not able to retrieve the data correctly from the disk this is a disk failure this occurs when disk hard disk has failed there could be other catastrophic reasons for you when you deal with databases  things like power failure  fire and other kinds of catastrophes like earthquake which could destroy the data and they are beyond the human control and one of the things you must realize is all these kinds of failures are possible and in the event of this failures  the database till should ensure that the data that is stored in the database is consistent and it is available by other means that is you are able to retrieve the data back  even when failures of this nature occur now a part of this lecture we will explore  how you can handle this situation when failures occur when transactions are executing  refer slide time  22  06  now what we are going to look at is we will intuitively understand the concept of transaction to start with and we will see what can happen to a transaction in the event of failures we in fact look at several kinds of failures  starting from a transactional error to a disk failure  to a power failure  to a more catastrophic failure so what we would like to see is what happens if this failures occur  when transactions are in progress imagine you are withdrawing money from your bank account and the power fails now what happens ? is your bank account still shows correct balance or is it going to show that you have already withdrawn when you have not taken your amount what is the state in which your bank database will be left when the failures occurs  when your withdrawing money from a account now here is a case where it is shown more precisely to say what kind of scenarios can prevail and how those scenarios have to be addressed look at the scenario one what happens if the credit transaction fails after executing ic 1 and before executing ic 2 ? remember ic 1 is a read account balance instruction and ic 2 is when it is actually modifying that balance local which still has not written that value back onto the database because the right has to be done in the end of it  ic 3 has to be done to write the balance back on to the backend database now what happens if the credit transaction fails after executing ic 1 but before executing ic 2 ? now in a normal scenario  if you don ’ t really take care of this situation  it is possible that the database is left in an unknown and undeterministic condition when the failure occurs but you have to actually prevent this from happening  by saying that it will bring back the database to a consist fashion if the failure occurs in this particular case  you have to ensure that all the instructions ic 1  ic 2 and ic 3 are either executed or not executed at all this is a very important property that needs to be ensured for database transactions we will go to see this property in more detail  this property is called the atomicity property of the transactions that is all the instruction put together have to be executed either in full or none of them should executed at all in fact if you carefully look at the initial example where we had proceeded the three instructions ic  1 ic 2 and ic 3 with begin transaction and end transaction all the instructions between begin and end have to obey this property called the atomicity property that either all the instructions are executed in full or none of them are executed this is what we see has scenario one what can happen if the credit transaction fails after executing ic 1 and before executing ic 2 let us move to scenario two what happens in scenario two ?  refer slide time  26  01  if the credit and debit transaction executes simultaneously  what are the likely things that can happen ? in fact shown a case where ic 1 is executed then followed by id 1 is executed then ic 2 is executed then id 2 is executed  then ic 3 and followed by id 3 if you carefully look at the way it was written here  both the credit and the debit instructions have been interleaved ic 1 is basically a read account number and the balance  id 1 is also a read instruction on the database except that this is the debit instruction  the earlier is the credit instruction now this will also read the balance in the account number now if you say that both are operating at the same account number  they are reading the balance at the same time now imagine ic 1 has the same balance  the value that is currently let us say the account has a balance of 500 rupees in your account then both ic 1 and id 1 read the value as 500 now imagine that your depositing 200 rupees and withdrawing 100 rupees now ic 2 will say that 500 plus 200 which is actually 700 and ic 3 will try writing 700 back into the database whereas id 2 will try to reduce the balance from 500 by 400 and id 3 will write the value has 400 now you can see  you have lost some amount in the process because the credit amount is completely lost because both the credit and the debit transactions are simultaneously operating and only the debit is shown here  the credit is lost the credit that is done into the database is lost in this particular case so this is what we see as scenario two when transaction operate concurrently on the database items  it is possible that the database state is left in an inconsistent fashion as shown in this particular example now we have to prevent this from happening and this is what we call as the consistency property of the transaction now what we mean by consistency here is when the transactions are operating concurrently  simultaneously we need to enforce the condition that the transaction in effect have executed one after the other rather than simultaneously this is in some sense we need to prevent if there is conflict between transactions they operating simultaneously on the database items has to be prevented and this is achieved by what we earlier called as concurrency control mechanisms so we need concurrency control mechanisms for making sure that the database  when it is operated upon simultaneously by multiple transactions is not left in an inconsistent state this is what we see has a scenario two and scenario two gives the property of consistency whereas scenario one gives the property of atomicity to the transaction  refer slide time  30  00  now let us move on to the third scenario scenario three where basically it is possible for one transaction to see the values of the other transaction before it is actually finished  its full operation now that what is actually stated here what happens if the result of the credit transaction are visible to debit transaction  before it is actually written onto the database ? what does this mean ? this is elaborated further by saying that ic 2 writes the balance the earlier case if you are actually depositing 200 rupees when your initial balance is 500  ic 2 will write the value of 700 now the debit can read this value of 700  even before the credit has actually committed its value to the backend database now we can  debit can now go and then withdraw the money from the new balance even before it is written back on to the backend database if this happens  the results of one transaction are visible in this particular case  the credit transaction are visible to the debit transaction before it is actually finished execution now this results in what we call as an isolation property because for some reason if the credit transaction fails later  for various other reasons if the credit transaction fails and if its results are already visible for debit transaction  you need to abort the debit transaction also because it has read the values of a transaction that is aborted this is what we mean by causing cascading aborts if a transaction values or results are available for some other transaction before it is committed  it could lead to cascading aborts to avoid this  what we have to do is we have to enforce the property called isolation isolation ensures that the transaction results  the values which the transaction has changed  the values of the data items which a transaction has changed are not available for other transactions till the transaction has actually committed actually concurrency control protocols and commit protocols which go together  ensure the consistency of the database in the presence of multiple transactions executing simultaneously on the database now let us move on to the forth scenario which leads to the final property  forth property of the transaction  refer slide time  33  17  now this scenario four tells what happens if the database server crashes before the changed data is written onto a stable storage one could imagine several situations where the database values have been written  the transaction have committed but their final values have not been written onto the database for various reasons now whatever happens after the transactions says it has committed  its value should be preserved the value that the transaction has changed should never be altered after the transaction has been committed so you have no way of saying that the results of the transaction is lost after it has committed this is what we mean by the property of durability all results of the committed transaction are preserved after that point once the transaction has committed you have to guarantee this inspite of any other kind of failure that may happen to a database  refer slide time  34  39  these four properties are very important properties when we talk about transactions now to just repeat this properties  atomicty ensures which we actually derived from scenario one ensures that all the instructions of a transaction are executed in full or none so the first question of some part of the instruction being executed and some part of the transaction instruction not being executed doesn ’ t arise at all because we ensure that all the instruction of the transaction are executed in full or none  refer slide time  35  23  the second property which we discussed is a consistency property when multiple transactions are accessing data simultaneously  the access is protected through concurrency control mechanisms to ensure that the updates which are done by the concurrently executing transactions are not lost on the database this is what we actually mean by the property of consistency and we also mention that the consistency is ensured in database management systems by using a set of concurrency control protocols and we are going to study this concurrency control protocols in depth during this lectures  refer slide time  36  10  now the third property is isolation the isolation property ensures that the results of one transaction will not be visible to the other transaction till the transaction commits this ensures that there are no problems relating to partial results being available for other transactions we also mention that when this happens cascading aborts takes place  when one transaction results have been read by other transaction and the earlier transaction has to be aborted and to prevent this cascading aborts  we enforce the property of isolation on the transactions  refer slide time  36  58  the forth property is the durability property and it says that the effects of the committed transactions are not lost after commitment for example if you have deposited some amount into your bank and you want to ensure that it ’ s never lost after you have actually deposited the money into the bank it will never be lost  that is basically the durability property  refer slide time  37  26  now all these four properties put together are nicely known as the acid properties of the transaction as show here it is the summarization of the four properties that we have been so far discussing a stands for atomicity  c stands for consistency  i stands for isolation and d stands for durability so these four properties put together are called as the acid properties of the transaction and normal process will not obey this acid properties whereas the transactions in the database context will obey this acid properties now one of the things that we are going to look at through this lectures is see how this acid properties are realized by the database management system when we are actually executing transactions in the database  refer slide time  38  27  now we will also further elaborate this acid properties littler more formally by actually taking what happens and how this acid properties are ensured now as you can see here in the case of the debit transaction  all the instructions starting from the begin transaction to the end transaction will be executed in full or none which actually means that id 1  id 2  id 3 have to be executed in full now one of the things we are going to do is when there is a begin transaction  we record the state of the database now whatever happens after the transaction starts executing  if there is a failure we will ensure that you get back to that state by restoring the state to the original state if the instructions are not executed in full for example if your original balance  starting balance is 500 rupees and for some reason the debit transaction can not be executed  all the instruction restore the balance back to 500 this is what we mean by undoing a transaction the transaction  all the instructions which were executed partially  till completion of the transaction are rolled back which means all those instruction will be nullified we actually rollback on those transactions  so that effect on those instruction is nullified so this is what we mean by atomicity property we ensure that either all the instructions are either executed together or none of them are executed  refer slide time  40  17  now again  to stress again what really we were talking about consistency in case of both debit transaction and credit transaction access the balance data simultaneously  we will protect them through the concurrency control mechanisms a simple mechanism that we are going to use is we lock the database items and allow only transaction which acquires this locks to change the values of those data items and only when the transaction releases the locks on those data items  other transaction will be allowed to use those data items  this is a very simple technique the most sophisticated techniques that can be used for enforcing concurrency control mechanism but this is what we would like to do to ensure that the consistency property is enforced or realized on the database  refer slide time  41  14  now when you go to the isolation property  you are going to look at either debit or credit transaction results will not be available unless they are committed in one way these transaction have to hold on to this locks when they require and should not release those locks for other transactions till they are committed to ensure the values that they are modify are not available to other transactions till they finished execution  till they reach the state of end transaction which means that now they have committed their values and after that only those results will be visible for other transactions  refer slide time  41  56  now there are several ways in which we can ensure the durability property the durability property will ensure that you have backups sufficient backups  you have written all your logs committed transaction locks and there are various ways in which the effects of this are preserved to make sure that all the committed transaction values can always be obtained by using the backups and the transaction logs you are going to look at this property and how this is realized in detail as we go along  refer slide time  42  31  now we come into more details of what really happens with transactions for this actually we will introduce certain terminology to start with the idea is to get more formal with the transaction concepts  see them in more detail as we progress so far we have been looking at the properties very intuitively  trying to understand them in a very intuitive fashion now we will try to understand the concepts in the more formal way now there are two case in which the transactions can enter into one is a commit state which actually means when the transaction has completely executed all its instructions  it can enter into a commit state which actually means that all the reads and writes of the data items which is actually read can now be written back and there been safely written back of the database in which case we say that the transaction has committed itself now for some reason  the transaction has started executing but it can not commit the values of the data item that it has changed which it has read from the database then we say that the database has entered the state of abort which actually means that all the effects of the transaction will be nullified and database state will be left when the transaction start executed that is equivalent to saying that i actually had a begin transaction the state of the database when i started executing this transaction which is the begin transaction and i actually keep the state back to that initial state when the transaction started executing that is called the abort state so we have commit and abort a transaction could be either committing or aborting when it is says it has committed  it is writing all the values that it has read and changed back onto the database when it is says it is aborting  it is not committing any of the values that it has changed  refer slide time  44  46  so to emphasis the transactions are not just ordinary programs instructions  all our discussion today highlights the fact that additional requirements are placed on transactions to ensure that this acid properties atomicity  consistency and isolation are realized with the transactions  refer slide time  45  14  now here is the case where we can quickly see with respect to commit and abort  what really happens for the atomicity property now atomicity property says that a real world event either happens or does not happen now if you take the case where a person either books or does not book his tickets which actually means that when he is actually book his tickets  the transaction has committed the values when he says he didn ’ t book the ticket  it means that the transaction has aborted and again to give the state the think that it is not true of ordinary programs  a hardware or a software failure could leave files partially updated which is not the case in the case of transactions when you say i have booked my ticket  it means that you book your ticket that means the transaction booking tickets as committed and when it says it is not book the ticket means that it is not committed  it is aborted  refer slide time  46  26  now coming to the consistency property  you have to ensure that the set of integrity constraints that are specified by the transaction are all enforced when the transaction has executed now what we mean by this is transaction designer must ensure that assuming the database was in a state that is satisfied all integrity constraints when execution of a transaction got started then when the transaction has actually completed execution  we need to ensure that all the integrity constraints are once again satisfied in a simple way  the consistency can be ensured by saying that the transaction execution results in a serializable execution that is the transaction is executed as if all the operation have been executed in a serial fashion  refer slide time  47  19  we are going to look at that particular property right now in the next few minutes in the little more detail and see what does it mean by consistency preservation and isolation essentially will avoid cascaded abort as explained earlier  refer slide time  47  36  here is a simple case where isolation was given in a more detailed fashion it relates to when multiple transaction execute concurrently and you want to actually ensure that the final execution thus preserves the consistency by ensuring that one transaction values are not read by the other transaction till it has finished  refer slide time  48  09  now we will see this property of concurrency and isolation together by taking a simple example as given in this particular case as explained in this diagram  as you can see this here t1 has two operations op 1 and op 1 2 and t2 has two operations operation 2 1 and 2 2 now it ’ s possible that these sequence of operations can be interleaved in multiple ways on your database as you can see here one possible sequence is operation 1 1 executed first of transaction 1 and then operation 2 1 of transaction of transaction 2 then operation 2 2 is executed of transaction 2 and operation 1 2 of first transaction is executed as you can see here from the execution sequence of this  we may not be able to say that transaction t1 completed all its operations before transaction 2 is executed but on the other hand if there is a way  you can ensure that all the instructions of transaction 1 are finished before transaction 2 it is equivalent to saying that the set of instructions that are executed are all in a serial fashion now one of the requirements of operation 1 1 and operation 1 2 to be serializable in this context is they are not operating on the same account it is possible that two people are withdrawing money from two different accounts since there is no conflict in this particular case between the two operations  it doesn ’ t really matter even if o1 2 is executed later we can always say interchanged the operations since there is no conflict in this particular case and rewrite the set of operations as if they have been executed as o1 2  o1 1  o1 2 and then o2 1 and o2 2 when operation don ’ t conflict  it is possible for us to interchange operations and then ensure what we see the property of serializability that is all the operations of t1 have finished before t2  that is what we mean by serializability or in other sense what we going to say is all the operations of t1 since they are finished  t1 precedes t2 in terms of the operations have been executed on the database this is called serializability however let us say that the operation 1 1 and operation 2 1 conflict with each other in the sense that they access the same database item in this particular case  we can say that they are accessing the same account and the same balance and they are trying to modify the same balance they are not just reading but writing values  modified values onto the database in which case we say there is a conflict now operations will conflict if they operate on a same data item and one of them is right that is what we mean by conflicting operations when transaction is conflict we need to serialize the transactions and this is what we mean by conflict serializability the conflicting operations should be executed in such a way that we know that the conflicting operations are executed in a serial fashion which actually means that we can ensure that the operations on the database in this particular case if o1 1 and o2 1 have conflicting there have been executed one after the other and that decides how the transaction precede with each other  refer slide time  52  33  now let us see the following scenario where on o1 1 and o2 1 which are two conflicting operations  we say that t1 preceded t2 and let us say o1 2 and o2 2 are also conflicting now let us say as far as those operations are concerned  t2 precedes t1 now this is a scenario that will result in the transactions t1 and t2 not being serializable because as far as the conflicting operations o1 1 and o2 1 are concerned  the t1 is preceding t2 and in the case of o1 2 and o2 2 which are again conflicting the transactions are preceding in the other direction  t2 is preceding t1 so we can ’ t say as far as the conflicting operations are concerned t1 is executed before t2  one case t1 executed before t2 in the other case t2 is executed before t1 this is a very interesting thing which we going to study in detail in the next lecture we see how transactions need to preserve the property of conflict serializability only when transactions execute and they are serializable  conflicts serializable we say that the database is  the transactions have executed in the correct fashion on the database we are going to further study this property in tomorrow ’ s lecture in detail as concurrency control mechanisms the concurrency control mechanisms are expected to provide the property of conflict serializability they ensure that when transactions are executing concurrently  we can serialize them in a  transactions are serializable and that ’ s the property that is ensured by concurrency control mechanisms we are going to study the concurrency control mechanisms in detail in tomorrow ’ s lecture database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no # 18 transaction processing and database manager in the previous lecture we have looked at the basic properties of transactions namely the acid properties atomicity  concurrency  isolation and durability in today ’ s lecture we are going to see how these properties will be realized by the transaction processing system within the database manager we will take a few simple examples and through this examples we will illustrate how the transaction processing system will ensure the acid properties of the transaction  refer slide time  02  04  here is a very simple example shown in the slides here there are two transactions here which are shown t1 and t2 t1 is a account transfer transaction  transfer of money from one account to other account now 1000 rupees had been transferred from account a to account b by transaction t1  transaction two is an interest payment transaction so it is actually crediting into each account a 5 % interest into each of the accounts now what is shown here is this t1 and t2 operating simultaneously on the banking system now what we will do is we will try to understand these two transactions in terms of the various operations performed by these transactions what i am going to do here is i will write t1 as performing several transactions or several operations now the first operation that is performed by t1 is to actually take the account a and read the value of the balance that is there in this account so it is basically a read operation of the account the second operation is essentially to add 1000 rupees into this account and the third operation is going to be writing the value back the same thing is going to be done for the account b so i will actually show that as the fourth operation but the subsequent operations will not be shown here they are self-explanatory one can understand after that  the other operations now the suffix here shows that this is the first operation and this gives the transaction id one is the transaction id and the one here indicates that it is the first operation so operation 1 1 indicates that this is the first instruction of transaction this indicates that it is the second instruction of transaction 1 like that it is shown here now if you basically take a operation oi j  it indicates that this is j th operation of i the transaction this is the notation that we will be using now as you can see here this is basically a read operation on a and this is basically a write operation on a so we have between the processing we have the reads and the writes happening on the data items now we can also understand the transaction t2 also has shown in the slide as trying to do the following operations o21 is going to be a read of a and then the o22 operation is going to be an update on the value of the data item and then o22 o23 is going to be an item again a the rest of the operations as shown in the earlier case for the b  operation b  refer slide time  06  17  now what we are going to show you is what happens when these transactions are executed simultaneously on the database now a list of actions form a set of transactions as seen by the dbms as you can see here o11 o12 o13 o14 are set of transaction  set of operations constituting t1 similarly we have o21 o22 o23 constituting the set of instructions constituting transaction t2  refer slide time  06.52  now it is possible for these operations to get interleaved in the sense that it is possible for these transactions  operations of the transactions to execute in an interleaved fashion now when this gets executed in an interleaved fashion  we basically call that as schedule a schedule is nothing but a series of operations as executed by the database management system now you can see here it is possible for these two transactions to execute concurrently  a set of interleavings that were shown is the operation t1 are constituting a equals to a plus hundred is executed here then t2 is executed which is equivalent to saying a equals to this operation is executed here  followed by the t2 of b then t1 again b equals to b minus this is one schedule which is called a possible schedule  we can call this is as sc1 as one possible schedule as you can see here these two constitute transaction t1 these two operations  these two operations constitute t2  refer slide time  08.29  now there is another possible schedule also shown in the slide there in which case t1 and t2 are executed as shown here but the other two operations are interchanged this is executed before the other operation now when we actually have the schedules  one of the important criterion for this schedules to be valid is to see that these schedules produce proper consistent results at the end of the execution  refer slide time  09.07  now this schedule one can be seen as shown in the slide as read and the writes on the various data items t1 is basically is reading a and then writing a it is reading the old value of the bank account balance of account a and writing the new value for the balance here t2 is also doing a read of a and write of a because you are computing the new value of a by calculating the interest that is payable for this account similar way we can also write for write b and then read b and write b and read b and then again write b what we are going to show you through this example is what happens when these reads and writes are interleaved from the consistency point of view now we can see here the notion of what is correct from the execution point of view is shown here suddenly we don ’ t want transactions to execute one after the other because then the throughput of the system will come down drastically you want as many operations as possible should be executed in a concurrent fashion to actually increase the throughput of the system now when concurrently executed transactions at the end of it whether they can be translated into what is called a serial execution now in this particular case  what you would like to say is t1 executed completely after t2 this is one possibility or t2 executed after t1 as long as this is possible for you say  we call this kind of schedule as a serial schedule and that is what is actually shown in the diagram here a schedule is which is equivalent to one of the serial schedules is equivalent to saying that either t1 executed before t2 or t2 executed before t1 that is one of these should be possible  refer slide time  11.28  now to see what really happens when the execution is not serial that means when finally you are not able to detect saying that the execution of the transactions is not as per the serial schedule  refer slide time  11.51  i will take a very simple example and show how exactly will deduce the serial schedules let us take a case of a transaction t1 with operation x and then 1 2 which is actually a write of y now we can take another transaction t2 where basically it is a read of y and then a write of x now no matter how these operations are executed as long as it possible for you to say that all the operations at t1 have been executed before t2 which is equivalent to saying that if there is a schedule that says o11  x  and o12  y  has actually finished before o21  y  and o22  x  are executed by the database manager this is equivalent to saying that t1 finished before t2 this is what actually we mean by a serial execution of the transactions  refer slide time  13.26  that is t1 finished execution before transaction t2 started executing now it is also possible for you to also have the reverse order where all the operations of t2 have been executed before t1 this is a very simple and straight forward case where we can easily save on all the operations of t1 and t2 are executed in this particular fashion very easy to see that t1 has actually finished all the operations before t2 stated executing the only case where you will have problems is when some operations of t1 have been executed in such a way that they are interleaved with the execution of the operations of t2  refer slide time  14.12  then the problem of deciding whether the schedule is equivalent to a serial schedule that all the operations of one transaction finish before the other becomes a important requirement and that is what actually we are going to look at how that can be done in this particular case what we are going to say is all that will be required for us as a criterion whereas schedule is produced is two operations are said to conflict  we say the notion of a conflicting operation is when one of them is a write operation one of the operations is a write operation  refer slide time  15.15  now to give you the little more simplistic view  let us say t1 is trying to read a data item x and t2 is actually trying to write the data item x these two operations o1 of some i  o2 of some j are said to conflicting because they are operating on the same data item and they are conflicting with each other here as you can see here  t1 is reading the data item x  t2 is writing the same data item x since both transactions are reading the same data item and one of the operations is a write operation  we say that these two operations are called conflicting now whenever we have conflicting operations like this  the first inference is here o1 i and o2 j are conflicting now in transactions  if there is conflicting operations and there is a way this conflicting operations have been executed  let us say the conflicting operation in this particular case is executed is such a way that this is the order in a schedule now this order actually determines that t1 actually preceded t2 because it is conflicting on data item x and t1 has been executed before t2  refer slide time  17.02  and now this order should be preserved  no matter what happens with respect to other operations and as long as you preserve that order with respect to all other operations  we say that the operations have been executed in a serial fashion or the schedule is reducible to a serial schedule this is the concept of serializibilty now this is the important notion here is when transactions are executing concurrently we need to ensure that the conflicting operations are serializable  all the conflicting operations are serializable now here is a very simple case shown in the slide where it shows that where it is not possible to serialize  we need to actually abort the transaction now in this particular case it is shown that t1 actually is reading as we go to the earlier case  t ¬ 1 is actually reading the data item a and then writing data item a now if you look at the t2 is also reading data item a and then writing data item a now the other part of t2 is read b and write b now as you can see here with respect to data item a  the order between t1 and t2 is t1 is before t2 now if you look at the data item b  it is coming in the revere direction which actually means that as you can see here  on the data item b as far as the conflicting data item a is concerned  t1 is before t2  as far as b is concerned it is t2 before t1 as you can see this is on data item b and this is on data item a  refer slide time  19.18  now from this it is not possible for us to say whether t1 actually has finished before t2 or t2 has finished before t1 since we can ’ t now decipher which one has actually finishing before the other  this schedule is non serializable schedule and this is what should be avoided a non serialzable schedule shows that the execution of the operations will lead to inconsistency the database will be in an inconsistent state when we have the operations executed in a non serlizable way now in this particular case it is shown here that t2 can commit but t1 has to abort that ’ s what was shown in the slide saying that only b can come  transaction t2 can commit but t1 has to abort because t1 trying to commit here will produce a non serialzable schedule this is also shown  as if you go back to the slide you can see that this is shown as dirty reads  that is write and read conflicts as you can see b has been  t1 has read a and b values but it is reading the  writing the value at a much later stage and now the one of the later updates will be lost if you allow t1 and t2 to execute in this particular way one of the operations will be lost and that ’ s the reason why the schedule is not allowed we look at other kinds of conflicts that can arise when transactions are executing first kind of conflict that we saw is a write read conflict  refer slide time  21.08  we can also have a read write conflict and a write write conflict what a read write conflict shows is the example shown here is transaction t1 is reading the data item a and if its value is more than zero then it decrements t2 is actually reading a and it is decrementing the value of a now when t1 and t2 are operating concurrently  t1 is actually reading data item whereas t2 is actually writing on to the data item this is what we mean by a read write conflict on the data item a  with respect to data item a both transaction t1 and t2 are conflicting in terms of t1 reading the data item and t2 modifying the data item and this is what we mean by a read write conflict again you can see that in this particular case the value read by t1  if it is before the value is changed will be inconsistent let us say right now the value of a is zero and t1 reads it as zero  then it is unlikely to decrement that because it has read the value as zero now if t2 also read the value at the same time as zero and reads it as zero then it decrements it as minus 1 so that value is actually in conflict the actual value that is read by t1 and t2 are not correct another example for conflict is the write write conflict where two transactions  both of them access the data items and tries to modify the value of the data item this is a case where it is shown here as two people simultaneously trying to credit into the same account is shown as a example of a write write conflict as can be seen here  t one is actually writing the value into the account  t two is also writing a value in the account a as well as into b and t1 at a later stage is trying to write the value into b now as you can see in this particular case  the writes on the same data item will be conflicting leading to non serializable schedule so typically this is what we mean by a write write conflict to summarize what we are actually seeing  i will give a simple example of an operation and show how exactly is the conflicts serializability is to be achieved if you take a transaction ti and say ti as a operation oip and data item x and you have a transaction t j which has an operation q on x now we say that these operations are conflicting if one them is write now when you say this is write and this is a read  you have between oip x and oj q you basically have a read write conflict the x data item is read by transaction t i and it is been modified by t j so this is basically a read write conflict now if you say that this is a write write  typically what you see is between operation o i p x and o j q x you see a write write conflict  refer slide time  25.01  now when there is a conflict between the two operations  whether it is a read write or a write write conflict  you need to actually serialize the operations by actually saying that they are executed in a serial order which is what we mean by conflict serializablitiy now whenever there is a conflicting operations  we need to actually serialize the two operations which is known as the conflict serializability in this particular case oi p  x  and oj q  x  are the conflicting operations and they need to be serialized in a particular fashion and this is what we mean by conflict serializability we are going to see in later lectures  how the transactions are executed by the transaction manager to ensure that conflicting operations are serialized or serial schedules are produced by the transaction manager one of the simple technique that is used is what we see as a two phase locking and we are going to study that two phase locking as a technique for achieving conflict serailizability later in our lectures now here is actually what is shown as how exactly the transaction manager achieves some of the properties that we have been discussing  refer slide time  26.52  the dbms ensure that a transaction either completes and its results are permanently written this is what we mean by committing a transaction or no effect at all on the database  this is equivalent to saying that the transaction has been aborted so we have two states for the transaction  either a commit state or an abort state in the case of a commit state  all the operations of the transaction or executed in full and then they are committed in the case of abort  no effect at all on the database as far as that transaction is concerned now the idea of transaction manager is it controls the execution of the transactions as we saw in this particular case  it controls the execution of the transactions in such a way that the operations of the transactions are serializable similarly if you take the recovery manager  recovery manager is responsible for undoing the actions of transaction which do not commit this is a equivalent to saying that the recovery manger is responsible for ensuring the property of atomicity all actions of the committed transactions survive any kind of hardware or software failures this is actually known as writing the committed transactions on to a stable storage what we are going to do is we are going to look at little further into how the recovery manager ensures that properties of atomicity and durability  refer slide time  28.28  now what are issues involved in ensuring atomicity and durability ? the following errors can occur when a transaction is executing first is it could relate to logical errors for example you are trying to withdraw some money from a bank account  it is possible that the account itself doesn ’ t exist or the account doesn ’ t has sufficient funds in all this cases  transaction can not proceed any further this is what we mean by logical errors the transaction may have to abort because of logical errors there could be system errors for example it is possible that there are problems of network  there are problems of system failures  temporary failure or power failure in which case when the power comes back  you need to know what really happened for your transaction with respect to already started transactions a simple example could be  you go to a atm and try to withdraw money from the atm and the power  when you actually press the button for withdrawing the money  the atm stops functioning atm failed due to various reasons you would like to know whether their system is actually debited the amount from the bank  from your balance or not that is basically system errors there could be crash  system crashes  there could be a hard disk failure  the disk head could have been corrupted so there could be various reasons why the system didn ’ t ’ perform  it could be a system crash so all these errors are possible when the system actually goes into any of these errors  you want to understand how exactly the atomicity and the durability properties can be maintained a simple example trying to illustrate this point will be something like a file which all of us open on a windows machine or any of our unix machines now here when you open a file in a editor mode and tries to edit your file  there is no guarantee in terms of what happens when a power fails because the file could be in a very corrupted state there is no guarantee for you in terms of the state of the file which all of us know we keep repeatedly saving the file  when we actually entering or writing some document  we try actually saving the document as many times as possible  so that when the power goes off or something else happens we still save the portion of the work we have actually done we don ’ t lose the file because of power failure all the work that we have done now the same thing can not happen in the case of database systems because here the more critical data that is been in saved in the file so we need to ensure that whatever happens when any of these failures happen  the system is still in a predictable state that is the difference between ordinary file systems implemented by an operating system and a database implemented by commercial systems they ensure that whenever these things happen  still the properties  the save properties for the transaction  the acid properties of the transactions are retained  refer slide time  32.11  to explain how these properties are retained by the system  we need to also understand the different storage types that are available in a computer system a simple volatile storage  we basically look at a simple volatile storage  this does not survive system crashes that means when the system actually crashes the storage is lost  the storage is volatile it is lost the minute the system crashes when you talk about non volatile storage  the system actually survives these crashes that means the storage is the  whatever you write into the storage is not lost when the system crash occurs a simple case is whatever is there in the main memory is lost when the power goes whereas if you have written it onto your hard disk  it survives a power failure because it is written into a more non volatile storage now we also have a concept of a stable storage which is an abstraction of maintaining replicated copies in multiple non volatile storage media  so that whenever higher disasters occur we still have a way of getting our data back and that is we mean by the concept of a stable storage now what we are going to see is how this concepts are used for actually achieving the atomicity and durability properties in the transaction manager  refer slide time  33.55  what we are going to show here in this particular case is what really happens when transactions have to roll back the rolling of the transaction has to happen mainly because of logical errors or the system crashes and hence it has to be restored back to a previous state transaction abort requires roll back which means undoing all the actions of that particular transaction now to ensure that roll back of the transaction occurs properly  what we have to do is all the writes of the transaction have to be properly recorded in what is called as a log file the log file retains all the information relating to the writes of the transaction and this will be used when the roll back has to occur now if the system crashes  all active transactions during the crash need to be aborted when the system comes back this is equivalent to saying that they will all be rolled back and the information that is there in the system in the log file will be used to properly undo the transaction activities  whatever the transactions are being doing what we are going to do is again in this particular case  we will take a very simple example and see how exactly this happens  refer slide time  35.22  now here is the case where case where the logs are maintained and how this logs are helpful in ensuring the atomicity property or the how the atomicity property will be realized by the database manager what is shown here is a simple case of writing the logs before the transactions starts executing and making sure that it is carried over whenever the data item is being written or a new value for the data item is being written what you can see here is the first log that will be written as far as the transaction t one is concerned is what is called is the begin log  transacting begin log now what we have is basically a relating to a transaction  every transaction is preceded by the begin transaction as a keyword now this begin transaction actually tells the database manager to write what we see as a log  this is the transaction begin log now this is a  the begin log has to be written onto the transaction now in between the transaction does various operations as we have seen their  it ’ s possible that there are several operations which are done by the transaction in between and then we have and end transaction now this is actually the last instruction that is executed by this transaction  so we will have a what is called a commit log indicating that the transaction has actually committed which is equivalent to saying that all these operation have been successfully executed so between begin and end  at any point of time when there is a crash  we need to actually recover back to the starting point and this is what we mean by actually roll back what we are calling as a roll back is basically rolling back all the things that a transaction is done to the beginning this is what is meant as a roll back now what we do is we actually ensure that whatever the transaction is doing  is written on to this log and this log will be used for rolling back the transaction whenever a crash occurs  refer slide time  37.57  for example you can see that in the slide it is shown that t i writes a data item now as you can see the first one transaction starts  there is a transaction log t i start now the second thing that you notice is t i writes a data item now there is a old value and a new value old value is the old value of the data item and the new value is the new value of the data item x so there is a log that is written there which shows that t i x old value and a new value is shown here  this is how actually the log is written whenever there is a change in the data item  we basically write the log and now this log shows what was the old value and what is the new value now when you come to the last transaction  basically you have a commit log that is a t i commit log so as shown here  we have a begin log and a commit log and in between whatever is happening is being recorded there as shown in the slide there so one of the things that we are going to look at now is how this logs can be used for recovery purposes how exactly this logs can be used by the database manager to ensure that whenever those kind of failures that we are talking earlier occurs how the system will recover back from those failures  refer slide time  39.54  now one of the things when writing this logs is one of the things that one should remember is the id ’ s of the transactions are stored so that we know to this logs pertain to which transactions  so transaction id ’ s are appended when the logs records to identify the transactions for which this have logs been produced logs are replicated and stored in a stable storage this is also very important because in the logs themselves are lost then there is no way you can recover back logs only assuming that the logs are written on to a stable storage  you can ensure that the transaction can be made to recover but if the logs themselves are subjected to failure then you will not be able to recover back and hence logs are replicated  one of the assumption we make it logs are replicated and they are stored in a stable storage so when we say a log is written  we assume that the values relating to the log have been written on a stable storage and it is possible for us to recover this information at any point of time  refer slide time  41.10  now as you can see here we are also showing how this log entries will also be ordered  ordering of the log entries we say a transaction t are can commit only if the log entry relating to that is saved on a stable storage this is equivalent to saying as you can see here  when you write this commit log this commit log is actually written on to a stable storage then we say the transaction is committed this is the point where it is possible for the transaction now to say that it is committed now before this is actually written  this log is written all the other entries before this pertaining to this transaction should have also been written onto the stable storage you should never write the commit log before all the other log entries relating to this transaction have been saved on the stable storage now only after the writing the entries relating to the logs  you should write the data items themselves after this point onto the stable storage this is very important  these steps are very important because if you perform them in any other order  you will have problems in terms of recovering back first requirement is all the log entries relating to this transaction should have written onto the stable storage in the first instance  before you are writing the commit log only after writing the commit log  the data item values pertaining to the transaction can themselves can be written onto the stable storage the reason for this is simple if you don ’ t write the log values first on to the stable storage  there is no way if something happens to recover from that particular failure for example if you have written the data value onto the stable storage  now something happens there is now way of finding out what is the state in which the transaction is unless the logs are written properly so logs are the bases for the database manager to find out what is the state in which the transaction is when a failure is occurred and hence it is important for you to first write all the logs relating to the transaction then write the commit log and then write all the data items onto the stable storage this is how one needs to order or write the various things relating to the transaction  refer slide time  44.09  we will take a very simple example a simple example in this case to see how the two transactions can really execute writing their logs in this particular case  it is shown t one and t two t one is actually reading certain data items and writing certain data items similarly t two is also reading and writing certain data items for completeness sake  we also have shown the initial values that are there in the database when this transactions t one and t two start executing as can be seen in the slide  the initial values of a are a is 100  b is 300  c is 5  d is 60 and e is 80 now t1 when starts executing  it is going to read the values of a will increment by 50 then read the value of b increment it by 100 then write the value of b back into the system then will read the value of c now c value is inc is double then the value of c is again written at the end of it a is recomputed as a plus b plus c and the value of a is written if you look at the transaction t2  t2 is actually reading the value of a it is incrementing it by 10 then it is actually reading the value of d  it is decrementing it by 10 then actually reading the value of e and reading the value of b then e value is computed by adding e plus b and then writing the value of e then d is recomputed as d plus e then finally the value of e is written what we wish to covey with this is the two transactions are simultaneously reading several data items and trying to modify it is equivalent to saying that there is set of operations which are going simultaneously in terms of reads and writes between these two transactions  refer slide time  46.21  now let us understand what really would have been the logs that would have been produced when this transaction t1 and t2 start executing now when t1 starts executing you can see that there is an initial log given there as t1 start this actually shows that t one started executing that is the start log for the transaction t t1 is the id of the transaction now when actually t1 tried writing the value of a  the old vale and the new value are actually stored in the database  in the log as you can see in this particular case  this log shows that a ’ s old value is 100 and the new computed value is 150 similarly when b has actually been recomputed  we have a old value for b as 300 and the new value has 400 similarly for the value of c  it is 5 and 10 and finally when the last computation for a actually took place a is 150 and then old value of a is 150 and then the new value is 560 it is at this point of time actually the t1 finished executing all its instructions and it is ready for commit and that is the time the commit log is written  t1 commit log is produced at that point of time a similar thing is shown for t2  as you can see there is a start log for t2 and then we have various new values and old values for the data item that are done by t2 are also shown here and finally commit log is shown for t2  refer slide time  48  08  now what really happens is once a failure is occurred  undo all transactions for which there is a start entry in the log but there is no corresponding commit entry this is equivalent to saying that i will just go back to the previous slide to show what really we are we are saying now we can say if t when you actually a system crash has occurred  you can see there is a start log for t1 but corresponding commit log is not present then all that we have to do is you have to actually undo whatever has been done by the transaction for example in this particular case  if t1 has actually modified the values of a b c then those values have to be reset back to the old values from the new values that is equivalent to actually saying that we have undone the transaction because it has not reached the commit state now the other case is redo all transactions for which there are both start and commit entries because these transactions have already gone to the finish stage we are going to redo the transaction for all those which commit  a start and the commit entries are there  refer slide time  49.26  and also if typically we have  if the data items are not yet written this is what we mean by actually looking at whether the  not just the log entries but the data item entries are not still written then we need to redo but if the data item entries are also been stored on the stable storage  there would have been along with commit log there would have been a complete log and the log book showing that the transaction has completed all the operations relating to it  in which case the redo need not be done and this is typically achieved by what is called a checkpoint record  refer slide time  50.04  if the checkpoint record is entered into the log  whenever the system writes the data item values onto the database the effect of the write operations are all committed on the transactions now all transactions whose commit log entry appears before the check point entry need not be redone in case of a system crash so the checkpoint is a place where you can decide whether when a commit log exists whether you have redo or need not redo those transactions this is how exactly the database manager will ensure  the transactions are executed atomically and they satisfy the property of durability what we are going to see in the next class is how the concurrency control properties of the transactions are realized by the transaction manager database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no # 19 foundation for concurrency control in the last class we have been looking at the basics of transaction processing systems what we have done is to look at the basic properties of transactions when they execute in the database system we looked at the essential properties of transactions in database systems and we have been looking at the four properties that are essential for programs  transactions atomicity  consistency  isolation and durability  refer slide time  1.56  atomicity property is required to ensure that all the instructions in the transaction is either executed or none of them are executed so the property of atomicity ensures that all the instructions either get executed or none of them will be executed the property of consistency ensures that when more than one transaction operates on the database  the consistent state of the database is maintained that is they don ’ t really malign the values written on to the database the property of isolation provides that the effects in one transaction are not visible to other transaction till it has committed all its values to the transaction the property of durability ensures that the transaction is returned its value on the database  it will be stored permanently on the database that ’ s the property of durability we have been looking at the four properties and trying to understand more on how the consistency property is realized in database systems what we are going to do in today ’ s class is to look at the foundations for the consistency management which is also known as the concurrency control mechanisms in database systems what we are going to look at it is the basic concepts of transactions from the view of concurrency control and this is what we call as foundations of a concurrency control in databases what i am going to do in today ’ s class is show the basic properties of  basic foundations for concurrency control in databases by first introducing the notion of a schedule what is a schedule and what are the different things that we can understand by the concept of a schedule the second thing that we are going to look at as part of the schedule is we look at how exactly we can produce what are called serializable schedules this is basically the basic notion of ensuring consistency in databases so we are going to use these two things  one we are going to look at what is a schedule  what is a meaning of a schedule and how exactly serializable schedules can be produced after understanding these two things what we are going to do is in the next class  we are going to see some protocols that are their in the database systems that ensure that this schedules produced by the database are serializable schedules so we look at the protocols in the next class but in today ’ s class  we will understand what we mean by a schedule and what we mean by a serializable schedule  refer slide time  05.28  now let us go and understand the meaning of what we mean by scheduling now the meaning of a schedule in database transaction system is  it is essentially a set of operations performed by the transactions on the database we will consider a very simple schedule let us say sa and then here what will do is we will write a series of operations performed by the database  by the transactions on the database we are going to understand first the notion of what are these different operation before we write the actual schedule what i am going to do is i will say essentially the database operations could be either a read operation which means that the read an item from the database read x shows that this is an operation trying to read a data item  read an item x from the database what this means is if this item is not in the main memory  this item will be fetched from the disc and will be transferred into the main memory of the database and after that this will be used as a program variable by the program to do some operations that means the value is available  now it can perform some computation using the variable the other operation that the database transaction can do is what we see as write x write x essentially means that write value of x back to database which means that the updated value of the transaction has to be now returned onto the database we will also have two extra operations which are performed either a commit or an abort at the end of the transaction execution commit means we are essentially committing all the things that you have done this is at the end of the execution of the operations a transaction can issue what is called a commit command  commit actually means do all the changes and then abort means essentially discard all the changes so typically the transaction decides at the end of the execution whether to store the values back on to the database or discard them these are essentially the operations that we are interested when we are dealing with a database a schedule essentially consists of read operations  write operations  a commit and a abort command  refer slide time  08.44  now what will do is we will further define the schedule much more clearly by taking a set of transactions that are operating on the database simultaneously for example take the case where we have the reservation system where passengers are trying to book their ticket simultaneously they could be one passenger who is trying to travel to delhi by rajdhani express and he will try to book his ticket for that particular train there could be another passenger simultaneously trying to book for the same train for going from chennai to delhi which actually means again both of them  both these operations simultaneously operating on the database now both of them should get different seat numbers when this operates simultaneously if both of them get the same seats  we actually leaving the database in the inconsistent state because two passengers can not get the same berth to travel from chennai to delhi so that is what we mean by looking at  i know when transactions are operating simultaneously on a database  how exactly we can produce consistent results on the database now in this particular case what we can see is there could be the first transaction t1 which reads the current reservation information from the database we will just abbreviate as read as an r symbol so and we will give an prefix r one here to indicate this is being done by transaction one this is looking at the rajdhani express availability which is given by r one x now after actually checking this  it will try to write the value back onto the database saying that it wants the reservation for this in some cases the passenger finds that he doesn ’ t have the money  at this point of time he can discard the changes which means that the transaction can get into an abort state after reading the availability but nor booking the final one it is possible for various reasons the passenger didn ’ t have a lower berth as he desires  so in all this cases the transaction after started could abort the operation of writing this values back on the system which means that essentially the passenger is not interested in booking the berth for himself after reading the values now let us say this basically transaction commits at the end of it which means that you have got the berth and your writing this value back this is what all the operations that could be performed by the transaction t1 now if you take basically another transaction t2  it could also be reading the value of this rajdhani express availability for a berth and then writing the value back and then later committing now if you basically look at  these are all the operations of the second transaction it ’ s possible that these two operations of the transactions can be interleaved in any order when they are executed on the database what does that mean ? it means that it ’ s possible that  i could execute r1 x of t1 then r2 x of t2 then w1 of x and w2 of x then say i commit c1 and c2 this could be one possible execution sequence because these can be interleaved in whatever fashion that is possible this is basically what we mean by a schedule sa or some schedule a schedule is nothing but a sequence of operations that we are performing on the database from transaction t one to t n some n transactions now the transactions keep continuously executing on the database which means that as the transactions are coming  we are executing the read and write operations relating to this transactions and either committing or aborting the transactions at the end of what they perform and this process continues now as it ’ s happening  we need to ensure that whatever operations are performed by the database is actually leaves the database in a consistent fashion for example we can look at the schedule that we have just produced and see what would have happen if the sequence of instructions are executed as shown in a schedule s  sa in this particular case now as you can see here the value of x  let us say at this point of time is basically 10 at the start of the execution so read x read one of x would have resulted in reading the value of x as 10  read two x will also result in reading the value of x as 10 then a write would actually resulted whatever computations that was done and writing the value let us say actually after this computation  i actually write a value of x equals to 5  i subtract 5 form 10 and let us say the w to actually adds 5 to 10 which means that it results in 15 and after that c1 commits which means the value of x will be written as 5  when c2 commits value of x will be written as 15 as you can see here  if the transactions have been executed one after the other  the end result would have been different x  the end value of x would have been different from what was actually produced here  refer slide time  14.38  if you carefully notice the effect of the first transaction are last on the database if because the second transaction also read the same value of x  not produced by the first transaction and hence this will result in an inconsistent operation of writing the values onto the database we will characterize this consistency more carefully by looking at schedule and then trying to characterize the schedules in terms of how they basically  whether they are consistent schedules or whether they produce what we see as consistent results on the database now a simple case is where the transaction t1 is written assuming that this is the only program that is operating on the database let us assume that t2 is also written assuming that this is the only transaction that is executing on the database this requires that t1 is consistent as long as it is executed from start to finish consistent from start that is start to end  all the instructions are executed without any other transaction seeing the values used by t1 similarly the same thing is true with actually t2 which actually means that it will also assume that the start to end is executed as far as t2 is concerned without being interpreted what this means is either you execute t1 completely before t2 or you execute t2 before t1 this is a very important notion here of saying that i have what is called a serial schedule a serial schedule is one where the transactions are executed in such a way that new transaction is executed only after finishing the earlier transaction so in this particular case  if you say a serial schedule all the transactions should be executed one after the other for example if i have n transactions  there should be a mechanism by which i categorize t i less than t j less than t n like this which actually produces a serial schedule the only problem with the serial schedule is this is very limiting because it ’ s possible that these transactions can be executed concurrently  simultaneously still actually producing correct results for example  let us assume that t1 is booking for rajdhani express and then let us say t2 is trying to book the reservation for let us say trivandrum mail now there is no conflict between these two which actually means that even when these two execute concurrently  there is not going to be any problem in terms of the end results because they are not conflicting with each other  refer slide time  18.20  so by unduly restraining that i know the transactions should be executing one after the other would only affect the database performance we can  when they are not conflicting suddenly we can execute them in a parallel way and get better performance from the database rather than enforcing a serial order this is the first important concept of trying to look at a serial schedule now what we will try to do is how exactly one can think of a serial schedule and produce a serial equivalence schedule not exactly serial schedule but equivalent schedule to a serial schedule now what we do is for this  we will define the notion of equivalent schedules what this means is two schedules can be seen to be equivalent under certain conditions for example  let us take a schedule sa and a schedule sb and define what we mean by an equivalent schedule two schedules are equivalent if basically all the operations which appear in sa also appear in sb for example for transactions t one to t n  all the operations will define all the operations appear in both schedules now after this point to define equivalent schedule  we need the property of saying what kind of equivalence is this between the two schedules one is to say as i have actually shown in the last slide that when they are actually not conflicting  it doesn ’ t really matter how the operations actually appear in the schedule s a and s b for this what we define is what we call as conflict serial ability which actually means that only when transactions are conflicting with each other  those operations alone need to taken care  other operations need not be  no they can be executed in any possible order what this actually means that you need to focus between the two schedules s a and s b on what we see is the conflicting operations and ensure this two conflicting operations are done in the same order in s a and s b  refer slide time  21.22  now for this  i will define what it means to say two operations in transactions conflict conflicting operations are the following now one of the operations of the transaction is basically a read operation let us say read one of x and the other operation is essentially a write operation this is conflicting because the transaction t1 and t2 are operating on the same data item x and one of the operations is a write in which case we say these two operations are conflicting with each other there are also other probability where the first operation is a write and the second operation is also a write which essentially means that t1 and t2 again will be conflicting with each other with respect to this write operation  refer slide time  22.39  so from this we can infer that  if one of the operations is a write and two transactions are operating on the same data item in this case x and one of the operations is a write then we say that these operations are conflicting now all the executions that a transaction does need to worry about how these conflicting operations are executed in a schedule now to give this notion what we say is all that we will be worried about is conflict serializability that means you don ’ t need to really concern yourself about serializing all the operations but you have to actually do what is called the conflict serializability that means when operations are conflicting  you have to do what is called the conflicts serializability now in this particular case  let us say the two operations r one w as shown in the last slide which means that r one x and w two x  if they have performed this operations one after the other  it essentially means that t one has executed before t two as for data item x is concerned now it could be the other way round also  depends on how this is done in the schedule  how exactly this conflicting operation is performed but suddenly if r one x is performed before w two x  this is the order as far as data item x is concerned now if the transactions are also conflicting on another data item  let us say y and on y database has actually performed operations such that w two y occurred before let us say w one y on the database item y if you actually want to order this transactions then it is going to be t2 before t1 as far as y is concerned now if these two operation occur in this order in a schedule it essentially means that there is no fixed order as far as t1 and t2 is concerned because as far as x is concerned t1 is before t2 as far as y is concerned it is the other way around t2 is before t1 which exactly means that i no longer can infer from this t1 occurred before t2 or t2 occurred before t1 which essentially means that on the conflicting operations  there is no way to actually serialize the transactions by saying t1 before t2 or t2 before t1 in which case such an execution is not obeying conflict serializability because the conflicting operations are not serializable in the schedule  refer slide time  25.35  now to actually give the notion how exactly this can be further looked at two schedules s a and s b can be seen to be equivalent  if the conflicting operations appear in the same order between these two schedules which means let us say i have a schedule where there is a set of operation that are performed in this particular fashion on schedule a if they actually are performed in the same order in s b  there could be some other operations interleaved but then as long as the final order that i see between these two conflicting orders is the same then i say these two are equivalent schedules now this won ’ t be equivalent if the order in which they appear here is different from each other for example if w x in the other schedule comes before r one x then they are not equivalent schedules if the operations are not conflicting  it doesn ’ t really matter in what order they are appearing for example let us say there is a data item here in this schedule x on which actually transaction one reads this here and then there is another data item which the transaction two is actually writing which is z in this particular case now these are not conflicting operations now even if you change the order of this operations  it still doesn ’ t matter because they are not conflicting operations which is equivalent to saying that even if you now transfer w2 to before to r1 x  it ’ s still is okay for me because these operations are not conflicting and hence we don ’ t really care to actually worry about the order of non-conflicting operations since in the first one  r one x we just recap what we are trying to do here two schedules s a and s b are equivalent as long as the conflicting operations appear in the same order between the two schedules in that sense the two schedules  one and two as shown here three cases where s a is you know some random order where r one x appears before w2 x  r1 x indicates that this is a read of transaction one on x this is read write of transaction two on x since these two are conflicting  the way in which the schedule a there appearing is the read x is before the write x if it appears in the same order in s b also then we say they are equivalence schedules that all conflicting operations appear in the same order in the two schedules we say they are conflict equivalent conflict it terms of equivalence  they are in terms of conflicting operations they are equivalent schedules whereas you can see they are not conflicting  it doesn ’ t really matter in what order they appear  refer slide time  29.22  now to introduce the notion of how we use this conflict equivalence in actually deciphering whether schedules are schedules can be use to decipher whether they produce consistent results we say a serial schedule is always a consistent schedule this is the benchmark to say that i produce serial schedule then it is consistent the simple reason here is t1  all operations of t1 executed before all operations of t2 i see this as consistent execution because i am able to execute all operations of t1 before t2 and hence the database is consistent what it is doing is consistent now when i have a schedule which is not equivalent to a serial schedule  i will try to change the operations which appear in the schedule s a i transform now this to s a dash but this is an equivalent schedule as long as the conflicting operations are same between s a and s a dash  this is still an equivalent schedule this can further be transformed to s a double dash and finally this s a double dash becomes a serial schedule which actually means that i have been able to transform a schedule s a into a serial schedule now we use this notion to say that s a is a serializable schedule  not serial schedule but it is a serializable schedule in terms of the conflicting operations  the way i see executed this conflicting operations is same as s a double dash a and hence s a is basically a serializable schedule now what we are interested in is producing the serializable schedule because serializable schedules can be reduced by swapping the non-conflicting operations in whatever way you want into a serial schedule  refer slide time  31.36  this is in effect conflict serializable schedules  conflict serializable schedules what we are  as long as a schedule is conflict serializable and at the end of the execution you are able to show that this schedule is equivalent to a serial schedule  s a is a consistent schedule or the operation of s a is consistent and that is what we are interested as far as serializability is concerned this is a very important notion of actually being able to serialize the transactions produce serializable schedules we will try to look at now is how exactly the notion of serializability will be used by database transactions to produce consistent schedules now for that what i will show is to start with  as the database is operating  transactions will be coming in at any given point of time into the database imagine for example  this is a railway reservation system which means that the passengers keep coming and keep reserving the tickets at any given point of time which means that there is this set of schedules  this set of transactions t1 t2 t n which keep generated at different points of time now as they keep arriving into the database  some operations of t1  t2  tn will be executed here which actually means that to just produce this we will say o1 x is the operation of transaction one  oi j is a general operation on a data item y on the database and this is how this operations are executed as far as the database is concerned this is what we actually mean by a schedule now since the database will be operating continuously  these transactions keep coming regularly into the database it is not possible at any given point of time to actually close the transaction  close the schedule and say i have actually looking at a particular schedule what this requires is at a given point of time if you want to analyze  you need to put a break point and say i will actually take what is called a projection of this for the complete schedule which means that all those transactions which have actually committed or aborted as far as the schedule is concerned whose operations are all performed that will be included in this complete schedule let us say up to t some r  i have been able to now do the all the transaction execution then i will say i will execute from t1 probably i will do a slightly change here and i say t r to just make sure that we get the thing right  t r comes later which actually means that up to t1 to t n these are completed transactions that means the complete projection of schedule s will include up to t1 to t n which means that my schedule s which is on a partial schedule of all the transactions coming in which includes the t r here this t r actually goes into this schedule here and then this from this actually  i am actually projecting from this set here to complete s  refer slide time  35.20  now which actually means that t1 at a given point of time the t1 to t r are the total set of transactions that i can consider but t1 to t n is a completed set and i am actually looking at complete schedule which means that all the operations of these transactions have been included in the complete schedule now when you actually take the c of s as the projection then we want to apply the notion of whether this schedule produced is a correct schedule or not this is when we are going to apply the equivalence this schedule let us say is called s a now i apply on s a and then see whether this s a is reducible to some serial schedule a simple check of seeing whether a serial schedule is being produced or not  what we can see is a simple algorithm which constructs a graph showing how the transactions are executed in your system  refer slide time  36.33  assume that actually t1 came into the transaction system  now you try to actually see t1 put it as a point in the graph now let us say another transaction actually t2 comes into the system now assume that t1 and t2 actually conflict on a data item x and then this conflicting operations on data item x are executed in such a way that t1 appears the operation of t1 appears t2 then provide arc showing that there is a precedence relationship between t1 and t2 showing that t1 comes before t2 as far as this operation is concerned now we assume that there is another transaction which came t3 and this is conflicting on let us say an item z and this is the operation as far as this is conflict is concerned let us assume now between t3 and t1 there is a relationship in terms of conflict on y and if in this case t3 is executed before t1 we have essentially a cycle in this graph which means that the conflicting operations actually in terms of the graph of forming a cycle what is the meaning of this cycle ? assume now t1 is less than t2 as far as first arc is concerned t2 is less than t3 as far as second arc is concerned as far as the third is concerned t3 before t1 which shows that it is not possible form this to actually say any particular order in which these three transactions have executed the last one will be wrong because by a transitive relationship t1 should have finished before t3  refer slide time  39.03  and this is what exactly is done to show whether a schedule is a serial schedule or not there should be any cycle if there is a cycle it shows that it is a non serializable schedule the presence of the cycle in the transaction graph is shown to produce non serializable schedule because a cycle prevents you from coming with a order in which the transactions are put in a particular order of one being finished before the other and hence this will not produce a serial schedule i think this is very important as far as the concept is concerned because we use the transaction graph to understand whether a protocol is basically produces serial schedule or not as a later lecture towards the next lecture  what we are going to build is several protocols for actually building the or executing the transactions essentially these protocols will try to construct the transaction graph in such a way that an incoming transaction is put in the correct order as far as the sequence is concerned all that you want to do is you don ’ t want a cycle in the graph  you are not trying to produce a cycle in the graph and the protocol has to ensure that there is no cycle in this particular graph and that ’ s what exactly we can understand for example imagine that there is currently a current transaction graph looks something like this let us say i have three transactions active in my database and this is the current sequence as far as the conflicting operations are concerned let us say a t4 now comes into the database at this point of time i have several options of where i can put this to avoid a cycle from coming in into this particular graph it is possible for me where the protocol always allow the transaction graph to grow only in the forward direction which actually means that it is possible for me to keep this t4 here that is one possibility or i can put t4 here or i can put t4 here which actually means that the graph goes only in the forward direction and when it goes in the forward direction it prevents any cycle from occurring because you are not going to but a backward arc as long as you don ’ t put a backward arc  you let the transaction graph only in forward direction  refer slide time  41.38  it is possible for you to avoid a cycle in the graph the other possibility is it is possible for the protocol to decide to put it even before which actually means that if it can be made to read the value  let us say there is a write x here and i let this transaction read the value of x before this is modified then it is possible for t4 to be put before t1 even when it is coming after the graph in that sense it is possible that t4 before t1 also doesn ’ t produce a cycle and hence this is also a correct schedule so it depends on how exactly the transaction graph can be allowed grow by these protocols essentially the concept is a graph is constructed and a cycle is prevented from happening in the graph so i think we understood now the basics concept of how exactly the serializable schedules is used by the transaction system now what i am going to show in the next few minutes is to look at other kinds of consistency requirements an interesting thing to understand as far as consistency is concerned is to look at basically t1 executing before t2 is a correct thing to happen but this need not be the case for example if you look at debit and a credit transaction it is possible that any number of debits and credits can be interleaved as long as the debit occurs as one unit and credit occurs as one unit now this is very interesting because we can start looking at what is a operational semantics and try looking at whether the way the transaction execute is consistent or not to give a more deeper treatment of this we will take a simple example and then see what exactly we mean by this understanding the semantics and seeing whether the execution is right or wrong as far as the database transaction execution is concerned for example imagine i am actually having account and i do a debit on my transaction which actually means that this is basically withdrawal and i actually add some numbers after that means i credit into my account which is equivalent to saying that i read the value of x here and then i basically add some number here as long as the read of x is consistent with respect to the write  it does produce consistent results what this means is there is basically a write that is happening on x before the read is happening on x now this write can be by any transaction  let us say this is by the transaction i and this is by another transaction j this is the relationship between the two transaction in terms of  i am actually reading the value let us say the j is reading the value produced by i that means t i has actually produced the value of x which is being read by t j  refer slide time  45.38  now as long as this relationship is maintained between transactions in terms of how exactly they read the values of the previous transactions and this is actually maintained between the two schedules  we say that two schedules are equivalent in terms of views this is called the as appose to actual conflict serializability  this is called the view equivalence of schedules now what this means is two schedules s a and s b are view equivalent as appose to conflict operations equivalent schedules  they are view equivalent schedules if the way actually read operations and the write operations are related is they actually read between the two schedules  the operations are actually the same in terms of the way it has been produced and read now if this order is changed between the two schedules then it is basically not  they are not view equivalent the final writes between the two schedules also have to be  this is first requirement the second requirement is the final write operations are same between the two schedules the same in both schedules these are the two conditions that need to be satisfied for two schedules to be view equivalent  refer slide time  47.06  it is possible that view equivalence can also be seen as producing consistent results for example if you look at the typical case of what we considered as debit and credit transactions you know occurring simultaneously it is possible to see that view equivalence is will produce correct results as appose to the conflict serializability now  this is interesting because we will start realizing that it is possible to enforce correctness by understanding what is happening with the transaction semantics for example ; it is possible to look at semantics of operations  finally we can look at semantics of an operation and then see whether a particular execution of this operation can be correct what i am going to do is i will take a very simple example to show how semantics can be applied for understanding the consistency criterion it is possible to say that i have i will take as slightly different example here to show what is semantics of an operation we can take a simple queue as shown in this particular figure now  the queue will have what we say as a front pointer and a rear pointer and it will basically have two methods which can be executed which is basically  an add and a delete now  if you can carefully look at how exactly the queue can be left in a consistent condition when adds and deletes are happening simultaneously now  you can see that basically an add will will happen at the rear end and a delete will happen at the front end now  adds and deletes can suddenly be concurrent assuming that the queue is not full  the queue is not empty ; under those conditions  adds and deletes can occur concurrently because add is actually trying to manipulate the rear pointer  delete is actually manipulating the front pointer  refer slide time  49.28  this is very important to look at little more deeper for example ; let us say  there is t1 here and you are actually saying q dot add now  there is a t2 which is actually saying  q dot delete now  we know that from the semantics of add and delete  t1 and t2 can happen concurrently and still produce correct results this is basically semantics  knowing the semantics of the operations ; i am able to say that these two produce consistent results now  if you basically further say that two adds can also happen simultaneously and i have a mechanism for producing  know two adds working simultaneously  it is possible because all that you need is lock the rear pointer and if you allow the rear pointer to be obtained by each add separately  then you need to lock only the rear pointer and ensure that this two adds at the add level can be concurrent but at the rear level  they will be blocking each other that means the access to rear will be may consistent ; but at the add level  they can be still working parallely or concurrently  refer slide time  50.54  and  this is basically understanding the operation of the transaction and applying what we call as semantic consistency since you know what is semantic actually means that  you know the meaning of the operation and apply the meaning of the operation to decide whether something is consistent or not and  that is very interesting because it is possible to apply a much greater level of consistency criterions by understanding the meaning of the operations to just recap what we have done in this particular class and then give you some indicators of how exactly the to go on further reading in this particular subject  i typically covered the idea of what is basically a schedule in this particular class and what i have also done in this particular case is i have actually produced equivalence schedules and this equivalence schedules are from different aspects two schedules are shown to be equivalent from a conflict operation point of view by saying that if the conflicting operations are executed between the two schedules in a particular way  the same order is maintained between the schedules ; we call that as conflict equivalence we also showed view equivalence which actually means that the writes produced by one schedule the writes and the reads  the way they occur on operations are same between the two schedules we call that as a view equivalent schedule finally  we also showed what is called semantics and based on semantics  how the schedules can be seemed to be equivalent you can do the commutative operations as long as they are parallel  whatever order they appear still the schedule is right as long as you commute the commutative operations are performed in any order is still will be producing consistent schedules and so we actually shown how exactly we look at equivalence  refer slide time  53.15  and  what we have further shown in this particular class is typically  how simple case of conflict serializability can be achieved by constructing a transaction graph a transaction graph is constructed by producing before and after relationships on the various transactions and that is how actually the conflict serializability is achieved by constructing the transaction graph finally  we have actually shown  how exactly the protocols  various protocols will be used  will be designed to produce the serializable schedule the criterion for this is will be designed to produce conflict serializability what i am going to do is in the next class  i am going to discuss a series of protocols which actually produce conflict serializability  refer slide time  54.43  we are going to look at a set of protocols we start with a most popular protocol of two phase locking and show how two phase locking will produce conflict serializability and also go on to show other kinds of protocols that exploit the property of the constructing the transaction graph without any cycle that is the essential property is here  there should not any cycle as far as the transaction graph is concerned and the protocols exploit this property of trying to construct the cycle and we essentially can divide the protocols as being optimistic or pessimistic on how actually they construct the transaction graph we are going to take this in the next class of looking at the protocols and seeing how different protocols can be constructed for producing serializable schedules as a thing of further reading on this  you can typically look at there is a book by burnstien on actually concurrency control in databases this is an excellent burnstien and others basically a book on concurrency control and you can have a look at this the book as a further reference i have also used basic the foundation thing was used by the book on fundamentals of database systems by elmasri and navathe  elmasri and sham navathe  refer slide time  56.39  i have used the chapter from this book while doing this particular ; foundations on concurrency control what  i am going to also do as part of next lecture is while while doing the protocols at the end of the next lecture  i am going to introduce a few problems and try solving them at the end of the next lecture we will stop here for this lecture database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no # 20 concurrency control – part -1 foundations for concurrency control in particular what we did was we looked at the problem of serializability when transactions are executing concurrently for example there are a set of transactions and all these transactions are executing concurrently we were looking at how this can be interpreted as a serial execution of the transactions for example some order in which they could be seen to be executed is what we looked at a theory for correct execution of the transactions  refer slide time  02.21  we also looked at two kinds of a serializabilty  the conflict serializability where the transactions which are executing on the same data item when they conflict  how they can be reduced to serializable transactions that is what we meant by conflict serializability we also looked at other forms of serializability like view serializability what we are going to in this todays class is we are going to look at specific concurrency control protocols and to begin with what we are going to do is we are going to look at what are the different types of algorithms that will allow concurrent execution of transactions to be serializable that means which produce correct execution of the transactions in a database so what we are going to do now is we are going to look at what are the algorithms that we use for achieving serializability and these algorithms are called concurrency control algorithms now there are two broad classes of these concurrency control algorithms based on how they actually view the system some view the system in an optimistic fashion and some view the system in a pessimistic fashion what we mean by optimistic and pessimistic  i just explain with a simple example now if you basically look at a narrow pass bridge where vehicles are crossing this bridge and this is relatively a very very low traffic bridge we don ’ t really look at  the vehicles will hit each other when they are passing through this narrow bridge which in effect means that there are actually vehicles coming from opposite ends so you actually assume that relatively  the conflicts are very low which actually means that a scenario where the conflicts are extremely low is one situation there could be another situation where the conflicts could be extremely high which means that this is a very high traffic bridge and hence you assume the possibility of two vehicles finding themselves passing through this narrow bridge at the same time is likely to be high in either case what we have to see is the two scenarios are different system configuration so what we will do is we will broadly classify conflicts being extremely low and conflicts being extremely high now if you typically look at the way we would see when the conflict are extremely high is we basically will put some kind of a traffic signal here and we will ensure only one of the vehicles is into the narrow pass bridge that means we in effect will ensure that only one of them is going to be inside if we don ’ t put the traffic lights her  very often what will happen is two vehicles will get into the bridge and realize they are in conflict and one of them has to back track for example if we assume that one vehicle v1 has passed up to this point and v2 has passed up to this point and you find there is a conflict here one of them has to backtrack and this basically involves lot of hard work and hence when the conflicts are very high  it is probably not advisable to actually allow the vehicles to get in without any kind of a control the situation where the conflicts are low is seem to optimistic that means you actually believing that they conflict are less which is an optimistic approach or you view the system in an optimistic way  the second one where the conflicts are very high  you look at it has pessimistic so this is broadly classified in the two scenarios optimistic and pessimistic scenarios  refer slide time  06.47  now what will do is we will see algorithms that operate on the database system assuming that the system is an optimistic scenario and those algorithm which actually operate assuming that they are in the pessimistic scenario for example if you know chennai for example in the mount road  if you assume optimistically no vehicles are going to hit each other and remove the traffic lights  everything is going to be chaos there but on the other hand if you know iit madras at gagendra circle  you are not going to install a traffic light because we don ’ t have that much traffic  it is going to be cumbersome so in some sense if the conflicts are rare and if you apply the other algorithm  it is going to be overkill so typically we have to understand  what is the scenario in which the system is in whether it is in optimistic scenario or in the pessimistic scenario and based on that you should be applying this algorithms now broadly what we are going to look is these two classes of algorithms which are actually categorized  two classes of cc algorithms but it is possible that there are more algorithms than these two classes now  the one class which we are going to look at is the lock based algorithms which all assume pessimistic scenario which assume that the transactions are going to conflict with each other often the other basically a time stamp based algorithms the time stamp based algorithms assume that the system is in the optimistic fashion and hence conflicts are there what we are going to look at is one lock based algorithm  the most popular algorithm called the two phase locking algorithm and we are going to look at a basic time stamping algorithm for the time stamp basic time for the time stamp based algorithms  we look at the basic time stamp based algorithm  refer slide time  09.16  both these classes will tell us that one belongs to the pessimistic class and other belongs to the optimistic class now what i am going to do in the rest of the lecture is look at the lock based algorithm called the two phase algorithm in detail and explain what are the properties of this two phase algorithm and we will study this algorithm in detail and this is the most popular algorithm implemented in practice as well so we are going to spend some time looking at two phase locking algorithm used for concurrency control in database systems as the name suggests  this actually is a locking based algorithm the lock is very important here because what we are trying to do here is when transactions conflict on data items ; we are in essential going to allow these transactions to lock the data items this is equivalent to saying that once the transactions is locked the data item  it is not accessible for other transactions and in effect that protects the data item from being wrongly manipulated by other transactions so locking is a concept that is used in this particular case now to explain the concepts of locking in more detail  we have a problem which we are also familiar in operating systems called the mutual exclusion problem what actually the mutual exclusion problem will try to do is it actually tells that if there is one process p1 which is trying to be in a critical section the critical section is nothing but as piece of code that is trying to address a share data structure between p1 and p2 let us say there is a shared data structure here between p1 and p2 and whenever p1 is in the critical section  it means that it is trying to manipulate the share data structure here now only one of them can be allowed to be in the critical section for manipulating this is basically the read write operations that they might perform on the shared data structure so what in a sense the mutual exclusion problem does is it allows only one of the processes to be in the critical section if one process is in the critical section  it excludes the other process from being in the critical section this is what we understand in operating systems as a mutual exclusion problem now this is achieved in the operating system by using two operators called the p and the v operators p is essentially a lock operator so when you actually apply a p operation  the process actually uses the p to lock the critical section and uses v to unlock the critical section this is like i know two people can not be in the room simultaneously then what the use is they use a lock and once somebody acquires this lock  gets into the room unless he actually unlocks it and comes out of it  the other person can not enter into the room that is what actually prevented by using this p and v operators in the operating system context  refer slide time  13.00  now in the case of databases  it is slightly different than being just mutually exclusion problem because what we are actually doing in the database context is we are actually assuming that there are different types of locks  there could be a read lock whereas in the case of a mutual exclusion problem  we have only one kind a of lock but here there are different types of locks that we introduce read lock actually says that transaction is trying to read the data item  is trying to read this is very important that means it is not going to right on the data item is only going to read the data item and if you say there is a write lock  the write lock means that it is trying to write  the transaction wants to write onto the data item transaction is trying to write on the data item it is also you can say read lock  i will abbreviate here afterwards read lock with an rl an rl shows that the transaction only wants to read the data item and this is also called a shared lock because it ’ s possible that more number of transactions can start reading the item at the same time it is shared  that means there won ’ t be any violation if more than one transaction tries to read the same data item but at any given point of time  only one transaction can write on the data item so when we abbreviate write lock as wl  this also is called exclusive lock that means this lock will be granted only to one transaction whereas the shared lock can be given to multiple transactions at the same time  refer slide time  15.05  by actually distinguishing the kind of locks  we will in sense that try to increase the concurrency because whenever a transaction acquires a read lock  it is possible for other transactions also to acquire this lock but whereas if one transaction is given the write lock  the other transaction can not be given an extra write lock on the same data item unless the work was finished by the first transaction so in other words as opposed to looking at the operating system context where we have only one lock  we actually distinguish the semantics of the lock here whether it is read lock or whether it is write lock  based on the database systems tries to optimize the concurrency that is possible when transactions are simultaneously executing  to also understand the granularity of the lock this is also important issue when you actually look at locking algorithms the granularity actually means that how the size of the data items that are locked by the database by the transaction now it is possible that the transaction only locks a simple data item which actually means that in a database table one single data item  for example in student in the case of a student record  it is possible that we are locking only a we are locking only a student name or we are locking only the students cgpa we are not locking any of the other information of the student record the other one could be the entire tuple  tuple means all the things relating to the students  one particular student getting locked that is called a tuple this is equivalent to also a row in a database the entire row is being locked the other one is the table which actually means that all student records are getting locked that means that the entire table is getting locked  this is very important to understand the granularity as you reduce granularity  the transactions only locks that small item but if you start increasing the granularity it starts locking a large number of items so consequently as you increase a granularity of the lock  you correspondingly reduce the concurrency that is available in the system for transactions to execute one simple example is for example if you provide for the entire iit  access at the in gate where there is there is going to be a lock there is going to be only one person entering into the entire into the campus at any given point of time and after he goes out the lock is given to the next person  it drastically reduces the number of people who can get into the campus at any given point of time but on the other hand if the access is controlled at a room level where when they enter into the iit  when they reach the particular room you want only one person to be entering into the room at aty given point of time and that is the point where the room is locked a particular room is locked then the amount of concurrency that is available in the system is extremely high but it depends on where the concurrency is to be provided but we suddenly should lock as small as a granularity item as possible  keeping the consistent criteria into account  refer slide time  19.01  now given the understanding of the lock  let us go on to see how a simple two phase locking protocol will be working in the database context as the term two phase locking explains  there are two phases in these particular algorithm the first phase is what we call as a growing phase of the transaction in which the transaction tries to acquire all the locks and there is a second phase which is called a shrinking phase where the transaction tries to release the locks so if you basically look at the execution in terms of time  let us say this is the time access now the transaction starts executing at some point t1 and what it actually does is as it starts executing  let us say it starts to actually reach execution points where it needs some data items for example let us take a simple transaction where it is accessing bank card database as shown in the earlier examples i will basically try to read the balance in a bank account now the first thing that will happen in this case is i try to acquire a read lock on the balance data item which could mean that i might actually lock the entire account of a particular person and that shows that i in effect reached a point where i have locked  i have asked for a lock now it is up to the database manager to see that if nobody else is actually has a lock on this at this point of time or if the other transactions own only the read blocks on this data item  i can be granted the lock request now as i proceed like this  i might keep asking for a locks on other items as well so this is basically the growing phase where the transaction is actually asking for locks this is the first phase of the transaction execution which is called the growing phase the growing phase  the transaction is trying to get all the locks it needs to do the work that it has to do for example imagine that you have to move some items form one room to another item  another room then you first step before moving the items from one room to the other room or manipulating the items in a particular room  you try to acquire locks on all the rooms that is the first phase of the growing which is called the growing phase after you have got all the locks  you do the operation that you have supposed to do and after that you basically start releasing the locks this is basically called the shrinking phase in the shrinking phase  the transaction is actually releasing the locks this is lock acquisition and this is lock release in this side it is basically releasing the locks now the important condition of two phase locking is once a transaction is released one lock  it can not ask for any more locks this is a very important condition and a subtle condition that enforces serializability we are going to examine this little more deeply little later but right now what we understand is the condition between these two phases says once the transaction has reached this point which is called the lock point this is called the lock point when the lock point has been reached let us say at time t  the transaction is not going to ask for any more locks and it only releases the locks this we will put it  so the first phase is the growing phase and the second phase is the shrinking phase now in terms of the two phases  the condition is you are not going to this lock point  you are not going to ask for any more locks after you have reached actually the lock point this is a very important condition that is enforced now let us examine how exactly the two phase protocol works for a transaction and understand deeply what are the consequences of applying a two phase locking protocol  refer slide time  23.58  now i will take a simple transaction and show what exactly would have happened  if i had actually applied a two phase locking algorithm i will take a simple case of a transaction 1 which is actually reading an item rx and actually writing an item data item x and then again it is basically trying to acquire or read an item y and then write an item y there is basically another transaction t2 which is also doing exactly the same work of reading a data item x  writing a data item x  reading a data item y and writing a data item y now if you basically look at how two phase locking protocol would have worked in this particular case  when both these transactions at some point of time come into the system now let us assume that actually t2 has actually come into the system at some point of time  the first thing it will start doing is as its starts  it requests for a read lock on x  read lock on x now when the read lock on x is granted  it is going to ask for an up gradation of this read lock to a write lock now it still can not release this lock on x  though it knows that at this point of time it is actually finished working with x now it is going to work with y but still it can not release the lock on x because if it has released the lock on x  it can not ask for the lock on y this is the condition for two phase locking if you have released one lock  you can not ask for any more locks so the transaction t2 will hold the lock x and ask for now read lock on y and it will upgrade it to the write lock on y and it this point of time  it will release all the locks  refer slide time  26.05  now only when it releases the locks on x and y  can the transaction t1 start executing which actually means that the locking will ensure if t2 has acquired a lock on x  it effectively prevents t1 from acquiring the same lock on x and hence when there are actually two conflicting transactions on a data item  they will be serialized based on who has acquired the lock first this is how exactly two phase locking protocol will work now the meaning of acquiring a lock is it subsequently prevents any other transactions from getting the lock on the data item as long as this transaction is using it this is typically what is achieved as part of the execution  what you are achieving here is what is called conflict serializability here in fact making sure that the transactions execute when they are conflicting on data items in a serializable order i am going to show a small proof to show that a two phase locking in effect produces serializable order of transactions before i do that  i am going to look at some more properties of two phase locking in terms what it can be doing since the first thing that is going to happen here is since transaction has to wait  if the transaction has been locked by some other transaction if a data item is locked by some other transaction  it is possible that transactions could be waiting for each other now this could result in what we call as the problem of dead locks this is one of the problems of two phase locking what we mean by dead lock is let us say t1 has actually acquired a lock on  t1 has acquired a lock on data item x and t2 has actually acquired now a lock on data item y it is possible that now requesting a lock on y and this is requesting a lock on x both can not proceed any further because they have reached this point but they are not going to release this x or y till they reach the point of lock point where this is the lock point so both will  they are at this point right now and there is no way both these transactions can reach their point but unless they reach the lock point there is no way they are going to release these locks so it is possible in which case t1 is actually waiting for t2 to release a lock and t2 is actually waiting for t1 to release a lock and both will keep on waiting for each other because they are in a continuous loop here this in effect means that there is a dead lock because neither of them will be able to proceed any further and they will be waiting for each other in this particular context this is what we mean by dead lock this is one of the problems of using a pessimistic kind of an algorithm because the algorithms which make the which make the system wait for each other  will transactions to wait for each other can result in this dead locks the dead locks will basically bring bring the system performance and the throughput of the system drastically which means that the system time  the system throughput  the number of transaction that are executed by the system can drastically get affected when there is a dead lock condition problems of deadlocks are they need to be detected when they happen and the system has to recover back from this deadlock in this particular case either of this transactions have to be aborted when a deadlock occurs and make sure they release their locks so that the other transaction can proceed for example to break the dead lock  one of the transactions has to be aborted to make sure that t1 can proceed so deadlocks needs detection and subsequently resolving this deadlock require that you abort one of the transactions that is involved in the deadlocks so that the system can proceed further  refer slide time  30.06  this is one of the consequences of two phase locking algorithm two phase locking algorithm also doesn ’ t produce optimal schedules and we are going to look at the problem of what is an optimal schedule at a later point of time because the reason for this is it basically ensures  two phase locking ensures that a transaction always proceeds after it gets acquires locks to the finish it never grants a lock and later actually aborts the transaction at a later point of time because there is a conflict whereas in the case of optimistic algorithms it is the other way they let the algorithm  they let the transactions proceed to execute up till some point of time and resolve the deadlocks by resolve the conflicts at a later point of time by looking at what they have operated upon and seeing at a later point of time  if the operate on a data items in a conflicting fashion  they will get aborted at a later point of time so in that sense optimal schedules may not be possible in the case of two phase locking we are also going to see a small example and show how time stamping produces optimal schedules compared to two phase locking algorithm i explain this concept in the slightly different way by taking what we are actually done in the earlier class of looking at a serialization graph for example if you look at as the transactions t1 comes into the system  in effect you actually creating this graph which shows how the transactions have executed in the system one before the other for example in this case let us say there are three transactions and then after that basically one more transaction has been executing now we are basically  what i am doing here is i am saying that t1 executed before t2  t2 executed before t4 this is required only when there is a conflict for example they conflict on a data item x  i need to know how exactly they have executed one before the other  otherwise it doesn ’ t make in this particular case t2 and t3 doesn ’ t have any conflict directly and hence there is no need for me to actually put a arc here because they are not operating on common data items now let us say there is an incoming transaction ti into the system at this point of time now what the two phase locking in effect does is if there is a lock  for example let us say there is a z item on which t3 is actually locked in now the only way ti can come into the system is only after t3  it can never be for example if it is conflicting on this z data item with respect to any of the existing transactions  the only way you can allow t i to execute is by being after t3 there is no way this transaction which arrived now after t3 has locked to come before this is prevented which means that the transaction graph  the tg graph here can grow only in the forward direction that is the only way the two phase locking allows the graph to grow  refer slide time  34.00  it doesn ’ t let the graph to grow in any other direction but let us understand this problem of this graph growing in other directions as well this is very interesting for various reasons because that allows a better schedules optimal schedules to be created when you are actually executing the transactions now to explain this  i will take a simple example of two transactions t1 and t2 executing on two data items x and y now it is possible that i actually allow the read and the write things to proceed in a slightly different way and ensure that as the transactions are operating on this  either they read a pre copy or a later version which is modified by t1 now as we saw earlier there is a read x and write x that is happening on x and there is a read y and write y happening on the now let us say t2 exactly does the other way of reading y  writing y and then reading x and then writing x now it ’ s possible that when the transaction is actually modified the value of x  you can allow still this transaction to read a pre value which actually means that it is still has not written the value of its modification on to the database this is the database storage since it has not yet modified the value on the database storage  it is possible to allow the t2 to read a value before modification which means that there are three possibilities depending on what you allow t2 to do after t1 has actually started executing if you allow the before value that is pre x to be read then the transaction t2 is actually coming before t1 because it is actually reading a value that is actually modified or that is not modified by t1 if you allow t2 to read a modified value then in affect it is coming after that is this is this is write x  this is after written  t1 has actually written the value on x you allow t to read the value  refer slide time  36.30  now two phase locking will not allow the transactions to read pre x which in effect means that it will prevent this from happening more advanced algorithms allow the transactions to be placed anywhere in the transaction graph  even they can come before a particular transaction or after the transaction graph in effect there is algorithm called the 5 color protocol which allows transaction graph to grow in all directions these are actually 5 different  the 5 colors here denote 5 different kinds of locks 5 kinds of locks that a transaction can acquire besides just acquiring a read lock  a write lock  the other kinds of lock with a transaction can be granted when it tries to acquire a read or write data items which were before or after depending upon the pre and post whether the transactions data items have been modified before or after  it is possible for the transaction to acquire the locks and this allows the transaction graph to grow in all directions and this produces sort of  it produces more optimal schedules than what we see in the case of two phase locking  refer slide time  38.12  the other interesting problem that you see if you actually look at two phase locking is presence of long live transactions which actually means that the transactions are executing for a longer time for example  the example for a long live transaction is a transaction that is trying to compute annual interest for every account in the bank what this does is every year ending time  the savings is taken and the rate  interest need to be paid for each account is calculated now what this long live transaction  it runs for a very long time normal transactions run only for a few milliseconds  this runs for a few hours and then it tries to acquire locks on almost all the data items  acquires locks on a large number of data items now  what this means is this is property one and this is property two  large number of data items this in effect introduces lot of problems because this blocks a large number of short transactions  what we actually have is in case short live transactions now in the presence of long live transactions  short transactions will have problems of execution because they don ’ t be able to run their response time is going to get drastically effected  when there are long live transactions  if you apply a two phase locking algorithm because the long live transaction will in effect lock all the data items and will not release the locks on other data items till it finishes because that is one of the conditions of the two phase locking  refer slide time  40.09  so if you apply a two phase locking  for long live transactions which are also there along with short live transactions  the performance of the short live transactions will drastically get effected  they won ’ t be able to execute the response time is going to be really bad when you actually apply two phase locking this is another very important issue when you actually look at two phase locking algorithm there are lot of protocols which modify the two phase locking there is not strictly two phase locking  they modify the two phase locking condition to actually allow long live transactions to execute along with short live transactions and there is a whole large number of protocols available for making long live transactions execute along with short live transactions we will not go into details of that but then there is i will i am going to give references at the end of it pointing out a papers which actually give this algorithms  a host of this algorithms now what i am going to now touch up on is the other aspect of how a concurrency control algorithm has to be integrated with a commit protocol now what we understand by commit protocol and concurrency control algorithm has to be made little more clear now as you can see here  it is only after the transactions commits if you remember the way we actually explained earlier  there is the transaction begins execution by actually saying begin transaction when it actually comes to the end of the transaction  you are actually that is the time when you are writing all the values that the transaction is modified back onto the database it is still that point the values modified by the transaction are not actually written onto the database that is what we mean by the commit protocol there is inter relationship between concurrency and commit  in the sense that as the transaction modifies the values  this is this the place where transaction is actually modifying the values  transaction modifies and this is the point only after commit point it is actually visible  the modifications are visible only after this point so there is going to be some kind of an interaction between concurrency protocols and the commit protocols and what we need to understand is how exactly the concurrency control algorithms get integrated with commit protocols because both together can only provide correctness and if you basically see that the transaction has it modified it is visible then it is going to create difficulties in terms of the other transactions reading the values which are not yet committed by the transaction on the database so both concurrency and commit protocols have to work together and we are going to explain in the next few minutes  how exactly the commit protocols works along with concurrency control algorithms what i am going to look at it is how the 2 pl algorithm gets integrated with a commit protocol  refer slide time  43.42  we are going to look at several ways  this can be done with two phase i will take this simple graph that we have actually taken earlier to see how the commit protocol gets integrated with this this is a familiar thing that we have actually  now the t1 is the start of the execution of the transaction this is basically the end of the transaction and this is the lock point of the transaction now what we are actually looking at here is  it is possible once the transaction is reached the commit point  it is possible as it is releasing the locks they are immediately made available for some other transaction i will take only a simple case of data items x on which the transaction has a write lock now let us say it actually finished and releases the lock release write lock on x which actually means that it is possible that this x is available for some other transaction t2 now to actually start working now when t2 actually is trying to now acquire lock on this same data item x  remember that this transaction has till not reached the commit because the commit point is here the transaction is actually committing itself here not before this so it actually means that we are allowing the transactions to be executed transactions to release locks before they actually reached the commit points this is where the interface between the locking protocols  that is the concurrency control protocols and the commit protocols come into picture because the commit protocols start operating at this point whereas the before that we have applied the concurrency control protocol now if this protocol releases the lock then the data item is visible for other transactions because they effectively can acquire the lock but the value that they are going to read is not the value that actually is produced by this transaction because it is still not committed but if it is reading the value produced by  but our understanding is since it is released the lock any transaction t2 acquiring the lock is after and hence there is a relationship between these two that t1 executed before t2 and hence this is to be preserved because the value t2 should read is now the value modified by t1 now what is going to happen in this case is if the lock has been released by a transaction before it is actually committed  it ’ s possible at a later point of time at the commit stage  the commit protocol issues an abort which actually means that this transaction t1 effectively has actually aborted  refer slide time  46.33  now this requires that t2 is also actually aborted this is what we mean by if you if you allow the transactions to release the locks before you basically result in cascading aborts t1 actually modified a value on x  t2 has actually read this modified value but now t1 actually aborted for various reasons now t2 should also abort and this is what really will happen if you apply the transactions to release the locks before they are actually committed now we are modification to the two phase locking algorithms taking this into the account to avoid cascading aborts will require that the transactions actually start acquiring the locks this is actually the lock point but actually none of them effectively will release their locks till they reach the commit point which actually means that the key point holding on to the locks till they reach the commit point and all the locks are released only after the commit actually happens  which actually means that the transactions commit that means they write their values whatever values they have actually got  they will write these values back on to the system and then they allow or they release all their logs the logs are not released before committing  this basically shows that the transaction effectively starts holding  starts holding the logs till it actually reach reaches the commit point this ensures that the concurrency control and the commit protocols work correctly by actually integrating the concurrency control protocols with the commit protocols now what we are going to look at it is all the protocols will require some kind of a modifications when we req when we look at how they integrate with the concurrency control protocols  refer slide time  48.25  i will show you typically the three properties of  if you if you remember for a transaction  we are basically looking at three properties of atomicity which actually ensures that all or none of the actions of the transactions are written then we are basically looking at concurrency control typically this is consistency when they basically operating together  they basically produce consistent results then we are actually looking at the property of isolation isolation means that one transaction results are not visible for other transaction till the transaction has committed then we are talking about durability this durability is the transaction values are permanently written on the database so this together what constitutes what we actually called as the acid properties of the transactions now they together have to hold for every transaction what we are looking at here is basically the concurrency control aspects now they have to get integrated with the isolation properties  this is where the commit protocols are coming into to picture this is where the integration has to take place between concurrency control protocols and the commit protocols when we discuss the time stamping algorithms also which operate in a slightly different way of actually ensuring that properties of serializability are actually enforced at the end of the transaction execution not before we have to see how that actually integrates with the commits protocols it is going to be interesting to see how time stamping protocols ensure commit protocols integrate together in a proper way when we discuss the time stamping algorithms  we are going to look at how commit and concurrency control protocols integrate with each other in that particular context typically the atomicity properties and durability properties are achieved by what we say as recoverability properties which are actually ensured using the logs typically the logs are maintained to make sure at any point of time  the transaction can redo or undo its actions and that is achieved using the logs  refer slide time  51.24  logs plus the concurrency and commit protocols together ensure that the transaction acid properties are realized and to just give you complete picture  what we are basically looking at the lowest tier is the database items these are nothing but the tables that are stored in the database now at the other end  this basically the applications which are trying to modify the database items now all the properties algorithms that we are talking about now come in the tier which is which sits between the applications and the database tables and this is what we mean by the database management system now in this case the dbms has various other things  this has to ensure among other things a part of the subsystem has to deal with the transactions and that is basically what we mean by the transaction manager now the transaction manager is part of the dbms and this transaction manager is the one which actually ensures that the as the applications are executing  they serializablity condition is actually enforced using the transaction manager now to give an idea of what exactly happens when a transaction t1 starts executing  when it is actually putting a lock request  let us say it is actually requesting a read lock  this is actually given to the transaction manager  refer slide time  52.44  it is up to the transaction manager now to grant this request or disallow this request at this point of time so  all the transactions in effect will make the request to the transaction manager the transaction manager when it locks the data items for example there is a table here the student record table  now typically it allows a particular tuple to be locked then this lock will be granted based on the request that is basically requested by the transactions  refer slide time  53.43  so all the lock requests are coordinated by the transaction manager and it knows which transaction holds what locks at the given point of time and ensures that the lock request are properly coordinated among the various transactions that are executing it also ensures that along with the  for example if there is a commit protocol that needs to be operated before the lock is released  the transaction manager ensures that the transaction locks are not released till the commit point of the transaction is reached this is how exactly the concurrency control protocols work in the case of two phase locking what we are going to do in the next few minutes is sum up and lead to the next set of algorithms which are typically time stamp based algorithms now what i am going to show you in this particular case is a simple execution of a transaction and show how two phase locking may not be a best way of executing the transactions and how one can think of in effect producing more optimal way of executing the transactions this is the very simple example  i will with this example i will lead lead to the next set of protocols that i will be talking in the next lecture which are called the time stamp based protocols in effect looking at the execution of two transaction in t1 and t2  if you typically look at a read x  a write x and a read y and a write y by transaction x and t1 and t2 let us say just repeats the same kind of execution you can in effect see that  it ’ s possible for t1 and t2 to execute in different possible directions now one way the two phase locking ensures that t1 executes after t2 is both locks of t1 let us say the write lock on x and write lock on y will be granted for t1 which in affect prevents t2 from start executing till t1 has actually finished which means that the schedule that will be possible in this particular case is read x write x of one  read y write y of the other this is the point where the transaction would have committed t1 and all the execution of t2 proceeds after this point  refer slide time  56.31  but it is possible for  if you carefully notice what is possible here is this is not the best possible execution of the transactions it is possible for you to say once t1 has actually finished working on data item one  data item x  it is possible at the point of time for t2 to start executing on the data item x because t1 no longer needs that data item on x but the whole set of problems will come if you let this happen first thing as we discussed  we will be sacrificing on the isolation property because you are letting transaction t2 read the values before t1 has actually committed so this will sacrifice isolation property of the transitions so how exactly if you want actually optimize the execution of the transactions  probably two phase locking is not the best possible way of executing but two phase locking is by far the best way of or a simple way of executing the transactions i think i will stop here and going to come back in the next lecture database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no # 21 concurrency control – part -2 we are continuing discussion on execution of transactions in database systems we in fact looked at various things like how the execution of the transactions to be controlled in the database systems to produce consistency results in fact transaction execution is one of the most important things in database system because it affects the performance of the system if you typically notice during the results time when cbse results or state board results are announced  now lot of people try to access the results through the net the results is stored more or else on a on a database system what you will realize is a large number of people simultaneously trying to access these results at the same time that is very important  at the same time because everybody wants to know his result or watch result at the same time as somebody else because everybody is curious to know about the results now this is the time when there are simultaneous hits on the database system  the large number of people were trying to access these systems at the same time now normally we notice that it is at this point of time the system fails  no that ’ s the time when it can not respond to so many requests at the same time which is equivalent to saying that the throughput of the system  the number of simultaneous users it can cater to is going to come down drastically that ’ s the time the system will be challenged in terms of how many number of transactions it can process in a given point of time that is per second how many transactions you are able to execute on your system another very interesting example is our railway information system if you have noticed in summer there is a lot of people trying to book the railway tickets online or go to the counters and book their tickets you can notice that the number of users trying to book the tickets in the last you know couple of days  this being summer seems to be peeking at 30000 per day which is you know  the system should be able to handle that many number of peoples simultaneously coming and try to book their railway reservation simultaneously so database systems  one of the very critical things in database systems is how many number of transactions the database is able to execute without losing consistency now it ’ s very important that if one user has booked his ticket  somebody else simultaneously accessing the ticket overwrites all the details and the same seat is allotted to two people at the same time  the same birth is given to two people then the system is in an inconsistent state when one user is trying to book and he gets the birth  it should be guaranteed that nobody else is going to get the same birth that ’ s were basically we were trying to understand how the database actually solves the problem of trying to give as much as throughput as possible and at the same time maintaining the consistency of system state this is the most important requirement of database systems what we did in the last class was we were looking at two phase locking  a very populated protocol that is employed by database systems to ensure that when simultaneous transactions are executing they log the data items for example in this case  if you what we are discussing few minutes back on the railway reservation case the birth will be for particular train if it is locked by one transaction  it will not be allowed to be locked by another transaction till this transaction finishes operating on that particular item that is how exactly the lock coordination is done by the transaction manager to ensure that more transactions can be executed but at the same time consistencies ensured unfortunately what we have observed in the two locking phase is beside it is prone to be dead locks because when you locked its possible two different transactions could have locked themselves in a different way and that could have resulted in a deadlock scenario and it is also possible that since the locks are released only after the commit point  it is possible that more throughput can not be achieved by using two phase locking that is were basically we see that two phase locking needs lot of you know  those kind of algorithms need change to ensure a better throughput two phase locking suddenly doesn ’ t gives that much throughput and so if you actually want to improve the transaction throughput  possibly you should explore other algorithms as well now one of the things that we did in the last class was to actually classify the algorithms into optimistic algorithms and pessimistic algorithm and we showed that the optimistic class of algorithms  we basically use what is called the time stamp based algorithms whereas in the pessimistic we have basically the conflicts are more  we said we basically using locking algorithms this is what we actually did in the last class and we said  we will continue to look optimistic based algorithms in this class  refer slide time  7.11  and we are going to look at typically the time based algorithms  they are very popular algorithms in terms of producing a better throughput i am going to compare both optimistic and pessimistic algorithms in a little more detailed fashion for a few minutes now and then get down to look at time stamp based algorithms in much more detail but before we go in to time based algorithms  let us understand essentially the difference between pessimistic based algorithms and optimistic based algorithms  refer slide time  8.28  now really what happens in the case of pessimistic and optimistic based algorithms case is they are in two different spectrums in terms of what actually they do for checking the consistency of the database transactions notice that when actually we admit the transactions into the databases if you admit only those transactions that are going to be consistent then you have actually doing the admission control for consistency in the beginning of the transaction execution  refer slide time  8.49  now it ’ s possible for you to let the transaction execute till the last point and start doing for the consistency check for the transactions at the end of the execution so it ’ s possible for you to actually do the consistency check at the start of execution versus the end of the execution now if you basically do the check at the end of execution  you typically waste the execution time of the transactions for example it ’ s possible that a transaction is actually executed up to it finish and it is at the point of time  it is told that the execution is not consistent with respect to other transaction execution and that transaction is aborted then it is like saying that you did lot of work but some body says at the end of the work  whatever you have done is not consistent  so please come back again and redo whatever you have done  refer slide time  9.11  so that is one way of ensuring that you know the consistencies is applied at the end of the execution on the other hand everything you do from the beginning  a teacher is by your side and looking over your shoulder and looking at every step what your doing and if you actually took a value that is not actually a correct value  he actually stops you at that point and says  wait till you have the correct value and then you are executing then basically that is what we actually see it as a very conservative way of executing the transactions the other way of execution is let people proceed and let them submit to you at the end of the execution  whatever they have done and now check whether what they have done is correct or not when two people are simultaneously working  they assume wrong values tell them that your thing is wrong because i have already accepted somebody who has actually done it before now you go back and redo assuming that this is the correct value so it depends on were exactly this consistency criteria is actually applied but if you assume lots of times  they are not going to be conflicts the people are going to operate on different set of data items  it ’ s possible for you to actually assume that the execution can be allowed to proceed independently and you check for the consistencies at the end of execution that is one approach the other approach is to actually look at these two ends of spectrum  seeing that one end of spectrum is what we are marking as the optimistic end of the spectrum and the other end is what we have marked as pessimistic end of the spectrum now we have a number of protocols which lie in between these spectrums  this full spectrum of things that we have what we have actually seen is 2 pl in grade detail and this falls under the pessimistic side of the things  refer slide time  11.45  we have a variety of time stamping protocols ; some of them can be seem to fall fully optimistic time stamping  fully optimistic protocols there are protocols which are time stamp based algorithms which are not completely optimistic  they fall in between the pessimistic and the optimistic protocol what we are going to do is we are going to look at protocol  called the basic time stamping protocol and then we will see how this protocol is different from the 2 pl locking kind of the algorithms that we have seen earlier  refer slide time  13.05  what we will do when we study the basic time stamping algorithm is to understand how time stamp based protocols work in the concurrency control then we will modify this basic time stamping protocol to produce what we see as a fully optimistic time version of the time stamping protocol also other protocols more notably the multi version protocol  it ’ s possible to map it somewhere here the multi version protocols what they basically do is they produce multi versions of the data item when reads and writes are going on which means that they keep multiple copies of the data item being manipulated in the database and consequently you have what are called the multi version multi version protocols  concurrency control protocols  refer slide time  14.06  one modification of the 2 pl  2 phase locking protocol for multi version is the other thing that we are going to look at which is the multi version 2 pl is another algorithm that we are going to look at as part of the multi version protocols  refer slide time  14.46  we will look at basic version multi protocol and we are also going to look at two version locking protocol which is called the modified 2 pl for multi version protocols what this actually shows is there is whole gamet of algorithms as shown in this picture if you basically look at this spectrum is quite wide in terms of number of protocols that can be put in between the pessimistic and optimistic kind of algorithms it depends to a large extend to what side the algorithm should be applied  depends to a large extent on the system configuration  refer slide time  15.29  for example if you assume that they are going to be too many conflicts  the data items are going to be having many conflicts for particular data items then it is not worth actually  putting optimistic kind of protocols on the other hand where there are not likelihood of lot of conflicts and you start mapping it to the optimistic side pessimistic side then you are likely to have the throughput drastically comes down and it ’ s not worth actually mapping or putting the pessimistic kind of algorithms into your database systems  refer slide time  16.20  a good example here again is to look at what we see as  typically when a new movie is released  a large number of conflicts you know large number of people try to book for the same movie you know on a particular theatre and also you going to see that there is a choice for a particular you know set of seats because there are more preferred seats in the theatre compared to other kinds of seats in the theatre if you have visited the theatre many times you know which is a convenient place to sit and view your movie in effect mean says a large number of people  when the new movie is released we try to book the tickets and many of them will start asking if you ask the preferences  will start asking for those set of seats that ’ s basically high conflict data items if you basically look at the data items  you viewed the data items you can see a large number of people trying to access or trying to modify or manipulate a small set of data items and this is what mean by conflicts being very high for the small percentage of data items now this suddenly requires some kind of concurrency control now in this particular case if you actually apply a pessimistic algorithm which ensures from the beginning that the transactions operate in a consistent way works well because ultimately a single seat can be booked by only one customer you can not have multiple people trying to book for the same seat whereas 10 people competed for the 1 ticket and ultimately 9 have to be aborted even if they have all gone ahead and then did whatever manipulations they have to do but at the end  the database is going to say only one of them is going to get the ticket which means that 9 of them abort after proceeding taking all the information they abort at the end  whereas the pessimistic concurrency control would have actually aborted all the other  would not have allowed the 9 to proceed in the beginning which means that the transaction would have been blocked from actually trying to manipulate the data item once it is actually booked that ’ s essentially the difference between optimistic and pessimistic kind of an algorithms what we are going to look at is look at a little deeper in this sense and understand what exactly is the way  this consistency is enforced by the pessimistic and optimistic algorithms as far as the transactions are concerned  refer slide time  19.24  now what we will do is we will actually take a sample graph and start showing how these graph actually has grown  if you typically looked at the two cases of optimistic and pessimistic concurrency control the idea of actually looking at this graph is to understand in a more deeper way how the consistency checks at different points will really help you in terms of resolving the conflicts as you see in the diagram there is typically at this point of the time  there are set of transactions which are executing in the time sense that t1 which is actually before t2 and now which is basically before t3 an order in which the transactions are trying to execute one after the other  refer slide time  20.32  if you keep a new transaction an incoming transaction t4  now there are two points which we are talking in the graph one is the entry point and other is the end point now t4  if you allow t4 to execute without really looking at  what it is trying to do will be consistent or in consistent  refer slide time  21.00  what would have happened is t4 would have proceeded to execute and when it comes at the end of the execution  then you try figuring out whether whatever the transaction is trying to do makes sense or makes consistency  whether it falls under the consistency criterion whether it satisfies the consistency criterion  refer slide time  21.30  now all that it means is if this has to be satisfied  if there is lightly to be an arc you know something which shows that the t4 at the end of the thing has to actually  t4 at the end of the thing will have a precedence relationship which are shown here carefully follow the diagram to understand what we actually trying to discuss here  refer slide time  21.51  there is a t4 that entered after i have this graph now where do i replace this t4 ? if i am actually saying that t4 comes after t3 this is fine because this is really doesn ’ t disturb the order  refer slide time  22.29  because t1 comes before t2  t2 comes before t3  t3 comes before t4 so there is specific order in which things are happening for you  refer slide time  22.49  and this is correct order because one after the other the relationship is maintained but on some other conflicting data items if t4 has to come before t1 which means that now i want to actually force the relationship on the graph something like this  you wont be able to insert this relationship in to the sequence of actions that you ’ re doing and this actually violates the consistency which means that it wont be possible for me to no longer say this because from this the relation that i get is t1 is before t4 by the transitive relationship  refer slide time  23.05  this is like that i ate breakfast then i ate lunch then i actually ate my evening snacks now i can ’ t say suddenly my dinner comes after evening snacks but actually my breakfast which is the first event that i have actually performed occurs after my dinner that is what exactly is happening here which is a violative because by this relationship  the breakfast should be coming before the dinner event but whereas i am saying that my dinner event comes before my breakfast event and this violates the consistency criterion because one after the other as you have actually able to see here  one after the other the relation is correct as long as the future relations doesn ’ t violate whatever order i have been able to come with  refer slide time  24.23  if you really understand what really we are trying to say in this graph is the following i have a set of transactions and when actually i write this relationship that i have here  all that is being talked about now is if there is a cycle in this graph  it is violative of the consistency criterion because a cycle  you can ’ t break the cycle one after other what essentially this is showing is this is before relationship now what this is saying that t1 is before t2  t2 is before t3  t3 is before t4 now suddenly i am saying t4 is before t1 and this actually is what introduces the inconsistency in to the execution of the transactions as one after the other  refer slide time  24.45  essentially this cycle in the transition graph is to be avoided  if you want to produce consistent execution of the transactions now understand what we will try to understand here is how does a pessimistic kind of algorithms will really try to solve this problem versus how optimistic concurrency control algorithms tries to solve this problem i will try use this same transaction graph to illustrate this point that an incoming transaction  we first map in to the transaction graph by the 2 pl which means that it will never allow a transaction to be executed unless it ’ s position in the transaction graph is fixed by the algorithm which means that there is no way t4 could have executed in the 2 pl case  once it starts executing this condition would have checked if this condition is not possible  what 2 pl does is it essentially makes t4 execution impossible t4 would have not executed  t4 can not execute which means that i prevent t4 from execution from the beginning this is what we call as basically not allowing disallow  disallow the transaction from the beginning and this is what 2 pl would have done now what the optimistic concurrency control algorithms will do is t4 is allowed to execute  allow to execute but then it would have failed when it wants to commit t4 can not commit because at before commitment we basically check whether what t4 is done consistent or not which is the two spectrum that we are talking this spectrum is where you don ’ t allow in the beginning itself  in the other case you allow the t4 to execute and stop it from committing after it has executed  it still can not commit because its violating the restriction now let us go deeper and understand this because this is basis for further discussion when we go on to timestamp algorithm also  refer slide time  26.46  what we will try to understand is how the 2 pl will disallow the transactions from executing from beginning itself  the consistency check will be done at the beginning itself and as i do this i will also try to informally prove this 2 pl actually produces serializable schedules now let us look at a set of conflicting operations ultimately a transaction transaction boils down to a set of operations performed by their transaction for example if you typically look a transaction t1  there is a set of operations this transaction performs and that can be o1 to on to get the correct subscript what i am going to do is i will actually make this order  this operation with the subscript which is the transaction number in this particular case and under that i will typically look at the further subscript which is the operation number what this means is an operation i of j means that this is the ith transcation and this is the jth operation since this is the first transaction that i am talking about what i am going to do is i will try to make this just the 1 of j and this becomes operation 1 of n and we will also to complete the notation what i will basically indicate also is the data item on which this operation being performed for example you can indicate here this actually accesses or does something on x or y or whatever it is now the meaning of this is operation of first transaction  this is the jth operation  this is the manipulation this is the xth data item this is the operation n of transaction 1 manipulating data item y now what we will basically look at it is when this is executing in the context of a 2 pl  we need to acquire a lock on this data item before you actually proceed on x or y because that is how 2 pl actually ensures that you are not operating unless you acquire a lock now in this particular case  we are typically looking at two transactions that are executing simultaneously to understand how they could be manipulating now let us say these are the sample transactions of 1 and 2  transactions t1 and t2  refer slide time  28.54  now we will say that an operation will be conflicting if it is actually operating on the same data item what does this actually mean ? if in a case of banking transactions if i go and withdraw cash from my account and somebody else also simultaneously withdrawing cash from his account  not my account we both are essentially operating on our own individual accounts there is no conflict in this particular case because he is operating on his account and i am operating on my account the minute both of us go to the bank and try to withdraw the amount from the same account then we will be conflicting again if we both are looking at the balance amount that is available in the account  we still not conflicting because both of us simultaneously can view  what is the current balance in the account without really withdrawing if you are just looking at the balance  still you are not producing any inconsistent results so a simple read  though it is on the same data item it is still not conflicting if either of us are withdrawing or both of us are withdrawing which means that if one of the operation is a right operation  if both are operating on the same data item in this particular case you can see both of these are actually operating on the same data item x and if one of them is a right operation then only there will be a conflict because if i am withdrawing and he is looking at the current balance that is available in the account now it depends on whether i read it after i withdraw it or before  it starts now becoming a conflict operation if one of it is actually right  refer slide time  33.00  so this is what we mean by two operations being conflicting now only when operations are conflicting  we need to worry about the order in which these operations have executed if the operations are not conflicting  for example if two of us are withdrawing the money from two different accounts  it doesn ’ t really matter because we are just operating on two different accounts but the minute actually we started operating on the same account  we need to know what exactly happen with respect to that account  who has first seen that account  how much has been withdraw by a person x  how was it actually added later by somebody else all these details one after other needs to be there the after relation is important  otherwise there is going to be inconsistencies in the final result that you see as far the account is concerned so when we actually look at t1 and t2  what we are interested in is if there is conflict between t1 and t2 then only we are interested in finding out the relationship between t1 before t2 or t2 before t1 and this relation essentially boils down to looking at some operation of 1 and some operation of 2 and saying how this two conflicting operations have actually been executed with respect to each other this is what we actually  at the end of it interested to see t1 before or t1 after  refer slide time  34.32  for example if one of it is a right operation then we say that it ’ s conflicting as we just explained now look at operation o1j and operation 2j of t1 and t2 since they are conflicting now  if they are not conflicting we can ’ t write this order because they can execute in any order they actually wish  without really producing inconsistent results  refer slide time  35.30  if they are conflicting  we have to understand which operation executed before the other based on that we are going to say that the transaction t1 is coming before t2 or t2 is coming before t1 now what as an end of the series of execution  if you typically look at now i basically will take only the one transaction suffix  i will drop the actual operation suffix  i will say two conflicting operations o1 and o2 belong to t1 and t2 are executed in this particular order which means that there is an order of t1 before and t2  refer slide time  36.00  now imagine i have a conflicting o2 versus o3 for the third transactions which means that t2 executed before t2 executed before t3 now if you look at another  this is what we have actually looked at  o4 this means that t3 is executed before t4 now look at the discussion that we actually had a while ago on the transaction graph this is what exactly happened in our transaction graph  refer slide time  36.32  we are just proceeding one after the other but when we actually came to t4  what is really happening was we are trying to say o4 also has a conflicting operation with o1 but these were executed in this particular way which actually means that this is the order that would have happened and which is what actually produces the inconsistent results now if you basically look at 2 pl  should this execution would have happened if i consider 2 pl now o1 would have actually locked the data item  let us say this is the conflicting operation is defined here on some data item x that i will indicate it here which means that there is a lock on data item x which was obtained by o1 now  let say there is a conflicting operation between o2 and o3 on data item y which means that in this particular case  the lock was obtained by o2 before o3 now let us say this is a conflicting operation z here and on which actually we have got a lock on jet for o3 because if the lock was not obtained this sequence is not possible because the transaction will never execute in the case of 2 pl unless the lock was granted so t1 would not have been able to execute unless it has obtained a lock on x because between o1 and o2 conflicting  t1 has got the lock before t2 that ’ s the reason why this relationship is possible  otherwise this relationship is impossible you can ’ t have the relation as shown in this particular equation here now let us say there is between t4 and t1  let us say there is an item  data item a this is doesn ’ t fit into regular sequence that ’ s why i am using different xyz this is a sequence  a is out of the sequence now between o4 and o1  there is a conflicting operation being performed on a right now o4 is saying that it locked  it has got the lock on a before o1 now for a minute think if this is possible in 2 pl what o1  what transaction t1 would have done is it actually requires a lock on x  a lock on a  right before its start executed now what this set of equations i have here says is i have got a lock on x  now o2 would not have got  transaction t would not have got unless i released this lock to it which means that i should have released a lock on x before getting the lock on a because o2 saying that it has actually got a lock on y before o3 similarly o3 is saying i have got a lock on z before o4 but o4 is saying that now i have got lock on a before o1 if you carefully understand this unless this lock is released by t1  t2 would not have got that lock  unless t2 has got that lock it would not have actually proceeded to get the other lock on y so actually if you use this  equation we are writing here is inconsistent because 2 pl prevents this by saying if i have a lock i won ’ t release that lock till i get all the other locks  refer slide time  37.18  remember the condition that was imposed by 2 pl which says that unless all the locks are obtained  you are not going to release the previous lock that is what we meant by lock point right all the locks will be obtained that 2 pl finishes  the transaction finishes the execution then it basically releases the locks if you release one lock you are not supposed to ask any more locks with that condition if i have released lock x  i would not have asked for lock on a which means that transaction t2 would not have been able to execute unless i finished all the executions since t2 would not have got a lock till i finish  there is no way t4 can say it has actually has come before me and obtained lock a lock on a if this is the way it ’ s supposed to execute  refer slide time  41.55  this is intuitively what happens with 2 pl and that is essentially the reason why this important condition is put in 2 pl saying that if you actually release one lock  you are not excepted to ask for any more locks if that is not followed  you would have ended up actually having this problem the cycle in the transaction graph would have happened if i allow this condition that x could be released but still you can ask for a that is what actually is prevented in 2 pl saying that once you have a lock on x  you have to ask for lock a before you release any of the locks that you have required earlier because if you release one lock you can not ask for any more locks this essentially prevents the cycle in the transaction graph this explanation makes you understand how 2 pl checks for the consistency at the entrance  when the transaction is start to executing how 2 pl ensures the consistency requirement  refer slide time  42.42  once the transaction starts executing and its start getting the locks  there is no way you can say the transaction is inconsistent it will never get in to an inconsistent state of execution what really happens in this particular context is the transaction can get blocked  when its get blocked this results in a scenario of a transaction either permanently waiting if there is a deadlock kind of a scenario or waiting for sufficiently long to acquire a lock but the transaction will never start execution unless it is in an inconsistent state there are certain issues which relate to what is the overhead of this kind of the algorithm which we will discuss towards the end time stamping based algorithms the other issue is when you actually move to the other end of the execution and model check for the consistency  how exactly that is going to be done now we will look at typically the scenario there and see what are the possibilities of that being done and how that will be that will be done in case of optimistic scenario now what really happens in the case of and optimistic algorithm is we basically take a transaction data items on which it is basically operated this set of data items which are actually manipulated  in this particular case it is going to be a set  so this set going to be x and a in the earlier case this is the set which transaction t1 has actually manipulated t2 manipulated a set of data items relating to y and probably some other set if you take t4  this is the set which we get here a and probably some other set now when t4 wants to commit  t4 can just execute whatever it wants to do  t4 executes now it says i want to commit that is basically when it is trying to write the values of a back on the database this is at the commit point  what your going to do is it check for the values of a and say whether whatever previous things done by the other transactions are consistent with respect to this if this is basically what we say as the validation check the validation check is done at the n and if t4 passes the validation then i actually allow t4 to commit what happens in the case of a purely optimistic kind of scenario ? every transaction is allowed to proceed  what really happens in the case is the transactions takes values  does it in a local copy of the data item and manipulates whatever it needs to do on the local copy and when it actually finally wants to write you perform this validation at the end of it i will give only an intuitive explanation at this stage to allow you to understand what ’ s happening here but more detailed discussion is going to follow when we actually take up the time stamping algorithms which are the optimistic case of the time stamping algorithm  refer slide time  44.58  we will go in depth  see how exactly this works to give to an intuitive explanation of the optimistic scenario  what is done in this case is let us say 4 people are trying to do simultaneously the something each one will be given a local copy this is like 4 people  4 people in this case as you know you can see the transaction t1  t2  t3 and t4 now all the 4 transactions here will be allowed to proceed by taking a local copy from the database now 2 pl would have said i am going to order all of you and then you have to get in with the with respect to the tickets i have given you t1 is number 1  t2 is number 2  t3 is number 3 and t4 is number 4 so that ’ s the way they are going to execute  refer slide time  47.38  now i basically will not bother to give them a ticket when they actually arrived my place what i will say is you actually try doing whatever you want to do that means each one will get a local copy of the data item in this particular case as you can see t3 t1 and t4 will get a copy of a in this case it ’ s going to be a and x  this is going to be y  this is going to be z and other values now when you actually finish doing something  for example let us say all the values are produced by the each one of them  at end of it submit to me when they finish execution  they submit whatever they have done to me now i will look at what is called validate this validate phase is going to look at  if this problem of a being done in an inconsistent way by t1 and t4 will be detected at this stage for example this will show me that there is something which t1 done did on a which is inconsistent with t4 how can that happen ? for example what we will say here is t1 has actually manipulated the value of a with some particular let us say 1 is the manipulated value t1 actually manipulate the value of x to 2 which actually has been used by t2 and t2 has actually manipulated the value and produced the value 3 which is actually used by … and now t3 manipulated this value to 4 which is used by … now this is actually manipulated this is to 3 and it ’ s actually to be used by t1 this is where exactly this problem of  i have actually should not have used this and manipulated because i am coming after but with respect to a  t1 actually has taken t4 value and manipulated it to 1 please understand this little more carefully i actually did something and produced a value for x equals to 2 and that value is used by t2 this is like i did something in my paper and passed it on to the next person t2 t2 use that value and produced a value for y and this y value is passed to t3 and we used that value to produce a value for z and that is given to t4 but t4 actually given a slip to t1 before this for a which actually means that there is a cycle here who is now took whose result  it is difficult to say because t4 seems to be a1  a1 seems to be affects to t2 like this  if you basically look at it  refer slide time  48.24  this is what actually produces the inconsistency and this will be what will be detected when they submit their values to me in this case of 2 pl they will not be allowed to proceed to execute and produce this values but in the case of optimistic algorithms  they will be allowed to proceed  they will be allowed to do this manipulation but when they give the values to me  when i look at them i can see that this is violating the property of consistency which is essentially the cycle in this particular graph what the validate phase does in the case of optimistic algorithms is essentially check this dependency at the end of execution and say that now t4 should be aborted  t4 has actually produced a value so  i basically abort all those transactions which has actually produced inconsistent results and i will say that t1 and t4 should restart the execution and again they will take the new values from the database and start executing this is what will happen in the optimistic scenario now the problem here is as you can see in the case of a pure pessimistic kind of an algorithm  t4 would not have wasted its time computing something because i have used some value and i tried computing so i actually sat down and worked out everything but at the end of it i submitted my value and i was told you are inconsistent because you used inputs which are not correct so you go back and redo what you have done so this is basically a loss of execution time you have actually unnecessarily executed t4 and found out that at the end of the day t4 didn ’ t use the correct value  so it has to abort and it has to restart where this would never have happened at all in the case of 2 pl pessimistic kind of algorithm because they would have been asked to read all these values in the correct way before they start executing the transaction  refer slide time  50.55  so in that sense the pure pessimistic kind of algorithm would not have allowed this scenario where the aborts are required whereas in the case of an optimistic let things proceed but then at the end of the thing you check whether what you have done is correct or not  both have as you can see here both have the plus and the minus if you except t1 t2 t3 t4 in this particular case  this diagram you can see they generally operate for example the scenario what we actually assume here is that a large number of transactions generally operate on different data items not on the same data items that means this is going to be x  this is going to be y  this is going to be z and this is going to be p and there is completely disjoint sets  refer slide time  54.11  then you can see that at the end of it they can all go very simply you know without really locking anything because the locking overhead is reduced here at the end of validation phase they all proceed very smoothly because they are not actually conflicting this is one case where all of them are using different doors  different keys so all of them pass the validation phase this is what you will see if you apply a optimistic scenario  refer slide time  55.00  try to apply a purely pessimistic scenario in this particular case to see what would have happened let us say most of the time  all of these transactions operate only on one data item which actually means that it is possible for only one of them to be able to proceed if they actually conflictive which means that all of them will tend to do the same thing data item but only one of them will be able to go  whereas in the other case all of them will be able to go which in effect means that you have to go back and then start reworking out for t2 t3 t4 there is going to be larger number of aborts if you apply  where there are large number of conflicts  if you apply for pessimistic algorithm there it is going to be a large number of aborts of the transaction and this results in wasted time in this particular case you can see only one of them have the chance of proceeding because all of them are conflicting now if you actually applied a pessimistic kind of scenario  you know that one of them can succeed in getting a lock which means that you would have serialized them one after the other with respect to this conflicting data item and they all coming here and trying to rush through this door would not have happened  refer slide time  55.17  they will get the key one after the other this is the case of actually  they are actually trying to hit each other now if you basically apply the locking  there is going to be an organized lock on this data item x which is something like this lock is going to be  this is the key for the for the data item  this key is going to be used  passed to one after the other and you can see they all can use this data item one after the other and there is going to be not going to be a conflict after they start executing this is the other case of applying the pessimistic scenario  refer slide time  56.20  this lecture what we did is we essentially looked in depth  the difference between optimistic kind of algorithm and pessimistic kind of algorithm both have their place in terms of in terms of applications where it should be  were there are contentions are very low  there is no point to apply pessimistic algorithms where there are large number of conflicts there is no point to apply an optimistic algorithm so  both actually have their place in fact what we are going to see in the next few lectures is whole gamete of algorithms that fall in-between which also produce excellent results when they are applied to transaction processing system in the next lecture we are going to take basic time stamping scheme and look at in detail database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no 22 concurrency control – part -3 we will continue our discussion on concurrency control algorithms in this class last few lectures just to recap what we have doing we looked at a class of algorithms try classify them into two broad classes  pessimistic algorithm and optimistic algorithm in the last class in particular  we have seen in depth where exactly this difference between optimistic and pessimistic kind of algorithm come class before we have seen how 2 pl works in the context of pessimistic algorithm and also we mention that this class we are going to looked at one optimistic concurrency control algorithm before we actually look at we also going to look at what we understand as time stamp based algorithms as supposed to locking we will look at time stamp based concurrency algorithms now at all time stamp based algorithms are optimistic algorithms that you have to understand time stamping algorithms essentially used the notion of time for serialiazation that is what they do time stamp algorithms need not necessarily be optimistic algorithms but one can formulate optimistic algorithm and really what we are meaning optimistic algorithms ? we were saying that the validity of the operations done by the transactions is checked at the end of the execution of transaction that ’ s what we actually mean by the optimistic algorithm now it is not necessary that all time stamp based algorithm check for the consistency of the results consistency of operation and by the transaction at the end of the execution they may do something in between they do not need to necessarily do it only at the end of the execution of the transaction it is possible to do it so what we will do in this class is we will try to understand in a simple way  which we call as basic time stamp algorithm try to understand how time stamp based algorithms work in the context of concurrency control what we are going to do is ? look at know how you apply time stamp based algorithms how the recovery properties commit properties and recovery properties are integrated in to the time stamp based concurrency algorithm if you remember when earlier lecture on two phase locking we discussed how locking algorithm are integrated with a commit protocol so we are going to do the same exercise of looking at time stamp based concurrency algorithm and also look at how time stamp based algorithms will integrate with commit based algorithms ? so that ’ s what we are going to do in lecture now we will proceed to look at basic time stamp based algorithm so what we will do is we will explain the time stamp based algorithms  refer slide time  5  10  by taking a simple case of a basic graph of basic algorithm which is called the basic time stamp based algorithm  concurrency control algorithm  refer slide time  5  40  the algorithm has several basic things that ’ s done like any other concurrency control algorithm the first thing the done is typically in this particular case every transaction is given a timestamp  refer slide time  6  18  this is if you typically look at you know very good example in real life of getting a timestamp and then executing what you want to do  the darshan of lord venkataeswara know tirumalai tirupathi devasathanam as this thing called they give the time band you know when you actually want to have darshan of the lord they say that you have to obtain a time band you know what exactly the time band means ? they give you the times sticker and says that if you actually go there know before the time or above just the time  you will be able to see the lord so basically in other words  the darshan of the lord is to be serialized because you can see one after other  the queue one has to move to see the lord so basically a serializable will be a problem there now one of the ways we solve this in real life by saying that you need this band you know this band is generated what is the band means there ? essentially getting a time stamp time band and says that all the people what is the time band have to proceed the order of time that they have got that means they will be viewing the lord in terms of the time band in a similar way  if you look at how the transactions are going to execute  each transactions when it enters the system  it get the similar time band that is what called time band how exactly this time stamp is given to the transaction ? later we will look at the step but the idea of giving the time stamp to the transaction is that essentially time stamp serializes the order in which transactions are executed all transactions are executed as in the increasing order of the time if a transaction has got a time stamp  its order in terms of execution is fixed because all those transactions which got time stamp before this transaction will commit before this transaction all those which got a which got a time stamp higher than this can proceed and are going to commit after this transaction is committed so in that sense in serialize order time the notion of the time here serializes the transaction execution this is a very important concept this notion of time and i am using the notion of time is a very important concept as we go further down  we are going to look at distributed transactions were a single transaction is split into sub transactions and execute on multiple sides this is like not now using single time stamp for example ; for each person could be carrying is own wrist watch so you get into additional problems of how do i actually take this different times and now put them all into one zone for example ; now london will be in one zone  chennai will be in one time zone so if you are actually generating transactions in different time zone  you get into problems now all of us using the single clock  automatically everything will be serializable but if they use multiple time zones then you get additional problem of making sure that the events across multiple time zone are again serialized we are going to look at the notion  when we go on look at the transaction and see how this notion of this time becomes critical when you want to address the concurrency control problems in the case of distributed databases so to understand at you know very simple level  what really happening is when transactions are coming in into the system giving them a time stamp and you are excepting the transactions are executed with respect to the time and essentially produces the serializability to look at the basic steps in little more detail as i was trying to do here every transaction start with will be given a time stamp this timestamp which will be used subsequently for checking whether the transactions are executing in a consistent fashion or not now all read and write operations of the transactions are tagged with the time stamp read operations and write operations the transactions at tagged with these time stamp  write operations of the transactions are tagged with these keys will call these time stamp as ts so all the read and write operations are actually tagged with this time stamp that ’ s the second thing  refer slide time  12  06  so when the transaction issues are read or write operations  then let us say transaction ti has got a time stamp of ten twenty just understand or at this point of time is eleven thirty we will say the transaction got a time stamp of eleven thirty now subsequently whatever the read or write operations that are being performed by transaction will all get the same number that ’ s one time stamp we are going to use for example  in a later point of time eleven thirty five or eleven forty  you are doing something but all belong to same transaction they are not giving the time stamp this is very important all the read and write operations of the transaction are tagged with the same time stamp which is given to the start of the execution of the transaction now what do we do after this point of time ? we also have to actually know for every data item in the data base okay for every data item x  we need actually what is the read time stamp of the what is the read time stamp of x and read time stamp of x shows the time stamp of the okay the time stamp of the transaction okay that has read the value of read the read the value of x what is this mean ? this means let us say this is the data item x and there is the transaction which read this data item and time stamp let us say eleven fifty and say that is the highest transaction there could be transaction time stamp lower than this which also read this data item but we are going to consider the highest time stamp of the transactions that has read the value of x will be the rts of x similarly the write time stamp of x is the time stamp  the highest time stamp if you want to actually qualify it the highest time stamp or ts of the transaction that wrote the value of x the transaction that wrote the value of x okay so for every data item  you are going to do have two time stamps corresponding to the transaction highest time stamp value of the transaction that has read the v value and  refer slide time  15  25  also the value and also the highest time stamp of the transaction which wrote the value why do i need this tool ? because this essentially tells with respect to this data item what are the earlier transactions that i have actually done whether the read or write operation are performed by the transaction on this particular data item now we will use this time stamp to see the validity criterion when a new transaction comes tries to do some manipulation on this data item we are going to look at it and say which are all those values which can be allowed by the transaction manager to proceed and which needs to be aborted now this is the preliminary thing in terms of how exactly the algorithm maintains the data now in terms of actual execution  what i does is  whenever a transaction okay ; now consider the case when a transaction issue a transaction issues the read operation the read operation now this read will be tagged with the time stamp of the transaction so basically  essentially this is ts of the read operation okay  time stamp of the read operation now these ts has to be compared with the data item time stamps now let us say   refer slide time  17  14  the read time stamp of x and write time stamp of read operation we will say on x a transcation with time stamp ts issues a read operations on x now the rts and now wts the value of x are the following now what is the condition ? now this read operation can be allowed to proceed now the read operation can be allowed to go further as long as the condition that the time stamp of x that the current transaction is greater than the write time stamp of x please remember the reads can be shared  refer slide time  18  00  after i write a value any number of people can read that value  but if you assume that only if i am actually reading a value after somebody actually written a value on x  which means that the my time stamp value is lesser than what actually has been produced then i need to be little careful and avoid such operations understand the condition where ts is less than wts of x  if ts time stamp of the transaction is less than time stamp of this current transaction is less than right time stamp of x what does it actually implies that means a later transaction actually return the value a later transaction than me has written the value  refer slide time  19  08  a later transaction has written the value of x now in terms of time stamp order if you want to execute the transactions strictly in the time stamp order  this should not have happen  refer slide time  19  20  because now i am actually reading a value produced by somebody who is coming later than me this is like saying that know if you look at senior junior relationship you know now if you essentially says that i am actually i am passing the batch which supposed to passed in 2004 says that its actually passing after the 2003 batch there is a violation  because 2003 batch has already gone  which means that the value return by x is by transaction later than me or older than its me now i can not come back and say to read its value okay so unless i had actually finished the later transaction would not be able to come and do whatever is trying to do so in that sense  now this is the case transaction t is to be the issuing transaction has to be aborted the transaction which is actually issuing if this is the case the transaction issuing the transaction which issues which is sure which should read the operation needs to be aborted okay and it is restarted okay and restarted with the higher order time stamp restart we are going to add higher values time restarted with a larger time stamp okay now understand what i am saying just going back  refer slide time  21  30  a transaction issues the read operation ts on x now the transaction time stamp is ts here as shown here and the current values in the database as far as rts and wts concerned are shown here rts shows the transaction larger time stamp of transaction that has read the value of x wts shows the time stamp largest time stamp of transaction which is return a value on x now my time stamp is less than wts i do not need to compare this rts  rts value is higher since reads can be shareable they need not be exclusive i am still not violating any consistency thing only write i have to read something earlier return by transaction this is earlier than me rather than a transaction later than me this essentially shows that a later transaction has written the value of x  refer slide time  22  39  the transaction which is issued the read operation needs to aborted in such case and restarted with the larger time stamp and when you restarted a chance of distractions succeeding his higher because now it will take this and proceed further and chance of the transaction succeeding later is very high  refer slide time  22  55  this is what we will be done as far as the read operation is concerned now if you look at the write operation  a transaction let us say with the write operation  transaction with the time stamp  with the time stamp okay ts issues a issues write operation okay now if issue a write operation you have to check now let us say there are again similar case of rts is the current value  wts is the current value of the data item x write operation will say on x okay  in which case the rts and wts are the current value now you need to check if either of these conditions are true  ts is less than the rts rts thats the lead time stamp or ts less than the write time stamp of x then what does this implies ? this implies that i am writing a value into x which was read by the later transaction or return by the later transaction in both cases  what i am saying is  now i am trying to manipulate the value of data item but somebody comes later than me actually read as value return this value  refer slide time  24  55  in either of the case  this results in an inconsistent situation because who is somebody coming later than me as actually read i am actually modifying something would have read some stale value okay similarly i have read value that value will be last come then and start modifying it so in that sense both these conditions the write has to be rejected okay the write operation has to be rejected write on x has to be rejected now what are the reject means ? same as the earlier case that  the issuing transaction has to be okay the issuing transaction needs to be aborted and restarted with a higher stamp transaction needs to be aborted and restarted with a higher time stamp now this essentially explains their basic steps of the time stamping algorithm okay  refer slide time  26  17  now what we are trying to do in this particular case is  we are trying to say the algorithm tries to provide a basis for know checking we give transaction a time stamp no we did not still discussed how the transaction will be given a time stamp know several ways it which can be done one way is actually you can use a counter for example ; first transaction will be given a counter value of one the second transaction will be given higher number than this since all i am interested is logically showing that the transactions comes one after the other  a counter is good enough for me only thing is the counter eventually might become infinitely large means that means the counter value will become so large that at the end of it  somewhere i have to reach at the counter so at some point of time when there is more brief period of time like you know this is period when everybody goes for lunch know like that when the when there are no no transaction for brief period know in the system you can just forget the entire earlier history you can just set the counter zero and then start doing it again from one that point of time okay that ’ s one way of actually giving time stamp to the transaction this is basically a logical time stamp because all that doing saying is transaction t 2 which got a counter value of two is higher than a transaction which got a counter value of one so in terms of actually looking at t 1 and t 2 all that that you have decide in terms of time order is the counter value okay  then are various other ways of actually giving the time stamps for the transaction the other way is actually to use a physical clock okay  physical clock that gives the values physical clock that gives okay this is like saying  refer slide time  28  37  i will use the clock time of machine okay the clock time and then i will basically say that every clock take i generate okay  every clock take is used clock take is used for generating the time stamp now there are interesting  this is important because every clock take can generate only one time stamp  refer slide time  29  22  so  which means that the number of transactions that can come limited now  because they are limited by the number of clock takes because the time stamp will be generated only one time stamp can be generated in a clock take because two transactions need to get two different time stamp they can not get the same time stamp if they get it then you will have problems of validating them at the end seeing how they will basically know you need to order them so  you would not be able to order them if both of them get the same time stamp then interesting algorithm for generation of time stamps  in fact there is a very interesting algorithm which does not generate time stamps with respect to the starting point he generates with respect to the time when the transaction is excepted to finish this is an extremely interesting way of looking at the problem in an entirely different perspective there is like saying when somebody entering in to iit  it is at that point i give a tick to him or i give a band to him which means that is excepted now to follow everywhere else with that band so the band was given with respect to when actually came in to the system the other thing is when is excepted to leave the iit gate i actually put a band there  saying that this is the time you were excepted to leave the system if somebody close up at the gate know after his time over you basically know abort that particular thing this is like saying that is excepted to leave the system within that particular time i come to the gate at ten o clock now one way of saying that is i get the ten o clock at the time stamp the other way interest when we saying is i am supposed to leave this system by twelve o clock so i give twelve o clock as a time stamp which means that ending time is given that means all the transaction if the transaction dint finish within that particular time period it will be aborted so possible for know for the algorithms to generate this time stamp in extremely interesting ways that used for um some algorithms to actually fixed a band a band of time during which the transaction is excepted to finish the execution so one can intelligently use generation of time to see how the transaction execution can be controlled now has to just come back to the point of how the algorithm works and then see for further issues with respect to the algorithm and how this algorithm is different from locking kind of an algorithm what we do in this particular algorithm is ?  refer slide time  32  20  we actually giving a time stamp ts to the transaction every transaction is getting a time stamp and then the read and write operations of the transactions read or write checked again the rts or the wts one of the things i have actually forgotten to mention is when you accept a read operation or the write operation  immediately you need to actually update the time stamp of the data item  refer slide time  32  54  for example  i showed only in the earlier case how the rts and wts will be used to abort a transaction for example ; if those conditions are not met you are going to abort the transaction  but if you say that you gone to accept the reads and writes on the data item for example ; let us say now these read or write on ts accepted if the operation is accepted  if the operation is accepted if the read or write accepted the abort conditions we actually saw earliest so i will just not mention them here if the read or write accepted then the rtx corresponding rtx  refer slide time  33  52  to be updated rts of x okay to ts okay highest of this is what should be made equal to know now similarly we are going to look at the write time stamp of x and then the ts okay the highest value is set to the highest of rtx  ts is set to rts of x similarly for the wtx the highest of wts x and ts when a write operation is accepted the corresponding value is basically updated what this means is ? the rts and wts always reflect the time stamp the highest time stamp of a transaction which either read or written on the data item x  refer slide time  34  52  that ’ s what actually happens by making these counters typically rts and wts are counters  data counters they stored the value of the transactions that have highest transaction value that either read or written on this data items now let us go little more deeper and look at how exactly the basic time stamping algorithm is different from the two pl kind of an algorithm i will take a simple example and show you where exactly the difference is going to come when you apply a basic time stamping algorithm verses the locking algorithm if you remember write  we have actually taken a simple case of  refer slide time  35  50  a transaction t 1 reading the value of x writing the value of x and then subsequently doing a read of y and then a write of y here basically x and y are the data items now i actually read the value of x  write the value of x then read the value of y and write the value of y now let us say there is another transaction t 2 which exactly does the same thing as done by t 1 now what would have happen ? if actually i applied a two phase locking kind of an algorithm now t 1 needs to acquire a lock on x and lock on y and after that it does whatever  then execute then commit after that release x and y lock on x and lock on y this is done at the end of the execution of the commitment so you going to release the lock on x and lock on y after the execution that means only at this point of time t 2 can acquire lock on x  lock on y and then execute and this is only way t 1 and t 2 can proceed so unless t 1 completely finishes releases its lock t 2 can not produce  refer slide time  37  58  this is very important thing that happen if i applied the locking criteria right but if you carefully observe after this initial point of t 1 trying to do rx and wx  it is that point you can see there is no more use of x for transaction t 1 which means that it is possible for t 2 to start executing from this point not at the later point of time now there will be a read y write y here and then there will be read y write y this is an optimal execution of this scenario  but this wont be possible apply the locking strategy  because locking would have required to two phase locking would have required you actually lock both these data item which means that is only at this point of time the lock on x will be released that means this will be shifted up to this point and you basically start executing the second transaction t 2 from this point so if you mark this is t 1 and t 2 you can see the overlap a significantly come down because you are not able to execute now this is the zone i could have shifted the execution of rx and wx to this point but this wont be possible if i apply two phase locking and let us see if it is possible for me to actually to do this if i apply time stamping based algorithm  refer slide time  39  38  now what would have happen ? is t 1 would have got a time stamp of ts 1 and t 2 would have got a time stamp of ts 2 now all that condition that we have is ts 1 is less than ts 2 which means that we have a case where transaction t 1 has been able to get time stamp which is lower than transaction t 2 now when they start executing at the end of at the end of execution of the first transaction t 1 it will write the value of time stamp on data items x it is at this point of time the transaction t 2 will try attempting accessing data item x since the time stamp of ts 2 is greater than ts 1 as can be see here it is possible for t 2 to access data item x at this point of time so what really will happen after this point is  you basically will have transaction t 2 accessing the data item x and then writing similarly here at this point will have y being accessed by transaction t 1  now the t 2 can start accessing y after this point of time now what can be seen here is that the overlap that we were talking earlier to recollect what we have been talking earlier is that its possible for the transaction t 2 to overlap with t 1 when transaction t 1 is finished with accessing x and now tries to manipulate y now this effectively prevented in two phase locking because t 2 can access lock only after it is released by t 1 and t 1 will not release lock on x till it actually reaches this point is basically the lock point for the transactions so unless it reaches the lock point its not going to release the lock on x and hence t 2 will not be able to start executing till this point of time  refer slide time  42  27  but whereas effectively  it can start executing from this point onwards that is what we actually looking at when we look at the time stamping algorithm all that matters here is the ts 1 is less than ts 2 and that is the order in which it will be allowed both x and y will be allowed to be accessed by the transactions manager  refer slide time  42  45  so in other words  both t 1 and t 2 will be executing in the time stamp order and this permits in some cases more concurrency then what we seen in the two phase locking but remember this is not straightly a optimistic kind of an execution because we still looking at how the transaction should execute by looking at the transaction time stamps which were given at the beginning of the execution of the transaction not at the end right so it is not fully optimistic in that sense in a fully optimistic scenario  this would have been done by the transaction at the end of the execution for example  t 1 would have written all its value t 2 would have written all its value and i will be checking which one should be sort of committing at the end of the execution that means both will execute to their finish and then i will actually use a validation point here and say which one of them will pass the validation and make that transaction commit whereas here  i am using the time stamp and using the time stamp to order these transactions in the beginning itself i know the t 1 has the time stamp ts 1 and t 2 has got the time stamp ts 2 when it started executing and now if ts 1 is less than ts 2 that is order in which t 1 and t 2 will commit if it is other way around  then the commitment is going to be t 2 before t 1  refer slide time  44  32  now this explains what we see as a basic time stamping algorithm ? how time stamps are used for concurrency control ? now one of the things that we still did not understand here is how this get integrated how the concurrency control gets integrated with the commit protocols if you remember  we did this exercise even for two phase locking  when we integrating the two phase locking with commit protocol and that is the reason why we actually modify the two phase locking saying that the locks will not be released by transactions till the transaction commits because if it releases earlier the other transaction can look at the value and this will create cascading abort and other problems now similar thing happens in the case of even time stamping algorithms we need to see how the time stamping algorithms get integrated with the commit protocols now what will do is  we look at a simple mechanism by which the time stamping algorithms get integrated with the commit protocols a simple exercise here will be to just look at not just how the transactions writes its value to explain the problem for example  you can you can see here that there is a write x  there is read x followed by write x in the case of transactions x transactions one now if you take the transactions has returned actually the values at this point of time that is  it is actually continuing to execute the other things before it actually reaches the commit point now if you understand the right x here the modified value of x is written this stage the modified value of x is written is strictly not correct because the transaction is not still reached the commit point here now let us say at this stage of commit for some reason  this has to be rolled back which means that whatever the value that has actually been written here still needs to be undone which means that any body who is actually reading this modified  refer slide time  47  00  transaction coming after this would be reading the value that is written by this t 1 and that potentially creates a problem in terms of how the transactions depend on each other in affect we will be relaxing the concept of isolating one transactions affects on the other and that ’ s what causes this difficulty of relaxing this what we do in this case is ? we will actually replaced this write in what we call as a pre write  that means every transactions to start with will issue not a write but a pre write that means this write instruction that we write seeing here will be a pre write and after exactly it wants to commit and it reaches this last point here this is the point  it should use a write transaction that means the pre write on x and this is a write on x  the pre writes are not exactly written on to the database  not written on to the permanent storage but they are buffered  refer slide time  48  05  pre writes are buffered and we will actually validate this pre writes and make sure that the pre writes once accepted are not rejected at a later point of time from consistency point of you and when a write is actually issued by the transaction  updating its pre write its never rejected its always accepted  but the pre writes the transactions can still not commit a pre write which means that the commit stage it does not issue a pre write it doesn ’ t issue a write its possible that the transactions pre writes will all be rolled back which means that there is not going to be any affect on this pre writes on the actual database what we will see is now a modification taking pre write in to account  how the pre writes will try to solve the problem for integrating commit protocols  with concurrency protocols  refer slide time  49  07  now if you remember we are actually two checks  when a transactions issues now let us say t issues a pre write now this pre write has to be checked for pre write on a data item x  refer slide time  49  24  now this has to be checked for on the database items on the database times stamp now this will be checked again is the read time stamp of data item x and also  write time stamp of data item x now if the pre write is pre write time stamp of the transactions is less than either rts or wts of x that means it is actually less than the write time stamp or the read time stamp  then t is aborted  refer slide time  50  24  otherwise we pre write is buffered pre write is actually not written but what is done here is pre write x is buffered with its corresponding time stamp buffered with ts ts has its time stamp okay as its time stamp okay now what is this mean ? this means essentially the following  all the time stamps that we are talking about here are  with respect to this buffered item here x for example ; if x is actually pre write  pre write on x is buffered with time stamp ts now any subsequence reads that we issue here needs to be checked again this pre write  refer slide time  51  24  now at a later point  this pre write becomes a write on x  then it is at this point of time the actual wts of x is updated to the corresponding ts okay that means this will stay like this for a while in the buffer when this actually comes write comes on x it is at this point of time it will be updated to a write time stamp of x now as these pre writes are buffered as the reads comes in to the system reads of a transaction come in to the system  we still needs to check those reads again as any of the pre writes that are already buffered in the system  refer slide time  52  15  now let us understand how the reads needs to be modified in this particular case ; now if you typically look at read of x issued by a transactions t with a time stamp ts is how we will read this now this read x time stamp is ts now if you remember earlier  this will be checked the write time stamp of x and if the write stamp of x is less than the ts will allow the read to proceed because any number of reads can be done on the data item does not really matter  as long as the read time stamp is higher than the write time stamp  refer slide time  52  57  because reads can all be done concurrently where as the writes have to be exclusive now in that sense it does not really required to be checked again the read time stamp what you need to do is you need to check only the time stamp of x in this particular case again as the write time stamp of the x now if you only do write time stamp of x is less than ts i think i will put other way around which will make things  if write time stamp of x is greater than ts then basically this read has to be rejected for obvious reasons  because we are actually read after some other transactions which came after you will be actually produces the write value the read issuing transaction is aborted transaction is aborted this is same as condition of the earlier one but other case read is allowed in the in this case the read is still not allowed when you actually have a pre write buffered okay now what is this pre write buffered means the pre writes buffered means if i have typically there is a pre write time stamp on x with a ts now i have to check this time stamp let us say it is tsp to just indicate that now this tsp is actually less than the ts  refer slide time  54  45  the ts is the time stamp that i am actually trying to read what does this indicate ? this condition indicates that there is a pre write buffered and that please remember that pre write will never going to be rejected when the actual writes comes in so i actually need to buffered in such a case we need to buffered we need to buffered the read transaction read transaction that means you actually postponed read transaction and allow read only after the write has been committed allow read to happen after the write happen after the write actually comes write on x comes this ensures that this typically will ensure that the read of the transaction happens after the writes affects  refer slide time 56.02  now the other conditions were the read on the transaction ts is less than the tsp that ’ s going to be the other condition  then you can allow the read to happen because this in this particular case there is no pre buffered writes on the transaction and hence this ts can be allowed to proceed  refer slide time  56  14  read will be allowed to proceed will be allowed to proceed now what this shows is ? it requires just a minor modification in terms of how we handle when we want to integrate both commit and the time stamping protocols  refer slide time  56  38  all that we have to do additionally do is  we need to actually make sure that the writes are handled properly in this case by ensuring that they do not actually write on to the database to start with and produce the pre write what we will do in the next lecture is  we will look at host of other protocols which are most optimistic than the time stamp based protocols in the next lecture database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no 23 concurrency control – part -4 in the last lecture  we have been looking at time stamp based concurrency control techniques what we have been looking at in more particular is  how the basic time stamp mechanism works in terms of ordering the transactions  in terms of its time stamps we also looked at how the basic time stamp protocol can be integrated with commit protocol towards the end of lecture  we saw how the basic time stamping techniques can be integrated with the commit protocol we just recap what we have been doing there we introduced instead of write  a pre write and then ask the transaction to issue a pre write instead of write to start with  and pre write is going to be buffered and any read transactions incoming read transactions will be checked against this pre write to see if they need to be buffered or they can be they can be satisfied  the read transactions can be satisfied so what we have done in the last lecture ? we also mentioned that in the last lecture  basic time stamp mechanism is not truly optimistic concurrency control algorithm because we are not actually looking at looking at validation at the end of transaction execution  what we are still doing is we are ordering the transaction as they enter in to the system rather than looking at the end of the execution whether what they have done is truly satisfies the consistency requirements what we are going to do in today ’ s lecturer is  to further continue looking at these models of concurrency control and see a slightly different kind of algorithms which include the truly optimistic version of a time stamping algorithm to start with  what i will do is  i will look at a time stamping algorithm  time stamping based protocol that is truly optimistic in the sense that the validation will be done at the end of the execution of the transaction and we are going to look at in depth that particular protocol and also we will look at another different approach to concurrency control which is actually the multi version concurrency control algorithms the multi version solves the problems of concurrency control by producing a new version of the data item  each time you write the value which means that the old value is still preserved when you are actually producing a new value further data item and that ’ s solves the concurrency control in a completely different way for example ; actually each time you try to do something since preserving a old value a large extent a problem of concurrency control is elevated by maintaining a multiple copies of the data item  but a consequent problem is that you will end up paying a huge overhead for this space a completely degenerated case is where you not only maintain the new version but also the time at which this was done this typically comes down what to see as a temporal database because we actually record when a value is changed not only the changed value  the new version of the changed value but also the time when it actually happened and that becomes what we call as temporal database concepts of temporal database are suddenly beyond the scope of this particular series of lecturers  but i encourage you to read the material on your own to understand after i finish this i will encourage you to read a little bit more on your own on temporal database which constitutes a very important and interesting aspect of databases by itself as we go further down  what we are going to do is  we will start our discussion by first looking at a truly optimistic time stamp based protocol to start with and then proceed on multi version protocols i am also going to look at multi version based concurrency control algorithm and figures them  version of the multi version two version protocols in the context of two phase locking and we were going to look at in depth in the next class what i am going to do is  i am going to review some of these things that i have done in last lecturers with set of questions and then giving more explanation of what was actually done  by looking at a series of questions review questions that we can attempt on the last seven lectures in the seven lectures  we are going to have review questions in the next class  we can be prepared on the things we have done so far so that  you will be able to look at the review questions more carefully at the end of the seventh lecture now we will start with the optimistic protocol now a truly optimistic protocol as we were taking about we have the approach to looking at the problem of concurrency control  the cc algorithm will have the approach of actually doing the checking at the end of the execution of the transactions now every transaction in this case  can be thought of as having several faces the first phase can be the read phase which means that the transaction typically reads whatever is needed by it  refer slide time  7  56  all the data items that are actually required by it and then it also manipulates them  but only one thing is does not actually write them back on to them database till such point actually the transaction gets validation so you have an extra phase here which is called the validation phase now the validation phase make sure that different transactions if they are conflicting with each other enter in to the validation phase and they get validated and subsequently enter after validation phase in to their write phase  which actually means that we have these three phases ; a read phase in which the transaction reads all the values and then you have the subsequent validation phase after the transaction finishes all its required things  it gets in to the validation phase in the validation phase  any conflicts are detected and make sure that if the transaction passes the validation phase  it is in a consistent fashion whatever is trying to do and after it enters the write phase it is allowed to write the values of the transactions on the database now if the transaction does not pass through the validation phase  it is actually the transaction does not pass through the validation phase is actually aborted  refer slide time  9  20  the transaction that fails validation  validation is aborted and restarted now it can be seen in this particular case  refer slide time  9  54  what is happening really is the transactions just enter the systems  they read in the read phase they take all the required data items they do whatever they need to do and they come in to the validation phase when they come in to the validation phase  the system checks for any possible conflicts between the various transactions and once the transactions passes the validation phase  you enter in to the right phase into the right value now the important requirement here is validation how does the transactions gets validated when they are conflicting with each other ? to ensure this  what we will do is we will essentially look at the read sets and the write sets of the transactions read sets of the transaction will agree with tr and the write sets of the transaction now this is the important requirement because at the end of the day  at the end of the transaction execution  what we essentially do is  we look at the read sets  refer slide time  11  09  and the write sets and decide whether there is any conflict and based on that we are actually going to decide whether the transactions validate against the each other or there is a possible inconsistency produced by the value transaction and that is how exactly  the transactions are allowed to commit or mean to abort come back and do the redo whatever they done in earlier this is truly optimistic  because what we are doing in this particular case as you can see here is  we are actually allowing the transactions to go through this read phase irrespective of whether they validate or they do not validate only when they come to the validation phase  we perform this check once they actually validation is performed and it passes through the validation phase and it actually enter in to the write phase depending on the answer to this yes or no if it is yes basically the transaction enters the write phase and tries to write the value on the database so this is how exactly as you can see the validation  since it is performed in the end of the execution this is a truly optimistic concurrency control to this how exactly the system proceeds to execute on the other hand  as we saw earlier the pessimistic case the validation is done at the beginning of the transaction execution  not at the end of the transaction execution for example  even before the transaction is allowed to read it needs to acquire to corresponding the logs only then  it will be allowed to proceeding to other phases which means that it will be blocked at the beginning it will never be asked to abort after it is actually got the logs for other than actually deadlock or something else happens in the transaction aborted for those reasons  refer slide time  13  03  otherwise the transactions once it get the locks  will proceed to execute finish its results write the values and then only it releases the logs when in this particular case  the transaction starts executing to start with and it get validated at the end of its execution to see what it has done is correct or not and it is aborted if it has produced wrong results and it is made to redo the things again so we have actually a completely two different approaches to solving the same problem now let us understand what exactly happens  if you apply the true optimistic concurrency control now one of the things that we will be doing here is we need to record though we have looked at three phases  refer slide time  13  54  we will be needing lot more information to see whether the transactions validate or not or they do not validate to explain this what we will what i will do is i will give intuitively give you what happens when we see algorithm and then go into details of the algorithm physically now as you can see if there is no read if read sets of the transaction do not conflict with each other in which case  it does not really matter how they actually went about doing their activities if there is a write set conflict  write conflict or read write conflict  then i need to worry about how to order these transactions now what i look at is for example  you imagine  there is somebody in front of me who has actually done something database this is what we called as the committed set now this committed set has gone ahead something on the database before actually i came try to do something now i need to worry about  what is the thing this committed set has done and see if i actually conflict them respect to the committed set which is equivalent to say that this is tj which has got committed now you have to realize that i have to maintain information about the current committed transactions  what all the data sets ? what is that they have done on the database ? now i will take a look at the committed sets of the database and see with which are those committed sets ? i am actually conflicting to see if i can validate again as well now if there is conflict between the committed set and my set i am the current transaction which is trying to validate now the committed set has to see whether there is an overlap between the read and write sets and there is no overlap between the read sets or write sets depending on that you can decide now what exactly has happened before or after and based on that you can say whether i validated against them or not a simple case is  if there is a conflict in the sets you have to make sure that a strict ordering is actually ensured between the two transactions that conflicting against each other and you progressively relaxing this requirement of the phases one before or one after depending on how much conflict is really existing between these two transactions now we will start explaining now with this background what exactly is done to see where these conflicts are coming  how we can look at validating the transactions now to give this explanation we will take simple example will start explaining how the algorithm works as i just explain what i am going to do is i will see i will maintain for example ; tj is the committed transaction we will take this as a committed transaction now ti is the transaction trying to validate  transaction trying to validate now okay  refer slide time  17  53  now what i am going to look at is how exactly the tj and ti sets u know read and write sets of ti and tj really conflict with each other and based on that we have to see whether tj should come before or ti should come in this case it is already clear that tj has come before  so the order is fixed since the order is fixed what i am going to do now is  since this order got fixed because ti tj is already committed ti ’ s all operations should be enforcing this requirement that it comes after tj now to what level ti should be coming after depends on the conflict for example  if there is non-conflict  it does not really matter how ti actually work with respect to read and write phases  but as you start seeing that is more and more conflicts in the system more and more conflicts in the system  you have to worry about how exactly this ordering is done  refer slide time  18  27  let us see now  how exactly this how can be further  there is read sets and there are the write sets the read and the write sets of each of these transactions have to be now looked at  this is the first thing now the requirement that i need to know read sets and write sets of the transaction is the strict one  refer slide time  19  30  in fact  this often causes problem because this is the restriction that requires that we know a priory what are the read sets and write sets of the transactions some level of pre processing will be needed for us to be generating the read sets and write sets this is the first thing that happened in the optimistic concurrency control some algorithms other than optimistic algorithms also require that i know the read sets and write sets of the transaction is a strict one  that will be the assumption that means for example  if i write my t begin and at this stage i actually gave what are the read sets and write sets of the transaction now one way is  the user gives the read sets and write sets that were the transaction writer grammar gives the reads and write sets  refer slide time  20  31  the other one is the compiler can look at and actually generate these reads sets and write sets if the compiler is generating the read sets and write sets  suddenly it is going to be superset of the reads and writes that actually might get executed when the transaction is executed i will not be knowing which are all the data item that i will be needing i do a particular transaction so i parse it actually a start it time  i might indicate a superset of the set of data items i might actually access  when i go to the  when i actually execute my transactions that is what see in terms of the reads sets and the write sets the other thing that we need to maintain as part of this is what we see as the write phase when it has actually starts and end of the write phase start and end of the write phase similarly in terms of read phase  we have to see the start and the end these are the times which we need to maintain each for these phases because based on this we are going to now say  refer slide time  21  58  whether the validation of tj coming before ti is true or not and that is the reason why we need to maintain for each transaction  the write phase for each transaction we need to do this each tr will be waited as transaction  we need to look at the write phase and read phase starting time and the ending time for both the read and write phase given this  now let us look at what really we will we need to do in terms of validating these transactions  refer slide time  22  16  now every transaction  as it is explained will have three phases every transaction will get into three phases here every transaction has a read phase now a read phase is marked by a start of the read phase  end of the read phase  there is a start time and the end time for the read phase then it enters the validation phase and then we have the write phase again we have the starting time and the ending time for each of the transactions now this is maintained as part of the execution  refer slide time  23  20  so for every transaction when the read has actually started  when the write has actually started when the read has actually ended  when the write has ended now if you understand now the criterion that tj should be coming before ti now  if there is a true conflict between tj and ti which means that there is basically some data item which are produced by tj have to be read by ti  which means that data items produced what we mean by data items produced is  the value of this is return by tj produced by tj or read by or consumed by ti consumed by ti now it is very clear that unless the write phase of j is finished  the read phase of i should not have started  refer slide time  24  14  if it starts early  then it means that this equation the equation we are looking at that tj should have should have lesser you know in terms of time order should be coming before ti would not be valid because ti would have read values not produced by tj but by some other transactions this is very important to understand so  for this to be validated conflict data item between ti and tj in terms of tj write some values ti reads some values which equivalent to conflict  refer slide time  24  52  there are common items between the write item of tj and the read set of ti in which case  we need to ensure that this criteria that is enforced is write phase of write phase of write phase ti of tj ended before ti read phase started  before ti read phase started what i am trying to explain here the meaning of this is  ti read phase would have read the values  tj write phase would have written the values which means that the write phase of tj ending before read phase of ti ensures that ti read the correct value produced by tj  refer slide time  25  45  since tj is already committed this ensures that ti is valid in the case where tj is committed and ti is read a value before tj is committed that means the write phase not ended which means that tj still not return the value but ti start reading those values which means that conflict has not been resolved properly this will not be valid in which case  this condition that tj is coming before ti can not be valid if this is not true to what we will ensure is first condition  what we will ensure is that as i said   refer slide time  25  57  the write phase of this is the true tj ended this ensures that before the read phase of ti this ensures that tj is strictly before ti because tj has finished all its work  then ti is coming the other condition where there are common data items between the two write sets progressively we can relax at the end of the thing we probably can say that there are no common data items there are no common data items before data items between tj and ti what does this actually mean ? this means ‘ tj ’ is working on a separate set of data item  refer slide time  27  41  ti is working on two separate sets of data items in which case  for the condition tj comes before tj will be notionally correct all that i need to do is their read phase of tj read phase of tj know start  start read phase of tj the starting time is before the ti staring time ti read phase staring time read phase starting time okay what does this actually mean ? i will try to slightly rewrite it for you so that  you understand the meaning of this read phase always has two times remember that read phase has a start time and an end time now that all i compare to that is start time of tj and then i need to actually compare the tj ’ s read phase again this will again compare have start time and the end time now all the i have to do is tj read start time this is what i am looking at is less than t is read start time read phase start time  because since they do not conflict  it does not really matter  how they actually got executed but tj to come before ti all should it done is  it read the data items from the database before ti take this condition satisfies  then it find with me that tj is committed coming after that but all my reads happened after tj has actually finished its read phase now this is what is exactly done in terms of you just recap what we have done in this algorithm all that we are doing is  we are ensuring that every transaction goes to three phases  refer slide time  29  29  the read phase  validation phase and a write phase and in the validation phase  we are actually looking for this transactions conflicts with any previously committed transactions and the validation phase ensures that whatever has been done has validates again and regerates reads and writes gets validate again the committed transactions and that ensures that whatever once the validation phase is crossed the transaction  it can safely go and commit all other future transactions that come now can validate as transaction right now the most interesting fact here is  for example  imagine there are two vehicles coming in to the iit if you know  how the iit chennai is organized there is only one in gate to which all four wheelers can get in to the campus now let us say all of them are raising know to go in to some program in our open air theatre or student activities center here there will be a limited parking lot there only limited space in the parking lot now where do we enforce know in terms of who will actually win  in terms of putting his car in the parking lot one thing is to say that  i know my parking lot will take only two hundred cars  then i say that when i enter to the iit in gate  i start giving the numbers one two three four five like up to two hundred say that  beyond two hundred cars will not enter which means that the cars will be turned back the minute the two hundred and one car actually tries to enter in to the in gate that means it is actually at the in gate level  i am actually enforcing the concurrency control i do not even let cars inside my system once i know i can not handle them but then i have the counter there which makes sure this criterion is ensured before it enters in to the system the other one is i do not enforce this rule there will let cars go in in to my through in gate but when they actually reach the parking lot  it is at that point i actually see which of them can get in to my parking lot now the one which probably came later at the in gate raised passed the car which was in between which means that though at the in gate level  it is not really first car enter in to that  but by the time it reach the parking lot  it is faster than the other car then i let that car to get it which means that the transaction is raised with each other for committing now when they raised with each other  they committed at the end of it that can be an altogether different transactions that are committing if the car is entering into parking lot are not necessarily the cars that came in the same order at the in gate if you remember a basic time stamping ordering would have been would have given a time stamp for each one of them and allowed them to enter in to the parking lot as the time stamp so it is not truly optimistic in that sense  a locking would have also worked in a similar way except in a slightly different way where it have actually ensure that for each one of them specific lot somebody would have got a token and that token would have been used for enter into the parking lot so in some sense  truly optimistic algorithm will allow it is possible for example  a car which entered but tries not to get in the parking lot or park for some reason it get struck  then there is no point actually trying to reserve the lot for you at the end of the day because you let these who ever comes in you let them get them in and then assume that going to be less than 200 cars at any given point of time using a strict ordering at the in gate does not really makes sense because you are given at point of time assuming that the conflicts are very rare as they enter in there  at the end of the day all the cars enter in your gate automatically committed for finding a parking lot  but when you start finding there is going to be large number of cars that will be coming in allowing them inside the system does not make the sense because many of them have to return back this is what would have been done in the case of an optimistic algorithm optimistic algorithm would have actually allowed all these cars in to parking lot so let us say there are only two hundred cars can be parked in the parking lot but then if you actually allow a large number of cars to get in there  many of them go back spending all the fuel of coming up to that point and going back whereas you know that you can only take only two hundred cars inside so  in the case where there is high conflict  high contention it is good to actually apply pessimistic approaches like two phase locking because they ensure that you do not really have lot of aborted transactions but whereas when you assume that there are going to be ten or fifteen cars are going anyway come inside putting a large restriction on them ask them to take token all that does not really is worth  that kind of approach does not really worth because at the end of the day all these find their value true in to the parking lot in which case optimistic concurrency control is quiet good to just sum up  we see is optimistic concurrency control allows the transactions to proceed but there will be wastage when they actually abort  because they have to redo all the work they have done pessimistic approaches block the transactions and allow them to proceed only when the road is clear  when the road ahead is clear so it make sure that the transactions proceed and they never abort when start proceeding  they never abort so both have placed in terms of where there applicable and where there can be used now this sort of things comes up our basic concurrency control algorithm discussion where put up in broad perspective the class of algorithms belonging to pessimistic broad class of algorithms belonging to optimistic algorithms what i am going to do in the next fifteen twenty minutes is to see a completely different set of algorithms they actually work in a different approach and different way to just give perspective on is this two classes but a whole set of classes that lie in between that ’ s what we are going to do in the next few minutes we have to start with  what i am going to do is  i am going to look at a class of algorithms that are termed as multi version protocol now these multi version protocols are different because they tend to actually not you know overwrite the value but what they do is  for example if the meaning of the multi version  if you understand here is  every data item x will have multiple versions now obviously for example  the x value is ten here now when you rewrite this x value at later point of time what you actually get is twenty  then this is going to be a new version of x it is called x version 1 assuming that this is x version gnome so you typically tend to produce what we see is several versions each time you actually change a value for example  now return it the ith time now this is going to be i th value that you are producing now in the traditional case what we do is  we actually overwrite this value  refer slide time  38  26  since we overwrite the value  the old value is completely lost that is where basically the problem of what i have done is correct or not  is multiple people are simultaneously writing this value is possibly what i have written what written by that is where we have problem concurrency control coming in to picture now the problem becomes completely different  if you say i maintain multiple versions for example  you assume in the case of banking database where when i actually  simultaneously two people are withdrawing from the same account or depositing in the same account you have to ensure that one finishes  one finishes writing the value then other comes and does its part of it otherwise  one of it writes going to be loss  but if you actually maintain multiple versions the problem is entirely different for example  i take one value produce a new version is take another value produces a new version is that all i need to worry is that versions produces a consistent there new versions being produced and these versions are consistent the problems becomes severe the older value is always with respect to that older value only producing the new value and if somebody takes the new value and again produces the other value  it becomes a new version of the old value now a completely degenerated case of this is  what we are talking in the beginning of the lecture which is called the time based or database or temporal databases now temporal databases are also sometimes called historical databases they preserve the history of what actually is being done for example  x value at ten at time t okay so this x value twenty at time a different time so you basically record the time as an event as a couple the time becomes one of the elements of the database values so value plus the time records at what value is this time and the value this is very important  refer slide time  40  51  if you look at some kind of applications like the stock prices for example  the stock price at this point of time will be different from the same stock price  stock value for example  if you take a particular company like tcs  its stock value is going to be at a particular point of time  its stock value is something  but at different point of time its stock value is something else so the database if you just give the stock value it is not really useful because you also need to know the current time at which this was actually done and that is what we mean by purely temporal database the temporal here means component the time component value is also stored if you start recording the entire history that is completely a degenerative case of multi version database in the case of multi version database  a multi version concurrency control you maintain certain number of versions not all the versions that can be two versions which means that you keep only the current value and the previous value you are not keeping value that are beyond that time and that becomes a two version database it is possible that you maintain n number of versions which means that all the values previous values of the data item is always stored now what we will see is  set of simple multi version protocols  concurrency control protocols  refer slide time  42  26  to see how exactly the multi version protocol works and i will also see that as a specific case  a two version two phase locking protocol which have been dealing earlier extension of this two version protocol to the case of a two phase locking protocol now let me explain how a general multi version protocol will be working and later explain how it extended for the two version two phase locking what we will do is  for every item data xi we basically going to do record this is the meaning of this is the ith version of x ith version of data item x now ith version would have been produced by some transactions now what i will record here is which is the transaction time stamp ? this is the time stamp of the transaction that is actually produced value time stamp of the transaction that has actually return this transaction producing this value  producing xi  refer slide time  43  53  now  if some other transactions actually try taking the value of x and producing this then it would have become xi plus one each time you write a value remember that you producing a new value new version of the value which means that it becomes if somebody wants to write on x xi it becomes xi plus one is no longer xi now the other time stamp that i will be interested in since this is the write time stamp  the other time stamp i will be interested in is a read time stamp now it is remembered  this has to be the highest because they could be remembered several transactions leading xi among them  i am interested in the highest time stamp of a transaction that read the value of this highest time stamp of a transaction that read the value of x i am making it very simple for you you can read at the end of it  i will give you some reference which you can read more of this so there are two things we are doing as shown in the earlier case  the write time stamp which gives the time stamp of the transaction that produced the read time stamp  highest time stamp that read the value of x  refer slide time  45  16  now what we are going to do now is  for example  you look at the write operation on a data item at the end of it  the transaction issues and now as it issues  a write operation on a data item x now as it issues  the write operations on x  i have to find out the x ith version which is basically the highest version produced highest version of x with time stamp  with write time stamp less than this transaction time stamp or equal to almost that means  i look at the latest value of xi which is the current value which i should be taking for xi to be now operating upon now for this particular value  i should see the write time stamp please remember  write time stamp is the highest time stamp of xi read by another transaction for example ; there is already a transaction that read the xi value now i am interested in finding out who read this value of xi  refer slide time  46  53  now if this transaction time stamp which has read is greater than ts  is current transaction time stamp  it means potentially what i will be doing we will be wrong in this particular case  because i am trying to produce a new version of this particular xi xi plus one  but this is already read by somebody who is coming later than me which means that i will be violating  i should not be producing a value which already read by somebody who should be coming later than me show potentially in this particular case  transaction should be aborted transaction is aborted and restarted  refer slide time  47  29  if this condition is not true  what i am going to do is  if the condition is not true that nobody has read this value if the condition is not true  what i am going to do is  if the condition is not true that nobody has else read the value  xi is now taken and then a new version of this value will be produced which is going to be x  i plus 1  for which the read write time stamp will be said to ts and the read time stamp will also be set to ts that is the current transaction which has actually produced this is how exactly a new version of the data item will be produced  if the earlier condition is satisfied  refer slide time  48  07  what in effect is saying is  if i essentially look at when i am trying to write the latest value which i should be using to produce this value that i have here what i am going to look at is  the write time stamp is suddenly less than the ts  then i look at the read time stamp of this and make sure that the read time stamp is no more than the xi time stamp which i am setting and if this is correct  then i will basically proceed and produce the new version the condition is here this is less than ts  is also less than ts in which case actually we produce the new version of the version of the value now read operations are quite simple compared to this read will essentially what it will do is  it will look at read operation of the transaction on x  it will actually first look at all the sizes and make sure that xi satisfying the rts  read time stamp highest rts is less than the ts  that means this is the latest value of xi which is less than the ts which this time stamp can read  which this transaction can read after this  the ts value will be set to the time stamp of the current read operation that means the xi  the highest xi will be taken always you read the latest value of xi and when you reach that latest value of xi  then make sure that you are actually setting the read time stamp of this to the current time stamp which means now the rts the highest read time stamp of xi will be equivalent to the current transaction which read the operation that is how the read operations will be performed on the transaction this explains  how the basic multi version protocols execute  refer slide time  50  15  in the case of multi version protocol  i did not keep any specific limit on the number of versions that are produced by the multi-version protocol what this means is  the ‘ i ’ can be any number now often the problem will be that  i will be ending in highest cost in this particular case because i have to store a large number of data items because not just the current value  but the previous values all have to be stored in the database which means that the database storage overhead is going to be somewhat higher in this particular case and that is one of the tricky issues of the multi version protocol that is where i actually paying a larger cost a more specific is actually the two version protocol which means that you keep only two versions of the data items two version protocol and we will look at these two version protocol extension to two phase locking if you remember in the case of two phase locking  we actually use two locks  one is the read lock and other is the write lock read lock is basically a shared lock and write lock is the basically an exclusive lock  refer slide time  51  57  now what we have done is in the case of two phase locking every data item is either locked in the read lock or the write lock more and in the presence of a read lock another read lock can be allowed there is a write lock another write lock another read lock will not be allowed if you look at typically the matrix of what will be allowed  in what case you end up actually having  let us say this read write lock which is the current transaction holding  then you have a requested ones which are read or write these are the requested and these are the holding  currently holding now it is possible at the end of the day  if it is basically a read lock in the presence of another read lock is yes if there is a read lock on a data item x  you can also grant another read lock if there is a read lock  this is going to be ‘ no ’  if there is a write lock  read will be disallowed if there is a write  another write will be disallowed which means a data item on which a read clock is currently there  another transaction can not ask for write lock  but it can ask for read lock since read is shared  refer slide time  53  10  if the transaction is holding write lock on a particular data item  no longer you can ask for on particular data item this is how two phase locking prevents on a particular data item things being pretend in an inconsistent way now  what we will do in the multi version extension of the protocol is we will introduce a new lock called the certified lock now what the certified lock will do is ? it will allow in the phase of write also reads which means that you can read a previous value of the data item when somebody still holds the write lock  because it is still not return a value on the database  refer slide time  53  59  for example  remember x read lock data value item value of  this is the read lock that i have got and another transaction has actually got a write lock on this  but it still not written the value of this this is what we will do here is  we will say this is the question mark here which means that it still not return the value now it is possible for me to actually allow this write lock in the presence of write lock  a read lock has to be taken  but then when actually this transaction wants to write  it will upgrade this to certified lock now when a certified lock comes  it is going to write the value and both write and read value are going to be disallowed in the presence of a certified lock now this becomes a two value in an extension of two version multi version protocol because there are two versions old value which will be allowed to read in the presence of write lock  but when actually the value is return that is the time  it will be updated in the earlier it will become somebody can use  refer slide time  55  06  so  this is basically two version extension of a multi version protocol that we see now to give better explanation what really we gain by doing this will again produce the matrix which we produce earlier by saying that  there are three locks now read write and certified lock and similarly will have a read  write and certified lock now  in the presence of a read lock you can still grant a read lock in the presence of write lock  you can still grant a write lock  yes but then certifies it is no now in the case of a somebody holding a read lock  you still will be able to grant a write lock here  because it can be on previous value  write value is basically no and all other cases it is going to be no  because in the presence of certified lock you can get any other locks  refer slide time  56  12  now the advantage of this is  when somebody has actually got a write lock  please understand intuitively what is the meaning of what we are trying to do here ? what we are trying to do here is  when somebody is trying to hold a write lock  it is only intentional lock i am intending to write i am actually not writing for example  if the transaction executes for a sufficiently long time  he need not actually block others from even reading the others can read the previous value commit and go as long as i have not trying to commit he commits again my old value so it comes before me finishes everything i do not need to like somebody trying to write in the railway reservation case takes the form  tries to fill the form  but while he is trying to fill  there is no point trying to block everybody else from trying to commit or trying to come before him and trying to finish his transaction that when he actually writes it and gives it  that is the time it is going to be blocked  not before that this allows a higher level concurrency  but at the cost of actually an additional lock that i will be holding  when i actually reach that particular point this is an extension because i am only trying to have two values of a particular data item and allow with respect to this two values  who can come before me or after me that ’ s what exactly is achieved when you use the two phase locking extension to the two version protocol and that is how it actually gives more concurrency as compared to earlier two phase locking this is the interesting extension to two phase locking using the concept of multi version protocol what we are going to do in the next class is take a few examples for all the lectures i have done and do review questions on the topic we have done so far thank you database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no 24 distributed transaction models we have been considering transactions in the data executing on a single system implicitly this is assumption that we been making do we did not explicitly state that underline assumption of the model transaction model is that the transactions are executing on the node and the data is also fully resident on same node what we mean by node is the node is a single computer system on which there is an operating system image and on which the database is running and now the transaction which are part of applications are all running on the same system  that ’ s the assumption which we are making when we are actually explaining all the transactions models now consider an example where for example  railway reservation system where sitting in chennai and your trying to make booking for train leaving out of delhi which could mean that you are actually trying to access the database that is resident physically on a computer system in delhi sitting in chennai now  it could mean several things it could mean just that you are actually trying to access the database to a physical connectivity which could be a telephone line  a lease line  a some kind of connectivity between your interface which could be just display interface to the system in delhi which means that you are still physically working on the computer system that is located in delhi that ’ s could be one model but  this will not be one model which could be present when you work with systems which are dispersed and which are connected by wide area network  local area network or this is what we called as distributed system when computer systems are connected by a network they could logically present a single node kind of abstraction for the end user this is what we mean by the distributed system now when the data is distributed and the transactions can be executed on different nodes of the distributed system  we called the scenario as distributed transactions that means we are no longer assuming that  the transactions execute on a single system computer system but they could be executed on different nodes of a distributed system to explain this scenario what i will do is  i will take a very example of a banking system and explain how the scenario look like let us say that we have a bank in chennai sbi branch now i have also a branch in let us say mumbai again sbi branch could be a different bank also and it is possible that i have an account one here and have an account two here and i am actually doing a fund transfer from account one to account two  which means that i am going to do a debate here from account one by let us say two hundred rupees now it is possible for me to credit the following thing on to the other thing now this is data for account one is resident on node 1 which is basically node okay and the account to which is the other account data that is resident on node two which is in mumbai now this means that physically the data is distributed in two locations one in chennai branch and the other in mumbai branch now if you assume that these two are connected by some kind of a network does not really matter what is the network but you can you can assume that physically these two nodes are connected by an underlined network this network could be an atm network okay or could it be a fiber optic network or it could be variety of even a satellite network is present between these two nodes so we are actually assuming that some kind of connectivity exist between these two nodes which affectively means that the information from one node to another node can be accessed or the data can be from one node can be accessed the thing in another node otherwise vice versa it is possible  refer slide time  7  15  now supposed to these if you are actually seeing a single case where both account one and account two present on a single node both are present in chennai this is what the case we actually look at account one and account two and all that we had begin transaction end transaction as part of the transaction here and all this will be executed in the same branch of one system  that means physically this entire transaction is executed on a single node now all the things that we so far have been talking belong to this scenario where both account one and account two are present on a single node and the entire transaction here tr is executed on a single node so tr is executed on node which is the single node here and account one and account two are present on the node so there is no remote access this tr does not access any remote information both accounts are present on the same node this is what ideally we are talking about  refer slide time  9  00  when we are talking about one single node of distributed system as opposed to this  if you consider this particular case  we are talking about account 1 and debiting some other two hundred rupees from this account here and account 2 which is present on different node and adding 200 rupees here now imagine what could happen if these are executed on two different nodes now it is possible in other case  a power has failed in the whole thing is proceeding in the single system case it is possible that power could fail or other things could happen when the transaction is executing  refer slide time  9  25  this is what we consider the acid properties of the transaction now if the transaction debiting and crediting from one account to another account you should ensure that the properties are completely preserved as acid properties are completely preserved as far as the transaction is concerned now in a single node now when the power fails the log records could be used to make sure that the transaction will always be an in consistent state for example  for part of a transaction executed that is credit part is executed the debit part is not executed this can be figure out from the log logs of the transaction and either you can do undo or redo the transaction depending upon the state of the current transaction logs record that that is preserved now the scenario becomes quite complicated if you essentially look at distributed system now you imagine account 1 debiting is happening in one node account 2 crediting is happening in another node now it is possible that  part of the transaction is got executed part of the transaction is not executed for various other reasons firstly  to know that they both got executed itself is an important issue which is like two friends separated in two different locations try to ascertain whether the other friend exactly the other than excepting to do  refer slide time  11  07  it requires a few phone calls and also making sure if the phone rings  the person is present so many other issues get involved when people are physically separated and they have to coordinate and do some activity  but on the other hand  if they are present in the same node  in the same room  it becomes lot more easier when they have to coordinate the minute they are physically separated when phone rings  somebody does not pick up does it mean they are busy with something that is why he did not pick up the phone you have to make assumptions really relating to behavior to other node with respect to your node and also if you put a telegram it could be a different mode of communication compared to ringing using a telephone and trying to reach him so what is the mode of communication between these two systems between node 1 and node 2 ? what is the kind of communication primitives that are present between these two systems that also makes an important requirement when you study this model  refer slide time  12  10  what you are trying to say is  if you assume the transactions execute on a multiple nodes of distributed system  the whole approach to concurrency control and commit protocols takes an extra dimension because you have to consider here the possibilities of distributed system what can happen in distributed system ? node can fail  network can partition  network can fail with results in the system getting partition for example  there is a temporary network failure between node one and node two  the node two may not be reachable from node one so all these are becoming important now the transactions take these in to account and they have to ensure the acid properties when these things happen that ’ s the most important thing so we will actually consider this model as a distributed transaction model and we will study some concepts relating to distributed transaction model starting from this lecture  refer slide time  13  46  now i will explain at very high level  the distributed transaction model and get start more details of some of more transaction models to explain the essential difference between centralized system database system and distributed systems with respect to the transaction models we will take a very simple example and illustrate with respect to the simple example how exactly we can build a distributed transaction model on top of distributed database systems now continuing the discussion we had on credit debit transaction  what we will see essentially in the case of distributed transaction is a transaction tr which we have seen there will have now  two portions which is the debit portion of the transaction and the credit portion of the transaction now  debit portion of the transaction is executing on a different account 1 which is on a node 1 which is chennai node this is resident on a different node and this resident on node 2 now we call this in credit and debit two sub transactions of this root transactions tr now tr becomes the root transaction  refer slide time  15  10  so there is a root transaction which is the complete transaction now what this root transaction will do is it will actually spawn two agents which are nothing but two sub transactions agent 1 and agent 2 this agent 1 is actually sub transaction of the main transaction agent 2 is another sub transaction 2 now a single transaction is split into two sub transactions so that  these two sub transactions are executed on two different nodes of the system for example  the root is going to be on one node  agent 1 is going to be in another node agent 2 is going to be in another node now agent 1 is going to execute the debit portion of the transaction and agent 2 will execute only the credit portion of the transaction that means  it will access account one here and make sure in the account two hundred rupees is debited and here it will access account two and make sure two hundred is added here now while doing this  all the acid properties have to be preserved as part of this which means that atomicity  concurrency  isolation  durability all the acid properties independently have to be preserved as part of this particular exercise  refer slide time  16  58  you do not want if power goes off in the middle of transaction you do not want to be unsure what really happen to your account on node one it should be the preserver the property that two hundred rupees detected by this transaction atomically you know when accessing this account  other transactions are not allowed to access this account when it is modifying  somebody not to allow the results before it finish its computation now after finish it executing it is completely preserved on the database all these in an atomicity concurrency  isolation  durability properties now to achieve this issue at the highest level  this is the model what we have is the agent the root agent spawning other agents okay agent one and agent one on different nodes these agent 1 and 2 are responsible for executing the transactions on the local node and together they form a complete transaction  refer slide time 18  19  for example  additionally we have to worry about if a 2 alone is done  a 1 is not done  then again we have the inconsistency because a 1 is debit  a 1 is debit a 2 is credit so you do not want the debit being done  not credit or vice versa or both have to be done simultaneously when this is done  this also done this is also not done this is also done that ’ s what we mean by atomicity either the whole thing is executed or none of its executed now to ensure that this kind of agreement is reached between the various sub portions or sub transactions that are executing on different nodes we need what is called an exercise a module that i actually does this which make sure that transaction properties which includes the commit property that all of them either commit or none of them commit the transaction the transaction end all these to preserve these we will have an all nodes manager running which is called the distributed transaction manager this is the responsibility of the distributed transaction manager this dtm stands for distributed transaction manager distributed transaction manager now that dtm on every node ensures that the transaction properties are obtained on each one of these nodes when they are participating in the execution of the transaction now each local execution which needs to again maintain transaction properties will come under what we will see in ltm  ltm is nothing but the local transaction manager and if you see  ltm stands for local transaction manager  refer slide time  20  24  if you see exactly what we have said in the earlier slide  the credit and debit here which are executed by agent 1 and agent 2 needs to be preserved by the transaction property and this is obtained by the acid by the preserved acid property for debit transaction and credit credit transaction this is ensured for agent 1 and agent 2 by the ltm ltm preservers the logs and all the related things done in the centralized database context the ltm make sure al the things are executed by the agent or as per the acid properties and that is what is achieved by actually having the ltm  refer slide time  21  41  on each of the nodes to explain this is the model  this is the distributed transaction model what you see at the highest level to just recap what we have been telling along the complete transaction now become the root transaction as part of the root transaction when you will have the sub transaction which needs to be started on different nodes of the distributed system now here agent1 gets started node 1 and agent 2 started on node 2  agent 1 indicates a sub transaction the root transaction agent 2 indicates the other two sub transaction of the root transaction now for staring this agents and also making sure that the coordinate among themselves when they are committing  corresponding properties are achieved by the distributed transaction manager which is called the dtm now the dtm is responsible for preserving the transaction properties across the various nodes now  when the agent starts on a local machine whatever it gets executed in the local node  it preserve by this local transaction manager that means the local transaction manager is responsible for ensuring whatever is executed in the local node obeys the transactional properties and this is the complete model of the distributed transaction model  refer slide time  23  17  what we are going to look at is  we are going to look at with respect to this model  how the concurrency control and model the commit protocols are going to be executed in this context and suddenly this becomes little more complicated than what we have studied in this centralized scenario for example  commit in the centralized scenario is just writing the logs and making sure all the values modified by the transactions are return back on to the database and then if your using two phase locking  then make sure that the locks are released so that other transaction can make use of now the data items modified by the current transaction so commit becomes a simple exercise of just writing the logs and writing the modified values back on to the database now if you consider the distributed scenario where there are multiple agents running on the multiple nodes of the distributed system  the commit no longer becomes that simple now you need to worry about whether node 1 and node 2 for example  some reason node 2 wants to commit each part of its transaction and then you need to know node1 should roll back you need some kind of agreement to be reached among the various nodes before they actually commit the values on to the database system so this is basically called the commit protocol  commit protocol is the most specialized protocol than a general agreement problem now all that you have to agree in the case of commit protocols is either to commit or abort there is no other agreement that you try to reach in the case of commit protocol because one could mean i want to agree to commit my part of the transaction zero could mean that i could abort a part of the transaction so all the nodes at the end of this transaction that agents involved in distributed transaction have to agree for one or zero depending on one actually means that all together to commit zero means they do not want to commit so we typically we going to look at commit protocols and study various commit protocols for achieving consciences this is the first part of the distributed transaction model  what kind of commit protocols exist and how do they work with respect to various scenario like node failure  network failure  partitions  refer slide time  26  15  so we are going to look at various scenarios what could happen in the distributed system with respect to this how these commit protocols really work that ’ s going to be the first part of the lecture  going to focus on the commit protocols we are going to essentially look at two kinds of commit protocols one is the two phase commit protocol which has the name suggest does it two phases that means the first phase which participations are prepared  in the second phase actual commitment take place and that is what we mean by the two phase commit protocol and this two phase commit protocol naturally integrates  refer slide time  26  59  with the two phase locking protocol that we have studied earlier in the case of a centralized database system so we will study the two phase commit protocol and study extension of the two phase commit protocol in to what we see as the three phase commit protocol now the idea of this three phase commit protocol is when sudden things happen in the two phase protocol which we are going to see the protocol will get lock that means the protocol any way to proceed further till the recovery take place in other words certain kind of failures occur  the two phase commit actually locks the protocol blocks the system whereas actually modifying it in three phase commit protocol  we will able to avoid the wait for  refer slide time  28  24  it actually becomes the wait free protocol in some sense  when some kind of failure occurs wait free protocol are extremely important because when failures occurs if the system is not blocked  because of the failure it is a good property of the system  because it allows the system to actually go forward and not blocked by the failure so wait free protocols are very interesting and important in distributed systems now from two phase to three phase commit protocol  what we see is  you can avoid certain kind of failures and make the system resilient for this kind of failures the system becomes more robust under three phase commit protocol as opposed to two phase commit protocol we are going to study the two phase commit protocol in detail and then going to study the three phase commit protocol in the context of distributed transactions to start with in this particular class  we are going to focus on two phase commit protocol and study the two phase commit protocol in detail and see what under circumstances two phase commit protocol works correctly now after looking at the commit protocols  they are going to look at concurrency control protocol in the context of distributed transactions   refer slide time  29  38  is going to be later part of the talk we will focus on the concurrency control mechanism as applied to the distributed transaction systems now the rest of the lecture from this point will concentrate on the two phase commit protocol and we will try to explain in detail the two phase commit protocol now let us look what kind of scenario will be required when you actually looking at the two phase commit protocol now to explain what really commit protocol means ?  refer slide time  30  14  we have actually taken a root transaction which actually started executing on this is basically the begin transaction this is the begin transaction now i have basically the agent 1 which is the debit portion of the transaction this is debit on node 1 debit on node 1 this is basically the sub transactions agent 1 agent 2 is the credit on node 2 this is particularly operating on account 1 and this is operating on account 2 assume that some point of time  this is the end transaction now when the actual agent 1 and agent 2 are started on different nodes of the distributed system and they start executing on the two different nodes at some point of time when i reach the end of the transaction i am going to look at this part and at this point of time i have to decide whether i will be committing my transaction i have reached this point now and when i reach this point  i need to understand now whether agent 1 and agent 2 are willing to now go with writing or doing the debit and credit completely for various reasons its possible that agent 1 or agent 2 may not be willing to complete part of the transaction one reason could be account 1 is not present on node 1 or the account has been closed for various reasons which means that account 1 is not present on node 1 which means the debit can not produce logically further in which case the agent 1 whatever agent 2 wants to do agent 1 will say i am going to abort my part of the transactions because i cant complete my part of the deal  refer slide time  32  33  i can not finish debit transactions on my node  because account 1 has been closed the other case is account 1 have sufficient funds let us say it has no balance and you try to withdraw two hundred rupees from the account  when there is no balance in the account again agent is to say look i am not going to commit my part of the transaction because there is no way i can commit this transactions so in all these cases both nodes have to agree both agent 1 and agent 2 have to agree to commit their transaction for this whole thing to go through  refer slide time  33  13  otherwise  there is no point to trying through have one part of the transaction executed and the other part not been executed now to ascertain this  this commit protocol gets started ideally at the end transaction for example  when you reach the end of the transaction  you have to actually now start this commit protocol to ascertain whether this whole transaction can be committed or not which means that at this point of time  all the agents involved in the transaction have to participate in this commit protocol to ensure whether they are committing the transaction or not committing the transaction that is the part of the commit protocol  refer slide time  33  58  now let me explain this by taking what we actually call as a coordinator of the transaction  which means that we will first fix a coordinator for the commit protocol which means that there is one single node  there is a node in the distributed system that is going to act as a coordinator for the commit protocol and ideally this is the one where the transaction has been submitted which will try to coordinate transaction has been that ’ s what we mean by coordinator now all the agents where this transaction is being executed we call them actually participants they are all participating in the commit protocol so they are called participants so all agents where the sub transaction is executed is called the participants where call the sub transactions is executed is called the participants  refer slide time  35  37  so we have actually two things in the two phase commit one the first coordinator and number of participants now the responsibility of the coordinator is to initiate the commit protocol and make sure some response from all the participants and based on to that the coordinator take a decision again inform all the participants and properly terminate the commit protocol at the end of the execution so this is what we will call as a coordinator each participant has a responsibility for its own local transaction it should replying on behalf of the local agent it should ensure that certain things are done by the local agent before the participant reply back to the coordinator with a reply now we are going look at full detail what are step that we will be done by the coordinator and what are the steps done by the coordinator participant in the case of two phase commit protocol  refer slide time  36  37  now what we do in the case of two phase commit protocol is the coordinator initiates the commit protocol at the end of the transaction he initiates the commit protocol by what we mean by imitating is he can send certain messages two various participants and make sure he get some response from this participant when he is ensuring when he is actually executing this commit protocol now this is the responsibility of the coordinator  to start the commit protocol at the end of the transaction this he does by sending what he is called a prepare message to the entire prepared message this message is a special message which is called the prepared message he prepares the participants for this prepare message is sent to all the participants  refer slide time  38  09  now after sending the prepare message  the coordinator enters what we call as a the wait state because still now decided on the final outcome of this particular weight state till all replies are received or some of this were basically the problem come if there is a problem in terms of some participants not responding because there is a failure either of the network  refer slide time  38  42  or the node then they are not going to respond if they do not respond what is going to happen in the coordinator so the coordinator basically puts a or time out period actually starts a time out period immediately after sending the prepare message now he will wait for the time out period and if replies do not come within this time out period he is going to decide based on what replies he got because he has not got the reply  refer slide time  39  18  he will ensure that you know based on this  he will make a decision which actually make sure that even in that case we can still make a decision but probably he might make a decision on the negative side saying that i will abort which is the safer decision to make because when the participant comes at lateral point of time  it is always possible to ask the participants to abort rather than to commit so you will wait for a time out period and after the time out period  he is going to make a decision relating to whether he should be committing or abort  always when time out has reached and replies are not come is going to make a decision relating to abort of the transactions rather than the commit of the transaction  refer slide time  40  16  now as far as the participant is concerned  participant actually decides now waits for the commit protocol that means to start with is in an undecided state wait for prepare message from the coordinator now when you get a prepared message from the coordinator now we have to apply the two things which participant can do  participant can actually reply with a ready message  refer slide time  41  04  what ready message means is that the participant is willing to commit his part of a transaction that ’ s what actually means by the ready message the ready message indicates that the participant is willing to commit his transaction that means he has executed successfully his part of the transaction and he is willing to actually commit his sub transaction that ’ s what it mean by the ready message now  before sending the ready message  if he is willing and before sending the ready message  he needs to actually ensure that before sending the ready message what should be he doing ? he should be ensuring that all the logs are return because if he does not write the locks and he replies with the ready message  refer slide time  41  55  the coordinator assumes that whatever may happen later  the participant is willing to go with the commitment of the transaction tomorrow you can not come back and say look i am not going to commit once you have replied with ready message  you are committing your self to commit this transaction so it is important for you write all the logs and also if you are using locking protocol  you are not going to release the locks till you know the decision of the coordinator because if the coordinator now says please come in  you need to write all the values on to the database and then release your locks so before sending the ready message  the participant should write all the required locks participant writes the required locks  refer slide time  43  16  now  if you carefully observe all this writing the locks preserving all the different properties of the transactions is achieved by the local transaction manager ltm ensures that all this local transactions properties are maintained using the ltm now once the participant it ’ s a ready message the coordinator can now decide if he receives all the ready messages he can now decide  what needs to be done up to this point  we call this is phase 1 that means phase 1 is preparing the participants for commit  refer slide time  43  41  you are preparing this is actually prepared phase actually  you are preparing the participants for committing only in the next phase  phase 2 you actually commit  refer slide time  43  52  so there are two separate phases  thats why called two phase commit protocol in the first phase  you are preparing all the participants for committing in the second phase you are actually committing the transaction now it is possible in phase 1  you reply back for some reason the participant is not willing to commit because as we explained earlier  there was no sufficient fund all these could be reasons while the participant may not to commit his transactions in all these cases  what the participant does is  he actually sends an abort answer message which means that he sense saying that i am aborting his answer for prepare is abort answer message  refer slide time  44  46  when a participant is actually giving an abort message  participant is not willing to commit his part as the transaction  participant is not willing to commit not willing to commit when coordinator receives this abort answer message  he will be forced to take only in the second phase we are going to take that commit decision so phase 2 once the coordinator enters phase 2  he is actually making the decision on commit he is deciding now on decision commit he is made now what are the different decisions ? it is possible in the time out period coordinator receive all the coordinator received all ready messages from all the participants all the ready messages from all the participants  refer slide time  46  01  then now  the coordinator in this particular case  we will take a decision to commit the transaction that is obvious because all the participants in the transaction willing to all the agent transaction are willing to commit their part of the transaction hence it is logical for the coordinator to take the decision to commit in the entire transaction but before actually we taking the commit decision okay  before sending the commit message the agent has coordinator has to write decision on a stable storage so that whatever may happen later  the coordinator can recover back see what is the decision that was made by him  with respect to this particular transaction that is the logical thing which he does  refer slide time  47  10  so  we except in this particular case the coordinator receives all ready message commit decision is taken commit message is sent to commit message is sent to all participants before sending the commit message  before sending these are the steps that the coordinator has to follow before sending the commit message  he needs to actually write the commit log the commit message he needs to write a stable log is written a stable log this is important because whatever may happen to the coordinator  he should be able to now tell the rest of the participants at later point of time  what was the decision that was taken on this particular transaction  refer slide time  48  03  when the participant receives now  at the second phase  phase 2 when the participant receives participant receives the commit message receives cm message commit message  the transaction is committed and an acknowledgement is sent to the transaction is committed and the ack is sent acknowledgement is sent an ack is sent is sent to the coordinator  refer slide time  48  50  now the coordinator  when he receives in the second phase all the acknowledgement the coordinator on receiving the coordinator on receiving on receiving all ack that is acknowledgement will write complete log will write the complete log now at this point of this time the transaction is completed and it can be closed  successfully by the coordinator now there could be various cases in between  refer slide time  49  41  this was talking about only a successful completion on the transaction by the coordinator as it is proceeding from one phase to another phase to recap what we have done phase 1  this is actually in the phase 1 coordinator imitates the commit protocol and here he actually sends the prepare message is sent to the all participants this is in the phase 1 first action of the coordinator and enters in to the wait state for a time out period for replies from all the participants now the participant who is waiting for the prepared message can actually respond with the ready message before sending the ready message  participants make sure he write all the stable logs and enters into a now a wait state of the decision of the coordinator  refer slide time  50  28  now the coordinator after this point enter phase two the phase two is entered by the coordinator when he receives all the replies from the no all the ready messages from the participant or a time out period as a write now in either case it takes a decision if all the messages are received ready messages are received he commits the transactions writes the commit log and sends the commit messages to all the participants now when the participant in the second phase and receive the commit message  they actually write all the transactions are committed and an acknowledgement is sent back to the coordinator  refer slide time  51  25  so that now he can write the complete log and close the transaction now it is possible that in the first phase we did not discuss the case were its possible for the participant to reply with an abort message now when the participant replies with an abort message or coordinator waits for an time out period  in either of its cases the coordinator is going to take a abort decision and this will be communicated to all the participants and when receive the abort decision  the participants are abort the transaction this is what actually happens with the two phase commit now before we going to in details of looking at what are the different things can happen what are the different failures scenario that can be presented in this case and how this two phase protocol is resilient that kind of failure we will try first looking at what was the state transition diagram for the coordinator and the participant  that explains this whole scenario in lot more detail and after that we are going to look in more details  how the two phase commit protocol is resilient to certain kind of failures when the commit protocols is executed now  to just explain this  what we are going to take is  this is actually the coordinator state transition diagram coordinator state transition diagram i am writing the state diagram of the coordinator to start with to start with  the coordinator is based on the initial state and from the initial state what the coordinator does is he actually sends prepare message to all the participants so this actually we will have some level of undecided state this is the state where the decision was not taken by the enters its wait state which is the undecided state now this is basically the prepare message is sent by the coordinator and he is waiting for the decisions now its possible that actually he waits from this state possible for the coordinator from this undecided state that he receives the all the ready messages this is what we actually seen now when he gets all the ready messages  he is going to take when the input to the coordinator is ready messages  refer slide time  54  27  all the ready messages have been sent to the coordinator  then you are going to get the commit command commit message from the coordinator and the state in which actually the coordinator reaches the state the commit state now it is possible that the coordinator actually got abort answer message from the participants when he got an actually abort answer message and he is going to taken an abort command message that means is going to send an abort command to all the participants and that is typically when an abort decision is taken by the coordinator now it is possible that  when from the initial state the coordinator can reach the abort decision if there are no sufficient replies  that is the time out period when a time out period is reached when the time out period is reached its possible for the actually by sending the prepare message and gets to the undecided state so strictly speaking  this arrow should come from the undecided state here which means that the this state from which this arrow will come  there is a time out period and after the time out period  the coordinator has to take an abort command message that he waits for the time out period and after the time out period he actually takes the decision to abort the message  refer slide time  56  09  now if you look at the participants ’ state transition diagram  it looks something like this to start with actually the participant is in initial state now the participants receives the prepare message and the answers with the ready message then he basically enters the ready state here now from the ready state  it is possible for the participant to reach when he reach when he receives the command message  he could be receiving the he could be sending an acknowledgement message and could be reaching the commit state if he actually receives an abort message  abort command message  again he will acknowledge  but he will reach the abort state  refer slide time  57  09  it is possible for the participant from the initial stage itself by actually responding with an abort answer message he could be reaching this state he actually sends an abort answer message  with which he actually reaches the abort message  refer slide time  57  27  this sort of summarizes what we are doing now in two phase commit protocol what we are going to do in the next class is  take these two transition diagram and then explain how the two phase commit protocol is resilient for failures database management system prof d janakiram department of computer science and engineering indian institute of technology  madras lecture – 25 basic 2-phase & 3-phase commit protocol in the last lecture  we have just looked at basic two phase commit protocol just stopped at that point where we just had the basic protocol and how it works actually two parts are there one part is the participant protocol and other is the coordinator protocol how the coordinator protocol working is  the coordinators starts with sending a prepare message and once he sends a prepare message from the initial state actually he reaches the state where he is now undecided so the prepare message is sent to all the participants once the prepare message has been received by the participants  they will reply back with the ready message to the coordinator if the coordinator actually receives all the ready messages  then he will take the commit and he will send the commit message then the coordinator will reach the commit state and if he receives any of the participant actually says he wants to abort that means for the prepare message he gives an abort answer message  then you are going to take an actually abort command message and this state actually the coordinator reaches the abort state  it also possible for the coordinator to reach the abort state if he times out  that means he does not receive the message from the participant  then one of the participant failed since one of the participants is failed you are bound to actually taken abort message right this is the basic part of the coordinator as far as the participant is concerned there is each participant site for the two phase commit is concerned  each participant start with an initial state now he is going to get a prepare message in response to the prepare message  he can basically give a ready message now he gives the ready message  he is in the basically ready state then he is in the ready state now wait for the coordinator ’ s decision if the coordinator actually gives the abort command message  then you are going to acknowledge and reach the abort state even after actually you reply with the ready message  it is possible for coordinator can still take a abort decision because in that particular case  the other participants may not have replied back unless he get all the ready message  he can not commit the transaction now in the other case  all the participants replied with ready message the coordinator is going to send the commit message in which case the participant actually commits  then sends an acknowledgement that means he has written all the values on to the database this is the basic two phase commit protocol that we have seen in the last lecture and were we have stopped is  just explain this protocol and then said that what happens in terms of different failure scenarios  we thought that we looked at in the last class that ’ s why we stopped in the last class now that several things can happen in this basic protocol for example  coordinator can fail in the first phase or second phase participant can fail in the first phase or in second phase network could fail which means that its possible for the various nodes to get partition  some nodes in one partition and some nodes in another partition so  all these are possible scenarios as far as this protocol is concerned now one of the things we looked at what are the cases were this protocol is resilient which means that which are the failure scenarios were this will still work correctly  refer slide time 5.58  now let us first take the case were you know we are going to look at different cases of failures now the first kind of failure is a participant fails in phase 1 what happen if the participant fails in the two phase commit ? we are talking about the first phase  refer slide time 6.39  now what happens in this case is the coordinator will time out and then once the coordinator times out is going to take the decision of aborting or sending an abort message here for example  here the coordinator is going to timeout and he is going to in that response is going to take an abort message  abort command message in the minute actually one of the participants has not replied  refer slide time  7  02  so  i think in that particular case  the coordinator will time out and then and take an abort decision co-coordinator will timeout as a result he is going to take an abort command message that ’ s what going to happen when participant fails so obviously two phase commit is resilient for participant failures in the first phase now what happens if the participant fails in the second phase of the participant fails in the second phase that means in the first phase  the participant has replied with the ready message it means that he actually replied the ready message and now he is waiting for the coordinator ’ s decision coordinator can take either an abort decision or a commit decision  refer slide time  08  09  now  if you say that the coordinators have taken an abort decision or commit decision ? after recovery you have to check with the coordinator and accordingly you have to do as far as you have concerned  you have replied with the ready message that means you actually now all the logs and you cant release any of the locks or any of the resources till such time you know the decision of the coordinator right in this particular case  you are going to wait for the decision of the wait for the decision now since you are waiting for the decision  since you have failed for the decision all that you have to do is after recovery that means after you recover from failure after recovery  the participant should check with the coordinator participant checks with the coordinator for the decision what happened about the transaction he has to check with the coordinator and accordingly he has to terminate the transaction that means based on what the coordinator tells now coordinator need not wait for the participant because it already replied with replied message  refer slide time  9  19  so  in this particular case  the coordinator is feed to choose a decision depending on read message or if he get all the ready messages then he take the decision to commit the message  commit the transaction in which case the commit decision would have been taken so  that participant has to find out what happens to this transaction he has to do accordingly one of the things is neither other participants will be blocked because of this participant failure the coordinator will be blocked all the coordinator has to do is to read the decision when the participant comes back  he has to tell the participant about his decision and when he get the acknowledgement from participant he is going to write the final log on this transaction  it has been successfully completed that ’ s the thing that needs to be done in this particular case now  it is possible also for the failures to happen for the coordinator at different phases and also possible that both coordinator and participant also fail need not be just failures of participant alone coordinator alone but right now we looked at what happens if the participant fails  one or more participants fail in the first phase and in the second phase  refer slide time  10  30  so  if you want to look at what happens to the coordinator failure ? suddenly you have to now understand in which phase the coordinator has failed again we can see the case were coordinator fail in the first phase if the coordinator fails in the first phase  that means he sends the prepare message and after that has failed he has not actually taken any from this transaction  refer slide time  11  36  then he fails before the ready messages actually arrived at the coordinator so the coordinator has not reached the decision but after sending the prepare message if he fail the coordinator has failed now the way to recover is simple all that the participants have to do is  they have to elect the new coordinator and then restart the protocol that means the new coordinator will send the prepare message and correspondingly he will take the decision about the transaction by running by the two phase commit protocol so in this particular case  a new coordinator has elected assuming that there are no failures  new coordinator is elected here and then the transaction is restarted  refer slide time 12.12  two phase commit of the transaction is restarted protocol and then based on that the commit protocol will be executed based on that one of the assumptions that is making here is that all the participants are live  refer slide time 12.24  they are not dead but if they are if it is not the case also  what will happen is  the new coordinator will be elected and he will be time out when he starts the protocol so  he will make an abort decision so in that sense  the protocol become reentered means any number of times  any number fails failures is occurred for example  you elected a new coordinator now that coordinator will fail again in the first phase again you are going to again have the election algorithm to elect a new coordinator this is basically protocol is very simple now in the other case  if the coordinator actually fails in the second phase this is an interesting problem actually the fact you are saying that the coordinator actually fail in the second phase means that taken a decision right now  it means that decision has been taken decision of aborting or committing has been taken now one of the conditions that can happen here is as this decision known by another partition which we can say now there is a case were at least one participant knows the decision coordinator before he fail  refer slide time  13  53  so  the scenario one is here one of the participants received one of the live participants  which means that he is actually live and then he actually receives the coordinator of decision live participants received the decision which actually means that all that you need to do in this particular case is  since the decision is known the same decision can be conveyed to everybody  refer slide time 14.27  if it is a commit decision everybody will commit  if it is an abort decision everybody will abort right it is very simple case where the decision of the coordinator is already known the coordinator fails some participant does not receives the decision  refer slide time 14.47  all that they have to do now is  find out if any of the participants has received the decision of the coordinator if they have then will become simple scenario were that decision can be implemented by other participant also in this particular case  it becomes non-blocking that means that normal that means is nobody actually block this is very important you are not waiting for somebody to recover back right  refer slide time  15  25  for example  if shooting is happening  our recording person decides to go out for tea or something  and then we are all blocked since the time backs know that is a blocking protocol you do not want to think blocked you wont actually even it goes and put in automatic modes and off then we are not blocked we can continue our job similar way  even one of the coordinators fails and it still wait for the participant to recover back from the failure it is a very nice thing it is a very simple case that is how we have to the model of the protocol we do not want to know any other participants suffer because of this failure suffer means have to wait now is you do not know when that recover is going to happen is you get indefinitely blocked if such a thing happen so one of all the cases we have seen so far two phase commit protocol is non-blocked it does not block anybody  refer slide time  16  44  now there is an interesting case that is going to come now where we can see the case were the scenario 2 were the coordinator fail none of the live participants know about the decision of the coordinator but again there going to be two sub cases here now we can say decision of the coordinator is not known in this case  it is possible that you know all the participants are live  that means there is no other participant failure that means only coordinator has failed the decision of the coordinator is not known but all the case where this is only first case is not the second case first case  were all the participants are live that means there is no participant failure all the participants are live in this particular case  since all the participants are live  a simple thing they can do is they can restart the two phase commit protocol by electing a new coordinator in this particular case  you will elect a new coordinator and restart the two phase commit protocol  refer slide time 17.54   refer slide time 18.20  you are going to see is  the case where if you say that the two phase commit the participant also failed along with the coordinator right so  there is not only a coordinator failure  but there is a participant failure which means that  now the rest of the people the interesting case is here  refer slide time 18.51  imagine that the coordinator took a decision of committing and that is only known to the participant and he actually wrote all the values of the database and then he actually fail now  if the remaining participant elect the leader  coordinator and then terminate the protocol  it might contradict if the earlier coordinator already taken a decision and that is know to this partition there is no way to recover back from this so basically this is the case were all the other participants in this particular case  all the live participants are blocked all the live participants are blocked till such time the coordinator recovers you can not do anything except to wait for the coordinator to recover wait for coordinator to recover now often a coordinator failure also means a participant failure  because the transactions were it is started normally coordinator is one of the participant sides  refer slide time 20.09  for example  if you basically look at how the transaction is started one of the sub transactions were it is getting executed he also takes up the responsibility of being a coordinator normally coordinator failure will be equivalent to a participant failure and that is the scenario were two phase commit will not be able to recover back and this is an important case normally you do not have a coordinator failure a coordinator failure normally means a participant failure only thing is  if it is fail in the first phase then you are bit lucky because in the particular case  the remaining participant can elect the coordinator can make a decision but if you actually failed in the second phase  phase two after actually sending the ready message  you discover that the coordinator actually fail you will be in trouble because you would not know whether the coordinator is taken a decision or not take the decision so that is the case were the participant is forced to wait till the coordinator recovers back from the failure now have explained for this  what we are going to see is how this protocol gets modified to also allow protocol to recover in the case where the coordinator fails in the second phase along with the participant and that modification is what we see as the three phase commit protocol what the three phase commit protocol does is  intuitively to understand what the three phase commit protocol does is  when the coordinator actually fail you would not know what the state it state is  and this is very important distributed system this is the typical problem you will face in the distributed system you need to know the state of other node when things have actually failed or when the system crash if you know the state of the system is easy for you to recover back but now if you do not know the state of the system  then you are in trouble recovering back from that failure of the particular system now  in this particular case when the coordinator actually fails you won ’ t know whether it is commit decision or an abort decision now what do you want to decide now is or want to see is  what is the state in which it is ? to get out of this ambiguity  for introducing a three phase that means the coordinator will not directly to get in to the phase of committing if you carefully observe the problem is in terms of moving um coordinator moving directly when it gets the ready message in to the commit state if you actually record in between a state where the coordinator says that  now i have received all the ready message  refer slide time 23.32  now i am prepared to commit  it is still not committed but  then now you introduce a prepare to the commit state for the coordinator which means that now you know whether the coordinator has prepared to reach the commit state or not if it is not reached to prepare commit state  then no harm done because he failed in the second phase but only in the third phase actual commitment would have taken place right basically you need to know the state of the coordinator when he actually die or when he fails so in that you remove that you need to introduce a third phase that ’ s were you basically look at the three phase commit protocol what were we are going to do now is see how it get modified for the three phase commit protocol or how the three phase commit protocol works and also look at what kind of failure scenarios the three phase commit protocol can tolerate or it is resilient to what kind of failure  refer slide time 24.49  now what we are going to do is  for both the participant as well as the coordinator will introduce a new state a new state were they have to first get in to the before commit state or prepared to commit state before they actually commit they can not directly get in to the commit state from the ready state now the coordinator part is going to look at something like this will start with an initial state and now as usual send a prepare message to all the sides then i basically reach the undecided state here because i have still not decided on the state now if i basically get all the ready messages then i will actually enter what i call as a before commit state i have still not committed so  i basically reads a before commit state here now once i reached the state before commit state and i receive acknowledgement from all the participants then i take the commit message  that means i get an all those participants then i will issue the commit message and then i reach the commit state after receiving ok from all the participants now it is possible for me for various reasons to move from before commit state to an abort state if i do not receive okay from all the sides  i can still end up in an abort state as usual i can also come to the abort state if there is an abort acknowledgement message from one of the participant then in response to this  as usual will take the abort command message it also possible to a timeout message i might reach the abort message now this is the diagram for the coordinator state diagram for the coordinator look something like this participant state diagram the three phase commit would look something like this participant will be initial in the initial state when it receives the prepare message it will reach the ready state  if it is willing to commit the transaction in that case the participant will reach the ready state in case  it is not willing to commit the message then it will answer with an abort answer message and will reach the abort state from the ready state also  it is possible for the participant to reach the abort state it receives the abort command from the coordinator which is possible for the participant to actually reach this state if there is an abort command message and then the participant reaches that state through the ready state if the other participants are not willing to commit  the coordinator can take the decision to abort the transaction in which case  participant will move from the debit state to abort state if all the participants are ready to commit the transaction which means that  all of them have actually responded ready message then it is for possible for the coordinator to take a commit decision in which case in the three phase commit he will initially enter the state called the prepare to commit state  which means that he will prepare to commit message to the participant when the participant receives the prepare to commit message  he will reach prepare to commit state now he reaches the prepare to commit state is not still commit the transaction is waiting for the final decision coordinator when the coordinator receives for all prepared to commit messages  then he will actually issue the commit message it is a commit instruction in which case the participant will reach the commit state this additional state is needed because is possible that if the coordinator fails know after one coordinator and one participant fails after reaching the prepare commit state is still possible for the participant to elect a new coordinator in which case it possible that decision to abort has been taken by the new coordinator in which case after coming back after recovery from failure  it is possible for the participant to move from prepare to commit abort state now this is the participant state diagram we looked at the actual failure scenarios here  refer slide time 31.06  now the case where the failures can occur as usual participant failing in the first phase second phase are similar similarly the coordinator failing in the first phase is also similar only the case were we need to look at the second case where the coordinator actually fails that is what we recorded there that means there is a coordinator failure in the second phase and there is also one participant failure or also we have a participant failure this is the case were we have a problem earlier now let us examine this case and see what happens in this particular case in terms of how the recovery takes place in this particular case as the coordinator fails in the second phase along with the participant now in this particular case  there is going to be third phase obviously  because the second phase is not the last phase now if the coordinator nobody receives the decision from the coordinator obviously at most  the coordinator is likely to be in the second phase which means that the participant here could have been in the prepared to commit state prepared to commit state it has not yet actually committed so at most  this state can be prepared to commit  it can not be commit state the coordinator would also been this is for participant this is the state of the participant can be this now the state of the coordinator can be also before commit state now if these two are in this state  the rest of the participants can still now elect a new coordinator  refer slide time 33.20  and still go ahead with the protocol  because they have not still committed yet they have not committed the transaction now  in a simple case were the coordinator actually passes the second phase  now you imagine this is going to be a sub case in this particular case where the coordinator has passed in to the third phase means it fail in the third phase not in the second phase now this becomes very simple case coordinator fails in third phase  refer slide time 34.06  this is the simple case because the coordinator actually taken a decision to commit and this is actually conveyed to all the participants otherwise  you would not have got him in the third phase because unless everybody replied for the prepared to the commit state  he will not reach to the second phase in the commit message which means even one of the participant have received it then only  it is cleared that actually moved in to the third phase otherwise he is actually not moved in to the third phase so in this particular case is easy for you just commit the message because you know the decision of the coordinator that it has been actually committed  refer slide time 34.58  so this ambiguity of not knowing the coordinators state when it actually failed is taken off in the case were the state of the coordinator is known by introducing an extra phase was the participant and the coordinator directly do not commit but they actually reach before state before commit or before prepared to commit state before actually committing and that is how this ambiguity is actually resolved that is how the failure is handled by introducing an extra phase for commitment now one of the interesting thing is  how the election is going to happen because every time  actually seeing that if the coordinator is failing these equivalent to actually some kind of a leader election algorithm because you are actually electing a leader whenever there is a coordinator failure  there doing a leader election in a simpler way  the leader election can be done by a signing in terms of each participant for each node an id and these ids could be in an ascending order obviously every node is going to get the unique id here that means there is a unique id node is going to get and you can actually assign at any given point of time  simple way assign the highest of the you can use the node id or highest node id so basically you can assign the live node with least id as the leader however  this is not simple because  now you actually find out you are you should take as the leader because  somebody has to dictate that the coordinator has actually failed that means what really is going to happen is  if a participant discovers it is going to be only the participant who is going to discover that there is the coordinator failure because he is waiting for some decision from the coordinator that is how the participant discovers that there is the coordinator failure so what happen is  if participant discovers it can be anybody a participant discovers that the coordinator has failed now what he does is  he has to do an extra if he basically discovers that  there was the failure of the coordinator  then he has the option of finding out or he has to do this process of finding out  who is the next highest or least node that is still live  refer slide time 38.04  so  he starts now looking for the possible highest least node  live node and tries to know make that node as the coordinator but then  that node also discovers that the coordinator would have fail participant so basically all that need is  it has to see whether it has a least node  live node and if it so basically it now elects himself as the coordinator starts running the recovery protocol  refer slide time 39.00  so in this particular way  any number of coordinator failures are tolerated by this protocol because all that protocol has to do in this particular case is  make a participant whenever it discovers that there is a coordinator failure  figure out the next highest or least node that is supposed to take over and its turn to take over it will basically become the coordinator tries to run the protocol now  there are two ways this can be further achieved in terms of how this protocol can be run by the coordinators one of the things the properties has to ensure that protocol is a reentrancy of a protocol that means the protocol can be run any number of times that means a coordinator fails then a new coordinator takes over now  while running the algorithm protocol that coordinator could also have failed which means that another coordinator has to be elected it has to take over this should become reentrance that means any number of such failures should be possible when you are actually recovering from the failures in which case basically the protocol is called reentrant protocol that means it is tolerant to multiple failures of the coordinator any number of times so  it becomes a reentrant protocol now there are two ways the reenter protocol can be configured one is a proactive way of actually making it move forward the other is basically you know a pessimistic way of reentrance  refer slide time 40.35  this simple way what the coordinator can take as the decision is  if the coordinator is in a state of before commit state  he will basically whatever the state in he basically enforces that on the other people let us understand this point a little carefully the coordinator has failed in the first phase which means that node decision has been taken by the coordinator now a new coordinator is in place how this new coordinator is to start the protocol now a new coordinator realizes that obviously the new coordinator is not likely to be any other state other than the first phase he has not received any of the coordinator so he is going to run the entire protocol staring from the first phase  refer slide time 41.45  now he actually imitates the prepare message to the other participants receives the ready message then get in to the second phase then goes in to the third phase things like that now the new coordinator can be further improve the protocol is  whatever he is state he is in he can actually find out if there is any other participant who is in a before commit state which actually means that now he can actually need not restart the protocol  but then he can knows that actually the decision of earlier coordinator is actually to commit the message  refer slide time 42.22  now you could actually when there is a before commit state  it is possible for the protocol to terminate as an abort because the new coordinator now ignore this decision but then just conducts the sends a message to the all the participants now some of them not going to reply back to him then he can actually enter the abort decision is quite possible for the new coordinator to find out if one of the participant has reach the commit state in which case  actually it can force the protocol to a commit state  refer slide time  42  51  to just explain this a little more probably in a using our diagram  you can see here the coordinator actually the new coordinator who has elected  refer slide time 43.30  now if you basically dint receive the prepare to commit message which means that he will be in a before state he is still not received this message which could have made the whole protocol terminated the abort decision now  if you actually checks with the other live participant and at least one of them has actually receive this pcm state  but is not the new coordinator this some other has been elected as the coordinator now then  it still can take a commit decision because everybody would have reached a state so it is possible for unless all of them replied with a ready message this state would have not reached right  now the new coordinator can take a decision to commit a transaction and then make it know basically  all that required for the elected coordinator in this case is not only check its state  but then checks the state of all live participant based on that restart the protocol at an appropriate point  refer slide time 44.29  it is done like that the reentrant protocol terminates in a forward direction in terms of committing wherever there is a possibility of a protocol to commit it will commit rather than always sending out of the abort state  depending on the state of the new elected coordinator  refer slide time 44.47  so these two approaches are possible in terms of recovery from the failures as far as the coordinator is concerned i think what we are going to do in the next two minutes is  not only look at this scenarios of failures but then also introduces now  the concept of what happens with the network failure  because you are now talking about only node failures so far  we are not talking about network failures as part of the recovery process now in terms of network failures as far as the protocol is concerned  the two ways of actually interpreting the network failure   refer slide time 44.51  network failure actually means that the nodes are not reachable so let us take for example a node here this node could be a chennai node and this other node could be delhi node now all that you are looking at in terms of this system is  both of them are connected by the timings of some  let us say back bone network that could be several network it involved connectivity with been these two nodes all that you have is basically a network  which is actually providing this connectivity now when the network actually fails  in a simple case this actually surfaces as the time out on the other side that is how actually you are going to discover you are waiting for a reply and this scenario actually come as the timeout in our diagram as we saw earlier and that timeout could be possibility  refer slide time  47  00  because the network has actually failed not the node has fail in either case it wont be able to detect the difference between a network failure and a node failure because the node as far as for example for this chennai node is concerned it does not receive the reply in time from the delhi node it could be network failure it need not be a node failure the node can be still be kicking doing some work there  but when it actually sends a message  let us say ready message  refer slide time  47  24  chennai node does not receive the ready message this is equivalent to actually a node failure that means a network failure in some sense actually translates to a node failure the simple case this translates to a node failure  refer slide time 48.00  but then  it is probably not appropriate just consider a network failure is just a node failure in our case it still works  because when the timeout actually reads out the other side all that you are assuming now is  the decision of the node is to abort so nothing no harm is done in this particular case but could have done positive thing is ends up in the negative side because you could have still commit the node has still responded with a ready message but since it does not reach the other node and it decided that when it did not have any information that best thing it can do is abort the message or abort the transaction that ’ s what actually assumes then abort so basically a network failure and node failure in a normal case both appear to be treated in this same way right and that is how this protocol both two phase commit and three phase commit are resilient to network failures in terms of translating them to the participant failure but one has to be careful when the network actually fails  but then that failure is not just to one failure of one node but it can result in not able to reach to multiple nodes so typically this scenario is a very simple case where it has been translated to a timeout case but then assume that a network failure actually results in multiple nodes not being reachable  refer slide time 49.43  now  let us imagine we have node in chennai let us say  this is chennai node and then we have actually bangalore node and then we also have mumbai node here and then we have one of delhi now it is possible that a network actually connecting them let us assume that complicated network here which actually connects all the nodes with one another but then  if there is actually some kind of hub that connects the southern nodes together and the northern nodes together  it is quite possible that this hub actually fail which means that these nodes are can still each other in this direction and these nodes can reach each other in this direction now assume that chennai is the coordinator now as far as chennai is concerned  the whole problem translates into failure of both mumbai and delhi  because it still reachable bangalore is still reachable and let us say  it replied with whatever required messages so  this basically results in partitioning of the whole set of nodes now you have actually two partitions there is one partition  one here which consisting of mumbai and delhi and there is another partition two consisting of chennai and bangalore  refer slide time 51.40   refer slide time 51.56  now it if you carefully looked at this scenario and try to understand what could have happen in this  it is possible that chennai and bangalore could start detecting that there is participant failure which means that two participants have failed and two phase commit is resilient to participant failure so what basically they do is depending on which phase you are in  you might end up actually making the transaction so basically these two might right to run this protocol saying that there are two participants failure  refer slide time 52.29  now as far as mumbai and delhi are concerned  for them it translates to one coordinator failure one participant failure they also try recovering back from this problem by saying that now they will have a new coordinator depending on what phase they are in and how they should be handling this problem they might actually try recovering back from this failure now in this case  it ’ s possible that the decision of two partitions need not be consistent and this is happening mainly because the nodes are delivering it is a network failure they are seeing them as failures of the participant nodes  refer slide time 52.58  the participating nodes are still live and they have working so for each one of them scenario is different unless this is discovered that is network failure not a node failure you would not be actually making an distinction network failure or the node failure and this really results in a complicated situation were even if you use a three phase commit protocol were the network actually partitions the nodes into multiple partitions  you wont be able to recover back from the problem in other words  you are making an assumption here is there is no partitioning that is happening this is the case no partitioning of the node and this is an assumption with your actually working on the two phase commit protocol if the network partition you have the problem of recovering back from the failure because all the case which discuss which we looked at are actually taking them participant failure and not really participant failures and it is a network failure basically will have a difficulty  refer slide time 54.18  an interesting assignment could be what are those cases were the network partitioning network partitioning can lead to problem which is the case in which the protocols the states of the coordinator and the participants in which case the partitioning could really be a problem the other thing is to discover if you are the majority partition probably if you are the majority partition  probably you can still go out do something which means that you have to discover that the system has actually partition and whether you are the majority partition and based on that probably can recover out of the failures it is still a very open ended problem in terms of how one can recover from network partitioning it is all interest in see how one can make a distinction between network partitions and node failure for example  one way is to actually  if you just do a thing you only know node whether the node is live or not  that ’ s still remain the interesting problem in terms of seeing how one can make a distinction between a node failure and a network failure in a distributed system right now  what we have seen is the basic commit protocol in the next class what we are going to see is  how this commit protocol can be integrated with concurrency protocol for example  how the two phase locking can be integrated with two phase commit protocol that ’ s going to continue with the next class database management system prof d janakiram department of computer science & engineering indian institute of technology  madras lecture no 26 concurrency control for distributed transactions in the earlier lecture  we just looked at how to face commit and three phase commit protocols work what we are going to look at in the lecturer is the concurrency control protocols for distributed system how this concurrency protocol can be integrated with the commit protocol in the first few minutes we are going to look at is how the two phase locking protocol can be integrated with the two phase commit protocol for the distributed system now what we have is basically in the two phase locking  there are two phases of execution which we have seen earlier there is basically a growing phase in which the transactions acquires the lock and then there is an actual point were they start releasing the locks and the rule here is that in terms of time  this is the time axis now in terms of the time axis  what you are looking at here is a particular phase were this is basically called the growing phase where the transaction is acquiring locks and this is the shrinking phase were it is releasing the locks now one of the condition is this is basically called the lock point so if you have actually released one lock  you can not ask more locks that is how basically ensures that transaction is always executing in a serializable order now  this is basically transaction execution point for example  this is the start point and this is the end point now  if you release a lock for a transaction  then somebody else in between can read the values of this data item that we have actually modified  refer slide time  3  30  for example  assume that even before you have finished you released a lock on a data item which actually means that somebody else can now read the value of that data item assume that there is a data item x and now you actually have acquired a write lock on x during the growing phase and then release this lock during the shrinking phase now it is possible that when you have actually modified the value of x let us say x is now modified from earlier value is twenty  now it got modified itself to thirty now during this phase when you release the lock  somebody can acquire this lock  which means that they can read the value of x which you have modified before you have actually committed which means that if you now want to abort  then the transactions which read this value earlier all have to abort  refer slide time  4  32  this will result in cascading aborts if you allow transactions to read the values other transaction to read the values of the data item before the transaction is actually committed so in general  this execution of the two phase locking is not correct because you can not release the locks before you have actually committed so what modification that we are going to get when you integrate with two phase locking commit is you will basically acquire all the logs you will come to the commit point but your lock point and you are going to keep this locks till the end of this transaction  execution  plus the commit phase only after the commit phase  after you have committed that this is the point were you have actually commit point your going to release your locks in terms of the time scale this is the number of locks that you have so the number of locks is actually increasing with time the transactions actually started at some point here and it is started executing it acquired all the locks it required the point actually where it reached it required all the logs  then it continued execution  somewhere here it actually finished execution that means this is the execution phase now after the execution is over the commit protocol is going to be started now because then it has to decide to commit  then this is the zone which the commit protocol gets executed and after the commit protocol is executed  the transaction actually finish execution  then the locks are released at the same time  refer slide time  6  46  if you release the locks earlier  this will result in the isolation property being sacrificed and other transaction able to read the values of still uncommitted  modified value of this transaction so you are not going to release the locks as shown in the earlier diagram but this integrates both the locking and the commit protocols together and after execution is over  you actually run the commit protocol and then release your locks this is how the two phase locking is integrated with the commit protocol now if you want to see in the distributed setting  how the distributed two phase locking will work it is actually a simple extension of the two phase locking  because what we now going to see is distributed two pl plus its integration with the two phase commit  refer slide time  7  46  two phase commit is for distributed transactions  because commit protocol makes sense only in the case when you have where your transaction is executed otherwise  there is no question of involving two phase commit there because you have actually finding out the participants can all commit their transaction there is only single node that nodes knows know already reaches the commit phase all that it has to now decide is whether its committing the transaction or not now  in the case of distributed two pl how you are going to execute the transactions is you basically have a coordinator which we have shown earlier this is basically the route transaction and the route transactions have actually spawn sub transactions which means you actually have the tr 1 is here and there is a tr 2 here now the tr 1 actually executes a two pl which actually means that it acquires all the locks that it actually needs and this other transaction also acquires all the locks it needs in terms of the two pl  refer slide time  9  10  now each one of them when they reach the commit phase have to decide whether they have are committing or not committing when the actual commit message has been taken by the coordinator that is the time they will commit and release their locks imagine that there is actually a data item x on which this transaction is operating  t1 is operating there is a data item y on which the transaction two is operating now t 1 will acquire locks on x  t 2 will acquire locks on y they finish their part of the execution  refer slide time  9  56  now at the end of the execution  each one is going to decide whether it is going to commit its part of the transaction that is were two phase commit will start know execution out two phase commit protocol is run you figure out whether both of these transaction are willing to commit now based on that  based on the commit protocol outcome  they will commit the transaction and then release the locks now the important thing is what we discuss is the earlier lecturer is what happens when failure occurs then  what is the problem of blocking ? now you assume that t 2 gets blocked transaction 2 get blocked which actually means that is not going to release the lock is because it acquired since such time it actually made a decision on committing or abort the locks are not released till the commit protocol has actually finished which means that the other transactions which try to acquire locks now on these data items will be blocked from acquiring the locks so basically blocking in that sense is not good  for the simple reason that the resources are blocked in this case  data items are blocked from access by other nodes what i am going to show you is a slight extension of two phase locking protocol the basic two phase locking is used on the actual nodes that means the each node will run a two phase locking for the data items it is actually accessing but this two  refer slide time  11  44  phase locking is used for local data items you are not further looking at tr 1 accessing a data item that is remote to that particular node and if you basically want to look at that model  you will realize that there could be problems of applying two phase locking where each sub transaction could access both local data items as well as remote data items  refer slide time  12  06  assume for example  you have a root transaction here this is the root transaction and then you basically have two sub transaction t 1 and t 2 here correspond on two different nodes  n1 and n2 now let us say n one as actually data items x y and let us say this is having p and q now assume that t 1 actually wants to acquire a lock on x a read lock on x and then also want to acquire a read lock on let us say p  remote lock on the now which means it has the send request lock to the other node n 2 now for example this is already locked by t 2 you have actually known t 2 has actually acquired a write lock already on p  refer slide time  13  22  now unless you have understand t 1 and t 2 are the part of the same transaction t 2 will be blocking t 1 from acquiring a lock on p normally  what you do in the distributed transaction model is to access the local data items only you will actually spawn a sub transaction here you would not actually allow a sub transaction running on one node  refer slide time  13  49  you acquire locks on basically any time you required to access a data item on a different node you spawn an agent there which is basically a sub transaction of your main transaction basically  if you want to provide this extension  there is an extension that is possible for a simple two phase locking which is the nested two pl  refer slide time  14  29  the extension two pl takes care of the extension of the two pl to the scenario of distributed transactions where all this transaction sub transactions are seen as children of the root transaction now any lock requires that basically is needed by the transaction is the lock request is sent to the root node and it is basically the root node which acquires the locks on behalf of the children node  refer slide time  15  01  this could mean that for example  the t 2 actually wants a lock on p 2 p actually this lock is acquired in terms of the root node and then inherited by the t 2 now when t 2 actually finishes  this lock is not released but then given back to the root node now if t 1 actually wants the lock  it gets it from the root node so  all data items the locks for the data items are with the root node now any time you actually require a lock you are making a request to your root node and acquiring a lock and when you actually finish you actually locking the root the root acquires this lock back now if these are actually the now till the same transaction the locks are held by the root so  it basically it does not conflict the two transactions child transactions of the same root node does not basically conflict with each other  refer slide time  16  29  the conflict is relived because they are children of the same node now the way this execution is going to be done in the nested two pl is there are two kind of models that one can think of one is called the open nested two pl and other is the closed nested two pl now in the case of open nested two pl  we are actually assuming that in the tree structure that you have  these are all actually sub transactions that are possible now  this is basically the root  can take this is as t 1 and this is t 2 and there actually two sub transactions for t 1 and they actually two more for t 21 t 22 so basically you have a tree in terms of how showing the transactions are spawn  refer slide time  17  29  now it is possible for only the child nodes to acquire the locks in terms of actually operating on the data item  that means t 1 will never do anything except saying that there are t11 t12 which basically access or modify the data item that means only the leaf nodes   refer slide time  18  00  one possibility is to say only the leaf nodes are allowed to modify or process data item and if you allow these intermediate nodes also to do this processing  then it becomes open nested 2 pl and if you put the restriction saying that only the leaf nodes can do the processing on the data items  it becomes a closed nested 2 pl  refer slide time  18  20  typically  what the nested transactions are meaning here is t 1 consist of t 11 and t 22 and it is possible for t11 to further go down and have t 11 and then this can be you can further have t 112 this is the sub transaction of t 11 you further have nesting which means that you can actually make this nesting  further nesting which means that you can always divide the main transaction in to a sub transaction now in terms of an open nested transaction  it is possible to actually covert an open nested transaction model in to a closed nested transaction model  because if there is processing being done at t 1  you can typically make that for example  this can be now here spawn separately as a sub transaction of t 1 dash which means that the processing is never done at the intermediate node  refer slide time  20  15  if there is a processing involved  you spawn a sub transaction to do it that means in terms of the root other than the leaf nodes  everybody else actually is spawning sub transactions and whatever the sub transaction do that result is being aggregated at the higher nodes they do not do any processing by themselves a simple extension of nested two pl two pl to nested two pl requires that  whatever locks are acquired by the children for doing processing are passed on to the parent when they finish execution of the transaction and it is easy to see why nested two pl will obey the serializability condition all that that has nested two pl has is that the children never release the locks but they are inherited by the parent when the transaction finishes now  it is possible for another child to acquire these locks from its parents that means when you need a log  you have to first see whether your parent has the lock  then you get the lock from the parent  refer slide time  21  28  you are inheriting from the parent the lock we assume that all the locks are with the parent so the only way you can acquire the lock is by making a request to your parent when you actually finish your execution  you are returning the lock back to your parent now you can see why this simple nested two pl will ensure serializability by looking at intuitively what is happening here now  if you basically see that there are two branches of the tree and there is a lock acquired by any of this children here  there is now an inherent no serializability at the higher level because unless let me put in terms of simple tree structure nodes one  two  three  four  five  six  seven now you can see that the inherent order in which this execution can proceed is  if three requires a lock or six requires a lock that is currently held by any of the other nodes the only way it can be obtained is parents which actually means that unless two finishes  refer slide time  22  51  one can not acquire the lock that means at the higher level the execution is always proceeds in terms of this tree  the nodes of finishing one node is which is earlier till finishes and later the other node can finish otherwise  there is no way other node than finish so inherently that tree structure will be ordering the way the transactions are executed and inherently provides a serializability condition now you can say that  that means the only execution sequence that is possible in the case is four after that five if there is a conflict  then six and seven  refer slide time  23  38  if there is no conflict  it is possible for this tree structure to be executed for example  where the leaf nodes are all so that the same tree structure that we saw earlier it is possible that  i can have execution where if there is no conflict  because if there is no conflict  it does not really matter how six and seven are actually executed if there is a conflict that means  if five is asking for lock on data item x and six is also asking for the data item x  this wont be possible because this lock would have been held by node unless it finishes execution  this would not have happen and hence this will be prevented you can see in a simple case  a nested two pl is an extension of a simple two pl where a transaction actually spawns a sub transaction  refer slide time 24.43  and now that sub transaction can further spawn other sub transactions and then each sub transaction when it finishes gives a lock to the parent and that is the way to execution can proceed and that inherently ensure is the serializability requirement this is an extension of simple 2 pl to nested 2 pl for distributed systems this is one possible model you also seen a number of actually time stamping scheme as part of our earlier discussion on transaction models now it is worth actually seeing what exactly happens with time stamping schemes in their distributed scenario now what we have is  the basic time stamping scheme which shows how exactly transactions can be executed by having a basic time stamp being given to both the transactions as well as to the data items now in the basic stamping scheme every transaction is given a time stamp at the beginning of the execution now this time stamp could be just the logical lock value that we saw in the distributed systems  refer slide time  26  47  which means that this is a local clock that is maintained by the distributed system which in affect means that this is to be either synchronized clock or some kind of logical synchronization happens these clocks such that the time stamp reflect the total order among the transaction that have actually executed in the distributed system one way to do this is the last two bits  know least significant bits a part of each node and the higher bit is actually a synchronized bit with the other clocks of the other nodes it is possible for you to give a time stamp for each of the time transaction originating in each of the nodes of the distributed system now  if you assume that every transaction entering the distributed system can be executed on any node of a distributed system  so this time stamp corresponds to the logical time that we saw in terms of the different node synchronizing their logical clocks one extension is a simple case of mutual exclusion algorithm that we had implemented in the case of lamport ’ s clock the same scheme is applicable here except that instead of mutual exclusion is going to have a read lock or write lock that you acquired on the data item that means in affect this is equal to the time stamp being used for writing on the data item or reading on the data item now different nodes will decide the order of the in which they can access this data item by using the time stamp value the basic time stamping scheme what you are essentially doing is you are actually giving time stamp at the start of the execution of the transaction now as the transaction is start executing your actually looking at its read set and you are seeing its write set and then you are deciding in terms of the read set and the write set  what are the time stamps of these data items and making sure that the transactions are executing in terms of the time stamp order now  assume that every read and write data items in the data base have given the time stamp value  refer slide time  29  54  for example  let us take a simple case of actually looking at a data item x which is in the database now each data item will have two time stamps corresponding to it one is the read time stamp that means this gives basically the read time stamp what the read time stamp will you give is the value of the timestamp  the highest value of the timestamp of the transaction that has read x that means it tells  which is the highest time stamp transaction that has read its value now similarly we will have one of write time stamp x which tells correspondingly the write time stamp of the transaction this is basically the write time stamp the explanation for the write stamp is similar except that instead of what we have put there as read now this is the highest value of transaction which has actually written on the data item x the value of the time stamp of the transactions  that has return on x  refer slide time  32  04  now the use of the read of time stamp and the write time stamp is for serializability  you always allow the transactions to only do in terms of increasing values that means if there is new transaction that comes in whose time stamp is lower than the write time stamp and if you want allow to read the value you obviously will have problem right for example  if the write time stamp of the data item is greater than the transaction time stamp  then you should prevent it from reading the x value that is basically you to abort the transaction and then ask it to come again with the higher time stamp so that you are preventing the access to the data item in terms of always the data items should be access by the transaction in terms of increasing time stamp if you ensure that you are automatically achieving the serializability condition  refer slide time  33  20  now in a simple way in a distributed setting you are going to extend the basic time stamp distributed transaction model by ensuring that these time stamps are generated in a consistent way by the nodes of the distributed system if each one of them generate the time stamps the transaction time stamp are generated in a global way even when the data items are distributed you are still ensuring that globally all the reads and writes are ordered according to the time stamps of the transaction since the time stamps of the transactions are generated in a consistent way in the global system  we are essentially ensuring a serializability order for your transaction coming in the distributed execution basically that is an extension of basic time stamp scheme to the distributed scenario the extension is quite simple and straight forward again in this particular case ensuring that the logical clocks of the distributed system are synchronized and executed properly as per that order now one of the interesting extensions that one can see is  a completely optimistic extension of the time stamping scheme were the data is fully replicated on all the nodes which means that now every node has a data item available locally so what you are going to do now is  you are going to look at each node is going to send its read and the write set to everybody and now everybody can vote on the reads and the writes and if whatever reads and writes you have appropriate number of votes then you will be able to view the modification which means that the majority of copies are always consistent and always proceed in the same order as being done by the nodes let us look at that extension and is often called the optimistic time stamping scheme and i will actually summarize the different time stamping schemes which have done earlier with respect to the distributed scenario the optimistic scheme is interesting extension because what you have is here a fully replicated database that means the data items are fully replicated on all nodes now take a simple case were there is node n 1 and there are two data items x and y and take another node n 2 what is going to happen now is  it will also have x and y which is essentially means that all that a transaction is going to do now is going to produce you known a list of data items that is willing to modify along with the transaction time stamp which is trying to do  refer slide time  37  00  now what is going to do now is  it needs to send the information to all the other nodes and other nodes have to now say whether they are willing to accept the list sent by another node  that means every transactions modification what is read what is to modify will be send to all the other nodes and now those nodes can decide whether that update list is consistent as far as they are concerned if they say yes  then if enough number of yes votes are come for a transaction then obviously can commit it does not get that it does not basically commit which means all the processing always going to happen locally but then you have to send your read list and write list for voting to other nodes now the other nodes can decide to vote yes in this particular data item let us take a little more detailed way and then see what really happens in this particular case and how the transaction execution is going to proceed in this particular case  what we going to see is let us say i have a transaction t 1 which started at let us say ten twenty here with respect to its time stamp now its basically trying to read x and write y so that means its read set consist of x data item and it is basically write set consist of y now it will modify whatever it wants to modify as far as this transaction is concerned and produces this list for other transaction to vote  that means it will communicate this list to node n 1 saying that as far as this t one is concerned  it is at time ten twenty it is modifying x and y and that list is being sent to the other node now assume that  there is basically a read time stamp on x which actually shows again  what is the highest value of the transaction that has actually read x and similarly the write transaction on y now what you are going to see is  if this list is actually non conflicting as far as the other node is concerned  what we mean by non conflicting is basically there is set of transactions want to execute on other node as well now what you have to check on the other node is before the modification is done whether this modification is consistent as far as the other node is concerned and the consistency criterion is here is  this x and y whatever this transaction is trying to do now is later than whatever has been done already in the other node  refer slide time  40  22  which means that the corresponding rts and wts tools are applied on the data item and if it is consistent as far as that is concerned then you have vote saying that  this is okay for me because as far as i am concerned  there is no conflict of this transaction for serializability and if it is basically conflicting and this condition does not hold good the rst or the wts not being know in the case of write transaction  it has to be more than both rts and wts in the case of read transaction  it should be more than the write transaction wts so  in that sense we apply the consistency rules of rts and wts on the other node and decide whether this list is consistent now that would not be enough because this is only a proposal to modify it is still not modify they could be several such list come to me now it is possible that there is a pending list with me and i have to actually compare these values with the pending list also  what is pending now with me ? now let us say in the pending list  none of it actually conflicts with what i have then the rule is simple then i will be basically give a ‘ yes ’ vote for it  refer slide time  42  02  if there is a conflict in the pending list  which means that there is a list which came to me which i has actually voted now let us say there is a conflicting list with me and in the conflicting list  i have voted and yes for the conflicting list now  there are two things that can happen with the conflicting list let us say there is another transaction whose time stamp value is let us say ten twenty now there is a conflicting t dash direction here whose value is let us say ten o clock time stamp value is ten o clock and let us say for this obviously i have voted and as earlier that is why it is actually my pending list i have still not heard about this transaction  refer slide time  43  08  now one of the things  i can do in this particular case is since i have already voted for t 1 as ‘ yes ’ t dash as ‘ yes ’  the best thing i can do for t 1 is say ‘ no ’ because it is conflicting and an earlier transaction already said yes since i have actually said yes to an earlier conflicting transaction which means that this t dash has actually ‘ yes ’ for my case now i actually another transaction at t 1 which is actually conflicting with my pending list let us say this x value trying to modify and when it is trying to modify the value of x probably if we take a specific example here to make this little more clearer  refer slide time  44  03  basically what you are saying is this node n 1 when it got a list  it is voting ‘ yes ’ or ‘ no ’ right now let us say there is basically let us take node n 1 the execution sequence  the node n 2 the execution sequence it received an update list of tr dash at ten o clock and this is the list with x and y let us say  it receives this is the update list now  it did not conflict with anything by actually voted for yes it still pending on my side because i have not heard the final decision on this  because the one who has actually sent me the list if it get enough yes votes then it is going to get committed and send it finally to me as  refer slide time  45  01  an update list  then will apply the updates to the actual that is my update but  in this particular case this is the pending list i have voted but then the decision on this still pending there is no decision on this particular list now what happened ? there is another transaction t 1 that came by and it is actually ten twenty and then now if it is not conflicting with my pending list  there is no problem what so ever then i basically saying ‘ yes ’  now let us say the one which came which came at ten twenty as a pending with my conflict pending list now for that pending list  i have actually voted as yes now one way for me to do is  this ten twenty i would not reply immediately but wait i say okay till i get the decision on ten o clock  i am not going to vote ten twenty which means that differ making a decision on that is one possibility that means this list is not voted by me i am not going to say yes for it a ‘ no ’ does not create a problem because a ‘ no ’ is already been i have actually said i am not willing to go with that pending list  refer slide time  46  31  so even if it conflicts no really does not bother you because you have actually voted but some other reason you actually not willing to go with that transaction only when you say yes to something  then the current one is conflicting with it  then you have you have a problem now whether will this create the problem or will not create this problem because based on that you have to take decision now assume a scenario where this is this scenario 1 this scenario 2 could be same t 1 the same t 1 arrived at my node and now it is conflicting now here also let us say  i take the decision in to differ making a decision in this case now both scenario 1 and scenario 2 differ  then imagine a possibility were none of the list the likely would have getting a majority may not be there because everybody is waiting for the list to be voted and there could be the case were none of them might receive enough votes and everybody is waiting for everybody else suddenly  it looks like differing voting on both cases is not a good idea now the next case which case i better vote no and which case i should better vote ‘ yes ’  now  if you basically looked at i have already voted at for that ten o clock case is yes the best bet for me is that ten twenty can be differing because it is the later transaction suddenly if i have done something at ten o clock and somebody is trying to come to do at ten twenty it is still alright with me  but if it is actually nine fifty  i better in this particular case vote as no because i have already this any way will be only one chance it would have succeeded is the case were ten o clock did not get enough votes in that particular case  the t 1 has a chance otherwise it does not have a chance right so i can wait i can always wait both cases  then decide only when i know about the know fate of ten o clock transaction because i have voted for it and make any future things that have coming in till the pending list is clear i would make any other decision but what happened in the case is the possibility of all the transactions waiting for each other pending list and result in no body progressive so you better vote the case where it is nine fifty as you vote for this as a no to avoid this confusion  refer slide time  50  14  now at the end of it  the majority votes if a pending list actually majority vote that basically sent as an update list for all the nodes  that means that list is now has to be updated by all the nodes now the very fact any node has got enough votes any pending list has got enough votes that becomes a permanent write on it and you can easily show that no to conflicting things can get both can not get majority vote at the same time because only one of them can win if the two transactions are conflicting  only one of them in the end will be able to achieve the majority vote and that is the one which will go through and the other is going to be restarted if you basically look at it here  it is a completely optimistic scenario because you are executing all the transactions and you are never checking the consistency before start execution of the transaction what you are doing is you are executing the transaction and at the end of it  you are deciding by sending this updates to all other nodes you are seeing whether you can go through with this execution or not so this is typically the pessimistic scenario verses the optimistic scenario pessimistic as we told earlier  the consistency check is made at the beginning of the execution and then this is the begin that means actually you do this checking a locking would have ensure that the check is done in the beginning unless you acquire the locks will not proceeding on execution on the other hand  this check is made at the end which actually means that we are actually executed in transaction fully and then applying the consistency check now in that sense  basically having the two spectrums one is the locking based algorithms which fall in this spectrum and then a fully optimistic time stamping scheme which for in the scheme were the consistency check is only applied at the end of the execution sequence a variety of you know models are possible for as the transactions a models are consistent based on the time stamp scheme  refer slide time  53  10  one of the most interesting scheme is to give what we have seen in the case of example  visiting tirupathi temple you have the model were the band of time is given for you ; you have to go and then see you know thing at the point of time basically  you are scheduling transaction which can be executed in the future for example  you can say as far as t 1 is concerned  you can give a band of time which it can really execute which means that when it start executing you can say that  this becomes the time before you which commits it is alright for you this is like telling  if it comes at the gate at ten twenty this is not the start time  virtual time of the commit as far as the transaction is concerned  if it commits at let us say ten o clock its fine with me yours telling when it commit and it can actually finishes it execution so you can actually give an order in which  this is basically end finish time this is not at the start time that you are trying to schedule you can actually do this at the end for example  it is possible for you to tell even a band let us say  this transaction i can allow between 10 and 10.15 to execute  finish its execution if i generate this band such that  conflicting transactions will not finish in the same band i have actually know it is getting the serializability condition for example  there could be another for example  if the band that is given to these two transactions is different  if they are conflicting  refer slide time  55  01  let us say  i give a band of 10.15 to 10.30 for this it actually means that  suddenly they are not you know the serializabililty condition will not be violated here because one is finishing at 10.15 and other is finishing so i actually gave the band of time in which they can finish their execution that is other model i can give completely different kind of model is also possible were you can say that i will basically do what is called only transaction time stamps  but not data time stamps for example  all the schemes which i have seen so far have two kinds of time stamps one is time stamp to the transaction and i am giving the time stamp to the data item you could do a scheme were all that you are going to do is as the transaction is executing you can give time stamps to different points of execution for example  you can give start of the transaction finish of the transaction  what is the time and used that for actually doing the consistency check but as long as one transaction finish after other their conflicting and that you are able to ensuring at your clocks times that you have actually happened you are actually able to ensure the consistency condition in a simple way what we are talking about is  let us say t 1 there is a start time and there is a finish time of t 1 and similarly t 2 there is a start time and finish time and these times are actually given by using a clock here now  as long as the conflicting transactions are all as they are executing these times are given  you can ensure that the serializability condition as far as the operation is concerned by making sure that  the start and the finish time of the properly synchronized as far as the transaction execution concerned by doing this  we actually do not need to maintain time stamp on the data item  refer slide time  57  27  that is a saving in terms of not needing to maintain the time stamp a variety of schemes are possible we have seen couple of scheme in this particular lecture database management system dr s srinath department of computer science and engineering indian institute of technology madras lecture no 27 introduction to transaction recovery hello and welcome in this session today we shall be staring with a new topic namely that of recovery that is how to recover data or how do we bring back database into what we called as a consistent state in the phase of any kind of failures failures could be of any kinds let us say disk crashes  power shutdown or network connection failure  many different kinds of failures and we are going to be somewhat specific when we say recovery obviously we can not recover data that involved that were being processed in main memory during ram main memory during the crash but only we can recover whatever has been returned on to persistent storage like disc but what is written on the disk ? good recovery or somewhat semantics associated to what is written on to the disk and do we have to do something more when in order to recover from crash schedule ? in order to answer this question we need to know the concept of a transaction in a database processing environment therefore  the title of session is called introduction to transaction recovery in fact we are going to define the notion of correct recovery from wrong recovery based on the concept of transactions we are not going to study about transactions in detail here they are covered in a separate topic under itself transaction processing itself is a vast topic with several different aspects to it and we shall be concerned mainly with the recovery aspect here when it comes to and we are using the transactions to help us guide in deciding which kind of recovery is correct recovery which is incorrect recovery let us first define the term oltp i am sure you might you heard of the term oltp in several different contexts it stands for online transaction processing and environment that is the database system plus an application program plus any other associated accessories like networks and so on that goes into form an environment that is meant for online transaction processing  refer slide time  3  22  what is meant by online transaction processing ? as the name suggests  it is a processing environment that can interactively process database transactions we will define these terms in a much more accurate fashion later on  refer slide time  3  53  but  let us first look at some environments that can be classified as online transaction processing environment some examples are like airline or railway reservation systems what are the characteristics of such system ? one of the main characteristic that you can straight away see in railway reservation systems  for example   refer slide time  4  29  is that there are several numbers of users who are accessing the database system simultaneously that is you might have experienced it if you have tried booking a railway ticket over the internet suppose a train is getting almost full and if you delay in booking your ticket given by let us say some times even by few minutes  then never you may not get a conform ticket at all might go in to waiting list so that means  at that particular instance of time when your checking the status of tickets  there were several other people accessing the same database throughout the country that is they could be accessing via the internet  could be accessing via let us say some kind of queuing system that is across the booth or whatever from  they are all accessing the same database and several different transactions are happening at the same time and similar examples are of that of banking systems and especially how cash dispensed in atm ’ s or wire transfer mechanisms and so on another over tip environments require super market checkout systems and where customers come in with their baggage of whatever things that they have bought  they have to be checked out praised and build and so on  refer slide time  5  52  there are hotels and hospital system  trading and breakage system where buying and selling of the shares keep happening continuously whenever trading is on and several such sessions are happening simultaneously now let us take scenario which helps us understand what should be the properties of these transactions that go on in oltp environment now consider a small banking example where different accounts are maintained in a bank there could be on different databases or within the same database or different locations or same location  it does not matter let us just consider that there are different accounts and this database is being accessed simultaneously by several users performing several transactions let us say that one percent using net banking to transfer some money from his account to somebody else account  refer slide time  7  14  at the same time  other person is using an atm to withdraw some money or deposit some cheque or some cheque is getting enchased some did is getting enchased or withdraw whatever several things are happening simultaneously in the bank now  among this let us consider a small one particular example that account number 2,565 sense 2,500 rupees to another account number 165 so let us say at time t equal to 0 however we define our time  a transaction begins or set of database operations begin the application program that is making this wired transfer will initiate database operations which will first read the balance of 2565 now it will read the read the balance amount and because it is withdraw  it is a withdrawal from account 2500 will be deducted from this balance and let us say we are using concurrent applications which can have different threads of execution which can run at the same time so when this balance is being deducted here at the same time the balance of second account is being read of account number 165 and due to some reason  this thread process gets swapped of into disk by the operating system some other process is running because note that operating system of the scheduling processes to and some other processes running at the time and by the time  this process comes back it is time number 4  time t equal to 4 at this time  balance amount of 2,500 is added to this account now meanwhile let us say there is another transaction let us say  the account holder of this of this account number 165 is meanwhile standing in an atm and depositing certain cash certain amount of cash to the atm or may be he has sent a cheque and that cheque is getting clear so  why this person with the account 2565 performing wires transfer ? the person having the account 165 is also depositing amount of 3,000 rupees and it is so happens that  the way processes are scheduled this set of operation that is reading the balance of account number 165 here and adding 3000 rupees is done before the previous transaction finished that is before the balance of before amount of 2,500 is deducted from that is based on the previous transaction you can see what happens what has happened now ? the previous transaction has read the old balance amount and added actually this should be added i am sorry there is a small bug here this should be plus equal to that it is taken it is taken previous balance amount and added to 2500 rupees here while before it could that the previous balance was read  3000 rupees was added by the customer who is depositing his cheque from the atm now what has happened here in that  this entire transaction is lost this entire serious of operation is lost after time t equal to 4 because suppose this person had 50,000 rupees in his account it will be now 52,500 rupees rather than 55,500 rupees 3000 plus 2500 rupees  refer slide time  11  12  so 3000 rupees is just gone  it is just lost now  as you can see this is not unrealistic situation especially we just saw today  when we have facilities like net banking or atm or booking train tickets over the web or using telephone calls or using sms from mobiles and so on this is not an unrealistic situation because concurrency is concurrent activities are happening at the same time i mean a concurrent activity is happening all the while and if you are not careful  such kind of activities can result in an inconsistent database the entire transaction of depositing 3,000 rupees is lost in this example so  if you are not careful what is the clear you have to take what is that you need to remember when we are dealing with situations like this the thing that you need to remember here is the first two activities let us say the first activity of wired transfer between account 2 5 6 5 1 6 5 is a completely different or is a conceptually separate or distinct activity from the second activity of depositing 3000 rupees  refer slide time  12  47  so the first activity is a conceptually or logically separate activity or functionally than the second activity such kind of logical units of works are called transactions and transaction activities in a transaction  the actual database activities transaction should be schedule in such a way such that these kinds of anonyms do not occur let us try to formulate these things in a little bit in much more detail now what is a transaction ? a transaction is a logical unit of program execution  refer slide time  13  35  as we saw the entire activity of withdrawing money from a ’ s account and depositing that money in to the b ’ s account constitutes one logical unit of operation and it is a combination of database updates which have been performed together they can not be independent of one another the withdrawal from a ’ s account is not independent of deposit into b ’ s account and vice versa there are several different transactions depending on where and how it is being used let us have a brief look at the different definitions of transactions which makes it clear what are the different fests to handling transactions or recovering from transactions one firstly we can define a transaction as a logical unit of work that is meaningful in the user ’ s environment as we can see here  the wire transfer that is withdrawal of money from a ’ s account and depositing in to b ’ s account is a meaningful semantic activity as part of the users environment because that constitutes a semantic process in the users environment  a wire transfer similarly depositing 3,000 rupees ; in order to deposit 3,000 rupees  there were two database operations network done that is reading previous balance and updating the balance so both these activities of reading and updation is one semantic activity that  it constitutes one meaningful activity namely that of depositing certain amount of money in to an account now one can define even transaction as a logical unit of work with respect to concurrency control and recovery not necessarily semantic activity in terms of the user ’ s environment many cases users depending on what granularity you are looking at users may not be concerned with water considered transaction set at the database level however  we might have to club or we might have to combine certain database activities in to transactions in order to maintain consistency in the phase of concurrency control and recovery process transactions are also called atomic unit of work instead of calling it as logical unit of work  much more stringent definition is to say that it is an atomic unit of work with respect to concurrency control and recovery atomic unit of work is a more stringent requirement than saying logical unit of work an atomic unit of work basically means that it can not be subdivided in to smaller works either all of the activities of a transaction are performed or none of them are performed you can not perform half a transaction and leave it at that or you can not perform 90 percent transaction and leave it either you have to perform the entire set of activities of a transaction or nothing at all or another definition that it is generally used that is transaction is an atomic unit of work that will apply to consistent database returns another consistent database  refer slide time  16  58  that is an atomic unit of work that is meaningful  that could be meaningful that is the atomic unit of work with respect to concurrency control recovery however  not all atomic units of work that can be managed for concurrency and recovery could be transactions because some of them could take it take the database to inconsistent state so the transactions sometimes defined as in those only units of work make transactions to a valid state of database now what are the properties that a transaction should satisfy that  we have seen we have motivated the need for transaction in several different from angles we first saw an example application an example oltp application where transaction processing incorrect transaction lead to anonymous and also we saw different definitions that look at the notion of transactions from different levels now how can we consolidate them together and synthesis what are the basic properties the transaction should hold ? so the basic properties that a transaction should hold are called as the acid properties of a transaction acid stands for atomicity  consistency  isolation and durability that is shown in the slide here so  what is atomicity in the acid property ? atomicity we just saw in the previous slide that a transaction should be viewed as an indivisible unit of work  refer slide time  18  46  that means either all activities in a transaction should be performed or none of them we can not perform half a transaction and leave it consistency  consistency of a transaction basically means that of the database is consistent transaction should be consistent after a transaction the transaction the atomic unit of work should not lead the database in to an inconsistent state what is meant by an inconsistent state ? any state that violate the integrity constraint of the database  refer slide time  19  17  we saw how to specify integrity constraints and how they are enforced in a database system the third property of transactions is isolation we saw an example of transaction violating isolations in our banking example that we saw before isolation essentially means that even though activities in the database are happening concurrently that is the readings and updates and reads and writes operation whatever is happening on to the database level are all happening in a simultaneous fashion  refer slide time  19  53  the net effect in an oltp environment  the net effect should be as though the transactions have been executed in some serial order it does not really matter to what should be the serial order as long as we can establish equivalence between the way in which database activities are performed to a serial sequence of execution that is it should be as though that transaction ‘ a ’ was completed before transaction ‘ b ’ begin ‘ b ’ was completed before transaction ‘ c ’ begin and so on and the last property of the transaction is called durability  that is once the transaction finishes are rather we used the term commit here  that is once the transaction says now i have done all my work and you can commit whatever changes have made in to the database once it is committed  the changes are persistent  refer slide time  20  52  you can not rollback or you can not undo the changes that are being made by the transaction after it is being committed to the database that is commit is something like in order to understand the notion of commit  it is something like a physical activity for example  dispensing money from an atm is a commit operation once it is committed  once money is dispensed  you can not rollback you can not expect the user to say no no no  we did something wrong we have to put back money that i gave you because some other transaction is conflicting so once commit operations is performed  it is durable that is the transaction can not be rolled back and we have to do something else in order to undo the operations of transaction once you dispense the money from an atm you have to do something else we chased the person who withdrew the money and get back from him if required and so on so we can not undo the transactions within per view the database let us look at the examples that specify each of these acid properties of the transaction have a look at slide here let us say the transaction involves again a wire transfer from account a to account b that is  the transaction should be like this account a dot balance minus equal to amount whatever amount has to be transferred and account b dot balance plus equal to amount let us suppose that the balance that is amount number of rupees has been debited from a ’ s account and for some reason the databases crashes let say there is disk crash or a network failure whatever or operating system crash or whatever now once the system is brought up again that is once there is a recovery process  this transaction has not completed therefore in order to make it atomic  we have to roll back we have to roll back the changes that you have made since the beginning of the transaction that is  we have to put back this amount back into a ’ s account and then restart the transaction once again otherwise  it would not be an atomic operation  refer slide time  23  17  this amount to if it did not perform the recovery operation this would amount to performing half a transaction and we saw that performing half a transaction and we saw that half a transaction is not an atomic transaction what about consistency ? have a look at the example here again this is again the wire transfer example from account ‘ a ’ to account ‘ b ’ that is the same series of operations have to be performed that is a dot balance minus equal to amount and b dot equal to b dot balance equal to amount now let us say that the query that has to be performed these two things have been that is the query planned has been performed and these two operations are given to two different threads in the operating system  refer slide time  24  05  and it so happens that the thread performing changes on b ’ s balance is scheduled first before that of ‘ a ’  let us say first ‘ b ’ s balance amount is crediting let us say ‘ a ’ is sending 2500 rupees to ‘ b ’  so we know the amount 2500 so these balances added by a value of 2500 and however when trying to recover  remove 2500 rupees from a ’ s account we see that a has zero balance in his amount he can not make this payment so this transaction fails we can not make this transaction so in order to keep this transaction consistent  we have to deduct whatever credit we made in to the account of b in order to bring back the consistency in the database systems  refer slide time  25  06  note that here there is no crash or anything of that sort here there is the normative failure the normative or failure with respect to nor the failure which violated the integrity constraint we can think of an integrity constraint that says odd raff are not allowed that is the balance amount in users account may never be negative so when we try to do this operation that is when you try to debit 2500 rupees from ‘ a ’ s account  we found that the balance is becoming negative and it violate the integrity constraint which in turn cause the transaction to roll back that is in order to maintain the consistencies in the database systems the third property is that of isolation and isolation like you said before deals with concurrency that is what how do we handle concurrent operations being performed from two or more transactions simultaneously so again consider the case of wired transfer another case of wired transferred that is account a ’ s is transferring some account to b at the same time  account the person holding account ‘ a ’ is also withdrawing some money that is the person holding account a has given a check at some time which is getting process now and if the same time  the account holder is withdrawing some money let us say the transaction t1 is reading account a ’ s balance debiting the amount and crediting the amount account b let us say that a ’ s balance will become zero after debiting this amount let us say it 2500 rupees and let us say the same amount is also being withdrawn by being as by account holder for withdrawal the net effect of running these two transactions should not be the case that both of them read the database or if the balance  there is 2500 rupees and then go ahead independently debiting them debiting the account because that would be in correct because we would have debited more than 2500 rupees from these two transactions where it is not could not be reflected so the net effect should be t 1 precedes t 2 that is t 2 begins only operation after t 2 completed is in effect that that should be the case which case t 2 will fail or tone begins operations after t 2 completed ? it does not matter which is the serializable schedule which is the serialize schedule that we want is it t 2 after tone after t 2.so in either case none of the 2 transaction will fail that is either the withdrawal will fail or the wired transfer will fail  refer slide time  28  02  and durability like we said is the commit operation that is one thing committed  then it is not change we gave an example of let us say money dispense information from atm  refer slide time  28  15  once the commit operation is performed  it is safe to dispense money from the atm and we can not roll back the transaction once the commit is performed what are the different states in which a transaction is in and this is important to know when we are trying to recover from a failure of a transaction ? now transaction is set to be in several different states depending on what has happened since it begin it is said to be in active set which is the initial set when the transaction is executed when the last statement has finish execution and it is ready to commit  the transaction is said to be partially committed when the transaction discovers that it no longer proceeds with normal execution because something else has happened  some crash or some violation of an integrity constraint or some violation of an isolation requirement and so on when it discover something like that then it is said to be in the fail state and once roll once the transaction is rolled back  it is said to be aborted and if the transaction successfully completes that its operation that is an atm successfully dispenses money it is said to be committed and either committed or roll back or aborted state is called terminated state  refer slide time  28  30  so this slide schematically depicts the different states in a transaction and also shows from which state you can go to which other state that is from the active state  you can go to either a partially commit state or a failed state and also you can reach fail state from a partially committed state that is after performing a few operations and from a partially committed state  if everything okay then you can go to committed state or if the things are not okay you can go in to the fail state turns take in to an aborted state  refer slide time  30  14  let us have a simple look at how these acid properties can be maintained or what it takes to maintain these acid properties and we are going to look at simple example called simple technique called a shadow copy shadow copy is extremely simple extremely inefficient and it is not used in practice several more sophisticated techniques for handling are maintaining acid properties or taken up in much more detail when we take up the topic of transaction processing itself here this is just to illustrate the concept of what it takes to perform  to maintain certain properties of a transaction  refer slide time  31  06  shadow copy transaction assumes the database to be a single file and assumes that ther is only one transaction that is active at any time note that it can only provide acd that is atomicity consistency and durability and not isolation  refer slide time  31  22  so shadow copy is simply like this suppose you have database in a file and you have to perform your transaction now before performing your transaction  make a copy of the database that is copy in to entire file the file and make your changes on the copy of the databases now if your changes succeed  that is it does not violate any integrity constraint and it is consistent and it is safe to commit and so on  then simply you delete the original database and then you keep the new updated copy of the database incase you have to abort your transaction  then you just delete the copy that you have created and let the original database be in its place as simple as that  that is you make the entire database to in to shadow  refer slide time  32  22  copy the entire database into another file and make changes on it and if it is safe to commit the changes  then delete the original file or if it is unsafe delete the new file and let the original file be as it is of course  how it is interactional and inefficient but of course later does it satisfy these acid properties of a database let us look at atomicity if i see that i can not do all operations in a transaction such that atomicity needs to be met then i just delete the new transaction that is i just delete the new file it is all are nothing  refer slide time  33  10  when all operations are committed  all operations are performed in the new file  will i delete the old file so  therefore it is all or nothing no operations have been performed consistency  if any consistency  if any integrity constraint is violated in the new database is deleted so assign that old database is consistent  we are still left in a consistent state isolation obviously not supported because when two or more transactions are copying making different shadow copies can not we can not support isolation here and durability at any point in time  once the transaction commits  it just ensures that either the old file or the new file remains that is once the transaction terminates it is either commit or abort if it commits  then the new file remains if it aborts  old file remains so it is durable what are changes made are persistent in the database let us have look at concept of serializability which is again very important  when it comes to recovering from failed transactions like we mentioned before  in the previous shadow copy example  isolation was not supported and in order to support isolation we should ensure the notion serializability in our transaction processing environment this serializability simply says that  if i set of activities from two or more concurrent transaction taking place  they should they should schedule in such a fashion as though transaction were executed in some serial order so have a look at the slide here slide shows two transactions here t1 and t 2 and transaction tone is a wire transfer that is taking fifty rupees from a ’ s account putting in to b ’ s account transaction t 2 is also a wired transfer that is taking ten percent of whatever amount is their in a ’ s account and crediting in to the b ’ s account  refer slide time  33  25  now suppose i have to perform all activities of t1and then start with all the activities of t 2 obviously it is a serialize schedule such a schedule is called a serial schedule that is performing all activities of one of transaction before starting first activity of the second transaction so this equivalent to performing t1 followed by t2  refer slide time  35  27  if i perform all activities of t 2 and then start with the first activity of t1 and then perform all activities of t 1  this is also a serial schedule  this is also correct schedule and this is equivalent to t 2 followed by t1 however serial schedules do not have does not necessarily mean that all activities pertaining to given to transaction completed before the first activity of the next transaction t1 is taken up for example  this one this slide shows how activities from t1 and t 2 are interleaved the color activities here belong to t 2 that is read a t equal to eight times “ point one ” a equal to a minus t write a and then the transaction t2 has not yet completed but transaction t1 is already begin read ‘ a ’ equal to a minus 50 and so on and then transaction t2 continues here and transaction t1 also continues here however  if you notice even this schedule is a serialize schedule or it is a serial schedule this schedule is equivalent to performing t 2 followed by t1 why is this so ? have a look here have a look at how the activities of t 2 and t1 are interleaved ? all activities performing are regarding updation of data element a is completed of the transaction t2 or from the transaction t2 before first operation involved involving data element a is even performed from transaction t1 same thing with respect to b and we can actually see that  we can rewrite it that is we can take these elements of b back here and put this back here without changing the semantics that is without changing the overall semantics of this serialized schedule that is once this schedule finishes  it is equivalent to as though t2 was executed first followed by t1 that is although activities of t2 before the first activities of t1 ever started  refer slide time  38  15  this brings to some definitions of how we can enforce serializability over a set of database activities we define the term conflict serilazability by first defining the term called conflict between database activities then we say that then we say that particular schedule is conflict serial able if there is no conflicts or with respect to or when it is being transformed to a serialize schedule a schedule in which  all transactions all activities of one transaction is performed before all activities of the second transaction now consider 2 activities i and g belong to two different transactions is t1and t2 now i and j can be swapped in their execution order if they refer to different data element because it does not matter one is referring to element a other is referring to b it does not matter we can perform them in any order and i and j can be swapped in their execution order even if they refer to same data element  however all that they are doing is reading all the contents of the data element even both are reading the same data element does not matter who is reading first and who is reading second as long as nothing else in between them that is and they are said to conflict that  i and j can not be swapped in their execution order if atleast one of them is a write operation if at least i is trying to write and j is trying to read we can not perform i should be read before j  we can not have j read the database before i writes it and so on  refer slide time  40  04  the same thing is true when both are write operations given a schedule ‘ s ’  a schedule is something like what we saw in this slide here  it is a schedule now suppose we are given a schedule like this that is the activities of t2 will perform will like this and activities of t1 are performed and activities of t2 continues and so on now given a schedule  if we in order to determine whether it is safe or not  whether it is serialazable or not  we can identify this  if we can swap or if we make one or more swapping of activities database schedule activities and bring them to a serialize schedule were all activities of one transaction are performed before all activities of the second transactions without encountering any conflicts as a way defined in the previous slide  refer slide time  40  12  then this kind of schedule is said to be a conflict equivalent schedule and it is also said to be a conflict serilizable schedule that is  it can be serialized or it can be equivalent to serialized schedule where the equivalent criteria is conflict equivalence that is conflict serializable  refer slide time 41.29  there is an alternate weaker notion of serializability called view serializability conflict serializablity is quite strong and in many cases we do not need the stringent property of conflict serializability the view serializability simply says the following suppose for each data item q  suppose there are set of transactions that are happening in a dbms system now for each data element q  suppose it was transaction ‘ ti ’ which reads the initial value of q in a serialized schedule that is in a serialized schedule ‘ s ’ that is in any other schedule which is view serializable  refer slide time  42  20  it should also be the case that  the same transaction is the first transaction to be reading this data element s or this data element q and similarly in the given schedule  for each data item q if t j precedes or t j writes to q before ti that is before ti reads  then the same dependency should be maintained in any other schedule that is anybody writing to a data element before somebody else is reading it  this kind of dependency should be maintained in whatever if the schedule has to be view serializable similarly  the last operation that is whoever performing the final write in a serializable schedule should be the same transaction who performs the final write in whichever schedule is view serializable  refer slide time  42  38  let us take an example  there are 3 transactions shown in this figure the first transaction t1 reads data element queue and writes it back to disk after performing some operations which is not relevant here it is only the reads and writes which we have concerned about it and t 2 just writes some something in to queue and t 3 alone it is writes something into q now the following schedule is a view equivalent schedule or a view serializable schedule note that it is not a pure serialize schedule the activities of t1 and t2 are being interleaved here all activities of t1 are not completed before activities are performed however  if you see who is reading the first  who is the first transaction to read the data element q suppose we take a serializable schedule that is t1is followed byt 2 that is t 2 followed by t3 if in that serialize schedule  the first transaction to be reading data element q is t1  that is the same thing in the schedule as well now is there any read before write dependency ? for example  t1 is reading before t2 is writing or rather t1 is reading before t3 is writing is that dependency maintained here ? that is read before writes write before writes either of those dependencies are maintained here and who is the last transaction writing to q that is t3 that is same thing here that is t3 is the last transaction that is writing to q therefore this is the view equivalent schedule that is it corresponds to t1 happening before t2 happening before t3  because the first data element to read was t1read q was read one last element write in to q was t3 so when the t 3 finishes  then there is no difference between saying it was performed as t1t 2 t 3 or it performed in this fashion however note that  this schedule is not conflict serializable if we try to swap updates here  that is if he try to swap the activities here in order to get serialize schedule  then we encounter a conflict that is take a look at the second and third activities here now in order to bring in a serialize schedule  we have to swap the second activities with the third activity so that t1comes here and t 2 comes here however both of them are writes and we saw that when both of them are write operations on the same data element and belong to two different transactions  then you can not swap them  they are in conflict therefore this schedule is not conflict serializable however it is view serializable that is  as far as database view is concerned  it remains the same whenever we look at before t1 or after t3 it is being the same  refer slide time  46  34  so every conflict serializable schedule is also view serializable however  the converse is not true which was the example we saw in the previous slide usually  this thing happens that is usually  we find view serializable schedule that are not conflict serializable whenever what are called as blind write a blind write is something that we saw in the previous slide here that is transaction t 2 t 3 shown in slide contains no read operation they just write in to the database  without any read operation so such it is a hall mark of blind writes which bring in schedules that are view serializable  but not conflict serializable  refer slide time  47  21  let us look at the last concept of what are called as recoverable schedule and in order to understand what are the requirements of database recovery ? consider the set of following set of transactions as shown in the slide here there are two transactions here t8 and t9 t8 reads a data element ‘ a ’ does some modifications and writes set and then goes about reading about some other read data element and so on after it writes here transaction t9 reads the data element a and possibly let us say displace it and it does not perform any writes so  as you can see this is the conflict equivalent schedule that is i can swap read a with read b which will give me serialize schedule that is t8 followed by t9 and swapping by read b and read a it is it is still conflict equivalent so  therefore performing read a write a read a of t9 and read b of t8 is conflict equivalent  refer slide time 48  27  however  suppose let us say t9 that is read a and displayed that is display the latest value of this stock price or whatever suppose this t9 commits and displays the value of the value of k but t8 is not committed  that is t8 is not completed still and it sees that it can not commit because some problem somewhere and it has to rollback now if it rolls back then t9 also has to be roll backed because it read value of a  after it has been written by t8 however we can not rollback t9 because it is already committed and committed is not the durability condition here  that is we have already made some commitment in some sense that is we have displayed the new value of the stock or whatever so in such a situation  it is impossible to recover from the failure of t8 because we can not the rolling back of t8 will also require rolling back of t9 it is impossible so this is in an example of a non-recoverable schedule  refer slide time  49  35  so serializability or conflict serializability alone is not enough we need to also look at recoverability of a particular schedule of transaction events if we have to be recover from a database crash so database system requires a recoverable schedule and finally let us have look at the problem of cascading roll back which is also quite important when it comes to recovery even if a schedule is a recoverable  to recover from a failure of a transaction in some times  there is need to roll back several transactions the previous example was also an example of cascading roll back that is supposes transaction t9 not committed and transaction t8 roll back  then t9 also has to roll back so in order to make it recoverable we have to defer the commit of t9 until after t8 has committed so that will make recoverable however  it still contains the problem of cascading roll back so this example also shows cascading rollback situation where there are three transactions t t1 and t 2 and has read a value of a and written it and whatever value is return by t is being read by t1 and t2 now t1 and t2 can not commit that is can not display the new value of a until t has committed otherwise  it will become non recoverable now even if they do not commit and suppose t has to roll back  it has a cascading effect in t1 and t 2 that is in fact t 2 is dependent on t1 and t1 is dependent on t so a roll back of t will cause a rollback of t1 which in turn will cause a roll back of t2 so such cascading roll backs will lead to an undoing of large amount of work from several different transactions in case of any database crash or system failure cascading rollback is an undesirable thing to happen and leads to an undoing of lot of work  refer slide time  52  05  so when we are looking at schedules of operations that are performed by oltp environments  they have not to be only serializable they have to be recoverable cascade list as far as possible we should try to avoid cascading roll backs  refer slide time  52  27  so that brings us to end of the introduction session on database recovery where we have said the ground for all the issues that make up that are concerned or that we have concerned ourselves with whenever we are dealing with database recovery  refer slide time  52  46  for example  we saw the notion of the transaction that is when we are recovering from the database crash  we have to ensure that we does not leave any transaction in a half or partially committed state it should either be fully committed or no operation should have performed that is all are nothing atomicity transitions have to be obtained whenever we are recovering from database crash or the system crash and in order of that in happen we have recoverable schedules and it is not sufficient for schedules to be serializable and also whether it is conflict or view serializability  it is not sufficient for schedules to be just serializable they should also be recoverable schedules and as far as possible they should be cascade list schedule that is crash or rollback of one transaction should not automatically mean that several other transactions or several other work that has been partially completed has to be rollback it is not even partially completed  even though they are completed just waiting for the commits which what we saw in the operations in the previous slide that is even though transactions t1 and t2 are completed they have read and written a value of a they are just waiting for the original transaction to commit and because of some problem the original transaction may crash and because of that even all the operations that have been completed have to be rollback without any reason by themselves so this brings to the end of this session database management system dr.s.srinath department of computer science & engineering indian institute of technology  madras lecture no # 28 recovery mechanisms ii hello and welcome in the previous session we started looking into recovery mechanisms in databases  especially we looked into the back ground of transactions and the idea of a transaction and how recovery should maintain consistency in terms of different transactions that is it should not leave a transaction in a unatomic form that is when a database is consistent or either all transaction is performed completely or they have not been performed and it should not violate an integrity constraints and it should be serializable and so on today ’ s session we are going to look at some mechanisms for recovery specifically we are looking into what are called as log based recovery as the name suggests log based recovery means that recovery mechanisms for the database are performed using transaction logs that is whenever transactions happen certain elements of the transactions are logged into log files and using these log files  we can try to recover the database into a consistent state in case of any kinds of failures  refer slide time  2  37  so let us briefly summarize whatever we have learnt about the transactional requirement of databases before we look into recovery mechanisms or log based recovery mechanisms firstly  why recovery or in what situations do we talk about recovery mechanisms ? recovery is pertinent in the phase of failures and given database system can be subject to different kinds of failures there could be system failures  the power could just go off database  there could be media crashes that is the disc crash or something of that sort there could be communication failures that is network has failed and transaction which was partially submitted or especially if you are having distributed databases  transactions which was started on other machines failed communication between the two machines failed and so on and there are of course transaction failures  that is transaction could fail for a variety of reasons including the above kinds of failures that we are talking about the transaction could fail because they violated integrity constraints  transaction could fail because they could not  there is no serializable schedule for the set of activities from these transactions or they could fail because whatever schedule that ’ s being currently performed has led to a dead locker something of that sort so in many of these failures we need a recovery mechanisms in the last case that is transaction failure usually its automatic that is the system is still functional the dbms  the database everything is still functional so it is just a matter of re submitting the appropriate transactions after waiting for a while and hoping that it succeeds this time rather than fail or if the transaction has violated an integrity constraints  it involves something like raising an exception or intimating the application program saying your transaction is wrong or i cant perform your transaction because for example your account doesn ’ t have enough money to withdraw so much amount that you have asked something like that so leaving aside the last point here  in most of the other cases the dbms or the databases has crashed and it has to be booted up  it has to be brought up again and once it is brought up again there is no guarantee that what ever data that ’ s there in the database is consistent and it will be un safe to just start the database and have it running from where ever it is left off because any amount of data that was there in the volatile memory in the ram would have been lost and we don ’ t know how we can set these things right  refer slide time  5  30  so what are the properties of transactions that we have to assure when we are providing recovery mechanisms ? the property of transactions as you know is called the acid property of transaction that is atomicity  consistency  isolation and durability properties so let us briefly summarize what is meant by the acid properties and what do they require atomicity means either all updates that are performed by the transactions are performed that is either all updates that are required by the transactions are performed or none of them are performed we can ’ t have a transaction that has performed half of the updates that were made for it that is we can not have a transaction that has debited my account in a wire transfer transaction and has not credited the other account and the money is lost so it should be either all or none kind of operation similarly consistency requirements is that when a transaction has to finish or if a transaction has to successfully complete  it should not violate any integrity constraints of the database that is given a consistent database a valid transaction should leave the database in another consistent state it need not be the same state  it could be another state but it should be a consistent state if it violates any kind of integrity constraints on the way then you have to roll back  you can ’ t complete the transactions and you cant leave it there as well because then we were violating the atomicity requirement of the transaction isolation constraint that is the i in the acid property states that whenever there are multiple transactions that are happening on in a dbms  the net effect of all the transactional updates should be such that or should be equivalent to a schedule in which all updates of one transactions are performed before the first update of the next transaction is taken up that is it should be as though the transactions have run in some serial order  it need not actually be run in serial order that ’ s what we saw in the previous session activities can be interleaved as long as this interleaving is safe that is we saw notion of what is meant by safe that is the notion of conflict serializability that is we should be able to conflict or view serializability which we saw that is we should be …  refer slide time  08  13  into a serialize schedule without encountering any kinds of conflicts and the last property is that of durability that is once a transaction commits it can not be rolled back the changes that are made after a transaction commits are durable  it is persistent and not only that the changes are made inside the database that is on to disk  the changes could also entile performing some kind of physical operation like we gave the example yesterday of dispensing money from an atm that is if once a transaction for withdrawing money succeeds and it commits then the atm dispenses the required amount of money that was asked by the customer for withdrawal now once the money is dispensed you can not roll back the transaction the problem …  refer slide time  09  13  if it is found that there was some error and the money shouldn ’ t have been dispensed  you have to look at solutions that go beyond the database systems  you cant ask the database to just roll back this transaction and leave it at that  refer slide time  10  31  what are the different states in which transaction lies ? the first state is the active state that is whenever a transaction becomes active and it is executing  it is said to be in an active state once a transaction has performed all its updates and it is ready to commit that is it has finished its executions and it is ready to commit then it is called a partially committed state and once a transaction discovers that it can not commit  mainly for example it has violated an integrity constraints or its schedule can not be serialized and so on then it is said to be in a failed state and once a transaction has rolled back from its failed state that is it has undone whatever it had done already then it is said to be in an aborted state and if the transaction has committed successfully then it is in a committed state and either aborted or committed state is called a terminated state now let us look at these states of a transactions in terms of recovery that is what kinds of states require recovery of a transaction now if a transaction is terminated  its either aborted or committed then we wouldn ’ t have lost atomicity as a part of the transaction that is if the database crashes after a transaction has committed or aborted  it should be such that atleast the dbms should be designed such that these transactions should not be executed again that is we should not submit this transaction again to the dbms for example if the user has requested for withdrawal of say 1000 rupees from an atm and the transaction has aborted or rather the transaction has committed and 1000 rupees has been dispensed from the atm and right after commit  the system crashes then the dbms should be designed such that whether or not this data is there on or has been updated on the dbms for a variety of reasons which will see shortly whether or not this data is updated on the dbms this transactions should not be run again that is the user should not be given another set of 1000 rupees after the system comes back because the transaction has already run and the operation has already been performed whatever operation has been asked for on the other hand if a failure occurs during let us say active or partially committed state then we may have to  in some cases undo whatever has been done by the transactions  whatever operations has been done by the transactions and then probably resubmit the transactions that is re run the transaction once again from the start  refer slide time  13  54  this slide shows the state diagram or state transition diagram for the different sets of a transactions that is we start from the active state and the active state can go to either partially committed state or a fail state depending on whether all operation in the transactions have been executed successfully or whether there have been some problems and even in a partially committed state  there is a chance of failure if the transaction finds out that it can not commit for example if the transaction is dependent on some other transaction to commit in case of and the other transaction rolls back and this transaction is subjected to a cascading roll back so in that case even if all the operations have been …  refer slide time  13  27  there is still a chance of failure even from the partially committed state and if nothing goes wrong then we can go ahead and go to the committed state or once we reached a failed state then the transaction goes in to the aborted state that is it rolls back whatever been done  so it undoes whatever operations has been done and it goes back into an aborted state  refer slide time  16  14  let us look at the concept of serializability again where we talked about what is meant by serial schedule and how do we know whether a serial schedule is valid or not in order to determine whether a serial schedule of transactional activities that are interleaved between one another  in order to know whether this is valid or not we have introduced the notion of conflict serializability as  if you remember conflict serializability is a mechanism which defines the notion of conflicting database activities what are conflicting activities ? consider two activities i and j belonging to two different transactions t1 and t2 now i and j can be executed in any order that is i before j or j before i doesn ’ t matter if i and j refer to different data elements because they don ’ t affect one another and i and j could still be executed in any order  if they refer to the same data elements as long as both of them are just read operations so both of them are just reading the given data elements  so it doesn ’ t really matter whether i read the data data elements first or j reads the data elements first on the other hand if either i or j or both contain a right operation on the same data element and both of them of course refer to the same data element then they are said to be conflicting so we can not swap the execution of i and j and expect that the swapping is an equivalent schedule to the earlier schedule so if i have a schedule of operations  database update operations i can verify whether the schedule is safe or not by seeing whether it is conflict equivalent that is can i keep rearranging the operations of this schedule so that i eventually end up in a serialize schedule that is all activities of one transaction are performed before the activities of the second transaction so i end up in a serial schedule without encountering any conflicts during the way  refer slide time  16  25  so if a schedule can be transformed in such a way or if it is conflict equivalent to a serialize schedule then it is said to be a conflict serialize schedule  refer slide time  19  07  we also saw the notion of non recoverable schedules that is in what cases  we can never recover from a crash and so on so the slide here shows an example of a non-recoverable schedule there is a transaction t8 which is reading a data element a and writing something back on to a that is it has performed some computation say as long as when we are concerned about recovery  we are not really concerned about what kind of operation it makes as long as there is some right operation we assume that some update has taken place  may be there was no update that is it has just read the data elements and return it back for whatever reason but at the level of recovery mechanism …  refer slide time  17  15  some change or there is some modification that has happened so this transaction t8 has read a data element a and it has returned it back onto the data base now after it has returned it back onto the data base  another transaction t9 read that element of a and of course did something and then committed so this commit operation could probably involves some kind of physical operation like say displaying the data element may be it is the new stock prize or whatever  it just displayed the data element however this transaction t8 try to do something more and crashed now because transaction t8 has crashed  it has to be rolled back that is whatever operations that are performed by t8 has to be rolled back but we can not roll it back because transaction t9 which has already read the changed data element has already committed and whatever data that is returned is already out in the open and it ’ s been displayed so such a schedule is a non-recoverable schedule and how do we prevent non recoverable schedules from occurring ? simple way of preventing non recoverable schedule is from this example is to note that transaction t9 can not commit until transaction t8 has committed that is if a transaction is reading a data element that is written by some other transaction then it can not commit until the previous transaction has committed so in that way transaction t9 can not display the value of data element a and until and unless transaction t8 has successfully completed  refer slide time  20  36  but even then that is even if we stipulate that transaction t9 can not commit until transaction t8 has committed …  refer slide time  19  18  cascading roll backs that is shown in this slide here that is there are 3 transactions t  t1 and t2 and this transaction t has read data element a and modified it and returned it back into the database now this modified data element now is read by t1 and then t1 in turn modified it again and wrote it back in to the database and t2 then read this second modified database  second modified element that is the data that was modified by t1 and then probably try to display it or something and of course  because we have ensured that none of them can commit until t can commit  they are just ready and waiting for performing whatever their commit operation tells them to do that is whether you display it or dispense money or whatever now transaction t instead of committing crashes for whatever reason now because transaction t has crashed  transactions t1 and t2 even though they have completed successfully have no option but to roll back so this is the problem of cascading roll back so even if the schedule is recoverable  sometimes suppose transaction t is a long running transaction  it runs for several minutes or probably sometimes even several hours transaction t1 and t2 are short transactions and there are several such transactions are waiting on transaction t2  for transaction t to commit now for whatever reason if the t transaction crashes whether it is a transaction failure or a system failure or a media crash or whatever  we have to roll back and all these transactions that are been waiting on transaction t  refer slide time  21  17  so let us look at how to tackle all these problems in a systematic fashion  so the concept of recovery recovery from transaction failure is a process of restoring the database to the … and where do we restore it ? we restore it to the most consistent state that was there before the failure and there are two kinds of recovery strategies that we are going to be seeing today which are called the deferred update strategies and immediate update strategies deferred update essentially means that the database or updations to the data base are deferred until after sometime which will formalize later on and immediate update techniques update the database as and when transactions are running the database is physically changed as and when transactions are running  refer slide time  22  20  so this slide defines both of these techniques again the database is not modified until a transaction reaches it commit point in deferred update techniques and in immediate update techniques database is updated as and when transaction progresses however transaction fails in immediate update techniques  you have to undo this operation that is you have to change this therefore they have to be logged whatever update were made to the database have to be logged before the updates are made obviously you can ’ t log the updates after making the updates because what happens if the system crashes  once you have made an update and before writing the log on the other hand if you have written a log and the system crashes before making the update  it is still not a so much of a problem as we will see later  refer slide time  23  15  before we go on to recovery techniques  there are two things that we have to define and we have to be careful about how these impacts recovery techniques the first issue is that of cache management you might have studied in an operating systems course that most operating system use what is called as buffer caches and what are buffer caches ? buffer caches are some buffered areas in memory that act as a cache for data that are present on disk that is whenever a disk block or a disk sector is accessed or is sort by the operating system instead of just reading one disk block  usually operating systems perform what is called as read ahead that it reads a set of blocks into main memory and all writes that are performed on to disk sectors are initially performed just on the main memory and not onto the disk and its only once in while these cache or this buffer cache is flushed onto disk this is done in the interest of performance that is why for example in most operating systems  you need to perform some kind of disk sanity checks if the operating systems crashed midway because not all blocks that have been modified would have actually been written on to disk now this buffer cache is an operating system construct that is it is in the control of the operating system and application programs or user level programs that run in an operating systems usually don ’ t have control for this buffer cache but for database recovery we need to have control over this cache because we can ’ t assume that the operating systems has written something onto disk after we have said write because operating systems in turn has its own mechanisms that might defer writings on to disk and which may impact a recovery process therefore typically in many database management systems  what is done is a part of the buffer cache that is maintained by the operating systems is given to the dbms that is the dbms is given control of this buffer cache so that it can  that is the dbms can control …  refer slide time  25  48  into the buffer cache and so on and such kinds of cache pages which are given to the dbms are called dbms caches and of course there is also the problem of what happens if the system crashes when the cache has being written on to disk that is i have written something onto cache and now i am flushing this buffer cache but during this buffer cache flush  the system crashed and what do we do the cache is partially written and the data could be inconsistent and so on for that a technique called shadow paging is used which we are going to study in the next session on database recovery technique the shadow paging technique that is used for data base recovery can also be used for maintaining or recovering cache contents in the case of crashes  refer slide time  26  50  the second issue that we are going to be concerned about is the concept of …  refer slide time  26  55  log now we have mentioned in passing that in order to aid the process of recovery from databases we maintain logs that is whatever updates are made to the database are all logged in some log file now this log file keeps on growing because every update that is made to the database  the information about this update is kept in this log files now this log files keeps on growing and we don ’ t know when a crash would occur and how much of history information we need and so on so how do we prevent this log from growing forever and probably becoming bigger than the database itself so the answer to this is the notion of a check point a check point in a log records a state where all transactions that have been committed until this point in time have been physically modified on the disk in the database that is the database has been updated and everything is fine for all the committed transactions until a check point so all committed transactions that have been  information about whom have been stored in the log until a check point can be thrown away that is at a check point we can throw away data about all the committed transactions that have happened before the check point and at what frequencies do we have to check point the log ? that is check pointing rather that is the process of introducing a check point in a log as you might have imagined is a separate process by itself that is we have to decide at what intervals at or at what frequencies are we going to introduce check points into the log and what should be done when a check point is being introduced and check pointing actually involves suspension of all activities of the database  all transaction activities of the database temporarily until we know for sure that all the that this check pointing criteria is made that is all the committed transactions have been successfully updated onto the disk  refer slide time  29  10  the algorithm for check pointing is quite simple but quite costly in terms of operations that is in order to take a check point  we first suspend all transactional activities temporarily because we don ’ t want more data to be written when we are handling this check pointing and then we force write that is we flush all main memory buffers that have been modified to disk that is whatever has been  whatever data that had to be updated onto the database we force write all of these committed …  refer slide time  29  47  and then we write a check point note in the log file and and also of course force write this log on to disk the fact that we have written  the fact that we have encountered a check point should also be recorded persistently onto disk because once we have thrown away information about other transactions  we can ’ t lose the fact that we have performed a check pointing operation and then we resume transaction activities  refer slide time  30  20  there is also a notion of fussy check points where that are more  slightly more efficient than the usual check pointing techniques note that check pointing requires suspension of all transaction activities and if this is done too frequently then it impacts database performance itself now let us see where is the biggest overhead during a check point and seek can we do something about making this check pointing faster the main overhead in check pointing and i am sure you would have imagined that is the flushing of all the buffer cache onto the disk that is each buffer cache contains set of disk blocks and so all of these disk blocks have to be physically flushed on to disk and this is what is going to take the most time now in fussy check pointing what happens is that transaction activities resume after writing the check point entry into the log  even though flushing has not been completed that is even the check pointing  check point entry could be in the cache and all the flushing actives activities are still going on but transaction activities  the transaction activities resume however the previous checkpoint is not released that is the older log entries are not deleted until after the new check point entry has been flushed onto the disk so it ’ s a background operation where until we are sure that the new check point operation has been written on to disk  this can be written onto disk only after all the buffer cache buffer has been flushed on to disk so until we are sure this has been done  the previous set of log entries are not deleted  refer slide time  32  11  so let us look at the first kind of recovery technique which we called as the deferred updates recovery so what is the notion of a deferred update recovery ? as the name suggests deferred updates means that the updates to the database are deferred until transaction commits that is until transaction has reached a ready to commit state the overall strategy for a deferred updates recovery is simply this thing that is a transaction can not change the database so even if a transaction has run half or 90 % or 95 % or whatever  it has not made any changes on to the data base as far as until it has committed so until it reaches the commit point  the database is not updated and a transaction does not reach its commit point until all its update operations are logged and the log is force written on to disk that is the transaction does not say am not ready to commit until all its operations that have been done have been logged and this log is available on the disk therefore even if the transaction crashes now and even if the database is not updated  we have the log entries which says that these are the changes that were made and the transaction is now ready to commit and it can commit  refer slide time  33  42  so using this let us see how we can perform recovery we first look at recovery in a single user environment that is it is a sequential database engine where transaction …  refer slide time  33  52  transaction is performed one after the other this is a simplistic case but it helps in understanding how the deferred update technique works so deferred update techniques uses two lists of transactions that is one is the list of all committed transactions  when does it use the two lists of transactions that is after  it is after the disk that is after the system has been brought up following a crash and the system is asked to recover to a consistent state now once the system is asked to recover to a consistent state  the recovery process starts by using two lists of transactions one is the list of all committed transactions since the last check point and the list of all active …  refer slide time  34  42   since it ’ s a single user operation there would be utmost one such transaction which would have failed in an active state and a logs that are maintained for these transactions are maintained in the following form which is shown here that is the first element here says that this is a write item that is something has been written on to disk note that as far as recovery is concerned  we are interested only in these write items  logs can be used for a variety of purposes something like to understand the behaviour of transaction  to profile the performance of the data base and so on and so forth but as far as recovery is concerned  we are just interested in what changes have been made to the database therefore we are interested in only these write items so it says this is a write item belonging to transaction t involving data element x and this is the new value that was written on to the x or that has to be written on to the database for element x  refer slide time  35  49  now once these two lists are maintained that is the list of all committed transactions since the previous check points and the list of all active transactions we first start by re doing all the committed transactions  that is we take the set of all committed transaction since the previous check points and then go about looking at the logs and see what values they had written on to what data elements and we start writing those values once again we don ’ t care what is the semantics of these values or whether these values were read or anything because these transactions were already committed  the cash has been dispensed already so we don ’ t have to do any physical operations  we don ’ t have to even tell the application program that we are doing these things because these were already committed and we know what are the values that has to go into the database  we just write those values we just start from the previous check points and start  data element x has to have a value of 10  data elements y has to have a value of abc or whatever and we just start writing those values back in to the database and then for all the active transactions which crashed midway …  refer slide time  37  12   during execution we just have to restart all those active transactions because none of the active transactions are physically modified the database that is what the deferred update technique all about that is transactions don ’ t modify the database until they are ready to commit and they won ’ t be ready to commit until they have made all the log entries on to disk after they have made all the log entries on to disk and said that they have been committed then they have been treated as a committed transaction and the first …  refer slide time  37  48  and all the redo operations from the transaction is performed if for any case the transaction that is the ready to commit tag does not go in to the log or whatever has been written by the transaction is not flushed on to the log and the system crashes then it is treated as an active transaction and it is just started once more and it runs once again and makes changes on to the logs and if everything goes well that is the log is flushed on to disk then it is ready to commit and the data base is updated  refer slide time  38  28  what about updates if the database is actually a multi user environment and there are several concurrent transactions that are running at the same time in a single user environment  we don ’ t have to worry about concurrency control that is how transactions are serialized that is they run in a serialized fashion by default but in a multi user environment there is the problem of serialization of transactions now serialization of transactions is not the problem of the recovery part of the database  it ’ s usually handled by the concurrency manager that is whatever concurrency control techniques are used for managing concurrency now we assume that for deferred update techniques  we require that concurrency control uses what is called as a strict two phase locking you might have heard of the notion of locking in several contexts in operating systems and probably even in systems design and so on where locking essentially means that if a transaction is performing some some updates on a database or some data elements  it obtains a lock that is it locks the data elements so that nobody else can read or write to the data element or can access the data element of course there are two kinds of locks that is read locks and write locks so read locks can be shared but write locks or exclusive locks can not be shared that is once a transaction has obtained a lock on a data element that it is going to modify  no other transaction can even read the element until the lock is released by the transaction so in strict two phase locking all the locks that are held by a database or by a transaction are not released until the transaction reaches its commit point now what does it mean that it reaches its commit point ? that is all the updates it has made are logged that is are written on to logs and the log is flushed on to disk now the transaction is sure that somewhere whatever updates it has made is persistently stored that is the updates it has made is safe somewhere  its not just lost once if the system crashes only then it will release its locks so only after it releases its locks can other transactions read the data element  read the corresponding data element  refer slide time  41  08  so assuming that we have strict two pl that is strict two phase locking  the recovery process can follow the technique shown here firstly make two lists of transaction  once after a database has crashed and it has restarted and the recovery algorithm is begun and it is starting to recover  make two lists of transactions that is first is the set of all transactions that have committed since the previous check point and the list of all active transactions now for all the set of transactions that have committed  we have to redo the operations this is the same thing we have done in the single user environment however here we have to ensure  we have to explicitly state that redo of the operations are performed in the same order as they appear in the log we can not try to optimize this redo operations by introducing some concurrency there because they may violate some kind of serializability conditions if they are performed in some other order …  refer slide time  42  24  between two or more updates such that they can not be swapped in their ordering and once the redo operations are performed that is once all the committed transactions have been persistently written on to disks  we then restart all the active transactions and before that we release all the locks that have been held by this active transactions and again the ground is free so that whoever wants the locks can now hold those locks so we release all those locks and resubmit all the active transactions once again  refer slide time  43  01  let us take an example of different transactions and see what happens in a multi user environment so this slide show a set of transactions and two different events  one is a check pointing event that happens here and one is the system crash that happens here now there are several transactions in a database here transaction t1 has already committed before the check pointing event happened therefore after the check pointing data about t1 are thrown out  we don ’ t even need data about t1 anymore however t3 and t4 have started before the check pointing operation but they have not completed  therefore we can not throw away these data about t3 and t4 even if check pointing is performed and transaction t2 has started only after the previous check point but has committed its operation before the system crash while t4 and t5 have not yet committed  t4 is really a long transaction that is taking place and they have not committed when the system has crashed so what happens in the update operation here that is during the deferred update operation ? t1 is not concerned at all  it doesn ’ t figure in to the picture at all because there is no data about t1 so at the time of the updates that is happening here …  refer slide time  44  31  committed but the data is still there because it ’ s occurring after the check point and because it has committed transaction t2 will undergo redo operation that is all its updates are logged into log file and using the log file  the database is updated transaction t3 has also committed before the system crash  it has started before the check point therefore its data will not be thrown away and because it has committed before the system crash this is also re done that is redo operation will be performed on transaction t3 and transaction t4 is still active at the time of system crash as this transaction t5 therefore both t4 and t5 have to be resubmitted to the dbms that is after releasing all their locks that they have held  they have to resubmitted back to the dbms  refer slide time  45  27  so that ’ s what this slide says that is data about t1 can be safely removed after check point  it doesn ’ t even figure during system crash that is it doesn ’ t even figure during updates recovery rather and data about t3 and t4 should not be deleted at check point because they are not committed and transaction t1 is unaffected during the recovery process t2 and t3 are subjected to redo operations and t4 and t5 were aborted and resubmitted back into the dbms  refer slide time  46  00  there is some efficiency issues about redo operations small thing we can notice  if a data element x has been written  refer slide time  46  15  it is enough if we just write the last update on to the database because anyways if we write a previous update  it is going to be over written by the next updates hence for redo operations we can start from the end of the log and start making updates moving backwards in the log and we should not write a data element on to the data base if it has already been written once during redo because we have already written the latest value during redo  refer slide time  46  45  what are some of the properties of deferred updates ? there is no undo that is required if you have noticed we have only talked about redo  there is no undo operations that are required why ? because the database is not modified at all  it ’ s only the transaction logs that are modified and since all locks are released in strict two pl  all locks are released only after commit a transaction can read a data element that is being modified that is no transaction can read a data element that is being modified by another transaction therefore there is no possibility of cascading roll backs because one transaction has already read a data element that is been modified by some other transaction and waiting for it to commit so there is no such possibility  so there is no possibility of cascading roll backs however potentially there is a large amount of disk operations during commit because enormous amounts of updates in especially large transactions that have been written on to logs have to be written back on to the disk  refer slide time  47  53  the next technique that we are going to look at is what is called the immediate update techniques in immediate update techniques  the database is updated as and when transactions execute however for the sake of recovery  database updates are performed after the updates are recorded in the log and the log is written on to disk that is only after we know that is there is still some kind of deferred updates happening here  that is updates are deferred only after we know that the log is written on to disk and before which we modify the database as and when we know that a particular log entry has been written on to disk  all those corresponding entries can be modified and however in the phase of a crash  we have to undo all un finished transactions since the last check point we need an undo operation here which is not required in the deferred updates and we still need the redo operations for redoing all the activities of the committed transactions since the last check point  refer slide time  49  03  so let us first look at the undo operations that is what should be done for all the un finished transactions now in order to perform undo  we need an extra element in the transaction logs this is shown here that is a transaction log contains this write element  write item element and and transaction t that is this element belongs to transaction t involving data element x and it says that the old value of x was this and the new value was this therefore when we are undoing it  we have to replace x by its old value and because we dint have an undo  we dint have to store old value in deferred updates and of course undo operations have to be performed in the reverse order obviously because the oldest values have to remain on the database and these log entries  the way these log entries are created is such that undo and redo operations are what are called as idempotent operations what is an idempotent operation ?  refer slide time  50  11  idempotent operation is something where it does not matter how many times you perform the operations for example some problems during undo and undo couldn ’ t finish  we can just restart this undo process from the beginning once again and then run it again and it doesn ’ t matter because it ’ s just re writing the old values  its not performing any computation it ’ s not saying that value of x was changed by 5 % so reduce it by 5 % or something like that so it ’ s not performing any computation  it ’ s just re writing back on to disk  refer slide time  50  44  so how do you perform recovery in single user environment using immediate updates ? again like deferred updates we use two lists of transactions  list of all committed transaction since the last check point and list of all active transaction since the last check point and first we start by undoing the activities of the set of all active transaction using the undo policy that we just saw in the previous slide and then we perform the redo operation of all the transaction that have been committed since the last check point and then we submit all the active transaction back to the dbms so that can execute once again  refer slide time  51  32  and how do we perform recovery in multi user environments using immediate update techniques it ’ s the same  it ’ s quite similar to that of the deferred update techniques where we use strict two phase locking so that none of the locks are released until the transaction is committed therefore there is no possibility of a cascading roll backs and as before use two list of transaction that is the list of all committed transactions and the list of all active transactions since the last check point then undo the writes of all active transactions using the undo policy and redo the write operations of all the committed transactions using the redo policy and then just release all the locks that have been held by the active transactions and give the transactions back to the dbms that is resubmit the transactions  refer slide time  52  28  so what are the properties of immediate update recovery ? as you can see database updates can happen as and when logs are written on to disk that is the operating rather the dbms should only keep track of when the buffer cache logs are written on to disks so as and when the buffer cache logs are written on to disks  the appropriate database update can also start happening so because of this database updates  the load on database updates is uniformly distributed they are not burstive as in deferred update transaction  deferred update techniques where all updates happen at the commit point but note that any physical activities that is being performed by the application program like say dispensing money or launching a missile or whatever  so any physical activities that is to be performed by the upper application program can be performed only after the database is updated that is only after we know that …  refer slide time  53  37  and the database is updated why ? this is because even if transaction is committed  the fact that it has been committed might still be in the buffer cache  it may not be written on to disk and the system could crash and once the system comes back again  it is treated as an un committed transaction and it is run once again …  refer slide time  54  05  even if in memory the dbms knows that it has been committed  it can not or it should not tell the application program saying everything is okay  everything is still not okay it will be okay only when the the disk is updated that is once they are flushed on to disk  so commits can be performed only either the database or the logs are updated that is force writing logs on to disk whenever a commit happens  refer slide time  54  34  so that brings us to the end of this session on transaction recovery using log based recovery techniques here we saw several different issues ; we started with several different issues concerning recovery in database systems we looked at the different kinds of failures that can happen and what it means to recover from a system crash or some kind of a failure and there are two issues that affect recovery processes one is cache management that is for the sake of efficiency disk blocks are usually cached into ram in the buffer cache and because we are talking about recovery  this can impact recovery process because we are not really sure whether something that has been written on to disk has actually been written on to disk so some part of the buffer cache is usually controlled by the dbms and which is called the dbms cache and then we looked at the concept of check pointing which allows us to throw away  safely throw away historical information that is stored in logs and then we looked at two kinds of log based recovery techniques deferred updates and immediate updates both of which use what are called as idempotent  undo and redo operations there is no undo operation in deferred updates but there are undo and redo operations in immediate updates and then we also categorized this log based recovery into two different kinds  single user environments and multi user environments and in multi user environments we have a requirement that we have to use strict two phase locking in order to prevent cascading roll backs in case of a crash recovery so that brings us to the end of this session database management system dr.s.srinath department of computer science & engineering indian institute of technology  madras lecture no # 29 recovery mechanisms iii hello and welcome in the previous session we have been talking about recovery techniques in database management systems how do we recover a database especially when the database was running and the system was subjected to some kind of a failure and we saw that there are different varieties of recovery techniques each having their own advantages and disadvantages let us continue with this topic further today and bring it to a logical conclusion by looking at all the other issues that entails database recovery  refer slide time  02  02  let us briefly summarize what we have studied until now whenever you are thinking …  refer slide time  2  00  given computer system whether it is databases or otherwise is subjected to frequent or frequent or non-frequent kind of failures  different kinds of failures and we could classify failures into different types like say system failures or media crashes or communication failures or transaction failures system failures and transaction failures are by far the most frequent or the most common kinds of failures what is a system failure ? system failure is something like suppose you are running your dbms on a machine and let ’ s say the power goes off and your machine crashes the characteristics of such a system failure is that all data that are there in the volatile memory like ram are lost however data that are present on persistent storage like disk are still retained so whatever has made it to the disk can still be retained  however whatever was still in the ram is lost similarly there are failures like transaction failures which are again quite common transactions failures entail failures where for a variety of reasons given transaction is not able to complete its operation this reasons could be something like too many processes running in this system or not enough privileges for running this transaction or the transaction trying to do illegal …  refer slide time  03  34  transaction and so on and so forth and these are again quite frequent kind of failures and here again the characteristic is that whatever the transaction was doing and whatever data that the transaction had in the main memory is lost  however the data that are present on persistent storage like disk are still retained then there are communication failures where especially if your database is distributed and you had to communicate between two or more different geographically distributed databases then you might encounter communication failures which need to abort a transaction or leave a transaction midway and there are somewhat infrequent failures called media crashes like a disk crash  like a disk developing bad sectors or some kind of a damage  physical damage to the disk where not only data that are there in the physical memories  in the primary memory is lost but the data that are present on the disk are also lost that is you …  refer slide time  04  47    refer slide time  4  52  now what is the main property that we have to consider or that we have to maintain when we are talking about recovery techniques as we all know transactions in databases follow this well-known property of acid which stands for atomicity  consistency  isolation and durability so atomicity essentially says that either all updates that are to be performed by a transaction are performed or none of them are performed this is obvious because suppose you are transferring money between two accounts  let us say you have a debit in one account and a credit in another account either both of them have to be performed or none of them have to be performed so either the transaction should run fully or it should not run at all and so on it ’ s not sufficient if money is debited but not credited or vice versa similarly  consistency a transaction when it completes can not leave the database in an inconsistent state we have seen some integrity constraints and notions of triggers where suppose data database consistency is violated  automatically triggers are enforced which will cause the transactions to roll in many commercial databases so if the database starts from a consistent state  at the end of a transaction it should remain in a consistent state and of course the third property is of isolation where when multiple transactions are executed simultaneously  for efficiency reasons they are executed in a concurrent fashion that is the updates made by each of the transaction are performed concurrently however the net effect that is not all possible concurrency is okay  essentially the kind of concurrency that is permitted is that the net effect of these multiple transaction should be such that it should be as though the transactions have been executed in some kind of serial fashion and the last property is about durability where once a transaction commits  once a transaction says that i have committed then whatever changes that it has made is persistent after commit it should never be case that let us say system crashes after a transaction has committed  it should never be case that the transaction is now aborted or rolled back once the transaction is committed  it means that it has performed some kind of physical operation that is some  it has performed some kind of an operation that is beyond the purview of the database itself for example when in an atm transaction  let us say in an automatic teller machine transaction once a transaction commits it means that it is safe for the atm to dispense money …  refer slide time  07  49  after the atm has dispensed money we decide to actually abort the transaction and then say okay it has been not committed and so on so once changes that are made by transactions are committed they have to be durable that is it has to be persistent  refer slide time  8  07  and we also saw that transactions can be in different states like active or partially committed or failed  aborted  committed or terminated so an active state is the initial state when the transaction is executing  it ’ s performing some operation  it feds some data elements and made some changes and so on and partially completed is a state when it has performed all its operations and it is ready to commit  it ’ s not yet committed and once it discovers that it is not able to proceed further  whether it ’ s not able to commit whether it ’ s not able to calculate or perform its computation further we say that the transaction has failed once a transaction has failed it has to roll back any changes that it has made to the database has to be reverted  has to be undone and once a roll back is complete we say that the transaction has aborted and on the other hand if the transaction successfully commits  it ’ s a committed transaction and a terminated transaction which has something either aborted or committed  refer slide time  9  15  so this slide which is now familiar shows the different states of a transaction in a schematic fashion that is an active transaction can become either partially committed or failed transaction depending on what it has  whether it has been able to perform all its operations or it ’ s not able to perform all its operations and from a partially committed transaction  you can still go to a failed state when you see that it ’ s not possible to commit the transaction but once you see that it is possible to commit the transaction then you go into a committed state and once you come to a fail state  you perform a roll back operation where you go to the aborted state in the transaction  refer slide time  9  57  what is meant by transaction recovery once again ? a transaction recovery is a process by which we recover the database system which is performing several different transactions into the most recent consistent state that is at any given point in time  any …  refer slide time  10  21  forming several transactions simultaneously now during such a time when there are some kinds of system crashes let us say some kind of failures like say system crashes or transaction failures or media crashes or whatever  it basically leaves the system in an consistent state  half of some transactions have been performed some of them have been committed  some of them have just started and some of them were shown to be committed but may not have been committed and so on and so forth it ’ s essential that when we recover from this system crash  we should recover to a state that is obviously consistent and it has to be not just any consistent state  it has to be the most recent consistent state that was there before the crash and all active transactions around the time of the crash that is when the crash happened  there were let us say 10 different active transactions that were running  once we recover from the crash of all these active transactions there should be either in a committed state or an aborted state after recovery that is either all the operations that are performed by the transactions are persistently stored in the database or they are rolled back so that they can be invoked once again buy the dbms system  refer slide time  11  46  we also saw that there are certain pre-requisites for recovery to happen that is there are certain kinds of schedules by which transaction activities are performed which are not recoverable at all for example this slide shows two different transactions t8 and …  refer slide time  12  07  following operations t8 performs a right operation on an element called a so it reads an element a and make some changes and writes it back to the element now let us say t9 reads the new element of a which has been return back by t8 and commits that means let us say displace so let us say something like you have just updated the bank account in your bank or updated your stock prize or something like that and there is another transaction which has read the new stock prize or the new balance account and displayed it or printed a statement let us say  commit is a some kind of physical operation so let us say it printed a statement however the transaction t8 has not yet completed here and while it is still going on  it encounters a system crash and once we recover we say that t8 has to be rolled back or has to be aborted and restarted again but what has happened here is that this transaction t nine has already said that the new value of a is so and so and it has already printed the statement or displayed it or done something of that sort so such a kind of schedule is not a recoverable schedule that is we will not be able to recover from such a schedule  refer slide time  13  29  there is also another kind of problem with recovery namely that of cascading roll backs this slide shows such a example here that is even when a schedule is recoverable  sometimes whenever we roll back a transaction there might be several other transaction that are waiting on it that have to be rolled back and all of them have to roll back one after the other which causes a cascade of different roll backs so here there are three different such transactions t  t1 and t2 where t has read element a and return something back to here now whatever has been returned back by ta or by t is now read by t1 and its return back and whatever has been return back by t1 is now read by t2 but transaction t has not yet committed here and it ’ s still going on and then it crashes now after we recover from the crash  suppose we decide that transaction t has to be aborted  it means that transactions t1 as well as transaction t2 have to be aborted because they have read some changes or they are basically depending upon the changes that are made by t itself so this causes the problem of cascading roll backs  refer slide time  14  49  so we saw how to prevent cascading roll backs and how to prevent non recoverable schedules in the previous session which i will not go into more detail here essentially in order to prevent non recoverable schedules  you need to ensure that no transaction reads a data element that is written by another transaction unless it has committed  only after committing a particular operation another transaction can read this transaction we also saw two kinds of log based recovery what is a log based recovery ? log based recovery uses what are called as write ahead logging or what are also abbreviated as wla write ahead logging addresses  wal rather write ahead logging now what is a write ahead logging ? write ahead logging essentially means that before performing any operation on the database  you perform a corresponding operation on to a log file and then perform the operation on to the database now whenever the database crashes  you have to use your log file in order to be able to recover from your system failures we saw essentially two kinds of techniques based on write ahead logging what are called as …  refer slide time  16  16   in deferred update techniques database is not modified until after the transaction reaches its commit point that is whatever changes that a transaction makes to the database is first just returned to the log and only when the transaction is ready to commit  will the log be return on to the or the changes be return onto the database itself so  until after the commit point transaction or the database is not modified and once the commit succeeds  you also enter a commit or committed entry into the log so essentially whenever the system crash  you just need to redo your transaction so that it reaches the point where it has been and there are other kinds of techniques called immediate update techniques where databases updated as and when the transaction progresses however as the name write ahead logging suggests that the updates are first written onto the log before they are written onto the database and immediate update techniques requires both undo and redo operation that is you need to undo all active transactions which have not committed and you have to redo all transactions which have committed actually  refer slide time  17  41  the general idea behind recovery using deferred updates is shown in these slides and such kinds of …  refer slide time  17  48  that is there is no undo operation as part of this database  as a part of this recovery technique however there is a redo operations because  why is there no undo operations ? because the database is not touched at all that is the database is not modified at all until the transaction is ready to …  refer slide time  18  12  multi user system in a multi user database system  you have to perform let ’ s say some kinds of two phase locking in order to prevent …  refer slide time  18  28  in order to enforce isolation constraints and all locks that is whenever you are holding such a lock on a database item all such locks are released only after the commit therefore no transaction can read any other  can read the values written by any other transaction before it has committed therefore there is no possibility of cascading roll backs and also there is no possibility of a non-recoverable schedule occurring as part of this recovery mechanism however the problem with deferred update techniques is that suppose a transaction performs let us say some 500 different operations now none of these 500 different operations are written on to the database until after the transaction is ready to commit therefore once a transaction is ready to commit  suddenly there is a huge spurt of activity  suddenly there is all 500 different operations have to be performed on to the database at one instance of time rather than being distributed over the entire life span of the transaction therefore there could be potentially heavy disk operations during commits which could bring down performance or which could cause the system to thrash or lead to some other such problems  refer slide time  19  53  with immediate update techniques that is where locks or where the database is updated as and when the transactions are written onto logs  we require both undo and redo operations so such kinds of recovery techniques are also called undo slash redo transaction or undo slash redo recovery techniques now what is the general idea behind immediate recovery techniques ? the general idea is that as and when the transaction is executing  the database is updated even before committed which is very important here that is the database is updated as and when the transaction is executing and the transaction may not commit at all  the transaction may become a fail transaction it may abort  it may have to roll back and so on however as and when the transaction is executing the database is getting updated what it means is suppose the transaction can not commit  suppose the transaction is not able to successfully perform its commit operation then we need to roll back the transaction now because we are changing the database as and when we are performing the transaction  we have to perform corresponding undo operation that is we have to undo whatever the changes that the transaction has made to the database so database updates are perform after the updates are recorded in the log and the logs are written onto disks and of course it may not necessarily be force written onto disk that is you just write it onto log and then write it to the database now once you know that the log has been written to disks  it ’ s safe for you to write to the database now suppose there is crash at any point in time and …  refer slide time  21  46  to do two different operations  one is you have to undo all unfinished transactions since the last check point and you have to redo all committed transactions since  refer slide time  21  55  as you know our different stages in transaction logs where we say that it is safe to throw away all other historical data that is there in the log because  the log is a monotonically increasing file where every operation keeps on adding to the log suppose we don ’ t truncate the log  it could well be the case that the log becomes far more bigger than the database itself and becomes far more difficult to manage than the database itself therefore it is important for us to be able to conclusively decide  when at time what parts of the logs can be safely deleted or can be safely thrown away so that is where the notion of check points also comes in to the picture  refer slide time  22  44  there is a third kind of recovery techniques which we have not seen in detail but i am including this here for the sake of completeness which are called undo based of transaction  undo variety of transaction recovery protocols  we saw that is undo slash no redo we saw that there was the deferred update technique was no redo slash undo and the immediate tech update techniques were undo slash redo and this is undo slash no redo the general idea behind undo based transactions is that it ’ s a recovery technique that is based on pure undo operation what this means is all committed transactions are flushed onto disks during check points that is at every check point in order to safely throw away certain transactions  we flush all committed transactions …  refer slide time  23  46  transactions are maintained that is the chain of all activities by an active transaction is maintained by what is called as backward chaining in the log that is whenever i enter an entry for a particular transaction in the log  i also maintain a pointer to the previous operation that it performed therefore the way that these log entries are maintained are what are called as compensating log records or also called as clrs where for every update operation  you perform you store what is called as a compensating operation for example if you let us say debit 5000 rupees from an account a  the compensating transaction or the compensating operation rather would be to credit 1000 rupees or 5000 rupees to this particular account so for every kind of update operation let us say you deleted a particular tuple  the compensating transaction would be to insert the same tuple back to the database  the compensating operation so you maintain the log of compensating operations and at every check point you see which are the transactions that are committed and flush them onto disk so that you don ’ t have to undo them once again and at any time the system crashes or we encounter a system crash  what we need to do is simply start from the state where the system has crashed and follow the backward chain of logs and perform all the compensating log records that is compensating operations that are performed so this is the pure undo based operation where there is no need to redo any transaction or there is no need to redo any transaction or there is no need to redo any particular transaction activity the redo is performed or the flushing of transactions data are performed only at check points and not after any system crashes  refer slide time  25  53  now let us look into the notion of check points itself in a little more detail as we said before  a check point is some point in a log where we say that it is safe to throw away everything before this check point that is all committed transactions until this check point have been are now durable that means they have been physically committed to the database therefore the records of all committed transaction need no longer be stored as part of this transaction log and it is safe to throw away all the data about committed transactions and the frequency at which we take a check point decides or some kind of a balancing factor between what is the amount of log that you are going to store versus what is the performance over head that you are going to incur because at every check …  refer slide time  26  55  you need to bring the database to what is called as the quiescent state that means you have to suspend all the activities all transactional …  refer slide time  27  05  transaction that have committed and ensure that they are physically modified in the database and …  refer slide time  27  13  restart the database system that is restart all the transaction activities therefore if check pointing is done at two frequent an interval  if check pointing is done too frequently then it impedes performance while if check pointing is done at too widen interval then it starts increasing the amount of log records that you have to store  refer slide time  27  45  so what is the general algorithm for taking a check point ? we have to first suspend all transaction activities temporarily that is we need to take the dbms to a quiescent state and then we have to force write or flush all main memory buffers that have been modified of committed transactions actually on to disk and then write a check point record into the log and force write the log onto disk as well and then now it is safe to resume transaction activities  refer slide time  28  18  but then as we saw that bringing a database system to a quiescent state is a costly operation just imagine a large database system like let us say the database system for railway reservation let us say for a given railway zone like say southern railway zone at any given point in time there are possibly hundreds or even thousands of transactions that are performing  so many requests coming from different places for reservations and cancellations or some other kinds of activities not just reservations and cancellations let ’ s say rescheduling and so on there are several different transactions that are running at any given point of time and even if we stop the database system let us say for a few seconds or even if or sometimes even a few minutes because the size of the database is so huge  it actually impedes or impairs the activities of a large number of requests that is there could be a large number of people waiting in different reservation centers waiting to reserve their tickets and bringing the dbms to a quiescent state will actually stall all of them and so on so there are other technique that are used which can try to obviate the need for bringing the database on to a quiescent state whenever a check point is being taken remember taking a checkpoint is quite important because otherwise the transaction log itself becomes too difficult to handle but …  refer slide time  30  00  it doesn ’ t impedes the performance too much that is where the notion of fuzzy check point comes into the picture that is fuzzy check points essentially does not require  normal check pointing requires suspension of all transaction activities but fuzzy check point does not require in bringing the database onto a quiescent state essentially what we do in fuzzy check pointing is that we write the check point entry in to the log and we resume operations but we keep watching the disk itself that is at the physical layer for example note that i mean note that when we write something onto disk  the operating system may not actually physically write it on to disk it would actually write it onto a buffer cache within the main memory  so it ’ s still volatile and it will write it onto disk at some later point in time so we wait until that point where the check point log actually comes onto the disk and then we start removing all the old entries that is whatever has to be safely removed are then removed  refer slide time  31  21  a second kind of check pointing which does not require the database to be brought to a quiescent state is also called a non-quiescent check pointing operation here unlike fuzzy check pointing  there is also no need to watch the physical disk sector or physical disk block to see whether the check point entry has actually appeared but the idea here in non-quiescent check pointing is that active and non-active transactions at the time of the check point are handled separately  refer slide time  31  56  let us look at how non quiescent check pointing actually performs there are essentially three different steps in this non quiescent check pointing at any time if i have to take a check point  what i do is i first determine which are all the active transactions at this point in time that is these are all the active uncommitted transactions that are currently running and then make an entry called start check point with even to tk which are all the active transactions running at this point now this entry will go into the buffer cache and so on and so forth and finally at some point in time it will reach the disk now wait until and we are not concerned when they are going to reach the disk and we are not going to watch the disk at all  we just wait until each of this transaction either commit or abort but we do not prevent other transaction from starting that is other activities could be resuming by themselves but we simply have to wait until any of these commit or abort when all of them are done that is they are finished or terminated  we write a end check point entry into the log and then force write the log onto disk that is we flush the log onto disk now what is the use of such a check pointing ? that is how can we recover from such a check pointing ?  refer slide time  33  18  recovery from such a non quiescent check pointing is also quite simple let us say at some point in time there is a crash and we recover from such a crash or we have started with the recovery process after the crash and we start to read the log backwards that is from the last entry onwards now suppose we meet a end check point entry first  what does that means that the previous check pointing operation has successfully completed that is we know all incomplete transactions that were there at the time of the check point have been completed therefore there is no need to do anything that is it is safe  we just have to restart whatever transactions were running after this time but suppose we reach a start check point without an end check point what does this mean ? this means that the crash has occurred during the check pointing operation itself that is these were the set of transactions that were running and all of them have not yet terminated  they have not yet committed or aborted and the crash has happened so therefore what is that we need to do ? we have to undo all these operations that is all the transactions  all the operations that have been performed by these transactions there again we can use some kind of backward like we saw in the undo only kind of recovery algorithm by which we undo all of these operations and until how far behind in the log we need to go ? we need to go just until the previous check point because we know that the previous check point would not have completed until all the active transactions at that point in time have not yet completed so therefore we never need to go beyond one check point that is beyond the previous checkpoint that is we can safely throw everything else beyond the previous check point and we just have to trace them through the log till the previous check point and undo all these operations so that ’ s another kind of check pointing operations that are used in addition to or in complement to the normal quiescent based check pointing where you need to suspend all activities of the database system  refer slide time  35  41  there is another …  refer slide time  35  42  which is called as shadow paging  it was …  refer slide time  35  50  part of sys …  refer slide time  35  53  in this but where the technique of recovery is not based on logging but on maintaining different copies of database pages that is the database is managed in terms of different pages or logical entities of disk of operation which pages could be just let ’ s say disk blocks or disk sectors or set of sectors and so on and we basically make different copies of these pages in order to maintain the log so there is no need for write ahead logging and all updated data are kept in a different location from the original data  they are not kept on the main or on the same data set and recovery basically entails that we discard the new page whatever is the updated page and revert back to the shadow original page that is the shadow of the original page which was still there that is we don ’ t delete the original page until it is safe to do so which we will see shortly shadow paging is quite efficient in terms  in the sense that there is no need to perform multiple write operations in log based recovery you need to first do a write ahead logging and then write on to the database system so there is a need to perform multiple write operations for every given multiple update operations for every update operation that is involved but in shadow paging there is no need to do that however shadow paging suffers from fragmentation of pages over an extended period of time and usually we need to defragment the set of database pages by an offline operation after a number of recovery techniques have been executed  refer slide time  37  54  so how does the shadow paging technique work ? here is a small illustration we usually keep a page table that is each page has a particular address where each page can correspond to a set of tuples for a given table for example and each page is given a particular address and there two different point of for each page address what is called as the current page pointer and the shadow page pointer initially we just have the current page pointer here and which is pointing to a particular page now let us say some tuple in this page has to be modified so there is a modification request for this page now we see that this page has no shadow page so therefore what we do is we create a copy of this page and point the current page pointer to the copy the original will now become the shadow that is the shadow of the original page that existed and we are perform the modifications  whatever update operations that we need to do will be done on this page here now suppose somewhere down the line when we are using this page there is a system crash now when is a system crash all that we need to do is to discard this page and go back to the shadow original page and then change the current page pointer to the shadow pointer here and set the shadow pointer to null so we just discard whatever was there and we have gone back to a consistent state now you might be wondering at what time are we going to start  how do we know that this is consistent that is how do we know at what time do we start creating a shadow page  refer slide time  39  41  essentially shadow pages are updated with current pages at every check point that is whenever there is any check point or check pointing operation not just crash recovery operation  we perform this following operation  we perform the following set of activities that is we replace the current page with a shadow page  make the shadow page pointer to null and the current page pointer to the previous shadow page pointer so at every check point we are flushing the shadow page or we are updating the shadow page with the new current page and every first modification after a check point requires creation of a new current page and as this kind of evident here  the check pointing operation that has to be used here should be quiescent mode check pointing operation that is we need to suspend a database activities for performing such a update operation that is replacing the shadow page with the new consistent current page  refer slide time  40  46  there are also other issues during that we have to content with for crash recovery what happens that is the first  the main issue is what happens if there is crash during the recovery process itself that is there is a system crash and we are trying to recover the database system and then there is again a crash now in order to be able to handle such issues that is so that crashes during recovery do not affect the database  we need to be or we need to ensure that the undo and redo operations are idempotent remember even in the previous session we had used this word idempotent idempotent essentially means that no matter how many times you perform the operation  the net effect would be as though you have performed the operation only once and it doesn ’ t matter how many times you perform this operation that is let us say copying an element a to element b is idempotent no matter  how many times you copy it  it is equivalent to saying that we have copied it only once but there is a problem with idempotent operations as well that is if you have to make undo and redo operations idempotent then we have to ensure that multiple transactions or multiple active transactions do not access the same element at the same time that is one transaction has to wait until previous transaction has committed before it is safe for it to access a data element and that actually impedes performance in terms of performance and ofcourse in shadow paging technique there is other problem which occurs when there is a crash during recovery  especially when we are manipulating pointers that is suppose you have discarded the new current page and we are changing the pointers to the to the old shadow page and then there is a system crash it may so happen that all pointers are lost and the page has just become garbage that is there is no way to access the page and basically place it in the context of the larger database  refer slide time  43  14  until now we have essentially talked about system crashes or transaction failures where the fundamental criteria was that we can lose or we may lose data in the main memory but not in the persistent storage what happens if there are media failures that is there is a disk crash  let us say or development of bad sectors in this case we can not even rely upon the data to be safe in the persistent storage as well that is even the data are there in the persistent storage are gone fortunately media failures are much more …  refer slide time  43  53  disk crashes happen far more in frequent fashion than let us say power offer or operating system hanging and so on  refer slide time  44  06  and …  refer slide time  44  08  crashes what are called as archiving or taking backups and i am sure you know the concept of taking backups it ’ s simply taking a copy of a entire set of database on to another offline media which is stored physically in a different location and however when you are talking about large database systems  again when we talk about database system always imagine a large database system because most of the problems of database management systems come from their size and not from anything else so imagine a large database system like a railway reservation or a bank or whatever now the problem is …  refer slide time  44  55  itself takes a huge amount of time suppose i have one giga byte of data in my database which is quite common  in fact it could sometimes we even have more than one giga byte of data  we have several giga bytes of data in any operational database and possibly even tera bytes of data now copying them to a separate let us say tape media or optical storage takes huge amount of time in the order of hours or possibly sometimes even days to copy the entire set of data elements onto backup storage and therefore there is a need to perform back up operations in an online fashion that is as and when database activities are going on  we can not obviously suspend all railway reservations let us say for one day we can not say tomorrow there is you can not reserve any train tickets and so on it has to be on everyday 24 hours a day or 7 days a week  so everyday or every instance of time the reservation activities are going on you can book train tickets over the net and you can book them over the counter so on and so forth  refer slide time  46  14  so there is a need to perform online backup operations so let us briefly see what is the basic idea behind online operations and which can help us appreciate the need and the complexities involved in a online backup operations before we do that we need to take up certain definitions  we distinguish between different levels of archivals so what to call as level zero archivals and level i archival level zero archival or what is called as a full dump is an entire archival or a copy of the entire database onto the archival storage and a level …  refer slide time  47  04  incremental dump is essentially copying of only the changes that the database has under gone before the  rather after the last archival that is only changes since the last i minus 1 level i minus 1 are archived  refer slide time  47  25  now the basic idea behind online archival is shown in this figure between main memory and disk there is this check pointing operation as you might have noticed similarity between archival and check pointing check pointing requires that the dbms system to be brought in to a quiescent state and archival requires a suspension of all database activities in order to copy them onto the archive so check pointing gets data from memory onto disk and whenever we need to recover  we recover based on logs and the archival process gets data from disk onto archive and because it is unrealistic to say that we suspend all dbms activities during archival we usually allow that the dbms to keep updating its database operations as and when the archival process is going on however in addition to the database  we also store the log from the previous check point until the previous check point in addition to the database onto the archive therefore whenever they need to recover  we need to recover from archive plus log to bring back the database onto the last consistent state before a media crash and the database is being modified as and when the dump operation is running  refer slide time  48  47  so a simple algorithm for online archival is given here we first begin by writing a begin dump record to the log that is starting from the logging operation itself we begin the archival process we then perform a check point usually a quiescent check point but not necessarily and then we flush the log that is we have begin the dump and began the check point that is we have flushed all committed transactions onto the database and we have a log of all the active transactions here then perform the dump  full or incremental or whatever kind of dump from the disk on to the archival data now once all data are transferred onto archive including the check point that is there in the log then enter end dump in the log  refer slide time  49  47  so what is the advantage ? the advantage is shown in this slide that is during recovery that is then we have to restore the database now the recovery from media crashes is called restore rather than recovery that is we are restoring the database from the archive so when we restore the database from the archive  we find the most recent full dump and reconstruct the archive based on all the incremental dumps and then we write it back on to the database and then we take the database log that is the surviving log entry that was also archived in the archives  in the archival storage and there we have a check point which shows what all has been done to the database and whatever was there after the check point we start redoing those database that is whatever kind of logging method is used that is suppose we have an undo logs then we perform the corresponding undo operations and suppose there were it was a redo log we perform the corresponding redo operations depending upon lets say it ’ s a deferred updates or an immediate kind of logging operations so that kinds of summarize …  refer slide time  51  05  there are number of other topics in database recovery which we have not touched upon  especially perhaps most significantly the kind of recovery techniques that are used in many of the commercial dbms systems most of the commercial dbms systems today use a combination of undo and redo logging operation that is this is to get the right tradeoff between performance and recovery over rates because in a pure redo operations you have a performance overhead in the sense that there is a huge spurt of activity during commits for every transaction therefore they use a combination of these undo and redo operations and there is a kind of a famous series of protocols called aries  a r i e s which can possibly search on the internet which was proposed it ibm unverdant research centre which was kind of used quite  which is kind of very popular and used in several different commercial database systems  refer slide time  52  23  we have not covered the details of the aries protocols here for the reasons of brevity but most of that concepts used in aries depend upon or based upon several of the concepts that we have studied here like deferred updates or immediate updates and check pointing and undo and redo operations and idempotent operations and so on and so forth so let us briefly summarize what we studied in database recovery under the topic of database recovery we looked into the idea of transaction states and what are the different states that a particular transaction can exist and the acid criteria for transactions which determines how or which determines how our recovery protocol should run and we looked at different kinds of failures like say system crashes media crashes transaction failures and so on and for the most frequent kind of failures like say system crashes or transaction failures we saw three different kinds of recovery mechanisms that is no undo slash redo operations that is deferred update recovery and undo slash redo which is immediate update and there is undo slash no redo operations which are based on compensating log records or clr ’ s we also saw the notion of shadow paging where you don ’ t have the need for a log therefore it ’ s much more faster  the recovery mechanisms or the dbms itself is much more faster because there are no multiple write operation overheads that are involved during every update but which suffers from a possible fragmentation that is so many pages created at different places in the disk are possibly when suppose there is system crash during recovery itself  there is a possibility of encountering garbage system pages as part of the disk itself so shadow paging has its own advantages and disadvantages with respect to write ahead logging we also saw the notion of archiving or the idea of taking online backups for handling media failures where we take a incremental backup and we recover or we restore from backup based on not just the archives but based also on the log that …  refer slide time  54  57   that brings us to the end of the session database management system dr.s.srinath department of computer science and engineering indian institute of technology madras lecture – 30 introduction to data warehousing & olap hello and welcome in today ’ s session  we are going to be looking at a slightly different topic from the conventional idea of databases and such a change in topic occurs simply because the kind of users that the database is going to be is meant for is going to change until now  we have a kind we have had an implicit assumptions that the users of who are going to be using the database are in some sense if i can put it this way or in some sense clerical users in the sense that  they are most interested in adding and retrieving data elements into the database as efficiently as possible that means  the users who are using this database are involved in the operational aspects of a larger system for example  if you are thinking of  let us say railway reservation we are talking about how best we can design a database system for reserving a ticket therefore  what it means is whenever a new ticket entry is made it has to be efficiently entered into the database and suppose if there are any modifications for a given ticket entry  it has to be efficiently modified that means  it should be able to efficiently search the given entry and make modifications at just one place if possible and not many more places and keep that overall consistency of the database in time and that is what is meant by operational aspects of a database system or operational aspects of a system is a day to day operation somebody comes you gives a request for a reservation  you enter the request  you reserve a seat for him and give him the ticket and or cancel it something or ask for concessions or whatever and so on but  there are other kinds of users who use the database system as well and specifically we are talking about users who take strategic decisions in addition to or in contrast to the tactical decisions that are taken by the operational people that is the folk sitting in front of a let us say a ticket counter take very tactical decisions how efficiently can you perform your operations ? but then  there are a variety of strategic decisions which is the best way which is the best location for me to place my next railway reservation counter which part of the city has the most people traveling by trains or which part of the city has the most people traveling by first class ac or which part of which time of year is the best for me to offer concessions on second class sleeper  something like that so these kinds of strategic decisions are of a qualitatively different nature than the tactical decisions that are taken for operational issues of any system but if you watch closely it is the same … …  refer slide time  4  20  or fed and retrieved in operational situations is the same data that is required for making strategic decisions as well if you have to make strategic decisions about let us say which is the best location for me to open my next open the next reservation … … …  refer slide time  4  40  for that we need a lot of information about what is the  from what addresses are people coming and booking tickets were right if somebody comes from area ‘ a ’ and books tickets in area ‘ b ’ and assuming that area ‘ b ’ is the nearest reservation counter and the address information will in turn show me that because there are a lot of such people coming from here to there  it probably makes more sense for me to open a next counter here and so on so we will see in the next few sessions how the whole aspect of data base design changes when the usage scenario changes from an operational data usage to a strategic decision making usage so that brings us to the topic of data warehousing data warehousing as you might understand the term warehousing  a warehouse is where you keep your inventory stocks right that is where you have stocks from several different sources are going to several different sources and you are essentially talking about a large number of stocks that are maintained in the warehouse and a warehouse is typically of strategic importance if you take up  let us say some kind of civil engineering project  the location of your warehouse is of prime importance depending upon because there are which depends upon several factors like say what is the cost of transportation  logistics coordination and so on suppose you are having you are handling a large infrastructure project somewhere let ’ s say building a flyover something like that  having the location of your warehouse in a in a way that is easily accessible with possible cost constraints is probably one of the most crucial strategic decisions that can be taken in an analogous sense  a data warehouse is a warehouse of data elements that have been captured from different operational data sources so that this whole set of data elements becomes is of strategic importance you can take strategic decisions based on the data elements that you have gathered and which leads us to the next term in the slide here which is called the olap or online analytical processing that means we are looking for requests or queries that are of an analytical nature rather than an operational nature or what you called as a transactional nature we have talked about transactions quite a bit now we are looking into analytical queries where queries in turn help in strategic decision … …   refer slide time  07  44   refer slide time 7  49  … … … …  by itself is a vast subject and that several research papers that have been written in several commercial implementations that have been implemented and the huge amount of interest in data warehouses however  in this course we shall probably be touching just a small part of this vast ocean of data warehouses and the way we have partitioned this  our exploration into data warehouses are in three parts as shown in the slide here in the first part  we shall be looking into an important difference between olap that is online analytical processing versus the online transaction processing that traditional data bases are geared towards …  in data warehousing namely that of data cleaning and data integration the next two parts deal with the deal with the warehouse core itself where we are talking about the data models that that go inside the data warehouse and what are some kinds of the thumb rules which go towards data warehouse design we will also look at some kind of index structures for data warehouse based on the data models that we studied and see that  how they differ from  refer slide time  9  13  data bases like say b plus trees or b trees or hash based indexes or something like that  refer slide time 9  26  when we are talking about data  we can essentially divide data into two kinds of data essentially i have called them as operational data and the next slide calls it as historical data  that is operational data can also be considered as data that works within quotes that is data that is involved in the operation of a particular system for example  if you want to withdraw money from your account  you need data about your account  your account number  your pin number if you are using an atm your account balance and so on all these data elements are all operational data elements because they are crucial  they are necessary for performing the operation of withdrawal of money right and what kinds or what are the characteristics of operational data ? operational data are subjected to frequent updates and queries you could be withdrawing money almost every day or some kind of operations would be happening on your account almost on a daily basis so there should be  there would be some kind of queries or updates happening quite frequently to the database that is maintaining your account information therefore we saw in the sessions on normalizations and functional dependencies we saw that in order to make the process of updates and queries efficient  we need to normalize the set of data elements that is we need to normalize what is to prevent any kind of redundancies occurring in the data and also to prevent any kind of update anomalies suppose update records of in some place and there are redundant copies of the same record of some fields  then i will have to update all occurrences of this field so in order to minimize update anomalies we essentially normalize the set of data elements and a set of tables that are normalized are essentially fragmented  because given data elements let us say your account information may require several information something like your account number  name  address  branch location  balance  transaction history and type of account so on and so forth and because they are normalized  the set of all tables are fragmented in different phrases and the operational data is usually of local relevance which is kind of emphasized in the should kind of emphasized that means it is very unlikely to expect let us say that you have opened an account in chennai and you go to delhi and you want to access your account it is unlikely to expect the account information that you have opened in your bank in chennai to also be present in delhi it has to be queried in chennai and your request has to be routed through chennai and it has to be queried here and the result sent back to delhi so  operational data is usually of local relevance  wherever the data is it is relevant there in in the geographic location and the kind of queries over operational data is also what are called as point queries what is a point query ? a point query is something like asking something about asking query about some individual tuple for example  what is the balance in your account with account number so and so or what was the last set of ten transactions that your account had therefore  what is happening here is that  there is there is a particular point or a specific tuple is the key where your account number in this case where which is being used to access all relevant information for the operationalization  refer slide time  13  38  on the other hand  there are you can think of another kind of data called as historical data or also called as archival data that is data which are archived over a long period of time over a long period of different operational data sets it is a long set of operational data sets and it is these kinds of data which tell us something that is it is also called as data that tell us something about the overall trends of operationalization that is happening  that is suppose i have a data collected over from all railway reservation centers for the past ten years it can tell us about trends as to which are the peak times in which people travel what kind of people travel ? in what kind of or make what kinds of railway reservations and so on i mean people having what kind of salary range travel in what kind of classes ? sleeper classes ac or whatever and so on so it is that data that tells us something about the larger system which is using the data set and as you can see here  the kind of updates that  this data sets undergoes is quite infrequent in nature it is not like every day that you are going to get historical data you have to get historical data over a period of time therefore  you collect historical data let us say once in six months or once in a year or something of that order so it is a very infrequent updates but what is more important here is  that updates are not all that important that is they happen very infrequently so  we can take care of them whenever they happen  but it is the queries that are more important  that is analytical queries require huge amounts of aggregation what is the average age of the person traveling in second class sleeper ? if you want to calculate that  it needs huge amount of aggregation over a large data set based on all different second class sleeper tickets that have been sold and this is an integrated data set with a global relevance we are not it usually it does not make sense to look at trends for trends in a given operational data source it does not make sense to say what is the trend in this particular reservation center ? i mean it is it usually would be very small compared to the larger set of all different operational data sources and the performance issues in managing historical data occurs mainly in query times and not in update times because of course  updates are far infrequent or far more infrequent than queries and queries need to access a large amount of data and you have to perform a number of aggregate operations before being able to return the query and they have to return it in an online fashion  that is why it is called online analytical processing you have to return it in a return query results in an interactive response time that is the user should be sitting in front of a terminal and the interactive response time is usually of the order of maximum a few seconds and you can not expect the user to be waiting for a large period of time after giving the query  refer slide time 17  22  let us look at some examples of a transactional data and historical data a transactional data or operational data are those kinds of data limits that are handled by systems that are called as oltp systems or online transactional processing systems that is these systems which maintain transactions and handle different transactional activities in the system now what are some examples of operational queries  something like say what is the salary of mr.misra ? it is some it is a point query as you can see here that is  you find out the employee number of mr.misra and then get the salary field and that is it or who is the what is the address and phone number of the person in charge of the supplies department ? again it is a point query that is just a question of following different references find the supplies department and look up the set of managers given the suppliers department id look up who is the manager managers id and then given the managers id  find the address and phone number of the manager and so on so these are some kind of typical  analytical queries  typical transactional queries which we have seen quite often now that is we have seen how to specify these queries in sql  how to optimize these queries  how to create transactions around them and so and so queries shown in this slide  refer slide time  18  58  how is the employee attrition scene changing over the years across the company ? as you can see  this kind of query is qualitatively different it is qualitatively in a different caliber than the kinds of queries which we saw in the previous slide if you want to answer this query  let us say how the employee attrition scene is changing over the years and across the company it is not a question of accessing one particular record or one particular tuple you need to look at employee attrition information across the company and across the years and then look for trends saying it is increasing  it is decreasing  and so on that is you have to find out some aggregate employee attrition information across the company and then plot it against the years and see how it is changing or something like  is it financially viable to continue our manufacturing unit in taiwan it is a pretty vague query right what do we mean by financial viability ? what do we how do we calculate financial viability profits against cost and so on ? so  how do we know that we are incurring more profits than costs or we are gaining more profits than the cost that we are incurring it is not quite easy to find that out  that is you need to be able to aggregate a number of different information sources and then say profits are generally more than the cost and so on right so  these are the kinds of queries which are of analytical nature and typically for interest for strategic decision  making managers who perform strategic decision making  refer slide time  20  54  a data warehouse is an infrastructure to manage such kinds of historical data and it is designed to support olap queries involving very gratuitous use aggregations aggregated queries and so on and it is not just a queries  there are also a number of post retrieval processing also called as reporting which is just as complex or possibly more complex than the query retrieval itself  that is i have gathered huge amount of information about all possible profits and costs from all possible centers and so on how do i project this information ? how do i give out all these information to the decision maker which is again a quite involved in itself ?  refer slide time 21  44  so these slide shows the schematic diagram of the oral architecture of a data warehouse a data warehouse as we saw in the previous slide is meant for managing historical data and where do you get the historical data from or how do you first of all come out with historical data ? historical data you essentially obtain from all the operational data sources or the oltp units so you have several oltp units for your organization whenever you are thinking about an organization here  think of a large organization something like say the life insurance corporation of india or the indian railways or something like that where they have a number of different units each of them having their own databases each of them handling their own accounts  each of them handling their own reservations and transactions and so on and from each of these oltp units  we obtain huge number of operational data elements everyday right everyday there are number of people traveling in trains and from number of different and they would have booked their tickets from number of different sources now all of these operational data are then subjected to a data cleaning and integration process we shall see shortly what is meant by data cleaning and integration ? there are huge number of possible inconsistencies or possible sources of what we called as dirty data that can exist in the oltp sources and we will see what these sources of dirty data that can exist and how we can clean  what is also called as data scrubbing and so on before we are able to present it as historical data now once operational data is subjected to data cleaning and are integrated across all of these oltp sources  we present them as historical data into a data warehouse that is  a data warehouse would already exist and you just integrate this set of data into the data warehouse you just update the data warehouse with these set of data  refer slide time  24  04  there is also a notion of data marts which is probably of interest here at least look at the definition whenever we talk about data warehouses  we look at then as a collection of data marts that is a data mart is a historical data about one specific kind of one specific segment or one specific oltp segment suppose you are considering let us say again take the example of indian railways let us say  we have one segment called express train reservations that is one data mart suppose  we are interested in strategic decision making only about express trains or may be let us say rajdhani express or something like that all the super-fast trains so all of the historical data that we gathered about rajdhani express would go into that data mart called rajdhani express that is of all rajdhani expresses across the railways and over a period of time and so on so and the data warehouse is usually seen as the collection of this different data marts that feeds into the data warehouse and data warehouse data marts are also seen as small warehouses where you can in in some way you can support all activities that you support on a data warehouse also on a data mart and but these are within a given segment  refer slide time 25  45  now let us expand this system or this box and data cleaning and integration and see what happens inside that ? as we mentioned earlier  we obtain data from different oltp data sources  operational data sources that feed into the data warehouse now can we or what kind of  refer slide time  26  12  … … … … before we are able to integrate operational data sources or before we are able to populate the data warehouse from the operational data sources operational data sources as we have seen are mainly meant for the data that works that is as long as the data is sufficient for the operation to be performed it is good enough it is  refer slide time  26  49  … … … characteristic of operational data is that operational data is of local relevance it is usually relevant  that is data in an operational data source is relevant only locally however  when we are talking about only historical data  in a data warehouse there are two different things that you have to  refer slide time  27  11  … … … one is that because we are using different kinds of data sources  we should have a uniform standard of data representation and semantics across all of these different data sources and secondly we should remove any kind of duplicate information that are present in these data sources suppose you have opened an account let us say  let us again take the running example of the indian railways now suppose you have opened an account for booking tickets over the web now you have booked a few tickets over the web and then you have not booked tickets for quite significant period of time in which time you forgot your user name and password and you forgot that you have ever opened an account here and then you go back and open another account for buying tickets over the web now as you can see if these two accounts the behavior in these two accounts are considered different are considered to be from different users  then it will give us erroneous information or erroneous aggregate information for strategic decision making it is important that for historical data that these two accounts should be clubbed or we should recognize that these two accounts belong to the same user and then club them together whenever we are performing any kind of aggregate queries so  let us see what does it entails to do all these or what are the kinds of complexities that we encounter in performing these kinds of data integration and cleaning and so on and so forth the general model of data integration and cleaning is shown in the slide here as we saw in the previous slide  these are the different oltp data sources which feed into the data cleaning and integration unit and usually the  refer slide time  29  26  … … … … we have opened the box which was a black box here now we have opened the box and you see that there are usually two different units here the data cleaning unit and the data integration unit and the process by which data flows through this unit is not a unidirectional process that is you garner a data from this oltp data sources and pass them through data cleaning  pass them through data integration and also perform some kind of a back flushing that is  refer slide time  30  02  … … … … … that you have back into the oltp database itself we will look at some examples which tell us what it means actually and once the data is integrated it is fed into the data warehouse and based on queries in the data warehouse or updates in the data warehouse you get a feedback from the warehouse itself which tells how we said that you should perform your cleaning ? what is important for the data warehouse ? cleaning essentially means that we have to change whatever is important for oltp ? we have to change representations from whatever is important for oltp to representations where whatever is important for the data warehouse  refer slide time  30  51  so  let us look at data cleaning or the dcu in a little bit more detail as the name suggests  data cleaning performs cleaning operation on data sets that is data sets that contain dirty data what is meant by dirty data that is  what are the different sources of dirt ? i am talking about dirt within quotes so that one can encounter for given operational data source there are several different possible sources of dirt that requires cleaning to be performed the first kind of dirt is what called as lack of standardization for example  because you have different branches  different reservation centers or different cities there could be multiple encodings one of them could be using ascii base systems  one of them could be using unicode or one of them could be using head side or something or the other so there could be multiple encodings  multiple locales multiple languages in which data is represented and you should be able on to standard encoding or language or whatever there could be spurious abbreviations somewhere somebody writes mahatma gandhi road in part of an address and somebody else writes m.g road now we need to be able to recognize that m.g road is same as the mahatma gandhi road so when we are talking about  when we are answering a query like how many people comes from in and around mahatma gandhi road or live in and around mahatma gandhi road who buy tickets we should be able to say that  we should be able to search both m.g road and mahatma gandhi road further because they are the same and similarly there could be semantic equivalence  m.g road and mahatma gandhi road probably you can write an intelligent algorithm that will see whether mahatma gandhi road is an expansion of m.g road and so on and you could kind a well fairly find out this is a spurious abbreviations and so on however what if there are semantic equivalence ? chennai and madras some people may use madras for the city name and some people may use chennai and because we are talking about historical data  note that which is important here that is at one point in time officially chennai was called as madras so even in official documents  you would be using the name madras but then again it would have become chennai now you should be able to query later or say that these two are the same and knowing though intelligent algorithm can do that you need extra knowledge in addition to the data sources you need knowledge about the overall environment  the governmental policies  the standards and so on in order to be able to identify these duplications similarly there could be multiple standards there could be one data source which could be using the metric systems  while somebody else could be using miles and feet and so on and so forth so somebody might in someone data source might have said 1.6 well 1.6 kilometers is the same as one mile so 1.6 kilometers and somebody else might say one mile and so on  refer slide time  34  30  there could be other sources of dirty data like missing spurious or duplicate data for example  the age field of an employee could be missing which will hamper my query of what is the average age of an employee or something like that  what is the average or what is the correlation between the age of an employee and the attrition and the probability of attrition and so on and so forth or there could be incorrectly entered sales values or incorrectly or some typographical errors and so on or there could be duplication of data sets and duplication appearing in different forms for example  take look at the last example a person called l bala sundar would have registered in one  would have bought a ticket in one reservation center sighting his name as l bala sundar the same person could have gone to a different reservation center and bought another ticket at some other point in time  sighting his name as b l sunder and with the same address and the same phone numbers and so on and so forth and we should be able to detect such kind of semantic duplication that is  thee is duplicate data that is occurring at different data sources  but not in the same form that is the name has changed or something has changed  but semantically they are still duplicate  refer slide time  35  56  there could be other sources of dirty data like inconsistencies or incorrect or inconsistent use of codes let us say  in one of reservation center the gender of the person who is reserving a ticket is maintained as a character either m or f or and in other reservation centre  it is maintained as 0 or 1 it is just the different standards that each data base designer has designed for each center and this is especially true if this has evolved over time let us say that reservation center  number one would be using historical data base that was designed which is not changed because of cost considerations and the new reservation centers using a new data base system that has been redesigned which is termed to be more efficient and so on and so forth but which uses different codes for the same data element and there could be codes with some outdated meaning let us say third class sleeper because we are talking about historical data it is quite possible that we encounter some data about third class sleepers which no longer exists in railways today and so on and there could be inconsistent duplicate data that is  there could be two data sets that are found to belong to the same persons we have reasons to believe that it is the same person but  he has given two different addresses in these two sources and so on how do we detect such things and other kinds of inconsistencies like inconsistent associations that is  let us say one department provides particular kinds of says that the sales have been so on and so on but  it does not add up to the total sales figure that is also been provided so  associations across different data sources may be inconsistent and there could be semantic inconsistencies somebody might have typed february 31st in a date so what is that referred to ? is it february 31st or march 1st or march 31st or what or integrative violations in referential integrity like referential inconsistency there is  let us say 10 lakh rupees sales reported from a unit that has actually closed down which is not even there and so on so several kinds of these sources of dirty data crop up in practice and perhaps cleaning or data cleaning is perhaps the single biggest research problem that is still kind of open in the field of data warehouses  because there is no single thumb rule for data cleaning we can not just say all data is pass through one kind of data cleaner and it will be cleaned  because there is several sources of dirty data that can occur and there is no single way of cleaning them  refer slide time  39  10  so what are the issues in data cleaning ? it  refer slide time  39  45  … … … … is automated it is quite difficult to automate for the computer to learn by itself that let us say chennai was called madras or burma was called myanmar and so on and so forth and the thing we don ’ t even know whether the data cleaner is performing correctly which is what is called as gigo or garbage in or garbage out suppose you give some garbage and we don ’ t have correct rules we just give some junk data and it just gives some junk outputs we don ’ t know whether it has actually cleaned or whether the output is correct or not and it is its quite hard to verify the cleanliness of the output data and data cleaning requires considerable knowledge that is tacit for example  one kilometer or one point six kilometer is equal to one mile which we know but it is kind of a tacit knowledge it is not exquisite knowledge and which goes beyond the purview of the database like chennai was called madras so you should know data about governmental policies you should know data about geography should know some rather some knowledge about geography  governmental policies  metrics and so on and so forth which has to go into the data cleaning unit so  it is not possible to design a data cleaning unit in a way which kind of paraphrase does one size fits all or which says that this is the data cleaning unit and this will work for any kind of data in any context what so ever and so on and the data cleaning complexity increases as we increase the number of data sources or they increase the history span that we have taken up for cleaning  refer slide time 41  10  what are some of the steps in data cleaning ? how does typical data cleaning process looks like ? essentially  there are five different steps that go into a data cleaning process firstly  we will start with data analysis given the set of oltp data sources ; we start by analyzing them and look for certain kind of metadata that is what we can learn about the data that we have and then give all those metadata back to the user or the data cleaning now the user in turn will now specify a set of transformation rules that is if this is the kind of dirty data this how we have to transform it into clean data and so on so the user specifies the set of transformation rules that that are performed either the scheme or data level which transforms dirty data into clean data we then verify the rule by running them on test data sets that is some kind of sample data sets and then we incorporate the transformation rules into the data cleaner and then perform the data cleaning process so once the data cleaning process is performed we usually also perform what is called as a back flow that is we repopulate the data sources with cleaned data for example  suppose we have seen that chennai and madras are used interchangeably in in the in the oltp data source  we see to it that or we perform a back flow  so that it is all either chennai or madras in the oltp sources that is  there is some kind of standard in the oltp sources itself which is called as back flow of data  refer slide time 42  55  we shall not be looking into great detail into how each of these transformation rules is performed but let us have a look at some specific or some typical examples of how dirty data can be cleaned for example  how do we search for illegal values within what is the strategy for illegal values we could use some kind of a max min or a mean deviation or cardinality criterion that is slide a window through your data source that contains a max min limit for the particular data value whenever a given data value is lesser than the mean or greater than the max  you know that it is an illegal value so  similarly mean and deviation or cardinality and so on and so forth and spelling mistakes how would you look for spelling mistakes ? there are some techniques what are called as n gram outliers an n gram is essentially a collection or a sequence of n letters that form a different words for example  suppose i take a three gram  let us say hashing i perform a three gram transformation on hashing  then i get several different three grams ‘ has ’ is a three gram  ‘ ash ’ is a three gram and ‘ she ’ is a three gram and ‘ him ’ is a three gram and so on so i get different three grams  then what we do is we cluster all these three grams based on their occurrences and we see if  refer slide time  44  36  … … … … … layers in the clusters that is  are there any n grams which standalone without belonging to any cluster first slightly  these outliers are going to be spelling mistakes and that is the strategy that is generally used for checking spelling mistakes similarly  lack of standards that is compare values sets from a given column across different tables and see whether they are using the same standard or so on and duplicate and missing values that is compare the number of rows with the cardinality of this particular column so  you find out if the number rows do not match the cardinality you see that there could be some kind of missing values or null values say in this  refer slide time 45  31  so  let us look at one or two algorithms in slightly more detail which can give you an appreciation of the kind of the complexity that it takes or the kind of techniques that are used for data training simple algorithm for duplicate elimination what is called as the hash merge algorithm if you remember the storage structures session that we that we covered you know what is meant by hashing right that is given a particular tuple run it through a hashing function so that it is mapped on to a given bucket now what can we say about hashing across different tuples ? duplicate tuples that is tuples with duplicate  refer slide time  46  21  … … … … … therefore elimination of duplicates now reduces to searching within a bucket for looking at any duplicate values and then eliminating them  refer slide time  46  36  so here is an illustration of the hash merge algorithm let us say  there are four different records like this and this is the hash key that is the name and address of the person that is we want to eliminate duplicates as for as person information goes so as you can see  k j amit and 50 lvl road  both of them map onto the same bucket and these map onto different buckets and so on so you just compare within a bucket and then eliminate duplicates  refer slide time  47  07  there is another technique called sorted neighborhood techniques this is mainly used for misspelling or detecting misspellings in data sets and sorted neighborhood techniques is given by the following algorithm shown in this slide here first of all  identify what is the key ? as in the previous case  we identified that name and address is the key so identify what is the key within a given tuple then sort the table with  refer slide time  47  41  … … … … … the table based on the key you can see that all duplicates will cluster together right similarly all misspellings  there is a high likelihood it is not that there will be but there is a high likelihood that all misspellings will also be clustered together that is will be quite close to the actual spelling that has to exist that is for example if k.j amit become let us say k.j aman or something like that a m a n so it is aman is still quite close to amit that is assuming that we are sorting tens of thousands of records  misspellings would be quite close to the real spelling then  you slide a window that is take a set of n rows and keeps sliding it through the data base to see if there are any misspellings and then merge the misspellings and we have to make multiple passes until there are no more merges of records  refer slide time  48  55  so here is an example again where this is used for duplicate elimination in addition to misspelling detection where given this table here it is sorted like this here and given a window of size three we see that we see that there are duplicates within this window and we start eliminating them and the last algorithms that we are going to see for duplicate  refer slide time 49  15  elimination and also kind of and also for misspelling detection and so on what is called as the graph based transitive closure for  refer slide time  49  32  … … …  this algorithm is more of improvement over the sorted neighborhood algorithm where it basically reduces the number of passes the idea behind the algorithm is based on the notion of transitivity in an equivalence relation for example  let us say records r1 and r2 we see that r1 and r2 are duplicates so we establish a relationship that  r1 is a duplicate of r2 and so on and then we find that r2 is a duplicate of r3 we can definitely infer that r1 is a duplicate of r3 as well  because the duplicate is duplicate of relationship is a transitive relationship if a is a duplicate of b and b is a duplicate of c the then a should be duplicate of c what this means is when sliding the window once we established that a and b are duplicates  there is no need for us to compare a and c and so on so that is the essential idea behind graph based transitive closure  refer slide time 50  57  this slide here shows the schematic example where there are five different records and in a naive sliding window sorted neighborhood protocol  we start by comparing r1 r2 and r3  then r2 r3 and r4 and r3 and r4 r5 we are just sliding it with a window size of three however when we encounter when we slide r1 r2 and r3 suppose  we see that r1 r2 and r3 are duplicates so r1 is a duplicate of r3 and r2 is a duplicate of r3 and so on now  there is no need to compare r1 and r2 with r4 and r5 it is sufficient if we just compare r3 with r4 and r5 so that is the general idea behind this algorithm which can reduce the number of passes in eliminating duplicates  refer slide time 51  52  the next topic that we come in in the data cleaning and integration is the issue of integration itself that is  there are different oltp data sources from which we are getting data most of which have different kinds of inconsistencies  sources of dirty data and so on and then we have to clean them and so on but  after we have clean them we have to integrate them under a under a common banner and this integration can occur at two different levels which are shown here as data integration and schema integration that is schema integration entails forming an integrated schematic structure based on the set of all desperate data sources that is you have data sources from counter a counter b counter  refer slide time  52  50  … … … under one common schematic structure and data integration essentially entails cleaning and merging data from these different sources that is eliminating duplicates and merging all of them under this schematic structure  refer slide time 53  08  let us briefly look at what are the issues or what is the complexity now in schema integration as well and it is not as simple as it looks like consider the following schemata that is shown here or there is a schema that let us say the one of the oltp sources used which is called which uses a table called cars and the car table has different fields like serial number  model  color  stereo glass tint so on and so forth everything is in one table on the other hand  another retail center let us say uses two different tables for the same information and look at how these tables could be different first of all  there are two tables rather than just one table then the names are different all these names are in german and these are in english so cars are called autos and serial numbers are called as serine numeral or something color is called farbe and so on so  it is not just the structure is different  the name of each of these fields could be different the data types could be different and so on there are several different challenges that we have to face when performing schema integration that is there are naming differences  there are structural differences  data type differences  semantic differences  missing fields and so on and so forth  refer slide time 54  42  the generic architecture of schema integrator is shown in the slide here where it basically consists of two specific stages one is the lower most stage is what is called a wrapper or an extractor that is given a particular schema  the extractor maps all given schema into a standard schema set that is whether it is color or whether it is cars or autos or whatever it maps it onto a common schema set and then the mediator constructs the overall schema set or rather it looks at the federation of different data sources having the same schema and then handles query based on or constructs the data warehouse or handles query or whatever  refer slide time 55  35  so the difference between a wrapper and an extractor is that an extractor essentially physically extracts data  creates a schema and physically extracts data from the data sources according to the schema  whereas the wrapper is just a logical wrapper similarly mediator is a logical mediator while a constructor physically constructs a data warehouses from all the extracted data sources  refer slide time 56  04  so there are different tools for data cleaning and integration and as i said there is no there is no common tool that can fit all possible requirements  but there are several tools that can make the life easier for sink so  here are some examples d f power or eti star and ssa name and so on and so forth right so let us summarize what we studied in this session we looked at the important differences between olap and oltp queries what are the difference between analytical queries and transactional queries ? what are the characteristics of olap  refer slide time   … … … … of a data warehousing system where you take data from different oltp fields and take them through data cleaning and integration phases before populating the data warehouse and then of course  there is a feed back and back flush and so on and this itself is a major issue with the machine so that brings us to the end of the session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 31 introduction to data warehousing and olap – part 2 hello and welcome back in the previous session  we were looking into specific kind of databases kind of any sort or any kind of databases mainly meant for managing historical data or archival data  namely what are called as data warehouses that is we are looking at how the database design changes if the application domain is changed from an operational data management that is operational data management to data management for strategic purposes that is asking queries that are of a strategic nature  refer slide time  01  41  and we saw that  we saw the overall architecture for a data warehouse were you have to populate a data warehouse from different  several different data sources mainly the oltp data sources and these data from these sources are routed or source through a data cleaning and integration engine where we saw what is really meant by data cleaning that is there are several different sources of dirty data being creeping in different kinds of inconsistency  different kinds of missing values  spurious abbreviations and so on and we need to clean those data elements before populating it on to the data warehouse and of course the data elements are to be integrated under a common schematic structure before the database can be populated and we have of course things like back flushing of clean data back in to the oltp databases and we have feedback from the data warehouse to the cleaning engine and so on let us move forward in this exploration of data warehouses to see what forms the core of the data warehouse what kinds of data models are used to the core  what kinds of index structures or what kind of storage structures and such things are actually applied to the core of the data warehouse  refer slide time  03  56  so in this session we shall be concentrating on data models and warehouse design to begin with let us have a look at some examples olap queries like we saw in the previous session have a look at this olap queries something like which says how is the employee attrition scene changing over the years across the company or is it financially viable to continue our manufacturing unit in taiwan now the thing is a characteristic nature of these queries is that most of them involve huge amounts of aggregation that is we need to span or we need to read huge amount of data elements before we are able to conclusively answer a query look at the first example how is the employee attrition scene changing over the years across the company ? see we need to detect the trend here that is we need to detect the trend or employees more likely to quit the company now or employees more likely to stay or they happy or they unhappy or they finding better opportunities away outside and so on now in order to answer this question  we need to take employee data from across the company that is different data sources how many ever branches i have  we need to collapse such data from across the company and across the period of time it ’ s not sufficient if i just look 1 year data  2 years data we need to collect data over a period of time may be 5 years 10 years or something like that and aggregate it across employee behavior across the entire company and then we get a result something which something of the form which says that employee attrition seen or employee attrition rate is so and so percent that is given whenever an employee joins  there is so much of a probability that he is going to leave in say 1 year or 2 years or 3 years and so on and so forth so just to answer this small query that is just to get the small set of percentages  you see we need to actually go through the huge amount of data that ’ s present in the data warehouse so what are some of the typical characteristics ? one  we saw of course aggregation that is we have to aggregate over a large set of data and summarize results over a large data set we need to also often cluster data based on different parameters  is there a correlation for example in the previous slide we saw some queries talking about correlation so suppose you want to find out correlation then we want to  we may actually want to find out clusters along a particular dimension and or along a particular set of dimensions to see whether there is actually some kind of correlation for some values in the dimension to some kind of behavior that we are searching for  refer slide time  06  03  there is also trend detection that it is common in olap queries where you need to  you need to in some sense integrate over time that is you need to start from the earliest source and start working your way through the through till the latest source of data and project the temporal behavior of the data elements along certain dimensions and say this is the trend that the trend is increasing towards this thing  the trend is decreasing  the trend is fluctuating and so on and so forth then there are what are called as multi-dimensional projections that is a behavior could be attributed to several different dimensions and we may want to ask how each dimension contributes to certain behavior how is the total sales dependent upon say the location of my branch  let us say i am having several branches  a retail branch across the country and i want to see how sales are correlated to branches or how sales are correlated to different events like say some festivals and some kinds of national holidays and so on or the weather is it  the sales is more in summer or winter or what and so on so you may have to actually project these facts  this fact called sales across several different dimensions and this has to be performed as efficiently as possible no matter what the dimension is this slide shows the typical design of data warehouse at the core of the data warehouse say as you can see in this slide  there is a kind of something typical about this slide that is there is a core here and a lot of other surrounding elements here the core is what is called as the hyper cube of or also called as the data cube and several other names which maintain facts that are sorted across several different dimensions  refer slide time  08  29  for example i may want to store sales data across several different dimensions like say the branch dimension  the year dimension  the event dimension  the product dimension  which product has the most sales or which branch has the most sales or which year has the most sales or which event brings in the most sales or which  whatever i mean product or the category of products and so on and so forth so we will look shortly in to how this hypercube itself looks like or how we can design such hyper cubes and that basically stores facts that are contained in the data warehouse however maintaining purely just the hypercube would make the data warehouse pretty inefficient this is because again the good old as aggregation problem that is even if there are facts that are stored efficiently in a hypercube we still require to access let us say tens or thousands or probably millions of different facts before we are able to answer a query  before we are able to answer a query like is it financially viable to continue this branch or whatever we need to look at several different facts before we will be able to come out with the response so in order to speed up such processing  there are number of what are called as materialized views remember the notion of views from traditional data base what is a view ? view is basically a logical substructure of the overall schematic structure that is all of these materialized views are in some sense sub cubes of this hypercube but in traditional databases  views are essentially virtual tables that is you store the view as a query not as a base table that is not physically in the database so whenever you ’ re querying a view  you first have to execute the view that is generate the view first on the fly and then query the view but  here that completely defeats the purpose of what we are looking at essentially speed up in performance that is query response times so these kinds of materialized views are actually physical views that is views which are physically computed from the core  from the hypercube core and stored on to disc these could be something like  some kind of partial aggregate information  something like let us say i have sales data across all branches now i might want to store sales data for a particular product suppose i have a retail store  i may want to store sales data for let us say some kind of vanaspati oil or whatever across all different branches i may want to store partial aggregate information like sales data across quarters  across different quarters in a year and so on and so forth so such kinds of such kinds of data which are queried often are materialized from the hypercube core and stored as materialized piece now i am sure you might be wondering  why this would make sense  why don ’ t we do that in traditional databases because  we can not do that in traditional databases because it hampers the concept of normalization that is materialized view is essentially creating extra functional dependencies which over and above whatever is present in the core itself now what happens if there are extra functional dependencies ? maintenance is a big problem  that is whenever an update happens because there is redundant information or derivable information or maintaining all those derivatives of information becomes a huge problem for example if i have a particular amount of sales data for a particular product in a particular branch and let us say tomorrow i update this data element with a new number and this automatically affects all aggregate information that i am storing here what is the aggregate information for all products in this branch or for this product in all branches and so on and so forth so maintenance of materialized views is a problem that is whenever the databases or data warehouse is updated the materialized view should also be updated however this is not too much of an issue in data warehouses mainly because i am sure you would have got  what is the reason because the number of updates is far less frequent than in a typical oltp set up in an oltp set up that is in an operational database there are frequent updates and queries the number of updates is comparable to the number of queries but here the number of queries is far more than the number of updates the number of updates is very infrequent  possibly of the order of once a month or once in 6 months sometimes and so on where you can actually afford to bring down the warehouse system  the olap system  update the olap engine with the new data it has come in and then compute all the materialized views  refer slide time  14  32  so this cube  this slide essentially summarizes what we have just talked about that is there is a hypercube core that makes of the core of the data warehouse and the core manages what are called as atomic data elements that is data elements which can not be sub divided into other kinds of data elements and it ’ s the global schematic structure for the entire warehouse remember this global schematic structure is what we have derived from the different oltp sources and this is based on the multidimensional data model and we will see mechanisms by which we can actually implement it in practice and materialized views are physical views for faster aggregate query answering and it is  it essentially amongst to de normalization of the core and this de normalization is performed to increase performance  is done to increase performance rather than updation and updation is an issue which but because updation is relatively infrequent  it ’ s not too much of a problem so let us look at the hypercube core again in a little more detail this slide here shows a particular hypercube having three different dimensions of course the reason why we are calling it as a hypercube is that it can have any n different dimensions need not just be 3  it could be 5  10  25 whatever so it could be any different dimensions  refer slide time  15  27  now as you can see here each dimension talks about specific aspect of the data that we are storing let us say this is the sales hyper cube  the amount of sales that has happened now this dimension stores the amount of sales across each product and this is across each week and this is for each branch therefore each cell in this hyper cube refers to sales of a particular product in a particular branch in a particular week so in this hyper cube  the constituent of the cells that is the sales data is what is called as the fact that is stored by the hyper cube  refer slide time  16  21  so the hyper cube manages the fact called sales and branch  product and week  along which the sales are projected  are what are called as dimensions so the sales fact is projected across three different dimensions in addition to normal relational database operators or rather in lieu of usual relational databases operators  there are several other operators that are defined on the hyper cube itself here are some of the operators that are commonly used for data warehouses pivoting  pivoting essentially means  what do you understand by the terms pivoting ? essentially it is rotating the cube given a pivot that is suppose you want to see the sales data projected across some 3 different dimensions or some 4 different dimensions or whatever we choose this set of dimensions so that these can be displayed on the visualization engine data warehouses or olap engines usually come with a  usually have a huge visualization counterpart which helps the user to visualize the inherent knowledge contained in the database or in the data warehouse in different ways that is one way is that of pivoting the warehouse across different dimension then there are slicing and dicing operators that is selection of some sub set of the cube you can think of it as slicing the cube across different dimensions or dicing that is forming a sub cube out of the larger hyper cube and so on  refer slide time  17  16  usually data cubes also supports this role up operator were you can aggregate information on one or more dimension for example we may want to roll up the weeks data into months data that is in the previous example  refer slide time  18  28  we have data per week now instead of data per week  we might want to collapse certain sets of dimensions like this in order to form month wise data so that means we have rolled up in the week dimension across one aggregate level so that the week has now become months and so on similarly the opposite operation of roll up is a drill down operation were given an aggregated dimension  you open it up in order to reveal more details that is open up months to reveal week by week information or open up weeks to reveal day by day information and so on and so forth provided that exists in the database  in the data warehouse how are these hyper cubes implemented in practice ? in practice there are two major approaches towards implementation of hyper cubes  what are called as rolap and molap approaches rolap essentially means the relational olap that is where the multi-dimensional hyper cube is transformed by certain set of transformation rules to the relational model and vice versa that is the relational model is transformed back in to hyper cubes for answering questions so every hyper cube queries are mapped on to relational database queries and vice versa  refer slide time  19  23  and the other kind of implementation strategy is called molap or naive or native multidimensional model where the storage structures and index structures are redesigned for storage of multi-dimensional data itself and which has got nothing to do with the relational model in itself so when you provide a data warehouse query  a molap query it is directly translated on to disc accesses and further such low level query primitives and answer directly so there are  this slide also shows some examples like say true relational olap  it ’ s from micro strategy which is an example of a rolap and arbor essbase which is an example of a molap system let us look at rolap systems in a little bit more detail that is how are rolap is actually implemented  how is a hyper cube implemented  how is a collection of hyper cubes implemented and so on the most fundamental form of implementing a hyper cube using a set of tables is what is called as the star schema structure or the star schema architecture the star schema architecture is shown in this slide here as you can see the name star schema comes from the way that the schema is organized that is the schema has the central fact table and the fact table obviously stores facts that is their sales data for which is the fact which is stored for all possible combinations of branches  branch products and weeks  refer slide time  22  16  that is branch one  product one  week one sales so and so branch one  product two  week one sales and so on and each of these columns here that is branch  product and week contain ids which are foreign keys in to the respective dimension tables that is there is a branch dimension table which contains information about branches so if branch number one is stated here  this would be a foreign key here which would give details about what is branch number one that is the name of the branch  the address  the manager  whatever and so forth and similarly product  each product is a foreign key here which gives details about each of this product and so on so this is the most simplest form of representing multi-dimensional hyper cube in a relational data model now let us briefly summarize what a star schema is about there is a central fact table and there are set of supporting dimension tables that is in the previous slide  the green table is the fact table and the other three brown tables are the dimension tables and the advantages of a star schema  it ’ s pretty simple to comprehend and also to design and there ’ s very small amount of meta data that ’ s required  small numbers of foreign key relationships that have to be maintained and its very quick for query responses as long as the queries are on the facts that are stored in the database  refer slide time  23  38  however there are certain limitations as well that is its not robust towards changes especially in the dimension tables suppose if i want to add something to the dimension table  it ’ s quite difficult and there is an enormous amount of redundancy in dimension table data because dimension tables need not be normalized and we may just be repeating information in the dimension table the next kind of relational model that is used is what is called as the snowflake schema the slide here shows an example of the snowflake schema well  it doesn ’ t look like a snowflake in the slide but you can imagine why it is called as snowflake schema that is it is a star schema that is augmented with something more that is the branch dimension here itself consists of 3 different tables the product dimension here consists of 3 different tables and the week dimension remains as it is what are these three different tables ? these are basically normalized data structures or normalized versions of the branch dimension table so if the branch contains several information the manger information  the location information  the turnover information and so on and so forth  you may want to essentially break it up into different table so that accessing data about branches becomes more efficient  similarly for other products  refer slide time  24  14  and the result is that each such dimension here has sub dimensions which kind of gives the gives an overall structure like a snowflake where there is a central fact table and branching out dimensions and so on so the features of a snowflake schema detail here that is like always there is a central fact table as in the star schema and however rather than just dimension tables  in storing de normalized or un normalized data it ’ s not of de normalized it ’ s more of un normalized data in each of these dimension tables  its actually dime normalized data that is stored in the dimension tables and what are the advantages of this thing ? the query responses are faster  especially on the dimension tables we don ’ t have to work search through the dimension tables for searching on a specific queries because it ’ s normalized and you have several foreign key relationships and index structures and so on and it ’ s more easier to update especially on the dimension table but there are certain limitations as well that is there is large amount of meta data in fact the because we are talking about aggregated data that is archival data  we might very well end up with a large amount of tables especially when we try to normalize the dimension tables  refer slide time  25  27  so a given branch information may create possibly hundreds of different normalize tables  if we have to normalize it to say let us say fourth normal form and it becomes as a result it becomes much harder to comprehend when we are looking at it manually  when we are looking at the overall schema manually the third kind of rolap model or the relational schema model for implementing multi-dimensional structures is what is called as the constellation and this is perhaps the most used model  most practical model because it contains multiple fact table not just one fact table  refer slide time  26  40  and in a data warehouse  we usually require to have multiple fact tables that is let us say each fact table corresponds to one data mart so one data mart stores information about sales  the another stores information about salaries  the other stores information or facts about procurements and so on and so forth or cause and so on so when we have several such fact tables  we may want to actually  we may want to actually have several stars schemata intermingling among one another and in addition  suppose whenever we have some kind of materialized view for example let us say this is a sales data and this is some kind of a materialized view that talks about aggregated sales data across different or it just talks about the discounted sales that is sales which have been made whenever discounts were offered and so on so it is some kind of a materialized view that has been materialized from the main fact table so as a result what happens here is this fact table here shares some of the dimension tables from the main fact table  from the sales fact table so as a result you have a dimension table actually having foreign keys in 2 or more fact tables and 2 or more fact tables having foreign keys in to this dimension tables  refer slide time  28  47  so the result looks like a constellation of different stars that is the different stars and they are staying together so that it forms a kind of a constellation so the constellation is the most commonly used architecture and used when multiple fact tables are required and usually not and that is even when multiple fact tables are not necessary or constellation schema is used in order to maintain materialized views in addition to the central fact table that is it usually has a main fact table and several auxiliary fact tables which are materialized views over the main fact table it helps in faster query answering for frequently asked queries  however it is costlier to update than a snowflake schema which is kind of obvious in this what are some of the issues in data cubes ? there are certain especially managing data cubes in a rolap architecture that is using a relational data model there are certain issues which in some sense refuse to go away  refer slide time  30  20  that is they can be sometimes so severe that it may render  it may render the data warehouse useless that is it may not able to provide interactive response times which is the most important factor for a data warehouse one of  probably the main such challenge is what is called as the curse of high dimensionality there is several known index structures by which we can index on different tables or on different dimensions as efficiently as possible but most of such known index structures degrade in performance  degrade to almost linear search in performance when the number of dimensions become high so with a large number of dimensions  searching efficiently for a particular data element becomes really difficult then they are what are called as categorical dimensions it may not be possible to order the elements of dimensions across a given scale for example week it is easy to order let us say i have a week information  so i can say week 1  week 2  week 3  week 4  week 5 and so on and there is an implicit ordering that is we know that week 3 should come after week 2 and before week 4 and so on it ’ s not possible for week 3 to appear after week 5 and so on so there is an implicit ordering information now this ordering information is sometimes exploited for faster retrieval and computation of clusters and so on but then there are certain kinds of dimensions which are categorical in nature  not ordinal for example products or say branches  products let us say vanaspati oil  rice and ghee and so on and so forth now which comes first and which comes second ? should oil come before ghee and should ghee come before rice or the other way around there is no such ordering that is given for the elements in this dimension so  categorical dimensions becomes difficult to index we look at some index structures later on where we will see that why it becomes difficult to index categorical dimensions we can not identify a region and say this data element falls between this and this region because there is no ordinal information about the dimensions and even when there is ordinal information about dimensions  sometimes ordinal classes vary whenever we aggregate for example this slide shows a given example here  refer slide time  32  38   let us say at the lowest level  we are having names about different sets of students that have come through a given university and passed out and so on so we have ordered the set of names  student names according to lexicographical order however when the set of students are aggregated into classes  let us say batch 1  batch 2  batch 3  batch 4 and so on then the ordering becomes  the ordering is now based on their graduation year  the batch 1 graduated in 94  batch 2 graduated in 95 and so on and so forth now it ’ s no longer lexicographical ordering so the same index structure that is used for the lower level dimension can not be used when the data cube is rolled up that is when we have aggregated the set of students to a set of batches and so on  refer slide time  33  56  when we are designing a data warehouse there is another  there is a very important dimension called the time dimension which probably needs to be addressed separately and that ’ s why there is a separate slide on the time dimension because there is certain properties about time which other dimensions may not really adhere to  may not really hold the time dimension as you see is usually mandatory in most warehouse applications otherwise that the whole idea of archival data or historical data becomes meaningless if there is no time dimension associated with it and if you are talking about trend detection and so on  we can not do it without time dimension and the time dimension has several different meanings based and for rolap techniques depending on the application context that is suppose we rollup weeks into months or months into year  should we follow the simple calendar based rollup or should we follow the physical calendar  let us say if it is a company or should we follow the academic calendar if it is a university and so on so it has several different meanings when we are talking about rollup and we need to also  in time we need to not only just order based on time  we need to also index some special events like graduation date or releases or some festivals or so on and so forth which actually affects the facts like the sales or whatever it is that we are storing and we have to we have to timestamp these events with specific whatever time scale that we are using whenever we are having the time dimension and lastly the order of traversal of the time dimension is important if we have let us say a set of all students or set of all products and we want to find out some aggregate information  it does not matter whether we traverse this dimension front to back or back to front but when we are talking about time and when we are talking about trends  is the trend changing over time  the order of traversal becomes extremely important and let us look back at the materialized views coming back to the materialized views  how do we know when to create a materialized view ? as you know materialized views are summary tables that actually create physical views of the fact table and creating a materialized view is a tradeoff between faster query answering and increased complexity during updates that is whenever i update the data warehouse  there is a question of view maintenance  materialized view maintenance so how do we know when to materialize ? a good measure for making such a decision is what is called as the rss ratio or the result to search space ratio the result to search space ratio is a simple ratio which is shown in the slide here  refer slide time  36  03  that is how many number of rows of results are returned divided by the number of rows that are scanned for a given query for example if i say what is an average age of what is the average age of the employee or how was the average age of the employee change over the years so let us say every year we have some 4000 employees  employee records and we have 10 years of data so the number of rows that are returned are just 10 for each year the average age however the number of rows that are scanned are 4000 times 10  about 40000 so that gives us the rss ratio so  we decide to summarize if the rss ratio is too small and the query is too frequent that is if 10 by 40000 is considered too small given the frequency of the query  we say we have to materialize this that is we have to physically create a separate table for this  refer slide time  38  11  there is also an other kind of table in rolap structures which are again useful  which are called as revision history tables and which are again quite crucial when answering different queries remember in data warehouses  we are storing archival data that is data over a period of history now archival data essentially means that there is more of a probability that some kinds of data elements are revised over time for example turnover per employee as shown in the slide here that is what is the turnover that the company is getting per employee  that is the total turnover divided by the number of employees now it keeps changing every year now let us say  suppose we just have to  we don ’ t have to  we don ’ t want to store it as historical data that is we don ’ t want to store it as how turnover is changing over time but rather what is the turnover per employee so the revision history table essentially stores the different values that this turnover per employee has taken up over each different updation that is the first updation  the turnover per employee was this and the updation was on this date whenever this value was updated the second time  the value is this and the updation date is this and so on so this information is used based on what time frame is the query addressing  when it is asking a question for example i might say what was the turnover per employee in the last decade  then i might want to take up some value that was updated in the last decade rather than this decade and so on so that ’ s where revision history tables become important so let us quickly look at what are the different steps that it takes while designing a data warehouse see when we are designing a data warehouse  we are looking at a warehouse from a managerial perspective that is we are essentially starting out with an enterprise model that is we have an enterprise model of how our enterprise works like whether it is a company or whether it is a large organization or university or whatever there is certain kind of enterprise model  a university is divided into departments and labs and faculties and so on and so forth and so on  refer slide time  40  00  now based on the enterprise logical model  we built the logical data warehouse model or the data warehouse logical model and from the data warehouse logical model  we come down to the data warehouse physical model which is the star schema and snowflake schema or constellation and so on and so forth now this part is handled by the data warehouse administrator or the database administrator along with the end user the database administrator has to clearly understand the end user requirements before building the logical model and here it ’ s usually a question of the database administrator using one or more automated tools that can translate are given logical model on to a physical model there are some design for the most pattern art where how to get good designer  how to calibrate design is actually quite a tough problem to answer however there are some thumb rules that can help the designer in designing good data warehouse models and this slide summarizes some of the thumb rules  things like the warehouse logical model should closely resemble the enterprise model and not let say the operational model the enterprise model is the overall strategic model of the enterprise while the operational model is the model of let us say given particular operation let us say withdrawal from an atm has a particular operational model but this operational model rarely figures in the overall enterprise model of the bank itself  refer slide time  41  49  so the warehouse logical model should resemble the enterprise model rather than the different operational models and note that the oltp sources are based around the operational models rather than the enterprise model and there are some transformation rules which are usually necessary from enterprise to warehouse models  refer slide time  42  44  and special planning is required for managing time dimensions and revision history which we saw a special tables that we saw in the previous slides so how do we  let us say how do we design this data integration that is from oltp sources to data warehouses what it is entitled for creating or integrating schemata from oltp sources to data warehouses so oltp to olap or oltp to warehouse models are also based around certain sets of design principles which again have certain kinds of thumb rules again like there is no simple way of or there is no algorithmic procedure for designing schema however this thumb rules help in creating a good design or evaluating different designs for example several oltp schemata has a number of operational data fields like counter sales table that is the cash register number or the cashier identifier and so on such operational data fields are usually not useful for strategic decisions something that talks about what is the trend in sales and so on we usually don ’ t care which cashier did what and so on so we have to remove all such operational data fields  it ’ s important to identify which data fields are purely operational and which data fields have some kind of strategic importance  refer slide time  43  20  and of course we have to add a time element time element may be implicit in the operational model but here we have to add them explicitly for the data warehouse model and also of course version elements if necessary and we have to decide on the derived data or the materialized views as early as possible and usually commonly used materialized view is what is called as the all view that is for example if i have product wise information let us say product is a dimension  so we have product 1  product 2  product 3 as different values in the dimension and the last value will be all so  all basically says what is the sales value for all products similarly all value in the branch dimension would say what is the sales value across all branches and so on so that was about rolap structures and let us briefly look at what kinds of strategies are used for molap structure that is native multi dimensional databases or hyper cube maintenance and what kinds of index structures are used for handling such multi-dimensional retrievals  refer slide time  45  09  so  just to summarize we have different classes of dimensions  there could be categorical dimensions  there could be ordinal dimensions dimensions could be sparse where there are small number of data points per value or dimensions could be dense where there are large number of data points per value  refer slide time  45  45  now if we are talking about indexing note that when we are talking about molap  i directly went into indexing because this is the main indexing in addition to the storage structures are what constitutes the native support for molap so when we are talking about indexing  indexes are usually based around ordinal classes but there are categorical indexes as well  as we will see and the performance may depend on the storage structure for any given data set  refer slide time  46  22  so what kinds of storage structures are used for multi dimensional data or how do we store multi dimensional data or hyper cubes on to disc  refer slide time  46  44  remember discs are just stored in blocks and sectors and so on so  one way of storing is simply multi dimensional sorting that is let us say there are 3 dimensions and there is a fact so you just sort the table along all 3 dimensions  first along dimension 1 then along dimension 2 then along dimension 3 and so on  so as simple as this so  it just becomes a table one long table  the hyper cube is just one long table and this is very simple to implement and searching is very fast  if the dominant attribute that is in this case dimension one is part of the query  else it becomes fragmented and it becomes slower another kind of storage structure that is commonly used are what are called as space filling curves have a look at the slide here carefully as if you see in the slide  this slide shows a two dimensional space discrete two dimensional space that is 0 1 2 3 4 like this  0 1 2 3 4 like this and there is a curve that has gone through the space like this  refer slide time  47  16  that is it has covered the entire space without revisiting any point again or without revisiting any point twice so it is quite simple to calculate the position of a data element whenever a space filling curve is used like this that is the first data element 0  0 is here and 0  1 is the second data element and 1  0 is the third data element and 2  0 is the fourth data element and so on so  it is very easy to compute the location of a data point however  it suffers whenever the number of dimensions are high or especially when there are sparse dimension that is when there are large number of cells which have no data points associated with them then there are  so from these storage structures let us move on to some kinds of index structures that is how do we index elements based on this storage structures the simplest kind of index is the ordered index on multiple attributes that is instead of when you have to index based on multiple dimensions  index on each dimension separately  refer slide time  48  47  so suppose you want to index on n different dimensions k1 to kn  index on k1 separately and k2 separately and k3 separately and so on and ordered index files are maintained by ordering each such key in sequence but of course ordered dimensions suffer from several short comings especially when the number of dimensions is high or when the number of data points is high  you need to compute intersections across these different dimensions which becomes a lot of overhead there is also what are called as partitioned hashing where given a composite key like this that is n different dimensions  a partitioned hashing returns n different bucket numbers and the hash bucket is simply the concatenation of the n different bucket numbers  refer slide time  49  42  and other kinds of multi dimensional indexes include the grid files we have seen the grid files when we are talking about index structures for normal databases  refer slide time  50  00  so  we shall not be covering grid files now in great detail except to state that we can think of set of buckets forming a grid and a multi dimensional index would in turn map on to a set of buckets in this grid are what is called as the bucket pool and then we essentially find out what is the or find out the corresponding data points  refer slide time  50  18  then there are what are called as the bit map indexes which are quite again  quite commonly used and these are used on fields that are sparse that is where fields have only a small number of values for example gender has only 2 values or grades in a university setting may be has 5 values a b c d e f  a b c d e and so on  refer slide time  50  55  and essentially what we do is we store a corresponding bit vector that enumerates all possible values and sets the corresponding bit for each data element and it ’ s much more compact than other index structures because we would be able to answer composite queries like this product and this branch and this week and so on and so forth  quite efficiently this slide shows a particular example let us say there are 3 subjects database  artificial intelligence and say parallel distributed systems and 6 different grades a b c d e f  refer slide time  51  21  now each grade is denoted by a specific bit that is set in a 6 bit vector that is there are 6 vectors in this and each grade has a particular bit set and no values  all zeros similarly there are 3 different subjects  so there is a 3 bit vector here and corresponding bit is set suppose we have to search for all students who have scored a in databases and ai what is that we need to do ? we just need to compute the logical and between databases and ai and grade a and that would be the logical intersection of the corresponding buckets or the corresponding sets that are stored in this values another kind of multi dimensional index is what is called as kd trees which is again used for efficiently answering point queries in a multi dimensional data space  refer slide time  52  25  we shall not be looking into kd trees in more detail let me just explain kd trees by an example  refer slide time  52  37  you might of heard of binary search trees were given a particular data element  all data elements having a key greater than this particular data element is in the right sub tree and all keys lesser than this given keys in the left sub tree the same strategy here is expanded to k dimensions that ’ s where the kd trees come from k dimensional tree  refer slide time  53  38  so let us say this is an example data set we have there are 2 dimensions x and y  so representing let ’ s say salary and age and these are the set of data points we have the way we build a kd tree is simply like this the first element becomes the root  for the second element we start by comparing the x dimension  the first dimension here now because 5000 is greater than 25000 it comes here for the third one we start by comparing the x dimension here the first dimension  so 4500 is greater than 25000 comes here now here we compare the second dimension that is 32 with 28 and because 28 is lesser than 32  it comes to the left of this tree and so on so as you can see here  all the odd number of places corresponds to dimension 1 and all the even number of places correspond to comparing it dimension 2 so it ’ s a simple extension of the usual binary search tree data structure for k different dimensions but of course there is an other way of looking at kd trees as where each data point here is found to bisect a given space into two different sub spaces we shall not go into much more details here because kd trees in itself is a vast subject by itself  refer slide time  54  15   refer slide time  54  25  and there are other kinds of data structures which i shall not be going through like quad trees and r trees which is again a very commonly used data structures where we manage regions and there are certain user specified regions and there are certain virtual regions  refer slide time  54  30   refer slide time  54  36   refer slide time  54  37  and so each data point is also represented as a region of zero area and so on so a region is supposed to consider  is supposed to contain other regions within it and it is supposed to have a particular capacity whenever a region exceeds its capacity  it is split to form 2 different regions so the slide here schematically shows an r tree where this is the root node which is the overall region and which has two children having two virtual regions  refer slide time  55  45  and each of these two children have two other children that is children that is child 1 and child 2 and there are to be a virtual region here  so child 1 and child 2 and so on and it is split correspondingly and r trees are suitable for range searches  neighborhood searches  nearness searches and so on  refer slide time  55  47  but however all of this suffer from the curse of high dimensionality and let ’ s not try to prove this here but it ’ s obvious when we analyze this data structures more closely lastly let us look at one data structure for indexing categorical data here as you can see until now we have been looking mainly at ordinal data whether it is k tree  kd trees or r trees it is important that there is an ordinal information associated with each dimension  refer slide time  56  08  that is this is lesser than this and this is lesser than this and so on but here in categorical data we use what are called as signatures or bit map indexes by which we can say if the set of all ingredients whichever appear in a particular element is set to 1 here and all other elements are set to 0  refer slide time  56  38   refer slide time  56  43  so which basically forms what is called as the signature tree for categorical data and there are other kinds of extensible signature hashing and so on and so forth which we are not going to cover in more detail here  refer slide time  56  56   refer slide time  56  58   refer slide time  57  10  so let us summarize what we learnt in the data model section of data warehouses we saw that a data warehouse contains the olap hyper cube at its core and having different materialized views and then we also saw different kinds of rolap and molap implementations of this materialized views and different kinds of index structures  so that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 32 case study  mysql hello and welcome to another session in database management systems in this lecture today  we will be looking at a case study of a real world dbms we have learned so many concepts in database management systems like especially the relational database  i mean how relational tables are stored  what is the relational algebra and what is the query processing engine do and what are the different kinds of indexing methods and so on and so forth let us see how it is actually implemented in some real world context in this lecture today  we are taking up a pretty interesting real world database system namely mysql database system  refer slide time  01  24  what is very interesting about this database system is that it is a phenomenon or it is an outcome of a very remarkable phenomenon of recent times namely the open source and the free software phenomenon if it were not for the open source or the free software phenomenon  i would contain that several different innovations that we see in the realm of algorithms and software design would not have been possible today it is possible for anyone around the world to be able to freely download this database here which we are going to  this dbms here which we are going to see today and you can download it for free of charge that is it ’ s not only you can download it for free for charge  you can also have access to the source code that made up the mysql database and you can make your own changes to the source code of the dbms and it ’ s completely free in that sense  free as in freedom like which is generally used  which is the term generally used by the free software foundation people who allocate free software around the world  refer slide time  03  37  so let us look at mysql in more detail mysql is a very popular open source database system and it is licensed under what is called as gpl or the gnu public license of course looking in to the inter cases of gpl is definitely not within the scope of our lecture here but essentially the gist of gpl essentially says that the software provided is free in the sense that you are free as in freedom that you are free to make any modifications to the source code you can free to redistribute the source code  you can free to resell the source code if you wish to however you need to also give the same freedom to whoever is going to next use your software and so on and there are different varieties of gpl and you should check up what exactly is the specific variety of gpl when you download mysql and it ’ s a widely used database management system and it is used in the wide variety of systems like embedded systems to large scale information systems and it ’ s largely written in c and c plus plus and its source code is available like i said and it has been ported to many different operating systems and operating platforms and the website on which it is available is shown here in the slide namely www.mysql.org let us have a brief look at the history of mysql  how it came in to being and what are its different features mysql is a relatively recent phenomenon in database management systems in fact the genesis of mysql stems from another small sql engine called msql  small m and sql as shown in the slide here  refer slide time  04  49  msql or i guess it stood for mini sql was a simple sql engine that allowed users to write sql queries and maintain very simple databases with a small set of tables here now some set of people in sweden actually who try to tweak msql  msql also was free software and they try to tweak msql and out of this endeavor arose a completely new set of database system which is now called mysql now mysql has now  the people who formed mysql has now formed their own company mysql ab which is a swedish company and run essentially by the three people who formed this database system in the first phase and of course there are also commercial versions of mysql which are sold for a price and which come with lot more support from the mysql team who can actually install and tune your database system and so on and so forth but for the free version of the software  there is no support and there is no warranty as such mysql has several different features  there are several apis or application programming interface that are available for many languages like c  c plus plus  eiffel  java  perl  php and so on and so forth this apis allow programs written in these languages to directly send sql commands to mysql so you can embed your database system or rather your database client within another application program it uses b tree based disk tables and mysql team themselves developed new storage structure is called myisam and also performs index compression  so that the shadow of the index on the disk is quite small and there are storage engines that are available which support both transactional and non-transactional operations remember what is a requirement of a transactional operation essentially the acid semantics  that it has to provide atomicity and isolation and consistency in durability and so on durability also in a sense implies recovery that is once a commit has happened  you should be able to  it should be reflected in the database even if there is a crash later on and so on mysql is a multi-threaded engine and if your operating system supports threads at the kernel level  mysql uses that and if your operating system and your hardware of course support multiple processors  mysql automatically uses multiple processors  refer slide time  08  44  so it is scalable to many processors on a multi-processor machine and is aware of the fact that there are many processors that are running and you can utilize them the kind of memory allocation system that it uses is a thread based memory allocation system rather than a process based where each thread which is a light weight process manages its own memory it also has other nice features like fast joins which is computed by an optimized algorithm which just uses one pass operation over the tables when performing the joins we saw several different kinds of join techniques merge join  hash join and so on  so which essentially forms the gist of the kind of algorithms that go into these techniques here that computes fast joints and whenever temporary tables are required  say virtual tables or tables as a result of a query in a nested sql query and so on  they are implemented using in memory hash tables and they are extremely fast to access and so on mysql server has these double features that it can run as a standalone server as well as it can run as an embedded server that is server that is actually embedded within some other application so your application actually runs as the server and it looks as though your application is supporting database system in itself  refer slide time  11  27  mysql supports several different data types  several different atomic data types we saw some kinds of data types when we are talking about sql for example integers and date type and character strings and characters and enumer or the set data type where you provide the enumerated set of values or the set of all possible values that a particular data element can take and so on similarly in mysql  there are several different atomic types that are supported  signed and unsigned integers and there are even different lengths of integers they are called as tiny int  small int  integers and big int and so on and so forth so there could be 1 byte or 2 bytes or 4 or 8 bytes long and there are floating point numbers  double precision numbers  characters  varchar essentially is a string which can  variable number of characters and text and blob is a binary large object which could be something like any media object or audio visual file or anything like that date  time and so on it even supports variable length records  your record length need not be fixed you can have a combination of fixed and variable length records within a single database and mysql supports functions in both from and where clauses of an sql statement remember an sql statement has three different parts where you say select something from something else where the condition so you say select and you say set of field names or attribute names and which says what to select but here in fact you can say select and give a function so you can say select average of the values of this lets say a total marks taken by or scored by students and so on  refer slide time  17  17  so you can say select average of marks from student where marks equal to whatever and so on  where mark is greater than 60 % or whatever and so on so you can have functions where average here is a function that ’ s actually in the from clause  not exactly in the where clause we saw in the session on sql that functions being supported in the from clause is only a recent addition to the sql standard and it was not primarily in the initial sql standards it also supports  mysql also supports outer joins both left and right outer joins remember what is an outer join  when you are joining two different tables  you basically join let ’ s say you are computing an equijoin  so you basically join based on the value of the certain attribute therefore join happens where  join happens by matching values different values that this attributes takes between the two tables now it might so happen that for a particular value in the first table  there may be no corresponding value in the second table such records are generally ignored if you are computing an inner join or a usual conventional join operator but in the outer join operator  you compute even those you don ’ t throw away such records and embed them with nulls essentially so this example essentially is an example of left outer join and the corresponding or the dual operation of that were a given attribute in the second table has no corresponding attribute in the first table would form a right outer join mysql also supports aliases on tables and columns  so you can refer to the same table and column using different names and in recent versions of mysql  especially in mysql version 3.2.2 and later you can actually provide an sql query across different databases itself so where  what is a database here ? databases essentially is a set of tables that are stored under one heading saying this is the student database  this is an employee database and this is the some other database and so on here you can actually provide  you can actually give sql queries that can span across different databases within a single query there are also many features like scalability and performance issues and so on mysql has been tested on very large databases up to databases which have up to 60000 tables and 5 billion rows in the table it can allow up to 64 indexes per table that is you can index up to 64 attributes in a table and up to 16 columns as part of an index essentially  16 columns as part of the key that makes with the index so your key can be up to 16 attributes long and it also supports prefix based indexing what is a prefix based indexing ? essentially on data types like varchar which is a string or text  you might want to ask queries like show me all students whose name starts with some pra or whatever something like that so you might want to search for similar strings or you might want to search for prefix match strings and so on so such kinds of search can be efficiently supported using prefix based indexes rather than indexing on the complete string as such mysql also supports user privileges and password based authentication by users and also host based authentication so if the same user or if an authenticated user logs in from a different host  here she would have go through a different set of authentication privileges here she may not be allowed access from different host and only from particular host as such  refer slide time  17  29  now what kinds of standards thus mysql comply to ? note that when we talk about standards  we don ’ t mean something that is set in time when we say sql standard  the standard called sql itself has been evolving over time they almost every several years or so new features are added to the sql standard and which becomes the new version of the standard in addition to this evolution in standards  mysql or any other database system would itself be evolving that is when the dbms is implemented there would be a need that is felt for some more features to be included and so on so there is a different set of evolution that is happening as part of the database itself or as part of the dbms itself so there are two different evolutionary streams  one of the evolution of the standard and one of the evolution of the dbms itself and when we say it supports or it complies to a particular standard  we should be clear about which version of the dbms supports which version of the standard because both are evolving in its own different trajectory so mysql actually has several different modes or it can operate in several different modes that can support several different standards in sql for example mysql versions greater than 4.1 can apply different modes to different clients itself that is let ’ s say client one connects to mysql and says i want to use the ansi standard of sql and second client connects to the same mysql server that is running and says i want to use the ibm db2 standard of sql and not the ansi standard sql and so on so the default mode of operation for a client can be set with this option  it is shown here  refer slide time  19  27  minus minus sql mode equal to a given mode  when we are starting mysqld that is for the server rather not for the client that is you can set up  this is the default standard or this is the default mode of operation for mysql and ansi sql can also be selected using the minus minus ansi option  when you are starting up mysqld that is in the command line itself and you can set the mode for any particular client at any time using the set command that is set sql mode equal to some particular mode so what are the different modes that it supports ? the ansi mode for example where set mode equal to ansi so when mode is set to ansi  there are certain implications in the way mysql treats given sql query for example a real suppose a attribute is named as real then it is by default treat it as double  that is double precision real number by mysql but in ansi standard sql it is treated as a floating point number  the single precision floating point number  refer slide time  22  00  and similarly you can use double quotes for identifiers rather than strings that is when you are identifying column name  you can actually use double quotes rather than when you are representing the string data which is part of a column similarly this double pipe operator that is shown here is treated as a concatenation operator rather than the logical or operator  the two vertical lines that you see here  refer slide time  21  14   and when i give a function let us say average  it ignores spaces between a function and the first parenthesis suppose this is the function here  now it ignores all the spaces between the function and the first parenthesis here  refer slide time  23  17  now what is the implication of this ? the implication of this is that i can not use the same function name as part of a let us say a column name or a variable name or something like that because this would clash with the function name because there is no way to  there is no way for the sql server to distinguish whether this is a function call or a variable name or anything or a column name during parsing so by default mysql does not ignore this thing that is you should not have a space between a function name and the first parentheses so that function names can be repeated or function names need not be unique that is you can use the same function name for your column names or variable names similarly there are other sql modes for example you can say mode equal to db2 or mode equal to oracle which supports some kinds of parsing  which has some kinds of parsing implications that are compatible with the way ibm db2 works or the way oracle database work for example both of this modes have more or less the same implication for example you have to treat double pipes as a concatenation operator rather than a logical or operator and allow double quotes for identifier quotes and ignore space between function name and parentheses that is there is you can not use the same names between the function and a column name there also other sql modes  we shall not go into the details here but let us just enumerate what are all the different sql modes  refer slide time  23  31  you can set mode equal to maxdb  maxdb is the variant of mysql which is primarily oriented towards enterprise application so it ’ s a very  it ’ s a transactional  it supports mainly transactional semantics and you can build large enterprise applications around the maxdb and then you can set mode equal to mssql which is microsoft sql or postgres sql and mysql different version like 3.2.3 or mysql 40.0 and so on there are other sql extensions that mysql supports which are different from the ansi standard these include the following that are shown here  refer slide time  24  12   for example you can insert a comment c like comment  c like comment means one which starts with slash star and ends with star slash you can include a comment within a sql query  anywhere within a sql query similarly you can perform some kind of macro operations or pre-processing operations from which you can bring about selective execution of an sql query depending on which version of mysql server this query is being executed on for example look at this statement here  it says select and within comments it says exclamation mark or bank 3 2 3 distinct name from employee now what it means is that if my sql server or mysql server which is executing this query is of version 3.2.3  then you execute whatever is there within this comment otherwise comment out this  that is remove this commented part from the query  refer slide time  24  52  therefore if i am running it on let us say mysql server 4 then the outcome would be select name from employee and only in 3.2.3 it would say select distinct name from employee now why would one want to do something like that because there are certain kinds of nuances or certain kinds of changes in the sql semantics between different versions of mysql and you can write a single sql query that can work on any version of mysql by selectively masking or enabling certain parts of the query let us look at how data is organized on the disk in mysql mysql does not create its own file system  in fact it uses the file system whatever file system is provided by the operating system and each database is stored in a separate directory in an existing file system what are the implications of doing this  that is what are the implications of using an existing file system and the fact that you are using  you are keeping one directory per database  refer slide time  26  32  some of the implications are shown here that is the names of tables can be either case sensitive or case insensitive depending on the operating system for example if you are using a unix based system or a linux based system  it would be case sensitive so if you say name with n as capital  this would be different from name with all small letters  so they would be two different tables having these two different names however on an operating system like windows it is case insensitive  where both of them would point to the same table and tables can be renamed and dropped using operating system commands not necessarily with in mysql for example you can just say move or rename a given file and then the table actually gets renamed so as shown here to rename a particular table  you just have to rename files with the following extensions .myd  .myi and .frm for example if you have a table called employee you will see that and in a database called employees database  you will see that under the directory called employees database you will have employee.myd  employee.myi and employee.frm now suppose you want to change employee to something else say contractor whatever  you just have to change or you just have to move these files to some other files names using operating system commands and this change will be automatically reflected within the database itself what kinds of storage engines does mysql use and what is the storage engine in the first place  refer slide time  28  39  a storage engine essentially is the engine that determines how tables are organized within files and what is the updation policy and so on  refer slide time  29  33  and storage engines also differ with respect to their support for transaction semantics that is does it support atomic updates  does it support isolated updates  does it support recovery and so on and so forth the different storage engines that mysql supports  the primary storage engine which many of the earlier versions of mysql where shift with was the isam storage engine as we saw earlier it is called the myisam storage engine isam essentially stands for indexed sequential access mode and provides indexed sequential access and does not provide any transactional support there are also other kinds of storage engines like the heap storage engine which is used for managing tables that are in memory that is temporary tables or virtual tables and so on this is also called the memory storage engine there is an other kind of storage engine called merge storage engine which can treat several isam tables as a single merge table that is it can efficiently merge different tables that have been created using the myisam storage engine and in recent versions of mysql there are storage engines like innodb  bdb which provides support for transaction safe updates that means it provide support for acid semantics they are available in versions only later than 3.23.34a so to be very safe and also you should also be aware of the fact that in these versions or if this is the first version in which this storage engine was released then it would be available only in source code form and not in pre compile binaries  refer slide time  30  45  therefore if you want to use transaction semantics on mysql  it would make a lot of sense to download a later version something like mysql 4 or mysql 5 which is the latest as of today and the bdb storage engine is not supported on all operating platforms and so you have to ensure that for whatever platform that you are using  the bdb storage engine is actually supported there is also the ndb clusters storage engine where you can implement mysql over a computational cluster what is a computational cluster ? it is a collection of different machines on a local area network which can provide what is called as the single system interface that is which can provide the combined power or processing power of all the processors of all the machines now ndb clusters is a storage engine that exploits this capability of a cluster  so it can support tables that are actually physically partitioned over different machines in a cluster and of course ndb cluster is also very recent storage engine that is supported by mysql and it is available in versions 4.1.2 and later on and like we had noted above that in 4.1.2 it ’ s only available in source form that is you have to compile this engine yourself but in later versions  the binary forms or pre compiled versions are also available  refer slide time  32  34  what about transaction semantics in mysql ? transaction semantics have been supported in mysql only after version 3.23 max and above and so on but before that mysql actually supported what are called as atomic updates we will look into atomic updates shortly and what are the relative advantages and disadvantages of atomic updates versus acid semantics and so on but if you want specific acid semantics  suppose you are using your mysql engine for let us say performing some debit credit updates in any kind of whether banking or any kind of money transaction that is going on which has to be recorded on this it ’ s better to use mysql which supports the inno db or the bdb storage engines because they have support for full transaction semantics and other non-transactional storage engines like the earlier versions of mysql and so on support what are called as atomic updates or atomic operations and this can be simulated in a inno db for example by setting auto commit equal to 1 that is commit is automatic and it ’ s not explicitly stated in the sql query these non-transactional atomic updates are actually faster than the transactional storage engines and because of course you don ’ t have to worry about locking and isolation and other isolation based semantics nor do you have to worry about logging and so on where the first was locking and the second is logging and logging in order to support log based recovery mechanisms that is in order to be able to support durability aspect of acid semantics now because all of this are not required and only atomic updates are required  it ’ s much faster and in many cases this is all that is required that is suppose you know that there are no concurrent operation that are happening or suppose you are relatively sure of the uptime of your dbms then it makes much more sense to use non transactional storage engines which support only atomic updates because they are many times faster than acid  i mean storage engines which support acid updates and a very interesting feature of mysql is that you can use storage engines on a per table basis so you are actually save only for this table use inno db and for every other tables use let us say myisam and so on so that you use atomic updates and only for the critical table  you say that i want full acid semantics the next topic that we will see are stored procedures what are stored procedures ? stored procedures is essentially a set of sql statements that can be actually be stored on the server and some kinds of sql statements which are used pretty often in different queries so you don ’ t have to keep writing it as part of the query again and again and you can actually refer to these stored procedures as part of a given query there are two kinds of  two ways in which you can store sql statements as what are called as stored procedures and stored functions a procedure essentially performs a set of operations and may or may not return a value or may even return multiple values when it is called  refer slide time  36  11  but a function has a specific semantics that is it can be embedded within any sql statement and it returns back exactly one value as part of its execution so stored procedures support is a very recent phenomenon in mysql and it is supported in mysql versions 5.0 and above and the latest version is actually as of now is actually 5.0 and whatever later versions would still support stored procedures and a stored procedures as you know increases the performance in terms of communication complexity  especially if you are using a large number of stored procedures in your query  you can save on a large amount of communication complexity between the client and the server so you don ’ t have to send all the query from the client to the server for execution but the flip side of stored procedure is that if several different clients are connecting to the server and calling several different stored procedures then the sever gets a huge load in trying to execute all this different stored procedures and mysql also allows libraries or a function to be used as a stored procedure within the server that is these libraries need not actually be sql statements but they could be pre compiled libraries written in some other application specific language like c or c plus plus and stored procedures are created using the create procedure construct in sql and functions can be created using the create function constructs and procedures are invoked using the call constructs  you can say call procedure 1 or procedure 2 and so on  while functions can be embedded within any sql statement like we saw select average of whatever or select anything and so on  refer slide time  37  41  so you can actually embed function within an sql statement  however the function has to return one specific value so functions return a value while procedures need not  it ’ s not actually do not but it need not return a value in fact a procedure can actually return multiple values through the input parameters itself and a function is distinguished from a procedure by this returns statement or the returns clause so function says returns this thing  returns a particular value when you are declaring a stored function and a stored procedure or a function is associated with the specific database what are the implications of saying that a stored procedure is associated with a specific database ? see you can say for example i have an employee database and there is a particular kind of query that is invoked on this database for example what is the average  what is the average working hours per week in this month and so on now you can actually store that as a stored procedure in the employee ’ s database now let us say you are using some other database  let us say salaries database or some other completely different database itself not some other table but you can still use or invoke a stored procedure on the employee ’ s database note that in sql  in mysql now you can actually give an sql statement that spans over multiple databases  refer slide time  38  54  so when you invoke a stored procedure on a given database and implicit command called use database is called use database is basically something like connect to the database and start using tables in this database  refer slide time  41  31  and this use statement is terminated when the procedure terminates and the procedure now has access to all tables of the database within which it is written and of course you can call stored procedure of a specific database by prepending its name with a database name for example here you say call employeerecords.updatesalary where updatesalary is a procedure that performs updates on salary and employee records is the database within which this stored procedure is stored and another important implication of the fact that stored procedures are associated with the database is that when a database is dropped or it is deleted then all stored procedures and functions that are associated with it are also dropped  refer slide time  41  37  support for triggers triggers  as you know we covered a session on constraints and triggers triggers as you know are essentially special kind of stored procedures that is they are written using eca rules as we saw in one of the previous sessions that is they are triggered automatically when an event happens and a given conditions is true now support for triggers is not very comprehensive in mysql as of now at least that is there is only preliminary support for triggers that is available in mysql version 5.0 and triggers are essentially stored procedures and which are actually called automatically in response to certain events mysql also supports views  as we saw view can be created either as a virtual table or as the materialized view that is view can be stored as a query or can be actually physically materialized depending on what kind of application that you are using in transactional applications views are usually virtual tables whereas let us say in data warehousing and other analytical applications  views are actually materialized mysql supports views as virtual tables and they are supported from mysql version 5.0 and above  refer slide time  42  31  and they are treated as real tables for all practical purposes that means each table has what is called as an access privilege system that is which user has what privileges for a particular table for example you can say this user has a read privilege but no write privilege or this user has a read privilege write privilege but no drop privilege that is you can read and write to the table but he can not drop the table  he can not delete the table and so on and view updation is automatic because only virtual views or virtual tables are supported and views are created using create view command or there is also a variant of this which says create or replace view so if the or replace construct is present then a view would be updated if another view of the same name exists and how does mysql handle constraints and of course triggers and so on ? in mysql the notion of triggers or calling stored procedures is separate from that of handling constraints the kinds of constraints for example data specific constraints like say primary key constraint or unique constraint  if they say violation on the primary key constraint essentially that primary key has to be unique and it can ’ t be null  refer slide time  44  20  so if there is a violation then mysql will roll back  if a transactional storage engine is used that is it just performs the transaction rollback and if a non-transactional engine is used then it just stops execution on the offending row that is it does update the row and from there on it does not update any other row in the engine so therefore it is not an atomic operation that is it could have updated some rows prior to it but it will stop execution at that point in a non-transactional storage engine and many of this insert or update queries also support a keyword called ignore essentially that means that constraint violations are ignored similarly if there are violations in not null constructs and say default constructs  mysql automatically inserts the default value for example if you say that some attribute should be not null and the default value is say zero then whenever that attribute value is null then zero is inserted automatically now the reason that mysql does not stop in this case is that you will be able to catch such violations only after the query parsing stage has been finished and it would make it quite in efficient to stop execution at this point and get back to the user for a different query so mysql automatically  because it already has a default value it automatically inserts the default value and similarly for queries that try to insert invalid values  for example if it tries to insert string into a numerical field or some kind of a numerical overflow happens during insertion and so on mysql automatically inserts the best value that is zero for numbers or null string for strings and so on  refer slide time  45  32  and similarly in an enumerated data type  what is an enumerated data type where you say this data type can or this column can take only these set of values so one can say the column called gender can only take m or f or column called grade let us say  can only take values a b c or d and so on so if an invalid value is entered in to an enumerated type then it is automatically replaced by the first element in the enumerated type that is the element number zero let us see now how you can actually download and install mysql on a linux based environment and get it running now of course in order to download mysql  we already saw where to visit in the internet www.mysql.org and from that you can actually download mysql on to your machine now there are several different sources and binary versions of mysql that are available and the pre compiled binaries that are available for different linux distribution so there are pre compiled binaries for say suse linux or mandrax linux or red hat linux and dban and so on so forth  refer slide time  48  15  the easiest way of installation is to use an appropriate binary rpm package rpm essentially is the red hat package manager which is the default way in which the packages are managed across different linux distributions and binary versions of mysql are complied with static option which uses static libraries so it ’ s quite stable that is there is very little possibility or no possibility of linking violations or run time error occurring after the mysql server has started running  refer slide time  48  51  so when you install mysql on your system  you need to install at least mysql server and mysql client packages in order to get a complete functional dbms on your system that is you should have a both server part and a client part you should also install this package called mysql shared compat for backward compatibility especially if you are upgrading to a newer version of mysql mysql servers are by default installed in to the in to this directory var lib mysql  so after you install you can actually visit the directory and see what all it is installed and a new user called mysql is also created and which is the owner of the mysql demon or the mysql server that is running and it ’ s also possible to make mysql server start up automatically whenever you boot up your machine  refer slide time  50  03  let us see how we can do all of these things in order to install mysql on your machine  you have to use this rpm minus i command or the mysql or the install command which installs the server and the client and of the appropriate version number and after installation of mysql it is necessary to allocate certain areas where databases can be created and this is automatically perform by the mysql install db program and in some kinds of installation procedures  this is  this program is automatically called after installation so you don ’ t have to even worry about calling this program and mysql install db creates a directory structure under which databases are created where each database as you know is a directory in itself  refer slide time  50  40  and by default it creates two databases mysql and test the mysql database is the database of databases that is it ’ s a database holding information about other databases in this installation and the test database is a sample database that is provided to the user so let us look at how mysql database looks like once you start let us say  once you have run your mysql install db there is something called mysql show which will show the contents of a given database  refer slide time  51  16  so when you say mysql show mysql  you see that mysql database has several different tables columns underscore prev db func host tables underscore priv user and so on so  which holds different information ’ s about all the databases that are stored on this machine for example this holds user information which are all the valid users and their passwords and their privileges and so on and so forth and tables privileges  that is per table privilege  this table is authorized to which user for doing what and so on then there are host based privileges and functions that are stored as part of this database and db is information about the other databases that are stored on this machine and this is a column based privilege information  refer slide time  52  46  and you can  like we had noted earlier you can make mysql to start and stop automatically or start automatically on boot up  whenever you boot up your machine for that you need to go to /etc/rc.star where star is either dot 3 or dot 5 or whatever in which you start your computer by default and those files should be changed in order to start up mysql server by default and the mysql server can be stared at any time using this command mysql.server start and it can be stopped at any time from the shell by mysql.server and then stop  refer slide time  53  22  there are other options for mysql server that can be added in a global configuration file and this is available in /etc/my.cnf and this is the typical global configuration file which is saying which is the database directory  which is the socket under which mysql is connected  which is the port on which the mysql is listening and who is the user owning the mysql and so on  refer slide time  53  41  and the mysql client is the client program using which you connect on to the mysql server so mysql client can be started by mysql minus h host minus u user that is connect to the server on this host using this user id and then it ask for a password and then you get in to the mysql prompt  refer slide time  54  17  and sql commands can be issued from the mysql prompt and it works within a user session that is you can say use particular database which is the same as connect to a database in several other dbms system and then you can start issuing sql commands and you can quit the client using the quit command  refer slide time  54  35  in addition to sql using the mysql client itself  mysql provides support for several application programming interfaces that is you can embed your mysql client within another application program like c  c plus plus  perl  java and so on and so forth let us have a brief look at what kinds of c apis that does mysql provides in order to use the c api in your c program  you have to include the mysql client library and once you include the mysql client library there are several data structures that are available to your program  refer slide time  54  53  for example there is a data structure called mysql which is a structure representing  which is the struct  structure is essentially the struct data type in c that represents a handle to a db connection or to a database connection that you have presently opened similarly mysql underscore res represents the result of a query and so on and so forth so there are several different such data structures which you can access similarly there are several different functions that you can also use as part of your program for example you can say mysql underscore init which gets or initializes the mysql structure that is it obtains the handle to a data base connection similarly you can say mysql real connect which actually connects to the server and you can issue a query using mysql underscore query and once you get the query results  you can say mysql fetch row which fetches the results of this query in a row by row fashion  so the first row and the next row and so on so forth  refer slide time  55  47  so using this you can actually embed your sql semantics or database semantics in to your larger application program itself  refer slide time  56  41  so let us summarize what we have learnt about mysql today so it ’ s a very interesting database  it ’ s a very popular open source dbms that is the source code is available to you and as a result mysql has or changes to mysql has been contributed by several people across the world in addition to the mysql ab people  who are in mysql ab and it provides several different storage structures and its scalable to number of clients and data sizes and it also has transaction support and limited support for triggers and stored procedures and best of all  there are several different  it has been ported on several different platforms you have mysql for window  mac and linux and so on and it has several different apis for application programming or embedding application programs in to mysql so that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 33 case study oracle & microsoft access hello and welcome in this lecture today let us look at one more case study of a commercial dbms this time  refer slide time  01  19  in fact we are going to look at case studies  the well-known oracle dbms and the microsoft access dbms which is shipped with microsoft office suite of a product whenever you buy them and in the previous session  we talked about mysql which is an open source or free software while the once that we are talking about or that we are reviewing here are both commercial software that are proprietary and you have to buy it off and which can be bought off the self and so on  refer slide time  02  06  now let us have a look at the main structures of this  firstly the oracle dbms in this lecture we are not looking at any specific version of oracle oracle is  the dbms oracle is manufactured by oracle corporation headed by of course larry wall and who has been a pretty early player in the whole field of commercial database management systems and there have been several versions of oracle that have been evolved over the years and the latest version at least as of today is oracle 10g  the g here stands for data grids where the database can sense that it is part of a larger data grid and operate accordingly that is a grid is a collection of different databases which could be distributed widely probably across the world and while functioning as a single data source or a database and it ’ s been touted or it ’ s been kind of targeted towards the data warehousing and business intelligence kinds of applications what are the basic elements of or the building blocks of an oracle database ? when we are talking about oracle databases  there are some terminologies that we need to be aware of many of these terminologies as we noted earlier  oracle has been an early player in the field of commercial dbms systems and while terminologies have been invented and evolved over time in the academic and research community for database management systems similar kinds of terminologies and evolutions have happened in the commercial field as well  refer slide time  03  18  so sometimes it may so happen that the terminology used  that we have used for example in this class might differ from the terminology used in oracle or let ’ s say db 2 or some other commercial database engine and so it makes a lot of sense to clear what each of these terminology mean or what kinds of terminology mismatches can we have between  what we have been using and what oracle for example uses when we talk about an oracle server in the database management system  the oracle server is actually the dbms server that we know very well that is it ’ s the dbms engine that serves one or more clients and when you talk about the oracle database  it is more than just the set of data files that form the database but oracle considers its database as a collection of all its stored data including log files and control files and other such auxiliary files that make up the database and usually in the academic community when you say database  we usually talk about only the data files and the index files  when we talk about for example the storage structure then when we say an oracle instance  this refers to the set of processes that is the set of all processes including oracle system processes which with oracle server runs and user processes created for a specific instance of the database operation what kinds of languages does oracle support ? oracle of course supports the ansi sql standard with some extensions of itself that was specific to oracle  refer slide time  05  43  and in addition to the well-known ansi sql  oracle has its own query language called pl/sql pl/sql is a procedural query language  on top of sql that is say for example sql language is in some sense a pure query language in the sense that it does not have other programmatic constructs like say using variables or well it has certain  you can use certain kinds of variables by aliasing and so on but generally it doesn ’ t have the generic structure of any procedural language like c or perl or some other kinds of language pl/sql on the other hand is a complete procedural language where you can actually define variables  you can define control flow  if then else constructs and looping constructs and so on and so forth and embed sql statement within these procedural constructs when we talk about the database structure in oracle  we usually refer to two different kinds of structures what are called as the physical structure and the logical structure of the dbms the physical structure refers to the storage structure that is a physical organization of the data on the database while the logical structure corresponds to what is a conceptual schema of the database that oracle is maintaining  i mean logical structure here is the logical structure of the way oracle manages data not that of a particular database itself  refer slide time  07  13  so the way oracle manages data itself can be treated as a database which comprises of several other databases and which comprises of a larger logical structure and update rules and associations and so on and so forth let us look at the physical structure in a little bit more detail we shall also briefly look at the logical structure as well when it comes to the physical structure of the database  the way oracle stores data on disk  it basically stores data in several different files on disk and very unlike let ’ s say mysql oracle uses its own buffer management policies for writing data into files what is meant by buffer management policies for writing data ? dbms like mysql uses for example  if you are taking a unix based implementation or linux based implementation of and we are comparing mysql and oracle  mysql uses what are called as high level system calls for writing data into disk that means high level system call essentially says  gives a block of data to the operating system kernel and lets the kernel take care of writing the data on to the disk by itself now what the kernel actually does is in order to speed up the process  it doesn ’ t immediately write your data block onto disk instead it keeps it in its own memory  in its own cache that is in main memory and then flushes it on to the disk at some point in time this cache is called the buffer cache we saw buffer cache and how buffer caches are come into play or affect dbms operations when we are talking about database recovery processes  refer slide time  08  08  so when we write something onto disk and we use a high level system call  there is no guarantee that whatever you have written on to  whatever you have written has actually been recorded on to disk now if there is suddenly a system failure then even though our write has returned us a successful operation and we said we have committed some transaction  it may not actually have been reflected on to disk on the other hand oracle uses low level system calls that means oracle manages its own buffer cache and oracle decides for itself when it has to flush the buffer cache and when it has to write to files and so on however it still uses files that are visible from the file system interface of the operating system so let us look at the files that oracle uses so when we are looking at the files here that oracle uses  we have to keep in mind that the way that files are updated in oracle is very different from the way that files are updated in say mysql so these are some of the files  refer slide time  10  45  that oracle uses for maintaining its databases firstly the data files of course the data files are the files that contain the actual data and in addition to the data files  a database is associated with a set of redo log files remember what is a redo log file  redo log files are the set of log files that are used for redo based recovery we looked at a set of log based recovery mechanisms where you first write data on to the log file and then after flushing the log file  you write data on to the actual database now whenever there is a system crash  you just have to redo based starting from the last check point in your log file all the operations that have been performed until the system crash happens so oracle does something like that  so it maintains the set of redo log files as part of database itself then there are a set of control files which contain different kinds of control information like the database name or the different file names and the locations of this different files and so on these control files are also used in the process of recovery  sometimes they are useful in the process of recovery and in addition to control files there are other files like what are called as trace files and alert logs trace files are essentially files that track certain background processes that oracle runs oracle basically runs several different background processes that for example  the server is a background processes and there are several system monitors that  oracle runs to monitor events and call triggers or enable triggers and so on and so forth now trace files essentially track this background files and logs them on to files and alert logs maintains a log of the major events that have happened from these background processes and from anywhere else in the system the log files and control files as you can see are the files that are essential for the process of database recovery therefore oracle also supports mechanism by which log files and control files can be multiplexed onto different devices  refer slide time  13  15  so when you write a log file  you can configure oracle so that it actually writes it in parallel to multiple devices the advantages of this is even if there is  in a system crash even if there is let us say a media failure and the log file itself is lost  we can use these backup copies of the log files from actual devices  from different devices to recover back so the resilience of the recovery process is increased as part of this so let us look at the logical database organization now that is how is database organized in oracle  what are the building blocks that make up a database in a logical sense and what are the different roles of these different building blocks and so on when we are talking about an oracle database  in the physical sense there are only files  everything is a file and file is managed by low level system call operation but from a logical sense  each of these files means certain different things and they are looked at as objects rather than files now what are the different kinds of objects that an oracle database contains  there are what are called as schema objects a schema object contains definitions of some relevant entity in the database for example it might contain definitions of tables  it can contain definitions of views or sequences or stored procedures  indexes  clusters or links to other database and so on  refer slide time  14  09  so everything is treated as an object here and the object is essentially in a sense serialized and stored on to disk so an object for using the object orientation methodology comes with its own set of procedures or methods which handles or which provides a set of services based on the objects and definitions so when we say a schema object represents a table  it also provides methods by which we can access or perform different kinds of table accesses  access a table row wise  access or perform any perform any index search on table and so on and in addition to the schema object  there is what is called as a data dictionary which is essentially the system catalog and it maintains different kinds of cataloging information for oracle so it contains information like user names and security information that is access privileges and what kinds of users or what kinds of privileges on which schema object and information about schema objects themselves  the modification date and the creation date and any other relevant information for any given schema object then integrity constraints which in turn trigger all the stored procedures or which in turn trigger certain schema objects which are stored procedures and then certain kinds of statistics essentially for example that help in performing good query execution plans we saw the role of cataloging information when we were looking into how do we manage or how do we process a query and optimize a query cataloging information contain certain  i mean the statistics here in the cataloging information contain certain estimates on let us say how many rows are there in a table  how many number of distinct values of are there for a particular column and which rows is accessed how many number of times or whatever all of these information go into performing a or forming a good query execution plan while making the query and then there are audit trails that is auditing information about the different aspects of the database  refer slide time  17  46  the next aspect of the logical schema or logical organization of the oracle databases is what is termed as the table space a table space is again an important concept in oracle which basically describes the physical storage structures of a given set of tables in a database and it basically governs how the physical space of a given database is used we look into how the tables space is organized  they basically contain different segments and which in turn manages the pages or the disk blocks or that make up a particular database so earlier we mentioned the concept of an oracle instance an oracle instance like we said has a specific definition that is in some sense it is a snapshot of the entire oracle system that is it is the set comprising of all processes that is oracle processes and user processes and all of which comprises one instance of a servers operation  refer slide time  18  41  now an instance itself can be logically seen as comprising of several different parts so an instance is logically divided into what is called as a system global area what is the system global area ? the system global area is essentially the area which in some sense to give an analogy to operating systems  it is the memory area that is used by a kernel for example in contrast to that of areas used by user processes that is all the resources and memory locations that are used by the kernel is a system wide and globally relevant so the system global area is in some sense the kernel of the oracle system it contains the database buffer cache which we saw is maintained by oracle itself in order to flush buffers on to disk and in order to basically control when buffers are flushed on to disk then there is the redo log buffer which is the buffer cache for the redo log so whenever a redo log is returned  it is actually first return into the redo log buffer and which in turn is flushed on to the disk at periodic intervals then there is shared pool of other resources which we will see then in addition to the system global area  when we talk about an oracle instance we also talk about the user processes an oracle instance again is like i said is something like a snapshot of the entire system so you might also take an analogy to operating systems where we say an instance of the operating system is the set of all processes comprising of the kernel processes and all user processes at any given instance of time in addition to user processes there is what is called as a program global area in contrast to the system global area  refer slide time  21  23  a program global area is a memory buffer that contains a data and control information for server processes that is where  this is the global area again there is no specific analogy as such to operating systems here but you can think of it as a global area which is code specific that is for the data and control information for the global processes that are happening in the database in addition there are certain system processes itself which are the oracle processes so oracle itself runs several processes in the background which comprises the server instance or the oracle instance at any given point in time so oracle processes may comprise of the server process itself or the server process is depending upon whether it is single threaded server or its concurrent server where it can actually serve several different clients concurrently and then there are other background processes like audit trails and system monitors and so on which belong to oracle so here is a schematic diagram of the system global area of course this of an oracle instance rather and of course this diagram is in some sense leaves out a lot of things that is for example the program global area is not explicitly shown here and the system processes is not explicitly shown but generally all of this form the oracle instance  refer slide time  22  54  so the system global area which comprises of the buffer cache and the redo log buffer they interact directly with the database files and user processes and of course the server processes they interact with the database actually through the sgr or through this system global area so there are user processes which in turn which interact with the oracle server processes and oracle processes which could be server processes and possibly other processes and these in turn rather than interacting directly with the database file  to be more correct this diagram should be  the arrow here should come through this thing that is it interacts with the database or with a disk through the system global area on the other hand if we are talking about database files  this logical arrow is correct which says that logically the server processes deal with the database files so let us look at oracle processes again in little bit more detail oracle processes can be categorized into different forms namely server processes server processes are those which actually handle request from user processes or oracle client or other application programs and so on which send sql queries to the server processes  refer slide time  24  24  there are two kinds of ways in which you can configure your oracle server  you can either configure it as a dedicated server or as a multithreaded server and dedicated server essentially is a server that is dedicated to a particular client  so it can take handle one client at a time whereas a multithreaded server is concurrent in the sense that it can handle many client connections at the same time by spawning different threads or processes if your kernel doesn ’ t support threads and so on in addition to server processes there are what are called as background processes and there are different functionalities for each of these background processes and they are created for each instance of oracle and one of the main functionalities of these background processes are to perform these asynchronous i/o that is there this process is that mediate between the buffer cache in the system global area and the physical disk so these processes essentially perform what are called as read ahead or delayed write operation so when you write on to a disk  your data goes into the buffer cache and in some point in time the background processes wakes up and asynchronous late at some point in time the background processes wakes up and then flushes all the buffer cache onto disk similarly when you give a read command  your data is read the relevant data blocks are read from the disk but at the same time a background process is awakened which in turn reads several other data blocks also from the disk into the buffer cache  again for performance considerations so the main functionality or the main requirement of the background process is to provide parallelism for better performance and reliability  refer slide time  27  25  so there are different kinds of background processes which take on different roles that perform these asynchronous operations and usually it is good to distinguish between these kinds of background process for example the database writer process  so this is the process that writes the buffer cache of the data blocks from the buffer cache to the data files on to the disk then there is the log writer background process which mediates between the log or the redo log buffer cache and the log files on the disk so data processes are managed separately from the log flushing process and then there is the check point background process a check point as we saw in sessions on recovery based techniques  this basically refers to a event in which all modified buffers are returned to the data files that is any log data or any data about transactions that have been committed before the check points can now be safely discarded so we saw what are check points so this check points as we saw in the session on database recovery have to be taken at some points in time and there is a tradeoff between the speed or the overhead introduced by the check pointing process versus how much background data or how much historical data that you need to store in order to perform a recovery in the phase of failures so this check point background process is a process that runs once in a while to perform this check pointing operation and as we saw in our sessions on database recovery  check pointing is a costly operation because you need to bring the database onto a quiescent state when you are performing a check point but there are other techniques by which  there are other check pointing techniques which can obviate this problem so that you don ’ t have to stop all database operation when taking a check point  refer slide time  28  55  then there are other process like the system monitor which performs recovery of an instance that is it identifies that when the system comes up  it identifies that the system had crashed and performs redo based log recovery then it manages storage areas and it also manages any kinds of recovers transactions that were skipped during the recovery process then there is the process monitor background process  it performs a recovery operations whenever a user process fails note that failure of a user process can also leave the database in an inconsistent state especially when it is in the midst of the transaction and especially even more when transaction data return directly onto disk so if the user process fails to complete the transaction then the database is still in the inconsistent state and then you need to recover but then this is different from a system monitor in the sense that the entire system has not crashed  so other transactions are running  so this recovery process has to be handled separately there is also a process called the archival process which archives the online log files on to archival storage especially  essentially for back up purposes we also saw some mechanisms of archival when we are talking about the database recovery techniques then there are recoverer processes which essentially are useful when oracle is used in a distributed database setting  we have not as yet looked into issues pertaining to the distributed databases but there are several issues when database is distributed across several different clusters especially to detect that a particular transaction that is spanning across different machines has actually crashed or it is spending or something of that sort  refer slide time  31  38  then there are other processes like dispatchers and lock processes where dispatchers in multithreaded configurations  they route request from user processes to the appropriate or the available server processes and then there are locking process which are in some sense again monitors that monitors the locks across different instances when oracle runs in parallel server mode  refer slide time  31  47  now let us look at sql in oracle what kinds of sql statements are handled ? as we noted earlier oracle supports ansi sql in addition to ansi sql  it has its own extension to sql namely the pl/sql which helps the user to embed sql statement into procedural language but as far as the support for sql itself is concerned  oracle supports all kinds of standards data definition statements and data manipulation statements and it supports transactional semantics  full acid semantics then other kinds of constructs like session control semantics and even embedded sql statements in addition to support for sql  oracle comes with an elaborate mechanism for methods and triggers and these utilities where added to oracle as part of its object relational extension so oracle essentially is an object relational database where we have not again as yet seen in to what constitutes an object relational database but essentially a database that uses object oriented semantics like method calls  triggers and triggers are also some kinds of method calls and inheritances  associations and so on so a method is actually a function that is part of the definition of a user defined data type and its slightly different from the stored procedure in the sense that program essentially that ’ s a user program invokes the method by referring to an object of its associated type  refer slide time  34  03  but stored procedures are actually called from sql statements independent of any particular object as such it ’ s only the database whose context is necessary for stored procedures and oracle methods have access to attributes of their associated objects and information about their types so a method for example  an oracle method for example would be a method in one of the schema object let us say schema object represents a table a method would be some code which says how do we access the next row in a table or how do we get the primary key field of a table and so on so get primary key or get next row or query or something of that sort which abstracts away the implementation or the storage structure of the table from the database engine as well so this is different slightly different from the stored procedure concept because stored procedures are visible to user programs or sql constructs while methods are essentially  i mean the methods are visible to user programs only in an object relational context and not in a pure relational context and triggers are those methods which have active rule capability that is which are called automatically in response to events and conditions let us have some more deeper look into the storage organization of the oracle database we noted earlier that storage is managed by what is called as the table space now table space is the space that manages the physical organization or physical allocation of memory or disk blocks for a given database  refer slide time  35  33  so a database is divided into several different table spaces and among these we can distinguish between what are called as system table space and the user table spaces a system table space maintains essentially all data that are visible only to the oracle system while the user table space maintains data that are accessible by user programs and data files are stored or managed by the table space itself every oracle database contains a unique system table space and this is labeled as system in caps here and the data dictionary objects and other system wide or information are stored in this system table space or managed by this system table space now the physical storage itself i mean how does the table space manages the physical storage the physical storage itself is divided into three different kinds of storage what are called as data block or and extents and segments a data block corresponds to what is called as a page in or data page in several other database systems it is a smallest level of granularity in which data is stored  refer slide time  37  22  data blocks usually have a one to one correspondence with a disk blocks or the granularity with which your disk accepts data most of the or at least all of the disks accept data in terms of one sector size of the disk but there are some disks with enhanced capability that can actually read and write multiple contiguous sectors at the same time now this multiple contiguous sector form one data block and then extent  the term extents is used to refer to a specific number of contiguous data blocks  so where data block is the minimum unit of storage now a set of segments that are allocated to a specific data structure like a table or an index or something like that is called as a segment so data blocks again each of these  let ’ s look at each of these different elements in a little bit more detail a data block whenever a particular data block is stored  it contains the following information it contains the some header and table directory information which talks about which table it belongs to and pointers to other tables and so on and there is a row directory information which talks about which are the specific rows that are stored in this data block and the data that pertains to a row and any free space information  if there is any free space information  if there is any free space left out in these data blocks  refer slide time  38  34  and an extent as we said earlier is a set of contiguous data blocks so when whenever you create a table  oracle allocates an extent to it and an extent is extended in a sense that is an extent is  more extents are added to the table as and when extents become full  refer slide time  39  10  and these extents are managed by segment that is we saw that a segment is a set of different extents and extents can be allocated for any given data structure like tables or indexes and so on so if i allocate an extent to an index  the extent remains allocated as long as the index exists and is automatically freed when the index is deleted and a collection of extent is what is called as the segments so there are different kinds of segments  what might be termed as data segments and index segments and data segments are those which store data or which handle elements that are stored in the data files  refer slide time  40  04  and data segments belongs to each what is called as a non-cluster table and to each cluster what is a non-cluster table ? as we noted earlier  a cluster is a collection of machines on a lan which gives us the single system interface now a cluster table is something that is spread across the cluster that is a part of my table is in one machine and part of it is in another machine and so on and so forth now for every such part in a given machine  a given data segment is allocated and index segments are allocated for every index structures that ’ s been created on these tables  so every create index command will allocate a index segment there are also other segments called as temporary segments which are used as temporary work areas especially to store intermediate results or to allocate data for virtual tables or results of an intermediate query and so on  refer slide time  41  17  and then there are rollback segments that are used for undoing transactions so like mysql or any other dbms  oracle servers are usually expose several different apis  so that user processes or application programs can directly talk to the oracle dbms  refer slide time  41  53  so the sql support or embedded sql support is provided for different languages like cobol  c  pascal and so on and in addition oracle has its own procedural language called the pl/sql  refer slide time  42  47  let us have a brief look at how pl/sql looks like and what kinds of operations does it provide over and above plane vennila sql pl/sql as we noted earlier provides procedural constructs within which sql constructs can be embedded a pl/sql block can be divided into three different parts what are called as the declaration part  executable part and the exception part as you know  if you notice correctly this is very similar to how a typical procedural language is also ordered except for this exception part and out of these three parts it ’ s only the executable part that is mandatory so the declaration part is where you declare variables or and any other characteristics or functions that pl/sql uses and the set of statements or the set of executions is embedded within a begin and a end construct here where you have the set of statements as well as the exception handlers that is the executable part as well as the exception part written within this block that is within this begin and end construct  refer slide time  43  23  so the declaration part like we noted is used to declare variables and this could be variables that are used in both sql and pl/sql data types and the sql and the executable part contain both sql commands and pl/sql constructs like if then else or for loop and while loop and so on  refer slide time  43  41  so you can say if some condition then send this sql statement  else send this other sql statement and so on and the exception part handles error conditions whenever an exception is raised and errors could be either system errors that are flagged by the database itself or they could be user defined errors that are flagged by the violation of for example some integrity constraint  refer slide time  44  54  so let us summarize what we have seen in oracle before having a brief look at ms access as well oracle is a very popular commercial database management systems it ’ s a commercial one and it ’ s in contrast to the mysql which is an open source dbms and the latest version 10g supports data grids as part of oracle and oracle actually supports not just relational  it also supports object relational databases  spatial databases and database of sequences and so on so different kinds of data can be stored in oracle rather than the pure relational database and it supports procedural constructs over sql in the form of pl/sql where you can write control flow constructs if then else and while loop and for loop and so on and oracle supports transactions and log based recovery  it also supports clustering where you can have a data file that is clustered over or a table that is distributed over several machines in a cluster and several other features let us now briefly look into the second commercial database whose case study that we are seen which is the microsoft access microsoft access we have we have included microsoft access as part of this case study mainly because it ’ s a commercial database which is kind of tailor made for ease of use for a non-technical user and where everything can be performed using graphical interfaces so the set up time and the learning curve for this database is much smaller than any other commercial database like oracle or other database like db 2 and even probably mysql so microsoft access provides the database engine and a gui based that ’ s the graphical user interface based mechanism for almost anything that is for data definition  data manipulation  queries and reporting and almost anything to do with a database can be performed using graphical user interfaces  refer slide time  47  18  but it ’ s not that  it ’ s only through the gui that ms access can be accessed you can also access the database using basic that is visual basic and several other macros that you can write to form of your own procedures and so on and one feature of ms access is it provides hyperlinks that is url as a native data type which is generally not supported by other databases where we can treat a url as a data type by itself and not as a string so what are the different components of the microsoft access database it ’ s called the engine of the of the microsoft access database all data for a given application are stored in a single file which has a suffix called dot mdb but even then  you can access these mdb files through different ways using the open database connectivity mechanisms we have not talked about odbc as yet but generally odbc is essentially is a common interface by which you can access several different databases as though they were data sources using a single common interface and ms access provides the support for data validation and concurrency control using logs but not full acid semantics and it also provides some amount of query optimization what kinds of guis are available ? we have some screen shorts which we will also see of these guis later on  refer slide time  47  47  there are guis that are available for specifying the structure of a table  formatting of the of the field layout  any kinds of masks which talks about invalid inputs and validation rules  default values  data types  index structures and anything to do with database design can be specified using graphical user interfaces  refer slide time  49  07  you can also use guis for defining foreign key relationships and you can also enable what is called as an automatic inference of relationships that is whenever we use the primary key field of one table as an attribute of another table  there is automatically ms access would infer that there is a foreign key relationship  refer slide time  49  43  so it also provides this facility of an automatic inference of relationships between different fields then queries can be graphically posed through a what is called as a query by example interface that is you can formulate or you can graphically show how your query should look like  that is i should have these tables and these tables and these tables with these associations and so on as part of my query result and then say now give me the query  refer slide time  50  19  so there is a qbe interface or query by example interface and as and when you build your query as part of the qbe interface  programmatically there is an sql statement that is being created to which also you can switch to and change your query whenever you can and you can perform joins by drag and drop operations between tables and you can form addition or and deletion of attributes to tables using drag and drop then there are what are called as expression builders where in which you can specify that the constructs of the where clause of your sql query  refer slide time  51  50  reports generation again can be performed in a gui fashion that is it ’ s an integral part of the access database the access database doesn ’ t just return a query and leave it at that  you can actually specify how the return data should look like in the form of a report  let us say in a page where different elements of your query go into different parts of the page so it becomes a reporting mechanism and the reporting  whatever reporting mechanisms that you have created are tightly bound to the underlying database tables and queries so there are different report generation wizards which help you in showing how a database report can be created and different styles in which report can be created so many other additional features what are called as cross tab queries which wherein you can perform group by on specific values within a column and aggregating within the group that you perform the group by on and the tables that you have generated or your database are available as ole objects ole is the well-known object linking and embedding mechanism that are used across different microsoft applications  therefore for example you can actually use your database table as an embedded table in a word document for example or in a power point presentation and so on  refer slide time  52  08  and there is a user level security based on login and password that is based around the nt server model and it also performs multi user  it also supports multi user operations and concurrent clients and so on so to summarize what we learnt on ms access  ms access is a dbms system that is meant primarily for the non-technical end user where the learning curve is much smaller than in using say oracle or mysql or any other larger commercial database system it is shipped with the ms office with  so whenever you buy the ms office suite of packages ms access usually comes shipped with it and the main feature of ms access is the gui or the graphical user interface which can help you perform almost any database operation whether it is specifying your database or formulating queries or reporting or any other kind of database operations so it gives support for graphical query specification and also report generation and these databases are available as dsns or data source names through an odbc interface that is any database client that is compatible with odbc can access ms access databases as easily as it can access other odbc comply databases oracle also is also odbc comply and several most widely available databases are odbc comply  refer slide time  53  02  and these tables or database objects in ms access are available as ole objects that can be linked or embedded within other ms office or within other microsoft applications like say ms word or excel or power point or so on  refer slide time  55  27  so let us have a look at some of these screen shots here that show how a typical ms access or working with ms access looks like so when you start a database  this is what you get that is you can start a database where you can specify your database design using different views  there what is called as a design view or you can create tables using a wizard where it will automatically fill up certain well known fields in a table or suggest you for certain structures for your database or you can even create tables by actually entering data using a form  refer slide time  56  03  this screen shot here shows an actual form by which data can be populated so here there are two different forms for two different tables there is table called mailing list and there is a table called assets and these are the different fields that go into this list so you basically fill up this form and the data is actually stored on to the database  refer slide time  56  21  the last screen shot here shows how you can graphically build a query using the qbe interface or the query by interface the window here essentially says that or the user who is formulating the query has said that the result of this query should contain these 4 different tables  out of these 4 different tables these are the fields that are required and the user can specify and basically there is a show which can be clicked or unclicked which will say what to show in the table and the criteria can be specified here which will say how to formulate the query so that is the beauty of this ms access databases so with that we come to the end of the second case study database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 34 data mining and knowledge discovery hello and welcome in this session today we are going to look at very interesting aspect of or interesting application in which database technologies are used namely the field of data mining and knowledge discovery in fact in recent years data mining has become an extremely or fields that eliciting an extremely large amount of interest not just from researchers but also from commercial domain i mean the commercial utility of data mining is probably of more interest than or at least as much interest as the research interest that lies in data mining  refer slide time  01  25  and in addition to commercial interest  there is also number of public debates that data mining has started which range from topics like legalities and ethics and the rights to certain information and the rights to non-disclosure of information or the rights to privacy and so on and so forth so data mining actually is in some sense has opened a pan door as box in and only time will tell whether the technology has given  has been on an overall sense completely beneficial or destructive in nature but then there is nothing beneficial or destructive about technology per say it ’ s how we use it  how we use technology which is what matters so any way in this session  we shall be concentrating mostly on the technical aspects of data mining obviously  refer slide time  03  59  and we shall look at the basic algorithms and concepts that make up data mining and what exactly is meant by data mining and how does it differ from the traditional operations of databases or traditional way in which databases are used so the overview of this or this set of two sessions would be as follows let us first motivate the need for data mining that is why data mining and what are some of the basic underlying concepts in data mining  what are the building blocks of data mining concepts then we look at data mining algorithms and several classes of this data mining algorithms we will start with tabular mining as in mining relational tables and we will look at classification and clustering approaches and we will also look at mining of other kinds of data like sequence data mining or mining of streaming data and so on and data warehousing concepts would be covered as a different session all together first of all  why data mining from a managerial perspective let ’ s first look at what a data mining has for the commercial world first before we go in to looking at the technical aspects of data mining  refer slide time  04  33  if you were to let us say give an internet search or talk to a manager  let us say about why he or she would invest in data mining  you would encounter a variety of answers one would say something like strategic decision making that is i look for some kinds of some ways or some patterns in or mind for certain nuggets of knowledge to understand something about strategic decision making or to help in strategic decision making somebody would say well it is very useful for something called wealth generation although there is no precise definition of the term wealth generation and you would say that data mining would help me in understanding or making the right decisions that can help me increase my financial portfolio or whatever somebody would say well i would use data mining for analyzing trends  analyzing how my customers behave or analyzing how particular market is behaving and so on and so forth and more recently data mining has been used extensively for security purposes especially mining network logs or network streaming data in order to look for abnormal behavioral patterns or patterns that might be potentially linked to abnormal activity in the network or in the system and so on so  security is now relatively recent and very important application area of data mining so  what is this data mining all about and why is this so controversial and why is it so interesting from a technical perspective at the same time data mining is the generic term used to look for hidden patterns in data or hidden pattern and trends in data that are not immediately apparent by just summarizing the data so if i want to look for certain patterns  let us say if i have set of all students and their grades if i want to look for certain patterns on how are the students performing over time or what is the is there some kind of relation between subject a and subject b i mean if a student does well in subject a  he or she does badly in subject b or so on and so forth  refer slide time  06  33  such things can not be discovered by just aggregating the data  by just saying what is the average or what is the summation or whatever besides  such things also can not be discovered by  i mean such things in a sense can not be within quotes discovered if we have to give queries that finds out these aggregations that is if we already knew what it is that we are looking for then it ’ s not a hidden pattern any more we know that such a pattern exists that is students performing in subject a will not perform well in subject b  we know that such a correlation exists and there is nothing hidden in the pattern anyway so data mining essentially has no query that is if you are performing a data mining on a on a database  we do not talk of any data mining query in fact it is the mining algorithm that should give us something which we don ’ t know now how do we say something which we don ’ t know  which is putting it in a very broad sense i mean which is making things so vague so data mining is actually controlled by what are called as interestingness criteria and we just specify to the database that this is what we understand by an interesting pattern let us say correlation between performances in subject a and subject b or some kinds of trends over a period of time this is what is interesting for us now find me something or find me everything which i don ’ t know about or which are interesting according to this criteria  refer slide time  09  24  so when we talk about data mining  we have a set of data to begin with that is we have a database and then we give one or more interestingness criteria and the output of which will be one or more hidden patterns which we didn ’ t know exists in the first place now given this model  we should say now when we say patterns then the obvious question to ask is what type of patterns  what do you mean by patterns or what do you mean that this is or when do you say that something is a pattern and something is not a pattern if we have to answer that we have to ask two further questions that is what is the type of data that we are looking at  what kind of data set is it that we are looking at and what is the type of interestingness criteria that we are looking what do we mean by interestingness  is it correlation between something  what exactly do we mean by interestingness so let us look at the different type of data that we encounter in different situations the most common kind of data is the tabular data or the relational database which is in the form of set of tables or now slightly different multi-dimensional form of database and it ’ s very common that any kind of transaction data that is let us say data array coming out from the database from an atm for example or the data coming out from the transactional database at a railway reservation counter or at a bank or any place like that are all tabular in nature so it ’ s a most common form of data and which is a rich source of data to be in mine  refer slide time  10  16  in addition to tabular data  there are spatial data for example where data is represented in the form of either points or regions which have been encoded with certain coordinates x y z coordinates so each point in addition to having certain attributes also has certain coordinates and mining in this context also requires us to know what is the importance of the coordinates system in addition to spatial data there are other kinds of data like say temporal data  temporal data in the sense that were each data element has a time tag associated with it so temporal data could be for example streaming data where network traffic or set of all packets that are flowing through a network forms streaming data which just flows fast and where each packet can be allocated some kind of a time stamp or something like activity logs  your database activity log is a temporal data there could be also be spatio temporal data that is data that are tagged both by time and coordinates and other kinds of data like tree data which for example xml databases or graph data where especially bio molecular data or volvoid web is a big graph data and so on then there are sequence data like data about genes and dnas and so on and again activity  i mean sequence is a kind of temporal data where timestamp need not be explicit in sequence then text data  the arbitrary text or multimedia and so on and so forth so  the several different kinds of data that can be the source from which we can extract or mine for unknown nuggets of knowledge  refer slide time  13  24  similarly when we talk about interestingness criteria  several things could be interesting if certain pattern of events or certain patterns of data keep occurring frequently then it might be of interest to us  something that happens very frequently so frequency by itself is an interestingness criteria or interestingness or a criteria on which interestingness can be based similarly rarity  if something happens very rarely and we don ’ t know about it or let us say rarity is again a very interesting pattern to be searched for when we are looking at say abnormal behavior of any system or abnormal behavior of network traffic and so on so something that happens rarely that is away from the norm is again an interestingness pattern correlation between two or more elements and if the correlation being more than a threshold is again interesting or length of occurrence in the case of sequence or temporal data and so on and consistent occurrence  consistency that is consistency is different from frequency in the sense that overall in the set of all databases  overall for the entire database a given pattern may not be frequent enough for example there could be one particular behavior pattern  let us say one particular customer comes to a bank every month at the tenth of each month so if we are looking for frequently banking customers  let us say this customer would not figure out in this algorithm because this customer comes only once a month whereas other customers could be coming many times a month however if we are looking for consistency in behavior then this customers behavior is far more consistent than someone who comes let us say arbitrarily 10 times the first month and once the second month and 50 times the third month and so on and so forth so in terms of consistency in his behavioral pattern across different months  this pattern is interesting even though it ’ s not frequent then repeating or periodicity is slightly similar to consistency except that a periodicity is i mean consistency is across the entire set  across the entire set of months if you have divided our database into months but periodicity  the time interval could vary in in a periodicity of a pattern if a customer comes let us say a 5 times to the bank every 6 month  we may not be able to catch it as part of a consistent pattern analysis but if we use an algorithm that detects periodicity of several occurrence of events  we will be able to detect it and similarly there are several other patterns of interestingness that which one could think of now when we talk about data mining  usually there is sometimes a misconception and not completely but usually there is a contention that data mining is the same as statistical inference for many cases it is yes  the answer is true that is several concepts from statistics have been incorporated in to data mining and data mining software uses statistical concepts or many kinds of statistical algorithms comprehensively however there is a fundamental difference between statistical inference and data mining which is perhaps the reason for the renewed interest in data mining algorithms and here is the general idea behind the data mining versus statistical inference  refer slide time  17  30  what do we do when we talk about statistical inference ? statistical inference in techniques  essentially have the following three steps as is shown in this slide here in statistical inference  we start out with the conceptual model or what is called as the null hypothesis that is we first of all present ourselves or perform a hypothesis about the system in concern that is we make a hypothesis that if some something to the effect that if exams are held in the month of march then there would be i mean then the turnout would be higher than if it is held in the month of june or something like that now based on this hypothesis  we perform what is called as sampling of the data set or of the system now sampling is a very important step in a statistical inferencing process there is huge amount of literature in to what is meant by correct sampling or what is called as a representative sample and so on now based on the sampling of data set from the system  we either prove or refute our hypothesis that is we show a proof saying  yes this hypothesis is true because statistical sampling of the system has shown that this is true otherwise it ’ s false now  when we sample for example if you are performing a statistical inference about user preferences or let ’ s say some kind of market analysis  we present questioner to different users based on our null hypothesis or based on our conceptual model now it is this set of questioner  now this questioner has been created by our conceptual model so this questioner already knows what to look for and the proof or the answers will either prove or refute the hypothesis but data mining on the other hand is a completely different process or rather it ’ s the opposite process  refer slide time  19  57  in data mining we just have a huge data set and we don ’ t know what is it that we are looking for we don ’ t have any hypothesis  we don ’ t have any null hypothesis to begin with we just have a huge data set and we just have some notions of interestingness now we use this interestingness criteria to mine this data set and usually there is no sampling that is performed on the data set that is the entire data set is scanned at least once by the data mining algorithm in order to look for patterns so there is no question of sampling and there is no null hypothesis to begin with so we just have a weighed notion of an interestingness based on which we present an algorithm  data mining algorithm over the data set out of this comes out certain patterns  certain interesting patterns which form the basis for forming a hypothesis so it ’ s sometimes also called hypothesis discovery obviously  of course we can not discover complete hypothesis using just data mining but we too discover patterns using which we can formulate a hypothesis so in a sense it ’ s an opposite process of statistical inference let us look at some data mining concepts two fundamental concepts are of interest in data mining especially in the core algorithms of data mining especially the apriori based algorithms these are what are called as associations and items sets an association  when we say an association it is a rule of the form if x then y as shown in this slide here and it ’ s denoted as x right arrow y  refer slide time  21  23  for example if india wins in cricket sales of sweets goes up  if india wins in cricket then sales of sweets goes up so here x is india wins in cricket and y is the predicate that sales of sweets go up so we say that we discover such a rule if we are able to conclusively say based on analyzing the data that whenever india wins in cricket  the sales of sweets go up and on other hand suppose if there is any rule of this form that is if x then y then i can imply that if y then x  refer slide time  21  27  that is the ordering of this rules is not important if india wins in cricket then sales of sweets go up  if sales of sweets go up then india has won in cricket and so on which may be true or may not be true but if that is the case then it is called an interesting item set that is it ’ s just a set of item for example people buying school uniforms in june also buy school bags or you can also say people buying school bags in june also buy school uniforms so it ’ s just a item set that is school uniforms and school bags are a set of items which are interesting by themselves once we define the notion of a association rule and an item set  we now come to the concept of support and confidence that is how do we discover a rule to be interesting we say that a rule is interesting in the sense of frequent occurrences of a particular rule  if the support for that rule is high enough that is the support for a given rule r is the ratio of the number of occurrences of r given all occurrences of all rules so we look into the exact or we will illustrate the notion of support in the next slide with an example where it will become more clear  refer slide time  22  41  and when we say the confidence of a rule  suppose i have a rule if x then y then the confidence of the rule is suppose i know that x is true  the ratio of all occurrences when y is also true versus when for all other occurrences when x is true and something else is here  refer slide time  23  46   so that is it ’ s a ratio of the number of occurrences of y given x among all other occurrences given x so if i know that x is true with what confidence  with what percentage of confidence can i say that y is also going to be true ? let us look at some examples here  refer slide time  24  04   let us say these are some item sets let us say these are data that have been distilled from purchases of different consumers over a period of time over  in a given month let us say so the first consumer has bought a bag  a uniform and a set of crayons  the second consumer has bought books and bag and uniform  the third one has bought bag uniform and pencil and so on and so forth now suppose i take the item set bag and uniform   bag  uniform  what is the support for this item set now the support for this item set is look at all the transactions or the rows here in which bag and uniform occur 1 2 3 4 and 5 uniform and bag out of a total of 10 rows  5 of them have bag and uniform occurring in that  refer slide time  24  34  therefore the support for bag and uniform is 5 divided by 10 which is 0.5 that is with a this dataset supports the assertion that bag and uniform will be bought together with 50 % support that is 0.5 as its support what is the confidence that  what is the confidence for the rule if bag then uniform ? that is what is the confidence by which we say whenever somebody buys a bag  they also buy uniform for this we have to look at the set of all item sets or the set of all transactions or rows here in which bag and uniform  bag occurs rather not just uniform in which bag occurs so bag occurs in 1 2 3 4 5 6 7 8 different rows  out of which bag and uniform have occurred in 5 different rows therefore the confidence for this assertion or this association rule is 5 divided by 8 which is about 62 %  that means if some consumer has bought a bag then with 62 % of confidence or 62.5 % of confidence  we can say that the consumer will also buy a uniform  a school uniform along with this so the question now is how do we mine or how do we find out the set of all interesting item sets and the set of all interesting association tools now have a look at this previous slide  refer slide time  26  50  once again now the association rule  when we talk about association rules we have just or rather when we talk about item sets first we just saw a single item set having two different elements here but that need not be the case  bag by itself could be an item set a single element item set  uniform by itself could be a single element item set  crayons could be a single element item set or let us say bag  uniform and crayons could be a three element item set and so on so item sets could be of any size size 1  size 2  size 3  size n any set of elements now we have to find the set of all item sets that is the set of all items that are bought together and that have been together frequently as part of this transaction log here  refer slide time  26  48  now how do we do that ? now there is a very famous algorithm called the apriori algorithm which performs such a discovery process that is a discovery process for all frequent item sets in a very efficient manner the simple idea behind apriori algorithm  it is shown in this slide here however let us not go through the slide in a lot of detail  since it will be more easier to explain apriori through an example the idea behind apriori algorithm is that  the essential idea behind an apriori algorithm is that suppose i have any n element item set let us say suppose i have any 5 element item set  that is interesting or that is frequent so if this 5 element item set is frequent then all sub sets of this item should also be frequent this seems obvious but this is a very important conclusion or it ’ s a very important observation in the apriori algorithm that is if i discover the set of all one frequent item sets that is the set of all item sets of size 1 which are frequent then there is no need for me to look at other item sets when i am looking for two frequent item sets that is the set of all item sets of size 2 which are frequent will be made up of combinations of set of all item sets of size 1 which are frequent so let us illustrate the process of apriori with an example let us take our consumer database again  the previous consumer database again where we have consumers buying several school utilities like bags and school bags and school uniforms and crayons and pencils and books and so on and so forth  refer slide time  29  22  now suppose we set when we say or when we ask the apriori miner to mine for all interesting item sets  we have to the interestingness criteria here is frequency that is frequent occurrence now frequency is or interestingness here is parameterized by a threshold parameter which is called the minimum support or min sup so let us say minimum support is 0.3 that is we term an item set to be interesting if its support is at least 0.3 or greater now given this what are all the interesting one element item sets ? what is that mean to say what are all the interesting one element item set  which one element item sets occur at least at a rate of 30 % or more now this database here or this data set here has a total of 10 rows therefore we have to look at all one element item sets which occur 3 or more times so given this we see that all of these are interesting that is bag  uniform  crayons  pencil and books bag occurs much more than three times  uniform also occurs more than three times  crayons also occur more than three times and so on so all of these elements here occur more than thrice which therefore all of this one element item sets have a minimum support of 30 % or more now from this  suppose we have to look at the set of all interesting two element item sets now how do we build the set of all interesting two element item sets ? we just look at all possible combinations between one element item sets  therefore we have bag uniform  bag crayons  bag pencil  bag books  uniform crayons  uniform pencil uniform books and so on and so forth now out of this for each such two element item set that have been created  we have to see how many times they occur in this data set now we see that it ’ s only these set of combinations which have a minimum support of 0.3 or more so for example bag uniform  bag crayons  bag pencil and bag books all of them along with bag are interesting however let us say uniform and book is not interesting that is it doesn ’ t occur more than thrice so let us see how many times uniform and book occur ? uniform and books occur once and second one twice here  so they occur only twice but we need a minimum support of three times so that ’ s not interesting similarly a pencil and uniform  so uniform and pencil is again is not interested so therefore we have filtered away or we have thrown away certain item sets from our exploration here and identified only a smaller subset of the set of all possible combinations of one element item set now from this if we have to look for all three element item sets  we have to generate the set of all candidate three element item sets what are the candidate three element item sets ? perform a union across all possible combinations of these interesting two element item sets to create all possible distinct three element item sets and then look for those three element item sets which occur at least three times or more in this database given that we see that there is only one three element item set that is bag  uniform and crayons that is interesting that is that occur at least three times or more or that has at least  that has support of at least 30 % in this in this data set  refer slide time  33  56  so as you can see the apriori algorithm  you can visualize the apriori algorithm in the form of let us say an iceberg such queries are also called as iceberg queries when given on to databases that is at the base there are large number of one element item sets but once we start combining them together  we start getting smaller and smaller numbers of combinations and we peak out at a very small of large item sets which are frequent so the beauty of the apriori algorithm is that for every parse  it does not need to go through the entire data set it does not have to parse through the entire data set  it only needs to consult results of the previous iteration or item sets that are of one element one lesser than the present iteration in order to construct candidates for the present iteration so given this algorithm here let us go back and look at the apriori algorithm given the explanation here with an example let us go back and look at the apriori algorithm which will now be a little more easier to understand initially we start with a given minimum required support s as the interestingness criteria now given minimum support s as the interestingness criterion  first we e search for all individual elements that is one element item sets that have a minimum support of s now we start  we go into a loop where we start looking for item sets of sizes higher greater than 1 so from the results of the previous search for i element item sets  search for all i plus 1 element item sets that have a minimum support of s this in turn is done by first generating a candidate set of i plus 1 item sets and then choosing only those among them which have a minimum support of s now this becomes the set of all frequent i plus element item sets that are interesting so this loop is repeated until the item set size reaches the maximum that is there no more candidate elements to be generated for the next item set or there are no more frequent item sets in the current iteration now that was about item sets a property of item sets is that there is no  i mean you basically consider item sets as one entity that is there is no ordering between the item sets that is it does not matter if somebody buys a bag first or a uniform first or a crayon first or whatever  as long as the  only thing that we are going that we infer from this is that the item set bags  uniforms and crayons are quite lightly to be bought together in in in one piece therefore if i am let us say a super market vendor  i mean someone having a super market then it would make sense for me to place bags and school uniforms and crayons next to each other so because there is a higher probability that all three of them are bought together but when we are looking for association rules we are also concerned about the direction of association that is there is a sense of direction saying if a then b is different from if b then a so association rule mining requires two different threshold  the minimum support as in the item sets and the minimum confidence with which we can talk about a  with which we can say or determine that a given association rule is interesting  refer slide time  37  22  so how do we mine association rules using apriori again we shall do the same thing like we did in the past we shall come back to this algorithm or the general procedure after we have illustrated an example by which we can mine apriori  using apriori algorithm by which we can mine association rules  refer slide time  38  47  now the main idea is the following now use the apriori algorithm and generate the set of all frequent item sets so let us say we have generated a frequent item set of size 3 which is namely bag  uniform and crayons with a min sup or of 0.3 that is a minimum support threshold of 30 %  now this bag  uniform and crayons can be divided into the following rules if bag then uniform and crayons or if bag and uniform then crayons or if bag and crayons then uniform and so on so forth  refer slide time  39  38  now what is this thing mean ? this thing means that when a customer buys a bag then the customer also buys uniform and crayons and this rule means that if a customer has bought a bag and a school uniform then the customer will also buy a set of crayons or if a customer has bought a bag and a set of crayons then the customer will also buy a school uniform and so on now we have got all of these different association rules now each of these association rule has a certain confidence based on this data set now what is the confidence for each of these rules ? what is the confidence for the rule if bag then uniform and crayon that is if a customer buys a school bag then here she will also buy a school uniform and a set of crayons in order to calculate the confidence of this  we have to first look at which are all the item sets here that have bags that is where the customer has bought a bag so  there are 1 2 3 4 5 6 7 8 different entries where customer has bought a school bag now among these 8 entries  in how many different entries did the customer also buy uniform and crayons ? 1 and 2 3  so there are 3 different entries  3 different instances out of 8 instances where this rule holds therefore whenever a customer buys a bag  one can say with 3 by 8 or 37.5 % of confidence that the customer is also going to buy a set of school uniform and crayons similarly we can calculate the confidence for each of these other association rules like this is 0.6  0.75  0.428 and so on and so forth now  given a minimum confidence as a second threshold and suppose we say that the minimum confidence is 0.7 then whichever the rules that we have discovered  every rule that has confidence of at least 70 % or more that means we have discovered the following three rules  bag if bag crayons then uniform  uniform crayons then bag and crayons then bag and uniform what is that mean in plain english ? it means that people who buy a school bag and a set of crayons are likely to buy a school uniform as well that is bag and crayons implies uniform  refer slide time  40  47  similarly people who buy a school uniform and a set of crayons are also likely to buy a school bag that is here  somebody buys uniform and a set of crayons then they are also likely to buy a school bag similarly if somebody buys a set of crayons then they are very likely to buy a school bag and a school uniform as well  refer slide time  43  09  so that is here  that is somebody buys crayons then with 75 % confidence one can say that they also buy bags and school uniforms so again it ’ s a question of direct marketing or whatever if somebody is interested in crayons then you might be reasonably sure that they are also interested in a bag and a school uniforms so on now so let us look at look back at the algorithm here  refer slide time  43  41  for mining association rules simple mechanism for mining association rules is first of all use apriori to generate different item sets of different sizes and at each iteration  we can divide each item sets in to two parts an lhs part and an rhs part  the left hand side part and the antecedent and precedent that is the right hand side part so this represents a rule of the form lhs implies rhs then the confidence of such a rule is support of lhs divided by that is support of the entire thing divided by the support of lhs that is support of lhs implies rhs divided by support of lhs will give us confidence of this rule and then we discard all rules whose confidence is less than minconf so now let us look in to the question of how do we generate or how do we prepare a tabular data for association rule mining or let us say item set mining and so on now because we use let us say relational data set  relational database you might have observed that or you might have got a little doubt when we have been considering a data set like this there is something peculiar about this data set what is peculiar about this data set here ? the peculiarity is that it looks like every consumer coming to this store is buying exactly three items which is very unlikely in fact what is more practical is that this set  this data set contains records of variable length that is one customer may have bought just two different items whereas some other customer may have bought 10 different items whereas a third customer may have bought only 5 different items and fourth customer may have bought only one item and so on and so forth  refer slide time  46  34  so it is not possible to represent this item set like a table  like a well form table like this because basically it is a set of all items of different lengths in fact the best way to represent this would be in a normalized form let us say in a database where for example the same bill number here 15563 15563  both of this refer to the same customer that is it ’ s the same customer who has bought books and crayons and this is not completely normalized because date is not really necessary here but nevertheless here all of these records are of uniform length  if you order this based on the set of bill numbers then we get the set of all different transactions now depending on what we are looking for this  this ordering might make a difference how does this ordering make a difference here when we are looking at data set like this ? suppose given a dataset like this  here performing group by ’ s on different fields will yield as different kinds of behavior data sets  refer slide time  00  47  19  so what does it mean ? suppose let us say we perform a group by based on the bill number  refer slide time  47  37  so suppose we perform a group by on the bill number on this table then each group will represent the behavior of one particular customer that is one bill represents one or one bill number represents one particular customer or one particular transaction so suppose we group by based on bill numbers and then perform apriori across these different groups then we would be getting frequent patterns across different customers on the other hand suppose we group by over date  so rather than bill number so all transactions happening on a given date will come in to one group and all transactions happening on another date will come in to another group but a given date may have transactions from several different customers but all of them are now grouped in to one single group and suppose we run apriori over this set  over this different groups then we would actually be looking for frequent patterns across different days that is across the different dates so we have to interpret what we mean by something that is frequent based on how we have ordered the data if we have ordered the data over different customers then it would show aggregate behavior over the set of all consumers with whom you are interacting with on the other hand if you are running apriori or if you have performed group by over dates then it would show you aggregated behavior over a given time period rather than over the set of all customers well  it also includes the set of all customers but what is more important here is that how does the behavior or how has the behavior changed over time so if something is frequent over time  it means that it is uniformly or in some sense consistent over this entire period of time so let us summarize what we have learnt in this session we started with the notion of data mining and like i said in the beginning  data mining is a very interesting sub field of databases which has elucidated a lot of interest not just from researchers or and not just from the technology perspective but from several other perspectives like defense perspective or security perspective  commerce that is business perspective and so on and there are several debate that have raged on whether it is right to use data mining to look for certain behavior pattern for example would it be right  if a government uses data mining over let us say the set of all different activities of people and find out the behavior pattern of any particular individual and so on and their pros and cons on both sides of the debate  one would say for security reasons it is right to look for behavior patterns and one would say well for privacy reasons it ’ s not right to look for behavior patterns and so on and so forth so it ’ s a topic which is very much pertinent and has spond a huge amount of interest from several different areas and data mining is in some sense  i called it as sub field of databases but that ’ s not entirely true in a sense that data mining and knowledge discovery many would claim is a field in itself that is it relies on database concepts as well as several other concepts like learning theory or statistical inference and several other concepts in order to perform data mine so don ’ t be really surprised if one would say that a data mining is a complete field in itself and its only associated with databases not really sub field of databases but anyway data mining as we said is the process of discovery of previously unknown patterns in the sense that we have not really sure what is it that database is going to give us or what new pattern or what new nugget of knowledge so to say is we are going to learn as part of the data mining process as a result there is no query as part of a data mining process that is a data mining algorithm is based around one or more interestingness criteria rather than a given query  refer slide time  50  11  and we saw that in conceptually  it is in some way the opposite of statistical inference where we start with a null hypothesis and either refute or prove or hypothesis by sampling  statistical sampling of the population while here we don ’ t start with a hypothesis but the end result of the data mining process is the set of patterns which can help us in formulating a hypothesis we also saw the notion of association rules and item sets as well and the concepts of support and confidence and two different algorithms the apriori algorithm for mining frequent item sets and from which we also saw the apriori algorithm for mining association rules in the next session on data mining  we are going to look at several other algorithms like say classification or discovery so that ’ s brings us to the end of this session thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 35 data mining & knowledge discovery part ii hello and welcome to this second session in data mining in the previous session we saw what this concept of data mining was all about and we saw some very fundamental concepts of item sets and association rules and how do you discover particular patterns in an item set that is how do you discover something that you don ’ t know from a data set using the concept of support and confidence and so on  refer slide time  02  06  so  essentially you give a particular interestingness criteria and then you start distilling out certain patterns from the data set let us move on further in this session where we will briefly look into some fundamental algorithms or some very simple algorithms on different kinds of data mining activities namely in discovering classification trees or discovering clusters of properties of data and mining sequence data  the data of different sequences or stream data mining and so on let us briefly summarize what data mining was all about  refer slide time  02  23  data mining essentially is the concept of or is the idea of looking for hidden patterns and trends in data that ’ s not immediately apparent by just summarizing the data so when we say hidden patterns  its essentially means that something that we don ’ t know about there is nothing hidden if you already knew such a pattern existed in the data base so in a data mining setting there is no query but we use the concept of an interestingness criteria that is we use let us say frequency or consistency or rarity or whatever be the interestingness criteria and certain parameters define each of these interestingness criteria like frequencies is parameterized by support and confidence for association rules and just support for item sets and so on and again there are different kinds of data we can think of tabular data  spatial data  temporal data  tree data  graph data and so on and so forth  refer slide time  03  24  so today or in this session we shall look at specifically at sequence data mining and streaming or mining streaming data and in addition to other mining algorithms and of course type of interestingness itself could be varied that we could talk of frequency as frequent patterns as being interesting or rare patterns being interesting and so on  refer slide time  03  48  now let us move further from here and look at the concept of classification and clustering that is discovering classification tree and discovering clusters within a given data set  refer slide time  04  03  now what is the difference between classification and clustering ? intuitively they both seem do the same thing that is when you classify a given data set into different classes or whether you cluster a given data set into different clusters but essentially few observe closely classification maps data elements to one of a different set of pre-determined classes based on the differences between data elements that is if data element a and data element b belong to different classes if they are different enough on the other hand clustering groups data elements into different groups based on similarity between elements within a single group and sometimes it ’ s also the case that in a classification we know the classes apriori we know what are all the different classes into which data can be classified into and sometimes in clustering  we don ’ t know how many clusters we are going to get before the clustering process begins let us look at mining in relation to classification techniques rather ; we are not interested here in the idea of classification itself but we are interested in the idea of discovering classification what is it meant by discovering classification ? discovering a decision tree or which decides how to classify data sets into different classes  refer slide time  05  19  let us take a small example discovering this algorithm is best represented by an example so let us take a small example and see how we can discover a classification tree let us say that we have data about different cricket matches that have been played over the last several years now we have a  let us say in a given city now the question is this city is notorious for it rains for its rains and its unpredictable weather now in the past several times  play had to be abandoned that is play were to be continued or was abandoned and so on now we have data like this from different data sets when it was sunny and the temperature was 30 degrees  play was continued when it was overcast and the temperature was 15 degrees play wasn ’ t continued  when it was sunny and temperature was 16 degrees play was still continued and so on so in some times play was continued and sometimes play was discontinued  its no now what is the classification problem is can i classify weather conditions which is a combination of the outlook and the temperature into one of two classification classes that is whether we are going to play or play is going to be discontinued that is what is the criteria  when play was discontinued and what was the weather criteria when play was continued  refer slide time  07  32  so there is a well-known algorithm called hunt ’ s method for identification of decision trees and like before let us first look at an example of how we identify a decision tree before looking at the algorithm itself  refer slide time  07  53  the way of identifying decision tree is quite simple first of all because this temperature field here  refer slide time  08  04  is a numeric value  it could take several different values and which might be of no interest to us so let us perform a hand classification of this numerical values into different classes  refer slide time  08  19  so what we have done here is that temperature is now classified into three different classes warm  chilly and pleasant so whether the temperature was warm whether the temperature was chilly or whether the temperature was pleasant based on dividing the set of temperatures into different classes now first of all because there are two values here that is there are two fields here outlook and temperature  both of them both of them will affect the decision on whether we are going to play so how do we know what is the best or how do each how do each parameter affects the decision whether to play or not let us start by looking at one parameter after another first let us look at sunny now if you see here that whenever the outlook was sunny  the cricket match was played it was not abandoned it is sunny only twice here and in both cases cricket matches played therefore we can directly conclude that if the weather is sunny regardless of whether the temperature is warm or whether the temperature whether the temperature is chilly or whatever  we can conclude that play will continue  the play is not going to be stop on the other hand let us look at cloudy here now when it is cloudy here play was continued in one case or rather in two cases and when it was cloudy here  play was discontinued in one case so from cloudy we are still in a what is called as a bivalent state that is it is still yes or no may be or whatever  may be yes may be no  we still don ’ t know similarly when the outlook was overcast  let us say here it was overcast and they didn ’ t play here once when it was overcast they actually played and once more  when it was overcast then they didn ’ t played so  from overcast we still say yes or no  we don ’ t know whether they are going to continue play or not  refer slide time  10  38  so what we can do now is we can safely remove the first rule from our process that is this is a rule that we have already discovered that is when it is sunny they are going to play so now let us remove this rule from our from consideration and take these two rules now because from cloudy and over cast  we are still in a bivalence state we have to ultimately reach to a state where we can remove this bivalence that is we can either conclude yes or no conclusively so we will try to we will try to now introduce the second parameter temperature into this state here to see whether we can remove this uncertainty about yes or no the first case the uncertainty is already removed  so there is nothing we need to do any more  refer slide time  11  32  so we have introduced let us say here  refer slide time  11  41  for cloudy  we have introduced all three possible cases warm  chilly and pleasant  similarly for overcast warm  chilly and pleasant so let us take cloudy and warm so  whenever it was cloudy and warm there is only one case here play was continued  yes so basically we have removed the bivalency that is we have conclusively stated that whenever it is cloudy but the temperature is warm  play is going to continue  we are not going to abandon play on the other hand whenever it was cloudy and chilly  there is only one case here where play was discontinued so again there is the bivalency is removed that is cloudy and chilly means no so we can again conclusively state that the play is going to be abandoned if the outlook is cloudy and the temperature is chilly similarly when it is cloudy and pleasant  cloudy and pleasant is here and there is only one case here  cloudy and pleasant is yes so when the outlook is cloudy but the temperature is pleasant  we can still conclude that they are going to continue play similarly overcast and warm there is no entry at all  so we don ’ t know there we can ’ t decide anything so overcast and warm remains as it is and overcast and chilly gives us no  that is play is going to be abandon similarly  overcast and pleasant gives us yes  refer slide time  12  53  so effectively we have removed this bivalency that existed here  refer slide time  13  20  when it was cloudy and overcast and decided or came to know when  under what conditions play is going to be continued when it is cloudy and under what conditions play is going to be discontinued when it is cloudy and the same thing for overcast so therefore what we have actually done is we have discovered this decision tree so initially we were in a bivalent state that is we don ’ t know play is going to be continued or discontinued now in this bivalent state we were told that the outlook is sunny then we can immediately conclude yes we are going to play today  refer slide time  13  53  on the other hand if you are in this bivalent state here  if you are told that the outlook is cloudy  we will still be in a bivalent state we still don ’ t know whether they are going to  whether the play is going to be continued or not so we ask for more information and then when you find out that the temperature is pleasant  let us say for example then we say that yes the play is going to continue on the other hand if the temperature is chilly then we have reasons to believe that play is not continued that is the data set tells us that play is going to be abandon and so on so what we have got here is a tree data structure where from a bivalent state  we eventually go into a univalent state that is a state were the uncertainty is removed and then we have concluded or we have classified his this play into two different classes that is yes or no that is play is going to be continued or play is going to be abandon so let us look back  refer slide time  15  11  at the algorithm little bit as how to go about this suppose we are given n different elements in our case in the example that we right now saw  n was equal to 2 that is outlook and temperature so suppose we are given n different element types and m different decision classes  in this case again m was two that is yes and no so what we do in this loop here  for each of the different element types we keep progressively adding element i to the i minus oneth element item sets from the previous iteration and then whenever and then we see whether we can decide  identify the set of all decision classes for each such item set if the item set has only one decision class that means we have already decided so this is done  removed that item set from subsequent iterations otherwise keep continuing until you finish all your element types and of course it could well be the case that even after finishing all my n different item sets  i may not be able to reach a conclusive decision  refer slide time  16  07  so it might well be the case that when it is over cast and chilly sometimes they actually play and sometimes they didn ’ t play and so on so that again  there are several methods to deal with such kinds of indecisiveness for example to use probabilities that is this is going to or some kind of fuzzy classification where we say that outlook is overcast and temperature is pleasant then they are going to play with a probability of 90 % or something like that so let us look further into what are some clustering techniques  refer slide time  17  03   now what is meant by clustering or how does it differ from classification ? we saw earlier that there is a philosophical difference between classification and clustering  probably not in the n result but philosophically there is a difference of course even in the end result there are differences but the most marked difference is philosophically that is classification is based on amplifying the differences between different elements so as to make them belong to different classes  refer slide time  17  48  on the other hand clustering is based on amplifying the similarities between elements so as to form them into different clusters so clustering essentially partitions the data sets into several clusters one or more clusters or equivalence classes and what is the property of a cluster or an equivalence class ? essentially the property here is that the similarity among members of a given class in a cluster is much more than similarity among members across clusters so members belonging to the same cluster are much more similar to one another than they are to some members belonging to some other clusters and there are several measures of similarities and most of which are reduced to geometric similarity by projecting these data sets into hyper cubes or n dimensional spaces and then use some kind of euclidian distance or other kinds of distance measures like manhattan distance and so on and several distance measures to compute the similarity  refer slide time  18  58  let us look at the first kind of clustering algorithm which is called the nearest neighbor clustering algorithm this is quite simple that is this clustering algorithm takes a parameter called threshold or the minimum distance or the maximum distance t between members of a given cluster so given n elements that is x1  x2 to xn and given a threshold t which is a maximum distance that can exist between elements of a cluster  we can find clusters in a very simple process initially the set of clusters is a null set then for each element let us say j equal to 1 here and j goes to  until j plus one here for each element find the nearest neighbor of xj now let the nearest neighbor be in some cluster if it is already in a cluster  if it is not in a cluster then fine you can just create another cluster by yourself so suppose the nearest neighbor is in cluster m now if the distance to nearest neighbor is greater than t that is if it is greater than threshold then we know that there is no other element that is nearer to me with a distance less than t therefore i should belong to a new cluster so then create a new cluster and increment the number of clusters else assign it to the cluster m were the nearest neighbor of it existed so  as simple as that that is given a small threshold  you basically start partitioning your set of elements into different clusters based on which is the nearest neighbor to a given element if the nearest neighbor is within this threshold distance then i join the cluster  otherwise i belong to a new cluster  refer slide time  21  03  there is another kind of clustering techniques which is again quite popular which is called as the iterative partitional clustering this is another clustering technique where this differs from the nearest neighbor technique in the sense that here the number of clusters are fixed apriori in the nearest neighbor technique or in the nearest neighbor clustering techniques  the number of clusters are not fixed apriori that means you don ’ t know how many clusters you are going to get  given a particular threshold and a data set so this is very much unlike classification where we know the classification  where we know the classes under which data can be classified into in iterative partitional clustering  the number of clusters are already known apriori and then we are trying to rearrange the clusters that is but that is we don ’ t know how many or what elements belong to which clusters so  given n different elements and k different clusters  each with a center what do we mean by a center here ? it ’ s the centroid in the statistical sense  for example it could be the first centroid that means if a cluster has several features  the average of all these features along all different dimensions will form the centroid of a given data set so let us say we have k clusters each with a center now assign for each element  assign it to the closest cluster center so each clusters has a cluster or a centroid for each element  find out which is its closest cluster center and assign it to that cluster after all assignments have been made  compute the cluster centroids for each of the cluster that is compute the average of all the points that made up this cluster and possibly this will shift the centroid to a different to a different location so once this centroid is shifted to a different location  the nearest centroid or the nearest cluster center will now differ for each element therefore we keep repeating these two steps  until the new centroid i mean with a new centroids that are formed until the algorithm converges that is until the algorithm stabilizes so that the centroids will stop shifting and then we know that we have found the exact or we have found the best centroids for each of the clusters  each of the k clusters so iterative partitional clustering essentially is a technique were something like saying  suppose i have a data set and i say that suppose i want to create 10 different clusters out of this data set  where would these clusters lie and so on on the other hand  a nearest neighbor clustering technique would say suppose i have this data set and suppose i have a maximum distance  a threshold distance of 5 between elements that can lie within a data set then how many clusters will i find whereas in the in the iterative clustering algorithm  we are interested in where the clusters are going to be  where are the cluster centroids of these 10 different clusters that are going to be formed  refer slide time  24  52  let us now move on further and look at different other kinds of data sets we have been looking into  until now we have been looking into let us say the tabular data as in apripri or association rule mining or some kind of multi-dimensional data tabular data can be treated as multi-dimensional data as long as they belong to certain ordinal classes which is of course beyond the scope of this session here that is how do we convert a tabular data into multi-dimensional data but any way as long as the data can be converted to multi-dimensional form  we can use clustering techniques for clustering them into different clusters similarly tabular data can be used to also infer classification trees let us now move on to different kind of data what is called as sequence data what do we understand by the term sequence ? sequence is essentially a collection of data elements wherein it ’ s not just the collection  it ’ s an ordered collection that is where in the ordering matters that is in a sequence each item in a sequence has an index associated with it that is some kind of a subscripted element  each element is a subscripted element so this is the first element  this is the second element and so on so when we say we have a k sequence  it means that we have a sequence of length k that is there are k different elements in a particular order in this there are different kinds of sequence data like for example any kind of transaction log over a period of time or let us say some kind of web browsing logs  http logs or dna sequences or the patient history  the medical history of a patient over time that is how is the history changing or what kinds of events happened and so on so all of these are sequence data  refer slide time  27  13  so let us look at some definitions in mining sequence data and which help us in formulating algorithm for looking at patterns in sequence data first of all when we talk of a sequence  a sequence is essentially a list of item sets of finite length that is each element in a sequence need not be atomic  it could actually be a set  it could actually be a different set of items so for example this is the sequence the first element here is pencil  pen  ink or pen  pencil  ink the second element here is pencil  ink the third element is eraser  ink and so on and the fourth element is ruler  pencil and so on so this sequence essentially for example could be denoting the purchases of single customer over time in this particular store or whatever so let us say the customer came in the first month and purchase these three things  the second month you purchase these two and the third month you purchase these two and so on in some stationary store now the order of items within an item set here does not matter but the order of item sets itself matters that is this is the first month  this is the second month  this is the third month  so the position of this item set matters but the position of items within an item set doesn ’ t matters so whether i read this as pencil  ink or ink  pencil it doesn ’ t matter and we define the term sub sequence  as any sequence with some item sets deleted from it so  some more definitions suppose i take a sequence a1  a2 until am  this is actually a sequence it ’ s not a set  so this curly braces should actually be a  it should not be there  refer slide time  28  49  so suppose i take a sequence s prime a1 a2 until am we say that s prime is set to be contained within another sequence s  if s contains a sub sequence of the form b1 b2 etc bm that is m different elements such that each corresponding element is a subset  a1 subset of b1 subset equal to rather and a2 subset equal to b2 and so on so  hence for example this sequence pen  pencil and ruler pencil is contained in this sequence that is pen is a subset of this  pencil is a subset of this and suppose you take this out and create this sub sequence pen  these three as a subsequence then ruler pencil is a subset of this one so  let us look at the apriori algorithm i think called the apriori gen algorithm or whatever apriori all algorithm where it is applied for sequence data rather than item sets or association rules  refer slide time  30  06  the apriori algorithm for sequences looks very similar to the apriori algorithm for item sets as well how does the apriori algorithm look ? first of all we set  we generate l1 that is the set of all interesting one sequences what is the one sequence ? a sequence containing just one element and then when lk is not empty when k equal to 1  we generate all candidate k plus 1 sequences and out of these  we take only the set of all interesting k plus 1 sequences what is interesting k plus 1 sequence here ? it is simply the set of all k plus 1 sequence which have at least the minimum support that we have specified and so on now the main question here lies in this statement here 3.1  that is how do we generate or what is the candidate generation algorithm ? how do we generate all candidate k plus 1 sequences ?  refer slide time  31  14  so how do we generate all candidate algorithms ? now given let us say different interesting sequences that is l1 l2 until lk  candidate sequences of lk + 1 are generated simply by concatenating all sequences in lk with all new one sequences found while generating lk-1 what is this mean ? let us illustrate this with an example  refer slide time  31  46  let us say this is my data set and this data set let us say denotes  let us say i have a website and this data set denotes which are all the different pages that have been visited by users in different usage sessions so one user a went from  one user went from page a to b to c to d to e and so on another user came from b and went to d and a and e and so on like this so we have different sequences and of course as you can see here that an element can repeat in a sequence that is this user has requested for the page a 4 times one after the other and same thing here  refer slide time  32  55  that is after b  a is requested three times and so on for whatever reason now from here in order to look at  in order to mind for all interesting sub sequences that is what will be visited before what in this data set  let us start with the set of all interesting one sequences now we have set a minsub as 0.5 that is at least 50 % of support now let us look at the set of all interesting one sequences what is it mean to say interesting one sequences ? essentially it means that which all sequence of length one have appeared at least 5 times or more so a has appeared 1 2 3 4 5 6 7 8 times in 8 different sequences  b has appeared 1 2 3 4 5 6 7 8 9 different times and so on so a b d and e are interesting one sequences  c for example has appeared just once here  so therefore it is not interesting at all as a one sequence now we generate all possible candidate two sequences that is it is now rather than a combination  it ’ s a permutation that is where the order matters so aa and rather it ’ s not a permutation  it is a concatenation rather that is concatenation of all possible concatenations that are possible between elements of this one so ab is different from ba and ad is different from da and so on so these are the set of all candidate two sequences now we just see which of these candidate two sequences have minimum support  refer slide time  34  34  now among these you see that only ab and bd have a minimum support of 0.5 that is all others aa for example has the minimum support of 1 2 3 that ’ s it  not 0.5 that is one is here rather 4  1 2 3 and 4  ab also has minimum support less than 5 and so on so the only set of interesting two sequences are ab and bd in this case so we have got the set of all interesting two sequences now how do we generate the set of all interesting three sequences that is candidate three sequences ? we concatenate ab and bd with all the interesting one sequences found in the previous iteration so the previous iteration here is still the one sequence here ab d and e therefore we concatenate both of this with a b d and e like this and then we see that there are no interesting three sequences at all and then the process stops otherwise we would have filtered out few more elements here and then out of these  again we would have concatenated with all possible interesting one sequences that we found in the previous iteration so here the interesting one sequences that we have found in the second iterations are a b and d so for level 4 there is no need to concatenate it with let us say e  so it ’ s enough if we just concatenate with a b and d  refer slide time  36  34  with sequence data there is an other kind of interesting mining problem that occurs  when we look at a sequence data as a behavioral pattern see for example when we say this is the way that users behave in a data  user behave in a website the user here comes to page a then goes to page b then goes to page c  d and e and so on now we are encountered with a question as to can we model the behavior of the user what would be a model that would explain me how users behave on my website ? so what this means is that we have to find out  suppose these are all the different strings of a given hypothetical machine  we have to find out some machine which can generate all of these strings and possibly other strings that belong to the same class in whatever sense that is so the question here is that given different sequences  treat this different sequences as strings that are generated by a particular machine the simplest kind of machine that we can generate is the state machine or the deterministic finite automate or the finite state machine or whatever now but that doesn ’ t mean that everything can be modeled by a finite state machine but it ’ s purely because of complexity considerations or practical considerations that we assume that the model representing user behavior is given by a finite state machine so given a set of input sequences  we have to find out what is the finite state machine that recognizes this class of input sequences this also called as language inference that is given the strings of a language  you are trying to infer the grammar of the language or you are trying to infer the structure of the language now what is the problem in language inference ? what is the big  where is the trickiest problem that occurs in language inference ?  refer slide time  38  39  take a look at these strings let us say i have these four strings abc  aabc  aabbc  abbc so on now if i want to give you these four strings and tell you that create a state machine that will recognize these four strings it is quite obvious that one would come out with the state machine like this which says which accepts these fours strings and exactly these four strings  so abbc  abc and aabbc and so on so which accepts exactly these four strings on the other hand  one can also write a machine like this comprising of a single state which leads on to itself and accepts all strings like this so this is a most general state machine that is this state machine is also correct in a sense that it accepts these four strings but it also accepts anything else made of a b and c in addition to these four strings  while this is a most specific state machine that is this is a state machine that accepts these four strings and these four strings only and nothing else now the challenge or now the trickiest problem in language inference is to find the right kind of generalization that is if we make something into a most specific state machine  it will be of no use  while we make something into a most general state machine  it will be useless as well so when we discover or when we try to discover a model of user behavior  we should discover a model which is not too specific and is neither too general  it has to have the right kind of generalization how do we do that ? there are several different algorithms that try to generalize a little bit and not too much and not be too specific and so on  refer slide time  40  34  we will just look at one specific algorithm which might be termed as the shortest run generalization that is generalize based on behaviors by using what is called as a shortest run technique of this thing now as we did for the previous algorithms  let us first look at the example and then come back to the algorithm  refer slide time  41  21  now the way shortest run generalization works is shown in this state machine here now let us say that we encountered different strings now let us say this is the first string that we encounter aabcb now there is no other string therefore we just build a state machine like this which accepts only aabcb and we haven ’ t seen anything else  so we can ’ t generalize anything else now second we encounter the string aac so what this means is this state machine should accept not only aabcb but also accept aac what does this mean ? this means that start from aa and after aa if i get a c i can go directly to the end state  so it has to accept not just aabcb but also aac now let us say that i get the third string  even here i won ’ t be able to generalize anything this is the state machine that accepts aabcb or aac  so we still haven ’ t generalize anything now let us say i encounter one more string of the form aabc now what is this mean ? this means that aabc that is this string  that is this is a prefix of this thing that is this is the substring of this thing  this is the prefix of string of the first one so aabc this state itself should be a end state so basically we come like this here and abc this becomes the end state now what we do is we merge both of these end states  so b comes back like this when we merge these end states  note that we have performed a specific particular generalization here now what is this machine recognize ? this machine recognizes aabc b star that means any number of b ’ s after aabc so essentially what it sees is that or any number of b ’ s after aac as well that means it has seen a b appear after aac that is this substring aa and c with or without b included  it has seen that b may appear or not appear and it generalized to the fact that any number of b ’ s may appear  including 0 number of b ’ s which may or may not be right that means to say that there might be an implicit  there might be some more hidden variables that says that at most 3 b ’ s can appear let us say 0 1 2 or 3 b ’ s can appear not 4 b ’ s but we don ’ t have that information here as such so basically the state machine generalize to the fact that after aabc or after aac zero or more b ’ s can appear and we still lie in the end state but then we also see that when we look at the end state here  we look at the tails of all the edges coming into the end state so there is a tail here which says c and there is a tail here which says c now whenever from the end state it finds that there are two or more tails having the same suffix  these two the corresponding states are also merged so what we finally get is aa b star c b star so that means what the machine generally is actually saying is that this language has to have two a ’ s to begin with  so it has two a ’ s and it can have 0 or more b ’ s following two a ’ s and then it should have a c and then it can have 0 or more b ’ s and so on so because it has found 0 or 1 b ’ s between a and c and it has found 0 or 1 b ’ s after this c  it has performed this generalization so this is one way of performing or trying to discover the behavior that is exemplified by a set of sequences let us look at the last kind of data set for this session namely streaming data streaming data has been of relatively newer interest among the data mining community and especially since the streaming data or mining on streaming data has several interesting applications now what is the characteristic of streaming data  what you understand by streaming data ? you have let us say streaming audio  streaming video  network traffic and sever several other such data sets which are essentially large data sequences possibly infinite data sequences in practice of course there are finite but possibly infinite data sequences and there is no or very little storage that is it is not practical to say that i am going to store the entire streaming data into a file and then start mining the file  refer slide time  46  53  because this if it is infinite or if it is extremely large  it will be impractical  it could be tera bytes or even more bytes of data that could eventually accumulate into the file so some examples are stock market quotes or streaming audio or video or network traffic and so on  refer slide time  47  12  so in order to mine streaming data or rather even in order to let us say query streaming data  there is a notion of what is called as running queries or also what are called as standing queries that means in a traditional database the data is standing  the data is there and the query actually slides through the data set in order to return you the answer but in a streaming data set it is the query that is standing and the data streams through the query and then the query keeps returning you answers as and when the data streams through it so how do we write some standing queries or how do we find some aggregate behaviors based on some standing queries ? let us look at some simple standing queries  computing the running mean of a data stream that is suppose i am getting a stream of different numbers and i have to calculate the average of these numbers as and when i read a new numbers  so it ’ s a running mean so a simple way to calculate this running mean is like this  let us say i just need to maintain two variables here one is the number of items that i have read so far or the number of numbers that i have read so far and the running average that i have calculated so far so whenever i read the next number  all i need to do is first compute n times average that is average times the number of numbers that i have read so far  add number to it and divide it by n plus 1 and then increment the number of numbers that you have read or the number of items that you have read that is n equal to n plus 1  so as simple as that that is as soon as a new number comes  you generate the sum  see n times average is basically the sum of all the numbers that have come so far so generate the sum here  add the new number and divide it by the new that is number plus 1  n plus 1 as the new set of numbers that have come and then increment your set of numbers similarly this slide shows how to write a running query that computes the running variance variance as you know is the square of the standard deviation of a given data set how do you compute standard deviation ? that is it is for every element x  compute x minus x bar that is number minus average whole square and compute the sigma or compute the sum over all of them  all of these differences  so mean square distances essentially  refer slide time  49  34  so in order to compute the running variance  we look at this formula little more carefully variance equal to sigma of number minus average whole square where number ranges from i equal to 1 to n or whatever now  when you expand this  you can expand this into number square minus 2 times number times average plus average square so essentially what this means is we have to maintain certain variables  one is sigma of number square so  every time you read a number  square the number and add it to the previous sum that you have maintained of course you also have to maintain the number of numbers that have been read so far then you also have to maintain two times number star average of all numbers that have been read so far so you know how to compute the running average  so every time you get new number compute the running average that is we saw how to compute the running average in the previous slide and then compute two times number times average and add it to this so essentially you can take out average out of this and sigma of number or two times average out of this and you just basically have to maintain sigma of numbers that is the sum of all the numbers that we have calculated until now and multiplied to the new average that we have found and then we have to maintain  there is no sigma that is necessary here because average is a single number and we have to just maintain the square of the average of all the numbers that we have read so far and we know how to maintain the average now by maintaining all this  we can easily calculate the running variance that is you just compute each of them  put each of them in their corresponding places and compute the running variance therefore even if i have a long  let us say stock quotes from the stock market giving me how the quotes of  how the stock price of a particular stock is changing i can maintain what is the mean stock price that it has recorded so far and what has been the variance and i can easily calculate standard deviation at any point in time by computing the square root of the variance so i know how much it has varied over time and what has been the mean behavior of this stock over the entire time that i have read so far  refer slide time  53  10  so this slide essentially shows how you can calculate the running variance that is whenever you read the next number first compute the average  we know how to compute the average then each of these is computed like this that is a equal to a plus n square b equal to b plus two times average star n and c equal to c plus average square and variance is a plus b plus c we shall also look at one more algorithm for streaming data essentially what is called as a gamma consistency or looking for events that have what are called as gamma consistency  refer slide time  53  49  what is meant by this gamma consistency ? essentially the idea behind this is as follows suppose an event happens at some point in time the interestingness of that event will be high in the vicinity of the event that is right after the event happens  let us say stock market crashes the interest in that event will be high in the next few days but over a period of time  the interest that event starts going down unless of course the stock market crashes again so that is the essential idea behind gamma consistency that is first consider this streaming data to be in the form of frames where each frame comprises of one or more data elements then we look for some interesting events within a frame essentially let us say support based interestingness so by let us say number of occurrences of k divided by number of elements in frame and then we see which of these events have sustained support over all frames rate so far with a leakage of 1 minus gamma that means in every frame let us say every day or every week or whatever  we look at events that are interesting with a support of k  refer slide time  55  02  and if this event keeps on occurring with at least this much support then you can consider this to be some kind of beaker where you are pouring in the events which are coming in with some kind of support and this beaker has a small hole underneath where in it leaks at a rate of 1 minus gamma so over a period of time if you take it over a period of time  if and only if this event has a sustained support over time this beaker is going to be full or this beaker is going to have a particular level and if the event does not sustain over time eventually  the beaker is going to empty itself so the level in this beaker is an indication of two things one is how sustained is the support for this event and second could also be how recent was this event so the more recent the event is the higher the level is going to be  similarly the more sustained the support for an event is again the higher the level is going to be so you can calculate the level like this and then you can again put a threshold for this level and look at all events which have a particular level or so or level are higher at any given point in time  refer slide time  57  04  so we now come to the end of this second session on data mining we have just crashed the surface of what is a vast area of knowledge discovery from databases and we have kind of scratched it in a breadth first fashion that is we looked at several representative algorithms for different kinds of data mining problems whether it was a apriori or whether it was classification or clustering or sequence data or something like language inference and streaming data and so on but this is just still the tip of the iceberg so anyway that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 36 object oriented databases hello and welcome we were looking at the ongoing saga of  trying to understand data management in terms of database management systems we have looked at several different topics in database management however there is some kind of a common theme or an implicit theme in a database management in whatever topic that we are looking at namely that database or databases are primarily or essentially represented using the relational data model so what is a relational data model ? that is the data pertaining to the uod or the universal discourse is maintained as a set of tuples or as a set of rows in a table and the main assumption here is that every possible kind of data can be reduced to a set of tuples  refer slide time  02  34  in this lecture and in the following lecture  we will kind of generalize on this assumption or we won ’ t accept this assumption and look at other kinds of database management requirements where data can not be in a sense easily mapped on to the relational model in this context specifically we shall be looking at the object oriented databases that is shown in the slide here so let us look at what kind of data that we are talking about when we are looking into object oriented databases but before we begin  in fact object oriented databases have been very popular in the last decade of the twentieth century in the sense that in the 1990 ’ s but however they were not as widely successful as say the relational model database mainly because the object oriented databases do not have a sound theory that is they do not have a nice little mathematical model that describes the complete data model as opposed to the relational data model where you have the relational algebra or the tuple relational calculus and so on where the entire data model is amenable to a nice theoretical framework so it ’ s because of one of these reasons which is probably cited as the reason why object oriented databases did not sustain lot of interest but however the contraries also true to some extent that is object oriented databases have been in use or have been put to use in several different applications mainly cad application  computer aided design of say electrical circuits or mechanical circuit  mechanical design and so on and they continue to be used and there are quite a few commercial implementations of object oriented databases so in this lecture and the next when we are talking about object oriented databases  we kind of implicitly assume an example application of a cad that is computer aided design scenario where users would be using computers to perform electronic design or electrical design where an electronic design comprises of several different components  i might have an ic  i might have a capacitor  a resistor  a transistor and so on each component having its own characteristics and having its own behavior and so on  refer slide time  05  07  but that ’ s the implicit assumption that we are going to make but that doesn ’ t really necessarily mean that object oriented databases are suitable only for cad applications  of course there could be several other applications as well so let us come back and look into what kind of complex data objects that we are talking about when we say that we are going to generalize or we are going to move away from the relational model and look into other kinds of models have a look at the first example here say it ’ s a multimedia databases what do you understand by the term multimedia databases ? databases that store multimedia objects what do we mean by multimedia objects ? you might have  you would have encountered several kinds of multimedia objects  if you have let us say work with any gui based operating systems like say windows  windows xp and so on where you encounter objects like menus  scroll bars  drawing areas then something like  when you click something that there is a sound that appears and there is a for specific kinds of events  specific kinds of sound and light so to say  a messages are thrown to the user and so on and there is flash animation and so on and look at the second kind of application that we have talked about that is namely the cad what would the typical cad application for electronic design comprise of ? a typical cad application  in a typical cad application the user should be able to let us say select a pcb a printed circuit board or select a resistor or a capacitor or a transistor or a particular kind of ic and voltage source and control grounds and current sources and so on and so forth now the main theme or the common theme between these two applications is that both of these applications are made up of fundamental objects which form the building blocks of these applications so multimedia applications are built from several of these different objects  they could be menus or scroll bars and so on  refer slide time  07  31  similarly cad applications or cad projects in a sense or multimedia projects and cad projects are in a sense built using this fundamental objects cad projects would have a pcb or ic and so on and so forth so what are the characteristics of these complex data objects ? it is quite apparent that these data objects are not easily amenable to reduction to a tuple that is we can ’ t really reduce all of this to one set of  one tuple having a set of characteristics because there is much more to an object than a set of different attributes so what is this  what comprises this much more essentially the idea of behavior of an object ? a transistor for example behaves as is represented not only by a set of attributes saying what kind of a transistor is that or is it pnp or npn or so on and so forth whatever else goes into describing the attributes of the transistor  in addition the transistor also has a particular kind of behavior you can apply voltage at one of the pins and measure the voltage at one of the other pins and so on and so forth so an object does not simply represent a set of attributes but it also abstract  rather an object is not merely a structural abstraction but it is also a behavioral abstraction that is when i say transistor  the kind of behavior that the transistor emulates is also abstracted by the object and in an object there are several instances of an object that can belong to the same class so i could have several different instances of the same transistor and each of this different instances may have different set of attributes at any given point in time that is the instance variables of each of these different transistors could be different that means each of the transistors belonging to this particular class has different states at any given point in time  refer slide time  09  56  so let us briefly take an overview of object orientation concepts object orientation concepts  here i am essentially looking at from object oriented programming point of view the idea of object orientation came from programming and there say several kinds of oopls or object oriented programming languages which were started right from the early 70 ’ s and so on so let us look at object orientation concepts from a programming point of view and then we shall look into how each of these changes when we consider object oriented database systems now the fundamental building block in an object oriented system is of course the object but then an object represents an instance and an object belongs to a particular type and here this is called a class so an object can belong to a particular class or rather we define specific classes of objects and then we instantiate different objects of specific classes for example we could define a class of objects called cars and we can instantiate an object of type car which specifically points to one specific car rather than the type of all possible cars then the idea of an object is to provide an abstraction to the user so depending on what the application is when an object of type car is created  it represents an abstraction called car that is each car is supposed to have a certain properties not just structural properties in terms of what attributes they have but also behavioral properties  what can you do with the car and so on for example take something like menu in multimedia database so menu is an abstraction that is it not only says what are the attributes that make up a menu but what is the behavior of the menu as well that is the menu should provide a list of items and there is a default selection by the menu and the user should be able to scroll up or scroll down the menu and so on and so forth so an object provides an abstraction or an object emulates an abstraction of not just structure but also of behavior and how is this abstraction provided ? through the notion of encapsulation that is an object encapsulates structure and behavior within its fold that is an object is defined by a set of attributes which define the structure that is described by the object and a set of methods or function calls that operate on these variables which define the behavior that is abstracted by this object and of course there is the notion of an interface that is interface is also called the signature of an object that is an object only exposes or the only thing that theoretically at least that is exposed to the outside world is the interface of an object and all external world entities should interact with the object through the interface by calling particular methods and changing attributes and so on so again just to give an example or just to give an analogy  a car for example gives an interface in the form of a steering wheel and a gear so you can interact with the car only through the steering wheel and the gear and of course the pedals that is the brake pedal and the clutch pedal and so on you can not directly go  when you are driving a car you can not directly go and manipulate how the engine behaves for example or how the wheels behave and so on you have to deal with the car through its interface so interface is the  as far as the user is concerned the interface is the signature of the object if you want to learn to drive a car  you should know how to handle the steering  how to handle the break and clutch and gear and so on and so forth so you should be able to know how to handle the interface rather than what lies within the interface and of course interface  in software interface is made up of methods which are function calls which change the state of the object and methods themselves have particular signatures that is each method requires zero or more input parameters and has 0 or 1 output parameter as well so that forms the signature of a method and attributes of a class is the set of variables that define the state of the class for example again in a car the attribute would be something like which gear the car is in or what is the speed of the car or what is the acceleration of the car and so on and so forth which basically describe what is the current state of the car so any method call would change the or would influence the attributes of this object and object state of course is a function of what are the values of different  each of these attribute so the state of an object is something like let us say the state of the car can be defined as say cruising when speed is so and so and the gear is in over drive and so on and so forth so basically you define set of values and say now if these are the set of values then this is said to be the state of an object and there are again some more concepts pertaining to object orientation which may also be important that is some notions of say message passing so when an external world entity invokes the method of an object  it is said to have pass the message to the object and the message in turn invoke the object that is invoke method of this object and some more concepts of object orientation which are particularly useful are the notion of inheritance  polymorphism and over loading that is when we define a class  we can define a generalization specialization relationship which we also saw in let us say the enhanced er model where a generalized class represents a general  more general entity than its specialized classes  refer slide time  15  21  that is wherever an object of a generalized class is required  it should be safe to substitute it within an object of a specialized class therefore suppose i have a generalized class called say suvs or whatever sports utility vehicles and so on so and there could be different classes of suvs whatever qualis and scorpio and so on and so forth  so several different kinds of suvs and what constitutes a correct generalization specialization relationship ? wherever i need an object of the generalized class it should be safe to substitute it with an object of the specialized class so the specialized class is said to have inherited properties from the generalized class and of course extended on the properties or over ridden on the properties and so on so a specific property or a specific method for example again coming back to cad databases or which we said we are going to have a running example so suppose i have a cad database and i have a generalized class called say a transistor and i have a specialized class called a specific kind of transistor of some particular number so whatever behavior is specified by the generalized class can be actually overridden by the specialized class and in a sense the same method signature seems to be giving different kinds of behaviors depending on which object of the specialized class is substituted so that brings in the notion of polymorphism that is the same signature  method signature giving rise to different kinds of behaviors that emanate from the system and there is also the notion of over loading which is a feature that present in many object oriented languages where slight changes in method signatures can be used to perform different classes of the same activity for example we can say something like add now when i say add to  i can say add int  int where it takes in two integers and gives out an integer now the same add could be defined as int  float or float  int or float  float where you can add different combinations of integer and floating point numbers and return back so at run time the message passing framework is going to determine which kind of add is being called depending on what is the type of the parameters that is passed and then there are pure object oriented languages where everything is an object there are no what are called as native types so every single entity like an integer  every integer or every character is an object and there are no native or fundamental data types other than objects and then there are hybrid object oriented programming languages where which do allow native types and of course what may be termed as semi object oriented programming languages where you can perform both object orientation and procedural programming in the same language  refer slide time  20  05  now let us come to object orientation as pertaining to databases now what extra features to be required in the concept of object orientation  when we talk about databases ? the main concept that is required for databases is the notion of persistence of an object or persistent object what is the persistent object ? a persistent object is something that can exist persistently or permanently that is the objects can exist even after the program using the object has finished that means the object exist on some persistence storage like disk and can be recreated or can be reread back from disk whenever required now for storing persistent objects in object oriented databases  another important requirement is the notion of an object identifier that is it is important to uniquely identify each persistent object that is stored in the database you might think of this as a as a primary key as in that we discussed in relational database systems but there are some slight differences between an object identifier or an oid versus a primary key an object identifier or an oid is automatically created by the system whenever a new object is added to the system  whether the user specifies it or not on the other hand it is the user who specifies what forms the primary key in any database relation and of course in pure relational algebra  each tuple is unique that is a table is a set of tuples not a bag of tuples therefore in the worst case the entire tuple form a primary key for the table however object identifiers are separate attributes that is the entire object can not form the or can not uniquely identify given object this is because two or more objects belonging to the same type can have the same state and hence be indistinguishable as far as their other attributes are concerned but they still would represent two different objects for example i can always have two different transistors in any given circuit board which have the same input voltage at the same time or same input or output voltage at each of its pins at the same time however they are still different transistors so by default an object database is a bag of objects that is all attributes of an object need not necessarily uniquely identify an object so we necessarily require an oid or an object identifier and the way objects are stored in databases are as far as possible should be in direct correspondence to real world objects so i can store an object like transistor or capacitor or pcb or whatever  so where you have direct correspondence to what one can see tangibly in the real world and of course there are several different attributes that define an object which alter the state of an object and of course these variables for attributes are defined at a class level whereas when an object is instantiated  these become instance variables and the instance variables of different objects could be different even though they belong they represent the same attributes and just like an object oriented programming languages  objects are defined by signatures which are the interfaces of objects and even methods have signatures that are defined for the objects and every other notion in an oopl are also reused here something like the inheritance and reuse of objects that is when you inherit the base  the derived class or the specialized class reuses certain properties of the base class that is because it inherits certain properties of the base class  we can think of it as some kind of reuse  refer slide time  24  05  and there is also a notion of referential integrity in object oriented databases by the use of oids that is every oid  suppose an object a refers to another object b  this reference is captured by putting the oid of object b as an attribute of object a and referential integrity is enforced by ensuring that at every point in time  the oid that is represented as an attribute in any given object is always a valid oid and of course there is operator polymorphism and overloading and other concepts that are common in oopls so let us come back to the object identity aspect like i said before the oid is mandatory in any object oriented database system and this is usually a system generated unique identifier that is the user need not even be aware that there is an oid that is created for each object however the system by itself creates unique object identifiers and of course the oids have no relationship to the values of attributes that is the set of all values of attributes of an object need not necessarily  uniquely identify a given object  refer slide time  25  53  and usually oid is a logical number and it is not advisable to base oid on the physical address of an object suppose i have stored object in a particular directory tree  we should not keep the directory tree as the oid of an object because of several reasons like if the database is migrated to a different system or if the directory tree is changed then the oid changes and the object becomes in accessible now like i said before every instance of an object is characterized by a state of an object now how do we define the state of an object in terms of the database or in the database parlance ? this slide shows  refer slide time  27  00  a formal model of how the state of an object is represented or in a sense the structure of an object the structure defines the state space in a sense so the different kinds of state that an object can be so an object structure is defined by a triple comprising of three values i  c and v where i is the object identifier and c is what is called as the type constructor and v is the object state so i is the well-known oid that we have been talking about and c can be  c is what is called as the type constructor that says what type of type in a sense or what type of value is this going to be and usually object databases define different kinds of type constructors like atom and tuple  set  lists  bags  arrays and so on  refer slide time  27  52  an atom type for example defines a specific atomic value so i can say atom and then give a value of 5 for the value so this object represents an atomic entity whose value is 5 on the other hand i can represent a tuple also as an object so instead of one single value  a tuple represents a list of values  an ordered list of values and list of atomic values essentially and a set is an unordered set of values  unordered collection of distinct values that we can take up and list is similar to a tuple except that in a tuple  the size is fixed  the size of a tuple is fixed but in a list different instances may have different sizes for the sequence of values that we can take and bag of course is a multi-set that is a set with a set with repetitions and so on  refer slide time  29  27  so this slide shows some examples here where we defined all these things already that is when type c is atom  object state v would be one particular value from a domain of basic values and when it is a set  it is the set of values and so on now this slide shows some examples here let us say i define an object o1 as a triple where i1 is the oid of the object and the object is of type atom and the value of this object is chennai that means this object essentially stores an atomic value or an atomic entity called chennai as part of this object  refer slide time  29  36  similarly o2 has as oid of i2 and it stores an atomic value called 35 and o3 is a tuple wherein each element of this tuple is an oid that is i1 is a oid belonging to the class called place so o1 that is i1  look here that i1 refers to this i1 here so this i1 is an object  i1 basically represents an object called o1 and o1 belongs to a class called place and similarly i2 represents an object called o2 which belong to a class called num so this o3 is a tuple of different oids where different  in a sense it ’ s a composition of different objects of different types in the form of a tuple similarly o4 is a set comprising of 3 different oids o1  i1  i2 and i3 so as you can see here  it is possible not only to represent specific atomic entities it ’ s also possible or rather or even collection of atomic entities  it is also possible to start composing objects  one object inside another for example o3 in a sense is a composition that is made up of o1 and o2 and similarly o4 is a set or a collection that contains all three elements that is o1  o2 and o3 so depending on these kinds of associations between objects whether it is a composition association or some kind of a whatever other kind of association that we can define  an object database can actually be represented as a graph structure  so where each object in turn has some kind of an association whether it ’ s a containment or inheritance or some other kind of an association with other objects in the database and when we are talking about the states of objects  remember i had mention that two or more objects may have the same state but that doesn ’ t necessarily mean that they are the same object because as long as their oids are different  they essentially refer to different objects so this slide shows such an example  refer slide time  32  41   so at any instance of time i may have two different objects o1 and o2 whose states are the same that is they represent one atomic value whose value is 35 and there is one more object of the same type called num that we defined in the previous slide which represents a value called 20 however even though both of these have the same state and this has the different state  all three are different objects namely because the oids are different i1 i2 and i3  refer slide time  33  17  similarly  here these objects i1 i2 and i3 or i2 i3 and i2 or whatever  so these two o4 and o4 here have the same state that is a1 i1 a2 i2 a3 i3 and in this case it does represent the same object why because i4 is the same as i4 here so at the end of it is just this oid which determines whether two objects are the same even when this regardless of what is the state of this objects and different object oriented database systems provide different mechanisms for defining custom types or custom classes  refer slide time  34  09  so here this is some kind of pseudo code for particular kinds of object database systems and later on we will be looking at one particular standard for representing types  namely the omdg standard but the idea here is that the user can define his own types for example the user defines an object of type employee comprising of different attributes that is there is first name  last name  salary  supervisor and so on and so this forms the tuple of different attributes that represents an object of type employee similarly  there is nested declaration here that is department is a tuple comprising of department name and department number and manager which itself is a tuple comprising of an oid of an object of type employee and start date and so on so that was  you might have got a question now that what is the difference  what really is the difference between declaring objects or types using what we saw here and with the relational data model itself that is both seem to be different ways of doing the same thing that is defining a set of attributes  refer slide time  35  37  however object databases differ from  in one important factor from relational databases namely that of object behavior so you need not  when you defining a type of an object like say employee and department  it is not just the attributes that you define but also the set of behaviors so let us look at what is the importance of behavior when it comes to object oriented database systems now object behavior is abstracted by a set of methods and which is visible as the object interface to the external world now the interface as i said before is also called the signature of an object that is each object should have a unique interface  each class which uniquely identifies what are the kinds of behavioral abstractions that it provides so for example if i have a object of type ic of a particular ic type  let us say some kind of let us say logic gate ic 7404 so this object has particular kinds of behaviors that is you can provide input voltage to a particular pin and you can provide ground to a particular pin you can provide inputs  logic inputs to particular sets of pins 7404 basically implements and gates and so you can basically provide logical inputs to certain pins and get logical outputs from certain pins and so on so probe an input pin or input voltage to a particular pin  all of these are methods that are abstracted by the object and of course when we are talking about attributes itself  by default or in pure object orientation  every attribute of an object is actually hidden from the external world that is the external world can access an object only thorough its interface or only through its method declarations but in reality though some attributes are visible to the external world  while some attributes are hidden from the external world that is which can be accessed only through method interfaces in most object oriented databases  the database management system allows the user to specify the interface of an object along with the attributes like the attributes here  refer slide time  38  17  first name  last name  salary and so on the user can also specify a set of interfaces and the implementation of these methods that is method declarations are provided here and the definition of these methods or the implementation of these methods can actually be returned or can be returned elsewhere using a programming language or method definitions can be done using any standard programming language like c plus plus or java or so on so the object database itself does not provide primitives or need not provide primitives to define methods but rather you can actually use an existing object oriented programming language in order to define a method interface so defining a method interface  now i mean embedding methods in addition to attributes will now enable us to define a class rather than a particular type  refer slide time  39  21  so one can define a class here for example this slide shows the definition of a class called department where the classes certain attributes like tuple which is a tuple of attributes which basically contains department name  department number  manager which is another tuple in projects and so on in addition there are certain attributes  certain operations that are also defined like number of employees is  number employees is the name of the method which returns an integer so when the external world calls this method  an integer is returned which essentially says what is the number of employees in this department similarly create department which is what is also called a constructor method that creates and instantiates an object of type department and put some default values in several  one or more of these attributes and assign employee that is add an employee to the department and so on so when operations are defined in addition to attributes  we get the definition of a class in contrast to a type and object persistence  so how are objects themselves persistently stored and referenced uniquely ? of course at the implementation level  the object database system uses the notion of oids that is when we refer to an object and if it is a valid object in the database it is given a unique oid  object id identifier  refer slide time  41  13  but what is the abstraction ? that is the oids completely hidden from the user that is the user or application program that is using this let us say the cad tool does not or need not have to know the oids of each object that have been instantiated and stored in the database instead the application program refers to each object by different kinds of mechanisms one well known kind of mechanism is by the use of a naming mechanism that is i can refer to a particular object like say ic 7401 or 7404 number 2 whatever so each specific object of 7404 that i have created can be given a specific number or this is the first ic or second ic and so on and like that  using that as a mechanism  the application program can uniquely refer to each object and the each unique name in turn translates internally to each unique oid on the other hand there is another kind of mechanism by which objects are referenced in in an object database system by the notion of reachability that is it may be difficult to give a unique name for every object that is stored in the database system for example if my let us say i am storing the circuit of a big computer like this  now it has several hundreds of components and this particular circuit is part of larger database of circuits and each different each of this different units or each of the object that are stored in this database has to be given a unique name and which might be impossible i mean it may not be a practical thing to do so another way of representing or referencing objects is through the notion of reachability that is let us say that one particular element in a circuit can be reached only through another particular element let us say transistor x can be reached only through the pin number 5 of this ic or whatever so we don ’ t  in such cases we don ’ t give a unique name to this transistor and instead we contain with just the name of the ic and any other object that can be uniquely reachable through the ic can be uniquely identified by naming the ic and then following the links so reachability essentially defines a sequences of references in the object graph that would leave from a well-known or named object a to an unnamed or reachable object b so this slide shows an example  refer slide time  44  16  where in some object oriented database systems where some objects can be declared to be persistent that is when we are working with an object oriented database system lets say cad application  a cad application is built around an odbms and the way of working with an odbms is seamless that is the user would be writing the application program and as part of the application program itself  the user would be interacting with the object database system so for example there are several  let us say there are several class definitions that make up this application program  refer slide time  44  59  and let us say one of these objects defined by this class should be persistent that is should be persistently stored in the database so you basically define this objects say all departments which is a persistent named object of type department set which is defined in the class here so as part of your application definition itself  you define which object should be persistent and which objects can be transient that is they lose their identity or they lose their state when the program finishes execution and of course this is using the pascal exsyntax where you can say d equal to create department where create a new department object in this variable called d and then make d persistent by adding it to a persistent set called all department so and then save it to the database system so in addition to these different features that are provided by an odbms like say type definition  class definition  method definitions and naming conventions and reachability and persistence and so on there are other kinds of features that are available in an odbms of what are called as type hierarchies and inheritances and so on so the concepts here are more or less analogous to the concepts in oopls itself that is whenever i use a type hierarchy  i am essentially referring to a generalization and specialization relationship  refer slide time  46  40  so a type hierarchy is defined by a subtype and a supertype that is i can define something like a student is a subtype of person and so where i could have defined person as a tuple comprising of name  address  age  social security number and so on and i can define student as a subtype of person where it contains all attributes that make up a person in addition there are attributes called branch and gpa which are important for defining a student as well and similarly i can one can define inheritances that is an object of type rectangle as a subtype of a geometric_object which is not just tuple here but tuple comprising of tuple in addition to certain behaviors and then you say a rectangle is defined by width and height in addition to whatever makes up geometric_objects then there is the notion of an extent in object oriented database systems extents is in some way to give an analogy to er modeling  in entity relationship modeling we had the idea of entity types and entity sets an entity type defined a type or a class of entities while an entity set is an entity type coupled with a collection of different instances of this entity type so the concept that is used here for an entity set is an extent that is extent is a collection of objects of the same type that is a type definition plus a collection of instances forms an extent  refer slide time  48  47  so the object database system is organized in the form of extents  different extents that is different typed objects are stored in their own extents and then usually because in most object oriented languages there is always a type hierarchy and there ’ s usually a root class or like in java there is what is called as the objects class there is a default extent that every object belongs to which is the object extent or the root extent and depending on the class hierarchy or the type hierarchy there can be different sub extents that can be defined on each of these  depending on the class definitions of each of the object that are stored in the database and of course the way in which objects are stored can either be structured complex objects  i mean now we are explicitly calling it complex objects that is objects which are not necessarily amenable or data that is not necessarily amenable to storage in a relational database form so  one can think of structured storage of a complex object or an unstructured storage structure storage essentially is some kind of a nested structure  a tuple comprising of other tuples or sets and so on so  some kind of structuring that is made out of the types that we define or the constructors  type constructors that we define something like atoms and sets and tuples and lists and so on  refer slide time  49  59  on the other hand there could be unstructured complex objects where especially multimedia objects where i could have a video sequence or an audio sequence and so on where there is no specific structure as such but it ’ s just one heap of data or binary data that makes up this object and there also called as blobs or what expanse to binary large objects so they are just binary data which are just stored and stored in the database and defined as part of this object now there are several different object database standards that exist and there was several different commercial implementations of object oriented database systems but many of them have in a sense gone out of business but quite a few of them have still survived and like i mentioned earlier  the main at least as of today the main application area in object oriented database systems is in cad applications where we need to store objects of a particular  having not only particular properties  a particular structural properties but also behavioral properties and this behavioral properties or the abstraction of this behavioral properties are extremely important when trying to build let us say an electronic circuit or a mechanical design and so on as part of a cad application until now we have been mainly talking about object database systems from a pseudo code perspective that is these are the features that several of these object oriented database systems have or had in a sense but more concretely there have been few standards that define what an object oriented database system should look like  refer slide time  51  29  and among them a well-known standard is the o odmg standard that is the object data management group standard and the idea of the standard is to enable certain kinds of features or certain kinds of properties that make up an object oriented database system so that they can be seamlessly ported across different object database management systems or odbms and the main idea behind the standard is the interoperability between different odbms and odmg 2.0 defines several different concepts  it defines a basic object model and an object definition and a query language and it also defines different kinds of bindings to programming languages so let us briefly look at what are the main or salient features of the odmg 2.0 standard and in the interest of time and brevity  we shall not be looking into great  we shall not be looking in great details into the standard but rather look at what are the main features that are provided by the standard  refer slide time  54  18  and odmg standard provides this  basically the idea here is the standardization of terminology  refer slide time  54  26  and as shown in this slide  objects in the odmg standard are defined by these different entities that is name  identifier  life time and so on and it also defines several types of attributes atomic attribute  collections  structures and so on  refer slide time  54  44  and interface definition is more or less the same that we saw as in the pseudo code and there are several default objects or default classes that are defined by odmgs standard  refer slide time  55  06  and of these a very interesting default object is the or default class is the collection class which defines a collection of different objects and this collection class has several methods that are defined like  you can query the cardinality of a collection  you can query whether the collection is empty or you can insert an element into a collection or remove an element from a collection and so on and so forth  refer slide time  55  26  and there is specialized built in  collection objects can further be specialized into different kinds of these  you can specialize a collection as a set or as a list or a bag and so on so all of these inherit the collection interface  refer slide time  55  41  so let us not go into each of these in detail  refer slide time  55  43   refer slide time  55  46   refer slide time  55  47   refer slide time  55  48   refer slide time  55  55  let us on the other hand look at some of the type hierarchies  look at the main type hierarchy that is defined by the odmg standard itself like in java there is a root object in the odmg standard which is object that is every object belongs to this class called object  root class which is called object and then collection is a special class of objects which represents a collection of objects and then there are several other kinds of objects like date  timestamp  interval  set  bag  dictionary and so on and so forth so this is a partial type hierarchy that is defined by the odmg 2.0 standard so let us summarize what we have learnt in this session today we talked about complex data objects and how they need not be amenable to a relational  reduction to a relational storage so we looked at object orientation concepts and wherein the notion of a state of an object and the oid of an object become important in order to be able to store objects then storage of objects are called persistent objects and how they can be accessed through naming and reachability and so on  refer slide time  56  36  and then we also looked at the odmg standard odmg 2.0 standard which defines its own class hierarchy of different classes so that brings us to the end of this session thank you database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 37 object oriented databases ii hello and welcome in the previous lecture we were looking into new kinds of data management problems where the kind of data that we have may not be easily amenable to reduction to a relational model we were specifically looking at cad databases that is databases in computer aided design where the uod or the universe of discourse is better described by a collection of objects and some kind of relationships between objects rather than a pure relational schema  refer slide time  01  39  and how is an object different from a tuple in a relational schema well  a tuple is part of an object but an object basically a tuple is an abstraction for particular structure of attributes  the way in which attributes are put together to form a tuple but then an object is an abstraction for not only structure but also behavior that is an object represents some kind of a complex data entity like say an electronic component like an ic or a transistor or something like that where it ’ s not just the attribute or it ’ s not just the structure that ’ s important but also what kinds of behavior that the object performs i mean you can ’ t expect a transistor to work as let us say something else i mean  i don ’ t know  may be some kind of a current source in some other form that is there is a specific set of behaviors that are associated with an object and in conjunction with the attributes  it ’ s also the behaviors that the objects provide an abstraction forum and we also saw different notions that define object oriented database systems for example in an odbms it is mandatory or it is essential to have a unique identifier for each object  for each persistent object in the database and this is exemplified by the oid relationship and oid is contrasted from a primary key in a rdbms by the fact that an oid need not be explicitly specified that is the user need not even be aware that the database system is tracking each object using an oid and just because the state of two objects are the same that is they have the same set of attributes and the same set of values for each attributes doesn ’ t necessarily mean that they are the same object which is very much unlike the case in relational algebra were two tuples that are the same  represent the same data object essentially or the same data element but here even if two or more objects have the same state  as long as their oids are different they represent different objects and then there are other issues like say inheritance and polymorphism and so on where or type hierarchies were an object can actually derive or specialize from a more general object and essentially as correct specialization is one where  wherever in the system you require an object of the general class it should be correct to substitute an object of the specialized class as well and there are other issues were the database system can be represented as a graph by associating different objects based on different relationships like containment and inheritance and other kinds of association and associations are made using oid references that is object a associates with object b if the oid of b is an attribute of object a and an odbms essentially is tightly integrated with opl or an object oriented programming language where there are some kind of programmatic constructs that are provided to the language where the programmer who is writing the application program can identify certain objects to belong to the database in the sense that these objects can be declared persistent as soon as they are defined that is as soon as they are instantiated so as and when they are instantiated  they are also associated with an object in the database rather than just in the programming system  other objects which are not persistent are called transient objects now having looked into all those  we started looking into some of  we started getting little bit more concrete and started looking into the object database management group standard  there is the odmg 2.0 standard for defining object databases  refer slide time  07  01  let us look at this standard in a little bit more detail today or in this lecture and look at what other options does  does the odmg standard provide first of all let us look back at what is the odmg standard which is slow shown in the slide here the odmg standard or the object data management group standard 2.0 model is a model essentially meant to standardize the notion of object databases and the main idea or the main reasons for this include portability and interoperability between different object databases and odmg 2.0 standard defines an object model and a language for defining objects and querying objects so object definition language and object query language plus it also defines a number of bindings to existing programming languages like c plus plus and smalltalk and so on where application programmers programming in any of these languages can directly interface with odmg objects or in odmg databases system so essentially like we said before  the object model is meant for meant to be a standard so that the terminology is standardized and then which in turn help aids in interoperability and portability of database models  refer slide time  07  56  so in the odmg 2.0 standard  objects are described by a set of different values of course an object has the well-known oid or the identifier and an object has a specific name that is associated with it plus also a life time life time in the sense you can either call an object as transient or persistent so a persistent object is said to exist even after the software or the application program has finished its execution in a sense permanently and of course the structure of the object and like we saw in the previous session  there are different kinds of data types that are usually supported by an odbms  refer slide time  08  34  and the odmg 2.0 standard supports different kinds of object types like  literal types like atomic attributes or atomic objects  collections and structures this slide shows a typical interface definition of an object in an odmg  refer slide time  09  22  remember that the interface is the signature for a persistent object that is the interface tells the external world how to interact with the object and the interface for example shows here  the interface does not show type declarations however it is showing the operation declarations like there is an operation called equals which takes in an object of type object and returns true or false and there is a method called copy and a method called delete and so on and the odmg 2.0 standard defines a set of or also proposes a predefined set of classes along with the class hierarchy  refer slide time  10  19  and one of the classes in the class hierarchy is the class called collection and as we saw again in the previous session at the top of the class hierarchy is a class called object so every object in this  in an odmg database by default belongs to the object class that is even if you can ’ t resolve in any other way which a class that a particular object belongs to  you can always say that an object of the odmg database belongs to the root class which is the object class a collection is another derived class or a sub class of the object class which actually defines a collection of different objects that is a collection class can have a set of different objects or objects of the object class and then the collection class also defines a number of methods like say cardinality of a collection  is_empty which checks for whether the collection is null or is empty then it supports insert_element where you can add an object into a collection  remove _element where you can remove an element from a collection  contains_element the searching element for a particular object and creating_iteratives we will come to iterative soon  what exactly is meant by an iterator and what are its use if you have program with this standard template library in  say in c plus plus for example you probably have guessed what an iterator is where it ’ s a kind of a template using which you can iterate over several different objects of any kind of a collection there are other kinds of built in interfaces that are also specified by the odmg standard and collection objects can also be further specialized into different kinds of these classes like say a set of collectors i mean set of collection or list or bag or array or dictionary and so on  refer slide time  12  25  so  all these are derived from the collection interface collector collection interface that is the collection class interface we now come to some iterator mechanism that is or rather the requirement for iterators if you are familiar with c plus plus programming  you would have probably come across the notion of a template wherein a template is in some sense a definition with a hole or a method or a operator definition containing a hole where different other objects can come and fit into the hole or different other objects of other kinds of classes can come and fit into the hole  refer slide time  12  59  so for example if i define a template called list  i can have a template i can have a list of different objects of class a or objects of class b or any of any kinds of objects so similarly the odmg standard defines different template  the first of which that we are going to see is the set template so set as shown in the slide here  refer slide time  13  55   set followed by a template mechanism here will create a set of classes of this particular type note the subtle difference between a template like this and a collection a collection by default is a collection of objects that is it just reads every entity in the every entity in the in the collection to be an instance of type object however every class or every object in an odbms is definitely an instance of type object because object is the root or is the top most class definition in an odmg database now therefore a collection can be a collection of potentially objects of different classes at lower level so you can have a collection comprising of one car plus one transistor plus one ic plus one truck which is very well valid on the other hand a set here is a set of only a specific kind of objects  so you can say that it ’ s a set of cars so you can ’ t really you can ’ t really include an object of class transistor in a car unless of course for some strange reason  transistor is a derived class of car and the set template also defines different kinds of operations for over which are basically set operations like create_union or create_intersection  create_difference or subset_of or proper_subset_of  superset and proper_superset and so on similarly there is just like the set template  we have the bag template where bag is like a set as we have seen earlier that bag or a multi set is a set which can allow duplicates in the collection  refer slide time  15  56  however duplicate should also be objects of the same type or of derived types which is specified in the template  it can ’ t be arbitrary objects and of course bag also specifies different kinds of methods like create_union and create_intersection  difference and so on again like set and bag  we have the list object type where unlike set and bag in a list  list is not just a collection of different objects  it ’ s an ordered collection of different objects that means the ordering between objects is also important so you can insert an object at the head of a list or you can insert at the tail of a list  you can insert after an element  after a particular element in a list or you can remove the first element and you can remove a particular element at some particular level in the list or some particular location in the list  position in the list and so on and so forth  refer slide time  16  31   refer slide time  17  24  and of course the array object type which is again array is like a list where the order is important  it ’ s a collection of or it ’ s a set of different objects or of the same type and where the order is important and usually the number of elements are fixed unlike in a list where the number of elements can vary there is also a dictionary data type which is derived from the collection data type which is analogous to a hash table implementation so it basically stores a collection of key value pairs and given a key  the dictionary returns a value and the dictionary object also defines a number of different methods like bind  unbind  refer slide time  17  42  bind essentially means that you are associating  you are inserting a value  you are inserting a new key value or rather you are binding a value v with key value k and you can unbind in a sense delete the value associated with key value k and you can contain  i mean you can look up the value of a key k or you can check to see whether the key  the given key is actually present in the dictionary or not and this slide which we also saw in the previous session shows a typical  i mean shows the class hierarchy that is or rather it ’ s more like the interface hierarchy that is the odmg does not define a complete classes rather it just defines interfaces and the definition of each methods is the responsibility of the programmer that is it can be the  definition can be written in any high level object oriented language like c plus plus or java or whatever depending on what support is available of course but this defines an interface hierarchy that is any sub interface in a sense that is any interface somewhere in the hierarchy will inherit all the interface elements like the method interfaces of all interface above in its hierarchy  refer slide time  18  44  so as seen in this figure here  refer slide time  19  44   the object interface is the top most interface so every object here is a kind of a  can cause a little bit of confusion in the sense that there is a class called object so every object in a database belongs to a class called object by default and a collection is a collection of different objects and sets  lists  bags  arrays and dictionaries are different specific kinds of or kind of templates where they define specific sets or lists over specific types of objects and then there are several other objects like  classes like timestamp and time and interval and iterator and date and so on the odmg 2.0 standard also defines or also proposes an object definition language plus an object query language as we had mentioned earlier in this session so the odl or the object definition language that is defined by the odmg 2.0 standard is a programming language independent mechanism of defining the structure and behavior of objects what is meant by programming language independent mechanism ? that is you can have the same odl definition interface with let ’ s say smalltalk or java or c plus plus or any other programming language and it supports different kinds of schematic constructs that define structural elements of objects and it can specify the database schema  refer slide time  20  57  note here that the database schema is actually a graph of different objects belonging to different classes and their relationships so it can specify a database schema that is also independent of the programming language and it has interfaces with specific languages like c plus plus and smalltalk where you can use language bindings for mapping the odl constructs  refer slide time  22  25  the odmg standard in addition to the odl also defines the oql or the object query language object query language is a mechanism by which you can query on either attributes or behaviors of particular objects  usually for attributes of course and it ’ s a query language that ’ s specified by the odmg data model and it can be integrated with existing programming languages like c plus plus and smalltalk or java and so on this slide here shows an example oql query where note that for the programmer  the programmer is not necessarily aware of the oid mapping or the oid management that is performed by the object database system  refer slide time  22  33  instead a programmer refers to an object or a unique object in the database using things like an entry point or a reachability condition and so on so an entry point in an oql is a named persistent object that is that defines the entry to which the specific object with the particular oid can be accessed so here in this case entry point is this object called d so d is the name that uniquely defines an object that is stored  uniquely defines a persistent object that is stored in the database and there are iterator variables that can iterate over or that can range over each object in the collection as you can see here  refer slide time  23  43  the structure of oql is very similar to sql itself where you just say select d.dname from d in departments where d.college equal to engineering except that here  the way it interprets this query is different from the way queries are interpreted in sql or in the relational model here d in departments essentially means that d is the name for all objects that are in the extent called departments remember  what is an extent extent is analogous to an entity set that is a collection of different objects of the same type or the same class as is  so when i say d in departments  d is a iterator variable that is it iterates over every oid of objects that belong to class departments and then it selects a particular attribute from that note here again that dname should be a visible attribute if it is an invisible attribute you can access attribute names only through method invocation so you should be saying something like d.get name  select d.get name from d in departments where d.get college equal to engineering and so on so however we are assuming that d name and college are visible attributes rather than hidden attributes in the object definition just as in sql  one can define views over the over the odbms using the oql and this is quite similar to sql query except that rather than say in create view  you are saying define  refer slide time  25  58  and look at how views are essentially created a view is created in the form of a method that is you are defining a method called has_minors department name as this oql query that is has_minors department name is select s where s is an iterator variable from s in students were s.minors in d dname equal to department name and so on so you are basically defining some kind of a method when you are saying that you are defining a view so you just invoke this method  you just invoke this view as though you are invoking a particular method so you just call a function called has_minors like this and the output of this is the set of all students where those students are minors under working in a department that is what is specified by the query here  refer slide time  27  23  similarly when a query returns a set of different objects or a collection of different objects  you can specify one single element of this collection and then use it as a separate declaration this slide shows an example of such a situation where there is a nested query  nested oql query here which says select d from d in department where d.name equal to computer science d.dname equal to computer science again  so as usual d is a iterator variable that iterates over all objects or all oids of objects in the extent called departments now suppose there is only one department called computer science  this query returns exactly one object and you can define this object as a specific element and use it as an attribute of some other object if required and that is the reason or that is the use for such a declaration and of course such a declaration will raise an exception if this query actually returns more than one elements or on the contrary  it does not return any elements at all that is if there is no department called computer science or two or more departments have a name called computer science again some more operators that are specified by oql are the aggregation operator or rather what are called as the collection operators where you can have aggregate operators just like in sql like min  max  count  sum  average and so on  refer slide time  28  40  so you can get the minimum of all values of min over certain attributes and max over certain attributes and so on and similarly count for example  count of something like this where the in command we have not actually see here but in is this operator that test for set membership whether s is a member of has_minors that is the set called has_minors and note that we had defined has_minors as another query and you can as a view defined by a oql query and you can just have one more element like this which just checks whether any given element s is in has_minors and average over this thing and so you are selecting s.gpa where gpa is a numeric attribute and of course again just like we mentioned earlier gpa is a visible numeric attribute if it ’ s an invisible numeric attribute then there has to be a associated method that has to exist in order to return the value of gpa so you should in effect say s.get gpa or something like that and once you have got the set of all gpa values  you can take the average of this and of course all this aggregation operators have to be applied to operators of the or collections of the appropriate type so even though collection can be collection of arbitrary objects  i really can ’ t apply average over a collection of objects comprising of 3 numbers and 4 animals and whatever so it has to be of the particular type and so averages are generally are average or min  max and sum and so on are best defined over the set t where t would point to a numerical attribute or numerical object like number and so on  refer slide time  31  34  similarly you can define memberships and quantifications operators in oql and for example  you can define something like e in c where c is a collection object and e is an element and this we saw it in the previous slide where you can actually  this expression actually returns true if e is an element of c and similarly you can say  for all v in c  b that means in a sense its checking whether for all v in c does the predicate b or does the condition b hold true so it is true if all elements in collection c satisfy this condition called b now what is this condition ? this can be again another expression like this  that is another conditional expression were e in c or e in v and so on where v is used as part of this expression in b and similarly there exists or exists v in c such that b is true that is there exists at least one element in c satisfying b therefore as you can see these are quantification operators that are for example typically used in predicate logic were by you can specify different kinds of queries there exist and for all and that ’ s existential and universal operators that are defined in predicate logic  refer slide time  33  02  then you can also define ordered collection expressions for example here you are selecting a particular struct that is a particular attribute from an object which is a struct where f is the set of all or f is an iterator over all oids in the extent called faculty and you are ordering by faculty.salary and designation or description or whatever so desc can stand for and then from out of these you are taking the first element so essentially out of a sorted list  you can take  you can look at any specific elements like first and second elements and then get or in effect you are seeing which is the faculty member who has the highest salary and so on and there are of course  we saw that an oql is conceptually different from an sql query that is an oql even though it looks very similar to an sql query  the way it interprets the way an oql query is interpreted is quite different from the way an sql query is interpreted and in oql we define extents and attributes and iterators and so on whereas in sql you have a tables and attributes and tuples and so on and there are other differences between odbs and rdbms ’ s especially in how relationship properties are handled  especially foreign key relationships and other kinds of relationships and in a sense an obd database or odbms is an un normalized database in the sense that it is not even in first normal form  refer slide time  34  31  remember that a relation is in first normal from if it is not a nested relation that is a tuple does not contain nested tuples however an object can contain tuples which can contain other tuples and so on to any levels of nesting so it ’ s not in first normal form and the way relations are handled in odbms ’ s are mainly through oid references and oid references have little  if any to do with normalization in an odbms and then the handling of inheritance ; inheritance is an integral part of odbms ’ s whereas inheritance is an alien concept in relational database so you don ’ t really inherit  you don ’ t really see types and classes inheriting or tables and tuples inheriting from one another and of course the specification of database operations and there are also certain techniques which talk about how you can map a conceptual schema written in the er model or the extended entity  eer model extended entity relationship model to an odb schema rather than a relational schema  refer slide time  36  18  we shall not be going through this slide in great detail though except to note that just like each entity look at the first step here where each entity in an er schema corresponded to one table in a sense in the relational schema here it roughly corresponds to one class for each eer entity in a sense and relationships are referenced  relationships are essentially shown using associations rather than say foreign keys or having the same values and so on and several different steps which are or several different thumb rules which specify  which tell a designer how to map from an extended entity relationship model to a odb schema and we shall not be going  looking at this in great detail  refer slide time  37  10  the next topic that we are going to see which is again of quite a bit of importance is the object relational databases or also what are called as ordbms ’ s and ordbms ’ s are in some sense a middle path between rdbms ’ s and odbms ’ s and both of them have their own pros and cons odbms ’ s are better suited for handling different kinds of complex objects like blobs and say and also in providing say behavioral abstraction where transistor is defined by a particular kind of methods and so on which are not present in the rdbms space however an rdbms by itself has its own advantages which can not be matched by a specific  by a pure odbms database for example concepts like normalization and query handling and indexing  storage structure and so on where lot of work has gone into specifying not just the relational data model but also building an implementational scheme around the relational data model and all of these would have to be reinvented for at least a large part in the object database realm  if pure object orientation is being considered now the middle path is to use object relational database that is can we use both objects within relations and vice versa so  an object relational database again are meant for applications dealing with complex data like satellite imaging and weather forecasting and so on and where the complex data is in turn associated with number of traditional data like which can be mapped on to tuples  refer slide time  38  57  however it ’ s not just the traditional data that is of interest here  it ’ s the complex data that is how these complex data can be manipulated that is what kinds of methods are there and how do we check the state of this data and so on so there are several extension that have been proposed for rdbms systems to make them compatible  to make them also to also support objects for example there are support for extensible data types  remember that inheritance is not an integral part of the relational model and there have been some efforts to introduce inheritances or data type extensions into the relational model and then support for user defined routines where the user can define certain methods by which data can be manipulated  refer slide time  39  50  and implicit notion of inheritance that is you can  inheritance as an integral part of the dbms system and some other extensions to indexing the traditional indexing of rdbms systems  refer slide time  41  01  so we shall be looking at one specific kind of ordbms as kind of a mini case study in a in a sense namely the informix universal server that ’ s from informix which provides extensions to the traditional relational data model by what are called as data blade modules the idea behind data blade is as though that blade actually means a racer in this context  that is the actual meaning of blade the concept is as though a new data types or new extensions can be cut through existing definitions as though like a blade cutting through a fabric so you can actually cut through something and then introduce new data types and there are several kinds of additional data types that are proposed for example the opaque type or rather the opaque table in a sense an opaque table is one where the contents of the table are hidden just like in an object  if variables are hidden or invisible then it becomes an opaque table now you can actually in a sense cut through an existing database system and introduce a new type called opaque tables and so that ’ s the idea of a data blade module and then there are distinct types and you have the notion of a row type that is you can define a tuple as a type and a collection type which is specified in the odmg standard as well then user defined functions are provided in the form of stored procedures  refer slide time  42  11  we already saw stored procedures when we are looking into case studies in rdbms ’ s so in fact many of today ’ s rdbms actually supports stored procedures which are in some sense  which in some sense can be argued to be some kind of support for object databases that is stored procedures in effect define some kind of a method that works on or that is applicable to tuples of specific kinds so here there is an example of creation of a stored procedure something like create function equal where you provide different arguments arg 1 and arg 2 which returns boolean and then you define your function and you also define the language in which the function is written where you say it is written in language c and then end function so note here that the stored procedure in informix universal server doesn ’ t really require the definition of the procedure to be specified here the definition can be specified in a third party language like language c  like c language and then it ’ s only the interface or the stub for the stored procedure which is available as part of the database itself  refer slide time  43  58  and the informix server also supports data inheritance as an integral part of its ordbms data model so for example here you are saying  you first create a row type row type is again a tuple type which is created like this so you create a row type as employee type which contains employee name and social security number and salary and so on and you are creating another row type  engineer type which says which has two other variables degree and license and then there is this key word called under employee_type that means  what this means is that the row type called engineer type is a sub type of employee type or it is a specialization over employee type that is in addition to the fields that exist in the employee type  engineer type has two extra fields like degree and license similarly you can also define function over loading as well then there are several kinds of indexing extensions that are defined and where you can create index not just on data values but also on user defined routines or user defined methods for example this slide here shows a declaration  shows an invocation where it says a create index emp_city on this function called employee city address  refer slide time  45  07  so this is actually a function over which the emp_city is being defined so it creates an index on the employee table based on the information given by the city method that is it first executes the city method on all tuples of this employee table and based on this information  you create an index that is some kind of a b plus tree index or whatever and there are several kinds of apis that the data blades provide and where you can something like two dimensional data types and image data types and so on there is also construct for abstract data types where you can create  where you can define an abstract data type along with specific methods to manipulate the variables or manipulate the elements of this abstract data type  refer slide time  46  02  the next kind of ordbms support going from informix universal server  let us move on to another kind of ordbms support namely in sql 3 or sql 3 standard sql 3 standard has a number of extensions from the earlier sql standard which provide some kind of constructs which are in some sense in line with ordbms requirements and for example sql 3 is divided into different parts like sql framework  sql foundation  sql bindings and sql object and it ’ s in the sql object part where number of ordbms support is provided as part of sql 3  refer slide time  47  12  and of course there are several other extensions that provides like temporal and transaction aspects of sql and persistent stored modules and call level interfacing and so on so as far as object relational support goes in sql 3  sql 3 supports a new data type called lob or binary large objects or a large object and so on where you can define some kind of a binary dump or binary data object to be a large object or a binary large object  refer slide time  47  48  so for example you can have a media file like a video or a music file and as part of an attribute type and reference it and call it a binary large object and there is also support for objects in the following ways where the first thing is the creation of row or tuple types that is just like in informix server  you can create a type out of a tuple so once you create a type out of a tuple  you can create abstract data types like a struct which contains different elements of type tuples so you can have different tuples that make up an adt and then in addition certain kinds of methods that manipulate this tuples so here are certain examples that are shown in this slide for example you can say this is an sql 3 by the way so you can say something like create row type where basically this is the tuple that is name is a varchar of 30 characters and age is an integer now this tuple is called emp_row_type that is this is a new type which creates emp_row_type and then you can simply say create table employee of type emp_row _type so it will just create a table of different tuples of this type and you can extend existing rows  again the notion of type hierarchies comes into play here where you say create row type comp_row_type and then you say employee ref employee row type that means you basically create another element and then just refer to the previous row type that you have defined and all the attributes that have been defined there automatically come into this definition here and again you can say create table employment of type comp_row_type or so with the new type that you have created  refer slide time  49  34  and in addition to row types  you can also define the notion of an adt or an abstract data type in sql 2 so adt essentially is a user defined type for a particular variable  for a particular attribute that is you can actually define a struct comprising of different tuples and different adts  nested adts in itself and then and then use that as a composite type  composite data object representing a type comprising of different attribute and so on and then an adt comes with built in functions like the constructor function wherein you can initialize and instantiate the adt then there is an observer function which can  which is basically like the get and put operation  refer slide time  51  20  so you can get the values of different hidden variables that the adt defines and mutator function which is the put function where you can actually change this thing and adts can also have  may have user functions associated within  refer slide time  51  38  and there are several kinds of implementation issues in the sql 3 specifications like suppose a user specifies a function or user defines a function then the implementation system  the dbms implementing sql 3 should dynamically link to a user defined function whenever required at run time and there are other kinds of client server issues as to where the function is defined  if the function is defined on the client and it has to be executed on the server or whether the function should be defined and stored on the server beforehand and so on and it ’ s not clear whether one can run queries within functions and of course efficient storage and retrieval of data is also important  refer slide time  52  31  so let us in a sense summarize the ordbms ’ s issues  one is of course that object relational database design is much more complicated than relational database design you might have already guessed why ? that is creating indexes over functions or defining inheritances and so on can hinder or hamper with the traditional relational database storage and access functions and query processing and optimization becomes more complicated when it comes to object databases especially because when user defined functions have to be run  stored procedures have to be run there is no way to further optimizer to know how much time or how efficient is a  what is the behavioral characteristics of the user defined function  refer slide time  52  49  so that ’ s the large unknown which the optimizer has to take care of when optimizing a query and of course there are the notions of triggers and transactions and integrity constraints and so on which can further complicate the matter of maintaining an object relational database system similarly because adts can define user defined types and an adt might may contain a tuple which in turn can contain another tuples and so on it basically an ordbms can become un normalized or de normalized because it ’ s not even in first normal form a first normal form each tuple has to contain atomic elements so it is necessary to remove the restriction of first normal form on object relational databases but internally even though that ’ s the abstraction that is provided to the user  internally perhaps it is still more efficient to map the user abstraction that is provided to an internal pure relational database schema so let us summarize what we studied in this lecture or in this class starting from the previous lecture on object oriented database systems  we looked into the odmg 2.0 standard and what kinds of object definition language and object query language constructs that it provides and we saw how oql queries although they look very similar to sql queries are interpreted in a very different fashion in an odbms  refer slide time  54  53  and the relative short comings and advantages of odbms  we saw rdbms prompted the emerges of ordbms or object relational database systems and we essentially saw two kinds of specifications  the informix universal server and sql 3 support for ordbms and of course ordbms themselves are by no means as elegant as the pure relational database that means they don ’ t have a nice specification and there are several issues like query optimization which is a major issue plus storage and retrieval and so on which still they have to contend with so with that we come to the end of this session database management system prof dr s srinath department of computer science engineering indian institute of technology  madras lecture no # 38 xml – introductory concepts  refer slide time  01  20  hello and welcome  in the previous two lectures  we have seen at different kinds of databases  or try to see what happens when we try to manage data  that not easily amenable to storage in a relational database and in this context we looked at object oriented databases where data that has stored or complex data object in the sense that  not only abstract the structure of some data item but also its behavior  what kind of behavior in this lecture and next three lectures  we are going to look at another important kind of data management issues that that occur in practice day in and day out i have we are which can be in a natural kind of captured as combined data management issues related to unstructured  semi-structured data and heterogeneous data sets and what are called as self describing data sets see  what happens in the real world is the set there are rather than having one specific database  even in specific  even in a given u r d or even in a given system context like let us say we have a huge company let us say huge multinational company which spans several countries or even not even not so huge company which let us say spans several cities they do not use one single database for the entire company ; it is quite impractical for several reasons  for historical and for several other kinds of for practical reasons  they use many different kinds of databases and many different kinds of data stores as it is in what happens is over a period of time  trying to reconcile these different data stores becomes more and more difficult and whereas it could well be the case set one part of the company is using let say windows based system  another part is after companies using is linux based system  another part is using mac system so on and you would not have  it would not be possible to create a single database system  which spans across all these different operating platforms in the works coherently and in a addition  in addition to the above huge amount of unstructured or in some sense semi-structured is cre  ate  is been created day in and day out in the form of letters  faxes  memos  webpage ’ s  emails  documents and so on and so forth so much of data is being generated  all of which can not be easily accessed or we it is not possible to simply define a database so that you put everything into this into this database what is required  in some kind of simple mechanism that can that can help describe a data element in a fashion that is independent of any operating platform or any encoding  i mean there could be different encoding as well i mean one could  one could be using asci encoding  one could be using unique code encoding  one could be using some other kind of encoding and so what are encoding  what you are be ah way data is being stored it should be able to  one should be able to reconcile between all of them in uniform fashion now what is how do we do that ? and how do we reconcile all of these different data elements under one  under one simple in a simple fashion for this the emerging answer over the last few years is xml or the extensible     language so we should be looking into xml and xml as applied to semi-structure data in the next in the following two lectures and xml is although it is so simple in practice where it would seem that there is nothing to it has become a very important source of data interchange and data representation and data description  in the post internet world       refer slide time  06  03  why xml and what are the  what are the reasons when or what are situation when when we can say that xml make sense  when we can use xml managing heterogeneous data source is very valid example  when there are several different data sources  let us say you are you are designing a data warehouse and you are taking transactional data from different     data bases or let us say you are trying to reconcile data bit from different different files where let say you have different spread sheet or word document or some other kinds of files and you are able  you are planning to reconcile all these data that is stored each of these different data bases and integrate them into one common data source  that is when xml comes really comes into      and similarly data self-description  what if there is no  what if you do not know or what if whoever is using your data does not know how your data is organized let us say your your employee record  what if whoever is requesting for your employee record does not know what are all the fields that is your employee record contains and what are the constraints on those fields and so on what if there is no single way by which employee record can be organized at all may be everybody has a different way of organizing employee information  one might say we require social security number as the primary key and in cases where in places where there is no such concept as social security number you could say something like pan number and or ration card number whatever our just name and so on so what all should go into some places have first name  last name  middle name  initial and so on and in some other places we just say name so  there may not be a single way by which we describe data about any specific entity so the based way to manage such situations is data self-description what your data that you are sending across between application in different context  let the data itself describe  in what way it is organize let the data in the meta data  that is data about data be sent in one packet that is be integrated in a way that just by looking at this packet of information we can not only get the data but also the way in which data is organized and the third major use of xml is semi-structured data management i mean several different kinds of data sources are not really structured documents for example documents  letters or faxes and so on where there is some resemblance there some resemblance of structures for example  letter should have from and to and some kind of addressing like dear sir and whatever and subject and complementary closing and so on  but beyond that text itself or what goes into each of them  there is no specific rules that says this is way the data has to be organized so such kinds of data bases or data sources are semi-structured data sources and huge supply of semi-structured data is of course the world wide web  i mean any html data that is that is written that is present over the or that is served over the world wide web is semi-structured data  that is there is no specific structure to html document  however part of the html document is some sense describes let say suppose you say h one and slash h one  it means there this is the first header  at the first level header semi-structured in a html document similarly  you say p and slash denote that the paragraph is started here and it is ends here and so on so there some sentence of structuring  but there is no rigid structure in that exist for the for any html document and to manage such data sources or manage such data xml becomes very important  refer slide time  10  42  so  what is xml ? what its properties ? xml are the extensible markup language it is actually a subject of an earlier markup language called sgml – standard generalized markup language what do you understand by the term markup language ? what is it mean by the term markup language ? i am sure ; you would have probably used the well-known html or the hypertext markup language in creating web pages even though there are number of web creation tools that are available today  most of us would have tried our handset working straight with plain vanilla html ; that is go straight to html and start changing the html document directly open it up in notepad or     or some text editor like that and open change it directly html is the markup language what is the markup language ? markup language is essentially  the name used for languages where the meta data that goes into describing the data is embedded within the data itself that is a html document  the meta data that is says which is the first level header  which is the second level header  which is the paragraph  which is the hyper link and where does the hyper link connect it and so on and so forth all these meta data is embedded as part of the data itself and it is embedded in this same form  that is using the same encoding and the same even the same font i mean if it is rendered by any text editors  editors are its rendered using the same font as them data itself is rendered so therefore if you are using unique code as your base then the entire set of data and meta data would follow unique code and let us said earlier xml is actually a derivative of the lesser known sgml the idea behind here xml although  xml is has become so famous and in the post internet era the idea xml for creates the internet itself and it tends from sgml which was the standard that was used for document management  how to how to manage documents within how to manage data and meta data within documents so there is several sgml based tools and ensure and sgml and related to for example  if you use linux i am sure  you might have used this software called info which is  which is based markup kind of language where which in some sense predates the world wide web in terms of hyper text usage within a single machine so  it was originally designed to be a flexible text formatting  text format for electronic publishing however  today xml plays a major role in data exchange seamless data exchange over through web and several different other application areas in fact  the de facto standard of exchanging application data over the web is fast becoming xml of course there are other standards  but xml because of its simplicity is faster     or taking over all other different standards of data exchange over the web  refer slide time  14  30  so what are  what are some other features of xml ? xml is essentially a cross-platform  entity which is independent of any software and hardware means for transmitting information that is it does not the way in which an xml document is stored is not dependent upon what software you are using  for example  whether you are working in windows or linux  and even hardware  what hardware you are using – that you are using an intel based pc  or macintosh having motorola based  macintosh powered by motorola processor and so on so  it does not really change  i mean across all of these platforms xml the structure and description of an xml document would remain the same an xml basically describe data in the form of one or more xml documents  where each xml documents is in the form of some kind of tree containing both data and meta data and xml is a markup language  but it is not a computer language ; that is not a procedure language however  there are it also  there are some kinds of computer program namely especially say xslt which read xml documents and behave in different ways accordingly for example  if we have c program or perl program and you run it through the perl interpreter  the perl interpreter behaves in different ways based on ah what is written in the perl program so  in a sense  so in that sense one can call xml as a programming language ; however it is not a complete programming language in that sense  it does not have all construct the makeup the programming language and hence it is just a markup language and xml can  can also use what is called as dtd or document type definition which is again being first replaced by what is called as an xml schema to actually describe the structure in which data has to be organized or to describe the data itself  refer slide time  16  57  so  what is xml contain ? if you worked in html  where html is a markup language for creating hypertext documents you would probably come across several different deserved tags that html itself provides  for example we talked about h one so suppose you     h one tags within      it means that whatever lies within this tags is the  is the first level header in the html document h one is for example  it is an example of reserve tag in html similarly  the tag called p defines the paragraph  p and slash p defines the paragraph  so again p is the reserved tag in html like that html has nearly hundred different pre-defined elements only using which you can define a html document on the other hand  xml has no pre-defined elements  that is there is no h there is no h one  h two  h three there is no p  there is no pr  there is no ul  there is no li any such element that is defined by  that is defined by xml in fact  it is the ownership or it is the responsibility of the creation a document to creative to actually invent tags or invent meta data that describes the data and it is completely up to the user or up to the creator of the xml document to describe or to find have to find what may be termed as the appropriate meta data terms in order to define the data that is available and that that is what makes an xml document self-descriptive we will see how that happens in a short while but before that let us look at the  look at what are the  what are the motivation behind xml what is the meant to do ? let us take a contrasting feature to begin with  html for example html was basically meant to meant to describe how data have to be displayed by a browser it is a logical data description language but nevertheless it is meant for display ; html is meant for display ; that is it tells the browser how to render the data it tells the browser  for example  this is the first level header so do whatever you are doing in order to describe first level header this is the paragraph break  so do whatever you are doing to display a paragraph break ok or this is bold so this is do whatever you are doing to display bold characters and so on so primarily html is design for display or logically describes describing how data are to be rendered on a browser  refer slide time  20  12  in contrast xml is designed to describe data and focus on what the data really is  so essentially xml is make to was oriented towards data exchange rather the data rendering and therefore xml isn ’ t really a replacement for a html in fact xml is complement of html and there are several tools that given an xml document will parse an xml document and convert it to appropriate html document  so that it can be rendered by a browser so xml is basically meant for describing the data  while html is meant for describing how the data are to be rendered  refer slide time  21  07  so what is an xml do ? have look at the slide that that is shown here this slide shows us small xml fragment so  look at how the fragment looks like there is a startup of tag here  which looks like there is startup of tag here  which looks like any html tag in the sense that it is it is embedded within angular braces so and just like html tag tags end with the slash tag so notice tag ends with the slash notice the to tag ends with slash to and from tag ends with slash from however as you see here  each of these or arbitrary  i mean just it is notice and it is to and from and so on so essentially what is fragments is describing is that this is the notice where the notice begins here and ends here the notice is send to the students of first year from hostel warden with the heading air conditioner  with the body of the notice as whatever that is written here right so body is denoted between body and slash body and heading is denoted between heading and slash heading now you might ask a how to any browser note how to interpret to and from and heading and body and so on the simple answer is it does not and it is the responsibility of the parser or whoever writes the parser to also write what have to be done when the parser come  comes across and notice or comes across the two field in a notice and so on ok otherwise if an xml document is rendered in a generic browser like say internet explorer as so if this particular fragment to be rendered in a internet explorer  you would just get a tree like structure as you can see this is tree like structure  this is a hierarchy  that is at the first level there is there is notice comma slash notice ok which is the first level element and there are one  two  three and four second level element so it is a tree comprising the two level of hierarchy one node at the top denoting notice  and four nodes below it or four children denoting to  from  heading and body  refer slide time  23  35  so as we saw that the xml fragment denoted something called notice and which has from  and to and message body and so on but  as such  the xml document itself does not do anything ; it is just describes that so it just say notice  slash notice and from and to  it beyond that nothing else it does not do anything else it is just some kind of what may be term this wrapped information  that is information in this sense i ’ m using the term information to mean meta data so meta data is wrapped around data elements in the xml document and how to display this xml documents  how to send  how to receive  how to what actions to perform in response to each of these elements all of them have to be handled separately xml by itself does not describe any       refer slide time  24  37  now what is the use of creating such a one might ask two generic a module for creating data  creating describing data what is the use of such an element or such a mechanism firstly  as you can see regardless of what kind of data that you are describing xml is pure text ; it is pure everything is in textual form  only that the encoding may differ obviously  but it is possible to find out what is the encoding and detect what is encoding and change it accordingly but whatever it is  it is given an encoding it is just plain text ; there has no preparatory forms by which data within the data attributes or values within the xml within the xml document is described so  it is just plain     data so  because it is just textual data  it can be exchange seamlessly between different systems so  whether it is different operating platforms like linux  or windows  or unix or solaris whatever or different processors itself different hardware platforms  it does not matter so  over the web or over any web an xml document can be exchange seamlessly without any need for any kind of interfacing for xml document but one which still ask that huge amount of an xml document is a huge waste of space  in the sense that just to describe a small notice and and to say this the notice was send from someone and send to someone and with the heading so and so  unit two at lot of tagging information like to and slash  to and from  slash from and so on however  that is not too much of a problem  because today storage is for cheaper than what it was earlier and storage is much more cheaper than what it was earlier and disk can disk can store large and larger amounts of data and processing is also faster  so it is quite and then many numbers of many very efficient xml parses that is freely available ; therefore parsing an xml document and creating a structure is quite simple also  there  there are different kinds of  there are different terms kinds of software that are available for compressing data you have  may have winzip or g-zip or     and compress and so on and so forth so which  which can be used to compress textual data very efficiently ; and especially with textual data contains repetition which an xml document describing some class of elements is apt to contain different kinds of repetition  one can one can expect huge percentage of compression to be performed so even if a xml document is takes a lot of spaces for describing a set of data  it can still be stored in a efficient fashion by compressing the document and decompressing it on the fly and so on and converting any data set like preparatory block level data set to xml can greatly reduce the complexity of data exchange between different incompatible systems and again over the internet and huge application area of xml naturally is in b2b or business to business interchange so each business house or even within one single business house  there could be different standards that are used and trying to interface between these standards become huge challenge and that is where the utility of xml comes becomes significant  refer slide time  28  52  so  xml see unlike for example ms-word data which can be opened only by ms-word or postscript data which can be only opened only by postscript viewers or pdf data which can be opened only by pdf viewers unlike those xml is just text data  so it is not bound to a specific application or specific browsers you can open xml using any application that can deal with textual data  you can open xml in notepad ; you can open it in vi  you can open it in normal internet explorer  which will show  which can support xml directly and which just show as a tree structured if a browser does not support xml directly  just shows you textual form of the xml data and there are several different that are written  wherein you can access xml data sources as though your accessing your databases  refer slide time  30  01  so  let us come back to xml syntax and look at some more aspects of what makes up an xml document now here again this is the     notice notice comma slash notice however  with a few more things that are added now  you might have noticed already that any symbol here  any tag that is opened here has to be ended so  notice is ended by slash notice  body is ended by slash body  to is ended by slash to so  in xml  it is mandatory for  for properly closing every tag  which is not show in html ; in html certain kinds of tags like say p and font and so on and so forth  need not be closed and whenever a second p tag comes in the the browser automatically closes the first p tag  but here an xml parser would flag syntax error if a given tag is not closed that is it is open and it is not closed however  there are exceptions have a look at the memo tag  memo type equal to d slash like this ; so that is an other way of expressing a tag or a or a meta data that does not contain a large amount of data in fact  the meta data contains what is called as an attribute of name type with value d  and that is it and there is no other no other data that is associated with this tag or tag is a meta data meta data item actually so  by default an xml document should be properly nested and properly closed that is every open tag should be properly closed  and it goes without saying that the nesting of xml element has also to be     exception cases where that is closing of for the case of closing of tags there are cases  where you can close a tag within a single line i have shown in this memo example here in addition look at the first line here  where the first line starts with one angular braces ; question mark and xml  the key word called xml  if the document begins with this  with this pattern here then the xml pattern  then and if it     xml parser  the xml parser knows that this is the valid xml document so  this one defines that this is the xml document  and this one defines the version of this document  and this one defines the encoding that is used for describing this document  refer slide time  32  46  so  just to summarize what we have seen  so the first line basically is a declaration that says that this document is an xml document with the given version  and with the given version and encoding and following the xml declaration is the root element of the xml document so  so every x m l document should be a rooted tree  that is a there should be one root element that begins here and ends at the end of the document right so  in this case the root of the xml document is the notice tag or the notice element  refer slide time  33  36  a tag is again some more x m l based definitions  this is what is called as a tag notice or from or body and so on  they are called tags a tag is a simple meta data  that is embedded within angular braces on the other hand the entire set of xml data from notice to slash notice inclusive is called an x m l element  that is it denotes an xml elements called notice  which in turn contain several different x m l elements like to element  and from element and so on right  refer slide time  34  15  so  the root element is the one which is the biggest element  which contains the entire xml document and every xml document must have a root element  and of course the next four lines describe the child element from the root  and to from heading body and so on and the last line describes the end of the document so  as you said before in xml  it is illegal to omit closing tags so  every tag that is open has to be closed properly so  notice has to be closed by slash notice and from has to be closed by slash from and so on  this is again contrary to xml so  all elements must have a closing tag with exception of unary tags  like the memo tag that is shown in this slide here so  where there can be a single end tag which can be closed directly  refer slide time  35  09  and the xml declaration  that is the first line here in this document here  this is not an element that is why to distinguish between a declaration  and meta data a declarations starts with less than and question mark therefore  that that gives an implicit constraint that that any name of an xml tag can not begin with a punctuation mark  like question mark so  because it will be treated differently  it will be parsed differently and that is not an element ; therefore  it would not have a closing tag and xml tags are case sensitive ; therefore  notice with all with all small letters and notice with capital n or different tags  and it goes without saying that the tags must be properly nested  refer slide time  36  03   refer slide time  36  13  and as we saw in the previous slide tag or meta data can have different attributes so  let us look at it again that for example here memo  memo is a meta data which says that this is the memo  and here is an attribute which     is to memo  and the attribute name is type and the value is d so … so  therefore it says that this is the memo whose which contains an attribute called the type  and who is the value is d so  it is type d king of memo so  and attributes values must be coated so  even when i say the type equal to d and d is a single letter it has to be coated it it should it should lying within double quotes so  so that is double quotes or single quotes so  that is in contrast html  where it is optional to coat an argument attribute value unless the values contains a space with in it  and in xml white spaces are preserved and not truncated as in html so  what this mean is that for example  if i put a white space here air conditioner  and three spaces and then slash heading then the data for this heading is actually air space conditioner followed by three spaces in html     all the training and leading white spaces are ignored when html is being ranted on a browser this is not so  an xml and all spaces are considered  and of course the syntax for writing comments in html in xml with slash exclamation mark  and two dash is and end with two dashes and open close ah angular brace right so  this is same syntax as that is used in html  refer slide time  38  05  so  xml elements are given are xml document forms as specific tree structure  and the relationship between elements primarily there are exception to it which will which will look in to more detail in the next session an xml  but primarily the relationship between in between two or more tags is either parent child or siblings ; that is that is one is     of the other or there it  the there it the same level in some sense that is or one is at a at a higher level ; and one is at a lower level so  so it can associated level value for each tag in an xml document  and xml element as said that what is mean by an xml element a xml element is everything from starting tag to the ending tag include the tag      so  so notice to slash notice including the tags forms the root element in the xml document  and an element what     element contain ; obviously  we have seen that an element can contain other elements for example  here the element call notice contains other element call to from heading and so on  refer slide time  39  45  so  so an element can contain other elements or can have a element element or mixed content ; that is it can contain both other elements and some textual data as part of this or could contains simple content  where it is just simple textual data or an element could be empty as well which is an empty content an example of empty element  we saw was the memo element so  which where you need not separately specify a closing tag for empty empty element  you can just close up the elements within a single line and an element also have attributes  refer slide time  40  23  so  how can you define a xml tag or an xml  xml element what tell what are the rules for describing xml element as we saw earlier ; obviously  you can not begin an xml element with the punctuation mark like exclamation mark or question mark and so on and so forth the element names  names of an element can contain letters numbers  and other kinds of characters like dot colon and so on ; however  they can not start with number are a punctuation character  and also they can not start with letters xml in any form that is case     form a xml  because they a are reserved for a possible future that use and names can not contain spaces so  you can not quote name within double quotes in say this  this is the xml element so  an xml element tag should be describe by a single word and any name can be used  because there is no such thing as a reserved word except for xml of course  right so  and you should try to make the names as descriptive as possible in order to describe the meta data  refer slide time  41  40  and this slide show an example of how attribute can be used as children element and vice versa so  the in the first example here  there is an element call person and person which slash person and the person element has an attribute     gender  whose value is male and there is the first name  and last name elements which which are all as shown and in the second example this general equal to male attribute becomes another element here  that is it becomes an element for general and the value is male now  which is preferable and when this is similar to the problem that we faced in e r modeling are entity relationship modeling in one of our best classes for example  if we were asking question whether an employee an employee working in a department should it be shown as department id attribute for the employee entity or an employee working with department is a relationship  now which you are going to show depends upon of course  depends upon this specific situation  but rulers     one needs to describe an xml element just like one  it is to describe an entity only for those concepts that have independent existence ; that is ah a person has an independent existence in a uod regardless of whatever else there is … however  general is closely associated with a person so  it is does not have a separate independent entities so  so that can help you decided  when the user particular data element as as in attribute versus as another child element of an existing xml element   refer slide time  43  47  when we talked about xml document  there is often a distinction between what is called as a valid xml document  and the well form xml document the distinction between the two is is very important mainly when when we note that well formed xml document     invalid where  whereas all valid documents are well formed so  well formed xml document simply is ah a xml document that that conforms to all the xml syntax specification  that is what are the xml syntax specifications namely that a xml document should be a rooted tree that was a it has it has a root element  and elements do not have spaces in them and every starting element has as a ending element and there elements are properly nested so on so  all these if an xml document conforms to all of these syntax rule  then it is called a well form xml document  but a well formed xml document need not always be valid various notion of validity come from validity we say that an xml document is valid if it conforms to a schema speciation so  so that … so  so it means that according to an xml schema for example  person person element has a attribute called gender  and no child element called gender in that case even though both of these are valid xml fragments ; the first one both both of these are well formed xml fragment  the first one is a valid xml fragment where as the second one is not a valid xml fragment so  varies the schema described in in an xml this schema described in what is called as dtd or a document type definition  refer slide time  46  00  so  i am a look at this fragment again here  where this is well formed xml document  and then here there is an extra declaration ; that is say stand alone equal to yes in the xml declaration  if this stand alone is equal to yes declaration is there then it is when it explicit states that the xml document here is only well formed  that is it does not have a schema associated with this is stand alone xml document so  in a sense the stand alone equal to yes indicates that the document is well formed provided  of course that that it is well formed that is parser gives no error when parsing   refer slide time  46  49  and on other hand you can say stand alone equal to no which is optional  and give up link here with with less than exclamation mark declaration doc type declaration so  with the doc type declaration you can specify the dtd  which describes this xml document so  note that it says doc type notice system notice dot dtd so  it means that in the file call notice dot dtd the dtd for this xml fragment for this xml fragment document is available   refer slide time  47  26  and the xml document are the xml fragment is valid if and only if of course first if it is well formed and it conforms to the dtd specification  refer slide time  47  45  so  what is a dtd  and and how do you describe a dtd  and how do you describe an xml document using a dtd a dtd simply says that ah dtd ; simply defines  what all should an xml document contain that is ah which element should be a child of which other element and which element contains what kind of data and so on and so forth so  dtd stands for document type definition and dtd  dtd defines all the legal elements of a xml document  and it also defines the documents structure within a list of legal elements and dtd can be either declared inline  that is as part of an xml document or it can also be a recall from a external reference  and this external reference can be anywhere on the internet ; that is it does not have to be on a on a different file or a it can be anywhere on the internet as long as you can you can dereference the dtd by a uri or a or a uniform resource indicator  where where which is which is the http equivalent slash whatever so  so which how you specify you are     document as long as you are able to dereference dtd document with url it is it is possible to place a reference in your xml document  refer slide time  49  17  so  if i xml if i dtd included within the xml source then it should be wrapped with in a doc type declaration like this so  here it is says doc type root and this is the this is the name of the root tag  and these are the element declaration what are the element that should going to this   refer slide time  49  35  and if if the dtc dtd is external to the xml source file  then it is should be a wrapped with a doc type definition with the following syntax like this ; that is doc type and root here specifies that specifies that root is the specifies the name of the root element in the in the xml document  refer slide time  50  08  and this is the file name of the xml document  and this says that this is the external reference  here is a example of a xml dtd as shown in this slide as you can see this is this is an inline d t d ; that means  it starts the doctype here and notice says that notice is the root element of this x m l document  and this one says that notice contains from here to here there     to be a box brace here so  or rather from here to here that is this is the definition of notice that this is the definition of the document  and the first one says element notice contains to from heading and body ; that is four different elements in this particular order  that is to from heading  and body and everything else that is element to element from element heading  and element body contains what is called as pc data or hash p c data  refer slide time  51  07  so  what is this hash p c data so  let us go to this slide which talks about what each of this elements mean so  elements to from heading etcetera are all this element names  and tags are used to markup elements and then an element can contain other elements like to from heading and so  on or it can contain c data or p c data so  what is c data or p c data so  what is c data what is p c data so  a p c data essentially means that parsed character data and c data just means character data so  so ah a parsed character data can contain other x m l tags as part of the data which which will actually be parsed and opened as and when required  but character data is not parsed it is just dumped in in water form it is present  refer slide time  52  03  and of course  you can use this predefined entity type like ampersand l t ampersand g t like using normal h t m l to to denote each of these characters  and we talked about p c data and c data where which stands for parsed character data  and character data respectively and in in addition to each of this ; there are also other kinds of option that that are provided when defining an x m l element for example  if let us say the the option called from is if the element called from is optional in a notice a notice should just have a two address  and the from address can be optional then you can a suffix from with a question mark that is from question mark says that the the element called from may may exist either zero or one times similarly  if i put a star here  let us say heading star it means that i can have zero or more heading elements in a notice element  similarly something like body plus means that a notice can have one or more body elements and so on so  so these are what are called as wilde card declaration     elements that says how elements can be structured  and in fact one can even says something like to from  and then heading pipe body pipe is the or symbol  so which says that notice can contain to element from element and either heading or body  so one of the two because there is only one of them and so on  refer slide time  53  57  so  that is the that is the simple way in which a dtd is written  and whenever an xml document fails to conforms with a dtd a validating xml parser  that is a parser which validates xml documents against a dtd will flag an error  and stop the parsing at that point now in recent times the world wide web consortium or what is called as the w three c has come up with alternative to xml dtd ’ s  which are called as x m l schemas or sometimes also termed as x schemas xml schema is a much more detailed description of of an xml document and it is it it is used for schema definition  refer slide time  54  49  we shall not be looking into xml schema in more detail in this session due to time constraints  but but we can look at them in one of the subsequent sessions on xml documents in one of the advanced classes on managing x m l data so  xml schema basically defines elements that can appear in ah in a in a document that is defines elements in attributes  which are ah which appear in a document  and like a dtd it can also it also define defines parent child relationship  and defines order in which the child elements are two appear and it defines the number of child elements one can have and several others things that that are typically described by a dtd   refer slide time  55  31  and there there for more richer and suppose it will more useful then then dtd is in described in schematic structure for xml documents and xml schemas good thing about xml schema is that they do not have a different syntax from the xml syntax itself  that is unlike dtd ’ s which have a different syntax in the xml syntaxes  xml schemas are written in xml itself and they also support data types and name spaces and so on  which which we look at in more detail in one of the subsequencetion  refer slide time  56  12  so  let us summaries what we have a learnt  today we have started this series of ah a few session  and xml data bases which is a world wide web constructions standard ; that is used for data exchange and and managing semi structure data  and and self describing data and … so  it is a it is a self expressive  it is it is a self describing way of specifying set of data elements  and then when we talk about an xml document  it is important to distinguish between what is a valid xml document versus what it is a well formed xml document  and well formednesss all valid documents have     well formed  but validity itself has to be checked again dtd ’ s specification  and now which is being replace by the xml schema specification so  that brings as to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 39 xml-advanced concepts hello and welcome in the previous session in the dbms course  we have been looking into managing xml data as we had mentioned earlier  xml is kind of a markup language which is in some way the de facto standard for information interchange over the internet and the power of xml comes from its simplicity in the sense that it ’ s a markup language which can be read and understood by both humans as well as machines and it ’ s independent of any operating platform or independent of any standards like how schema is describes and so on and essentially it ’ s a self-describing  it describes a self-describing data set in a sense now in this session let us continue with xml by looking into some advanced aspects of how xml data are stored or how they are queried and so on and xml as i had mentioned in the previous session has elicited wide area of interest and there are several different not just in computer science in the sense that xml is elicited interest across several different disciplines because information interchange or managing different facets of information  different points of information and integrating them  interchanging information is a common problem in several different domains and there are different kinds of standards in specific domains like banking or finance or bio informatics or transportation or whatever and several different domains which have defined xml dtds pertaining to their specific area of concern so let us look into xml little more deeply in this session and see what we can do with xml  refer slide time  03  36  first of all let me start by acknowledging that some of the material in the slides have been derived from an invited talk by jayant haritsa in the vldb summer school held in bangalore in june 2004 so let us have a brief recap of what we studied about xml and what are its main features like i had mentioned before xml is a platform independent and standardized extensible markup language now each of these different terms means a very specific thing platform independent means that you just store xml in plain vanilla text  character data essentially and every platform  no matter what you use  what platform you use would support textual data regardless of whatever underlying encoding that you are using  refer slide time  05  04  and it ’ s a standardized markup language in the sense that there are specific rules that specify how an xml data set should look like for example it should be a rooted tree and a begin tag should end with an end tag and there is this notion of well formedness and validness and so on and it ’ s an extensible markup language the markup language does not define  what are the kinds of tags that you need to have in your xml data you can define your own tags and we took up some examples where a notice was a kind of tag notice and slash notice or from and to and so on and so forth it ’ s a self-describing dataset  in the sense that the structure of the data is implicit in the data set itself or meta data is embedded within the dataset itself so you don ’ t need a separate descriptor for how the data are to be described of course you may have a separate set of what might called as restrictions or which specify whether that description is valid or not and it ’ s a very simple standard for data interchange made up of simple building blocks like elements and attributes and nesting of different elements and so on and we also saw the difference between what is a well formed xml fragment and what is a valid xml fragment a well formed xmls fragment essentially conforms to the xml structural requirements  i mean structural requirements of an xml document in the sense that it ’ s a rooted tree and the nesting is proper  every begin tag is closed by an end tag and tag names do not have spaces in them and they do not begin with a special character and so on and so forth but a valid xml data has to be well formed of course that is unless data set is well formed it ca n't be valid however valid is most stringent in the sense that it ’ s not sufficient for an xml data set to be well formed but also it should conform to a given document type definition  a dtd or a xml schema that is a schema that is specified as part of this document  refer slide time  07  12  why xml or what is the significance of xml ? it ’ s a very convenient way  probably the most convenient way of exchanging data over the web and like i had mentioned before it ’ s easier for both machine understanding and human understanding so in the worst case you can actually open an xml document in a text editor like notepad or emax or whatever and understand what the xml data is all about and make changes if required and it has a simple tree structure like a simple hierarchical structure which is easier to understand  easier to enforce because the tree has very specific constraints and well known constraints as to what makes up a tree structure and enforcement is also easier that is it ’ s easy to build parsers which using stacks or whatever there are several kinds of tree algorithms that can validate a given tree and it ’ s also easier to navigate  nice directory structure is a tree structure and hence xml data sets are usually rendered in the form of a directory structure on browsers and like i had mentioned before  xml is eliciting interest not just in the computing community but perhaps more so in several other communities and information interchange is not just a problem with database researchers or computer science researchers but it ’ s a problem in almost any domain of concern  refer slide time  08  42  what is the information interchange problem ? simply stated that simply stated it means it is a problem of how to integrate the varying or desperate sources of information that exists in any given system fpml for example as shown in the slide is an xml standard for the finance industry and fpml essentially standardizes the different kinds of contracts or transaction data sets that are routinely specified  generated and specified in the finance domain pertaining to banking or stock trading or loaning and several different allied activities where contracts would have more or less some common features in the sense that or contract would have to specify what are the parties to a given contract  what is the validity period of the contract  when does the contract expire  what are the components of a contract and what ’ s the amount involved in this contractual interchange and so on and there are several standards that are used independently by different sources and fpml in effect tries to standardize this contractual specifications so that it becomes easier to integrate different kinds of contracts that are generated in different places  refer slide time  10  27  similarly bioml as shown in the slide is an xml standard primarily for use in bio informatics applications and that too in annotating gene sequence data and gene sequences as you know are very long sequences comprising of one of four different characters 80 g and c sometimes u and sub sequences of this long sequence specify  this long sequence is what is called as the genome sequence and sub sequences of this genome sequences specify genes or sometimes what are called as some a set of codon which code for a particular kind of behavior in the organism or however when the gene folds so such kinds of codon  such kinds of code on strings or genes are annotated and this annotation is done in several different ways by several different researchers and because there several researchers trying to sequence genomes of different kinds and including of course the human genome and or annotating different parts of a genome string now bioml is a standard which specifies how this annotations had to be performed and of course it allows for free text for annotations that is it does not really place any constraints on what should go into the annotations themselves but it specifies how and where annotations should have and bioml in addition to its  for example fpml or bioml actually specify the schema  they basically specify the xml schema of this so the xml schema also supports full text queries for different kinds of sequence matching problems  refer slide time  12  37  and then a very commonly used standard that uses xml is the soap protocol you might have heard of the soap protocol which stands for simple object access protocol which forms in some sense the building blocks of web services soap is a mechanism by which a software can access a remote object or invoke  send methods to remote object via the internet so what is the soap protocol do ? it is simply a message passing or a message sent to a remote object that is wrapped in an xml document that is the message is sent as an xml document and at the other end where the services provided  the xml is parsed and the actual message is taken from the xml document and then sent to the object and question so in this for example you might have some kind of an object that performs a given kind of calculation let us say currency conversion so you might want to perform some kind of currency conversion and that is a service that you provide over the web now if this currency conversion object is soap compliant then the clients speaking with your object or speaking with your component sends messages in an xml format and the object contains an xml parser embedded into it which parses the xml document and divides the xml input into two different parts a header and a body so the header contains meta data about the message saying what kind of message  where is it coming from and any other restrictions and the body contains the actual message itself  refer slide time  14  24  so this slide shows a schematic diagram of how an xml  how xml processing would probably look like given xml document  given an xml document we have what is called as an xml parser when an xml parser can check for the well formedness of an xml document so that it can parse an xml tree and create some kind of a data structure here  also called as an info set which in turn goes into a document validator document validator essentially validates a given xml document against an input dtd or an xml schema so once the document is validated then the xml is ready for the application of the storage system where whatever it is being used for and of course the application itself accepts different kinds of queries using xquery or xpath and so on and can answer queries on the info set that it received from the document validator so what are the key concepts in xml ? you have the data model which comprises of elements attributes and data and rooted tree structure plus of course comments and processing instructions and namespace declarations and so on and so forth  refer slide time  15  57  in addition you have the dtd or the xml schema which specifies the meta data structure and there are several different features that are available in a dtd in the sense that it can define regular expressions over xml tags so you can say that  notice can have zero or more headers or one or more two elements and so on and so forth so here is an example xml fragment were this is a rooted tree again it ’ s an xml element but every element  every well-formed element is a rooted tree in itself  refer slide time  16  40  so this is a rooted tree which begins at imdb and slash imdb and of course you might know that imdb stands for the internet movie database which is a huge source of xml information that is it stores a lot of movie related information in xml and then imdb itself contains an element called show and show year equal to 1993 and comments are given like this that is angular braces with an exclamation mark followed by two hyphens and then again two hyphens with the angular braces here so show is the element from here to here  so the show element contains one or more reviews so review one starts from here to here and review two starts from here to here and then there is a set of box office numbers and each review itself in turn says where the review is from  so this one says the review is from sun times and the review itself is a mixed xml plus free text that is there is free text going around here within which there are some xml tags that is roger ebert is surrounded by the reviewer tag and two thumbs up is given  is surrounded by the rating tag and so on so that ’ s how a simple xml fragment would look like and a typical dtd for this would look like this where you start your dtd by a doctype declaration and then specify what is the root element of your xml fragment then within this specify each element and the set of attributes  refer slide time  18  22  so imdb is the root element can contain show star  show star essentially means that zero or more occurrences of the show element similarly the show element contains title  review star that is it has to contain exactly one title followed by zero or more reviews  review elements and show can also contain an attribute called year which is character data and title is character data and so on so i have not completed the dtd as yet but this is what the typical dtd would look like for this xml fragment in contrast we have what is called as xml schema which is an emerging standard for which is fast replacing dtds  refer slide time  20  00   xml schema is far more expressive than a dtd in the sense and it supports  it ’ s a strongly typed language in the sense that it supports a type of a particular kind and most importantly an xml schema document is an xml document itself that is you can use the same xml parser to parse an xml schema as you would use for parsing an xml document so how would an xml schema for this xml fragment look like ? you see that it starts with an element declaration and the element declaration has an attribute called name equal to show so i am not starting from the imdb declaration itself  i am just declaring the schema here for the show element so this one says the show element is declared like this that is the show element is a complex type having a sequence of a two or more elements where the first element in the sequence is the title and the type of this element is string  refer slide time  22  58  and this is a single element  so the element is closed right here and then the second element is and then there is a sub sequence so the first element of this sequence is an element of name title and the second element of the sequence is another sequence which can have repetitions by itself so min occurs zero and max occurs unbounded is another name for saying the star declaration in dtd that is something can occur zero or more times so what is this sequence contain ? this sequence contains an element called review and which is of mixed data that is a review can contain both free text and xml tags in itself and of course this sequence finishes now and then after this there is a choice element that is the third element of this sequence where i can have a choice between box office or seasons or so on so one of these two  i can have either box office or seasons as the third element in the sequence so note that show itself comprises of three different elements title  review star in a sense and box office or seasons which was not specified completely in the dtd in the previous slide and of course this sequence finishes and the next element  the next attribute of this element is year which is optional and slash element so that is typically what an xml schema would look like as you can see here this is a simple xml document in itself and then you can parse this xml document using any xml parser and then based on the outputs of this parser  you can enforce the constraints here on the given xml fragment  refer slide time  23  20  let us now turn to a specific problem that occurs when parsing xml data namely the notion of naming conflicts now consider these two xml fragments here that are shown  one xml fragment is shown here and the second xml fragment is shown here the first xml fragment  if you have worked with html you might have recognized that this is a html fragment and html fragment can also be treated as an xml fragment there is no harm in that so of course well-formed html fragments that is in the sense that every start tag is paired with an end tag and there is a nice hierarchical structure for this fragment so this is a table fragment which begins at table and ends at slash table and there is a table row which begins at tr and slash tr and table descriptor that is some table cell which begins at td and slash td and then somewhere down the line there is another fragment here which says table  slash table and name and width and length and so on and so forth  so african coffee table 80 and so on now these two could occur within a single xml document that is this could actually be some kind of pcdata that is parsed character data for given xml element and this could be another xml element by itself but these two tables are different now this is required because the data contains html and the html has to be rendered and this is required because the semantics require that we need to declare something called table and these two tables are different now such a conflict is called a naming conflict that is when two or more tags mean different things but have the same name in them  refer slide time  25  55  so in order to resolve conflicts  how would we resolve conflicts one simple way to resolve them is to prefix each of these  each occurrence of this meta data with a particular string so for example this one shows  refer slide time  25  28  a prefix called h where it says  where the html table is prefixed by a character called h so h table and slash h table and note the use of this colon character here so the html table is discriminated or distinguished from the xml table by using a different prefix called h  refer slide time  25  60  and similarly the xml table itself is prefixed by f now that is one simple way of doing that but again the question still remains as to what if the prefix also is the same  i mean if the name of the tag can be identical  the prefixes can also be identical when two or more xml documents are brought together and meant to be integrated into one schema so for this the notion of names spaces becomes valid or namespaces become important a namespace essentially defines a unique space globally  worldwide across the web so how does it define a unique name space across the web ? remember the notion of a url or a uri uniform resource identifier a url or the more general form which is the uri is a unique name for a resource across the web therefore whenever you see a address like this  refer slide time  27  09  http  //osl.iiitb.ac.in whatever it is unique that is this stands for  it has to map on to a single ip address or a single address across the world wherever in the world that it is referred to  refer slide time  28  15  now using the concept of uris  we can ensure that naming conflicts in xml documents can be resolved for example we can  when we say that when we put a prefix called h for example for this table meta data  for this table tag then we describe that h means a specific uri now this could be  this need not actually contain anything the uri need not contain anything with respect to this table but let us say this is the uri in which we work in  let us say where we created the xml fragment now once we alias a given prefix to a uri it becomes unique so in a sense whenever the xml parser looks at looks at a prefix called h followed by colon  it replaces that with this uri so if a different h comes from a different source and it maps to a different uri  it means that it is a different prefix therefore it basically means some other tag and not the same h as this prefix that is specified here  refer slide time  29  30  so similarly we can use a different that is the same uri having a different prefixes so that different prefixes from the same uri can again be distinguished therefore what the xml parser actually does when it encounters name called f colon is that it replaces f colon by this whole thing slash f  refer slide time  29  20  internally that is http  //osl.iiitb.ac.in/oslwiki/f so that several different prefixes that have been defined at this uri remains distinguishable  refer slide time  30  00  so the namespace attribute as we saw in the previous slides is placed in the start tag of an element so for a given element  for every element you can define a separate name spaces where names for that element are uniquely describe globally across the world and the syntax for name space begins with an xmlns  xml namespace declaration followed by a namespace prefix equal to a coated string following the namespace or coated string following the uri and it basically gives a qualified name associated with a namespace  refer slide time  30  25  so  all child elements if i define a namespace for a given element like in the example table there  all children elements of this element are associated with the same namespace so their names are term to mean unique meta data that have been defined in this uri the address is just simply the uri address that is specified is used to simply identify a namespace but the parser itself does not try to connect to that uri or look up that information or whatever it is simply used for resolving naming conflicts and it doesn ’ t necessarily have to check or validate whether the uri exists and whether the uri defines this names or anything of that sort  refer slide time  31  19  similarly let us look at some more issues related to cdata cdata if you remember is character data character data  when i define an element as character data then it means that everything until the slash of that element is taken in without parsing that is the data itself might contain other tags which are not parsed and everything is taken until the end of that element  so everything is ignored until the end of this element but then if the text or the character text contains a number of these characters  let us say less than or ampersand and so on  it is quite easy for xml parsers that especially if the y are little buggy it can be quite easy for xml parsers to get confused and look and try to parse them and especially if it is html data and not well formed and the parser might flag errors even though the xml itself is well formed and valid and so on so there is another way of declaring cdata where a cdata section can be declared using a specific tag like this  refer slide time  32  44  so this slide shows an example where script and slash script is an element that defines cdata and this element contains a cdata section which says cdata and then defines a function or defines some script here with less than and greater than symbols and ampersand symbols and then ends the cdata section like this here now what are some of the rules for cdata sections simply that the body of a cdata section can not contain this string which defines the end of the cdata sections  refer slide time  33  28   hence  which also implies that nesting of cdata sections is not allowed so you ca n't have nested like you can ’ t have nested comments in c  you can not have a nested cdata sections and there should be no spaces or line brakes inside this string  refer slide time  33  51  now let us look into querying xml data and what kinds of query languages are present and what paradigms of queries exist the common querying tasks that are usually done over an xml data are something like filtering and selecting  navigation  selection  extraction and so on in addition you could define some kind of joins or aggregation like we do in sql and transformation that is convert one form of an sql data to another  refer slide time  34  11  so before we start up with querying itself  we have to address a more fundamental problem or fundamental issue of xml parsing itself and there is one  there are two specific kinds of xml parsing which have implications on how querying is performed and let us briefly look at these two paradigms of xml parsing and what kind of impacts they have on queries  refer slide time  35  27  the first kind of xml parser is what is called as a full navigational parser that means of which you might have heard of this name called dom or document object model this is an example of such a parser which requires an entire xml document to be available for it before it can start parsing that is the xml document should be full  complete and well-formed and so on and which can then be parsed and dom basically creates some dom object which can be called by application programs in order to query the xml data so the application requirements state that it should provide full navigational access to the document so you can not have partial xml string that is available and ask the dom parser to start parsing and kind of queries that dom allows is something like this that is dom essentially creates an object which contains a xml document and then you can say something like if document element.getelementsbytagname show then  so which basically gets a given element of this thing then for s in this thing that is s is a element which is the set of all show elements and in the show element if the title contains the fugitive then get the review and so on so you can address or dereference an element by the tag name and then it returns an element object to you and then you can get the data associated with that object and so on  refer slide time  37  02  so this slide here shows how the dom parser works that is you give an xml document plus a dtd or xml schema to the dom parser the dom parser performs both parsing and validation and creates an object or a dom instance which is then given to the application the application then start calling the dom instance by using it or integrating it with the other sets of its objects  refer slide time  37  47  the other kind of xml parsing is what is called as stream based parsing and example for such a stream based parser is sax which is also widely available on the microsoft windows platform again this is a language independent and programmable  programmatic api and sax in contrast to dom does not require full navigational access to the given xml document instead  you can stream the xml data and it parses as the xml data parses through it that means you can put an x  you can put a sax parser on a network stream which is sending you xml data and as and when the xml data streams through  sax creates a xml object  i mean it creates an object on the fly for the xml data that is parsing in other words it also means that sax performs just one parse over the entire xml stream and there are several applications where sax is more important something like stock quotes which are sent in a streaming data over the web and as and when codes change  data streamed and an xml parser  a sax like parser can parse them and what sax does is it also has a feature of call backs that is sax not only parses the xml data but it also creates events that can call back into the application and interrupt the application and tell that something is happened  so take appropriate action and so on on the other hand in dom it is the application who is in control and the dom just creates a  dom parser just creates an object and the application decides when to call the object and what to do with the object and so on and of course in sax its read only access for un typed notes and there is no in place updates that ’ s possible  refer slide time  39  56  so  this slide shows how a schematic diagram of how the sax parser works that is you have a xml document plus dtd which is streaming that is which need not be the entire document as such now this streaming xml document parses through a sax parser and validator which in turn sends sax events to the application that is the application keeps interrupting  sax parser keeps interrupting the application by sending appropriate signals or events to the application and the application has to perform specific tasks associated with each of this events so it will say that found an element called show and which is valid and this is the element and so on so  as and when a show element comes  the application says that a new show has arrived and this is the data that has to be rendered and so on so the application begins the sax parser but once the application begins  it ’ s the sax parser that ’ s in control which calls back the application  refer slide time  41  00  so  let us look at xml query languages now look at these two kinds of parsing techniques let ’ s see how they can affect queries as well but of course before that we should look at different kinds of xml query languages itself and some examples are shown in this slide here you have query languages like xpath  xpath 2.0 which is very commonly used language for specifying navigations or selections or extractions from an xml tree and it ’ s also used in  i mentioned the name called xslt which is another kind of query language it ’ s more like a transformation language that is it can convert xml to html or it can convert xml to text or one form of xml to another form of xml and so on so if you want to render an xml document  you generally use an xslt query language which will take an xml document and give out a corresponding html document in return and then there is a xquery kind of language which is a  what may be termed as a composible language in the sense that one xquery takes an xml document as input and gives out another xml document as output it ’ s very similar to the relational algebra queries which takes a relation as input and gives a relation as output and it ’ s a strongly typed xml  it ’ s a strongly typed query language and useful for large scale database accesses itself  refer slide time  42  40  so let us briefly look at xpath xpath is a syntax for defining parts of an xml document and basically the way it defines parts is to use a directory like parts structures where in order to define xml stands or xml elements  refer slide time  42  56  let us quickly go to an example to illustrate this this slide essentially says that path expressions in xpath look very similar to the directory structure in a computer file system so i might have a directory called this slash this and slash this and so on so if this is what to be an xpath expression  all this would be elements so some osl.iiitb.ac.in would be an element and grace would be an element and so on  refer slide time  43  48  so  this slide shows an example where an xpath selects a book element which lies under a catalog element where the price attribute of the book element is equal to 10.80 and the catalog element is the root element of this xml document so this is just like the root directory in your simple file system language and so the root directory contains  defines the element called catalog  under the catalog look for an element called book and look for an attribute matching this particular criteria  refer slide time  45  14  similarly there are other examples here if you have a double slash like this  as shown in this example it says that return all titles at any level in the imdb xml document that is the root element is imdb and title can occur at any level  so double slash essentially means any level in your document and similarly this one double slash at the beginning again says that return a show element at any level and doesn ’ t matter what the root element also is but where the show year is greater than 2000 that is all shows released after 2000 and you can also have full text operators like text contains russell crowe and so on anywhere in here … there is no element name as well that is given  so it just says star so any element in this xml document at any level return that element where the text contains russell crowe so that ’ s about xpath where  it ’ s a very brief introduction to xpath where which is a file system like language for representing different navigational aspects of an xml document  refer slide time  46  04  and this is where the hierarchy or tree structure of an xml document becomes significant in the sense that it is easy to express a tree structure in the form of a directory structure as done in xpath plus it is easy to enforce constraints that is in a tree structure  a node can have at most one parent that is any non-root level node will have exactly one parent and there is exactly one path from the root to any given element in a xml tree so you can actually specify one long file system like path which uniquely identifies each element in the xml document the next kind of query that we are going to look at or query paradigm that we are going to look at is the xquery paradigm xquery is a functional language in the  it ’ s a strongly typed query language for querying xml data and xquery as i said earlier is an xml to xml converter that is it can query an xml document and return an xml document just like the relational algebra queries that can query a relation and return a relation an xquery in turn can use xpath expressions for its queries  so xpath can become a part of xquery in specifying its query expressions and many people would term xquery as an sql for xml databases so it ’ s analogous to sql in the sense that it takes an xml document  returns an xml document  it contains several different operators  it can contain several different sophisticated query operators something like select from where or and so on so you can actually specify a query like select all papers which have been sighted more than 50 times from the collection of papers stored in the xml document called citeseer papers and so on so you can give complex expressions based around a select from where kind of class  refer slide time  48  20  so what are some of the xml xquery features ? they are what are called as flower queries or flwr queries which say that which stands for let where return repeat queries that is it ’ s a looping kind of queries where you can specify a for condition initial condition let where is similar to the sql where and then repeat  so you can repeat these queries we will see an example of this shortly then there are sql like select from where clauses were select so and so from this xml element where this condition matches then there are sort by operators  so you can sort elements based on certain attributes then xml construction that is transforming one xml document to another xml document you can also have user defined functions where on this and xml xquery basically supports strong typing so you can say something like an integer or character or you can perform operations that are specific to integers versus character strings and so on and it also has supports for processing recursive data sets  refer slide time  50  04  here is an example of an xquery query that is a query written in the xquery language so the query essentially says that for each actor return box office receipts of films in which they starred in the past 2 years and essentially let us go back to  let us go first to this last part of the xquery document were here you say lot of let for and so on and so forth  for let where and so on and repeat and so but this last one here is what is going to tell the xquery engine what to return now here that imdb engine or the imdb xml document did not contain an element called receipt however what it does contain are elements like box office or actor and so on and what the xquery languages returning is an xml element called receipts and slash receipts which in turn is made up of actors and totals and so on so receipt is  receipt essentially is an xml document comprising of character data which tells what is the actor and then one more xml element called total which contains the sum of all box office i mean films per box office receipts that they have obtained over the past 2 years and here an xquery for example can first define variables like using a pascal like syntax  so it says let dollar imdb equal to this document so imdb basically specifies this particular document and then this is for any actor in imdb actor that is actor is another variable note that all variables are prefixed by a dollar sign and where actor stands for any actor element at any level in the imdb document so for any actor in this thing  let films equal to this one that is the show element where it contains a box office element and year greater than 2000 and actor name is the particular actor is entered as a star in this show  so star name and so on so essentially you let this one and for each actor this for let and the where is implicit here and return is also specified so  this one iteratively performs for each actor at any level in the specified in the imdb document and then it returns a set of this receipts elements which says actor and total that the actor has grossed  refer slide time  52  51  and here are some www links or world wide web where you can get more information about dom parsers or sax parsers or xpath queries and xquery standards and so on but before this let us go back to the parser problem that we are talked about that is how does a parser affect the queries that you give on an xml document both dom and sax parsers for example create documents or create objects which the application can access that is which the application can send messages to an access however the dom parser requires a entire xml document to be present whereas the sax parser calls back into the application as and when xml elements parse through the parser now what kind of implication does it have on queries ? let us take the query here let us say for each actor return box office receipts of films in which they starred in the past 2 years this is when you give this  such a query to a xml document that is parsed using dom  you essentially know that the entire xml document has been navigated and parsed and that is present in the object that is available here so you can essentially go and look through the object and return the query results on the other hand if it is a sax parser and it ’ s a streaming xml data then you wo n't be sure whether you have encountered all possible actors in this loop that is take a look at this  refer slide time  54  45  for actor in imdb slash actor  so you will have no idea whether all actors have been processed as part of this xml document so in such a sense  in such cases this query has to be in the form of what is called as a standing query that is in the traditional database setting  the database is static and the query parses through the database and then returns query results but here it ’ s the other way around  the query is standing  the query is static and the database or data set parses through the query and the query returns backs or returns events or performs call back into the applications whenever a data set matching the query is available so as and when the query finds an actor and show satisfying this criterion  it returns a receipt kind of xml fragment by calling back into the application  refer slide time  56  14  so let us summarize what we learnt today in this session we started with xml namespaces and how to resolve naming conflicts by assigning a unique  globally unique uri for each name prefix in an xml document and we also looked at some cdata handling issues where especially if your document  if your cdata contains angular braces and ampersand symbols and so on  you can embed it within a cdata section next we looked at two different kinds of xml parsing namely the navigational parsing of the entire document versus stream parsing and then we looked at xpath and xqueries as query languages or query paradigms over xml data sets and how these kinds of parsing techniques can impact the kinds of queries that or how the query behaves in each of these parsing techniques so that brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 40 xml databases hello and welcome in this final session on introduction to xml  we shall be delving primarily into xml databases that is until now we have been talking about what is xml and what are the benefits of xml or data interchange using xml and xml queries or schemas and so on  refer slide time  01  49  but today in this session  we shall be looking into xml databases and what are the issues that come up when xml data has to be stored or transmitted and exchanged over preexisting or some set of existing data stores we shall also be looking into the larger problem of what is called as semi structured data management semi structured data as you might have imagined from the name is data which do not have specific rigid structures and we shall show that more and more the data requirements of today ’ s world what we call the post internet world is being defined by semi structure data rather than structured organized data and semi structured data possess some unique problems in fact some fundamental problems in database management and what we shall do here is that in this session is we shall only motivate those problems rather than providing any solutions as such many of which are in some sense areas of active research so we shall be motivating the problems with some examples or some analogies and instances and we shall conclude this session with a list of what one could expect in terms of semi structured data management so let us proceed further with this session and as with earlier session some of these material have been derived from an invited talk by jayant haritsa in the vldb summer school held in bangalore in june 2004 and of course including many of the analogies and jokes that have been there in the slide and so on  refer slide time  03  20  so here is an acknowledgement for those set of material that have been derived from those slides so let us look back at xml again and recap what we have learnt about xml and what are its characteristics first of all before we look into the points in the slide itself  let us remember that xml is a platform independent  standardized and extensible markup language so  in a sense that its platform independent because it is written in character data and every platform should at the very least support character data and it ’ s an extensible markup language that is it does not have a specific set of keywords as such i mean of course it has key words where you say  where you give declarations but for describing the data itself  it doesn ’ t have any keywords the user who is describing the xml document can come out with his or own set of tags or meta data that describe how the data is organized and what semantics to attach to each data set in an xml store and xml syntax has a plain hierarchical structure which is easy to navigate and easy to enforce and easy to parse as well so we have  as a result we have query languages like xpath which translate an xml query into a directory like path expression which is quite intuitive for users who have been using computers and so to traverse directories and look for something in a specific directory and so on and last but not the least  xml is understandable or parsable by both the machine and the computer so if everything fails what you can do is just open an xml document in notepad or vi or emax or some such text editor and look through the document and see where there could be a problem especially  if may be starting tag doesn ’ t have an ending tag or some problem of that sort and of course there are issues like what about the amount of space  extra space that xml takes up and this can be answered by the fact that one need not store xml in character form on disk you might want to actually compress xml data and then store it on disk  so that you can and decompress it when reading it  so that the actual disk space that is taken up by xml is reduced so let us address the question of data interchange now using xml xml is said to be a platform independent source of data representation and representing data that is self-describing in the sense that the meta data is embedded within the data now  but what is the problem  what is real and of course the advantage of this being that you don ’ t have to worry about whether the end user is using the same platform as yours  refer slide time  03  38  if you are using linux  the end user could be using windows or solaris or mac or whatever and the end user could still use your services using soap or some other form of xml based message parsing protocol and you can still answer or you can still provide those services via an xml wrapper but what is the interchange problem actually or in its entirety ? for the interchange problem  we have to first note that most data is already stored in some existing databases it ’ s quite unlikely that databases that have been existing for large periods of time now will be replaced by an xml data store or will be replaced by something else that is unifying so databases are going to continue to exist and the question of data interchange will boil down to the fact that how to interface these databases using xml or using a common interchange language and databases will not only exist  will be updated through existing interfaces it ’ s again quite unlikely to expect that all updating and all interface to the databases will come through a common interchange format so what is really required is to provide xml wrapping to existing data bases and soap for example in this regard that is message parsing between objects existed long before xml came into the picture there were several different object middle wares like corba or dcom and so on which supported message parsing between objects over a distributed system but however one needed to be corba complaint in order to be able to  a platform needed to be corba complaint in order to be able to support corba that is corba had to be returned for that platform on the other hand when using xml  it does not bother about what kind of platform that ’ s being used because every message is parsed using an xml wrapper so soap is some kind of a generalization over message parsing frame works in distributed object base systems which has been extended from simple  small distributed systems like corba to a larger web base services using soap so a similar analogy also exists in data integration where data exists in different databases in preexisting forms and preexisting applications and they have to be integrated using  by wrapping the data around using xml wrappers  refer slide time  10  18  so one simple way to convert databases and now here we are assuming relational databases and for the most part we would be correct because majority of the database implementations around the world are relational databases so assuming that existing databases are relational databases and we need to be able to interface between these different relational databases which are across different platforms a simple way to perform this is to convert relational tables in their canonical form to xml documents and this slide shows  refer slide time  10  49  such an example where there is a slide called actors containing of two different columns last name and first name and this table became one xml document actors and slash actors  it ’ s a rooted xml document and it is some kind of a flat xml document in the sense that the number of levels in this document is fixed that is the actors file or the actors document comprises of several children called row and slash row where each row comprises of again several children  each child corresponding to a specific attribute or attribute name and this is an attribute value within the attribute name so it ’ s quite simple to map a given relational table on to a canonical flat xml file and given a flat xml file  it ’ s again straight forward to map it back into a relational table  refer slide time  12  07  now let us come to the question of storing xml documents itself that is having xml databases itself in many cases for example in several commercial implementations  oracle 9i and db2 and so on  the database gives an impression that it supports xml storage and xml based updates however what it actually underneath  it ’ s still a relational database and then there is an xml wrapper around the database however that is there are several other approaches to storing xml documents and in many cases it becomes necessary to treat xml not just as an interchange language but also as the language in which the data is stored and some of the issues that occur in this regard would be issues like data layout how would you organize the xml data there are again many alternatives to this question one can map xml data on to a relational database or one can map an xml dataset as a special kind of object in an object relational database or one can store xml data in native xml form or in text form and so on and what about updates ? i mean xml databases are prone to updates and here in xml unlike in say relational database it ’ s not just the values of attributes that can change  the actual structure of the database can also change very frequently in a relational database we basically assume that once we fix the schema  it is intact and it ’ s only the data sets that are updated that is new attributes are added or existing attributes might be deleted or modified and so on but the evolution of the schema itself that is change in a structure of the database itself is considered to be far less frequent if at all it happens so the database is modified  database is oriented  the relational database is oriented towards fast updates and retrieval of data and not the structure as such but in an xml database that may not be the case that is there could be updates of not just the values but also of the structure of in which data is stored in the database and in many case there may be no explicit schema that ’ s available  so well formedness itself is the schema so we don ’ t we don ’ t really know which is a valid structural update and which is an invalid structural update and what kind of constraints exists between different elements and so on  unless there is a xml schema or a dtd that ’ s available for us similarly there are different kinds of query supports that need to be supported by an xml database this standard flower queries that we saw earlier where for let where return kind of queries and also select  project and join based queries where which is also supported in xquery in addition to recursion or document construction that ’ s transformation from one xml document to the input xml document to the output xml document and a far bigger problem is of indexing how do we maintain meta data or how do we maintain indexes into an xml store ? such that we can retrieve elements as quickly and efficiently as possible whenever required and it ’ s not just the normal attribute and value index that we need to store but we also need to store full text indexing  what are called as inverted indexes  so to be able to search a full text elements or data sets quite efficiently in addition to these kinds of indexes we also need what are called as structural indexes  i mean structural indexes essentially talk about what are the relationships between  structural relationships between different elements is one a child of the other is one reachable from the other or what are the siblings of a given element and so on and in addition there are general requirements like scalability and recovery in case of failures  concurrency control  updates and during updates and so on  refer slide time  17  00  so let us look at xml storage structures itself and what are the different kinds of choices we have for xml storage basically we can divide the different kinds of xml storage that storage structures that are  storage paradigms or storage mechanisms that are available into three different classes  what might be termed as flat stream based storage or native xml storage and colonial storage so what do each of this terms mean ? a flat stream based xml storage essentially stores xml data as some kind of objects in an object relational database they are stored as clobs or remember what is a blob a blob is a binary large object and a clob is a character large object so you just store an object comprising mostly of characters  so character large object which contains its own mechanisms of access and retrieval as one of the attributes in a relational database so just like storing any multimedia object or some such object  you can store an xml dataset in a relational object relational database itself but of course the advantage of this is that  you don ’ t need to reinvent anything in the sense that there are several object relational databases that are already available and it ’ s just a question of using one of the object relational databases to store xml documents but however of course the flip side of this argument is that the database itself does not support any xml centric queries so you can query the database based on object relational constructs but not an xml constructs itself and all those xml specific query constructs like flower queries or xpath queries and so on can not be directly supportive the next kind of database storage strategy for xml is the native xml storage native storage essentially means that you design a new database from scratch for storing xml data  optimized for storing xml data so here you have to worry about everything that you thought about let ’ s say for designing a relational database  one is to think about the storage structure  block accesses that is the physical storage structure  indexing structures then some kind of query mechanisms and recovery mechanisms  concurrency control  transaction i mean throughputs and i mean how to maintain efficient transactions and so on so everything that goes into designing any conventional dbms should go into designing this native xml storage as well and there are quite a few examples of databases supporting native xml storage the third strategy is what might be termed as a colonial strategy a colonial strategy essentially means that colonize an existing paradigm using xml that is use some kind of  use an existing paradigm like say relational paradigm and then map every xml construct to a relational construct rather than note the difference between a colonial strategy and the first strategy which you just put xml storage as character large objects or one of an attributes of a relational table however a colonial storage essentially decomposes or deconstructs an xml document into relational constructs and reconstructs them back so whatever xml related query that you provide like xpath expressions or flower queries and so on  they have to be rewritten in sql and you have to have a mapping  one each to have a mapping between xquery constructs and sql constructs and vice versa and of course this can be both an advantage and a limitation the advantage primarily being that you don ’ t have to worry about number of techniques that the native xml storage has to worry about storage structures  block storage structures  concurrency control and recovery and so on however there is a significant if not huge performance overhead in terms of mapping an xquery construct into an sql construct especially when there is a recursive kind of a query that ’ s given  mapping it into an sql construct can take quite avail and running the sql query on the database can also be quite inefficient so let us look into the second strategy in a little bit more detail namely the strategy of native xml storage that is redefining or redesigning an xml database by looking at all the different aspects that needs to go into an xml store so what are the issues  what are the typical kinds of issues that one needs to worry about when designing a native xml storage one need to think about data layout  how is data organized on the disk and physical data layout is essentially that is if my disk is organized in terms of pages  disk pages or disk blocks what should each block contain and how should the blocks be organized and so on indexing is again a major requirement like we said before it ’ s not just attribute and value indexes that are important  we need to also look into full text indexing or indexing of phrases within text or structural indexes that talks about how elements are related to one another and so on  refer slide time  22  25  and it needs to also address query support  what kinds of queries are supported in this store and how do we optimize queries and how do we preprocess  do we need to preprocess or and if so or how do we preprocess for managing efficient query retrieval and so on then access control  concurrency control  updates and so on transactions  recovery and so many other issues  refer slide time  24  00  now let ’ s look at the question of data storage in a little bit more detail and what are the different approaches that are used for managing or storing physical disk blocks or managing the physical data storage that is how is an xml tree mapped on to physical disk blocks essentially one can think of  essentially an xml data set is a tree so this slide shows a particular tree like this where there is an imdb at the root node and there are different show elements in the second node and each show element has different sub trees like title  year  box office and so on and so forth and finally the leaf node contains the dataset that is available in this xml tree now how do we divide this tree into physical data blocks and which kind of division makes sense ? one way to think of  one way to look at dividing a tree into disk blocks is to cluster trees based on sub trees that is this show sub tree would go into one disk block and this show sub tree would go into another disk block and the imdb would just be in an index which in turn just stores pointers to each of this disk blocks now what is the advantage of such a storage ? the simple advantages that the entire sub tree is in one disk block  hence if i am looking at navigational queries where the user just navigates from like opens an element and looks at the sub tree and opens another element underneath and looks at that sub tree and so on like in a explorer kind of file system navigation for such kinds of navigational queries  this storage structure is very efficient because when the user is navigating  the navigation path is quite close to the actual way in which elements are related in the tree itself it is quite unlikely that the user would open this sub tree and start navigating here so in one disk access  one can read an entire sub tree however there is a flip side to this kind of data access itself suppose the user gives a xquery kind of  a user gives a query saying show me all elements  show me all show elements where the title contains whatever the fugitive or something like that now if the query has to be searched on a particular element like say title and even though we know which element has to be searched for  we still have to access every data block in the disk because every data block which contains the show element  which stores the show element would contain a title element so we still have to access every data block containing the show element in order to access such a query so to answer attribute kind of queries  such a kind of organization is a file organization is actually counterproductive or is not very efficient however for answering navigational kind of queries  such a kind of access is quite useful  refer slide time  27  23  the second kind of database storage structure is to cluster similar elements within a database  within a data page that is within a disk block so  this slide shows such an example where the same tree is taken imdb and show and title  year  seasons and so on however rather than storing an entire sub tree within a data block here  each element at a particular level are clustered together so  all title elements are clustered together under stored in one data blocks  so in the red data block let us say and all year elements are clustered together and stored in one single data block that is a blue data block and so on and then there are pointers that point to each of this data blocks and so any of these elements that don ’ t contain cdata or pcdata  you can cluster all of these into one data block and then maintain pointers from them to each of these data blocks now  again this is in some sense the dual of the earlier mechanism where such a kind of organization is very useful or very efficient for answering attribute queries so if i want to say show me all show elements or return all show elements where the title contains the term the fugitive then all that you need to do is to first access this block which contains the show element and find a pointer for the data block containing all the title elements and with just one data block access  for one will and of course one or more i mean depending on how many title elements are there but generally with far lesser data blocks than that are necessary in the previous case  we can access all title elements that are there in this xml store so answering an attribute query is far simpler  however answering a navigational query becomes difficult in this case and there are other techniques for this thing  it depends on how something is clustered if clustering is performed based on what may be termed as the lowest level elements where the elements contain just cdata or pcdata  it becomes difficult to navigate that is from show  you need to open a title and year and from title to year it requires a different block access and so on however it might be possible to cluster based on paths rather than based on single elements  so rather than clustering similar elements  some other techniques cluster similar paths therefore one would say that cluster all paths of the form imdb show and title in one block and all parts of the form imdb show and year in another block and so on so there are different variants  however if one were to ask the question which is the best way of storing  which is the best storage structure for xml databases  the answer would be depends essentially there is no single  there is no single technique that is universally most efficient way of accessing or storing xml databases and to a large extent it depends on what kind of queries that you expect from the users so if it is navigational queries  it might be better off to cluster the tree based on sub trees  so store sub trees within data blocks on the other hand if it is more of attribute searches or even say full text searches  it might make sense to cluster similar elements in a page rather than sub trees  refer slide time  31  54  let us look into the problem of indexing xml documents and what kind of indexing requirements arise for xml document firstly  we said that in addition to attribute value indexing  for which we can use traditional rdbms indexing like say b plus tree or a b tree  here we need two other kinds of indexes one is what may be termed as full text indexing were we should be able to efficiently search on free text data that are written within an xml document so one might just write a paragraph within an xml document and be able to search for some key word in that paragraph and very common mechanism of indexing full text is what is called as an inverted index an inverted index is very similar to what you would find at the end of a book like say text book were you have an index and there are certain keywords and if page numbers or section numbers in which the keywords are  in which those keywords are either defined or used or whatever so an inverted index on an xml document would index different keywords that appear in the document and then maintain links in to the xml document saying where each of these keywords can be found and keyword based indexing can be of two kinds  it may either be an xml aware keyword indexing or an xml unaware keyword indexing so what is the difference between the two ? xml unaware keyword indexing just looks for keyword searches so you just give a keyword search called the fugitive or jerry sign field or whatever that appears in the xml document and keywords are searched  this table here shows xml unaware keyword searches that is for every term that appears  it just stores the document reference which document contains this or probably the element reference or whatever so it doesn ’ t bother about where keyword appears and it only bothers about the keyword and the value in the keyword on the other hand an xml aware keyword or an xml aware index not only contains the keywords but also another element or another column which says in which element does the keyword appear from so there is one more index that this ampersand t1 here is an element index or is a key into an element index where it identifies each element uniquely that is present in the xml store so what this says is that the term fugitive can be accessed or the term fugitive appearing in the element whose key is t1 can be accessed using this pointer from wherever so if you are looking at attribute based searches and we want to say return all show elements where the title sub element of show contains the term the fugitive then an xml aware indexing makes much more sense than xml unaware indexing and of course the flip side of xml aware indexing is that as the number of elements increase and keywords are repeated across different elements  there is a huge amount of combinational choices that appear between a given term and an element pair so the term 1993 for example may appear in different kinds of elements  it might appear in births date  it might appear in release date  it might appear in show date or whatever and so on so different kinds of elements  so each of these have to be indexed separately and which leads in increase in the size of the index structure so this table shows different xml  native xml databases that are available and quite a few of them are already available like xyleme or natix or goxml and so on and most of them have been built from scratch and some of them like exelon or tamino have been built over existing databases so it ’ s not in a pure sense  a native xml storage but they do they are called native mainly because they do address many of the questions that are typically addressed in native xml storage and there is a wide diversity or wide ray  wide diversity in the kind of features that are supported  refer slide time  36  43  so xyleme for example supports full text searchers and xpath searchers and xquery searches but it doesn ’ t have any what kind of apis that it provides or unknown and natix doesn ’ t support any of these but just supports some low level primitives and so on and there is partial support for xquery and so on  refer slide time  37  10  let us now move into the second part of the stock where we look into managing semi structured data so until now we have been looking primarily into xml and xml is a very comprehensive tool to manage semi structured data and so what is semi structure data and what is the significance and why is it important to study about semi structured data ?  refer slide time  37  53  so  let us first define what is semi structured data there are sever several different definitions and of course semi structured data what we understand often is data that whose structuring is not rigid and data which doesn ’ t conform to a very rigid structuring mechanism there are other kinds of definitions as well like data that is inherently self-describing and self-describing data  so with no rigid schema which basically implies there is no rigid schema  the data itself defines its schema and that is known as semi structured data and again data which are generated half hand and without planning and so on  there also called semi structured data  refer slide time  38  29  now in today ’ s world semi structured data is getting more and more prevalent and data structuredness is becoming much harder to impose and define an impose for example what is the structure of the world wide web  i mean the world wide web is the huge data store but without any structure  in fact but one can not even say that it is an unstructured data store because there is some semblance of structure that is present like one can think of some meta data tags that are available for each html text  some href hyper link references and so on and directories and so on and so forth but on the whole it is not possible to define specific structure and then impose the structure on the world wide web so the world wide web is the best example for huge semi structured data store and most semi structured data stores are characterized by rapid or rapid changes or very frequent changes to the data set so not only is the data not defined apriori or the structure of the data not defined apriori  it is also changing dynamically or it ’ s also changing rapidly and it is not able to  it ’ s not possible to formalize semi structured data using a nice formal model like the relational model and the best data structure that ’ s used to formalize a semi structured data are usually graph structures and there are several different examples for semi structured data like web information systems and digital libraries or even data integration from heterogeneous data sources can be considered to be a semi structured data problem where there are several different databases that are already defined and there are so many such databases that it becomes impossible or impractical to be able to impose a common structure over all of them and it is easier to treat them as a large semi structured data store so the very common example of semi structured database is of course the internet movie database which we have been seeing continuous examples of when we are talking about xml as well  refer slide time  40  54  so imdb is a classic example of a collection of semi structured data and even though i mean what makes a semi structured ? the fact that even though it just stores information about movies  each movie is different from the other each movie may belong to a different journal and it may belong to different country  some may have language fields  some may have some star cast fields  some may have some other kinds of fields which may not be available in the other records and so on so let ’ s consider an example from a movie database let ’ s say imdb is the database and of course imdb not just stores movies  it also stores information about tv serials and documentaries and other such movie related or movie like data that have been released  refer slide time  41  49  so even within a movie  let us say even within the movie category different movies could be different  movie one may have information about the cast and the director in the movie and who could be the cast in the director but movie three may have something called actors and actresses  it may make a distinction between who is the actor and who is the actresses and the direction that is rather than just the director  it can talk about a direction team or who directed it and so on so the structure of each record that makes up a movie element in imdb may be different from one another and some data elements may annotate more information than others and some may have missing fields and the kind of relationships that exist between each of these different elements may also change  may also vary between different records  refer slide time  43  02  and in addition to that  in addition to changes in structure the way in which data is organized itself could change for example one might represent an actress name as first name  last name and one might represent it as last name  first name or one might  some other record might have its something like just a name and so on  refer slide time  43  20  and data gets added to this database dynamically and as a result and dynamically and from different sources from different independent sources so it becomes difficult to enforce a particular kind of schema restriction on this database so what is the problem here or what is the main problem in managing semi structured data ? the main problem is trying to ascertain what structure or what is the common structure to use for different data sets that are being added to the database and to be able to formulate queries and formulate query languages and so on  refer slide time  44  15  and in addition we should also note that the structure of data element is implicit so it ’ s not that the user providing the dataset first defines the structure and then provides data according to it but structure is embedded within the data itself  refer slide time  44  33  now the structure has to be first discovered and then a common structure has to be evolved over the entire data set and this is what is called as the problem of discovery of structure that is the structure should be discovered such that the structure is indicative in nature rather than constraining in nature  refer slide time  45  09  that is the common structure that evolves out of a database should not constrain the database to adhere to a specific structure but rather should be indicative of what kind of data is available in the database and how they are interrelated to one another  refer slide time  45  45  so here is an example this slide shows an example of what is the main problem in or what is the main challenge in semi structured data  refer slide time  45  58  in relational or what may be termed as traditional data management what really happens is you have a uod or a universe of discourse like company or an academic institute or university or whatever and then you have a model of the uod that is there is the schema that defines how data in the uod should be organized it ’ s not ease organized but how the data should be organized and data that is collected from the uod is first taken through this model and populated into the database so when we say that an employee should have a pan number as the primary key and name and dependents and salary and so on  it ’ s only those sets of data that are extracted from the uod and then sent into the database and especially for example  if an employee doesn ’ t have a pan number and the pan number is the primary key then it is not possible to add that employee record into the database because the primary key has to be not null  it has a not null constraint and so on so constraints are enforced when the database is being populated and the query also is formulated within the model of the uod so the query just takes the model of the uod and queries the database accordingly  refer slide time  47  35  however in what might be termed as the post internet data management which is the main problem with semi structure databases  the universe of discourse whatever is the universe the world wide web or the internet movie database where users can independently add movie data into the database  the universe directly populates the database it doesn ’ t go through any common mental model by which the database is populated  in fact there might may or may not be any mental model here as to how the data is organized but the data is directly populated by the uod  refer slide time  48  29  now the query  when the query is searching the data it should not only know what data to search  it should first try to find out what is the mental model or how is the data organized  what is the schema for this by which the data can be searched so that is basically the schema discovery problem or the implicit schema discovery problem and in addition to that  the schema discovery problem often encounters the problem of what is called as the large schematic structure that is even when we discover a schema  this is again called the maximalist world notion  in contrast to the minimalist world model of a traditional database system in a traditional database system whatever is not allowed by  whatever is not explicitly permitted by the schema is forbidden so everything is forbidden unless explicitly allowed by the schema so it ’ s a kind of exclusivist data model where things are thrown away unless they are permitted  refer slide time  49  05  however in a schema discovery process  it ’ s an inclusive model where everything should be permitted unless it is sure that it is forbidden  that is unless it is sure that some kind of a relationship can not exist  all kinds of relationships between data elements are permitted so it is not  one can not apriori define what kinds of relationships exist between data elements unless of course we know that some kind of relationships do not exist in the database or can not exist in this uod so the associated problem from this is that the discourse schema can be quite large rather than in contrast to a relational schema where the schema is much smaller than the data set itself so in the internet movie database for example  we might discover that lot of different  we might discover lot of different things that go into a movie based on what people add into the database and we should allow for all such relationships  unless of course we know explicitly or unless of course we know specifically that some kind of a relationship can not exist for example we might know as a rule  i don ’ t if this is a true but we might know as a rule that in the universe of discourse that in the world of movies  it is not possible for a director to be the boss of the producer or something like that so the producer reporting to the director or whatever so unless of course we know that some kind of relationship does not exist  we have to accept all kinds of relationships that are  we might have to accept a dataset about a movie which does not contain any movie star we might have to accept a data set where a movie contain 10 different movie stars and so on so all such relationships should be accepted unless explicitly forbidden and as a result  the actual schema that is generated is far bigger than a typical relational schema  refer slide time  52  26  there are several different application areas where this is useful and of course where these have been tried out and these include data integration where you design an interface to integrate different desperate data sources coming from different locations  each having their own schematic structures and the second major area of application is in digital libraries which consists of again different kinds of semi structured data coming from different sources several more application areas like genome databases or scientific databases that talks about scientific documents  similar documents  citations  references and abstracts and where it was published and ratings and so on and so forth and of course in e-commerce applications where the discovery of structure problem becomes very important in business to business systems where each business if it is quite big  it might be difficult or impractical to impose a very specific schematic structure over the entire business house so one needs to be able to resort to semi structure data management  when managing b two b business systems  refer slide time  52  44  so let us skip through these slides where the need for discovery of structures are motivated even more or which talks about how discovery of structures can go about and addressing the discovery of structures problem in itself would take a complete  would involve a separate session and it is clearly beyond the scope of this particular lecture  refer slide time  53  38  however we can give some kind of thumb rules which talk about how implicit structure can be discovered from a set of desperate data sources and most of these revolved around looking at some kind of regularity in the data set and then generalizing based on this regularities and so several different kinds of data mining and machine learning and artificial intelligence kind techniques are explored for trying to fit a structure on to a data set and there is also a notions of what is a best fit a best fit data structure or a schematic structure should not be too general and should neither be too specific and so it has to be  the structure discovery process should be able to generalize based on whatever examples were encountered while passing through the data however it shouldn ’ t be too general in the sense that it can accept anything or any structure in the data set that is it should also identify what are the forbidden relationships among data elements in addition to what are all the possible relationships among the data elements  refer slide time  55  41  and several different kinds of query languages are also supported for semi structured data in addition to xquery which is primarily meant for xml databases and of course keyword based searches which are useful for  which are useful for full text searching there are other kinds of primitives like navigation based queries or searching for patterns or temporal queries based on how particular data element evolves over time and so on  refer slide time  56  16  so xml is an embodiment of semi structure data in the sense that xml is the natural choice in which semi structure data can be organized  refer slide time  56  27  and the problem of discovery of structure over xml  over semi structured data can reduced to discovery of a xml schema given a desperate set of xml documents  refer slide time  56  27  and queries can be revisited using xpath and xquery expressions based on whatever structure that have been discovered  refer slide time  56  43   refer slide time  56  51  so let us summarize whatever we learnt in this session and of course the idea of semi structure data itself is a vast ocean and it is beyond the scope of this lecture to explore all of them so therefore we looked at native xml storage and xml publishing and different kinds of storage structures that have been proposed for xml and mainly we touched upon the problem of the semi structured data and the larger problem of discovery of structure which is very important for semi structured data management so with that we shall end this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 41 case study  part one database design hello every one welcome to another session in database management systems until now we have seen different aspects of dbms design we have seen what a typical life cycle of a database management system looks like essentially we saw that a dbms is something like or could be treated analogous to the engine of an information system and what is an information system ? anything  a part of a larger system that deals with information flow  management  storage  retrieval  handling and so on so everything to do with information is usually driven by a database management system at the core so what i assume that you should know by today ’ s class is that you should be familiar with what are the roles of different  what are the roles of a typical dbms system  what are the different kinds of actors that exist in a typical dbms and conceptual modeling using the er model  we saw little bit about er or entity relationship based design for conceptual modeling and also the relational model which is the physical model or rather it ’ s not exactly the physical model as in terms of the disk storage that ’ s used but it ’ s still called the physical schema because that is the way in which the database schema appears to all the programs that utilize this dbms system so the relational model and different terms as to what is meant by a table relation or normalization  functional dependencies and so on  refer slide time  02  15  and also a little bit of set of rules as to how to convert a given conceptual model in er diagram to a given to its correspondent relational schema so today what will do is let us look at a typical case study database design case study so how do we go about designing an application around a dbms system note that we are not here talking about the design of a dbms itself but we are talking about design of an application on top of a dbms so when i was talking about uods in one of my earlier class  this is what we are going to look at that is we are going to consider particular universe of discourse and then take it up so rather than taking toy examples and rather than taking an example comprising of just a little bit of database or data management requirements i have taken up fairly comprehensive example at the same time one should be aware of the fact that real life databases for example the moment when we talk about databases  we are first reminded about banks and railway reservations and so on i have not taken either of them because they are massive database systems  indian railways for example huge in terms of the amount of transactions that happen and amount of data that is generated and stored every day so it would be a disservice  in fact it would be plain wrong to take up such a massive database as a case study and in fact we would be simplifying it so much that you will not appreciate the actual complexity that lies in managing such a huge system so what i have done here is to take up an actual system that you might actually want to implement as part of a class project or something which and several of such database management systems exists in practice  refer slide time  5  04  so the case study that we are taking is shown here it is a conference management system as you know several conferences today are managed by a web based interface where you can manage all the activities and data that are related to the conference so what is the conference management system contain ? so  here is a brief description of the uod that the universe of discourse and the different kinds of requirements that make up this uod so let me read it out from the requirements itself of course this is a simplified conference management system  it does not make sense to take up a real life database in its complete gory details but at least what i hope is that the gist of a particular requirements of a given uod should be captured by these requirements so let us look at the requirements once again the technical program of a large conference is decided by a program committee so there is a committee of people who decide which paper should be published or which paper should be presented and which paper should not be presented in a given conference and the program committee is headed by a pc chair or a program committee chair all other members of program committee will act as reviewers so people who would submit papers to the conference and they would be reviewed by different members of the program committee now that ’ s about the program committee so let us look at the next set of requirements what about a paper ? a paper is authored by one or more authors of course and it should have a unique contact author so there should be one author in the paper who should take responsibility of the paper so it is to this author  its to him or her that all further correspondence will be addressed to so correspondence regarding whether the paper is accepted finally or is it rejected or it should be accepted after another process of review and what kinds of changes to be made in the paper and so on and so forth so look at the other set of requirements  refer slide time  07  10   so any person who is a member of the program committee can not be an author of any paper that is published in the conference of course in real life conferences  it ’ s a bit more relaxed than this that is you can actually submit papers to a conference even though you are a program committee member but for our purposes let us keep it kind of stringent  stringent meaning it ’ s just going to make things simpler so as long as you are on the program committee  you can not publish any papers in this conference  refer slide time  08  27  what is the reason for that ? because a program committee member should not push his or her own papers into the conference  so they should act only as reviewers what about authors ? an author may submit one or more paper  there is no limitation on that but each paper has a unique identity so we are selecting papers and not authors  so that ’ s an important thing here and a paper can not be submitted to more than one conference or a journey so if i submit a paper somewhere  i can not submit the same paper to somewhere else and i can not obviously also summit a published paper somewhere else now the last set of requirements is  the last block of requirements is that a paper is reviewed by at least 3 reviewers so when i send a paper to a conference  it goes to at least 3 other reviewers and a reviewer will give suggestion as to whether to accept or reject the paper so that is shown here  refer slide time  08  44   a reviewer may either accept or reject a paper or be neutral towards a paper  if the reviewer can not take decision the reviewer just says that i am neutral to this paper so the actual decision should be taken by the other two reviewers and in very rare cases all three reviewers would be neutral and well the program committee chair or the pc chair should take a call on such papers so based on reviewer comments pc chair prepares a set of paper for acceptance and then those set of papers are accepted into the conference now let me pause for a little while here and go through the requirements once again so carefully look at the requirements of your end user  there is a program committee  there is a pc chair ultimately what is that we have to do ? we have to take care of the conference activities  refer slide time  09  47  now if you look at any set of requirements carefully  you ’ ll see that there are two things that a requirement says a set of requirements will indicate an explicit set of required behavior that is these things have to be there  the paper says that every paper has to be reviewed by 3 reviewers and every paper has to have a contact author and so on there are some things which are explicitly required by our specifications similarly there are some things which are explicitly forbidden by this specification if i have sent a paper to a conference  i can not or i may not send the same paper to some other conference so this is a specific forbidden condition  you shall not do this and so on but if you see again carefully  there are number of requirements or number of things here which one might talk about which or neither required nor forbidden by the requirements can you think of some requirements for the conference management system itself that is neither required nor forbidden ? let us take something like how many paper should a reviewer review ? can i say that a reviewer can review 5 papers  10 papers or exactly 1 paper and so on there is nothing that is said in the requirements here if you look at this carefully the requirements says neither yes or no  so there is nothing said about this requirement itself so that is an important thing to note in most application design when we capture requirements  the requirements tells us something that needs to be there and tells us something that should not be there but is silent on a large number of things as well so that greatly affects how we design our application and whether our application  suppose you design a dbms system and you say that because of some constraint  somewhere you say that a reviewer can not review more than two papers is it correct or is it wrong ? so there is no specific answer to this because the requirements neither requires this nor forbids such a thing so usually this is how a systems development life cycle  some of the top or the early stages of a system development life cycle would look like so if you look at the slide here  you have the systems requirements specifications were there are set of explicitly required conditions and there are set of explicitly forbidden conditions and this is the entire uod where the number of things which are not addressed by the requirements  refer slide time  12  35  now based on these you get a high level design of your system  usually this in the form of a er model or whatever when it comes to dbms design so you end up with a er diagram here  in turn you reduce the er diagram to a relational schema or a low level design and then you get a system model  relational schema plus transactions and so on small set of application logic and some set of constraints  triggers  stored procedures or so on and then you get a system model usually what happens is this process  how do you get design from requirements or how do you move from high level design to low level design these sets of processes involve human activity or human creativity to be more specific and as is so common with human actions  the system model may not exactly reflect the systems requirements specs ideally what should the system model do ? the system model should exactly reflect the requirement specification here so as shown here  the red spot is slightly bigger here what is that mean ? that the system model has more forbidden conditions than what is explicitly forbidden by the requirements specification itself so it brings us to some two important concepts  when we are designing real life system  the concepts are what are called as liveness and safety  refer slide time  14  14  so look at the english definitions of liveness and safety liveness means what  that something is alive or something is existent and so on and safety of course is obvious now if you look at the systems requirements specifications  why would a set of requirements so let us go back here  refer slide time  14  31   why would a set of requirements say that this is forbidden  why would a set of requirement say that a member of the pc committee shall not be an author of a paper why would a set of requirements say ? because it would compromise the integrity of the system if that were to be alone because if i allowed a pc committee member to be an author of a paper  there is quite a likelihood or there is quite a chance that the pc committee member may push his or her own paper and have an unfair advantage over others so it is the safety of the system is getting compromised  so that is why i forbid the behavior  forbid this activities so essentially whatever is forbidden usually constitutes of safety requirement  in order to safe guard the system against integrity violations i say that this shall not be there however what is the simplest form to build a safe system ? how do we build a system that is absolutely safe and from any kind of integrity violations ? simple  don ’ t start the system at all if a system that doesn ’ t work  it is absolutely safe if your database system doesn ’ t work at all  it is absolutely safe because it does not violate any integrity constraints at all so that is why a trivial way of ensuring safety is to make a system that doesn ’ t work but that is not what we want in addition to safety we need  we require certain behavior to happen so those are what are called as liveness requirement that is the system should perform certain activities and should not perform certain activities so let us use some notations when just to talk about mismatches now suppose i say that r of srs here  refer slide time  16  17  is the set of required behavior by the srs or the systems requirements spec similarly f of srs is the set of forbidden behavior or safety constraints specified by the srs similarly let us say r of m or where m is the model that we build  the final system model that we build so let r of m denote the set of all liveness criteria in the system model that is the system model will do this and f of m denotes the set of safety criteria in the system model that is the system model will not do this and so on now when we talk about a system model that is when we talk about building a system model from a set of requirement spec  we can think of various kinds of mismatches that can occur so  various things can go wrong when we are talking about capturing user requirements into a system model what are the things that can go wrong ? a tentative list of things i mean these are not the only thing that can go wrong  in fact in addition to this a huge number of things can go wrong but anyway  refer slide time  17  20  now let us say what if r of m  remember what is r of m r of m is the set of required behavior of the model what if r of m is a proper subset of r of srs what is this mean ? the set of required behavior by the model is a proper set of the required behavior by the srs that means that the model is incomplete the srs require certain behavior to be done but you don ’ t implement all the behavior  you don ’ t factor all those behaviors  you factor a subset of those behaviors now what if in addition to this r of m being a proper subset of r of srs  in addition to this let us say the r of srs minus r of m that is the set of requirements specified by the systems requirements spec which are not factored into the model is actually a part of f of m  is actually a part of the set of forbidden behaviors by the model what is that mean ? it means that not only does the model address all the requirements in the requirement spec  in fact there are certain requirements of the requirement spec that the model actually forbids that is that the model will not do so it means that the model is not only incomplete  it is incorrect it forbids certain required behavior similarly what if r of srs is a proper subset of r of m that means that the model is performing more activities than what is required by the srs itself that is the model has extraneous behavior and having extraneous behavior is not that is having an added feature for example  suppose the model as for the birth date of the author when were you born and so on does not always be a desirable feature  it can actually be potentially unsafe when is it potentially unsafe  when r of srs is a proper subset of r of m that is what i saw here and the difference between r of m and r of srs is actually a part of f of srs that is the extra behavior that or the extra so called value addition that your model is doing is actually part of the set of safety conditions that is actually forbidden by your requirements so the model has extraneous and unsafe behaviors so when you build a system model  you should be careful to or this is one set of guidelines by which you can measure whether your system model is good enough against the requirements that is just try separating the requirements into set of required behavior and set of forbidden behavior and your model also into set of required behavior and a set of forbidden behavior so let us see some more mismatches here now what if f of m is a proper subset of f of srs that is the set of all forbidden things of the model is a proper subset of the set of all forbidden things of the srs that means that the model is unsafe that is the requirements require you to forbid certain things which you are not forbidding similarly  what if it is the other way around that is the model forbids more than what is required by the srs then you say that the model is conservative now conservative again doesn ’ t mean that you are safe usually in english  we say that oh let us be conservative and go about like this and take this action and so on but just be saying let us be conservative doesn ’ t necessarily mean that you are safe why ? because you could actually be violating a liveness criteria  refer slide time  20  58  so this is the case here that is f of srs is a proper subset of f of m that is the model forbids more than what is required to be forbidden by the srs and the difference that is what the model forbids which is not forbidden by the requirements is actually part of the required behavior of the srs so if forbids something which actually needs to be there  so in being more conservative you are actually hampering the liveness of the system so just being conservative doesn ’ t always mean you are building a safe model  refer slide time  22  56  so let us see  let us take a step by step approach to see to let us try to divide our requirements coming back to the conference example to see what kinds of required behavior are there by the model and so  by the requirements spec and so on so of course the kind of the example that we are seeing here is a simplified example and real life examples are far more difficult than these but anyway this gives the gist of how to factor a requirements into set of required behavior or liveness behavior or set of safety conditions and so on so what could be the step by step approach ? let us  the first thing is we have to find the set of required and forbidden conditions then once you start that then start identifying the various entities  their attributes  the relationships between entities and so on then build a complete er model for the problem statement and then convert the er model into a relational model and perform normalizations if they are not already normalized  refer slide time  23  46  so let us look at some of the required conditions a paper is reviewed by at least 3 reviewers that actually means that a paper should be reviewed by at least 3 reviewers so if i try to review or if i try to accept a paper that is being reviewed by only 2 reviewers then your conference management system should flag an error  it should not allow it to do that so this is a required condition that is a paper is reviewed by at least 3 reviewers each program committee has a pc chair  so this is another required condition  you can not have a program committee without a chair each paper has a contact author if you go back through the requirements that we saw  all these have been picked from the set of requirement itself so each paper has a contact author that means that each paper should have a contact author and so on a paper is authored by one or more authors  so obviously this means that you should not accept a paper without any authors in it and reviewer must comment or must give comments which can be one of the following accepted  rejected or neutral you can not  the reviewer can not give any other comment other than this three and the reviewer should give one of these three the reviewer can not remain silent saying that i am neither accepted nor rejected nor i am neutral about the paper and so on and the reviewer should give only one of this so this is a set of required condition what are some of the forbidden conditions ? we saw some examples already a paper can not be submitted to more than one conference or a journal so you may not submit a paper to more than one conference and so on an author of a paper may not be a member of a program of the program committee so that ’ s another forbidden  explicitly forbidden conditions so that have been explicitly stated in the requirements that these are forbidden and a paper may not have more than one contact author so there has to be one and only one contact author  so that it may not have more than one so these are some kinds of required conditions and forbidden conditions and so on so when you build a system model  what you should be able to do is take up each set of required conditions and see whether your model also has that required behavior take each set of forbidden conditions and see that whether your model also forbids those conditions and the other way around  take each set of required behaviors by your model and see that whether they are actually required by the set of requirements so on  refer slide time  24  53  so let us now go to the next step and start identifying entities so how do we identify entities and what is an entity an entity is some logical item one could say or logical something of which has an independent existence of its own so i was about to say logical entity which kind of becomes a circular definition in this case so any way let us look at the problem statement once again  refer slide time  25  57  the technical program of a large conference is decided by a program committee headed by a pc chair if you just look at that statement  you can already find several entities here so the conference is an entity of a large conference  conference is something that has a logical existence program committee is an entity here essentially the nouns in this sentence and is headed by a pc chair is an another entity and one could even say the technical program could also be an entity and so on so just reading through each sentences  you can identify what could be potential entities in your system  refer slide time  27  25  similarly the next statement  all other members are some statement down here all other members of program of the program committee will act as reviewers so reviewers is another entity as soon as we found so similarly a paper is authored by one or more authors  so paper is an entity now author is a  now here there is a question this is not as simple as that so is author is it an entity or is it an attribute is an author an attribute of a paper that is a paper has an entity and this paper is authored by so and so authors and so on now some cases we can make author as an attribute of a paper but here we see that author also has an independent existence why because we have something called a contact author  we have something called author may not be a member of the program committee and so on so the author may actually participate in other relations as well and an author may write more than one paper if i make author as an attribute of the paper entity  there is no way to relate or there is no relate to equate that paper one and paper two have been published by the same author and so on so there is no way to equate those two papers so  in our case it ’ s better to take author or its better to design author as an entity itself similarly again some more  a paper can not be submitted to more than one conference or a journal again there is an entity called journal and so on conference we already saw is an entity now what about relationships ? now that we have identified entities  of course we are no way near to finishing the identification of entities but let us look at relationships i mean you should have got the ideas as to how to go about identifying entities and its attributes and so on  refer slide time  28  31  now again take a look at the statement now what are the entities that you can see in a given statement ? the entities would generally be the nouns of a particular statement now what could be the relationship here  refer slide time  29  05  now look at the verb something is headed by something else and so on  so or handled by and so on so the verb statements that connect two or more nouns would actually be prime candidates for relationships so if you look at this statement here  the technical program of a large conference is decided by a program committee headed by a pc chair so as you can see here  this part already forms a relationship that is or rather the first part is a relationship here that is conference is handled by program committee that is handled by or technical program decided by if i have to make it very explicit so conference is handled by a program committee and as you can see the technical program of a large conference is decided by a program committee so basically it is a one to one relationship that is one conference  one program committee but then look at this here program committee  we have made program committee into a week entity here why is it a week entity ? first of all what is the week entity ? if you notice carefully  a week entity is an entity or if you remember your er modeling classes  a week entity is an entity which does not have an independent existence of its own  its existence is defined by a relationship and the relationship that defines a week entity is also called a defining relationship so this is a defining relationship  so shown by double arrows like this  refer slide time  30  53  and this is what is called as the total participation if you remember er classes again so a program committee totally participates in this conference that is the same program committee may not participate in more than one conference entities and this is the defining relationship so if there is no program committee then there is no conference similarly let us look at another statement the technical program committee of large conference is decided by a program committee headed by the pc chair  same statement so again program committee here and pc chair heads program committee that is a pc chair is an entity which we have found and a pc chair heads a program committee  refer slide time  31  53  now here if you can see again  we see that this cardinality that is a program committee is headed by a pc chair is clear that is one program committee should have exactly 1 pc chair but 1 pc chair can head how many committees ? it is neither specified or rather it is neither required nor forbidden  it ’ s not specified in the requirements so here we have made it into a n cardinality 1 to n or whatever that is a program  a pc chair can head any number of program committees and so on so because there is no explicit specification as such in terms of how many program committees can a pc chair head  refer slide time  32  26  again some more relationships  so take a look at this statement the technical program of a large committee whatever headed by a pc chair and so on all other members of the program committee will act as reviewers so let us say we already had this one that is program committee is here and that is headed by a pc chair and 1 pc chair can head 1 to n program committee  program committees and so on now a program committee consists of reviewers which is apparent by the second sentence that is all other members of program committee will act as reviewers however it is not exactly correct  it ’ s not exactly what is specified by the requirement that is what is the requirement say all other members of the program committee so basically what does this means ? that is all members who are not pc chair that is who are not acting as pc chairs can be reviewers of this so if i take two separate relationships like this in isolation  they don ’ t form a consistent set here because it is violating a forbidden condition  it is violating a condition that the pc chair may not be a reviewer so how would you rewrite this condition here ? so basically we will introduce a new entity called members and basically form  what might be termed as a generalization specialization relationship so remember the extended er model allows for a specialization relationship were given a member or given a entity  you can inherit one or more entities from it that is it actually shows the is a relationship and in addition to the is a relationship  here we have this circle called d what is the d specified ? d basically specifies that these are disjoint entities that is no entity instance that is part of reviewer can also be pc chair and vice versa  refer slide time  34  07  so our committee would now look like this committee would consist of members where members would in turn consist of reviewer and pc chair which are disjoint so committee can consist of 1 to n member  n number of members but there has to be exactly one pc chair so basically in addition to this  we have to give a cardinality of 1 here and n here for n reviewers and 1 pc chairs again let us look at some more statement when to identify relationships a paper is authored by one or more authors and has a unique contact author so again we can see that we can identify relationships straightaway here  paper is authored by authors  one or more authors and an author can author how many papers there is no specification  so we just introduced 1 is to n so we are kind of being liberal model  we are not being very conservative model that is we are allowing for more behaviors than has been required that is an author can submit any number of papers unless it is explicitly forbidden of course  refer slide time  35  31  and a paper should have a contact author that is a paper here any number of papers should have exactly one contact author so  one author who acts as the contact author again in isolation these two relationships are not sufficient because why do you think they are not sufficient  let me pause for a little while here why do you think going back to these two set of entities  why do you think they are not sufficient in themselves i am sure you would have got the answer the thing is while a paper can be authored by one or more authors and a paper can have contact authors  there is no relationship that states that the contact author should be one of the authors here so one of the authors from here should be taken and be formed as the contact author for a given paper that is so you have to  in the earlier case there was a disjoint relationship  pc chair can not be or may not be a reviewer and here there is a membership requirement that is a contact author ought to be one of the authors of the paper  refer slide time  37  29  so how would you go about  let us come back to this again later so again any person who is a member of the program committee can not be the author of any paper so we will come back to that earlier thing after taking this other constraint also into perspective and then draw the entire set of relationships at one goal so what is this say here ? any person  note that now again person is a noun here so we need another new entity called person so any person who is a member of the program committee can not be the author of any paper one way to show this is have a person called  have an entity called person and make a disjoint specialization between member remember we had an entity called member here  members or whatever so we had an entity called member and an entity called author so an author may not be a member or a pc committee member and a member may not be an author and both are persons and so on so that way you can identify that any person can not be both an author and a member of the pc committee or the program committee and a paper may not be submitted even though sometimes when talking in english  we say a paper can not be submitted to more than one conference or journal to be more precise  it actually should be a paper may not be submitted to more than one conference or a journal  refer slide time  38  35  so again here what  first of all what can we imply from that ? a paper can be submitted to a journal and a paper can be submitted to a conference so these two can be implied but what is that we actually need what we actually need is that while a paper can be submitted to a journal as well as submitted to a conference  it may not be submitted to more than one conference or a journal  the same paper may not be submitted to more than one conference or a journal  refer slide time  39  21  so here in order to identify that we have used the union or the concept of a union were it is  one might call it the opposite of the specialization condition were you take two or more entities and form a union out of them and form a single entity so take a journal or a conference and form a union out of them and make an entity called event and the paper is submitted to an event and how do you say that it has to be submitted to only one conference or a journal at any point in time  only thing is make this  the cardinality of event as one here so given paper can be submitted to one paper here or 1 to n papers that is a given event may have n papers and a given paper may be submitted to exactly one event and what is that event ? an event could be a conference or a journal calling a journal as an event is not exactly correct sounding in terms of the english definition but anyway we have used this term but you might think of a better term than event to specify or to take the union of journal and a conference  refer slide time  40  50  similarly a paper is reviewed by at least 3 reviewers and a reviewer may either accept or reject or be neutral towards the paper so what is this statement say ? a paper is reviewed by reviewers  so n number of papers is reviewed by 3 to n that is at least 3 or anything more than 3 now take a look at the second half of the statement a reviewer may either accept reject or be neutral towards a paper that means the reviewer is going to give a result the result is either accept  reject or neutral or be neutral now  if you see carefully the attribute called result does not belong either to the reviewer nor to the paper because a paper  the result of a paper is actually a combination of the results of three or more reviewers and a reviewer may be reviewing more than one papers  so you ca n't assign result to a reviewer as well so the attribute called result is actually an attribute of the relationship itself so remember that we had talked about attributes which belong to relationships so as long as there is an instance of this relationship existing in the system  an instance of this attribute may also exist in the system whenever the relationship instance does not exist  when can a relationship instance not exist ? for example when there is a paper which is not assigned to any reviewers for example then there is no instance of the relationship at all that is reviewed by and so on so there is no question of a result existing in this or even when a paper is assigned to just 2 reviewers so if you look at the relationship here  the relationship requires that a paper be submitted to at least 3 reviewers so therefore there is no instance of such a relationship existing and therefore there is no instance of the result in the databases similarly coming back to attributes now let us say conference now the requirements doesn ’ t say anything as such but as application designers  it is our responsibility to identify some of the major attributes of a particular entity and also identify key attributes so here in this case a conference name and the date and the place  topic and all of those things would be attributes of the conference and usually something like the conference name would be the primary key or would be the key attribute of the conference  refer slide time  43  21   refer slide time  43  37  similarly program committee does not have any primary key because it ’ s a week entity type which we actually saw earlier so a program committee does not have a key attribute but it may have other attributes like what is the strength of the program committee  how many people are there in the program committee as of now and so on so a program committee has a is a week entity type having no key attribute but it has its own other attributes and have a look at the entity called person for person again you can think of lot of different attributes  what is the name of the person now when you say name  usually in several cultures  you actually divide a actually name into first name  last name  middle name and so and so on  the initials and title and so on and so forth so name could actually be a composite attribute here which in turn has multiple other attributes  many other attributes say first name  middle name  last name  title  initials and so on and so forth  refer slide time  44  11  then there could be age or date of birth address and usually you need to have a unique identity to unique way of identifying a person this was actually created by some of my students whose pan number has a key attribute for a person but usually in a  it is quite unlikely that in a conference setting  you would ask for a person ’ s pan number usually it would be the email address of this person which or the contact email address of this person which would be the key attribute similarly they could have something like phone numbers and phone number here is treated as a multi valued attribute which means that this attribute can have multiple values so what is that mean ? that a person can have multiple phone numbers and i hope you know the difference between a multi valued attribute and a composite attribute a composite attribute is also made of multiple attributes but each of these different attributes may belong to different domains so name can have first name  middle name  last name were a middle name can be constraint to be a single letter  if the middle name is an initial whereas first name and last name can be varchars or strings and so on but when i say phone number  when there are multiple values for that phone number  all of these values belong to the same domain or of the same type so that ’ s the difference between a multi valued attribute and composite attribute  refer slide time  46  25  so similarly other these thing when i say that  when i say author you can give an author_id for each author  a login id or whatever and every other attributes of a person would be inherited by author because author is a person and so an author is supposed to have a pan number and date of birth and phone number and so on and so forth  refer slide time  46  45  similarly for paper  its already specified there that each paper would have a unique identification or a unique key so for paper  paper_id would be the key and several other things what is the title  what is the category  the classification of those attributes  the paper content itself  the keywords that are given for the paper and so on  all of them could be attributes of a paper  refer slide time  47  16  and when i say pc member  you can again give member_id for each members  again some kind of a login id or something which would form the primary key for each member  refer slide time  47  32  and again several other this thing reviewers and pc chair so reviewer would have something called subject of expertise and pc chair would have something called conference headed and so on which can be attributes of those respective entities and journal again  so journal _id year of publication topic were i can always have a journal_id as a primary key  refer slide time  47  49  so finally we come to an overall schema for the entire system where we put all of these together to form one big er schema so let us spend a little bit time in turn by reviewing this schema so what all did we go about looking at we started out by saying where is the conference yeah  the conference is here  refer slide time  48  06  so conference is handled by a program committee and program committee is a week entity type  so it has no existence without a conference and program committee consists of different members  so among the members there are reviewers and a pc chair and there is one pc chair  there is exactly one here and this is a disjoint relationship that is a pc chair may not be a reviewer and a reviewer may not be a pc chair and a pc chair heads program committee so pc chair may head one or more program committees like this and similarly you have a conference and a journal forming an event that is a given conference or a journal may be forming an event to which a paper is submitted so a paper may be submitted to 0 or 1 event so you may not submit a paper at all or you may submit it to at most one event and an event should have at least one paper or it may have any number of papers and so on and a reviewer reviews a paper or paper is reviewed by reviewer and there is a constraint here that is a paper is reviewed by at least 3 reviewers now the reviewer  the review of a paper  the process of review of a paper will result in a result being assigned or will create a new attribute called result which the reviewer assigns for this paper so this result is actually a attribute of the relationship itself now again a paper is authored by an author and there is a contact author so there is exactly one contact author and it is authored by one or more authors and so on and both author and members or persons  why do we need this persons ? because we are having attributes of a person separately that is a person should have a pan number and address and telephone number  email and so on so  all of those attributes of a person are inherited by both members and authors similarly all of the attributes of or a combination of the attributes of conference and journals is inherited by comes to event and all attributes of members are inherited by reviewers and pc chairs so a member should have certain privileges or benefits or whatever  all of those are inherited by both reviewers and the pc chair and because pc chair is a separate member  a pc chair may also have some attributes which are not shared by reviewer or which may not exist for a particular reviewer and so on so what we saw today is we have taken a fairly complicated example  i mean it ’ s not a and this is a realistic example a conference management system in fact you can search the internet for something called confman which is a freely available i guess open source conference management system which uses a back end database management system in order to manage activities like this or you might going to msrcmt which is the microsoft research conference management tool which is actually used by major conferences around the world and which also has something like this that is there are reviewers  there are authors  there are papers  there are pc chairs  there are committees and so on and so forth and there are little bit or rather significantly more complicated than this but the level of complication to which or the level of detail to which we have seen in this is fairly representative enough because we have seen some of the major kinds of conceptual requirements that arrive for example a pc chair is a member of the program committee but may not be a reviewer  but may not be any other reviewer and contact author should be part of the author list and so on and so forth so  all of this form tricky details which manifest themselves during your conceptual design so what will do in the next class is to take this idea forward and take up individual chunks or pieces of this er diagram and try to convert them to the lower relational schema and see what kinds of tricky situations arise when we convert them to a relational schema so let us finish this class here and see you all in the next class so this brings us to the end of this session database management system dr s srinath department of computer science & engineering indian institute of technology  madras lecture no # 42 case study  part 2 database design hello everyone welcome to this session in dbms so in this session today what we would be doing is we would be continuing from the previous session were we talked about a case study of a database design so again let me put forth a kind of assumptions or the prerequisites that i am expecting for this session i am assuming that you know or you have gone through the first few sets of lectures on the life cycle of a dbms and you also know what is conceptual modeling  what are the building blocks of conceptual modeling  what are the different kinds of nuances in conceptual modeling like a week entity type  like a multi valued attribute  like a composite attribute and so on you also know the relational schema or the relational model and the characteristics of relational model like  the models like the functional dependencies and normalization and so on and you also have an idea of how to convert what kinds of rules that you can use to convert a given er schema to a relational schema so i am assuming that so will not be going into the rules of how to convert a er schema into a relational schema except that we are going to take a running example of a conference management system that we started in the previous class and start by explaining which kinds of  we just take up specific aspects of the er diagram of the conference management system and just convert them to the corresponding relational model and of course we will also look at few kinds of transactions on top of this relational model i know that we have not really looked into transactions as part of this series of lectures as such but i will also define transactions as we go along and in an informal sense of course there we shall be looking at transactions and transaction processing in much more detail in later sets of classes but at least i will be defining transactions in an informal sense so that we can see what kind of transactions at least we can think of on top of these sets of requirements so the main idea behind this being that to try to cover the large or the first few aspects of a systems development life cycle that the high level design  the architecting  the conceptual modeling  the high level transaction design and just have a look at how does the entire system look like and then you can over a period of time  when we address several different questions throughout this course  you can try to target each one of these different architectural aspects of the system like the relational model  the conceptual schema  the business logic and so on and transactions  transaction manager and so on and then try to see how you can make a detail design into each of these different aspects of  each of these different building blocks of the overall system  refer slide time  04  28  so let us come back to the case study for a database design that we are looking at so today we look at the relational schema design and the transaction design for the case study that we are going to look in  refer slide time  04  52  so what was the case study that we have been looking at ? let us briefly review the case study once more and before going into today ’ s work so the case study was about a conference management system and we went through the set of requirements in the previous class so let me briefly go through them once again to try to see what exactly is required by this conference management system like i said in the last class  this is a fairly comprehensive example in the sense that it ’ s fairly representative of a real life conference management system although not as detailed as one a real life conference management system would be far more detail than this but at least it kind of captures the main essence of a conference management system so what are the requirements ? the technical program of a large conference is headed by a program committee so a conference is  a large conference the adjective called large actually we haven ’ t really made use of here because there is no other further set of requirements as to how to handle small conference so we are just talking about a conference so as far as we could understand as a systems designers or systems analyst  the technical program of a conference is headed by a program committee and is determined by a program committee and the program committee in turn is headed by a pc chair  a program committee chair where we saw a program committee chair is a one person who heads the program committee and all other members of the program committee will actually act as reviewers and again we saw in the previous class that the reviewer and the program committee chair should not be the same person so a person who is the pc chair will not be a reviewer and vice versa and so that ’ s about the program committee the next chunk of requirements for about the paper itself  the papers which are going to be presented with the conference so a paper is an entity again which is authored by one or more authors and as a unique contact author it basically says that it should have a contact author and it ’ s a unique contact  there is only one contact author for a paper and any person who is a member of a program committee can not be the author of any paper  like i said in the previous class also this is not a realistic requirement were because in most real life conferences this is not exactly true but just to make things simpler  we are imposing this constraint in order to see the repercussions on our database design  refer slide time  07  56  and an author may submit one or more papers each of which has a unique identity so a paper has to have a unique identity which in turn means that a paper ought to be treated as a separate entity in its own so an author may submit one or more papers  there is no constraint on that however a paper can not be submitted to more than one conference or a journal at the same time so  a given paper can not not be submitted to more than one conference  so given a particular paper it has to go to a particular conference and so on and a paper is reviewed by at least 3 reviewers  so is reviewed so it basically says that a paper shall be reviewed or should be reviewed by at least 3 reviewers so even if a paper is reviewed by 2 reviewers  it ’ s not sufficient it has to be reviewed by 3 reviewers and a reviewer should give a decision and the decision should be either accept or reject or be neutral towards the paper so a reviewer has to give one of the 3 decisions and the reviewer ca n't say  i am neither going to say accept nor am i going to say reject nor am i going to say that i am neutral about this paper i have an opinion but i am not going to state it and so on  refer slide time  09  09  so anyway based on the reviewer comments  the pc chair either accepts or rejects papers and prepares a set of papers for acceptance now again we also saw this in the previous class were the characteristics of a given requirement specifications so it ’ s again good to review them once more before we go in to relational model design as well so that will help us understand the suitability of our model much better so  any given requirement specification has a set of explicitly required behavior so the requirements or the specification say that this and this has to be there  this functionality has to be there and then there are a set of explicitly forbidden behavior which says this and this functionality should not be there or shall not be there and so on so we also noted that the set of forbidden behavior constitutes what are called as safety constraints which usually you forbid something in the interest of safety so if you violate that constraint  it means that you are compromising the integrity or the safety of the system similarly the set of required behavior constitute the liveness of the system so a completely safe system is one which does not work at all so if you don ’ t go out into the battle field then there is no danger of you being hit by a bullet but the thing is that does not make a soldier  you have to go into the battle field so there is a set of required conditions were which form the liveness properties of the system  so that are specified by the explicitly required behavior and in addition to liveness and safety usually in almost all cases  the uod will contain a number of behaviors that are neither required nor forbidden so the requirement specifications neither says yes  it is required nor does it say no it ’ s forbidden so one might call that  call them as permitted behavior or whatever so depending on how you treat this kind of mid way area  your conference management tool can offer an edge over the others in the sense that my conference management tools manages birth dates of and sends greeting cards and so on i mean they are not  they are neither required nor forbidden and so on and coming to the first few steps of a systems development life cycle or an sdlc  we see that what we want to do or what we are going to do here is given a set of requirements specs  requirement specifications we come out with the high level design we kind of address this in the previous class in the sense that given srs  we created an er diagram for the given srs  refer slide time  11  35  so today we are going to be doing this one and of course with a little bit of transaction design will also be doing this one so from the high level design  we go to the low level design and from the low level design add dynamics to that is high level design low level design were purely in the static so add dynamics to that by adding the transactions and so on  so you get into a system model so the system model should ideally reflect the systems requirement spec or the srs document and of course there are  usually there are mismatches between the systems model and the requirements spec and rather than going about wildly looking for features or looking for characteristic features of the systems model  it is good to classify this deviations between the systems model and the requirement spec into different kinds which helps us understand in what way is the systems model deviating from the set of requirements  refer slide time  12  48  so we also saw this in the last class  refer slide time  12  47  were we saw how to quantify in some sense or the kind of deviations that system model has from the set of required requirement specifications so let us briefly again revise through all these so that it helps understand how to evaluate a given system model  two or two or more systems model against one another so one can say that a given systems model is incomplete  if the set of required behaviors that the model performs is a proper subset of the set of behaviors of the requirements itself  so it is incomplete and being incomplete itself doesn ’ t  may not be all that serious but sometimes it may be quite serious  it could be incomplete and incorrect  refer slide time  13  07  so when is the systems model incomplete and incorrect ? when not only does it not  only does it not satisfy all requirements but it forbids certain requirement that is it explicitly does not satisfy certain required behavior so it forbids certain required behavior that is that ’ s when you say that the model is actually incorrect and it ’ s possibly unusable and if the set of requirements  if the set of required behavior by the model is a super set  proper super set of requirements spec then the model has extraneous behavior  it does something extra and again providing an extra feature doesn ’ t always necessarily mean a good thing  it could potentially be unsafe as well when will it be potentially unsafe ? when this extraneous behavior actually trespasses on the forbidden conditions of the requirement spec so r m minus r of srs actually belongs to f of srs that is where you trespass on the forbidden requirements  refer slide time  14  57  similarly if i have a set of forbidden conditions by the model being a subset of the forbidden conditions by the set of srs then the model is unsafe that is the model does not forbid everything that the srs is asking you to forbid and if it is the other way around that is if the model forbids more than what the srs asks you to forbid then you say that the model is conservative but again conservative doesn ’ t necessarily mean that it ’ s a safe system why ? because in trying to be conservative you might be blocking certain liveness behaviors  you might be actually stopping certain behaviors that are required functionality for the model so if f m minus f srs is actually a part of r of srs that is part of the required behavior then the model is not only conservative  its actually incorrect so it forbids certain required behavior and what are the required conditions  what are some of the required conditions in this case study ? we saw certain required conditions in the case study that is a paper should be reviewed by at least 3 reviewers for example and each program committee should have a pc chair  refer slide time  16  10  and each paper should have a contact author and a paper is authored by one or more authors  i mean we can not accept papers with no authors in them and so on and a reviewer must give a comment or must give a decision either accepted or rejected or neutral so reviewer can not afford to keep silent on a paper and paper and what are some of the forbidden conditions in the model ? a paper can not be submitted or may not be submitted to more than one conference or a journal which is shown in the slide here and an author of a paper can not be a member of the program committee like we said even though this is not realistic  we have chosen that for the sake of simplicity here that is i can not submit a paper as well be in the program committee to review the paper and so on and a paper may not have more than one contact author  each paper should have one and only one contact author  refer slide time  17  11  so there is only may email sent out for each decision and it has to go to the contact author and so on so  having said that in the previous class  we went into each requirement sentence by sentence and identified several kinds of entities and relationships among them and also attributes for these entities  refer slide time  17  40  and finally put all of these entities and relationships together to form one big er diagram for the problem so let us spend a few minutes to revise the er schema for this problem this is important because now we are going to cut this er schema bit by bit and then convert it into a relational schema now what are the first thing that we did ? we started by a conference a conference is handled by a program committee and so on so a conference is an entity and it is handled by a program committee and we also saw that the program committee is a week entity type it ’ s not  if you can see  if you can notice carefully it is not really specified by the requirements and that the program committee are to be a week entity type but we have to infer it  i mean that is why the high level schema design that the conceptual schema design is a very human centric process  you have to use your common sense in order to identify or in order to say which kinds of  what should be the characteristics of each of this entity type and so on so program committee is a week entity type and this is a identifying relationship which says that a program committee that handles a conference defines the existence of the program committee in the first place and a program committee consists of different members and the set of members are divided into a disjoint set of specialized members which are called reviewers and pc chair so given a member a member could either be a reviewer or a pc chair and there are to be only one pc chairs  so there is a cardinality constraint here and these are disjoints  so therefore a reviewer may not be a pc chair and vice versa so  a program committee consists of members and the pc chair heads the program committee so one pc chair heads any number of program committees or given a program committee there is just one pc chair for that program committee and similarly a reviewer reviews a paper or a paper is reviewed by a reviewer so a given paper  a given reviewer may review 0 to n paper that is a reviewer may not be assigned any papers at all there is no  there is neither a requirement nor a forbidden condition as to how many paper should be assign to a reviewer however there is a condition on how many reviewers should be assigned to a paper that is a paper should be assigned at least to 3 reviewers  so 3 to n so there can be any number of reviewers but there should be at least 3 reviewers now once a reviewer reviews a paper  the reviewer gives a result now the reviewer has to give a result  it ’ s a required condition again that is a reviewer has to say either accept  reject or neutral so now this result is an attribute of neither the paper nor the reviewer itself why because a paper is reviewed by more than one reviewer and a reviewer may review more than one paper so you can not assign result to any one of them in fact the only way place you can assign result is to the relationship itself that is that the process of being the process of reviewing a paper by a reviewer gives out an attribute called result so in addition the paper itself is related to authors and a paper is authored by one or more authors note that here it is one or more  it ca n't be zero or more so we also had a condition  forbidden condition that authorless papers are not accepted  are not acceptable and a paper has to have exactly one contact author so one of them is a contact author and it is authored by many of these other papers and so on and in addition authors and members form a disjoint specialization for person that is we store certain personal details of people in the system their address  name  date of birth  phone numbers and so on and all of them have to be stored for both members and authors and similarly a paper can be submitted to exactly one event or zero or one event  maybe submitted to exactly one event that is it need not be submitted so but if i choose to submit  i can submit it to exactly one event and what is an event ? event is a conference or a journal so if i am submitting a paper to a conference then i should see that it ’ s neither submitted to any other conference or nor to any journal as such so this forms the overall er schema as part of the problem that we saw in the previous class now let us go to the next step now that is from the er schema we should reduce it down to a relational schema and which is actually managed by the dbms now the parlance of a relational schema is quite different i mean you have  you don ’ t have entities and relationships and attributes and so on although  you do have something called attributes but in a different sense you have tables and columns and rows and attributes which is a column and there are key relationships then foreign keys and decomposition of tables and functional dependencies and so on and so forth so let us take things step by step and take up er to relational mapping so the first set of er to relational mapping that we are doing is shown in the slide here so we started out with conference now conference is handled by a program committee and we also saw that one conference  one program committee and program committee is a week entity type that is its existence is defined by a conference and the conference is defined by several attributes like conference name  place  date  topic and so on  refer slide time  23  34  and the conference name is used as the conference key or the primary key for the conference so how do we reduce this to an er model ? first thing we notice that the entity conference can become a table called conference and all the attributes that the conference has will form the attributes of table of which conference name is the primary key now here if you look at program committee  program committee may have a key but that is not sufficient why because  the program committee has no existence without a conference so essentially while we can create a table called program committee which corresponds to the entity called program committee  it is not sufficient to just add the attributes of program committee here and leave it at that why because we need the conference name as well so  because it is defined by the conference name  program committee for which conference so take the primary key of conference and make it into the primary key of the program committee itself so in fact this would be a foreign key relationship into the conference table  refer slide time  25  17  so the next set of conversions now a pc chair heads a program committee and a pc chair and every other member note that again for the sake of clarity  i have not shown the entire er diagram here but if you remember how it was a pc chair here  refer slide time  25  41  is a member this kind of relationship here defines a is a relationship so a pc chair here is a member and every member is defined by a member id the every member is uniquely identified by a key attribute called member_id and whenever a general entity class or an entity set has a key attribute every subclass of it or every specialization of that entity type also inherits the key attribute therefore the pc chair also inherits the member_id as its key attribute so now the member_id is the key attribute of this pc chair and what is the key attribute of program committee it is the conference name  so that is a key attribute of the program committee now a pc chair heads program committee and look at this relationship it ’ s a 1 to n relationship so 1 pc chair heads  may head several program committees and so on so how do we convert this ? first of all converting a pc chair to a table is trivial so make a table called pc chair and put all the attributes of pc chair in addition to all the attributes of member into this table and make the key of the super of the general entity type that is a super class if one might call it that  so of the general entity type called member plus any key attributes in pc chair make into composite key in the pc chair table now for this relationship itself  how do we go about doing that ? take program committee as the key  now as the table and its key attribute was conference name now in addition to the key attribute called conference name just put the member_id of the pc chair here as one more attribute so there are two foreign keys here  so one forming the conference name attribute and that is one is the conference name and the other is the member_id however it is sufficient if we just keep the conference name as the primary key even though this is a foreign key here why is it sufficient to keep just the conference name as the primary key ? because let us look back here  refer slide time  28  02   program committee is a week entity type and its existence is defined by the conference and what is the primary key ? a primary key usually of course is something which a key attribute which defines the existence of a particular tuple in a table so it ’ s the conference name which defines the existence of the program committee therefore while we add member_id to the program committee table  it is sufficient if we just keep conference name as the key for this program committee table now the next set of attributes er to relational mapping so have a look at this thing  so we already saw pc chair here pc chair is a member and a member could be a either a reviewer or a pc chair and a program committee can consist of several members and each member is uniquely defined by a member _id now because this is a super class or a generalized entity type  this member_id is inherited by both reviewer and pc chair so it forms a part of the primary key for both reviewer and pc chair and they may have other key attributes as well  refer slide time  29  20  so one way to reduce this is we already made a table for pc chair with member_id as the  with member_id as the key  we can just make one more table for reviewer and again say member_id as the key and all other attributes now you might ask a question  why not make table for members itself members is an entity and why not make a table for members itself note that here it is not really necessary why because  a member is either a reviewer or a pc chair but not both because it ’ s a mutually exclusive relationship between reviewer and pc chair so there is no special identity assigned to a member other than a reviewer or a pc chair and we have seen that we need this pc chair table in some other context  in the previous context here so we need pc chair as a separate table in some other context and therefore we have also made one more table for reviewer and that ’ s it and both of them are collectively exhaustive in terms of this thing so not only are they mutually exclusive  there are also collectively exhaustive and they collectively define the complete set of members now the bigger problem here is the reduction of this larger relationship what is that relationship ? a program committee may comprise of several members however a member can also be a member of several program committees  so it ’ s a n  n relationship not a 1  1 relationship so how do you reduce a n to n relationship into a relational schema ? the best way to reduce that is make a separate table so we are calling this table as membership here where just take the primary keys of both of these what is the primary key here ? the conference name so just take the conference name and member_id  so of a given member so given a member i know of which conference is that member of  pc member of and so on so  n to n relationships can be reduced by creating a separate table and combining both of the primary keys into that table  refer slide time  32  21  the next set of reduction here so we have a paper that is authored by an author however a paper can be authored by more than one authors and an author can submit any number of papers and of course a paper has a key attribute called paper_id whereas author has a key attribute called author_id so again because this is an n to n relationship  you need one entity called paper with paper _id as a primary key rather one table called paper with paper_id as the primary key  one table called author with author_id as the primary key and a separate table called authored by which relates this and this so it ’ s an n to n relationship  so which contains both paper_id and author_id as part of its tables so  two foreign keys into both these earlier tables  refer slide time  33  01  now on the other hand for the contact author each paper ought to have exactly one contact author and it ought to have a contact author so as you can see here this forms a 1 to n relationship so in such case in a 1 to n relationship  there is no need to create a separate table here except that for a given paper  you just add the author_id here which is the id of the author who is the contact author so the primary key of the single entity type here can just go as an attribute to the table of the multiple entity type in a 1 to n relationship now again another part of the schema as you can see here we are taking parts of the schema  breaking it down and bringing them down to the actual relational schema now here we saw that both members and authors are persons obviously of course and why do we need that person relationship here ? because there were certain other requirements that we needed to capture that is for every person in the system  we need to have some personal details like the pan number or the email address or whatever that we use to uniquely identify a person  the phone numbers  the address name  last name  first name and so on and so forth and whatever else that is required for maintaining records about a person  refer slide time  34  10  now we said that members and authors are persons  that is member is a person and author is a person in addition they are disjoint that is the members and authors are disjoint why ? there is again a specific safety condition that is explicitly specified in the requirements that no pc member ought to be or is allowed to be an author of any paper so you only submit papers  you only consider papers by authors who are not members of your program committee and members have member_ids and so on and authors have author_ids as their words and a member can be a reviewer and a pc chair so among here we already have tables for reviewer and pc chair and we saw that we don ’ t need a table for member and we already have a table for author so  all that we need is a table for person that is a person and pan number and so on and so forth and when we say person  we also have to add all of these details but we see here that phone number is a multi valued attribute that is a person can have more than one phone numbers so when you have a multi valued attribute  you can not  you don ’ t know how many number of phone numbers a person is going to have one person may have just one phone number  another person may have 5 phone numbers  another person may have 3 phone numbers and so on so in order to deal with multi valued attributes  you have to create a separate table here called phone details where you put the primary key for person plus the phone number of the person so the primary key may repeat while the phone number may change  so you can have all possible phone numbers associated with a person and of course you might have got a doubt that why are we creating a separate table called person here and not club them into members and authors because members and authors are mutually exclusive as well as they are collectively exhaustive as far as this system is concerned that is in this system we are only talking about either a pc member or an author and nobody else as far as the requirements is concerned the answer is yes  you can do that is you can put  if you want to do that then what you have to do is take this pan number and bring it down to the member table or an author table so that it forms part of the primary key of each of them and then take all other details of person and bring it down to either member or author and then for phone number  you need to take just this one the pan number of this one and then create the different phone numbers here and so that would form a good  i mean that would form a valid derivation schema but as you can see the problem there is that when you bring pan number down to member  you suddenly have two key attributes there is a pan number and member_id so which attribute do you associate with phone number ? now it is not incorrect  it is not in correct if i associated member_id with phone number as well in phone details it is not in correct if i say member_id plus phone number or it is not in correct if i say author_id and phone number but it is bad design why is it bad design ? because when you perform any kind of systems design  we should always be concerned about what kinds of changes will the system expect in the future so in the future we may want to add more categories of people into this system for example so right now the only two categories of people we have in the system are either member or author and they are mutually exclusive and collectively exhaustive but then they need not be collectively exhaustive in the future  we might add one more kind of a person maybe say delegate or a conference registrant or conference volunteer or something like that who is neither a member nor an author and in that case the conference delegate or a volunteer is also a person who has a phone number and pan number and so on and so forth so the safest bet to associate phone number is with the pan number so that is the primary key of the person attribute is the safest bet for associating the phone number attribute so as far as this particular schema is concerned here  it is not in correct if i say that oh i won ’ t have separate table called person but in the interest of or in the interest of anticipating what might be required in the future  this might not to be a good idea in future and the software design or any kind of information system design is full of such implicit requirements in the sense that if you write a software that works here and now  it is not usually sufficient your software should be extensible and extensible with minimal changes and with minimal impact and that is what makes a good design  refer slide time  40  11  so basically this is what i said earlier that is one way to take this up is either bring all of them down that is from person bring it down to member but then we see that member  there is no table called member as at all so bring it in turn down to reviewer or pc chair so you can either bring down everything to either reviewer or pc chair and author who are all persons and then put pan number and every other attribute there but in the interest of robustness of your system  it might probably be a good idea not do that and create a separate table called person itself  refer slide time  41  23  now coming to some more parts of the er schema so a paper maybe submitted  a paper may not be submitted to more than one conference or journal at the same time that is if i am submitting a paper to this conference then one of the requirements is that  it is not submitted to any other conference neither is it submitted to any other journal note that even though we have not using journal at all in this system that is we are not  the journal is not playing an active role in the system but it is still required because we need to check whether a paper is submitted to a journal or not and here we have  how did we model that using a conceptual model ? we used a separate entity called event were an event could be either a conference or a journal and conference has its conference name has its key and journal has journal_id has its primary key and a paper can be submitted to 0 or 1 event that was what we had made here so how do you reduce this to a relational model ? conference  we already have a table called conference so just create a table called journal as well because journal is an entity and it has its own key and attributes and so on now because a paper is submitted to one of these two  journal or a conference just take these two primary keys and add them as foreign keys here so put both journal id and conference id or conference name rather as part of the paper so given a paper  you know exactly to which journal or conference was it submitted now note that these are added as foreign keys and not primary keys of course that is they don ’ t define the existence of the paper and a characteristic of foreign keys is that foreign keys can be null that is because a paper can be submitted at most to one of these two  that is one of either journal or a conference one of these is guaranteed to be null in any given tuple in fact if they are  if both of them are non null then we have an integrity violation  we have a constraint violation in the system already and the last one is the attribute which is assigned to a relation itself that is a paper should be reviewed by at least three reviewers so 0 to n papers reviewed by 3 to n reviewers and the reviewer should assign some result accept  reject or neutral and the result as we see here belongs neither to the paper nor to the reviewer in fact the result belongs to the process of reviewing itself so the result belongs to the relationship called paper is reviewed by a reviewer so how do we manage this ? in order to capture that the result belongs to a relationship make a separate table in the name of the relationship so reviewed by becomes a table and how do you recognize or how do you define tuples in this table take the paper id or take the primary key here  make part of the primary key here  take the primary key here member_id that becomes a primary key here and the result  refer slide time  43  38  so for this paper_id  this member_id gave this result and so on so whenever we have a relationship that is part of the  that is part of the rather whenever we have an attribute that is part of the relationship then we have to create a separate table in order to maintain this  in order to reduce this to the relational model  refer slide time  46  01  so that brings us to the relational schema itself  so the final relational schema so how does the relational schema look like ? there are so many tables that that we have already found there is the conference table which we created to begin with where conference name is the primary key and then there is the date of the conference place  topic and so on and so forth and then there is the pro program committee table which actually represents the week entity type called program committee therefore it takes the primary key of the conference table namely conference name as its own primary key and then there are other attributes like strength which is the attribute of the program committee and then there is member_id which is the foreign key into the pc chair table which says who is the pc chair of this program committee and so on and then we have the reviewer table  after the program committee table we have the reviewer table where member_id is the is the primary key and of course pan number  we inherited from the person attribute because we may require it for other purposes and then some other attributes like subject_of_expertise and so on and then there is the pc chair table where member_id is again the primary key and maybe some other attributes like number of conferences that is headed and so on and whatever and in addition we have inherited any other attributes from members or persons and so on  because pc chair is a member which is in turn a person and so on and then there is the membership table  remember were the membership table came from the membership table came because a member or pc member can be a member of more than one conference committee and a conference committee would have more than one pc members and so on so in order to maintain this n to n relationships  we created a separate table called membership where both conference name and member_id form the primary key and which are both foreign keys into their respective tables that is the conference and either program committee or either reviewer or pc chair and so on and it ’ s actually the reviewer not the pc chair and then we have the author table where author_id is the primary key then pan number is inherited from the person entity and everything else i have not specified any other attributes that an author might have then authored by is another table in order to denote the n to n relationships between an author and a paper that is an author may submit any number of papers and a paper may have any number of authors  more than one authors in there similarly person is a separate entity now we created the separate entity called person like i said previously in the interest of extensibility of the design rather than even though it is not in correct to just take all this person details like first name  middle name  last name  date of birth and put them straight to reviewers or authors and so on but in the interest of extensibility and the interest of the robustness of the system  we have made person into a separate table and of course as you can see  there is no mathematical rule that says that you have to do that and its again a common sense decision that we expect that more kind of persons may be added to the program  may be added to the system at a later point in time and so on then a phone details is another table where which is required because of the multi valued attribute in the person table were a person can have any number of phone numbers so you have a pan number and phone number combination which details any number of phone numbers that a person may have and then there is the journal which is required just to keep track of the fact that a paper may not be submitted to more than one conference or journals at the same time and then the journal_id  in fact for our purposes here the only requirement of journal is the journal_id or the primary key of the journal in this thing so the journal_id comes into the paper as well as conference name or not the conference_id and we can add an integrity constraint that no tuple shall have both journal_id and conference_id as non-null attributes or not null attributes and so on and reviewed by is another table which comes out because of the attribute called result which is a member of the relationship reviewed by rather than a member of either paper or reviewer  so we get a separate table so once i create table tables like this then you can essentially for each table  you can write the corresponding sql statements  refer slide time  50  35  so create table conference conf_name place and so on and say primary key conf conf_name and so on or something like create table program committee and conference name and strength and so on  refer slide time  50  57  and say primary key is conference name and but conference name is the same conference name as the conference name of the other table therefore you have a foreign key relationship saying that it refers to that attribute in that other table and so on so  basically that i wo n't go in to the creation of those sql statements again i am assuming that you know sql and given a set of relational  i mean given a set of table specifications i am sure that you know how to write corresponding sql statements and give it to a dbms client which in turn can create a tables in a dbms at the back end now let us briefly come to the last aspect of today ’ s lecture as well namely that of designing transactions or designing the dynamics of the system we have not really talked about transactions as yet in the course of this series of lectures but nevertheless for the sake of completeness  let us just look at one or two transactions and we leave it at that and we won ’ t go in to lot of details into transactions itself so what ’ s a transaction ? a transaction is a set of activities or a set of tasks which represents one semantic activity or a set of database tasks like read  write  update and so on which represents zero or more database updates which represent one semantic activity this is a very simplistic definition of transactions but we will just have it at that  refer slide time  52  29  and transactions have to follow  have to possess what is called as the acid property again i shall not be going into more details into this acid property but you will be looking into acid properties in a later lecture in much more detail acid basically means stands for atomicity consistency isolation and durability so essentially what it means is that the entire set of activities or tasks that form a transaction should be performed as one atomic whole either you do the entire thing or none of them and they should be performed in isolation of other transactions so no two activities of  no activities of two or more transactions should interfere with one another logically and so on and in order to manage a design transaction  we should also know what kind of system architecture we use in this so let us make some assumptions about the system architecture so we are using a client server architecture where the databases serve or is powered by a dbms or a database management systems and the client sessions are stateless meaning what ? the server does not remember previous client interaction so every time you need the server to do something  you need to give every possible requirement or every possible data to it in order to perform this activities  refer slide time  53  42  so let us look at some transactions some simple transactions that give us an idea as to what kind of activities would go on so let us say registration  let us say i want to register a new author or user or whatever reviewer or pc chair into the system  refer slide time  54  37  so how would you register ? typically you would have a form in front of you where you would have all details what are the details ? look at the person or author or reviewer tables and just make all those details there  the name  first name  last name  pan number and so on and so forth and phone numbers and so on so verify those details for any syntactic errors then read the corresponding tables  verify that there are no duplicate registration that is one person is not trying to register multiple times once this is not there  once this is not the case  once we verify that this is not the case then you can update the table now what this means is that all this four activities have to be performed as one semantic whole as you can see how many different activities are happening here  there are n number of reads happening here and one updates happening here either all of them should happen or none of them should happen so  all of these form one semantic activity called the transaction similarly login so let us say that i have created user id for myself  i have to login so verify user name and password field for syntactic correctness that is i provide my user name and password for login then read the corresponding table author  pc chair  reviewer table  match specified user name and password  we have not really taken care of password then  refer slide time  55  35  so another idea here is that once you start designing transactions  you will see that you need these two attributes also that is user name and password which we had not factored in at all when we are looking at the er or the relational model so you have to go back and make those changes and this thing and then you have to maintain sessions and so on and let me not go into sessions  session management and so on and one more transaction which i am not going to go into detail here  upload paper transaction or whatever  refer slide time  56  19   refer slide time  56  27  and you can think of several more transactions like this assign reviewers  submit review  arbitrate so all of these are semantic activities which perform zero or more updates of the entire system so how would the entire system look like ? see why did i just take up transactions for a brief while because just to show you how does the entire system look like  refer slide time  56  49  so here we had the er model and from the er model we develop the relational schema and this is the dbms here  actually this whole thing would be the dbms  so this is the database here and on top of the relational schema there would be a transaction manager which would manage transactions  on top of this would be what is called as a business logic manager which handles multiple transactions and so on which in turn would be served by some kind of a dbms server through which the client connections are handled so this would  basically this would conclude the overall system design of one particular case study of the entire system of course firstly  we are not comprehensive i mean this not comprehensive enough to take up a real life system but mainly the whole idea here was to give an idea  the whole idea behind this is to give you a perspective into what it takes into designing a particular dbms so with that let us come to the end of the session 